{"notes": [{"id": "rJEyrjRqYX", "original": "HJxQXBo8tm", "number": 53, "cdate": 1538087735272, "ddate": null, "tcdate": 1538087735272, "tmdate": 1545355378282, "tddate": null, "forum": "rJEyrjRqYX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Reduced-Gate Convolutional LSTM Design Using Predictive Coding for Next-Frame Video Prediction", "abstract": "Spatiotemporal sequence prediction is an important problem in deep learning. We\nstudy next-frame video prediction using a deep-learning-based predictive coding\nframework that uses convolutional, long short-term memory (convLSTM) modules.\nWe introduce a novel reduced-gate convolutional LSTM architecture. Our\nreduced-gate model achieves better next-frame prediction accuracy than the original\nconvolutional LSTM while using a smaller parameter budget, thereby reducing\ntraining time. We tested our reduced gate modules within a predictive coding architecture\non the moving MNIST and KITTI datasets. We found that our reduced-gate\nmodel has a significant reduction of approximately 40 percent of the total\nnumber of training parameters and training time in comparison with the standard\nLSTM model which makes it attractive for hardware implementation especially\non small devices.", "paperhash": "elsayed|reducedgate_convolutional_lstm_design_using_predictive_coding_for_nextframe_video_prediction", "keywords": ["rgcLSTM", "convolutional LSTM", "unsupervised learning", "predictive coding", "video prediction", "moving MNIST", "KITTI datasets", "deep learning"], "authorids": ["nelly.elsayed5@gmail.com", "maida@louisiana.edu", "mab0778@louisiana.edu"], "authors": ["Nelly Elsayed", "Anthony S. Maida", "Magdy Bayoumi"], "TL;DR": "A novel reduced-gate convolutional LSTM design using predictive coding for next-frame video prediction", "pdf": "/pdf/b5c5837202d2f45bc52d02ef302a9dbf4ac5f77c.pdf", "_bibtex": "@misc{\nelsayed2019reducedgate,\ntitle={Reduced-Gate Convolutional {LSTM} Design Using Predictive Coding for Next-Frame Video Prediction},\nauthor={Nelly Elsayed and Anthony S. Maida and Magdy Bayoumi},\nyear={2019},\nurl={https://openreview.net/forum?id=rJEyrjRqYX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BJlDc3nZeE", "original": null, "number": 1, "cdate": 1544830094721, "ddate": null, "tcdate": 1544830094721, "tmdate": 1545354530760, "tddate": null, "forum": "rJEyrjRqYX", "replyto": "rJEyrjRqYX", "invitation": "ICLR.cc/2019/Conference/-/Paper53/Meta_Review", "content": {"metareview": "The submission suggests reducing the parameters in a conv-lSTM by replacing the 3 gates in the standard LSTM with one gate. The idea is to get a more efficient convolutional LSTM and use it for video prediction. Two of the reviewers found the manuscript and description of the work difficult to follow and the justification for the proposed method lacking. Additionally, the contribution of this submission feels rather thin, and the experimental results are not very convincing: the absolute training time is too coarse of a measurement (and convergence may depend on many factors), and the improvements over PredNet seem somewhat marginal.\n\nFinally, I agree with the reviewer that mentioned that a proper comparison with baselines should be done in such a way that the number of parameters is comparable (if #params is a main claim of the paper!). It is entirely plausible that if you reduce the number of parameters in PredNet by 40% (in some other way), its performance would also benefit.\n\nWith all this in mind, I do not recommend this paper be accepted at this time.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "metareview"}, "signatures": ["ICLR.cc/2019/Conference/Paper53/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper53/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reduced-Gate Convolutional LSTM Design Using Predictive Coding for Next-Frame Video Prediction", "abstract": "Spatiotemporal sequence prediction is an important problem in deep learning. We\nstudy next-frame video prediction using a deep-learning-based predictive coding\nframework that uses convolutional, long short-term memory (convLSTM) modules.\nWe introduce a novel reduced-gate convolutional LSTM architecture. Our\nreduced-gate model achieves better next-frame prediction accuracy than the original\nconvolutional LSTM while using a smaller parameter budget, thereby reducing\ntraining time. We tested our reduced gate modules within a predictive coding architecture\non the moving MNIST and KITTI datasets. We found that our reduced-gate\nmodel has a significant reduction of approximately 40 percent of the total\nnumber of training parameters and training time in comparison with the standard\nLSTM model which makes it attractive for hardware implementation especially\non small devices.", "paperhash": "elsayed|reducedgate_convolutional_lstm_design_using_predictive_coding_for_nextframe_video_prediction", "keywords": ["rgcLSTM", "convolutional LSTM", "unsupervised learning", "predictive coding", "video prediction", "moving MNIST", "KITTI datasets", "deep learning"], "authorids": ["nelly.elsayed5@gmail.com", "maida@louisiana.edu", "mab0778@louisiana.edu"], "authors": ["Nelly Elsayed", "Anthony S. Maida", "Magdy Bayoumi"], "TL;DR": "A novel reduced-gate convolutional LSTM design using predictive coding for next-frame video prediction", "pdf": "/pdf/b5c5837202d2f45bc52d02ef302a9dbf4ac5f77c.pdf", "_bibtex": "@misc{\nelsayed2019reducedgate,\ntitle={Reduced-Gate Convolutional {LSTM} Design Using Predictive Coding for Next-Frame Video Prediction},\nauthor={Nelly Elsayed and Anthony S. Maida and Magdy Bayoumi},\nyear={2019},\nurl={https://openreview.net/forum?id=rJEyrjRqYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper53/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353354103, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJEyrjRqYX", "replyto": "rJEyrjRqYX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper53/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper53/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper53/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353354103}}}, {"id": "SkxhhqFDJ4", "original": null, "number": 10, "cdate": 1544161972155, "ddate": null, "tcdate": 1544161972155, "tmdate": 1544161972155, "tddate": null, "forum": "rJEyrjRqYX", "replyto": "rygGifMvJ4", "invitation": "ICLR.cc/2019/Conference/-/Paper53/Official_Comment", "content": {"title": "Incremental contribution, needs more experimental validation", "comment": "Thanks for pointing out the link to the revision. \n\nAfter rereading the paper, I'm still not entirely convinced by the proposed model, neither by the intuition or by the experiments. Since this paper conducts experimental research, it may make sense to compare the standard LSTM and the proposed rgcLSTM using a simpler task before moving on to the complex model, predNet. A simple case can more straightforwardly show what we can gain and will lose by using rgcLSTM. As shown in the experiments, both quantitative and qualitative results are not significantly better than the original predNet. It is reasonable to question that the rgcLSTM might lose some capacity, e.g., memorizing a long sequence, but this is not noticeable maybe because it reduces the model complexity of the predNet, which simplifies the optimization and reaches to a comparable local minimum. We could use rgcLSTM in predNet to gain some efficiency, but how about other models built on convLSTM? If the paper only focuses on this particular application, the contribution is incremental. "}, "signatures": ["ICLR.cc/2019/Conference/Paper53/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper53/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper53/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reduced-Gate Convolutional LSTM Design Using Predictive Coding for Next-Frame Video Prediction", "abstract": "Spatiotemporal sequence prediction is an important problem in deep learning. We\nstudy next-frame video prediction using a deep-learning-based predictive coding\nframework that uses convolutional, long short-term memory (convLSTM) modules.\nWe introduce a novel reduced-gate convolutional LSTM architecture. Our\nreduced-gate model achieves better next-frame prediction accuracy than the original\nconvolutional LSTM while using a smaller parameter budget, thereby reducing\ntraining time. We tested our reduced gate modules within a predictive coding architecture\non the moving MNIST and KITTI datasets. We found that our reduced-gate\nmodel has a significant reduction of approximately 40 percent of the total\nnumber of training parameters and training time in comparison with the standard\nLSTM model which makes it attractive for hardware implementation especially\non small devices.", "paperhash": "elsayed|reducedgate_convolutional_lstm_design_using_predictive_coding_for_nextframe_video_prediction", "keywords": ["rgcLSTM", "convolutional LSTM", "unsupervised learning", "predictive coding", "video prediction", "moving MNIST", "KITTI datasets", "deep learning"], "authorids": ["nelly.elsayed5@gmail.com", "maida@louisiana.edu", "mab0778@louisiana.edu"], "authors": ["Nelly Elsayed", "Anthony S. Maida", "Magdy Bayoumi"], "TL;DR": "A novel reduced-gate convolutional LSTM design using predictive coding for next-frame video prediction", "pdf": "/pdf/b5c5837202d2f45bc52d02ef302a9dbf4ac5f77c.pdf", "_bibtex": "@misc{\nelsayed2019reducedgate,\ntitle={Reduced-Gate Convolutional {LSTM} Design Using Predictive Coding for Next-Frame Video Prediction},\nauthor={Nelly Elsayed and Anthony S. Maida and Magdy Bayoumi},\nyear={2019},\nurl={https://openreview.net/forum?id=rJEyrjRqYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper53/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615519, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJEyrjRqYX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper53/Authors", "ICLR.cc/2019/Conference/Paper53/Reviewers", "ICLR.cc/2019/Conference/Paper53/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper53/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper53/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper53/Authors|ICLR.cc/2019/Conference/Paper53/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper53/Reviewers", "ICLR.cc/2019/Conference/Paper53/Authors", "ICLR.cc/2019/Conference/Paper53/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615519}}}, {"id": "rygGifMvJ4", "original": null, "number": 9, "cdate": 1544131226010, "ddate": null, "tcdate": 1544131226010, "tmdate": 1544131271245, "tddate": null, "forum": "rJEyrjRqYX", "replyto": "rygDNTOHkN", "invitation": "ICLR.cc/2019/Conference/-/Paper53/Official_Comment", "content": {"title": "Apologize for this inconvenience ", "comment": "Dear AnonReviewer2,\nWe apologize for this mistake during uploading the last version of our paper. We contacted to the ICLR 2019 Program Chairs about this issue and they said to us that we can mention to you the previous version as we can not upload the new one. \nWe have the same version that was uploaded correctly on (17th November )\nin the Revision History link:  https://openreview.net/revisions?id=rJEyrjRqYX\nOr its direct link: \nhttps://openreview.net/references/pdf?id=SJCkcu0pm\n\nThe version that we intended to upload but we missed the correct file uploading is the same as the draft version that we mentioned above except that we had corrected Figure 1 as last tanh on the right hand side is not on the C^t arrow.  It was a mistake during a trial to reduce the figure design size which caused the mis-matching between the equations and the figure. \nWe appreciate your time and considerations. We would be pleased if you review the version that mentioned above which contains all the corrections and recommendation you and the other reviewers mentioned. And please consider the correction about  Figure 1 what we did but missed to upload correctly.\nWe apologize for this inconvenience due to our mistake that happened due our fear of missing the deadlines. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper53/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper53/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper53/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reduced-Gate Convolutional LSTM Design Using Predictive Coding for Next-Frame Video Prediction", "abstract": "Spatiotemporal sequence prediction is an important problem in deep learning. We\nstudy next-frame video prediction using a deep-learning-based predictive coding\nframework that uses convolutional, long short-term memory (convLSTM) modules.\nWe introduce a novel reduced-gate convolutional LSTM architecture. Our\nreduced-gate model achieves better next-frame prediction accuracy than the original\nconvolutional LSTM while using a smaller parameter budget, thereby reducing\ntraining time. We tested our reduced gate modules within a predictive coding architecture\non the moving MNIST and KITTI datasets. We found that our reduced-gate\nmodel has a significant reduction of approximately 40 percent of the total\nnumber of training parameters and training time in comparison with the standard\nLSTM model which makes it attractive for hardware implementation especially\non small devices.", "paperhash": "elsayed|reducedgate_convolutional_lstm_design_using_predictive_coding_for_nextframe_video_prediction", "keywords": ["rgcLSTM", "convolutional LSTM", "unsupervised learning", "predictive coding", "video prediction", "moving MNIST", "KITTI datasets", "deep learning"], "authorids": ["nelly.elsayed5@gmail.com", "maida@louisiana.edu", "mab0778@louisiana.edu"], "authors": ["Nelly Elsayed", "Anthony S. Maida", "Magdy Bayoumi"], "TL;DR": "A novel reduced-gate convolutional LSTM design using predictive coding for next-frame video prediction", "pdf": "/pdf/b5c5837202d2f45bc52d02ef302a9dbf4ac5f77c.pdf", "_bibtex": "@misc{\nelsayed2019reducedgate,\ntitle={Reduced-Gate Convolutional {LSTM} Design Using Predictive Coding for Next-Frame Video Prediction},\nauthor={Nelly Elsayed and Anthony S. Maida and Magdy Bayoumi},\nyear={2019},\nurl={https://openreview.net/forum?id=rJEyrjRqYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper53/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615519, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJEyrjRqYX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper53/Authors", "ICLR.cc/2019/Conference/Paper53/Reviewers", "ICLR.cc/2019/Conference/Paper53/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper53/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper53/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper53/Authors|ICLR.cc/2019/Conference/Paper53/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper53/Reviewers", "ICLR.cc/2019/Conference/Paper53/Authors", "ICLR.cc/2019/Conference/Paper53/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615519}}}, {"id": "rygDNTOHkN", "original": null, "number": 7, "cdate": 1544027438939, "ddate": null, "tcdate": 1544027438939, "tmdate": 1544028369511, "tddate": null, "forum": "rJEyrjRqYX", "replyto": "BJlt-pOAp7", "invitation": "ICLR.cc/2019/Conference/-/Paper53/Official_Comment", "content": {"title": "Missing manuscript", "comment": "After reading the authors' response, I'd like to check the updated manuscript. However, the attached pdf only includes the formatting instructions. Could you fix this first?\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper53/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper53/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper53/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reduced-Gate Convolutional LSTM Design Using Predictive Coding for Next-Frame Video Prediction", "abstract": "Spatiotemporal sequence prediction is an important problem in deep learning. We\nstudy next-frame video prediction using a deep-learning-based predictive coding\nframework that uses convolutional, long short-term memory (convLSTM) modules.\nWe introduce a novel reduced-gate convolutional LSTM architecture. Our\nreduced-gate model achieves better next-frame prediction accuracy than the original\nconvolutional LSTM while using a smaller parameter budget, thereby reducing\ntraining time. We tested our reduced gate modules within a predictive coding architecture\non the moving MNIST and KITTI datasets. We found that our reduced-gate\nmodel has a significant reduction of approximately 40 percent of the total\nnumber of training parameters and training time in comparison with the standard\nLSTM model which makes it attractive for hardware implementation especially\non small devices.", "paperhash": "elsayed|reducedgate_convolutional_lstm_design_using_predictive_coding_for_nextframe_video_prediction", "keywords": ["rgcLSTM", "convolutional LSTM", "unsupervised learning", "predictive coding", "video prediction", "moving MNIST", "KITTI datasets", "deep learning"], "authorids": ["nelly.elsayed5@gmail.com", "maida@louisiana.edu", "mab0778@louisiana.edu"], "authors": ["Nelly Elsayed", "Anthony S. Maida", "Magdy Bayoumi"], "TL;DR": "A novel reduced-gate convolutional LSTM design using predictive coding for next-frame video prediction", "pdf": "/pdf/b5c5837202d2f45bc52d02ef302a9dbf4ac5f77c.pdf", "_bibtex": "@misc{\nelsayed2019reducedgate,\ntitle={Reduced-Gate Convolutional {LSTM} Design Using Predictive Coding for Next-Frame Video Prediction},\nauthor={Nelly Elsayed and Anthony S. Maida and Magdy Bayoumi},\nyear={2019},\nurl={https://openreview.net/forum?id=rJEyrjRqYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper53/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615519, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJEyrjRqYX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper53/Authors", "ICLR.cc/2019/Conference/Paper53/Reviewers", "ICLR.cc/2019/Conference/Paper53/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper53/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper53/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper53/Authors|ICLR.cc/2019/Conference/Paper53/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper53/Reviewers", "ICLR.cc/2019/Conference/Paper53/Authors", "ICLR.cc/2019/Conference/Paper53/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615519}}}, {"id": "rJeVYa8NCX", "original": null, "number": 5, "cdate": 1542905211741, "ddate": null, "tcdate": 1542905211741, "tmdate": 1543938141071, "tddate": null, "forum": "rJEyrjRqYX", "replyto": "r1eJYyFC6Q", "invitation": "ICLR.cc/2019/Conference/-/Paper53/Official_Comment", "content": {"title": "Justification for tied LSTM gates and chained non-linearities still unsatisfactory, furthermore contribution seems marginal.", "comment": "1. Apologies, I misread your sentence in the paper. I will amend my review accordingly.\n\n2. The term \"vanilla\" means indeed \"plain\" or \"basic\", which is why would be better referred to the 1997 implementation rather than the more advanced one with the forget gates. Nonetheless, being a matter of semantics I don't want to further object to your choice of referring to the 2000 implementation as vanilla. What is more important is that in the manuscript you seem to imply that both the implementation by Gers et al (2000) and that of Lotter et al. (2017) shall be referred to vanilla LSTMs: \"[referring to Lotter et al (2017)] Greff et al (2017) has termed this design the vanilla LSTM\". This is confusing.\n\n3. Apologies for not being more clear. I referred to this sentence: \"There were other attempts to design smaller recurrent gated models [...] based on either removing one of the gates or the activation functions [...] these models had no significant reduction in trainable parameters\".\n\n4. Thank you for adding subsection 4.1. I believe this subsection would be more clear if it was part of the model description in section 3 and referred to the formulas in that section. \nMost importantly, while I understand the practical motivations behind this modelling choice, stacking a tanh and a sigmoid nonlinearities in a row will make the optimization problem harder, constrain the network and limit its manoeuvre space. In particular, the sigmoid activation of the gate will likely be in the [0.3, 0.7] range when given an input in [-1, 1] if I am not mistaken. This halves the range of values one would expect the gate to be able to take. \nI believe this is the main problem of the suggested approach and that a much more convincing justification than the findings about the LSTMs gates reported by Greff et Al is needed.\n\nIt is also unclear to me why it would be sensible to tie the values of the input, forget and output gates, given that they play a very different and in some cases complementary role. Again, this approach should be fully justified with an intuition as to why this should be a sensible idea.\n\nFurthermore, I still find the description of the network very difficult to follow. I suggest to move the discussion on the number of parameters and the description of the dimensionality of the layers to the end of the paragraphs/section, in the interest of readability. I also recommend to mimic the notation used in the original convLSTM paper that the authors compare against (I refer to eq 3 of the original paper), to make it easier for the reader to spot differences and similarities.\n\nMore to this, it is unclear to me the difference between the \"network gate value\" f_gate and the \"gate value\" f. In particular the former doesn't seem to be used in any of the following equations, and neither of them is depicted in the figures.\n\nTo summarise, I believe that the manuscript still fails to provide a convincing justification for the proposed approach of tying the LSTM gates together and the clarity of the description of the model is still unsatisfactory. I am also very sceptical about the modelling choice of stacking two nonlinearities in a row to compute the activations of the gate. Finally, I agree with Reviewer 2 that the contribution of tying the weights together and replacing Hadamard product with convolution seems to be marginal.\n\n* there is a typo in the updated version of the manuscript, just after eq 1: hight --> height "}, "signatures": ["ICLR.cc/2019/Conference/Paper53/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper53/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper53/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reduced-Gate Convolutional LSTM Design Using Predictive Coding for Next-Frame Video Prediction", "abstract": "Spatiotemporal sequence prediction is an important problem in deep learning. We\nstudy next-frame video prediction using a deep-learning-based predictive coding\nframework that uses convolutional, long short-term memory (convLSTM) modules.\nWe introduce a novel reduced-gate convolutional LSTM architecture. Our\nreduced-gate model achieves better next-frame prediction accuracy than the original\nconvolutional LSTM while using a smaller parameter budget, thereby reducing\ntraining time. We tested our reduced gate modules within a predictive coding architecture\non the moving MNIST and KITTI datasets. We found that our reduced-gate\nmodel has a significant reduction of approximately 40 percent of the total\nnumber of training parameters and training time in comparison with the standard\nLSTM model which makes it attractive for hardware implementation especially\non small devices.", "paperhash": "elsayed|reducedgate_convolutional_lstm_design_using_predictive_coding_for_nextframe_video_prediction", "keywords": ["rgcLSTM", "convolutional LSTM", "unsupervised learning", "predictive coding", "video prediction", "moving MNIST", "KITTI datasets", "deep learning"], "authorids": ["nelly.elsayed5@gmail.com", "maida@louisiana.edu", "mab0778@louisiana.edu"], "authors": ["Nelly Elsayed", "Anthony S. Maida", "Magdy Bayoumi"], "TL;DR": "A novel reduced-gate convolutional LSTM design using predictive coding for next-frame video prediction", "pdf": "/pdf/b5c5837202d2f45bc52d02ef302a9dbf4ac5f77c.pdf", "_bibtex": "@misc{\nelsayed2019reducedgate,\ntitle={Reduced-Gate Convolutional {LSTM} Design Using Predictive Coding for Next-Frame Video Prediction},\nauthor={Nelly Elsayed and Anthony S. Maida and Magdy Bayoumi},\nyear={2019},\nurl={https://openreview.net/forum?id=rJEyrjRqYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper53/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615519, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJEyrjRqYX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper53/Authors", "ICLR.cc/2019/Conference/Paper53/Reviewers", "ICLR.cc/2019/Conference/Paper53/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper53/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper53/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper53/Authors|ICLR.cc/2019/Conference/Paper53/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper53/Reviewers", "ICLR.cc/2019/Conference/Paper53/Authors", "ICLR.cc/2019/Conference/Paper53/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615519}}}, {"id": "S1gT4QZc3m", "original": null, "number": 2, "cdate": 1541178164931, "ddate": null, "tcdate": 1541178164931, "tmdate": 1542905708501, "tddate": null, "forum": "rJEyrjRqYX", "replyto": "rJEyrjRqYX", "invitation": "ICLR.cc/2019/Conference/-/Paper53/Official_Review", "content": {"title": "Unconvincing ConvLSTM variant, unsatisfactory justification of the approach, unclear model description. Contribution seems marginal.", "review": "The authors propose a variant to convLSTMs with convolutional peephole units (as opposed to the original Hadamard product) and tied gates. The description of the model is confusing, the authors don't offer a strong justification for the proposed approach, some of the technical choices seem flawed.\n\nIt is also obviously false that removing an LSTM gate does not incur in a reduction of trainable parameters: \"There were other attempts to design smaller recurrent gated models [...] based on either removing one of the gates or the activation functions [...] these models had no significant reduction in trainable parameters\".\n\nThe model description is difficult to follow, but from what I can gather the model depiction is flawed. In particular, the proposed model bases on the idea to use tied gates to reduce the number of parameters. f^{(t)}, i.e., the value that governs the input, forger and output gates, is derived via a sigmoid. When the sigmoid is zero, the network will hence set all the gates to zero, ignoring the input, forgetting the memory and suppressing the output, which is hardly a behaviour one could ever want. Unfortunately, almost half of the sigmoid spectrum will lead to such behavior. This seems to be a poor modeling decision.\n\nThe nodes in Figure 1 seem to suggest that two consecutive non-linearities are applied in a row, if W_fc is clamped to zero, as reported by the authors, there is no reason to specify it, the corresponding part of the equation can be removed and the narrative simplified. In sec3, the meaning of \u201cnet input image\u201d and \u201cnetwork gate image value\u201d is unclear. Also the description of the equation is hard to follow, e.g., the square bracket notation is eventually explained only after 8 lines of text.\n\nAt the beginning of Sec4 err_1 and err_2 are defined as the difference between the predicted frame and the target frame, and vice versa. This error is then fed to the \u201crgcLSTM input arranger unit and to the next higher layer\u201d. By my understanding the error of one layer is fed in the next as an input. I wonder if such error is provided also at inference time, giving a clear guidance to the network to produce the correct output exploiting privileged information. This could also explain why the training process was completed in only one epoch.\n\n- Typos:\n*Intro: More important \u2192 more importantly\n* page5: ReL -> ReLu", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper53/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Reduced-Gate Convolutional LSTM Design Using Predictive Coding for Next-Frame Video Prediction", "abstract": "Spatiotemporal sequence prediction is an important problem in deep learning. We\nstudy next-frame video prediction using a deep-learning-based predictive coding\nframework that uses convolutional, long short-term memory (convLSTM) modules.\nWe introduce a novel reduced-gate convolutional LSTM architecture. Our\nreduced-gate model achieves better next-frame prediction accuracy than the original\nconvolutional LSTM while using a smaller parameter budget, thereby reducing\ntraining time. We tested our reduced gate modules within a predictive coding architecture\non the moving MNIST and KITTI datasets. We found that our reduced-gate\nmodel has a significant reduction of approximately 40 percent of the total\nnumber of training parameters and training time in comparison with the standard\nLSTM model which makes it attractive for hardware implementation especially\non small devices.", "paperhash": "elsayed|reducedgate_convolutional_lstm_design_using_predictive_coding_for_nextframe_video_prediction", "keywords": ["rgcLSTM", "convolutional LSTM", "unsupervised learning", "predictive coding", "video prediction", "moving MNIST", "KITTI datasets", "deep learning"], "authorids": ["nelly.elsayed5@gmail.com", "maida@louisiana.edu", "mab0778@louisiana.edu"], "authors": ["Nelly Elsayed", "Anthony S. Maida", "Magdy Bayoumi"], "TL;DR": "A novel reduced-gate convolutional LSTM design using predictive coding for next-frame video prediction", "pdf": "/pdf/b5c5837202d2f45bc52d02ef302a9dbf4ac5f77c.pdf", "_bibtex": "@misc{\nelsayed2019reducedgate,\ntitle={Reduced-Gate Convolutional {LSTM} Design Using Predictive Coding for Next-Frame Video Prediction},\nauthor={Nelly Elsayed and Anthony S. Maida and Magdy Bayoumi},\nyear={2019},\nurl={https://openreview.net/forum?id=rJEyrjRqYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper53/Official_Review", "cdate": 1542234548553, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJEyrjRqYX", "replyto": "rJEyrjRqYX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper53/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335638543, "tmdate": 1552335638543, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper53/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1eLNxt0Tm", "original": null, "number": 3, "cdate": 1542520877634, "ddate": null, "tcdate": 1542520877634, "tmdate": 1542520877634, "tddate": null, "forum": "rJEyrjRqYX", "replyto": "B1gC5rSPnX", "invitation": "ICLR.cc/2019/Conference/-/Paper53/Official_Comment", "content": {"title": "response to AnonReviewer1 comments and suggestions", "comment": "1. More datasets could be used to demonstrate the enhancement in performance.\nResponse: This is a helpful suggestion which, because of practical limitations, we will reserve for future work. We have added this point to the conclusions."}, "signatures": ["ICLR.cc/2019/Conference/Paper53/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper53/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper53/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reduced-Gate Convolutional LSTM Design Using Predictive Coding for Next-Frame Video Prediction", "abstract": "Spatiotemporal sequence prediction is an important problem in deep learning. We\nstudy next-frame video prediction using a deep-learning-based predictive coding\nframework that uses convolutional, long short-term memory (convLSTM) modules.\nWe introduce a novel reduced-gate convolutional LSTM architecture. Our\nreduced-gate model achieves better next-frame prediction accuracy than the original\nconvolutional LSTM while using a smaller parameter budget, thereby reducing\ntraining time. We tested our reduced gate modules within a predictive coding architecture\non the moving MNIST and KITTI datasets. We found that our reduced-gate\nmodel has a significant reduction of approximately 40 percent of the total\nnumber of training parameters and training time in comparison with the standard\nLSTM model which makes it attractive for hardware implementation especially\non small devices.", "paperhash": "elsayed|reducedgate_convolutional_lstm_design_using_predictive_coding_for_nextframe_video_prediction", "keywords": ["rgcLSTM", "convolutional LSTM", "unsupervised learning", "predictive coding", "video prediction", "moving MNIST", "KITTI datasets", "deep learning"], "authorids": ["nelly.elsayed5@gmail.com", "maida@louisiana.edu", "mab0778@louisiana.edu"], "authors": ["Nelly Elsayed", "Anthony S. Maida", "Magdy Bayoumi"], "TL;DR": "A novel reduced-gate convolutional LSTM design using predictive coding for next-frame video prediction", "pdf": "/pdf/b5c5837202d2f45bc52d02ef302a9dbf4ac5f77c.pdf", "_bibtex": "@misc{\nelsayed2019reducedgate,\ntitle={Reduced-Gate Convolutional {LSTM} Design Using Predictive Coding for Next-Frame Video Prediction},\nauthor={Nelly Elsayed and Anthony S. Maida and Magdy Bayoumi},\nyear={2019},\nurl={https://openreview.net/forum?id=rJEyrjRqYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper53/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615519, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJEyrjRqYX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper53/Authors", "ICLR.cc/2019/Conference/Paper53/Reviewers", "ICLR.cc/2019/Conference/Paper53/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper53/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper53/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper53/Authors|ICLR.cc/2019/Conference/Paper53/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper53/Reviewers", "ICLR.cc/2019/Conference/Paper53/Authors", "ICLR.cc/2019/Conference/Paper53/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615519}}}, {"id": "r1eJYyFC6Q", "original": null, "number": 2, "cdate": 1542520694585, "ddate": null, "tcdate": 1542520694585, "tmdate": 1542520825776, "tddate": null, "forum": "rJEyrjRqYX", "replyto": "S1gT4QZc3m", "invitation": "ICLR.cc/2019/Conference/-/Paper53/Official_Comment", "content": {"title": "response to AnonReviewer3 comments and suggestions", "comment": "1. Criticism: Update gate in GRU does not combine input gate, forget gate, and memory gate and memory unit of LSTMs.\nResponse: We quote below from the following paper as our source, which we cited in our paper, and of which Schmidhuber is a co-author.\n\n. . . Gated Recurrent Unit (GRU). They used neither peephole connections\nnor output activation functions, and coupled the input and the forget gate\ninto an update gate. Finally, their output gate (called reset gate) . . .\n\nThe above is from the last paragraph in Section III of the paper by Greff, Srivastava, Koutnik, Steunebrink, and Schmidhuber \\LSTM: A Search Space Odyssey,\" Transactions on Neural Networks and Learning Systems, 2017, axXiv:1503:04069v2.\nPlease reread the last paragraph of page 2 of our paper and then look at the above quotation.\n\n\n2. The term vanilla is \\normally\" used as a synonym to \\original.\" Vanilla refers to the\noriginal 1997 implementation not to Greff et al.'s work . . .\nResponse: We stand by our use of the term \\vanilla.\" This term comes from a Cambridge ice-cream shop which was frequented in the 1970's by the developers of the \ffirst Lisp-based object-oriented system known as Flavors. It referred to the base class in an object-oriented system. To see if the meaning changed or evolved over the\ndecades, I searched for a definition on the web. It still means \"plain\" or \"basic,\" not \"original.\"\n\nMore to the point, the original (1997) LSTM of Hochreiter & Schmidhuber does not have a forget gate and all modern \"basic\" LSTMs do have forget gates. This gate wasn't added until the (2000) publication that we cite in our submitted paper. All modern LSTMs use a forget gate and it has since become known that the forget gate is the most important gate as stated in Greff et al (2017). Schmidhuber, an inventor of the LSTM, is the senior author on that (2017) paper cited above in point 1 which designated that the LSTM with all three gates and no peephole connections is the\n\"vanilla\" LSTM. Section II of the Greff et al paper is titled Vanilla LSTM. We followed their lead.\n\n3. \"It is obviously false that removing an LSTM gate does not incur a reduction in parameters.\"\nResponse: Either the above is a misstatment, or we agree with the referee and never stated otherwise.\nEach gate has a set of associated incoming weights (parameters). Removing a gate removes the associated weights. We've added an appendix showing how the parameters were counted to remove any doubt about the parameter reduction.\n\n4. When the sigmoid is zero, the network will hence set all the gates to zero, ignoring the input, forgetting the memory and suppressing the output, which is hardly the behavior once could ever want.\nResponse: Yes, the gate value must stay in a functional range. We have added a subsection 4.1 titled \"Keeping the gate output within a functional range\" to explain how this is addressed in our application.\nThe referee asked why there where two consecutive non-linearities (tanh) in a row. The positioning of the second tanh forces both c and h stay within the range (0; 1), and since they are both input to the network gate, this improves the likelihood of the network gate value staying in a functional range.\n\n5. In sec3, the meaning of \"net input image\" and \"network gate image value\" is unclear.\nResponse: The confusion seems to come from use of the word \"image.\" We've added a sentence at the beginning of Sec3 after the introduction of Figure 1 which states that \"Since this is a convolutional LSTM, the information on each wire is a multi-channel image.\" We have also removed the word \"image\" from the above mentioned phrases.\n\n6. The square bracket notation is eventually explained only after 8 lines of text.\nResponse: We moved the square bracket sentence forward to just after the equation where the notation is \ffirst used and revised the sentence to be more explicit: \"The square brackets indicate that multi-channel images or \ffilters with compatible dimensions are stacked on top of each other.\"\n\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper53/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper53/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper53/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reduced-Gate Convolutional LSTM Design Using Predictive Coding for Next-Frame Video Prediction", "abstract": "Spatiotemporal sequence prediction is an important problem in deep learning. We\nstudy next-frame video prediction using a deep-learning-based predictive coding\nframework that uses convolutional, long short-term memory (convLSTM) modules.\nWe introduce a novel reduced-gate convolutional LSTM architecture. Our\nreduced-gate model achieves better next-frame prediction accuracy than the original\nconvolutional LSTM while using a smaller parameter budget, thereby reducing\ntraining time. We tested our reduced gate modules within a predictive coding architecture\non the moving MNIST and KITTI datasets. We found that our reduced-gate\nmodel has a significant reduction of approximately 40 percent of the total\nnumber of training parameters and training time in comparison with the standard\nLSTM model which makes it attractive for hardware implementation especially\non small devices.", "paperhash": "elsayed|reducedgate_convolutional_lstm_design_using_predictive_coding_for_nextframe_video_prediction", "keywords": ["rgcLSTM", "convolutional LSTM", "unsupervised learning", "predictive coding", "video prediction", "moving MNIST", "KITTI datasets", "deep learning"], "authorids": ["nelly.elsayed5@gmail.com", "maida@louisiana.edu", "mab0778@louisiana.edu"], "authors": ["Nelly Elsayed", "Anthony S. Maida", "Magdy Bayoumi"], "TL;DR": "A novel reduced-gate convolutional LSTM design using predictive coding for next-frame video prediction", "pdf": "/pdf/b5c5837202d2f45bc52d02ef302a9dbf4ac5f77c.pdf", "_bibtex": "@misc{\nelsayed2019reducedgate,\ntitle={Reduced-Gate Convolutional {LSTM} Design Using Predictive Coding for Next-Frame Video Prediction},\nauthor={Nelly Elsayed and Anthony S. Maida and Magdy Bayoumi},\nyear={2019},\nurl={https://openreview.net/forum?id=rJEyrjRqYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper53/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615519, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJEyrjRqYX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper53/Authors", "ICLR.cc/2019/Conference/Paper53/Reviewers", "ICLR.cc/2019/Conference/Paper53/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper53/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper53/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper53/Authors|ICLR.cc/2019/Conference/Paper53/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper53/Reviewers", "ICLR.cc/2019/Conference/Paper53/Authors", "ICLR.cc/2019/Conference/Paper53/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615519}}}, {"id": "BJlt-pOAp7", "original": null, "number": 1, "cdate": 1542520065059, "ddate": null, "tcdate": 1542520065059, "tmdate": 1542520815378, "tddate": null, "forum": "rJEyrjRqYX", "replyto": "ryx7mFSq3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper53/Official_Comment", "content": {"title": "response to AnonReviewer2 comments and suggestions", "comment": "1. The description in the paper is very confusing.\nResponse: We have improved the clarity of \ffigure 2. We have also added an appendix giving many more details of the model architecture. The main purpose of the appendix is to spell out how the parameter counts were performed but a side-effect is to offer a quite detailed specifi\fcation of the architecture. We have also put the source code on github and listed the link in Section 4 of the paper.\n\n2. Something is wrong with Eqns. 14 and 15.\nResponse: We have been continuing work on the project since our initial submission to ICLR and have found these errors. We have corrected these parameter count equations in the revision.\n\n3. Eqns 5 and 6 are not consistent.\nResponse: The inconsistency was due to an error in Eqn 6. We have corrected this.\n\n4. The intuition of having one gate instead of three is not clear. Mixing all the functions, i.e., input, forget, and output \ffilters, does provide freedom for learning but also introduces learning complexity. Decoupling a complex function into a couple of simpler functions is a common way to narrow down the search space. This is what the standard LSTM does. The authors should provide reasonable arguments to explain intuitively why using one gate is better.\nResponse: The decoupling advice of the referee is very persuasive. We weren't fully cognizant of this at the start of the project. Our main inspiration comes from the paper of Greff et al (2017) which concludes on the basis of a vast number of empirical studies that the LSTM variant with three gates and no peephole connections gives the best overall performance. They also note that the forget gate is the most important gate in the LSTM. Inspired by the GRU and the MGU, we just tried to see what would happen if we used one gate for all three functions. Since this was a convolutional LSTM, we also decided to replace the original peephole connections that were implemented by an elementwise multiply in the Shi et al model with a convolutional operation. This seemed far more appropriate for image processing. We tried various alternatives and had the most success with the model proposed in our paper.\n\n5. The experimental performance comparisons would be fairer if we let the models converge.\nResponse: For the KITTI dataset, we used the initialization and number of epochs (150) used by the original PredNet authors. In this sense, we were fair to the model. For the moving MNIST dataset, the original authors didn't do an experiment using it. However, the models converged in one epoch. This can be seen in the MSE and SSIM values. When using three epochs, these values were approximately the same.\n\n6. The rgcLSTM may be performing better, despite the use of fewer parameters, because the LSTM may be suffering from the vanishing gradient problem but this doesn't indicate that the rgcLSTM is better than the standard LSTM.\nResponse: This point is well taken and we don't have an answer at the present time. We've amended the conclusions to admit this possibility. We definitely have to examine this possibility in future work, include the ResNet idea suggested by the referee.\n\n7. Summary: experimental results are good to verify the goodness of a model, but the theoretical support or intuitions are more important to understand the benefi\fts that we can gain by making this change to the standard LSTM. This part is not clear from the paper.\nResponse: We've added subsection 4.1 to the paper which enhances the theory\nand provides intuitions. Unfortunately, we acknowledge that we probably have not\ncompletely address this concern."}, "signatures": ["ICLR.cc/2019/Conference/Paper53/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper53/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper53/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reduced-Gate Convolutional LSTM Design Using Predictive Coding for Next-Frame Video Prediction", "abstract": "Spatiotemporal sequence prediction is an important problem in deep learning. We\nstudy next-frame video prediction using a deep-learning-based predictive coding\nframework that uses convolutional, long short-term memory (convLSTM) modules.\nWe introduce a novel reduced-gate convolutional LSTM architecture. Our\nreduced-gate model achieves better next-frame prediction accuracy than the original\nconvolutional LSTM while using a smaller parameter budget, thereby reducing\ntraining time. We tested our reduced gate modules within a predictive coding architecture\non the moving MNIST and KITTI datasets. We found that our reduced-gate\nmodel has a significant reduction of approximately 40 percent of the total\nnumber of training parameters and training time in comparison with the standard\nLSTM model which makes it attractive for hardware implementation especially\non small devices.", "paperhash": "elsayed|reducedgate_convolutional_lstm_design_using_predictive_coding_for_nextframe_video_prediction", "keywords": ["rgcLSTM", "convolutional LSTM", "unsupervised learning", "predictive coding", "video prediction", "moving MNIST", "KITTI datasets", "deep learning"], "authorids": ["nelly.elsayed5@gmail.com", "maida@louisiana.edu", "mab0778@louisiana.edu"], "authors": ["Nelly Elsayed", "Anthony S. Maida", "Magdy Bayoumi"], "TL;DR": "A novel reduced-gate convolutional LSTM design using predictive coding for next-frame video prediction", "pdf": "/pdf/b5c5837202d2f45bc52d02ef302a9dbf4ac5f77c.pdf", "_bibtex": "@misc{\nelsayed2019reducedgate,\ntitle={Reduced-Gate Convolutional {LSTM} Design Using Predictive Coding for Next-Frame Video Prediction},\nauthor={Nelly Elsayed and Anthony S. Maida and Magdy Bayoumi},\nyear={2019},\nurl={https://openreview.net/forum?id=rJEyrjRqYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper53/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615519, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJEyrjRqYX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper53/Authors", "ICLR.cc/2019/Conference/Paper53/Reviewers", "ICLR.cc/2019/Conference/Paper53/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper53/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper53/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper53/Authors|ICLR.cc/2019/Conference/Paper53/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper53/Reviewers", "ICLR.cc/2019/Conference/Paper53/Authors", "ICLR.cc/2019/Conference/Paper53/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615519}}}, {"id": "ryx7mFSq3Q", "original": null, "number": 3, "cdate": 1541196058594, "ddate": null, "tcdate": 1541196058594, "tmdate": 1541534326825, "tddate": null, "forum": "rJEyrjRqYX", "replyto": "rJEyrjRqYX", "invitation": "ICLR.cc/2019/Conference/-/Paper53/Official_Review", "content": {"title": "Interesting paper but with incremental contributions", "review": "This manuscript proposes replacing the three gates in the standard LSTM with one gate to reduce the number of parameters and the computation time. The proposed reduce-gate convolutional LSTM is applied in PredNet to predict next frames of a video. \n\nThe main contribution of this paper is proposing an efficient convolutional LSTM. Although the number of the parameters and the training time in the experiments support this statement, the description in the paper is very confusing. \n\n1) In the standard LSTM, the cell state c^{t-1} is not an input for the computation of the three gates and the cell state's candidate. That is, in Eq(15) 2\\kappa and 2n should be \\kappa and n. Compared to Eq(14), it may not show that the standard LSTM has more parameters than rgcLSTM.\n\n2) Eq(5) and Eq(6) are not consistent. If I_g = I_f, the coefficient before \\kappa should be 2; otherwise, the input update shouldn't include c^{t-1}. \n\n3) The intuition of having one gate instead of three is not very clear in the paper. Mixing all the functions, i.e., input, forget, and output filters, does provide freedom for learning but also introduces the learning complexity. Decoupling a complex function into a couple of simpler functions is a common way to narrow down the searching space. This is exactly what the standard LSTM does. The authors may want to provide reasonable arguments to explain intuitively why using one gate is better. \n\nThe model performance comparison in the experiments would be fairer if let the models converge. Perhaps, the standard LSTM is just suffering the gradient-vanishing issue and using ResNet design, for example, might improve the performance. Similarly, the rgcLSTM has fewer parameters as shown in the experiments. A possible explanation for its demonstrated better performance could be that it's less suffering the vanishing gradient. But, this doesn't indicate it's better than the standard LSTM. In summary, experimental results are good to verify the goodness of a model, but the theoretical support or intuitions are more important to understand the benefits that we can gain by making this change to the standard LSTM. This part is not clear from the paper. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper53/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reduced-Gate Convolutional LSTM Design Using Predictive Coding for Next-Frame Video Prediction", "abstract": "Spatiotemporal sequence prediction is an important problem in deep learning. We\nstudy next-frame video prediction using a deep-learning-based predictive coding\nframework that uses convolutional, long short-term memory (convLSTM) modules.\nWe introduce a novel reduced-gate convolutional LSTM architecture. Our\nreduced-gate model achieves better next-frame prediction accuracy than the original\nconvolutional LSTM while using a smaller parameter budget, thereby reducing\ntraining time. We tested our reduced gate modules within a predictive coding architecture\non the moving MNIST and KITTI datasets. We found that our reduced-gate\nmodel has a significant reduction of approximately 40 percent of the total\nnumber of training parameters and training time in comparison with the standard\nLSTM model which makes it attractive for hardware implementation especially\non small devices.", "paperhash": "elsayed|reducedgate_convolutional_lstm_design_using_predictive_coding_for_nextframe_video_prediction", "keywords": ["rgcLSTM", "convolutional LSTM", "unsupervised learning", "predictive coding", "video prediction", "moving MNIST", "KITTI datasets", "deep learning"], "authorids": ["nelly.elsayed5@gmail.com", "maida@louisiana.edu", "mab0778@louisiana.edu"], "authors": ["Nelly Elsayed", "Anthony S. Maida", "Magdy Bayoumi"], "TL;DR": "A novel reduced-gate convolutional LSTM design using predictive coding for next-frame video prediction", "pdf": "/pdf/b5c5837202d2f45bc52d02ef302a9dbf4ac5f77c.pdf", "_bibtex": "@misc{\nelsayed2019reducedgate,\ntitle={Reduced-Gate Convolutional {LSTM} Design Using Predictive Coding for Next-Frame Video Prediction},\nauthor={Nelly Elsayed and Anthony S. Maida and Magdy Bayoumi},\nyear={2019},\nurl={https://openreview.net/forum?id=rJEyrjRqYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper53/Official_Review", "cdate": 1542234548553, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJEyrjRqYX", "replyto": "rJEyrjRqYX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper53/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335638543, "tmdate": 1552335638543, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper53/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1gC5rSPnX", "original": null, "number": 1, "cdate": 1540998549888, "ddate": null, "tcdate": 1540998549888, "tmdate": 1541534326424, "tddate": null, "forum": "rJEyrjRqYX", "replyto": "rJEyrjRqYX", "invitation": "ICLR.cc/2019/Conference/-/Paper53/Official_Review", "content": {"title": "A new LSTM architecture that demonstrates some improvements over other LSTM modules", "review": "Authors present a new LSTM architecture, i.e. reduced gate convolutional LSTM. Authors use only one trainable gate, i.e. the forget gate which leads to less trainable parameters. They demonstrate the superiority of ti in two datasets, the moving MNIST and KITTI. Their results show that their architecture performs better than others in more accurately predicting next-frame. The paper is clearly written and the evaluation is based on other proposed similar architectures. For the moving MNIST they compared their model against the vanilla convLSTM with three gates and no peephole connections, based on previous work which has shown that it exceeds the accuracy performance of other LSTM based approaches. \nThe results show that the training time is reduced and the standard error alike. The only limitations is that more databases could have been used to demonstrate the enhancement in performance.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper53/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reduced-Gate Convolutional LSTM Design Using Predictive Coding for Next-Frame Video Prediction", "abstract": "Spatiotemporal sequence prediction is an important problem in deep learning. We\nstudy next-frame video prediction using a deep-learning-based predictive coding\nframework that uses convolutional, long short-term memory (convLSTM) modules.\nWe introduce a novel reduced-gate convolutional LSTM architecture. Our\nreduced-gate model achieves better next-frame prediction accuracy than the original\nconvolutional LSTM while using a smaller parameter budget, thereby reducing\ntraining time. We tested our reduced gate modules within a predictive coding architecture\non the moving MNIST and KITTI datasets. We found that our reduced-gate\nmodel has a significant reduction of approximately 40 percent of the total\nnumber of training parameters and training time in comparison with the standard\nLSTM model which makes it attractive for hardware implementation especially\non small devices.", "paperhash": "elsayed|reducedgate_convolutional_lstm_design_using_predictive_coding_for_nextframe_video_prediction", "keywords": ["rgcLSTM", "convolutional LSTM", "unsupervised learning", "predictive coding", "video prediction", "moving MNIST", "KITTI datasets", "deep learning"], "authorids": ["nelly.elsayed5@gmail.com", "maida@louisiana.edu", "mab0778@louisiana.edu"], "authors": ["Nelly Elsayed", "Anthony S. Maida", "Magdy Bayoumi"], "TL;DR": "A novel reduced-gate convolutional LSTM design using predictive coding for next-frame video prediction", "pdf": "/pdf/b5c5837202d2f45bc52d02ef302a9dbf4ac5f77c.pdf", "_bibtex": "@misc{\nelsayed2019reducedgate,\ntitle={Reduced-Gate Convolutional {LSTM} Design Using Predictive Coding for Next-Frame Video Prediction},\nauthor={Nelly Elsayed and Anthony S. Maida and Magdy Bayoumi},\nyear={2019},\nurl={https://openreview.net/forum?id=rJEyrjRqYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper53/Official_Review", "cdate": 1542234548553, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJEyrjRqYX", "replyto": "rJEyrjRqYX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper53/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335638543, "tmdate": 1552335638543, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper53/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 12}