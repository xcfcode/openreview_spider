{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730191507, "tcdate": 1508512574690, "number": 24, "cdate": 1518730191497, "id": "SJvrXqvaZ", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "SJvrXqvaZ", "original": "BJISm5vT-", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Adversary A3C for Robust Reinforcement Learning", "abstract": "Asynchronous Advantage Actor Critic (A3C) is an effective Reinforcement Learning (RL) algorithm for a wide range of tasks, such as Atari games and robot control. The agent learns policies and value function through trial-and-error interactions with the environment until converging to an optimal policy. Robustness and stability are critical in RL; however, neural network can be vulnerable to noise from unexpected sources and is not likely to withstand very slight disturbances. We note that agents generated from mild environment using A3C are not able to handle challenging environments. Learning from adversarial examples, we proposed an algorithm called Adversary Robust A3C (AR-A3C) to improve the agent\u2019s performance under noisy environments. In this algorithm, an adversarial agent is introduced to the learning process to make it more robust against adversarial disturbances, thereby making it more adaptive to noisy environments. Both simulations and real-world experiments are carried out to illustrate the stability of the proposed algorithm. The AR-A3C algorithm outperforms A3C in both clean and noisy environments. ", "pdf": "/pdf/c25bf035784d1264c1a41f3000bfe219c202b361.pdf", "paperhash": "gu|adversary_a3c_for_robust_reinforcement_learning", "_bibtex": "@misc{\ngu2018adversary,\ntitle={Adversary A3C for Robust Reinforcement Learning},\nauthor={Zhaoyuan Gu and Zhenzhong Jia and Howie Choset},\nyear={2018},\nurl={https://openreview.net/forum?id=SJvrXqvaZ},\n}", "keywords": ["Adversary", "Robust", "Reinforcement Learning", "A3C"], "authors": ["Zhaoyuan Gu", "Zhenzhong Jia", "Howie Choset"], "authorids": ["guzhaoyuan14@gmail.com", "zhenzhong.jia@gmail.com", "choset@cs.cmu.edu"]}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260079725, "tcdate": 1517250061239, "number": 735, "cdate": 1517250061225, "id": "ryBMUJ6rz", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "SJvrXqvaZ", "replyto": "SJvrXqvaZ", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "Reviewers are unanimous in scoring this paper below threshold for acceptance.  The authors did not submit any rebuttals of the reviews.\n\nPros:\nPaper is generally clear.\nHardware results are valuable.\n\nCons:\nLimited simulation results.\nProposed method is not really novel.\nInsufficient empirical validation of the approach."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversary A3C for Robust Reinforcement Learning", "abstract": "Asynchronous Advantage Actor Critic (A3C) is an effective Reinforcement Learning (RL) algorithm for a wide range of tasks, such as Atari games and robot control. The agent learns policies and value function through trial-and-error interactions with the environment until converging to an optimal policy. Robustness and stability are critical in RL; however, neural network can be vulnerable to noise from unexpected sources and is not likely to withstand very slight disturbances. We note that agents generated from mild environment using A3C are not able to handle challenging environments. Learning from adversarial examples, we proposed an algorithm called Adversary Robust A3C (AR-A3C) to improve the agent\u2019s performance under noisy environments. In this algorithm, an adversarial agent is introduced to the learning process to make it more robust against adversarial disturbances, thereby making it more adaptive to noisy environments. Both simulations and real-world experiments are carried out to illustrate the stability of the proposed algorithm. The AR-A3C algorithm outperforms A3C in both clean and noisy environments. ", "pdf": "/pdf/c25bf035784d1264c1a41f3000bfe219c202b361.pdf", "paperhash": "gu|adversary_a3c_for_robust_reinforcement_learning", "_bibtex": "@misc{\ngu2018adversary,\ntitle={Adversary A3C for Robust Reinforcement Learning},\nauthor={Zhaoyuan Gu and Zhenzhong Jia and Howie Choset},\nyear={2018},\nurl={https://openreview.net/forum?id=SJvrXqvaZ},\n}", "keywords": ["Adversary", "Robust", "Reinforcement Learning", "A3C"], "authors": ["Zhaoyuan Gu", "Zhenzhong Jia", "Howie Choset"], "authorids": ["guzhaoyuan14@gmail.com", "zhenzhong.jia@gmail.com", "choset@cs.cmu.edu"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642414950, "tcdate": 1511818971888, "number": 1, "cdate": 1511818971888, "id": "S14kDbqlG", "invitation": "ICLR.cc/2018/Conference/-/Paper24/Official_Review", "forum": "SJvrXqvaZ", "replyto": "SJvrXqvaZ", "signatures": ["ICLR.cc/2018/Conference/Paper24/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Interesting approach and hardware validation, but methods and comparisons are lacking", "rating": "4: Ok but not good enough - rejection", "review": "Positive:\n- Interesting approach\n- Hardware validation (the RL field needs more of this!)\n\nNegative:\n- Figure 2: what is the reward here? The one from Section 5.1?\n- No comparisons to other methods: Single pendulum swing-up is a very easy task that has been solved with various methods (mostly in a cart-pole setup). Please compare to existing methods such as PILCO, basic Q-learning, classical methods... \n- I'm not sure what's going on with the grammar in Section 5.3 (\"like crazy\", \"super hot\"...). This section also seems irrelevant (move to an appendix/supplementary or remove).\n- You should plot a typical control curve for the motors (requested torques). This might explain your heat problem (I'm guessing the motor is effectively controlled by a bang-bang controller).\n- Why did you pick this task? It's fine to only validate on a single task in hardware, but why not include additional simulation results (e.g. double pendulum)?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversary A3C for Robust Reinforcement Learning", "abstract": "Asynchronous Advantage Actor Critic (A3C) is an effective Reinforcement Learning (RL) algorithm for a wide range of tasks, such as Atari games and robot control. The agent learns policies and value function through trial-and-error interactions with the environment until converging to an optimal policy. Robustness and stability are critical in RL; however, neural network can be vulnerable to noise from unexpected sources and is not likely to withstand very slight disturbances. We note that agents generated from mild environment using A3C are not able to handle challenging environments. Learning from adversarial examples, we proposed an algorithm called Adversary Robust A3C (AR-A3C) to improve the agent\u2019s performance under noisy environments. In this algorithm, an adversarial agent is introduced to the learning process to make it more robust against adversarial disturbances, thereby making it more adaptive to noisy environments. Both simulations and real-world experiments are carried out to illustrate the stability of the proposed algorithm. The AR-A3C algorithm outperforms A3C in both clean and noisy environments. ", "pdf": "/pdf/c25bf035784d1264c1a41f3000bfe219c202b361.pdf", "paperhash": "gu|adversary_a3c_for_robust_reinforcement_learning", "_bibtex": "@misc{\ngu2018adversary,\ntitle={Adversary A3C for Robust Reinforcement Learning},\nauthor={Zhaoyuan Gu and Zhenzhong Jia and Howie Choset},\nyear={2018},\nurl={https://openreview.net/forum?id=SJvrXqvaZ},\n}", "keywords": ["Adversary", "Robust", "Reinforcement Learning", "A3C"], "authors": ["Zhaoyuan Gu", "Zhenzhong Jia", "Howie Choset"], "authorids": ["guzhaoyuan14@gmail.com", "zhenzhong.jia@gmail.com", "choset@cs.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642414860, "id": "ICLR.cc/2018/Conference/-/Paper24/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper24/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper24/AnonReviewer2", "ICLR.cc/2018/Conference/Paper24/AnonReviewer1", "ICLR.cc/2018/Conference/Paper24/AnonReviewer5"], "reply": {"forum": "SJvrXqvaZ", "replyto": "SJvrXqvaZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper24/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642414860}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642414914, "tcdate": 1511831233423, "number": 2, "cdate": 1511831233423, "id": "r1tT8Ncez", "invitation": "ICLR.cc/2018/Conference/-/Paper24/Official_Review", "forum": "SJvrXqvaZ", "replyto": "SJvrXqvaZ", "signatures": ["ICLR.cc/2018/Conference/Paper24/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "The proposed technique is of modest contribution and the experimental results do not provide sufficient validation of the approach.  ", "rating": "4: Ok but not good enough - rejection", "review": "The authors propose an extension of adversarial reinforcement learning to A3C. The proposed technique is of modest contribution and the experimental results do not provide sufficient validation of the approach.  \n\nThe authors propose extending A3C to produce more robust policies by training a zero-sum game with two agents: a protagonist and an antagonist. The protagonist is attempting to achieve the given task while the antagonist's goal is for the task to fail. \n\nThe contribution of this work, AR-A3C, is extending adversarial reinforcement learning, namely robust RL (RRL) and robust adversarial RL (RARL), to A3C. In the context of this prior work the novelty is extending the family of adversarial RL methods. However, the proposed method is still within the same family methods as demonstrated by RARL.\n\nThe authors state that AR-A3C requires half as many rollouts as compared to RARL. However, no empirical comparison between the two methods is performed. The paper only performs analysis against the A3C and no other adversarial baseline and on only one environment: cartpole.  While they show transfer to the real world cartpole with this technique, there is not sufficient analysis to satisfactorily demonstrate the benefits of the proposed technique. \n\nThe paper reads well. There are a few notational issues in the paper that should be addressed. The authors mislabel the value function V as the  action value, or Q function. The action value function is action dependent where the value function is not.  As a much more minor issue, the authors introduce y as the discount factor, which deviates from the standard notation of \\gamma without any obvious reason to do so.\n\nDouble blind was likely compromised with the youtube video, which was linked to a real name account instead of an anonymous account.\n\nOverall, the proposed technique is of modest contribution and the experimental results do not provide sufficient validation of the approach.    ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversary A3C for Robust Reinforcement Learning", "abstract": "Asynchronous Advantage Actor Critic (A3C) is an effective Reinforcement Learning (RL) algorithm for a wide range of tasks, such as Atari games and robot control. The agent learns policies and value function through trial-and-error interactions with the environment until converging to an optimal policy. Robustness and stability are critical in RL; however, neural network can be vulnerable to noise from unexpected sources and is not likely to withstand very slight disturbances. We note that agents generated from mild environment using A3C are not able to handle challenging environments. Learning from adversarial examples, we proposed an algorithm called Adversary Robust A3C (AR-A3C) to improve the agent\u2019s performance under noisy environments. In this algorithm, an adversarial agent is introduced to the learning process to make it more robust against adversarial disturbances, thereby making it more adaptive to noisy environments. Both simulations and real-world experiments are carried out to illustrate the stability of the proposed algorithm. The AR-A3C algorithm outperforms A3C in both clean and noisy environments. ", "pdf": "/pdf/c25bf035784d1264c1a41f3000bfe219c202b361.pdf", "paperhash": "gu|adversary_a3c_for_robust_reinforcement_learning", "_bibtex": "@misc{\ngu2018adversary,\ntitle={Adversary A3C for Robust Reinforcement Learning},\nauthor={Zhaoyuan Gu and Zhenzhong Jia and Howie Choset},\nyear={2018},\nurl={https://openreview.net/forum?id=SJvrXqvaZ},\n}", "keywords": ["Adversary", "Robust", "Reinforcement Learning", "A3C"], "authors": ["Zhaoyuan Gu", "Zhenzhong Jia", "Howie Choset"], "authorids": ["guzhaoyuan14@gmail.com", "zhenzhong.jia@gmail.com", "choset@cs.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642414860, "id": "ICLR.cc/2018/Conference/-/Paper24/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper24/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper24/AnonReviewer2", "ICLR.cc/2018/Conference/Paper24/AnonReviewer1", "ICLR.cc/2018/Conference/Paper24/AnonReviewer5"], "reply": {"forum": "SJvrXqvaZ", "replyto": "SJvrXqvaZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper24/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642414860}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642414875, "tcdate": 1513635513867, "number": 3, "cdate": 1513635513867, "id": "HJzp02rMM", "invitation": "ICLR.cc/2018/Conference/-/Paper24/Official_Review", "forum": "SJvrXqvaZ", "replyto": "SJvrXqvaZ", "signatures": ["ICLR.cc/2018/Conference/Paper24/AnonReviewer5"], "readers": ["everyone"], "content": {"title": "Ok but not good enough", "rating": "4: Ok but not good enough - rejection", "review": "Clarity \nThe paper is clear in general. \n\nOriginality\nThe novelty of the method is limited. The proposed method is a simple extension of L. Pinto et al. by replacing TRPO with A3C. No evidence is provided to show the proposed method is competitive with the original TRPO version. \n\nSignificance\n- The empirical results on the hardware are valuable. \n- The simulated results are very limited. The neural networks used in the simulation have only one hidden layer. The method is tested on the Pendulum domain. \n\nPros:\n- Real hardware results are provided. \n\nCons:\n- Limited simulation results. \n- Lacking technical novelty. \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversary A3C for Robust Reinforcement Learning", "abstract": "Asynchronous Advantage Actor Critic (A3C) is an effective Reinforcement Learning (RL) algorithm for a wide range of tasks, such as Atari games and robot control. The agent learns policies and value function through trial-and-error interactions with the environment until converging to an optimal policy. Robustness and stability are critical in RL; however, neural network can be vulnerable to noise from unexpected sources and is not likely to withstand very slight disturbances. We note that agents generated from mild environment using A3C are not able to handle challenging environments. Learning from adversarial examples, we proposed an algorithm called Adversary Robust A3C (AR-A3C) to improve the agent\u2019s performance under noisy environments. In this algorithm, an adversarial agent is introduced to the learning process to make it more robust against adversarial disturbances, thereby making it more adaptive to noisy environments. Both simulations and real-world experiments are carried out to illustrate the stability of the proposed algorithm. The AR-A3C algorithm outperforms A3C in both clean and noisy environments. ", "pdf": "/pdf/c25bf035784d1264c1a41f3000bfe219c202b361.pdf", "paperhash": "gu|adversary_a3c_for_robust_reinforcement_learning", "_bibtex": "@misc{\ngu2018adversary,\ntitle={Adversary A3C for Robust Reinforcement Learning},\nauthor={Zhaoyuan Gu and Zhenzhong Jia and Howie Choset},\nyear={2018},\nurl={https://openreview.net/forum?id=SJvrXqvaZ},\n}", "keywords": ["Adversary", "Robust", "Reinforcement Learning", "A3C"], "authors": ["Zhaoyuan Gu", "Zhenzhong Jia", "Howie Choset"], "authorids": ["guzhaoyuan14@gmail.com", "zhenzhong.jia@gmail.com", "choset@cs.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642414860, "id": "ICLR.cc/2018/Conference/-/Paper24/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper24/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper24/AnonReviewer2", "ICLR.cc/2018/Conference/Paper24/AnonReviewer1", "ICLR.cc/2018/Conference/Paper24/AnonReviewer5"], "reply": {"forum": "SJvrXqvaZ", "replyto": "SJvrXqvaZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper24/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642414860}}}], "count": 5}