{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1395643500000, "tcdate": 1395643500000, "number": 7, "id": "Kz2rIbvO-Uzih", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "ZZ7T6hXbaEcAQ", "replyto": "ZZ7T6hXbaEcAQ", "signatures": ["Phil Bachman"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "For the experiments presented in Fig. 3, I think you guys should just train the tied-weight ensembles with a reduced number of masks too. Then you could directly expose the effect of weight tying on the performance of the induced ensemble, while conveniently avoiding the need to 'max-out' (heh) the size of the ensembles you train with untied weights. I.e., plot the expected performance of both tied-weight ensembles and untied-weight ensembles as a function of the number of distinct masks (i.e. ensemble members) used in training.\r\n\r\nWhile the untied-weight ensembles would still be implicitly 'averaging an exponential number of models' at runtime, you would only be explicitly training as many of those models as are explicitly trained in the corresponding untied-weight ensemble."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An empirical analysis of dropout in piecewise linear networks", "decision": "submitted, no decision", "abstract": "The recently introduced dropout training criterion for neural networks has been the subject of much attention due to its simplicity and remarkable effectiveness as a regularizer, as well as its interpretation as a training procedure for an exponentially large ensemble of networks that share parameters. In this work we empirically investigate several questions related to the efficacy of dropout, specifically as it concerns networks employing the popular rectified linear activation function. We investigate the quality of the test time weight-scaling inference procedure by evaluating the geometric average exactly in small models, as well as compare the performance of the geometric mean to the arithmetic mean more commonly employed by ensemble techniques. We explore the effect of tied weights on the ensemble interpretation by training ensembles of masked networks without tied weights. Finally, we investigate an alternative criterion based on a biased estimator of the maximum likelihood ensemble gradient.", "pdf": "https://arxiv.org/abs/1312.6197", "paperhash": "wardefarley|an_empirical_analysis_of_dropout_in_piecewise_linear_networks", "keywords": [], "conflicts": [], "authors": ["David Warde-Farley", "Ian Goodfellow", "Aaron Courville", "Yoshua Bengio"], "authorids": ["d.warde.farley@gmail.com", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1394206680000, "tcdate": 1394206680000, "number": 6, "id": "SIn-NMWgu4I90", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "ZZ7T6hXbaEcAQ", "replyto": "ZZ7T6hXbaEcAQ", "signatures": ["David Warde-Farley"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We have addressed many of the comments kindly provided by our reviewers and apologize for not posting an updated manuscript sooner. Pending acceptance to arXiv.org, a revised version is available at http://www-etud.iro.umontreal.ca/~wardefar/iclr2014.pdf\r\n\r\nIn addition to incorporating reviewer feedback, we made one change to the experiments in Section 6 in order to be more charitable to the untied weights ensemble. Specifically, we altered the manner in which we set the hyperparameters used to train the ensemble members -- we now use a network with the same number of hidden units as the best performing dropout network but reoptimize all other hyperparameters to perform well when training via SGD without any masks. This considerably improves the untied ensemble's performance, but dropout still performs considerably better. We also provide the SGD baseline that resulted from optimizing these hyperparameters. We now employ 600 ensemble members; we hope to increase the number of ensemble members for the final copy."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An empirical analysis of dropout in piecewise linear networks", "decision": "submitted, no decision", "abstract": "The recently introduced dropout training criterion for neural networks has been the subject of much attention due to its simplicity and remarkable effectiveness as a regularizer, as well as its interpretation as a training procedure for an exponentially large ensemble of networks that share parameters. In this work we empirically investigate several questions related to the efficacy of dropout, specifically as it concerns networks employing the popular rectified linear activation function. We investigate the quality of the test time weight-scaling inference procedure by evaluating the geometric average exactly in small models, as well as compare the performance of the geometric mean to the arithmetic mean more commonly employed by ensemble techniques. We explore the effect of tied weights on the ensemble interpretation by training ensembles of masked networks without tied weights. Finally, we investigate an alternative criterion based on a biased estimator of the maximum likelihood ensemble gradient.", "pdf": "https://arxiv.org/abs/1312.6197", "paperhash": "wardefarley|an_empirical_analysis_of_dropout_in_piecewise_linear_networks", "keywords": [], "conflicts": [], "authors": ["David Warde-Farley", "Ian Goodfellow", "Aaron Courville", "Yoshua Bengio"], "authorids": ["d.warde.farley@gmail.com", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392817500000, "tcdate": 1392817500000, "number": 5, "id": "K2d4DqsAXe2CJ", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "ZZ7T6hXbaEcAQ", "replyto": "ZZ7T6hXbaEcAQ", "signatures": ["David Warde-Farley"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We thank all of our reviewers for their very useful feedback. We are actively working on a revised version of the manuscript, which will be available by the end of the week.\r\n\r\nTwo of the reviewers mentioned difficulty interpreting Figures 1 and 2. We apologize that these were not as clear as they could be and are endeavouring to improve them. It seems likely we will simplify with a scatterplot of test error (approximate) versus test error (exact), and test error (arithmetic mean-exact) versus test error (geometric mean-exact). The main issue with these, especially in the case of Figure 1, is that the differences are barely noticeable and all points appear to fall on the y=x line.\r\n\r\nAnonymous 3e6b:\r\n- We have made efforts to increase the number of masks used in Section 6. We will endeavour to further increase this for the final copy. As far as larger datasets, while this is an interesting direction, we believe that since it is a regularization method, the problems solved by dropout (that of overfitting) are still very acutely present and studiable on small, relatively simple datasets, which have the added benefit of being amenable to the kinds of exhaustive enumeration of masks that we have carried out.\r\n\r\n- The investigations in sections 4 and 5 specifically concern the properties of dropout-trained networks at test-time, therefore we felt comparisons to networks trained without dropout to be unwarranted as it is not clear what these comparisons would be aiming to demonstrate. Sections 6 concerns the performance of untied weight ensembles versus dropout; we can easily add a non-dropout baseline. Section 7\u2019s results are all taken with respect to a network with identical hyperparameters trained without dropout; the test error without dropout is plotted on the X axis and test error with dropout (or dropout boosting) is plotted on the Y. We apologize that this was not clear, we can make this clearer in the axis labels.\r\n\r\n- Thank you for the suggestion for a point-by-point summary of all possible (or at least all of the ones considered in the paper) interpretations of dropout, we will add this.\r\n\r\n- While some of the conclusions presented in this paper undoubtedly apply to maxout as well, we felt that thoroughly addressing the very popular case of rectified linear networks was important. Had we repeated all of our analyses with maxout (bearing in mind the additional confounding factor of the pool size hyperparameter associated with maxout units) the paper would\u2019ve extended well beyond the recommended length.\r\n\r\n- We apologize that these were not clear. Differences are relative to the magnitude reported on the x axis. See our comments above.\r\n\r\nAnonymous 925a:\r\n\r\n- We appreciate the feedback on Figure 1 and 2. Reviewer 3e6b also found them confusing, so we will be addressing this (see comments above).\r\n\r\n- Regarding Figure 3, we chose 120 in order to match Nitish Srivastava\u2019s similar figure, but ran 360 because we would then have 3 independent ensembles with which we could report error bars. We have already increased the number of networks trained and will plot the curve all the way until the end, even without error bars.\r\n\r\nAnonymous eb20:\r\n\r\nThank you for your constructive comments. We will address all of them in turn. A few specifics that warrant a particular reply:\r\n\r\n- We do cite Jarrett et al in our Introduction, we will add a further citation where you suggest.\r\n- We speculate that since the scaling is exact in the case of linear networks, locally linear networks (being \u201cclose\u201d to linear, at least in a small region around training examples) will more closely approximate the ensemble geometric mean than with saturating nonlinearities. We will explicitly state this.\r\n- The arithmetic mean is roughly as expensive as the geometric mean, but lacks a high quality tractable approximation like the weight scaling trick. We will explicitly state this.\r\n- In terms of the stochastic estimator of the ensemble gradient, dropout boosting employs masks drawn from the same distribution as dropout bagging (a.k.a. ordinary dropout). Indeed, the first term of the dropout boosting update is simply the update utilized by dropout. The second term is the gradient of the log likelihood of the same randomly chosen submodel (i.e. same dropout mask) but substituting the true targets with the approximately-averaged ensemble prediction. Training proceeds in the same fashion as dropout, where one randomly selected subnetwork (from the same distribution over masks) is updated, but according to a more globally aware criterion. If the mask application is viewed merely as the addition of noise, both criteria employ identical noise, as the selection procedure and random distribution over masks is identical. We will endeavour to make the text clearer on this point."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An empirical analysis of dropout in piecewise linear networks", "decision": "submitted, no decision", "abstract": "The recently introduced dropout training criterion for neural networks has been the subject of much attention due to its simplicity and remarkable effectiveness as a regularizer, as well as its interpretation as a training procedure for an exponentially large ensemble of networks that share parameters. In this work we empirically investigate several questions related to the efficacy of dropout, specifically as it concerns networks employing the popular rectified linear activation function. We investigate the quality of the test time weight-scaling inference procedure by evaluating the geometric average exactly in small models, as well as compare the performance of the geometric mean to the arithmetic mean more commonly employed by ensemble techniques. We explore the effect of tied weights on the ensemble interpretation by training ensembles of masked networks without tied weights. Finally, we investigate an alternative criterion based on a biased estimator of the maximum likelihood ensemble gradient.", "pdf": "https://arxiv.org/abs/1312.6197", "paperhash": "wardefarley|an_empirical_analysis_of_dropout_in_piecewise_linear_networks", "keywords": [], "conflicts": [], "authors": ["David Warde-Farley", "Ian Goodfellow", "Aaron Courville", "Yoshua Bengio"], "authorids": ["d.warde.farley@gmail.com", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391804460000, "tcdate": 1391804460000, "number": 4, "id": "jLaDjXnpvfjwh", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "ZZ7T6hXbaEcAQ", "replyto": "ZZ7T6hXbaEcAQ", "signatures": ["anonymous reviewer eb20"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "This paper provided a very interesting analysis of dropout, which has recently shown great success in a variety of DNN applications. The paper is well written and the experimental analysis is strong. My comments are mainly to help improve the paper and clarify ambiguity in some places\r\n\r\n\u2022\tSection 1, page1: when you give the equation f(x) = max(0,x) you should state that this is known as a recitified linear unit (ReLU) and provide the appropriate reference, which to my knowledge was first done here: Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y. What is the best multi-stage architecture for ob- ject recognition? In Proc. International Conference on Computer Vision (ICCV\u201909). IEEE, 2009.\r\n\u2022\tSection 2.2, page 3: why does ReLU work better than sigmoid when using dropout? You say this as well but providing some intuition would be good.\r\n\u2022\tSection 3, page 3: The reason you have chosen small networks for your initial analysis is so that you can do an exhaustive enumeration. You state this in Section 4 but not 3. You should state this upfront in Section 3, otherwise the reader might think your analysis might not be generalizable to larger data sets.\r\n\u2022\tSection 3, Page 4: Your training criterion (early stopping, etc) has been done in previous papers. Pls cite one of these so readers know that this is a commonly used approach\r\n\u2022\tSection 4, Page 5: Pls switch the order of sentences \u201cThe overall result\u2026 and In order to make differences..\u201d to make the flow easier to read. \r\n\u2022\tSection 4, Page 5:  Pls give references for Wilcoxon signed rank test and Bonferroni correction, as not all readers will be familiar with this.\r\n\u2022\tSection 4, Page 5: There seem to be some outliers in Figure 1, but your significance test shows that this doesn\u2019t matter. You might want to add a sentence stating that the outliers in Figure 1 are not really significant as shown by the test.\r\n\u2022\tSection 5, Page 5: Can you comment on how expensive the arithmetic mean is?\r\n\u2022\tSection 6, page 6: Pls provide reference for \u201cutilized norm constraint regularization\u201d \u2192 this is known as max-norm and can be found in Nitish Shrivastava\u2019s thesis\r\n\u2022\tSection 6. Page 6: You need a period after the \u201c2\u201d footnote.\r\n\u2022\tSection 6, page 7: You should also state that the ensemble method required training 360 different networks which is computationally expensive compared to training just 1 network with dropout. \r\n\u2022\tSection 7, page 8: It is not clear to me how the dropout boosting injects the same amount of noise as dropout, and should be clarified."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An empirical analysis of dropout in piecewise linear networks", "decision": "submitted, no decision", "abstract": "The recently introduced dropout training criterion for neural networks has been the subject of much attention due to its simplicity and remarkable effectiveness as a regularizer, as well as its interpretation as a training procedure for an exponentially large ensemble of networks that share parameters. In this work we empirically investigate several questions related to the efficacy of dropout, specifically as it concerns networks employing the popular rectified linear activation function. We investigate the quality of the test time weight-scaling inference procedure by evaluating the geometric average exactly in small models, as well as compare the performance of the geometric mean to the arithmetic mean more commonly employed by ensemble techniques. We explore the effect of tied weights on the ensemble interpretation by training ensembles of masked networks without tied weights. Finally, we investigate an alternative criterion based on a biased estimator of the maximum likelihood ensemble gradient.", "pdf": "https://arxiv.org/abs/1312.6197", "paperhash": "wardefarley|an_empirical_analysis_of_dropout_in_piecewise_linear_networks", "keywords": [], "conflicts": [], "authors": ["David Warde-Farley", "Ian Goodfellow", "Aaron Courville", "Yoshua Bengio"], "authorids": ["d.warde.farley@gmail.com", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391804340000, "tcdate": 1391804340000, "number": 3, "id": "nn6Na4T5NHmCy", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "ZZ7T6hXbaEcAQ", "replyto": "ZZ7T6hXbaEcAQ", "signatures": ["anonymous reviewer eb20"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of An empirical analysis of dropout in piecewise linear networks", "review": "This paper provided a very interesting analysis of dropout, which has recently shown great success in a variety of DNN applications. The paper is well written and the experimental analysis is strong. My comments are mainly to help improve the paper and clarify ambiguity in some places\r\n\r\n\u2022\tSection 1, page1: when you give the equation f(x) = max(0,x) you should state that this is known as a recitified linear unit (ReLU) and provide the appropriate reference, which to my knowledge was first done here: Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y. What is the best multi-stage architecture for ob- ject recognition? In Proc. International Conference on Computer Vision (ICCV\u201909). IEEE, 2009.\r\n\u2022\tSection 2.2, page 3: why does ReLU work better than sigmoid when using dropout? You say this as well but providing some intuition would be good.\r\n\u2022\tSection 3, page 3: The reason you have chosen small networks for your initial analysis is so that you can do an exhaustive enumeration. You state this in Section 4 but not 3. You should state this upfront in Section 3, otherwise the reader might think your analysis might not be generalizable to larger data sets.\r\n\u2022\tSection 3, Page 4: Your training criterion (early stopping, etc) has been done in previous papers. Pls cite one of these so readers know that this is a commonly used approach\r\n\u2022\tSection 4, Page 5: Pls switch the order of sentences \u201cThe overall result\u2026 and In order to make differences..\u201d to make the flow easier to read. \r\n\u2022\tSection 4, Page 5:  Pls give references for Wilcoxon signed rank test and Bonferroni correction, as not all readers will be familiar with this.\r\n\u2022\tSection 4, Page 5: There seem to be some outliers in Figure 1, but your significance test shows that this doesn\u2019t matter. You might want to add a sentence stating that the outliers in Figure 1 are not really significant as shown by the test.\r\n\u2022\tSection 5, Page 5: Can you comment on how expensive the arithmetic mean is?\r\n\u2022\tSection 6, page 6: Pls provide reference for \u201cutilized norm constraint regularization\u201d \u2192 this is known as max-norm and can be found in Nitish Shrivastava\u2019s thesis\r\n\u2022\tSection 6. Page 6: You need a period after the \u201c2\u201d footnote.\r\n\u2022\tSection 6, page 7: You should also state that the ensemble method required training 360 different networks which is computationally expensive compared to training just 1 network with dropout. \r\n\u2022\tSection 7, page 8: It is not clear to me how the dropout boosting injects the same amount of noise as dropout, and should be clarified."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An empirical analysis of dropout in piecewise linear networks", "decision": "submitted, no decision", "abstract": "The recently introduced dropout training criterion for neural networks has been the subject of much attention due to its simplicity and remarkable effectiveness as a regularizer, as well as its interpretation as a training procedure for an exponentially large ensemble of networks that share parameters. In this work we empirically investigate several questions related to the efficacy of dropout, specifically as it concerns networks employing the popular rectified linear activation function. We investigate the quality of the test time weight-scaling inference procedure by evaluating the geometric average exactly in small models, as well as compare the performance of the geometric mean to the arithmetic mean more commonly employed by ensemble techniques. We explore the effect of tied weights on the ensemble interpretation by training ensembles of masked networks without tied weights. Finally, we investigate an alternative criterion based on a biased estimator of the maximum likelihood ensemble gradient.", "pdf": "https://arxiv.org/abs/1312.6197", "paperhash": "wardefarley|an_empirical_analysis_of_dropout_in_piecewise_linear_networks", "keywords": [], "conflicts": [], "authors": ["David Warde-Farley", "Ian Goodfellow", "Aaron Courville", "Yoshua Bengio"], "authorids": ["d.warde.farley@gmail.com", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391474340000, "tcdate": 1391474340000, "number": 2, "id": "Lkv0jtOBIqkz-", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "ZZ7T6hXbaEcAQ", "replyto": "ZZ7T6hXbaEcAQ", "signatures": ["anonymous reviewer 925a"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of An empirical analysis of dropout in piecewise linear networks", "review": "The authors attempt a further understanding of dropout through a set of empirical analyses that test a number of questions: 1, how close is the weight-scaling approximation to the geometric mean; 2., how good is the geometric mean compared to the arithmetic mean for classification; 3., Is the role of weight-tying in dropout important; and 4., is the ensemble aspect of dropout important compared to the benefit of using masking noise.\r\n\r\nThe conclusions of the authors are convincing, and the paper is a illuminating companion to some of the more theoretical analyses of dropout that have been presented recently. The dropout bagging vs dropout boosting is especially interesting. Standard dropout, which most resembles ensemble bagging, is compared to a version of weight-tied boosting. This comparison is constructed to try to determine whether there is a benefit to the bag ensemble, where each sub-model is independently tested and trained, compared the weight-tied boosting, where each sub-model is trained based on the performance of the entire ensemble. In weight-tied boosting, the benefit of weight-tying and masking noise are preserved, but not the independent sub-model training. The results seem to show that the independent ensemble/bagging aspect of dropout is important, rather than just the noise or the weight-tying.\r\n\r\nThe submission is relevant to the ICLR community. The experiments are carefully chosen and the conclusions are not overstated. The only significant barrier to publication is the lack of analysis of the empirical data and the figures, which are poorly chosen and not well explained. Figures 1 and 2 are difficult to read/interpret. A single summary/analysis for each of the 2 sets of data points would be very helpful. Figure 3 only goes to 120 ensemble members, but the text describes results at 360. It would be valuable to plot the full results, even if it is flat after 120."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An empirical analysis of dropout in piecewise linear networks", "decision": "submitted, no decision", "abstract": "The recently introduced dropout training criterion for neural networks has been the subject of much attention due to its simplicity and remarkable effectiveness as a regularizer, as well as its interpretation as a training procedure for an exponentially large ensemble of networks that share parameters. In this work we empirically investigate several questions related to the efficacy of dropout, specifically as it concerns networks employing the popular rectified linear activation function. We investigate the quality of the test time weight-scaling inference procedure by evaluating the geometric average exactly in small models, as well as compare the performance of the geometric mean to the arithmetic mean more commonly employed by ensemble techniques. We explore the effect of tied weights on the ensemble interpretation by training ensembles of masked networks without tied weights. Finally, we investigate an alternative criterion based on a biased estimator of the maximum likelihood ensemble gradient.", "pdf": "https://arxiv.org/abs/1312.6197", "paperhash": "wardefarley|an_empirical_analysis_of_dropout_in_piecewise_linear_networks", "keywords": [], "conflicts": [], "authors": ["David Warde-Farley", "Ian Goodfellow", "Aaron Courville", "Yoshua Bengio"], "authorids": ["d.warde.farley@gmail.com", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391401860000, "tcdate": 1391401860000, "number": 1, "id": "GfqAwRlBAJfi7", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "ZZ7T6hXbaEcAQ", "replyto": "ZZ7T6hXbaEcAQ", "signatures": ["anonymous reviewer 3e6b"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of An empirical analysis of dropout in piecewise linear networks", "review": "* A brief summary of the paper's contributions, in the context of prior work.\r\nPaper experimentally verifies how relevant are anecdotal descriptions of dropout. \r\n\r\n* An assessment of novelty and quality. \r\nIt is novel, and has a good quality. However, it doesn\u2019t bring any new ideas. It rather presents experiments which were missed during initial development of dropout.\r\n\r\n* A list of pros and cons (reasons to accept/reject).\r\npros:\r\n- Verifies some of previously postulated intuition.\r\n- Gives some indications what are the major building blocks of a good regularizer for neural networks.\r\n\r\ncons:\r\n- Experiment should be run on a larger datasets where not all possible masks are utilized but a large amount of them (e.g. not all 2^N, but e.g. 10^6 random one).\r\n- All the comparisons should be with respect to networks without any dropout or model averaging. Maybe on the datasets which you considered regardless of dropout you would get the same results. Such setting should be compared on all the plots. It was unclear for me if such experiments were executed.\r\n- Authors should write point-by-point what are all possible interpretations of dropout, and which one they are going to validate. It would be also good to have some suggestions how other anecdotal interpretations could be validated (e.g. co-adaptation).\r\n- The same authors were working on max-out networks, which seems to play well with dropout. It should be explained here (or just experimentally compared) why maxout is a good architecture for dropout. \r\n- Figures 1, and 2 are hard to interpret. How should I know if relative difference of 0.1 is big or small."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An empirical analysis of dropout in piecewise linear networks", "decision": "submitted, no decision", "abstract": "The recently introduced dropout training criterion for neural networks has been the subject of much attention due to its simplicity and remarkable effectiveness as a regularizer, as well as its interpretation as a training procedure for an exponentially large ensemble of networks that share parameters. In this work we empirically investigate several questions related to the efficacy of dropout, specifically as it concerns networks employing the popular rectified linear activation function. We investigate the quality of the test time weight-scaling inference procedure by evaluating the geometric average exactly in small models, as well as compare the performance of the geometric mean to the arithmetic mean more commonly employed by ensemble techniques. We explore the effect of tied weights on the ensemble interpretation by training ensembles of masked networks without tied weights. Finally, we investigate an alternative criterion based on a biased estimator of the maximum likelihood ensemble gradient.", "pdf": "https://arxiv.org/abs/1312.6197", "paperhash": "wardefarley|an_empirical_analysis_of_dropout_in_piecewise_linear_networks", "keywords": [], "conflicts": [], "authors": ["David Warde-Farley", "Ian Goodfellow", "Aaron Courville", "Yoshua Bengio"], "authorids": ["d.warde.farley@gmail.com", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1388123580000, "tcdate": 1388123580000, "number": 61, "id": "ZZ7T6hXbaEcAQ", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "ZZ7T6hXbaEcAQ", "signatures": ["d.warde.farley@gmail.com"], "readers": ["everyone"], "content": {"title": "An empirical analysis of dropout in piecewise linear networks", "decision": "submitted, no decision", "abstract": "The recently introduced dropout training criterion for neural networks has been the subject of much attention due to its simplicity and remarkable effectiveness as a regularizer, as well as its interpretation as a training procedure for an exponentially large ensemble of networks that share parameters. In this work we empirically investigate several questions related to the efficacy of dropout, specifically as it concerns networks employing the popular rectified linear activation function. We investigate the quality of the test time weight-scaling inference procedure by evaluating the geometric average exactly in small models, as well as compare the performance of the geometric mean to the arithmetic mean more commonly employed by ensemble techniques. We explore the effect of tied weights on the ensemble interpretation by training ensembles of masked networks without tied weights. Finally, we investigate an alternative criterion based on a biased estimator of the maximum likelihood ensemble gradient.", "pdf": "https://arxiv.org/abs/1312.6197", "paperhash": "wardefarley|an_empirical_analysis_of_dropout_in_piecewise_linear_networks", "keywords": [], "conflicts": [], "authors": ["David Warde-Farley", "Ian Goodfellow", "Aaron Courville", "Yoshua Bengio"], "authorids": ["d.warde.farley@gmail.com", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "writers": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 8}