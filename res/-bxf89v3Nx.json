{"notes": [{"id": "-bxf89v3Nx", "original": "NgQT3t0hXc", "number": 2065, "cdate": 1601308227508, "ddate": null, "tcdate": 1601308227508, "tmdate": 1616054476853, "tddate": null, "forum": "-bxf89v3Nx", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Calibration tests beyond classification", "authorids": ["~David_Widmann1", "fredrik.lindsten@liu.se", "dave.zachariah@it.uu.se"], "authors": ["David Widmann", "Fredrik Lindsten", "Dave Zachariah"], "keywords": ["calibration", "uncertainty quantification", "framework", "integral probability metric", "maximum mean discrepancy"], "abstract": "Most supervised machine learning tasks are subject to irreducible prediction errors. Probabilistic predictive models address this limitation by providing probability distributions that represent a belief over plausible targets, rather than point estimates. Such models can be a valuable tool in decision-making under uncertainty, provided that the model output is meaningful and interpretable. Calibrated models guarantee that the probabilistic predictions are neither over- nor under-confident. In the machine learning literature, different measures and statistical tests have been proposed and studied for evaluating the calibration of classification models. For regression problems, however, research has been focused on a weaker condition of calibration based on predicted quantiles for real-valued targets. In this paper, we propose the first framework that unifies calibration evaluation and tests for general probabilistic predictive models. It applies to any such model, including classification and regression models of arbitrary dimension. Furthermore, the framework generalizes existing measures and provides a more intuitive reformulation of a recently proposed framework for calibration in multi-class classification. In particular, we reformulate and generalize the kernel calibration error, its estimators, and hypothesis tests using scalar-valued kernels, and evaluate the calibration of real-valued regression\nproblems.", "one-sentence_summary": "Unifying framework for calibration evaluations and tests of probabilistic predictive models", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "widmann|calibration_tests_beyond_classification", "supplementary_material": "", "pdf": "/pdf/2bd555e04e5c7af5c723e69ed706339c99f212a1.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwidmann2021calibration,\ntitle={Calibration tests beyond classification},\nauthor={David Widmann and Fredrik Lindsten and Dave Zachariah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-bxf89v3Nx}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "zts_7GIiG8f", "original": null, "number": 1, "cdate": 1616010823898, "ddate": null, "tcdate": 1616010823898, "tmdate": 1616010823898, "tddate": null, "forum": "-bxf89v3Nx", "replyto": "swdmxaEI2K6", "invitation": "ICLR.cc/2021/Conference/Paper2065/-/Comment", "content": {"title": "Camera-ready version", "comment": "We extended the abstract as suggested by mentioning the kernel calibration error.\n\nAdditionally, we improved the paper in the following ways (see https://github.com/devmotion/Calibration_ICLR2021/pull/1 for a detailed changelog):\n- We fixed multiple typos (mostly in the supplementary material which is not separated from the main text anymore)\n- We updated the experiments (does not affect the results and explanations):\n   * We updated Julia package dependencies resulting in better visualizations and major performance improvements (particularly helpful for estimating p-values of the ensembles which was missing in the initial submission)\n   * We ensured reproducibility by pinning package versions and providing nix project environments\n- We added a webpage that displays automatically generated HTML output and Jupyter notebooks of the experiments (https://devmotion.github.io/Calibration_ICLR2021)\n\nMoreover, we made the calibration evaluation methods proposed in our paper (and ECE) available in the Julia packages CalibrationErrors.jl, CalibrationTests.jl, and CalibrationErrorsDistributions.jl. An (initial) documentation of the Julia packages is available at https://devmotion.github.io/CalibrationErrors.jl/dev/. Additionally, we published a Python wrapper called pycalibration that provides an interface for these packages (https://github.com/devmotion/pycalibration). Hopefully, these tools are useful for practitioners and can increase the practical relevance of our research."}, "signatures": ["ICLR.cc/2021/Conference/Paper2065/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2065/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Calibration tests beyond classification", "authorids": ["~David_Widmann1", "fredrik.lindsten@liu.se", "dave.zachariah@it.uu.se"], "authors": ["David Widmann", "Fredrik Lindsten", "Dave Zachariah"], "keywords": ["calibration", "uncertainty quantification", "framework", "integral probability metric", "maximum mean discrepancy"], "abstract": "Most supervised machine learning tasks are subject to irreducible prediction errors. Probabilistic predictive models address this limitation by providing probability distributions that represent a belief over plausible targets, rather than point estimates. Such models can be a valuable tool in decision-making under uncertainty, provided that the model output is meaningful and interpretable. Calibrated models guarantee that the probabilistic predictions are neither over- nor under-confident. In the machine learning literature, different measures and statistical tests have been proposed and studied for evaluating the calibration of classification models. For regression problems, however, research has been focused on a weaker condition of calibration based on predicted quantiles for real-valued targets. In this paper, we propose the first framework that unifies calibration evaluation and tests for general probabilistic predictive models. It applies to any such model, including classification and regression models of arbitrary dimension. Furthermore, the framework generalizes existing measures and provides a more intuitive reformulation of a recently proposed framework for calibration in multi-class classification. In particular, we reformulate and generalize the kernel calibration error, its estimators, and hypothesis tests using scalar-valued kernels, and evaluate the calibration of real-valued regression\nproblems.", "one-sentence_summary": "Unifying framework for calibration evaluations and tests of probabilistic predictive models", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "widmann|calibration_tests_beyond_classification", "supplementary_material": "", "pdf": "/pdf/2bd555e04e5c7af5c723e69ed706339c99f212a1.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwidmann2021calibration,\ntitle={Calibration tests beyond classification},\nauthor={David Widmann and Fredrik Lindsten and Dave Zachariah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-bxf89v3Nx}\n}"}, "tags": [], "invitation": {"reply": {"forum": "-bxf89v3Nx", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper2065/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2065/Authors|ICLR.cc/2021/Conference/Paper2065/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649464815, "tmdate": 1610649464815, "id": "ICLR.cc/2021/Conference/Paper2065/-/Comment"}}}, {"id": "swdmxaEI2K6", "original": null, "number": 1, "cdate": 1610040489931, "ddate": null, "tcdate": 1610040489931, "tmdate": 1610474095769, "tddate": null, "forum": "-bxf89v3Nx", "replyto": "-bxf89v3Nx", "invitation": "ICLR.cc/2021/Conference/Paper2065/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This is a well written paper addressing a challenging problem with an original approach.  While one reviewer claims there is not a strong call for calibration of regression tasks, this may well be because methods don't exist.  Certainly, calibration is a critical tool for classification.\n\nThe major failing of the paper, however, is the empirical evaluation.  Given that no prior work exists, it is arguably OK to not do this, but one could easily reject the paper on this issue alone, as AnonReviewer4 was inclined to do.  One reviewer, however, thought highly of the paper, which bumped up its average score, more than I think it should have got (due to the poor experimental work).\n\nThe abstract could be improved by mentioning the use of kernels, the nature of this solution is a substantial part of the paper."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Calibration tests beyond classification", "authorids": ["~David_Widmann1", "fredrik.lindsten@liu.se", "dave.zachariah@it.uu.se"], "authors": ["David Widmann", "Fredrik Lindsten", "Dave Zachariah"], "keywords": ["calibration", "uncertainty quantification", "framework", "integral probability metric", "maximum mean discrepancy"], "abstract": "Most supervised machine learning tasks are subject to irreducible prediction errors. Probabilistic predictive models address this limitation by providing probability distributions that represent a belief over plausible targets, rather than point estimates. Such models can be a valuable tool in decision-making under uncertainty, provided that the model output is meaningful and interpretable. Calibrated models guarantee that the probabilistic predictions are neither over- nor under-confident. In the machine learning literature, different measures and statistical tests have been proposed and studied for evaluating the calibration of classification models. For regression problems, however, research has been focused on a weaker condition of calibration based on predicted quantiles for real-valued targets. In this paper, we propose the first framework that unifies calibration evaluation and tests for general probabilistic predictive models. It applies to any such model, including classification and regression models of arbitrary dimension. Furthermore, the framework generalizes existing measures and provides a more intuitive reformulation of a recently proposed framework for calibration in multi-class classification. In particular, we reformulate and generalize the kernel calibration error, its estimators, and hypothesis tests using scalar-valued kernels, and evaluate the calibration of real-valued regression\nproblems.", "one-sentence_summary": "Unifying framework for calibration evaluations and tests of probabilistic predictive models", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "widmann|calibration_tests_beyond_classification", "supplementary_material": "", "pdf": "/pdf/2bd555e04e5c7af5c723e69ed706339c99f212a1.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwidmann2021calibration,\ntitle={Calibration tests beyond classification},\nauthor={David Widmann and Fredrik Lindsten and Dave Zachariah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-bxf89v3Nx}\n}"}, "tags": [], "invitation": {"reply": {"forum": "-bxf89v3Nx", "replyto": "-bxf89v3Nx", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040489918, "tmdate": 1610474095754, "id": "ICLR.cc/2021/Conference/Paper2065/-/Decision"}}}, {"id": "5dmTeP-VWiZ", "original": null, "number": 9, "cdate": 1605654882197, "ddate": null, "tcdate": 1605654882197, "tmdate": 1605654882197, "tddate": null, "forum": "-bxf89v3Nx", "replyto": "yredTQOClAY", "invitation": "ICLR.cc/2021/Conference/Paper2065/-/Official_Comment", "content": {"title": "Author comments (II/II)", "comment": "*Regarding the significance of the work, I can add that in practice, I find that relatively few ML users are concerned with the calibration of their models, and these are entirely restricted to problems of classification (almost always binary classification) or quantile regression. The novelty of this work seems to lie mostly in its applicability beyond these types problems: the authors write, \"A key contribution of this paper is a new framework that is applicable to multivariate regression, as well as... discrete ordinal or more complex (e.g., graph-structured) [output],\" so I venture a guess that the intended audience for this work is relatively small.*\n\nWe agree with this observation. However, to the best of our knowledge there are no calibration evaluation techniques available that go beyond classification problems and quantile regression. This makes it difficult for practitioners to reason about and evaluate calibration, even if they do care about it. We are convinced that calibration evaluation will be adopted and performed also in more general and complex problem settings if tools such as the methods proposed in this paper become available to practitioners.\n\n*In equation (2), I believe the RHS should be $\\max P_X$ instead of $\\arg \\max P_X$.*\n\n*Section 2, paragraph 2, you wrote \"instead of the discrepancy between the conditional distributions $\\mathbb{P}(Y | X)$ and $P_X$.\" Did you mean to write $\\mathbb{P}(Y | P_X)$ instead of $\\mathbb{P}(Y | X)$? That would make more sense to me, since the sentence would refer to comparing the LHS and RHS of Equation (1).*\n\nThank you for spotting these errors! Indeed, your assumptions are correct. We fixed these mistakes in the latest revision of our paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper2065/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2065/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Calibration tests beyond classification", "authorids": ["~David_Widmann1", "fredrik.lindsten@liu.se", "dave.zachariah@it.uu.se"], "authors": ["David Widmann", "Fredrik Lindsten", "Dave Zachariah"], "keywords": ["calibration", "uncertainty quantification", "framework", "integral probability metric", "maximum mean discrepancy"], "abstract": "Most supervised machine learning tasks are subject to irreducible prediction errors. Probabilistic predictive models address this limitation by providing probability distributions that represent a belief over plausible targets, rather than point estimates. Such models can be a valuable tool in decision-making under uncertainty, provided that the model output is meaningful and interpretable. Calibrated models guarantee that the probabilistic predictions are neither over- nor under-confident. In the machine learning literature, different measures and statistical tests have been proposed and studied for evaluating the calibration of classification models. For regression problems, however, research has been focused on a weaker condition of calibration based on predicted quantiles for real-valued targets. In this paper, we propose the first framework that unifies calibration evaluation and tests for general probabilistic predictive models. It applies to any such model, including classification and regression models of arbitrary dimension. Furthermore, the framework generalizes existing measures and provides a more intuitive reformulation of a recently proposed framework for calibration in multi-class classification. In particular, we reformulate and generalize the kernel calibration error, its estimators, and hypothesis tests using scalar-valued kernels, and evaluate the calibration of real-valued regression\nproblems.", "one-sentence_summary": "Unifying framework for calibration evaluations and tests of probabilistic predictive models", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "widmann|calibration_tests_beyond_classification", "supplementary_material": "", "pdf": "/pdf/2bd555e04e5c7af5c723e69ed706339c99f212a1.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwidmann2021calibration,\ntitle={Calibration tests beyond classification},\nauthor={David Widmann and Fredrik Lindsten and Dave Zachariah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-bxf89v3Nx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-bxf89v3Nx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2065/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2065/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2065/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2065/Authors|ICLR.cc/2021/Conference/Paper2065/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2065/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852681, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2065/-/Official_Comment"}}}, {"id": "bH5jfoiYwxF", "original": null, "number": 8, "cdate": 1605654576946, "ddate": null, "tcdate": 1605654576946, "tmdate": 1605654576946, "tddate": null, "forum": "-bxf89v3Nx", "replyto": "yredTQOClAY", "invitation": "ICLR.cc/2021/Conference/Paper2065/-/Official_Comment", "content": {"title": "Author comments (I/II)", "comment": "We thank you for your comments.\n\n*The main novelty of Definition 2 seems to be the introduction of a dummy variable $Z_X$ which is sampled from the predicted distribution $P_X$ over the target space, in order to compute an integral probability metric that compares the joint distribution of $(P_X, Y)$ to the joint distribution of $(P_X, Z_X)$. The authors note that in some cases, $Z_X$ can be integrated out of the definition either analytically or using numerical integration. It is not clear, from my limited background knowledge in the related work, why it should be helpful to introduce a dummy variable and then integrate it out.*\n\nConceptually, the main advantage of introducing the artificial random variable is that it allows us to recast the calibration condition in the language of two-sample tests by comparing two distributions. With this reformulation we avoid checking the almost sure equality of the predicted distributions and corresponding conditional distributions of targets explicitly, which is challenging in particular for general probabilistic predictive models. By doing so we can exploit existing results from the MMD literature, however in an unusual and special setting since the conditional distribution of the artificial random variable is known (typically we would only be given samples from the two distributions). In the MMD case the special setup can be exploited in the estimators and tests by integrating out $Z_X$, but we nevertheless work with the same probabilistic (re-)formulation of the calibration error.\n\n*The experiment in Section 5.2 demonstrates the utility of the proposed SKCE calibration metric, alongside more common metrics like negative log-likelihood (NLL) and mean squared error (MSE). It appears that the SKCE curves (for both training and test data) have a very similar shape to the NLL curves, so it's not clear what benefit SKCE provides above and beyond the more common and easily-computed NLL. I would have liked to see more convincing experimental evidence of the marginal benefit of this approach beyond common calibration metrics.*\n\nWe want to emphasize that neither log-likelihood nor mean squared error are calibration metrics. As shortly mentioned in the discussion, the decomposition of these scoring rules as sum of a so-called resolution term (which quantifies the sharpness of the predictions), a reliability term (which is a specific instance of the expected calibration error and hence quantifies calibration), and an entropy term (which quantifies the inherent uncertainty of the targets) shows that these metrics are evaluation metrics of probabilistic predictive models but not calibration metrics: models can trade off calibration for sharpness, i.e., a less calibrated but sharper model might yield a smaller NLL or MSE than a calibrated but less sharp model. One main advantage of our framework is that it provides a principled way for quantifying ONLY calibration for any probabilistic predictive model which previously was only possible for specific models such as classification models.\n\nThus in our opinion the SKCE provides an additional benefit and serves a different purpose than common evaluation metrics such as NLL and MSE. Moreover, while the shape of the NLL, MSE, and SKCE is similar (which can be seen as promising as well since it indicates that the SKCE does not behave in completely unexpected and uncommon ways), the plots show that the models are ranked differently by the different metrics. For instance, the models with the smallest SKCE can be found at an earlier iteration than the best models according to NLL or MSE. This indicates that the model overfits in terms of calibration, before we can see any clear indication of overfitting in terms of NLL. Monitoring the SKCE in addition to the NLL can thus provide additional valuable information regarding the model during the training procedure."}, "signatures": ["ICLR.cc/2021/Conference/Paper2065/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2065/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Calibration tests beyond classification", "authorids": ["~David_Widmann1", "fredrik.lindsten@liu.se", "dave.zachariah@it.uu.se"], "authors": ["David Widmann", "Fredrik Lindsten", "Dave Zachariah"], "keywords": ["calibration", "uncertainty quantification", "framework", "integral probability metric", "maximum mean discrepancy"], "abstract": "Most supervised machine learning tasks are subject to irreducible prediction errors. Probabilistic predictive models address this limitation by providing probability distributions that represent a belief over plausible targets, rather than point estimates. Such models can be a valuable tool in decision-making under uncertainty, provided that the model output is meaningful and interpretable. Calibrated models guarantee that the probabilistic predictions are neither over- nor under-confident. In the machine learning literature, different measures and statistical tests have been proposed and studied for evaluating the calibration of classification models. For regression problems, however, research has been focused on a weaker condition of calibration based on predicted quantiles for real-valued targets. In this paper, we propose the first framework that unifies calibration evaluation and tests for general probabilistic predictive models. It applies to any such model, including classification and regression models of arbitrary dimension. Furthermore, the framework generalizes existing measures and provides a more intuitive reformulation of a recently proposed framework for calibration in multi-class classification. In particular, we reformulate and generalize the kernel calibration error, its estimators, and hypothesis tests using scalar-valued kernels, and evaluate the calibration of real-valued regression\nproblems.", "one-sentence_summary": "Unifying framework for calibration evaluations and tests of probabilistic predictive models", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "widmann|calibration_tests_beyond_classification", "supplementary_material": "", "pdf": "/pdf/2bd555e04e5c7af5c723e69ed706339c99f212a1.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwidmann2021calibration,\ntitle={Calibration tests beyond classification},\nauthor={David Widmann and Fredrik Lindsten and Dave Zachariah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-bxf89v3Nx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-bxf89v3Nx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2065/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2065/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2065/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2065/Authors|ICLR.cc/2021/Conference/Paper2065/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2065/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852681, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2065/-/Official_Comment"}}}, {"id": "f366_PbLCS_", "original": null, "number": 7, "cdate": 1605653486677, "ddate": null, "tcdate": 1605653486677, "tmdate": 1605653486677, "tddate": null, "forum": "-bxf89v3Nx", "replyto": "GSZuabszvv", "invitation": "ICLR.cc/2021/Conference/Paper2065/-/Official_Comment", "content": {"title": "Author comments", "comment": "We thank you for your comments and are very happy to hear that you found the paper interesting and well written.\n\n*I honestly do not have much critic to address to this work which seems to have reached a level of maturity perfectly adapted for publication in ICLR. The only damper is that the proposed methodology allows to detect miscalibration not yet to cure it. However, the authors seem to have some ideas on that too as mentioned in their conclusion.*\n\nIn our opinion, it is mandatory to be able to detect miscalibration before curing it, and therefore in this paper we focused on evaluations and tests of calibration within the proposed framework. However, we agree that follow-up questions such as how to obtain calibrated models, possibly using the kernel estimators, is indeed an interesting topic for future work. As discussed in the conclusion, the differentiability of the kernel estimators might allow to incorporate the framework into the training procedure which would also demonstrate its usefulness for practical applications more clearly."}, "signatures": ["ICLR.cc/2021/Conference/Paper2065/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2065/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Calibration tests beyond classification", "authorids": ["~David_Widmann1", "fredrik.lindsten@liu.se", "dave.zachariah@it.uu.se"], "authors": ["David Widmann", "Fredrik Lindsten", "Dave Zachariah"], "keywords": ["calibration", "uncertainty quantification", "framework", "integral probability metric", "maximum mean discrepancy"], "abstract": "Most supervised machine learning tasks are subject to irreducible prediction errors. Probabilistic predictive models address this limitation by providing probability distributions that represent a belief over plausible targets, rather than point estimates. Such models can be a valuable tool in decision-making under uncertainty, provided that the model output is meaningful and interpretable. Calibrated models guarantee that the probabilistic predictions are neither over- nor under-confident. In the machine learning literature, different measures and statistical tests have been proposed and studied for evaluating the calibration of classification models. For regression problems, however, research has been focused on a weaker condition of calibration based on predicted quantiles for real-valued targets. In this paper, we propose the first framework that unifies calibration evaluation and tests for general probabilistic predictive models. It applies to any such model, including classification and regression models of arbitrary dimension. Furthermore, the framework generalizes existing measures and provides a more intuitive reformulation of a recently proposed framework for calibration in multi-class classification. In particular, we reformulate and generalize the kernel calibration error, its estimators, and hypothesis tests using scalar-valued kernels, and evaluate the calibration of real-valued regression\nproblems.", "one-sentence_summary": "Unifying framework for calibration evaluations and tests of probabilistic predictive models", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "widmann|calibration_tests_beyond_classification", "supplementary_material": "", "pdf": "/pdf/2bd555e04e5c7af5c723e69ed706339c99f212a1.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwidmann2021calibration,\ntitle={Calibration tests beyond classification},\nauthor={David Widmann and Fredrik Lindsten and Dave Zachariah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-bxf89v3Nx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-bxf89v3Nx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2065/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2065/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2065/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2065/Authors|ICLR.cc/2021/Conference/Paper2065/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2065/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852681, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2065/-/Official_Comment"}}}, {"id": "ffanilP0v4F", "original": null, "number": 6, "cdate": 1605653087717, "ddate": null, "tcdate": 1605653087717, "tmdate": 1605653087717, "tddate": null, "forum": "-bxf89v3Nx", "replyto": "FHjZ-Fwvi4n", "invitation": "ICLR.cc/2021/Conference/Paper2065/-/Official_Comment", "content": {"title": "Author comments", "comment": "We thank you for your comments and are very happy to hear that you found the paper interesting and well written.\n\n*My main issue comes with the lack of empirical studies. The toy problem is not terribly interesting and does not reveal any particular insight. It leads me to believe that maybe this is not that useful of a method, since the authors did not have anywhere that they could apply it to and derive meaningful insights or uses.*\n\nIn our empirical studies we wanted to focus on two points:\n1. an experimental confirmation of the derived theoretical properties of the kernel-based estimators and hypothesis tests\n2. a demonstration of how the framework can be applied to neural network models and ensembles of neural network models\n\nSynthetic models are required to empirically validate the theoretically expected statistical\nproperties and computational efficiency of the estimators and tests (section 5.1), since the experiments have to be performed with (un)calibrated models.\n\nFor demonstrating the application of our framework we deliberately chose a well-known regression problem from the statistics literature. Moreover, we tried to highlight that the framework can be applied to ensemble models as well.\n\n*The comment in the conclusion about the differentiability of their kernels is interesting and I think incorporating this into the training procedure could potentially show some very clear pragmatic use of this method.*\n\nWe agree that incorporating the framework into the training procedure might demonstrate its usefulness for practical purposes more clearly. However, in our opinion, it is mandatory to be able to detect miscalibration before curing it, and therefore in this paper we focused on evaluations and tests of calibration within the proposed framework. Follow-up questions such as how to obtain calibrated models, possibly using the kernel estimators, is indeed an interesting topic for future work."}, "signatures": ["ICLR.cc/2021/Conference/Paper2065/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2065/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Calibration tests beyond classification", "authorids": ["~David_Widmann1", "fredrik.lindsten@liu.se", "dave.zachariah@it.uu.se"], "authors": ["David Widmann", "Fredrik Lindsten", "Dave Zachariah"], "keywords": ["calibration", "uncertainty quantification", "framework", "integral probability metric", "maximum mean discrepancy"], "abstract": "Most supervised machine learning tasks are subject to irreducible prediction errors. Probabilistic predictive models address this limitation by providing probability distributions that represent a belief over plausible targets, rather than point estimates. Such models can be a valuable tool in decision-making under uncertainty, provided that the model output is meaningful and interpretable. Calibrated models guarantee that the probabilistic predictions are neither over- nor under-confident. In the machine learning literature, different measures and statistical tests have been proposed and studied for evaluating the calibration of classification models. For regression problems, however, research has been focused on a weaker condition of calibration based on predicted quantiles for real-valued targets. In this paper, we propose the first framework that unifies calibration evaluation and tests for general probabilistic predictive models. It applies to any such model, including classification and regression models of arbitrary dimension. Furthermore, the framework generalizes existing measures and provides a more intuitive reformulation of a recently proposed framework for calibration in multi-class classification. In particular, we reformulate and generalize the kernel calibration error, its estimators, and hypothesis tests using scalar-valued kernels, and evaluate the calibration of real-valued regression\nproblems.", "one-sentence_summary": "Unifying framework for calibration evaluations and tests of probabilistic predictive models", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "widmann|calibration_tests_beyond_classification", "supplementary_material": "", "pdf": "/pdf/2bd555e04e5c7af5c723e69ed706339c99f212a1.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwidmann2021calibration,\ntitle={Calibration tests beyond classification},\nauthor={David Widmann and Fredrik Lindsten and Dave Zachariah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-bxf89v3Nx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-bxf89v3Nx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2065/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2065/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2065/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2065/Authors|ICLR.cc/2021/Conference/Paper2065/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2065/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852681, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2065/-/Official_Comment"}}}, {"id": "pL4xg9h7e-m", "original": null, "number": 5, "cdate": 1605224565849, "ddate": null, "tcdate": 1605224565849, "tmdate": 1605224565849, "tddate": null, "forum": "-bxf89v3Nx", "replyto": "yredTQOClAY", "invitation": "ICLR.cc/2021/Conference/Paper2065/-/Official_Comment", "content": {"title": "users and calibration of models", "comment": "So, use of this approach does raise questions, as you say.  I'd guess that regression folks don't use calibration much because they have a hard time at it.  So this paper, possibly, fills a need.  And calibration definitely has critical uses for managing uncertainty and in doing active learning."}, "signatures": ["ICLR.cc/2021/Conference/Paper2065/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2065/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Calibration tests beyond classification", "authorids": ["~David_Widmann1", "fredrik.lindsten@liu.se", "dave.zachariah@it.uu.se"], "authors": ["David Widmann", "Fredrik Lindsten", "Dave Zachariah"], "keywords": ["calibration", "uncertainty quantification", "framework", "integral probability metric", "maximum mean discrepancy"], "abstract": "Most supervised machine learning tasks are subject to irreducible prediction errors. Probabilistic predictive models address this limitation by providing probability distributions that represent a belief over plausible targets, rather than point estimates. Such models can be a valuable tool in decision-making under uncertainty, provided that the model output is meaningful and interpretable. Calibrated models guarantee that the probabilistic predictions are neither over- nor under-confident. In the machine learning literature, different measures and statistical tests have been proposed and studied for evaluating the calibration of classification models. For regression problems, however, research has been focused on a weaker condition of calibration based on predicted quantiles for real-valued targets. In this paper, we propose the first framework that unifies calibration evaluation and tests for general probabilistic predictive models. It applies to any such model, including classification and regression models of arbitrary dimension. Furthermore, the framework generalizes existing measures and provides a more intuitive reformulation of a recently proposed framework for calibration in multi-class classification. In particular, we reformulate and generalize the kernel calibration error, its estimators, and hypothesis tests using scalar-valued kernels, and evaluate the calibration of real-valued regression\nproblems.", "one-sentence_summary": "Unifying framework for calibration evaluations and tests of probabilistic predictive models", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "widmann|calibration_tests_beyond_classification", "supplementary_material": "", "pdf": "/pdf/2bd555e04e5c7af5c723e69ed706339c99f212a1.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwidmann2021calibration,\ntitle={Calibration tests beyond classification},\nauthor={David Widmann and Fredrik Lindsten and Dave Zachariah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-bxf89v3Nx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-bxf89v3Nx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2065/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2065/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2065/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2065/Authors|ICLR.cc/2021/Conference/Paper2065/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2065/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852681, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2065/-/Official_Comment"}}}, {"id": "QEGvhaBQg51", "original": null, "number": 4, "cdate": 1605224391631, "ddate": null, "tcdate": 1605224391631, "tmdate": 1605224391631, "tddate": null, "forum": "-bxf89v3Nx", "replyto": "FHjZ-Fwvi4n", "invitation": "ICLR.cc/2021/Conference/Paper2065/-/Official_Comment", "content": {"title": "on the empirical studies", "comment": "Must admit, I did notice the experimental work was on rather restricted data.  This tends to happen with theoretical work, but I agree, I'd be much happier to see how this works on larger data.  The beauty of this approach is opening up calibration to a broader class of models, but we have to show it works."}, "signatures": ["ICLR.cc/2021/Conference/Paper2065/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2065/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Calibration tests beyond classification", "authorids": ["~David_Widmann1", "fredrik.lindsten@liu.se", "dave.zachariah@it.uu.se"], "authors": ["David Widmann", "Fredrik Lindsten", "Dave Zachariah"], "keywords": ["calibration", "uncertainty quantification", "framework", "integral probability metric", "maximum mean discrepancy"], "abstract": "Most supervised machine learning tasks are subject to irreducible prediction errors. Probabilistic predictive models address this limitation by providing probability distributions that represent a belief over plausible targets, rather than point estimates. Such models can be a valuable tool in decision-making under uncertainty, provided that the model output is meaningful and interpretable. Calibrated models guarantee that the probabilistic predictions are neither over- nor under-confident. In the machine learning literature, different measures and statistical tests have been proposed and studied for evaluating the calibration of classification models. For regression problems, however, research has been focused on a weaker condition of calibration based on predicted quantiles for real-valued targets. In this paper, we propose the first framework that unifies calibration evaluation and tests for general probabilistic predictive models. It applies to any such model, including classification and regression models of arbitrary dimension. Furthermore, the framework generalizes existing measures and provides a more intuitive reformulation of a recently proposed framework for calibration in multi-class classification. In particular, we reformulate and generalize the kernel calibration error, its estimators, and hypothesis tests using scalar-valued kernels, and evaluate the calibration of real-valued regression\nproblems.", "one-sentence_summary": "Unifying framework for calibration evaluations and tests of probabilistic predictive models", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "widmann|calibration_tests_beyond_classification", "supplementary_material": "", "pdf": "/pdf/2bd555e04e5c7af5c723e69ed706339c99f212a1.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwidmann2021calibration,\ntitle={Calibration tests beyond classification},\nauthor={David Widmann and Fredrik Lindsten and Dave Zachariah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-bxf89v3Nx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-bxf89v3Nx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2065/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2065/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2065/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2065/Authors|ICLR.cc/2021/Conference/Paper2065/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2065/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852681, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2065/-/Official_Comment"}}}, {"id": "GSZuabszvv", "original": null, "number": 2, "cdate": 1603802537028, "ddate": null, "tcdate": 1603802537028, "tmdate": 1605024297683, "tddate": null, "forum": "-bxf89v3Nx", "replyto": "-bxf89v3Nx", "invitation": "ICLR.cc/2021/Conference/Paper2065/-/Official_Review", "content": {"title": "A very nice piece of work", "review": "This paper addresses probabilistic data driven model calibration, i.e. aligning predicted target probabilities with actual ones. \n\nThis problem has been extensively studied for classification tasks but solutions for regression tasks have limitations as illustrated by Fig.1. The authors intend to fill this gap and introduce a general kernel-based calibration framework that subsumes other ones previously defined for classification. The contribution of the authors is thus clearly stated and positioned w.r.t. prior arts. \n\nThe authors start by proposing an alternative definition of calibration (Def.2) in order to cast the problem into integral probability metrics. It is this re-definition of the problem that allow them to encompass prior arts as special cases. \nFor a number of practical and theoretical reasons, the authors focus on a special case of this framework which involves the computation of the MMD as a metric. \n\nBased on the MMD literature but also relying on the structure of their problem (where the auxiliary variable can be marginalized out), the authors provide several consistent estimator with known rates in dataset size.\n\nThe validity of the proposed estimates is assessed through convincing numerical experiments that involve a calibration test.\n\nI honestly do not have much critic to address to this work which seems to have reached a level of maturity perfectly adapted for publication in ICLR. The only damper is that the proposed methodology allows to detect miscalibration not yet to cure it. However, the authors seem to have some ideas on that too as mentioned in their conclusion.\n", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2065/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2065/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Calibration tests beyond classification", "authorids": ["~David_Widmann1", "fredrik.lindsten@liu.se", "dave.zachariah@it.uu.se"], "authors": ["David Widmann", "Fredrik Lindsten", "Dave Zachariah"], "keywords": ["calibration", "uncertainty quantification", "framework", "integral probability metric", "maximum mean discrepancy"], "abstract": "Most supervised machine learning tasks are subject to irreducible prediction errors. Probabilistic predictive models address this limitation by providing probability distributions that represent a belief over plausible targets, rather than point estimates. Such models can be a valuable tool in decision-making under uncertainty, provided that the model output is meaningful and interpretable. Calibrated models guarantee that the probabilistic predictions are neither over- nor under-confident. In the machine learning literature, different measures and statistical tests have been proposed and studied for evaluating the calibration of classification models. For regression problems, however, research has been focused on a weaker condition of calibration based on predicted quantiles for real-valued targets. In this paper, we propose the first framework that unifies calibration evaluation and tests for general probabilistic predictive models. It applies to any such model, including classification and regression models of arbitrary dimension. Furthermore, the framework generalizes existing measures and provides a more intuitive reformulation of a recently proposed framework for calibration in multi-class classification. In particular, we reformulate and generalize the kernel calibration error, its estimators, and hypothesis tests using scalar-valued kernels, and evaluate the calibration of real-valued regression\nproblems.", "one-sentence_summary": "Unifying framework for calibration evaluations and tests of probabilistic predictive models", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "widmann|calibration_tests_beyond_classification", "supplementary_material": "", "pdf": "/pdf/2bd555e04e5c7af5c723e69ed706339c99f212a1.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwidmann2021calibration,\ntitle={Calibration tests beyond classification},\nauthor={David Widmann and Fredrik Lindsten and Dave Zachariah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-bxf89v3Nx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "-bxf89v3Nx", "replyto": "-bxf89v3Nx", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2065/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538104855, "tmdate": 1606915797452, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2065/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2065/-/Official_Review"}}}, {"id": "yredTQOClAY", "original": null, "number": 1, "cdate": 1603747314428, "ddate": null, "tcdate": 1603747314428, "tmdate": 1605024297624, "tddate": null, "forum": "-bxf89v3Nx", "replyto": "-bxf89v3Nx", "invitation": "ICLR.cc/2021/Conference/Paper2065/-/Official_Review", "content": {"title": "Experiments not convincing enough", "review": "The authors define (Definition 2) a generalized form of calibration error for any model with probabilistic output and any output space (binary classification, multiclass classification, real-valued regression, ordinal regression, structured prediction, etc.).\n\nThe main novelty of Definition 2 seems to be the introduction of a dummy variable $Z_X$ which is sampled from the predicted distribution $P_X$ over the target space, in order to compute an integral probability metric that compares the joint distribution of $(P_X, Y)$ to the joint distribution of $(P_X, Z_X)$. The authors note that in some cases, $Z_X$ can be integrated out of the definition either analytically or using numerical integration. It is not clear, from my limited background knowledge in the related work, why it should be helpful to introduce a dummy variable and then integrate it out.\n\nAt first reading, Definition 2 seems too general to be useful in practice, as it requires the choice of a space of functions $\\mathcal{F}$. The authors argue (with details in the appendices, which are not provided to me at this time) that it generalizes several previous definitions of calibration error including the maximum mean discrepancy, the total variation distance, the Kantorovich distance, and the Dudley metric.\n\nSection 3 went beyond my area of expertise and beyond my comprehension; I feel unqualified to provide an informed review of this section. I think some reference to kernels or RKHSs should be made in the title or the abstract.\n\nThe experiment in Section 5.2 demonstrates the utility of the proposed SKCE calibration metric, alongside more common metrics like negative log-likelihood (NLL) and mean squared error (MSE). It appears that the SKCE curves (for both training and test data) have a very similar shape to the NLL curves, so it's not clear what benefit SKCE provides above and beyond the more common and easily-computed NLL. I would have liked to see more convincing experimental evidence of the marginal benefit of this approach beyond common calibration metrics.\n\nRegarding the significance of the work, I can add that in practice, I find that relatively few ML users are concerned with the calibration of their models, and these are entirely restricted to problems of classification (almost always binary classification) or quantile regression. The novelty of this work seems to lie mostly in its applicability beyond these types problems: the authors write, \"A key contribution of this paper is a new framework that is applicable to multivariate regression, as well as... discrete ordinal or more complex (e.g., graph-structured) [output],\" so I venture a guess that the intended audience for this work is relatively small.\n\n**Minor comments**\n\nIn equation (2), I believe the RHS should be $\\max P_X$ instead of $\\arg \\max P_X$.\n\nSection 2, paragraph 2, you wrote \"instead of the discrepancy between the conditional distributions $\\mathbb{P}(Y | X)$ and $P_X$.\" Did you mean to write $\\mathbb{P}(Y | P_X)$ instead of $\\mathbb{P}(Y | X)$? That would make more sense to me, since the sentence would refer to comparing the LHS and RHS of Equation (1).\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2065/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2065/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Calibration tests beyond classification", "authorids": ["~David_Widmann1", "fredrik.lindsten@liu.se", "dave.zachariah@it.uu.se"], "authors": ["David Widmann", "Fredrik Lindsten", "Dave Zachariah"], "keywords": ["calibration", "uncertainty quantification", "framework", "integral probability metric", "maximum mean discrepancy"], "abstract": "Most supervised machine learning tasks are subject to irreducible prediction errors. Probabilistic predictive models address this limitation by providing probability distributions that represent a belief over plausible targets, rather than point estimates. Such models can be a valuable tool in decision-making under uncertainty, provided that the model output is meaningful and interpretable. Calibrated models guarantee that the probabilistic predictions are neither over- nor under-confident. In the machine learning literature, different measures and statistical tests have been proposed and studied for evaluating the calibration of classification models. For regression problems, however, research has been focused on a weaker condition of calibration based on predicted quantiles for real-valued targets. In this paper, we propose the first framework that unifies calibration evaluation and tests for general probabilistic predictive models. It applies to any such model, including classification and regression models of arbitrary dimension. Furthermore, the framework generalizes existing measures and provides a more intuitive reformulation of a recently proposed framework for calibration in multi-class classification. In particular, we reformulate and generalize the kernel calibration error, its estimators, and hypothesis tests using scalar-valued kernels, and evaluate the calibration of real-valued regression\nproblems.", "one-sentence_summary": "Unifying framework for calibration evaluations and tests of probabilistic predictive models", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "widmann|calibration_tests_beyond_classification", "supplementary_material": "", "pdf": "/pdf/2bd555e04e5c7af5c723e69ed706339c99f212a1.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwidmann2021calibration,\ntitle={Calibration tests beyond classification},\nauthor={David Widmann and Fredrik Lindsten and Dave Zachariah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-bxf89v3Nx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "-bxf89v3Nx", "replyto": "-bxf89v3Nx", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2065/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538104855, "tmdate": 1606915797452, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2065/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2065/-/Official_Review"}}}, {"id": "FHjZ-Fwvi4n", "original": null, "number": 3, "cdate": 1604268499045, "ddate": null, "tcdate": 1604268499045, "tmdate": 1605024297557, "tddate": null, "forum": "-bxf89v3Nx", "replyto": "-bxf89v3Nx", "invitation": "ICLR.cc/2021/Conference/Paper2065/-/Official_Review", "content": {"title": "Well-written and an elegant idea", "review": "Summary:\nThe authors present an approach for testing calibration in conditional probability estimation models. They build on a line of work in the kernel estimation literature assessing whether the conditional distributions are well calibrated (i.e. P(Y | f(X)) = f(X), where f is some predictive model). They develop an MMD kernel estimator and expand on practical choices of kernels that are computationally tractable. They then derive an asymptotic null distribution for calibrated models, enabling control over the error rate when labeling a model uncalibrated. A few simulation studies are done with neural networks to show the applicability of the method.\n\nReview:\nThis is an excellently written paper. The intro and first few chapters are a joy to read and really explain the problem well. There is a lot of nuance to calibration, so I really appreciated the precision and clarity in the exposition.\n\nThe idea itself also seems quite elegant. Generalizing a previously published kernel approach from only discrete distributions to handle a more general class of problems may seem like a small conceptual step. However, I think the authors did a good job explaining the challenges of this extension. The resulting estimators are now applicable to many more problems than the existing work.\n\nNote I am not an expert in kernel learning, so I have not evaluated the proofs for correctness.\n\nMy main issue comes with the lack of empirical studies. The toy problem is not terribly interesting and does not reveal any particular insight. It leads me to believe that maybe this is not that useful of a method, since the authors did not have anywhere that they could apply it to and derive meaningful insights or uses. The comment in the conclusion about the differentiability of their kernels is interesting and I think incorporating this into the training procedure could potentially show some very clear pragmatic use of this method.\n\nOverall, I like the paper. It is clearly written and presents what I think is an interesting and novel idea. ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2065/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2065/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Calibration tests beyond classification", "authorids": ["~David_Widmann1", "fredrik.lindsten@liu.se", "dave.zachariah@it.uu.se"], "authors": ["David Widmann", "Fredrik Lindsten", "Dave Zachariah"], "keywords": ["calibration", "uncertainty quantification", "framework", "integral probability metric", "maximum mean discrepancy"], "abstract": "Most supervised machine learning tasks are subject to irreducible prediction errors. Probabilistic predictive models address this limitation by providing probability distributions that represent a belief over plausible targets, rather than point estimates. Such models can be a valuable tool in decision-making under uncertainty, provided that the model output is meaningful and interpretable. Calibrated models guarantee that the probabilistic predictions are neither over- nor under-confident. In the machine learning literature, different measures and statistical tests have been proposed and studied for evaluating the calibration of classification models. For regression problems, however, research has been focused on a weaker condition of calibration based on predicted quantiles for real-valued targets. In this paper, we propose the first framework that unifies calibration evaluation and tests for general probabilistic predictive models. It applies to any such model, including classification and regression models of arbitrary dimension. Furthermore, the framework generalizes existing measures and provides a more intuitive reformulation of a recently proposed framework for calibration in multi-class classification. In particular, we reformulate and generalize the kernel calibration error, its estimators, and hypothesis tests using scalar-valued kernels, and evaluate the calibration of real-valued regression\nproblems.", "one-sentence_summary": "Unifying framework for calibration evaluations and tests of probabilistic predictive models", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "widmann|calibration_tests_beyond_classification", "supplementary_material": "", "pdf": "/pdf/2bd555e04e5c7af5c723e69ed706339c99f212a1.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwidmann2021calibration,\ntitle={Calibration tests beyond classification},\nauthor={David Widmann and Fredrik Lindsten and Dave Zachariah},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-bxf89v3Nx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "-bxf89v3Nx", "replyto": "-bxf89v3Nx", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2065/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538104855, "tmdate": 1606915797452, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2065/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2065/-/Official_Review"}}}], "count": 12}