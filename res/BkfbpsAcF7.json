{"notes": [{"id": "-7m5xfM7u2", "original": null, "number": 4, "cdate": 1583545010886, "ddate": null, "tcdate": 1583545010886, "tmdate": 1583545010886, "tddate": null, "forum": "BkfbpsAcF7", "replyto": "BkfbpsAcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper778/Public_Comment", "content": {"comment": "This is a novel method to generate adversarial samples.\nHowever, I could also generate targeted-adversarial attacks such that:\ndifferent inputs to a model, but the same output from the model\n\nI just tried targeted-adversarial attacks on vgg-11 trained on ImageNet\nx1:  an image of an ant (224x224),  logits is z1=vgg11(x1)\nx2:  an image of a bee,   logits is z2=vgg11(x2)\nz1 and z2 are very different\nThen, targeted PGD-attack is applied to x2\nx3 =   x2  + noise, z3=vgg11(x3)\nthe loss function of the attack is   sum(||z3- z1||^2) \nafter many..many iterations of PGD, the loss decreases from ~3000 to 0.06399869173765182\nx3 still looks the same as x2,  L2_norm(x3-x2) is 14.2280\nNow, z3 ~= z1 and predicted class labels are the same, but x3 is a bee and x1 is an ant\ndifferent inputs, the same output", "title": "what is the difference between the attack in the paper and targeted-adversarial attacks?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Excessive Invariance Causes Adversarial Vulnerability", "abstract": "Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from epsilon-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. We show such excessive invariance occurs across various tasks and architecture types. On MNIST and ImageNet one can manipulate the class-specific content of almost any image without changing the hidden activations. We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an information-theoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.", "keywords": ["Generalization", "Adversarial Examples", "Invariance", "Information Theory", "Invertible Networks"], "authorids": ["j.jacobsen@vectorinstitute.ai", "jensb@uni-bremen.de", "zemel@cs.toronto.edu", "matthias.bethge@uni-tuebingen.de"], "authors": ["Joern-Henrik Jacobsen", "Jens Behrmann", "Richard Zemel", "Matthias Bethge"], "TL;DR": "We show deep networks are not only too sensitive to task-irrelevant changes of their input, but also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks.", "pdf": "/pdf/bcabe02aeedd350f06c997ab0013889b5e624155.pdf", "paperhash": "jacobsen|excessive_invariance_causes_adversarial_vulnerability", "_bibtex": "@inproceedings{\njacobsen2018excessive,\ntitle={Excessive Invariance Causes Adversarial Vulnerability},\nauthor={Joern-Henrik Jacobsen and Jens Behrmann and Richard Zemel and Matthias Bethge},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfbpsAcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper778/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311754909, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "BkfbpsAcF7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311754909}}}, {"id": "Sklf4aO_jV", "original": null, "number": 13, "cdate": 1556806953970, "ddate": null, "tcdate": 1556806953970, "tmdate": 1556806953970, "tddate": null, "forum": "BkfbpsAcF7", "replyto": "HJe5elx8jE", "invitation": "ICLR.cc/2019/Conference/-/Paper778/Official_Comment", "content": {"title": "Response", "comment": "Dear Ryota,\n\nThank you very much for raising this point.\n\nYou are right, we have made a mistake in the proof. We have fixed this in the latest revision. Further, we have refined the definition of the adversarial distribution shift to exclude synergetic effects in the interaction information I(y;z_s;z_n). This assumption is also in line with our shiftMNIST experiments where the interaction information between newly introduced features, original digits and labels is >= 0.\n\nWe have acknowledged you for pointing this out!\n\nBest,\nJ\u00f6rn"}, "signatures": ["ICLR.cc/2019/Conference/Paper778/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper778/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Excessive Invariance Causes Adversarial Vulnerability", "abstract": "Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from epsilon-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. We show such excessive invariance occurs across various tasks and architecture types. On MNIST and ImageNet one can manipulate the class-specific content of almost any image without changing the hidden activations. We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an information-theoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.", "keywords": ["Generalization", "Adversarial Examples", "Invariance", "Information Theory", "Invertible Networks"], "authorids": ["j.jacobsen@vectorinstitute.ai", "jensb@uni-bremen.de", "zemel@cs.toronto.edu", "matthias.bethge@uni-tuebingen.de"], "authors": ["Joern-Henrik Jacobsen", "Jens Behrmann", "Richard Zemel", "Matthias Bethge"], "TL;DR": "We show deep networks are not only too sensitive to task-irrelevant changes of their input, but also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks.", "pdf": "/pdf/bcabe02aeedd350f06c997ab0013889b5e624155.pdf", "paperhash": "jacobsen|excessive_invariance_causes_adversarial_vulnerability", "_bibtex": "@inproceedings{\njacobsen2018excessive,\ntitle={Excessive Invariance Causes Adversarial Vulnerability},\nauthor={Joern-Henrik Jacobsen and Jens Behrmann and Richard Zemel and Matthias Bethge},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfbpsAcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper778/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621621315, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkfbpsAcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper778/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper778/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper778/Authors|ICLR.cc/2019/Conference/Paper778/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621621315}}}, {"id": "BkfbpsAcF7", "original": "SJgw_Fh5Y7", "number": 778, "cdate": 1538087865314, "ddate": null, "tcdate": 1538087865314, "tmdate": 1556805356901, "tddate": null, "forum": "BkfbpsAcF7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Excessive Invariance Causes Adversarial Vulnerability", "abstract": "Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from epsilon-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. We show such excessive invariance occurs across various tasks and architecture types. On MNIST and ImageNet one can manipulate the class-specific content of almost any image without changing the hidden activations. We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an information-theoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.", "keywords": ["Generalization", "Adversarial Examples", "Invariance", "Information Theory", "Invertible Networks"], "authorids": ["j.jacobsen@vectorinstitute.ai", "jensb@uni-bremen.de", "zemel@cs.toronto.edu", "matthias.bethge@uni-tuebingen.de"], "authors": ["Joern-Henrik Jacobsen", "Jens Behrmann", "Richard Zemel", "Matthias Bethge"], "TL;DR": "We show deep networks are not only too sensitive to task-irrelevant changes of their input, but also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks.", "pdf": "/pdf/bcabe02aeedd350f06c997ab0013889b5e624155.pdf", "paperhash": "jacobsen|excessive_invariance_causes_adversarial_vulnerability", "_bibtex": "@inproceedings{\njacobsen2018excessive,\ntitle={Excessive Invariance Causes Adversarial Vulnerability},\nauthor={Joern-Henrik Jacobsen and Jens Behrmann and Richard Zemel and Matthias Bethge},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfbpsAcF7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 18, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HJe5elx8jE", "original": null, "number": 3, "cdate": 1556639730034, "ddate": null, "tcdate": 1556639730034, "tmdate": 1556639730034, "tddate": null, "forum": "BkfbpsAcF7", "replyto": "BkfbpsAcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper778/Public_Comment", "content": {"comment": "There might be an error in the proof of Theorem 6. Could the authors clarify what \"the properties of conditional mutual information under independence I(y; z_n) =0\" means?\n\nTo me, it looks like (I might be wrong) the authors are saying I(z_s; y |z_n) = I(z_s; y) if I(y; z_n) = 0 (independence). But I think this is wrong as in the following example:\n\nP(z_n = 0) = P(z_n = 1) = 1/2\nP(z_s = 0) = P(z_s = 1) = 1/2\n\ny = z_s if z_n = 1 else 1 - z_s\n\nThen\nI(z_s; y) = I(z_n; y) = 0\nbut\nI(z_s; y| z_n) = 1\n\n\n", "title": "Theorem 6"}, "signatures": ["~Ryota_Tomioka1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Ryota_Tomioka1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Excessive Invariance Causes Adversarial Vulnerability", "abstract": "Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from epsilon-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. We show such excessive invariance occurs across various tasks and architecture types. On MNIST and ImageNet one can manipulate the class-specific content of almost any image without changing the hidden activations. We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an information-theoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.", "keywords": ["Generalization", "Adversarial Examples", "Invariance", "Information Theory", "Invertible Networks"], "authorids": ["j.jacobsen@vectorinstitute.ai", "jensb@uni-bremen.de", "zemel@cs.toronto.edu", "matthias.bethge@uni-tuebingen.de"], "authors": ["Joern-Henrik Jacobsen", "Jens Behrmann", "Richard Zemel", "Matthias Bethge"], "TL;DR": "We show deep networks are not only too sensitive to task-irrelevant changes of their input, but also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks.", "pdf": "/pdf/bcabe02aeedd350f06c997ab0013889b5e624155.pdf", "paperhash": "jacobsen|excessive_invariance_causes_adversarial_vulnerability", "_bibtex": "@inproceedings{\njacobsen2018excessive,\ntitle={Excessive Invariance Causes Adversarial Vulnerability},\nauthor={Joern-Henrik Jacobsen and Jens Behrmann and Richard Zemel and Matthias Bethge},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfbpsAcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper778/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311754909, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "BkfbpsAcF7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311754909}}}, {"id": "Byx6IYubX4", "original": null, "number": 2, "cdate": 1547958613265, "ddate": null, "tcdate": 1547958613265, "tmdate": 1547958872007, "tddate": null, "forum": "BkfbpsAcF7", "replyto": "BylSu67bQV", "invitation": "ICLR.cc/2019/Conference/-/Paper778/Public_Comment", "content": {"comment": "How much does iRevNet differ from fiRevNet?\nFrom my understanding, the logits (N-classes output) from DCT-II output is optimized with class labels, and the remaining output is optionally optimized with the proposed loss function.\nSorry if I missed something.", "title": "isn't fiRevNet is iRevNet with average pooling swapped with DCT-II as (spectral) invertible pooling ?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Excessive Invariance Causes Adversarial Vulnerability", "abstract": "Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from epsilon-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. We show such excessive invariance occurs across various tasks and architecture types. On MNIST and ImageNet one can manipulate the class-specific content of almost any image without changing the hidden activations. We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an information-theoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.", "keywords": ["Generalization", "Adversarial Examples", "Invariance", "Information Theory", "Invertible Networks"], "authorids": ["j.jacobsen@vectorinstitute.ai", "jensb@uni-bremen.de", "zemel@cs.toronto.edu", "matthias.bethge@uni-tuebingen.de"], "authors": ["Joern-Henrik Jacobsen", "Jens Behrmann", "Richard Zemel", "Matthias Bethge"], "TL;DR": "We show deep networks are not only too sensitive to task-irrelevant changes of their input, but also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks.", "pdf": "/pdf/bcabe02aeedd350f06c997ab0013889b5e624155.pdf", "paperhash": "jacobsen|excessive_invariance_causes_adversarial_vulnerability", "_bibtex": "@inproceedings{\njacobsen2018excessive,\ntitle={Excessive Invariance Causes Adversarial Vulnerability},\nauthor={Joern-Henrik Jacobsen and Jens Behrmann and Richard Zemel and Matthias Bethge},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfbpsAcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper778/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311754909, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "BkfbpsAcF7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311754909}}}, {"id": "BylSu67bQV", "original": null, "number": 12, "cdate": 1547939181038, "ddate": null, "tcdate": 1547939181038, "tmdate": 1547939297448, "tddate": null, "forum": "BkfbpsAcF7", "replyto": "BylmnLXZm4", "invitation": "ICLR.cc/2019/Conference/-/Paper778/Official_Comment", "content": {"title": "Not related to our submission", "comment": "1) The codebase you are referring to is not related to this paper.\n\n2) The paper presented here makes no claims about Cifar10. Our focus is on Imagenet, which is a much more challenging problem. MNIST is used to illustrate our proposed solution clearly.\n\n3) The Imagenet model we trained works just as well without invertible DCT pooling. We simply found DCT pooling to make our fi-RevNet conceptually closer to standard ResNets that apply a final global average pooling step, but this is a matter of taste.\n\n4) iRevNets differ from fiRevNets, we describe this in the paper, so you should not be surprised if they perform differently as well.\n\nIt is unfortunate that DCT pooling does not work for you on Cifar10, but neither did we make any claims about it, nor did we experiment with it or provide any implementation of DCT-pooled fiRevNets on Cifar10. However, feel free to drop me an email and I'll try to do what I can to help you.\n\n- J\u00f6rn"}, "signatures": ["ICLR.cc/2019/Conference/Paper778/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper778/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Excessive Invariance Causes Adversarial Vulnerability", "abstract": "Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from epsilon-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. We show such excessive invariance occurs across various tasks and architecture types. On MNIST and ImageNet one can manipulate the class-specific content of almost any image without changing the hidden activations. We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an information-theoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.", "keywords": ["Generalization", "Adversarial Examples", "Invariance", "Information Theory", "Invertible Networks"], "authorids": ["j.jacobsen@vectorinstitute.ai", "jensb@uni-bremen.de", "zemel@cs.toronto.edu", "matthias.bethge@uni-tuebingen.de"], "authors": ["Joern-Henrik Jacobsen", "Jens Behrmann", "Richard Zemel", "Matthias Bethge"], "TL;DR": "We show deep networks are not only too sensitive to task-irrelevant changes of their input, but also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks.", "pdf": "/pdf/bcabe02aeedd350f06c997ab0013889b5e624155.pdf", "paperhash": "jacobsen|excessive_invariance_causes_adversarial_vulnerability", "_bibtex": "@inproceedings{\njacobsen2018excessive,\ntitle={Excessive Invariance Causes Adversarial Vulnerability},\nauthor={Joern-Henrik Jacobsen and Jens Behrmann and Richard Zemel and Matthias Bethge},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfbpsAcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper778/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621621315, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkfbpsAcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper778/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper778/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper778/Authors|ICLR.cc/2019/Conference/Paper778/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621621315}}}, {"id": "BylmnLXZm4", "original": null, "number": 1, "cdate": 1547937450584, "ddate": null, "tcdate": 1547937450584, "tmdate": 1547937592026, "tddate": null, "forum": "BkfbpsAcF7", "replyto": "BkfbpsAcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper778/Public_Comment", "content": {"comment": "The big jump from MNIST then Imagnet is intriguing.\nI tried training with CIFAR-10 using author's open source i-RevNet code, modified the original code to do pooling with DCT II, \n\nand the result is, accuracy is so bad, because everything must fit in 10-class logits.\n\nImagenet has 1000-class where the logits is much informative.\n\nThe author should clarify the limitation of DCT as invertible pooling", "title": "Why no CIFAR-10 experiment"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Excessive Invariance Causes Adversarial Vulnerability", "abstract": "Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from epsilon-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. We show such excessive invariance occurs across various tasks and architecture types. On MNIST and ImageNet one can manipulate the class-specific content of almost any image without changing the hidden activations. We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an information-theoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.", "keywords": ["Generalization", "Adversarial Examples", "Invariance", "Information Theory", "Invertible Networks"], "authorids": ["j.jacobsen@vectorinstitute.ai", "jensb@uni-bremen.de", "zemel@cs.toronto.edu", "matthias.bethge@uni-tuebingen.de"], "authors": ["Joern-Henrik Jacobsen", "Jens Behrmann", "Richard Zemel", "Matthias Bethge"], "TL;DR": "We show deep networks are not only too sensitive to task-irrelevant changes of their input, but also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks.", "pdf": "/pdf/bcabe02aeedd350f06c997ab0013889b5e624155.pdf", "paperhash": "jacobsen|excessive_invariance_causes_adversarial_vulnerability", "_bibtex": "@inproceedings{\njacobsen2018excessive,\ntitle={Excessive Invariance Causes Adversarial Vulnerability},\nauthor={Joern-Henrik Jacobsen and Jens Behrmann and Richard Zemel and Matthias Bethge},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfbpsAcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper778/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311754909, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "BkfbpsAcF7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311754909}}}, {"id": "Syel9-Y7gV", "original": null, "number": 1, "cdate": 1544946055820, "ddate": null, "tcdate": 1544946055820, "tmdate": 1545354490646, "tddate": null, "forum": "BkfbpsAcF7", "replyto": "BkfbpsAcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper778/Meta_Review", "content": {"metareview": "This paper studies the roots of the existence of adversarial perspective from a new perspective. This perspective is quite interesting and thought-provoking. However, some of the contributions rely on fairly restrictive assumptions and/or are not properly evaluated. \n\nStill, overall, this paper should be a valuable addition to the program. ", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "An interesting angle with some issues in terms of execution"}, "signatures": ["ICLR.cc/2019/Conference/Paper778/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper778/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Excessive Invariance Causes Adversarial Vulnerability", "abstract": "Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from epsilon-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. We show such excessive invariance occurs across various tasks and architecture types. On MNIST and ImageNet one can manipulate the class-specific content of almost any image without changing the hidden activations. We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an information-theoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.", "keywords": ["Generalization", "Adversarial Examples", "Invariance", "Information Theory", "Invertible Networks"], "authorids": ["j.jacobsen@vectorinstitute.ai", "jensb@uni-bremen.de", "zemel@cs.toronto.edu", "matthias.bethge@uni-tuebingen.de"], "authors": ["Joern-Henrik Jacobsen", "Jens Behrmann", "Richard Zemel", "Matthias Bethge"], "TL;DR": "We show deep networks are not only too sensitive to task-irrelevant changes of their input, but also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks.", "pdf": "/pdf/bcabe02aeedd350f06c997ab0013889b5e624155.pdf", "paperhash": "jacobsen|excessive_invariance_causes_adversarial_vulnerability", "_bibtex": "@inproceedings{\njacobsen2018excessive,\ntitle={Excessive Invariance Causes Adversarial Vulnerability},\nauthor={Joern-Henrik Jacobsen and Jens Behrmann and Richard Zemel and Matthias Bethge},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfbpsAcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper778/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353091247, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkfbpsAcF7", "replyto": "BkfbpsAcF7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper778/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper778/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper778/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353091247}}}, {"id": "rkl8B7OVJ4", "original": null, "number": 11, "cdate": 1543959357763, "ddate": null, "tcdate": 1543959357763, "tmdate": 1543959357763, "tddate": null, "forum": "BkfbpsAcF7", "replyto": "ByeDB22aRQ", "invitation": "ICLR.cc/2019/Conference/-/Paper778/Official_Comment", "content": {"title": "Thank you for the discussion!", "comment": "We were glad to see your positive feedback.\n\nIndeed we agree some open questions (summarized below in point (II)) remain. Yet, we hope that our efforts to prove the underlying principles of our objective sparks future analysis how/when our optimality assumptions (discussed below in point (I)) can be achieved and why the objective succeeds in our current setting.\nThat being said, as pointed out above, the objective function itself is one out of 4 major contributions and therefore this analysis would be out of scope for the presented work.\n\nThank you once again for the constructive discussion!\n\n------------------------------\n(I) Optimality assumptions:\n- Lemma 8 (i) (Appendix A): CE- and MLE-term is Maximum Likelihood under a factorized prior p(z_s, z_n) = p(z_s) p(z_n). In the optimum, it thus holds I(z_s; z_n) = 0 as I(z_s; z_n) = KL(p(z_s, z_n) || p(z_s) p(z_n)).\nFurthermore, in the optimum I(y; z_s) = H(y) = const.\n- Lemma 8 (ii) and (iii) (Appendix A): If the lower bound is tight (nuisance classifier can decode all information about y in z_n), we minimize I(y; z_n) provably.\n------------------------------\n(II) Achieving optimality / Possible alternatives:\n- Connecting independence of z_n and z_s with the model architecture: Due to information preservation, bijective networks are particularly suitable for our task, but other architectures could be considered.\n- Tightness of lower bounds: How tight are lower bounds given by a nuisance classifier or alternative lower bounds by the MINE estimator (Belghazi et al. 2018)?\n- Lack of alternatives: As I(y; z_n) is bounded (Remark 9, Appendix A.2), non-trivial (smaller than H(y)) upper bounds on I(y; z_n) are difficult and to the best of our knowledge, we are not aware of any."}, "signatures": ["ICLR.cc/2019/Conference/Paper778/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper778/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Excessive Invariance Causes Adversarial Vulnerability", "abstract": "Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from epsilon-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. We show such excessive invariance occurs across various tasks and architecture types. On MNIST and ImageNet one can manipulate the class-specific content of almost any image without changing the hidden activations. We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an information-theoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.", "keywords": ["Generalization", "Adversarial Examples", "Invariance", "Information Theory", "Invertible Networks"], "authorids": ["j.jacobsen@vectorinstitute.ai", "jensb@uni-bremen.de", "zemel@cs.toronto.edu", "matthias.bethge@uni-tuebingen.de"], "authors": ["Joern-Henrik Jacobsen", "Jens Behrmann", "Richard Zemel", "Matthias Bethge"], "TL;DR": "We show deep networks are not only too sensitive to task-irrelevant changes of their input, but also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks.", "pdf": "/pdf/bcabe02aeedd350f06c997ab0013889b5e624155.pdf", "paperhash": "jacobsen|excessive_invariance_causes_adversarial_vulnerability", "_bibtex": "@inproceedings{\njacobsen2018excessive,\ntitle={Excessive Invariance Causes Adversarial Vulnerability},\nauthor={Joern-Henrik Jacobsen and Jens Behrmann and Richard Zemel and Matthias Bethge},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfbpsAcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper778/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621621315, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkfbpsAcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper778/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper778/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper778/Authors|ICLR.cc/2019/Conference/Paper778/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621621315}}}, {"id": "HklfNANAh7", "original": null, "number": 2, "cdate": 1541455402205, "ddate": null, "tcdate": 1541455402205, "tmdate": 1543519429438, "tddate": null, "forum": "BkfbpsAcF7", "replyto": "BkfbpsAcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper778/Official_Review", "content": {"title": "An interesting problem with an unconvincing solution", "review": "This paper studies a new perspective on why adversarial examples exist in machine learning -- instead of seeing adversarial examples as the result of a classifier being sensitive to changes in irrelevant information (aka nuisance), the authors see them as the result of a classifier being invariant to changes in relevant (aka semantic) information. They show how to efficiently find such adversarial examples in bijective networks. Moreover, they propose to modify the training objective so that the bijective networks could be more robust to such attacks.\n\nPros:\n -- clarity is good (except for a few places, e.g. no definition of F(x)_i in Definition 1; Page 6 \"three ways forward\" item 3: I(y;z_n|z_s) = I(y;z_s) should be I(y;z_n|z_s) = I(y;z_n).)\n -- the idea is original to the best of my knowledge\n -- the mathematical motivation is sound\n -- Figure 6 seems to show that the proposed defense works on MNIST (However, would you provide more details on how you interpolated z_n? Moreover, what do the images generated with z_s from one input and z_n from another input look like (in your method)?)\n\nCons:\n -- scope: as all the presented problems and solutions assume bijective mapping, I wonder how is it relevant to the traditional perspective of adversarial attack and defense? It seems to me that the contribution of this paper is identifying a problem of bijective networks and then proposing a solution, thus its significance is restricted.\n -- method: while the mathematical motivation is sound, I'm not sure if the proposed training objective can achieve that goal. To elaborate, I see problems with both terms added in the proposed loss function:\n (a.) for the objective of maximizing the cross entropy of the nuisance classifier, it is possible that I(y;z_n) is not reduced, but rather the information about y is encoded in a way that the nuisance classifier is not able to decode, similar to what happens in a one-way function (for example, see https://en.wikipedia.org/wiki/Cryptographic_hash_function ). In the MNIST experiments, the nuisance classifier is a three-layer MLP, which may be too weak and susceptible to information concealing.\n (b.) for the objective of maximizing the likelihood of a factorized model of p(z_s, z_n), I don't see how optimizing it would reduce I(z_s; z_n). In general, even if z_s and z_n are strongly correlated, one can still fit such a factorized model. This only ensures that I(Z_s; Z_n) = 0 for Z_s, Z_n *sampled from the model*, but does not necessarily reduce I(z_s; z_n) for z_s, z_n *used to train the model*. The discrepancy between p(Z_s, Z_n) and p(z_s, z_n) could be huge, in which case one has the model misspecification problem which is another topic.\n (c.) a side question: why is the MLE objective using likelihood rather than log likelihood? Since the two cross entropy losses are similar to log likelihood, I feel there is a mismatch here.\n\n----------------------------------------\nAFTER REBUTTAL:\n\nThanks for your reply to my comments. The new revision has improved clarity and provided new supporting evidences. I would like to raise my rating to 6.\n\nThat being said, (as you agreed) the link from the conceptual goal to the proposed objective has mostly empirical support. Therefore I hope it may encourage future investigation on when and why the proposed objective is successful in achieving the conceptual goal.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper778/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Excessive Invariance Causes Adversarial Vulnerability", "abstract": "Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from epsilon-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. We show such excessive invariance occurs across various tasks and architecture types. On MNIST and ImageNet one can manipulate the class-specific content of almost any image without changing the hidden activations. We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an information-theoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.", "keywords": ["Generalization", "Adversarial Examples", "Invariance", "Information Theory", "Invertible Networks"], "authorids": ["j.jacobsen@vectorinstitute.ai", "jensb@uni-bremen.de", "zemel@cs.toronto.edu", "matthias.bethge@uni-tuebingen.de"], "authors": ["Joern-Henrik Jacobsen", "Jens Behrmann", "Richard Zemel", "Matthias Bethge"], "TL;DR": "We show deep networks are not only too sensitive to task-irrelevant changes of their input, but also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks.", "pdf": "/pdf/bcabe02aeedd350f06c997ab0013889b5e624155.pdf", "paperhash": "jacobsen|excessive_invariance_causes_adversarial_vulnerability", "_bibtex": "@inproceedings{\njacobsen2018excessive,\ntitle={Excessive Invariance Causes Adversarial Vulnerability},\nauthor={Joern-Henrik Jacobsen and Jens Behrmann and Richard Zemel and Matthias Bethge},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfbpsAcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper778/Official_Review", "cdate": 1542234379278, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkfbpsAcF7", "replyto": "BkfbpsAcF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper778/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335800348, "tmdate": 1552335800348, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper778/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ByeDB22aRQ", "original": null, "number": 10, "cdate": 1543519294605, "ddate": null, "tcdate": 1543519294605, "tmdate": 1543519294605, "tddate": null, "forum": "BkfbpsAcF7", "replyto": "Bkeye2iT0X", "invitation": "ICLR.cc/2019/Conference/-/Paper778/Official_Comment", "content": {"title": "Thanks for the rebuttal and revision. I would like to raise my score.", "comment": "Dear Authors,\n\nThanks for your reply to my comments. The new revision has improved clarity and provided new supporting evidences.\n\nThat being said, (as you agreed) the link from the conceptual goal to the proposed objective has mostly empirical support. Therefore I hope it may encourage future investigation on when and why the proposed objective is successful in achieving the conceptual goal.\n\nBest,"}, "signatures": ["ICLR.cc/2019/Conference/Paper778/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper778/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper778/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Excessive Invariance Causes Adversarial Vulnerability", "abstract": "Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from epsilon-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. We show such excessive invariance occurs across various tasks and architecture types. On MNIST and ImageNet one can manipulate the class-specific content of almost any image without changing the hidden activations. We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an information-theoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.", "keywords": ["Generalization", "Adversarial Examples", "Invariance", "Information Theory", "Invertible Networks"], "authorids": ["j.jacobsen@vectorinstitute.ai", "jensb@uni-bremen.de", "zemel@cs.toronto.edu", "matthias.bethge@uni-tuebingen.de"], "authors": ["Joern-Henrik Jacobsen", "Jens Behrmann", "Richard Zemel", "Matthias Bethge"], "TL;DR": "We show deep networks are not only too sensitive to task-irrelevant changes of their input, but also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks.", "pdf": "/pdf/bcabe02aeedd350f06c997ab0013889b5e624155.pdf", "paperhash": "jacobsen|excessive_invariance_causes_adversarial_vulnerability", "_bibtex": "@inproceedings{\njacobsen2018excessive,\ntitle={Excessive Invariance Causes Adversarial Vulnerability},\nauthor={Joern-Henrik Jacobsen and Jens Behrmann and Richard Zemel and Matthias Bethge},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfbpsAcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper778/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621621315, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkfbpsAcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper778/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper778/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper778/Authors|ICLR.cc/2019/Conference/Paper778/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621621315}}}, {"id": "Bkeye2iT0X", "original": null, "number": 9, "cdate": 1543515111023, "ddate": null, "tcdate": 1543515111023, "tmdate": 1543515111023, "tddate": null, "forum": "BkfbpsAcF7", "replyto": "B1xjee27aX", "invitation": "ICLR.cc/2019/Conference/-/Paper778/Official_Comment", "content": {"title": "Further concerns after revision?", "comment": "Dear Reviewer2,\n\nwe would be most grateful if you can let us know if there are any further concerns you have after considering the thoroughly revised manuscript, added experiments and answers above."}, "signatures": ["ICLR.cc/2019/Conference/Paper778/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper778/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Excessive Invariance Causes Adversarial Vulnerability", "abstract": "Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from epsilon-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. We show such excessive invariance occurs across various tasks and architecture types. On MNIST and ImageNet one can manipulate the class-specific content of almost any image without changing the hidden activations. We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an information-theoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.", "keywords": ["Generalization", "Adversarial Examples", "Invariance", "Information Theory", "Invertible Networks"], "authorids": ["j.jacobsen@vectorinstitute.ai", "jensb@uni-bremen.de", "zemel@cs.toronto.edu", "matthias.bethge@uni-tuebingen.de"], "authors": ["Joern-Henrik Jacobsen", "Jens Behrmann", "Richard Zemel", "Matthias Bethge"], "TL;DR": "We show deep networks are not only too sensitive to task-irrelevant changes of their input, but also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks.", "pdf": "/pdf/bcabe02aeedd350f06c997ab0013889b5e624155.pdf", "paperhash": "jacobsen|excessive_invariance_causes_adversarial_vulnerability", "_bibtex": "@inproceedings{\njacobsen2018excessive,\ntitle={Excessive Invariance Causes Adversarial Vulnerability},\nauthor={Joern-Henrik Jacobsen and Jens Behrmann and Richard Zemel and Matthias Bethge},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfbpsAcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper778/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621621315, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkfbpsAcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper778/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper778/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper778/Authors|ICLR.cc/2019/Conference/Paper778/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621621315}}}, {"id": "r1e1SFU_2m", "original": null, "number": 1, "cdate": 1541069111255, "ddate": null, "tcdate": 1541069111255, "tmdate": 1542649682104, "tddate": null, "forum": "BkfbpsAcF7", "replyto": "BkfbpsAcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper778/Official_Review", "content": {"title": "The ideas are appealing and should enventually lead to fine contributions but the paper is disbalanced with wrong detail distribution ", "review": "The paper focuses on adversarial vulnerability of neural networks, and more specifically on perturbation-based versus invariance-based adversarial examples and how using bijective networks (with so-called metameric sampling) may help overcoming issues related to invariance. The approach is used to get around insufficiencies of cross-entropy-based information-maximization, as illustrated on experiments where the proposed variation on CE outperforms CE. \n\nWhile I am not a neural network expert, I felt that the ideas developed in the paper are worthwhile and should eventally lead to useful contributions and be published. This being said, I did not find the paper in its present form to be fit for publication in a high-tier conference or journal. The main reason for this is the disbalance between the somehow heavy and overly commented first four pages (especially in Section 2) contrasting with the surprisingly moderate level of detail when it comes to bijective networks, supposedly the heart of the actual original contribution. To me this is severely affecting the overall quality of the paper. The contents of sections 3 and 4 seem relevant, but I struggled find out what precisely is the main contribution in the end, probably because of the lack of detail on bijective networks mentioned before. Again, I am not an expert, and I will indicate that in the system of course, but while I cannot completely judge all aspects of the technical relevance and the originality of the approach, I am fairly convinced that the paper deserves to be substantially revised before it can be accepted for publication.   \n\nEdit: After paper additions I am changing my score to a 6. ", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper778/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Excessive Invariance Causes Adversarial Vulnerability", "abstract": "Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from epsilon-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. We show such excessive invariance occurs across various tasks and architecture types. On MNIST and ImageNet one can manipulate the class-specific content of almost any image without changing the hidden activations. We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an information-theoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.", "keywords": ["Generalization", "Adversarial Examples", "Invariance", "Information Theory", "Invertible Networks"], "authorids": ["j.jacobsen@vectorinstitute.ai", "jensb@uni-bremen.de", "zemel@cs.toronto.edu", "matthias.bethge@uni-tuebingen.de"], "authors": ["Joern-Henrik Jacobsen", "Jens Behrmann", "Richard Zemel", "Matthias Bethge"], "TL;DR": "We show deep networks are not only too sensitive to task-irrelevant changes of their input, but also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks.", "pdf": "/pdf/bcabe02aeedd350f06c997ab0013889b5e624155.pdf", "paperhash": "jacobsen|excessive_invariance_causes_adversarial_vulnerability", "_bibtex": "@inproceedings{\njacobsen2018excessive,\ntitle={Excessive Invariance Causes Adversarial Vulnerability},\nauthor={Joern-Henrik Jacobsen and Jens Behrmann and Richard Zemel and Matthias Bethge},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfbpsAcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper778/Official_Review", "cdate": 1542234379278, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkfbpsAcF7", "replyto": "BkfbpsAcF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper778/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335800348, "tmdate": 1552335800348, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper778/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SyeLf8916Q", "original": null, "number": 3, "cdate": 1541543437960, "ddate": null, "tcdate": 1541543437960, "tmdate": 1542590873206, "tddate": null, "forum": "BkfbpsAcF7", "replyto": "BkfbpsAcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper778/Official_Review", "content": {"title": "Very interesting ideas, could use a few additional experiments to be more convincing", "review": "This paper explores adversarial examples by investigating an invertible neural network. They begin by first correctly pointing out limitations with the commonly adopted \"l_p adversarial example\" definition in literature. The main idea involves looking at the preimage of different embeddings in the final layer of an invertible neural network. By training a classifier on top of the final embedding of the invertible network the authors are able to partition the final embedding into a set of \"semantic variables\", which are the components used for classification of the classifier, and a set of \"nuisance variables\" which are the complement of the logit variables. This partition allows the authors to define entire subspaces of adversarial images by holding the logit variables fixed and varying the nuisance variables, and applying the inverse to these modified embeddings. The authors are able to find many incorrectly classified images with this inversion technique. The authors then define a new loss which minimizes the mutual information between the nuisance variables and the predicted label. \n\nI found the ideas in this paper quite interesting and novel. Starting with the toy problem of adversarial spheres is great, and it's convincing that the inversion technique can be used to find errors on this dataset even when the classification accuracy is (empirically) 100%. The resulting adversarial images generated by applying their technique are also quite interesting, and this is a cool interesting way to study the robustness of networks in non-iid settings.\n\nThe main weakness is on the evaluation of their proposed new training objective, and I have a few suggestions as to how to strengthen this evaluation. It would be very convincing to me if the authors could show that their new training objective increases robustness to distributional shift. A potential benchmark for distributional shift could be https://arxiv.org/abs/1807.01697 (or just picking a subset of these image corruptions). If the proposed objective shows improvement on this benchmark (or a related one) then this would be a solid contribution.\n\nOne question I have for the authors is how typical the behavior in Figure 4 is? For any fixing of the logits, are all/most metameric samples classifiable by a human oracle? That is do you ever get garbage images from this sampling process. Adding a collection of random samples to the Appendix to demonstrate typical behavior could help demonstrate this.\n\nEdit: After paper additions I am changing my score to a 7. ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper778/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Excessive Invariance Causes Adversarial Vulnerability", "abstract": "Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from epsilon-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. We show such excessive invariance occurs across various tasks and architecture types. On MNIST and ImageNet one can manipulate the class-specific content of almost any image without changing the hidden activations. We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an information-theoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.", "keywords": ["Generalization", "Adversarial Examples", "Invariance", "Information Theory", "Invertible Networks"], "authorids": ["j.jacobsen@vectorinstitute.ai", "jensb@uni-bremen.de", "zemel@cs.toronto.edu", "matthias.bethge@uni-tuebingen.de"], "authors": ["Joern-Henrik Jacobsen", "Jens Behrmann", "Richard Zemel", "Matthias Bethge"], "TL;DR": "We show deep networks are not only too sensitive to task-irrelevant changes of their input, but also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks.", "pdf": "/pdf/bcabe02aeedd350f06c997ab0013889b5e624155.pdf", "paperhash": "jacobsen|excessive_invariance_causes_adversarial_vulnerability", "_bibtex": "@inproceedings{\njacobsen2018excessive,\ntitle={Excessive Invariance Causes Adversarial Vulnerability},\nauthor={Joern-Henrik Jacobsen and Jens Behrmann and Richard Zemel and Matthias Bethge},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfbpsAcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper778/Official_Review", "cdate": 1542234379278, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkfbpsAcF7", "replyto": "BkfbpsAcF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper778/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335800348, "tmdate": 1552335800348, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper778/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Byx23FoQaQ", "original": null, "number": 2, "cdate": 1541810612333, "ddate": null, "tcdate": 1541810612333, "tmdate": 1541812642778, "tddate": null, "forum": "BkfbpsAcF7", "replyto": "BkfbpsAcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper778/Official_Comment", "content": {"title": "Revision uploaded", "comment": "Dear Reviewers, we thank you very much for helping us to substantially improve the manuscript.\n\nWe have addressed all raised concerns either with additional experiments and results, with additional discussions in the manuscript or through other aspects of our revision.\n\nWe were delighted to see the positive reaction by all reviewers to our developed ideas and your suggestions and concerns greatly improved the paper. The new distribution shift experiments, as well as new results and discussion of non-bijective networks and their relationship to bijective ones, significantly increase the practical relevance of the work. \n\nGiven the tension between the positive comments to most of our contributions, the ratings and the fact that the main concerns are related to our proposed solution, we would like to point out that the developed training objective is only one out of four major contributions of the paper.\n\nWe list our updated contributions here again for clarity:\n\n1 - We introduce an alternative viewpoint on adversarial examples, one of the major failures in modern machine learning algorithms, give a formal definition of it and show its practical relevance for commonly used architectures in the updated experiments and discussion.\n\n2 - We build a competitive bijective ImageNet/MNIST classifier to tractably compute such adversarial examples exactly. Based on this, we provide what may be the first analytic adversarial attack method in the literature.\n\n3 - We prove that a major reason for invariance-based vulnerability is the commonly used cross-entropy objective and show from an information-theoretic viewpoint what may be done to overcome this.\n\n4 - We put our theoretical results into practice: based on bijective networks we introduce a practically useful loss and illustrate as a proof-of-concept that it largely overcomes the problem of excessive invariance, making it a promising way forward. Additionally, we have now included more quantitative experiments showing robustness to adversarial distribution shifts on a newly introduced benchmark.\n\nIn the revision we have:\n\n-- Thoroughly revised and updated the whole manuscript to make all of our contributions more clear and incorporate all raised concerns.\n-- Updated figures and descriptions and moved large parts of section 2 to the appendix to improve clarity. \n-- Added an adversarial distribution shift benchmark to stress test our proposed objective and show its effectiveness in challenging settings.\n-- Added new results on non-bijective networks for the metameric samples and the distribution shift experiments to show non-bijective networks have the same issues as the bijective networks we use. \n-- Added a discussion on the relationship between ResNets and RevNet-type networks, providing evidence that they are closely related. \n-- Added additional references from the literature providing evidence of false excessive invariance in non-bijective architectures.\n-- Added a random batch of metameric samples to the appendix, to showcase the consistency of our results.\n\nPlease let us know if you have any more questions or if there is anything else we can do to make you reconsider your rating.\n\nThank you once again for your effort."}, "signatures": ["ICLR.cc/2019/Conference/Paper778/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper778/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Excessive Invariance Causes Adversarial Vulnerability", "abstract": "Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from epsilon-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. We show such excessive invariance occurs across various tasks and architecture types. On MNIST and ImageNet one can manipulate the class-specific content of almost any image without changing the hidden activations. We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an information-theoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.", "keywords": ["Generalization", "Adversarial Examples", "Invariance", "Information Theory", "Invertible Networks"], "authorids": ["j.jacobsen@vectorinstitute.ai", "jensb@uni-bremen.de", "zemel@cs.toronto.edu", "matthias.bethge@uni-tuebingen.de"], "authors": ["Joern-Henrik Jacobsen", "Jens Behrmann", "Richard Zemel", "Matthias Bethge"], "TL;DR": "We show deep networks are not only too sensitive to task-irrelevant changes of their input, but also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks.", "pdf": "/pdf/bcabe02aeedd350f06c997ab0013889b5e624155.pdf", "paperhash": "jacobsen|excessive_invariance_causes_adversarial_vulnerability", "_bibtex": "@inproceedings{\njacobsen2018excessive,\ntitle={Excessive Invariance Causes Adversarial Vulnerability},\nauthor={Joern-Henrik Jacobsen and Jens Behrmann and Richard Zemel and Matthias Bethge},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfbpsAcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper778/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621621315, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkfbpsAcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper778/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper778/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper778/Authors|ICLR.cc/2019/Conference/Paper778/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621621315}}}, {"id": "ByeqLhs7TX", "original": null, "number": 3, "cdate": 1541811282469, "ddate": null, "tcdate": 1541811282469, "tmdate": 1541812251939, "tddate": null, "forum": "BkfbpsAcF7", "replyto": "SyeLf8916Q", "invitation": "ICLR.cc/2019/Conference/-/Paper778/Official_Comment", "content": {"title": "Added new experiments on distribution shift and large batch of metameric samples", "comment": "\n--------------------------------------------------------\n\nWe thank you very much for acknowledging our work as interesting and novel, as well as for the appreciation of our developed methodologies.\n\nWe answer your questions below.\n\n--------------------------------------------------------\n\nQ: Does the new training objective increase robustness to distributional shift?\n--\nThank you for raising this point. To shed light on the effect of our loss under adversarial distribution shifts we have added new experiments on a dataset we introduce to precisely test our claims. We term the dataset shiftMNIST and designed it such that it follows distribution shifts D_Adv of the form we assumed for Theorem 6.\n\nOur results reveal, that our proposed loss does indeed reduce the errors under challenging distribution shifts up to 38% as compared to cross-entropy trained ResNets and RevNets, highlighting the efficacy of our proposed objective. \n\nFurther, the results also show once again how badly standard networks can fail, even though in one task only one single pixel is removed, leaving the image semantics almost entirely unchanged. The results are one more piece of evidence for the insufficiency of cross-entropy based information maximization and the excessive invariance it may lead to in practice. \n\nWe sincerely thank you for bringing this up.\n\n--------------------------------------------------------\n\nQ: What is the typical behavior of samples shown in Figure 4?\n--\nThe metameric samples shown are representative and we have observed similar quality throughout the whole validation set, sometimes with slight colored artifacts though. We have added a large batch of metameric samples to the appendix to give the reader a better idea about their typical behavior. \n\n--------------------------------------------------------\n\nWe believe your review have substantially improved the manuscript, thank you."}, "signatures": ["ICLR.cc/2019/Conference/Paper778/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper778/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Excessive Invariance Causes Adversarial Vulnerability", "abstract": "Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from epsilon-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. We show such excessive invariance occurs across various tasks and architecture types. On MNIST and ImageNet one can manipulate the class-specific content of almost any image without changing the hidden activations. We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an information-theoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.", "keywords": ["Generalization", "Adversarial Examples", "Invariance", "Information Theory", "Invertible Networks"], "authorids": ["j.jacobsen@vectorinstitute.ai", "jensb@uni-bremen.de", "zemel@cs.toronto.edu", "matthias.bethge@uni-tuebingen.de"], "authors": ["Joern-Henrik Jacobsen", "Jens Behrmann", "Richard Zemel", "Matthias Bethge"], "TL;DR": "We show deep networks are not only too sensitive to task-irrelevant changes of their input, but also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks.", "pdf": "/pdf/bcabe02aeedd350f06c997ab0013889b5e624155.pdf", "paperhash": "jacobsen|excessive_invariance_causes_adversarial_vulnerability", "_bibtex": "@inproceedings{\njacobsen2018excessive,\ntitle={Excessive Invariance Causes Adversarial Vulnerability},\nauthor={Joern-Henrik Jacobsen and Jens Behrmann and Richard Zemel and Matthias Bethge},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfbpsAcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper778/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621621315, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkfbpsAcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper778/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper778/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper778/Authors|ICLR.cc/2019/Conference/Paper778/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621621315}}}, {"id": "BklM_OiQ6m", "original": null, "number": 1, "cdate": 1541810282434, "ddate": null, "tcdate": 1541810282434, "tmdate": 1541812244866, "tddate": null, "forum": "BkfbpsAcF7", "replyto": "r1e1SFU_2m", "invitation": "ICLR.cc/2019/Conference/-/Paper778/Official_Comment", "content": {"title": "Thoroughly revised manuscript uploaded", "comment": "\n--------------------------------------------------------\n\nWe thank you very much for acknowledging our work being appealing and our contributions being publication-worthy.\nWe also thank you for your thoughts and comments on the structure of the manuscript.\n\n--------------------------------------------------------\n\nQ: Overly commented first pages, imbalanced with section 3 and 4.\n\nWe have done our best to fix this and have substantially revised the paper. We removed large portions of section 2 and added it to the appendix, we added additional details about bijective networks, re-structured section 3 and 4 and added another experiment to emphasise our main contributions more. Finally, we have adjusted the abstract and contributions in the introduction accordingly.\n\n--------------------------------------------------------\n\nQ: Lacking detail on bijective network.\n\nThe main components we are using are based on Real-NVP[1]/Glow[2] and iRevNet[3] networks, which are widely known and cited in the paper, so we decided not to put too much focus on their details.\nHowever, in the revision we have added some additional details, for instance, we have added figure 3 that explains the architecture we are using.\n\n[1] Dinh, Laurent, Jascha Sohl-Dickstein, and Samy Bengio. \"Density estimation using Real NVP.\" \n[2] Kingma, Diederik P., and Prafulla Dhariwal. \"Glow: Generative flow with invertible 1x1 convolutions.\"\n[3] Jacobsen, J\u00f6rn-Henrik, Arnold Smeulders, and Edouard Oyallon. \"i-RevNet: Deep Invertible Networks.\"\n--------------------------------------------------------\n\nPlease let us know if you have any more comments or concerns!\n\nThank you once again."}, "signatures": ["ICLR.cc/2019/Conference/Paper778/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper778/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Excessive Invariance Causes Adversarial Vulnerability", "abstract": "Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from epsilon-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. We show such excessive invariance occurs across various tasks and architecture types. On MNIST and ImageNet one can manipulate the class-specific content of almost any image without changing the hidden activations. We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an information-theoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.", "keywords": ["Generalization", "Adversarial Examples", "Invariance", "Information Theory", "Invertible Networks"], "authorids": ["j.jacobsen@vectorinstitute.ai", "jensb@uni-bremen.de", "zemel@cs.toronto.edu", "matthias.bethge@uni-tuebingen.de"], "authors": ["Joern-Henrik Jacobsen", "Jens Behrmann", "Richard Zemel", "Matthias Bethge"], "TL;DR": "We show deep networks are not only too sensitive to task-irrelevant changes of their input, but also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks.", "pdf": "/pdf/bcabe02aeedd350f06c997ab0013889b5e624155.pdf", "paperhash": "jacobsen|excessive_invariance_causes_adversarial_vulnerability", "_bibtex": "@inproceedings{\njacobsen2018excessive,\ntitle={Excessive Invariance Causes Adversarial Vulnerability},\nauthor={Joern-Henrik Jacobsen and Jens Behrmann and Richard Zemel and Matthias Bethge},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfbpsAcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper778/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621621315, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkfbpsAcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper778/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper778/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper778/Authors|ICLR.cc/2019/Conference/Paper778/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621621315}}}, {"id": "HJeo0yhma7", "original": null, "number": 4, "cdate": 1541812179430, "ddate": null, "tcdate": 1541812179430, "tmdate": 1541812223444, "tddate": null, "forum": "BkfbpsAcF7", "replyto": "HklfNANAh7", "invitation": "ICLR.cc/2019/Conference/-/Paper778/Official_Comment", "content": {"title": "Added new experiments on non-bijective networks and proposed objective alongside thorough revision of manuscript", "comment": "\n--------------------------------------------------------\n\nWe are glad that you find most of our major contributions original, interesting, clear and mathematically sound.\nWe also thank you for your thoughtful questions and comments, we address them below.\n\n--------------------------------------------------------\n\nQ: How are findings related to non-bijective networks?\n--\nThank you for bringing this up, we have revised the manuscript to answer this important question very clearly to show our identified problems, analysis and conclusion are not limited to bijective networks.\nWe summarize below.\n\n----------\n-- Our identified problem of excessive invariance occurs in many other networks as well.\n----------\n\nWe have added results on the gradient-based equivalent of our analytic metameric sampling attack to the paper. We match the logit vector of one image with the logits of another image via gradient-based optimization and no norm-based restriction on the input. We do so on an ImageNet-trained state of the art ResNet-154 and see that the problem we have identified in bijective nets is the same here, if not worse as the metameric samples look even cleaner. Qualitative results are added to figure 5.\n\nBesides that, multiple papers have observed excessive invariance. On the adversarial spheres problem [1], for instance, the authors show their quadratic network does almost perfectly well while ignoring up to 60% of *semantically meaningful* input dimensions. Another line of work has also shown that similar behavior can appear in ReLU networks as well [2].\n\nWe have also added an additional set of experiments to the revised manuscript that shows how cross-entropy trained ResNets fail badly under distribution shifts that exploit their excessive invariance, giving another piece of evidence that our findings are not limited to bijective networks, but applicable to the most successful deep network architecture around as well.\n\n----------\n-- There is a close relationship between bijective nets and SOTA architectures.\n----------\n\nBijective networks are closely related to ResNets, they are in fact provably bijective under mild assumptions, as shown by a recent publication [3]. Further, it has been shown that ResNets and RevNet-type networks differ only in their dimension splitting scheme from one another [4]. And finally, bijective iRevNets have been shown to have many equivalent progressive properties to ResNets throughout the layers of their learned representation [5].\n\nIn summary, there is ample evidence, that bijective RevNet-type networks are not the reason for the problems we observe, but rather extremely similar to ResNets, the de-facto state-of-the-art architecture, while providing a powerful framework to study and combat problems like excessive invariance.\n\n[1] Gilmer, Justin, et al. \"Adversarial spheres.\" \n[2] Behrmann, Jens, et al. \"Analysis of Invariance and Robustness via Invertibility of ReLU-Networks.\"\n[3] Behrmann, Jens, David Duvenaud, and J\u00f6rn-Henrik Jacobsen. \"Invertible Residual Networks.\"\n[4] Grathwohl, Will, et al. \"FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models.\"\n[5] Jacobsen, J\u00f6rn-Henrik, Arnold Smeulders, and Edouard Oyallon. \"i-RevNet: Deep Invertible Networks.\""}, "signatures": ["ICLR.cc/2019/Conference/Paper778/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper778/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Excessive Invariance Causes Adversarial Vulnerability", "abstract": "Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from epsilon-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. We show such excessive invariance occurs across various tasks and architecture types. On MNIST and ImageNet one can manipulate the class-specific content of almost any image without changing the hidden activations. We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an information-theoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.", "keywords": ["Generalization", "Adversarial Examples", "Invariance", "Information Theory", "Invertible Networks"], "authorids": ["j.jacobsen@vectorinstitute.ai", "jensb@uni-bremen.de", "zemel@cs.toronto.edu", "matthias.bethge@uni-tuebingen.de"], "authors": ["Joern-Henrik Jacobsen", "Jens Behrmann", "Richard Zemel", "Matthias Bethge"], "TL;DR": "We show deep networks are not only too sensitive to task-irrelevant changes of their input, but also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks.", "pdf": "/pdf/bcabe02aeedd350f06c997ab0013889b5e624155.pdf", "paperhash": "jacobsen|excessive_invariance_causes_adversarial_vulnerability", "_bibtex": "@inproceedings{\njacobsen2018excessive,\ntitle={Excessive Invariance Causes Adversarial Vulnerability},\nauthor={Joern-Henrik Jacobsen and Jens Behrmann and Richard Zemel and Matthias Bethge},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfbpsAcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper778/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621621315, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkfbpsAcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper778/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper778/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper778/Authors|ICLR.cc/2019/Conference/Paper778/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621621315}}}, {"id": "B1xjee27aX", "original": null, "number": 5, "cdate": 1541812211073, "ddate": null, "tcdate": 1541812211073, "tmdate": 1541812211073, "tddate": null, "forum": "BkfbpsAcF7", "replyto": "HJeo0yhma7", "invitation": "ICLR.cc/2019/Conference/-/Paper778/Official_Comment", "content": {"title": "Part II", "comment": "\n---------------------------------------------------------\n\nQ: Can the training objective achieve its goal?\n\n----------\n-- (a) The nuisance classifier is not powerful enough to decode y from z_n.\n----------\n\nThis is indeed a common problem when formulating a bound this way and it is the same problem GANs face. However, in practice, GANs often work and we also find that the nuisance classifier does indeed do its job, one could even validate this post-hoc by training a more powerful nuisance classifier to confirm it.\n\nAdditionally, we also have metameric sampling as a validation method. If the information about the class is only hidden in z_n, but not removed, then metameric sampling would reveal this. Replacing z_n of one category with a z_n from another category would then change the category of the reconstruction, but we see that this is not happening when applying our loss. Thus, we conclude that the objective is successful, albeit it having its challenges.\n\n----------\n-- (b) The factorial maximum likelihood objective does not lead to independence.\n----------\n\nWe agree, that there is no guarantee that the loss will lead to full independence, but it does encourage it at least.\nOn the other hand, our evaluation method (metameric sampling) is not based on samples from the model but is based on the activations of real data points. Thus, according to your argumentation, this sampling method would reveal strong dependencies between the subspaces. In practice, we see this is not the case, as shown in figure 7 on the right, where combinations of z_n from one class and z_s from another do indeed lead to a change of nuisance/style in the original image, but not to a change of category. Empirically this means our objective was successful and most of the label information has been removed from z_n.\n\nTo further analyze the objective, we have added another experiment to assess if it can successfully defend against targeted distribution shifts as considered in Theorem 6.\nWe introduce a new dataset termed shiftMNIST, it augments MNIST with additional highly predictive features at train time and removes or randomizes those features at test time, while leaving the digits themselves as the stable predictive variable.\n\nOur experiments reveal, that the baseline cross-entropy trained ResNet and fiRevNet fail badly on these problems, while our proposed loss reduces the error under such distribution shift up to 38%. This provides more evidence that our proposed objective does achieve its goal in practice.\n\n----------\n\nIn summary, we do agree that the lower bound and the maximum likelihood objectives have their respective issues and we added some discussion on this to the manuscript. However, in practice, the metameric samples and our additional distribution shift experiments show that the loss does, in fact, work as intended, making it a promising way forward.\n\n---------------------------------------------------------\n\nQ: What do the images generated with z_s from one input and z_n from another input look like (in your method)?\n--\nThose images (the metameric samples) are already shown in the last row in the top block of figure 7, we have adapted the figure and added some more description to it, to make everything more clear.\nIn the baseline the metameric samples are adversarial examples, meaning one can turn any image into any class without changing the logits at all. With our objective (shown on the right side), this is not possible anymore as keeping z_s fixed and exchanging z_n only affects the style of the image, not its class-specific content. The objective has achieved its goal and successfully defended against the metameric sampling attack.\n\n---------------------------------------------------------\n\nMinor:\nWe have fixed the typos and added the log to the MLE objective, thank you.\n\n---------------------------------------------------------\n\nThank you once again for the detailed review, we were able to significantly improve the manuscript based on it.\nWe have revised multiple parts, added new experiments and added discussions to answer your concerns.\n\nWe hope we were able to answer everything to your satisfaction, please let us know if there are any more open points.\n\nThank you once again!"}, "signatures": ["ICLR.cc/2019/Conference/Paper778/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper778/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Excessive Invariance Causes Adversarial Vulnerability", "abstract": "Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from epsilon-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. We show such excessive invariance occurs across various tasks and architecture types. On MNIST and ImageNet one can manipulate the class-specific content of almost any image without changing the hidden activations. We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an information-theoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.", "keywords": ["Generalization", "Adversarial Examples", "Invariance", "Information Theory", "Invertible Networks"], "authorids": ["j.jacobsen@vectorinstitute.ai", "jensb@uni-bremen.de", "zemel@cs.toronto.edu", "matthias.bethge@uni-tuebingen.de"], "authors": ["Joern-Henrik Jacobsen", "Jens Behrmann", "Richard Zemel", "Matthias Bethge"], "TL;DR": "We show deep networks are not only too sensitive to task-irrelevant changes of their input, but also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks.", "pdf": "/pdf/bcabe02aeedd350f06c997ab0013889b5e624155.pdf", "paperhash": "jacobsen|excessive_invariance_causes_adversarial_vulnerability", "_bibtex": "@inproceedings{\njacobsen2018excessive,\ntitle={Excessive Invariance Causes Adversarial Vulnerability},\nauthor={Joern-Henrik Jacobsen and Jens Behrmann and Richard Zemel and Matthias Bethge},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfbpsAcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper778/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621621315, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkfbpsAcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper778/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper778/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper778/Authors|ICLR.cc/2019/Conference/Paper778/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper778/Reviewers", "ICLR.cc/2019/Conference/Paper778/Authors", "ICLR.cc/2019/Conference/Paper778/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621621315}}}], "count": 19}