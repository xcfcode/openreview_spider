{"notes": [{"id": "g4szfsQUdy3", "original": "S3yrRfjqygB", "number": 1100, "cdate": 1601308123735, "ddate": null, "tcdate": 1601308123735, "tmdate": 1614985629064, "tddate": null, "forum": "g4szfsQUdy3", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Implicit Regularization Effects of Unbiased Random Label Noises with SGD", "authorids": ["~Haoyi_Xiong1", "~Xuhong_Li3", "~Boyang_Yu1", "~Dejing_Dou1", "~Dongrui_Wu1", "~Zhanxing_Zhu1"], "authors": ["Haoyi Xiong", "Xuhong Li", "Boyang Yu", "Dejing Dou", "Dongrui Wu", "Zhanxing Zhu"], "keywords": [], "abstract": "Random label noises (or observational noises) widely exist in practical machinelearning settings. we analyze the learning dynamics of stochastic gradient descent(SGD) over the quadratic loss with unbiased label noises, and investigate a newnoise term of dynamics, which is dynamized and influenced by mini-batch sam-pling and random label noises, as an implicit regularizer. Our theoretical analysisfinds such implicit regularizer would favor some convergence points that could stabilize model outputs against perturbation of parameters. To validate our analy-sis, we use our theorems to estimate the closed-form solution of the implicit reg-ularizer over continuous-time SGD dynamics for Ordinary Least-Square (OLS), where the numerical simulation backups our estimates. We further extend our proposals to interpret the newly-fashioned noisy self-distillation tricks for deep learning,  where the implicit regularizer demonstrates a unique capacity of selecting models with improved output stability through learning from well-trained teach-ers with additive unbiased random label noises", "one-sentence_summary": "The random label noises perform as an implicit regularizer of SGD and help the learning procedure select a stable solution.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiong|implicit_regularization_effects_of_unbiased_random_label_noises_with_sgd", "supplementary_material": "/attachment/5fc2de52a56284b0e9bca931abc1ebbd2261793d.zip", "pdf": "/pdf/6570b0d3140d0a41799d93b8b5988cf7bda4042c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jL_TF5J1yy", "_bibtex": "@misc{\nxiong2021implicit,\ntitle={Implicit Regularization Effects of Unbiased Random Label Noises with {\\{}SGD{\\}}},\nauthor={Haoyi Xiong and Xuhong Li and Boyang Yu and Dejing Dou and Dongrui Wu and Zhanxing Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=g4szfsQUdy3}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "h18WIilBH0", "original": null, "number": 1, "cdate": 1610040531204, "ddate": null, "tcdate": 1610040531204, "tmdate": 1610474140783, "tddate": null, "forum": "g4szfsQUdy3", "replyto": "g4szfsQUdy3", "invitation": "ICLR.cc/2021/Conference/Paper1100/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The reviewers pointed out that the claims made in this submission have already appeared (in even stronger forms) before, to which the authors seem to agree. Therefore, this submission is not ready for publication in its current form. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Implicit Regularization Effects of Unbiased Random Label Noises with SGD", "authorids": ["~Haoyi_Xiong1", "~Xuhong_Li3", "~Boyang_Yu1", "~Dejing_Dou1", "~Dongrui_Wu1", "~Zhanxing_Zhu1"], "authors": ["Haoyi Xiong", "Xuhong Li", "Boyang Yu", "Dejing Dou", "Dongrui Wu", "Zhanxing Zhu"], "keywords": [], "abstract": "Random label noises (or observational noises) widely exist in practical machinelearning settings. we analyze the learning dynamics of stochastic gradient descent(SGD) over the quadratic loss with unbiased label noises, and investigate a newnoise term of dynamics, which is dynamized and influenced by mini-batch sam-pling and random label noises, as an implicit regularizer. Our theoretical analysisfinds such implicit regularizer would favor some convergence points that could stabilize model outputs against perturbation of parameters. To validate our analy-sis, we use our theorems to estimate the closed-form solution of the implicit reg-ularizer over continuous-time SGD dynamics for Ordinary Least-Square (OLS), where the numerical simulation backups our estimates. We further extend our proposals to interpret the newly-fashioned noisy self-distillation tricks for deep learning,  where the implicit regularizer demonstrates a unique capacity of selecting models with improved output stability through learning from well-trained teach-ers with additive unbiased random label noises", "one-sentence_summary": "The random label noises perform as an implicit regularizer of SGD and help the learning procedure select a stable solution.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiong|implicit_regularization_effects_of_unbiased_random_label_noises_with_sgd", "supplementary_material": "/attachment/5fc2de52a56284b0e9bca931abc1ebbd2261793d.zip", "pdf": "/pdf/6570b0d3140d0a41799d93b8b5988cf7bda4042c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jL_TF5J1yy", "_bibtex": "@misc{\nxiong2021implicit,\ntitle={Implicit Regularization Effects of Unbiased Random Label Noises with {\\{}SGD{\\}}},\nauthor={Haoyi Xiong and Xuhong Li and Boyang Yu and Dejing Dou and Dongrui Wu and Zhanxing Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=g4szfsQUdy3}\n}"}, "tags": [], "invitation": {"reply": {"forum": "g4szfsQUdy3", "replyto": "g4szfsQUdy3", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040531191, "tmdate": 1610474140765, "id": "ICLR.cc/2021/Conference/Paper1100/-/Decision"}}}, {"id": "Y-e9tlZiJOC", "original": null, "number": 6, "cdate": 1606291293455, "ddate": null, "tcdate": 1606291293455, "tmdate": 1606291293455, "tddate": null, "forum": "g4szfsQUdy3", "replyto": "g4szfsQUdy3", "invitation": "ICLR.cc/2021/Conference/Paper1100/-/Official_Comment", "content": {"title": "Thanks for the reviews", "comment": "Many thanks for the reviews. We acknowledge that the high-level conclusions made in this manuscript already exist in the earlier work (Blanc et al. 2020), though we adopt a different model to characterize SGD and empirically evaluate the findings using deep neural networks with benchmark datasets. We regret that we failed to complete the literature review prior to the submission. In addition, the manuscript has a weaker theoretical argument than the ealier work. We acknowledge that this manuscript is not ready for publication. Thank you again for pointing all these flaws out."}, "signatures": ["ICLR.cc/2021/Conference/Paper1100/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1100/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Implicit Regularization Effects of Unbiased Random Label Noises with SGD", "authorids": ["~Haoyi_Xiong1", "~Xuhong_Li3", "~Boyang_Yu1", "~Dejing_Dou1", "~Dongrui_Wu1", "~Zhanxing_Zhu1"], "authors": ["Haoyi Xiong", "Xuhong Li", "Boyang Yu", "Dejing Dou", "Dongrui Wu", "Zhanxing Zhu"], "keywords": [], "abstract": "Random label noises (or observational noises) widely exist in practical machinelearning settings. we analyze the learning dynamics of stochastic gradient descent(SGD) over the quadratic loss with unbiased label noises, and investigate a newnoise term of dynamics, which is dynamized and influenced by mini-batch sam-pling and random label noises, as an implicit regularizer. Our theoretical analysisfinds such implicit regularizer would favor some convergence points that could stabilize model outputs against perturbation of parameters. To validate our analy-sis, we use our theorems to estimate the closed-form solution of the implicit reg-ularizer over continuous-time SGD dynamics for Ordinary Least-Square (OLS), where the numerical simulation backups our estimates. We further extend our proposals to interpret the newly-fashioned noisy self-distillation tricks for deep learning,  where the implicit regularizer demonstrates a unique capacity of selecting models with improved output stability through learning from well-trained teach-ers with additive unbiased random label noises", "one-sentence_summary": "The random label noises perform as an implicit regularizer of SGD and help the learning procedure select a stable solution.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiong|implicit_regularization_effects_of_unbiased_random_label_noises_with_sgd", "supplementary_material": "/attachment/5fc2de52a56284b0e9bca931abc1ebbd2261793d.zip", "pdf": "/pdf/6570b0d3140d0a41799d93b8b5988cf7bda4042c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jL_TF5J1yy", "_bibtex": "@misc{\nxiong2021implicit,\ntitle={Implicit Regularization Effects of Unbiased Random Label Noises with {\\{}SGD{\\}}},\nauthor={Haoyi Xiong and Xuhong Li and Boyang Yu and Dejing Dou and Dongrui Wu and Zhanxing Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=g4szfsQUdy3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "g4szfsQUdy3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1100/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1100/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1100/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1100/Authors|ICLR.cc/2021/Conference/Paper1100/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1100/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863689, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1100/-/Official_Comment"}}}, {"id": "Re_R5E0QhC8", "original": null, "number": 2, "cdate": 1603875322432, "ddate": null, "tcdate": 1603875322432, "tmdate": 1605024531140, "tddate": null, "forum": "g4szfsQUdy3", "replyto": "g4szfsQUdy3", "invitation": "ICLR.cc/2021/Conference/Paper1100/-/Official_Review", "content": {"title": "Main result previously known", "review": "This paper studies the implicit regularization effect that arises from using stochastic gradient descent with label noise and squared loss. They derive the expression for the implicit regularization term and show that it favors solutions which are stable against perturbations of the parameters. This paper validates their empirical findings using SGD on linear regression task with label noise. This paper also uses their results to study the self distillation technique.\n\nThe main concern with this paper is that I think that the authors claim that they are the first ones to characterize the implicit regularization arising from SGD with label noise but this result is already known [1]. I believe that [1] works in the same setting and their implicit regularization for SGD exactly matches as derived in this paper.\n\nI would recommend rejecting this paper because I believe that the main result presented in this paper is already known [1] and this paper has not cited or compared with that paper.\n\nOther questions:\n1) Why do the authors assume that the noise terms in equation 10 are independent when step size is small as mentioned on page 4?\n2) I am not convinced by the self distillation experiments. Why do the authors consider self distillation with label noise added to the logits? Is this form of distillation considered in previous works before? Did the authors consider adding label noise to just the original labels? \n3) Moreover, in the plots in figure 2, why do the curves follow this zig zag pattern with increasing noise?  Moreover, many points in the plots are actually below the red dotted line which is the accuracy without label noise. So, I am not fully convinced that label noise is actually leading to better performance. \n4) It is unclear to me what is the motivation for considering these two different types of noises - gaussian and symmetric and what is the take away for these types of noises?\n\nSome typos\n1. Page 2 - extent -> extended\n2. Page 2 - not clear what new convergence exactly means\n3. Page 4 -  brownie -> brownian\n4. Page 6 - increases -> increase\n\n1) Implicit regularization for deep neural networks driven by an Ornstein-Uhlenbeck like process, COLT 2020", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1100/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1100/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Implicit Regularization Effects of Unbiased Random Label Noises with SGD", "authorids": ["~Haoyi_Xiong1", "~Xuhong_Li3", "~Boyang_Yu1", "~Dejing_Dou1", "~Dongrui_Wu1", "~Zhanxing_Zhu1"], "authors": ["Haoyi Xiong", "Xuhong Li", "Boyang Yu", "Dejing Dou", "Dongrui Wu", "Zhanxing Zhu"], "keywords": [], "abstract": "Random label noises (or observational noises) widely exist in practical machinelearning settings. we analyze the learning dynamics of stochastic gradient descent(SGD) over the quadratic loss with unbiased label noises, and investigate a newnoise term of dynamics, which is dynamized and influenced by mini-batch sam-pling and random label noises, as an implicit regularizer. Our theoretical analysisfinds such implicit regularizer would favor some convergence points that could stabilize model outputs against perturbation of parameters. To validate our analy-sis, we use our theorems to estimate the closed-form solution of the implicit reg-ularizer over continuous-time SGD dynamics for Ordinary Least-Square (OLS), where the numerical simulation backups our estimates. We further extend our proposals to interpret the newly-fashioned noisy self-distillation tricks for deep learning,  where the implicit regularizer demonstrates a unique capacity of selecting models with improved output stability through learning from well-trained teach-ers with additive unbiased random label noises", "one-sentence_summary": "The random label noises perform as an implicit regularizer of SGD and help the learning procedure select a stable solution.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiong|implicit_regularization_effects_of_unbiased_random_label_noises_with_sgd", "supplementary_material": "/attachment/5fc2de52a56284b0e9bca931abc1ebbd2261793d.zip", "pdf": "/pdf/6570b0d3140d0a41799d93b8b5988cf7bda4042c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jL_TF5J1yy", "_bibtex": "@misc{\nxiong2021implicit,\ntitle={Implicit Regularization Effects of Unbiased Random Label Noises with {\\{}SGD{\\}}},\nauthor={Haoyi Xiong and Xuhong Li and Boyang Yu and Dejing Dou and Dongrui Wu and Zhanxing Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=g4szfsQUdy3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "g4szfsQUdy3", "replyto": "g4szfsQUdy3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1100/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538126985, "tmdate": 1606915808773, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1100/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1100/-/Official_Review"}}}, {"id": "Anyitg_gxCh", "original": null, "number": 3, "cdate": 1604044143437, "ddate": null, "tcdate": 1604044143437, "tmdate": 1605024531071, "tddate": null, "forum": "g4szfsQUdy3", "replyto": "g4szfsQUdy3", "invitation": "ICLR.cc/2021/Conference/Paper1100/-/Official_Review", "content": {"title": "Official Blind Review #2", "review": "The paper studies the implicit regularization effect of unbiased random label noise on the dynamics of stochastic gradient descent(SGD). More precisely, an unbiased random noise is added to the true labels and the paper aims to analyze the regularization effect of it. The paper shows that the unbiased label noise would favor convergence to points which \nstabilize model outputs against perturbation of parameters.\n\nI found the paper very obscure, rigorous justification to the hypothesis and claims are missing. For instance, \na) Below equation (12), it claims that under mild conditions, the variance of the noise vector are the given matrices. What would be the conditions? Please state it properly if it is a mild condition. \nb) In the paragraph \"SGD Learning Dynamics with Unbiased Random Label Noises\", it assumes that terms coming from mini-batch and label noise are independent. This is not true in general, please include some positive/negative examples when the assumption holds or violates. As this is one of the key assumptions which implies the additive Brownian motion, a justification is definitely necessary showing that the assumption is valid most often in practice. \nc) In the paragraph \"Inference Stability/Escape and Converge\", why does Prop 1 implies that the solution has low gradient norm. Intuitively, the unbiased noise increases the variance of the gradient and the algorithm would be less stable compared to noiseless version. \nd) It is claimed in performance tuning that larger $\\sigma$ introduces larger regularization effect with higher inference stability. If this is true, then one should use $\\sigma$ as large as possible, which is clearly wrong. \ne) I don't understand what is the purpose in the first experiment shown in Figure 1. It is clear that SGD ( with or without label noise) converges to a neighborhood of the solution rather than the exact minimum and the radius directly depends on the variance of the gradient. What need to be shown is the claim regarding inference stability. Moreover, one trajectory is clearly not representative as the noise $\\epsilon_i$ is random. Hence experiments should be performed by drawing multiple times $\\epsilon_i$, namely multiple trajectories.  \nOverall, there is a lack of justification regarding the claims in the paper. I won't recommend publication of the paper in the current state. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1100/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1100/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Implicit Regularization Effects of Unbiased Random Label Noises with SGD", "authorids": ["~Haoyi_Xiong1", "~Xuhong_Li3", "~Boyang_Yu1", "~Dejing_Dou1", "~Dongrui_Wu1", "~Zhanxing_Zhu1"], "authors": ["Haoyi Xiong", "Xuhong Li", "Boyang Yu", "Dejing Dou", "Dongrui Wu", "Zhanxing Zhu"], "keywords": [], "abstract": "Random label noises (or observational noises) widely exist in practical machinelearning settings. we analyze the learning dynamics of stochastic gradient descent(SGD) over the quadratic loss with unbiased label noises, and investigate a newnoise term of dynamics, which is dynamized and influenced by mini-batch sam-pling and random label noises, as an implicit regularizer. Our theoretical analysisfinds such implicit regularizer would favor some convergence points that could stabilize model outputs against perturbation of parameters. To validate our analy-sis, we use our theorems to estimate the closed-form solution of the implicit reg-ularizer over continuous-time SGD dynamics for Ordinary Least-Square (OLS), where the numerical simulation backups our estimates. We further extend our proposals to interpret the newly-fashioned noisy self-distillation tricks for deep learning,  where the implicit regularizer demonstrates a unique capacity of selecting models with improved output stability through learning from well-trained teach-ers with additive unbiased random label noises", "one-sentence_summary": "The random label noises perform as an implicit regularizer of SGD and help the learning procedure select a stable solution.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiong|implicit_regularization_effects_of_unbiased_random_label_noises_with_sgd", "supplementary_material": "/attachment/5fc2de52a56284b0e9bca931abc1ebbd2261793d.zip", "pdf": "/pdf/6570b0d3140d0a41799d93b8b5988cf7bda4042c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jL_TF5J1yy", "_bibtex": "@misc{\nxiong2021implicit,\ntitle={Implicit Regularization Effects of Unbiased Random Label Noises with {\\{}SGD{\\}}},\nauthor={Haoyi Xiong and Xuhong Li and Boyang Yu and Dejing Dou and Dongrui Wu and Zhanxing Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=g4szfsQUdy3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "g4szfsQUdy3", "replyto": "g4szfsQUdy3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1100/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538126985, "tmdate": 1606915808773, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1100/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1100/-/Official_Review"}}}, {"id": "busy-cBhoVj", "original": null, "number": 4, "cdate": 1604422627351, "ddate": null, "tcdate": 1604422627351, "tmdate": 1605024531003, "tddate": null, "forum": "g4szfsQUdy3", "replyto": "g4szfsQUdy3", "invitation": "ICLR.cc/2021/Conference/Paper1100/-/Official_Review", "content": {"title": "Review of Implicit Regularization Effects of Unbiased Random Label Noises with SGD", "review": "My review is going to be short because I truly believe that this paper, in the current form, does not reach the acceptance bar for ICLR.\n\nIndeed, the paper tries to show that SGD carries an implicit regularization along the procedure. The list of the contributions is summarized on page 2: the authors proposed a decomposition of the noise into two terms (i) a noise due to the subsampling of the gradient (ii) a noise due to the noisy labels. Neither this decomposition nor the conclusion from it is new. Worst, there are no proofs or experiments backing up there fuzzy conclusions.\n\nDespite, the fact the story behind SGD seems understood by the authors, I see no contributions in this paper.\n\n", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1100/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1100/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Implicit Regularization Effects of Unbiased Random Label Noises with SGD", "authorids": ["~Haoyi_Xiong1", "~Xuhong_Li3", "~Boyang_Yu1", "~Dejing_Dou1", "~Dongrui_Wu1", "~Zhanxing_Zhu1"], "authors": ["Haoyi Xiong", "Xuhong Li", "Boyang Yu", "Dejing Dou", "Dongrui Wu", "Zhanxing Zhu"], "keywords": [], "abstract": "Random label noises (or observational noises) widely exist in practical machinelearning settings. we analyze the learning dynamics of stochastic gradient descent(SGD) over the quadratic loss with unbiased label noises, and investigate a newnoise term of dynamics, which is dynamized and influenced by mini-batch sam-pling and random label noises, as an implicit regularizer. Our theoretical analysisfinds such implicit regularizer would favor some convergence points that could stabilize model outputs against perturbation of parameters. To validate our analy-sis, we use our theorems to estimate the closed-form solution of the implicit reg-ularizer over continuous-time SGD dynamics for Ordinary Least-Square (OLS), where the numerical simulation backups our estimates. We further extend our proposals to interpret the newly-fashioned noisy self-distillation tricks for deep learning,  where the implicit regularizer demonstrates a unique capacity of selecting models with improved output stability through learning from well-trained teach-ers with additive unbiased random label noises", "one-sentence_summary": "The random label noises perform as an implicit regularizer of SGD and help the learning procedure select a stable solution.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiong|implicit_regularization_effects_of_unbiased_random_label_noises_with_sgd", "supplementary_material": "/attachment/5fc2de52a56284b0e9bca931abc1ebbd2261793d.zip", "pdf": "/pdf/6570b0d3140d0a41799d93b8b5988cf7bda4042c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jL_TF5J1yy", "_bibtex": "@misc{\nxiong2021implicit,\ntitle={Implicit Regularization Effects of Unbiased Random Label Noises with {\\{}SGD{\\}}},\nauthor={Haoyi Xiong and Xuhong Li and Boyang Yu and Dejing Dou and Dongrui Wu and Zhanxing Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=g4szfsQUdy3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "g4szfsQUdy3", "replyto": "g4szfsQUdy3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1100/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538126985, "tmdate": 1606915808773, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1100/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1100/-/Official_Review"}}}, {"id": "yUbGoEo1oV7", "original": null, "number": 1, "cdate": 1603868864938, "ddate": null, "tcdate": 1603868864938, "tmdate": 1605024530944, "tddate": null, "forum": "g4szfsQUdy3", "replyto": "g4szfsQUdy3", "invitation": "ICLR.cc/2021/Conference/Paper1100/-/Official_Review", "content": {"title": "overlap with previous work", "review": "This paper studies the implicit regularization effect from random label noise in L2 regression, and argues that it can be captured by the average squared gradient norm over samples, $\\frac1N\\sum_{i=1}^N \\|\\| \\nabla_\\theta f(x_i, \\theta) \\|\\|^2$. This conclusion is achieved via analyzing the additional noise term in the update caused by the label noise, which in the SDE approximation has expected squared L2 norm proportional to $\\frac1N\\sum_{i=1}^N \\|\\| \\nabla_\\theta f(x_i, \\theta) \\|\\|^2$.\n\nThe paper does not cite a very related paper:\n\nImplicit regularization for deep neural networks driven by an Ornstein-Uhlenbeck like process. Guy Blanc, Neha Gupta, Gregory Valiant, Paul Valiant. COLT 2020\n\nThe above paper studies the same problem and obtains a similar conclusion that the implicit regularization of label noise can be captured by $\\frac1N\\sum_{i=1}^N \\|\\| \\nabla_\\theta f(x_i, \\theta) \\|\\|^2$. It is proved that a solution $\\theta$ which achieves zero training error is an attractive fixed point of the dynamics if and only if it is a local minimizer of the said implicit regularizer.\n\nThe high-level conclusion in the current submission is the same as Blanc et al. In addition, the current submission has a weaker theoretical argument, since it is based on the SDE approximation which may not capture SGD as the noise distribution may not be Gaussian. On the other hand, Blanc et al. rigorously prove that any attractive interpolating solution must also locally minimize the implicit regularizer. Given the overlap, the submission does not meet the bar of publication.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1100/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1100/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Implicit Regularization Effects of Unbiased Random Label Noises with SGD", "authorids": ["~Haoyi_Xiong1", "~Xuhong_Li3", "~Boyang_Yu1", "~Dejing_Dou1", "~Dongrui_Wu1", "~Zhanxing_Zhu1"], "authors": ["Haoyi Xiong", "Xuhong Li", "Boyang Yu", "Dejing Dou", "Dongrui Wu", "Zhanxing Zhu"], "keywords": [], "abstract": "Random label noises (or observational noises) widely exist in practical machinelearning settings. we analyze the learning dynamics of stochastic gradient descent(SGD) over the quadratic loss with unbiased label noises, and investigate a newnoise term of dynamics, which is dynamized and influenced by mini-batch sam-pling and random label noises, as an implicit regularizer. Our theoretical analysisfinds such implicit regularizer would favor some convergence points that could stabilize model outputs against perturbation of parameters. To validate our analy-sis, we use our theorems to estimate the closed-form solution of the implicit reg-ularizer over continuous-time SGD dynamics for Ordinary Least-Square (OLS), where the numerical simulation backups our estimates. We further extend our proposals to interpret the newly-fashioned noisy self-distillation tricks for deep learning,  where the implicit regularizer demonstrates a unique capacity of selecting models with improved output stability through learning from well-trained teach-ers with additive unbiased random label noises", "one-sentence_summary": "The random label noises perform as an implicit regularizer of SGD and help the learning procedure select a stable solution.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiong|implicit_regularization_effects_of_unbiased_random_label_noises_with_sgd", "supplementary_material": "/attachment/5fc2de52a56284b0e9bca931abc1ebbd2261793d.zip", "pdf": "/pdf/6570b0d3140d0a41799d93b8b5988cf7bda4042c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jL_TF5J1yy", "_bibtex": "@misc{\nxiong2021implicit,\ntitle={Implicit Regularization Effects of Unbiased Random Label Noises with {\\{}SGD{\\}}},\nauthor={Haoyi Xiong and Xuhong Li and Boyang Yu and Dejing Dou and Dongrui Wu and Zhanxing Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=g4szfsQUdy3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "g4szfsQUdy3", "replyto": "g4szfsQUdy3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1100/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538126985, "tmdate": 1606915808773, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1100/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1100/-/Official_Review"}}}], "count": 7}