{"notes": [{"id": "_sCOYXNwaI", "original": "njW6tGOH2np", "number": 834, "cdate": 1601308096349, "ddate": null, "tcdate": 1601308096349, "tmdate": 1614985740588, "tddate": null, "forum": "_sCOYXNwaI", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Diffeomorphic Template Transformers", "authorids": ["~Tycho_F.A._van_der_Ouderaa1", "~Ivana_Isgum1", "w.veldhuis@umcutrecht.nl", "~Bob_D._De_Vos2", "~Pim_Moeskops2"], "authors": ["Tycho F.A. van der Ouderaa", "Ivana Isgum", "Wouter B. Veldhuis", "Bob D. De Vos", "Pim Moeskops"], "keywords": [], "abstract": "In this paper we propose a spatial transformer network where the spatial transformations are limited to the group of diffeomorphisms. Diffeomorphic transformations are a kind of homeomorphism, which by definition preserve topology, a compelling property in certain applications.\nWe apply this diffemorphic spatial transformer to model the output of a neural network as a topology preserving mapping of a prior shape. By carefully choosing the prior shape we can enforce properties on the output of the network without requiring any changes to the loss function, such as smooth boundaries and a hard constraint on the number of connected components.\nThe diffeomorphic transformer networks outperform their non-diffeomorphic precursors when applied to learn data invariances in classification tasks. On a breast tissue segmentation task, we show that the approach is robust and flexible enough to deform simple artificial priors, such as Gaussian-shaped prior energies, into high-quality predictive probability densities. In addition to desirable topological properties, the segmentation maps have competitive quantitative fidelity compared to those obtained by direct estimation (i.e. plain U-Net).", "one-sentence_summary": "We propose a special kind of spatial transformer with diffeomorphic transformations and show that it can be particularly useful in certain classification and segmentation tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ouderaa|diffeomorphic_template_transformers", "pdf": "/pdf/06f15ed656485d193f3da689705f4ba986b595fc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Z2qRbmMONg", "_bibtex": "@misc{\nouderaa2021diffeomorphic,\ntitle={Diffeomorphic Template Transformers},\nauthor={Tycho F.A. van der Ouderaa and Ivana Isgum and Wouter B. Veldhuis and Bob D. De Vos and Pim Moeskops},\nyear={2021},\nurl={https://openreview.net/forum?id=_sCOYXNwaI}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "xdUG4w4q2pq", "original": null, "number": 1, "cdate": 1610040397593, "ddate": null, "tcdate": 1610040397593, "tmdate": 1610473993008, "tddate": null, "forum": "_sCOYXNwaI", "replyto": "_sCOYXNwaI", "invitation": "ICLR.cc/2021/Conference/Paper834/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper has initially received mixed reviews, with two favorable and two unfavorable reviews. Several serious issues have been raised, in particular on experiments and validation; limited novelty; limited performance improvement; on the preliminary stage of the paper, in particular presentation and writing, and on the justification of key choices.\n\nThe authors provided responses to some of these issues, but in the discussion phase the reviewers (and the AC) judged the the responses did not sufficiently address the weaknesses of the paper, in particular:\n- The experimental setup does not assess the key innovation of the paper. \n- Several contributions claimed by the authors (in the paper and in the response) are judged not to be novel.\n- Concerns regarding the metric chosen to measure smoothness.\nand other issues.\n\nThe reviewers and AC agreed, that the paper has potential and merits, but that at at this point it is not yet ready for publication."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diffeomorphic Template Transformers", "authorids": ["~Tycho_F.A._van_der_Ouderaa1", "~Ivana_Isgum1", "w.veldhuis@umcutrecht.nl", "~Bob_D._De_Vos2", "~Pim_Moeskops2"], "authors": ["Tycho F.A. van der Ouderaa", "Ivana Isgum", "Wouter B. Veldhuis", "Bob D. De Vos", "Pim Moeskops"], "keywords": [], "abstract": "In this paper we propose a spatial transformer network where the spatial transformations are limited to the group of diffeomorphisms. Diffeomorphic transformations are a kind of homeomorphism, which by definition preserve topology, a compelling property in certain applications.\nWe apply this diffemorphic spatial transformer to model the output of a neural network as a topology preserving mapping of a prior shape. By carefully choosing the prior shape we can enforce properties on the output of the network without requiring any changes to the loss function, such as smooth boundaries and a hard constraint on the number of connected components.\nThe diffeomorphic transformer networks outperform their non-diffeomorphic precursors when applied to learn data invariances in classification tasks. On a breast tissue segmentation task, we show that the approach is robust and flexible enough to deform simple artificial priors, such as Gaussian-shaped prior energies, into high-quality predictive probability densities. In addition to desirable topological properties, the segmentation maps have competitive quantitative fidelity compared to those obtained by direct estimation (i.e. plain U-Net).", "one-sentence_summary": "We propose a special kind of spatial transformer with diffeomorphic transformations and show that it can be particularly useful in certain classification and segmentation tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ouderaa|diffeomorphic_template_transformers", "pdf": "/pdf/06f15ed656485d193f3da689705f4ba986b595fc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Z2qRbmMONg", "_bibtex": "@misc{\nouderaa2021diffeomorphic,\ntitle={Diffeomorphic Template Transformers},\nauthor={Tycho F.A. van der Ouderaa and Ivana Isgum and Wouter B. Veldhuis and Bob D. De Vos and Pim Moeskops},\nyear={2021},\nurl={https://openreview.net/forum?id=_sCOYXNwaI}\n}"}, "tags": [], "invitation": {"reply": {"forum": "_sCOYXNwaI", "replyto": "_sCOYXNwaI", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040397579, "tmdate": 1610473992992, "id": "ICLR.cc/2021/Conference/Paper834/-/Decision"}}}, {"id": "IIgwyLqmmXh", "original": null, "number": 1, "cdate": 1603911045852, "ddate": null, "tcdate": 1603911045852, "tmdate": 1606742729036, "tddate": null, "forum": "_sCOYXNwaI", "replyto": "_sCOYXNwaI", "invitation": "ICLR.cc/2021/Conference/Paper834/-/Official_Review", "content": {"title": "Review of Diffeomorphic Spatial Transformer Networks: Weak Accept", "review": "Summary:\n\nThe authors present a modification to spatial transformer networks that restricts the transformations to the group of diffeomorphisms. When combined with shape priors, this imposes topological constraints on the mappings produced by the network. These are important considerations in applications such as segmentation tasks where we expect there to be constraints on, for example, the number of connected components. The authors demonstrate the effectiveness of their approach in MNIST experiments and a breast tissue segmentation task. \n\nStrengths:\n1) The paper is clear and well written, and positions its contributions well in relation to existing research.\n\n2) The method is well motivated and the procedure is reasonable. The paper tackles an important consideration when applying neural networks to segmentation tasks, as constraining to the group of diffeomorphisms imposes smoothness on the transformations that ensures the outputs share topological properties with our prior expectations (e.g. number of connected components, smoothness of boundaries).\n\n3) The authors use the scaling-and-squaring to solve the ODEs that describe the diffeomorphic transformations, which I have not seen used in other ML studies.\n\nWeaknesses:\n\n1) Section 3, which describes the components of the method, can be expanded (there is space for this at the end of page 2) and reorganised to make it clearer. Specifically:\n\n\ta) the last two lines of paragraph one of page 4, starting with \u2018The scaling-and-squaring...\u2019, say this algorithm is not amenable to time-dependent parametrisations as laid out by the ODE in eq. 1. It is not immediately clear to me as to what the A and B refer to in this situation, are these the separate time steps t?\n\n\tb) I would like to see more discussion about the binary tree assumption on the composition of the diffeomorphisms. I can understand why this is a useful assumption with regards to complexity and numerical stability, but I do not have good intuition about the implications of this assumption on the transformations the model outputs.\n\nMy suspicion is the combination of these two assumptions explain some of the behaviours seen in the results. \n\n2) The results section could also be expanded for a bit more clarity. My main concerns are around the metric that measures the percent of the Jacobian determinants that are less than zero. Is this computed from all observations or just on a hold out test set? Why is it the case that the supposedly non-diffeomorphic models in Table 1 perform better on this metric than the diffeomorphic models?\n\nWhy is there such a large discrepancy in the size of the standard deviations in Table 2?\n\nI do not think the results are particularly convincing in terms of improvements in the quality or the accuracy of the methods compared with, but the point of the paper stands if the behaviour of this metric is properly discussed. \n\n\nReasons for score: \nI vote for accepting the paper. On the whole, it is a solid paper with an interesting and novel contribution, but I think it is hampered by the lack of clarity in the areas listed above. I am willing to revise my assessment up if these concerns are addressed. \n\nQuestions for the rebuttal period:\n\nPlease refer to the questions in the weaknesses section. \n\n\n---------------------------------------------------------------------------------------------------------------\nUpdate after discussion:\nAs stated in my initial review, I wanted to see an in depth discussion of what the implications of the binary tree assumption are for the types of transformations that are produced and the impact on the metric using the Jacobian. If these concerns are addressed, I think the paper will be greatly strengthened in the future. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper834/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper834/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diffeomorphic Template Transformers", "authorids": ["~Tycho_F.A._van_der_Ouderaa1", "~Ivana_Isgum1", "w.veldhuis@umcutrecht.nl", "~Bob_D._De_Vos2", "~Pim_Moeskops2"], "authors": ["Tycho F.A. van der Ouderaa", "Ivana Isgum", "Wouter B. Veldhuis", "Bob D. De Vos", "Pim Moeskops"], "keywords": [], "abstract": "In this paper we propose a spatial transformer network where the spatial transformations are limited to the group of diffeomorphisms. Diffeomorphic transformations are a kind of homeomorphism, which by definition preserve topology, a compelling property in certain applications.\nWe apply this diffemorphic spatial transformer to model the output of a neural network as a topology preserving mapping of a prior shape. By carefully choosing the prior shape we can enforce properties on the output of the network without requiring any changes to the loss function, such as smooth boundaries and a hard constraint on the number of connected components.\nThe diffeomorphic transformer networks outperform their non-diffeomorphic precursors when applied to learn data invariances in classification tasks. On a breast tissue segmentation task, we show that the approach is robust and flexible enough to deform simple artificial priors, such as Gaussian-shaped prior energies, into high-quality predictive probability densities. In addition to desirable topological properties, the segmentation maps have competitive quantitative fidelity compared to those obtained by direct estimation (i.e. plain U-Net).", "one-sentence_summary": "We propose a special kind of spatial transformer with diffeomorphic transformations and show that it can be particularly useful in certain classification and segmentation tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ouderaa|diffeomorphic_template_transformers", "pdf": "/pdf/06f15ed656485d193f3da689705f4ba986b595fc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Z2qRbmMONg", "_bibtex": "@misc{\nouderaa2021diffeomorphic,\ntitle={Diffeomorphic Template Transformers},\nauthor={Tycho F.A. van der Ouderaa and Ivana Isgum and Wouter B. Veldhuis and Bob D. De Vos and Pim Moeskops},\nyear={2021},\nurl={https://openreview.net/forum?id=_sCOYXNwaI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_sCOYXNwaI", "replyto": "_sCOYXNwaI", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper834/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538133945, "tmdate": 1606915771902, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper834/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper834/-/Official_Review"}}}, {"id": "R-18a4-GC5-", "original": null, "number": 9, "cdate": 1606131507118, "ddate": null, "tcdate": 1606131507118, "tmdate": 1606131507118, "tddate": null, "forum": "_sCOYXNwaI", "replyto": "IIgwyLqmmXh", "invitation": "ICLR.cc/2021/Conference/Paper834/-/Official_Comment", "content": {"title": "Reply to AnonReviewer3", "comment": "We thank the reviewer for the insightful comments, suggestions and questions.\n\n\n(1a) We apologize for the unclarity. A and B indeed refer to two separate time steps. To obtain a time-dependent parameterization, opposed to integration of a single stationary velocity field, we split up the field in a sequence of several time-steps (yielding a piecewise time-dependent parameterization). However, the scaling-and-squaring can only be used to find the exponential of a single field Z and we use the BCH formula to approximate the field Z that corresponds to our time-dependent parameterization. We thank the reviewer for pointing this out and we will clarify this in the final version.\n\n(1b) Now that we have a way to combine two fields A and B into a single field Z, we use a binary tree structure to approximate parameterizations that consist of more than two time steps:\n```\nBinary tree:                Naive composition:\n((A, B), (C, D))            ( ( ( (A, B), C), D)\n     /      \\                       / \\\n   /  \\    /  \\                   / \\   D\n  A    B  C    D                / \\  C\n                               A   B\nMax-depth scales Big-O(N)      Max-depth scales Big-O(log N)\n```\n\nDoing so in a binary tree structure maximally limits the amount of times that the BCH formula, and therefore possible propagation of approximation errors, for each field to Big-O(log N) compared to Big-O(N) if fields are composed sequentially. We added an example composition with N=4 in ascii art above, and will add a more in-depth discussion on this in the \u2018Binary Tree Composition\u2019 paragraph on page 4.\n\n(2) All presented measurements, including percentage of negative Jacobian determinant counts, were calculated on entire volumes on a hold-out test set. The TPS models generate inherently smooth fields, and are therefore less prone to folding resulting in fewer negative Jacobian determinants. On the other hand, coarser TPS grids have less flexibility, which would make them unsuitable for application in complex anatomical segmentation tasks. We can see this effect in the application to the breast segmentation task, where the benefit of our method becomes apparent: the diffeomorphic vector fields offer flexible transformations, while respecting essential properties such as preservation of topology, which is supported by the low Hausdorff distances and low number of connected components. We will clarify these considerations in Section 5.2, Section 5.3 and the discussion section.\n\nThank you for noticing the discrepancy in standard deviations. We are very sorry but see that we have missed to include the proper values for a few standard deviations and will correct this in the manuscript:\n\nU-Net (direct estimation) | | | 0.846 \u00b1 0.24 | 12.62 \u00b1 15.48 | - | 51.79 | \\\nSpatial Template Transformer ((Lee et al., 2019)) + Shape Prior (ours) | X | | 0.877 \u00b1 0.21 | 12.58 \u00b1 12.91 | 0.43 \u00b1 0.01 | 19.68 |"}, "signatures": ["ICLR.cc/2021/Conference/Paper834/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper834/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diffeomorphic Template Transformers", "authorids": ["~Tycho_F.A._van_der_Ouderaa1", "~Ivana_Isgum1", "w.veldhuis@umcutrecht.nl", "~Bob_D._De_Vos2", "~Pim_Moeskops2"], "authors": ["Tycho F.A. van der Ouderaa", "Ivana Isgum", "Wouter B. Veldhuis", "Bob D. De Vos", "Pim Moeskops"], "keywords": [], "abstract": "In this paper we propose a spatial transformer network where the spatial transformations are limited to the group of diffeomorphisms. Diffeomorphic transformations are a kind of homeomorphism, which by definition preserve topology, a compelling property in certain applications.\nWe apply this diffemorphic spatial transformer to model the output of a neural network as a topology preserving mapping of a prior shape. By carefully choosing the prior shape we can enforce properties on the output of the network without requiring any changes to the loss function, such as smooth boundaries and a hard constraint on the number of connected components.\nThe diffeomorphic transformer networks outperform their non-diffeomorphic precursors when applied to learn data invariances in classification tasks. On a breast tissue segmentation task, we show that the approach is robust and flexible enough to deform simple artificial priors, such as Gaussian-shaped prior energies, into high-quality predictive probability densities. In addition to desirable topological properties, the segmentation maps have competitive quantitative fidelity compared to those obtained by direct estimation (i.e. plain U-Net).", "one-sentence_summary": "We propose a special kind of spatial transformer with diffeomorphic transformations and show that it can be particularly useful in certain classification and segmentation tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ouderaa|diffeomorphic_template_transformers", "pdf": "/pdf/06f15ed656485d193f3da689705f4ba986b595fc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Z2qRbmMONg", "_bibtex": "@misc{\nouderaa2021diffeomorphic,\ntitle={Diffeomorphic Template Transformers},\nauthor={Tycho F.A. van der Ouderaa and Ivana Isgum and Wouter B. Veldhuis and Bob D. De Vos and Pim Moeskops},\nyear={2021},\nurl={https://openreview.net/forum?id=_sCOYXNwaI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_sCOYXNwaI", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper834/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper834/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper834/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper834/Authors|ICLR.cc/2021/Conference/Paper834/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper834/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866698, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper834/-/Official_Comment"}}}, {"id": "qi0s6xbmvIq", "original": null, "number": 8, "cdate": 1605790702692, "ddate": null, "tcdate": 1605790702692, "tmdate": 1605791009402, "tddate": null, "forum": "_sCOYXNwaI", "replyto": "PLFWabOlZS", "invitation": "ICLR.cc/2021/Conference/Paper834/-/Official_Comment", "content": {"title": "Reply to AnonReviewer1", "comment": "We thank the reviewer for the insightful comments, suggestions and questions.\n\n(1) The main novelties of our paper are: (I) using diffeomorphisms to enforce properties on the output of a neural segmentation model, (II) a time-dependent parameterization of the diffeomorphic field using the BCH formula and (III) the analytic shape prior itself (see also answer (3)).\nFrom a conceptual perspective, using diffeomorphisms (in combination with a carefully chosen shape prior) to enforce certain properties on the output of a neural segmentation model has not been done before and arguably interesting from a conceptual perspective. \n\n(1a) We thank you for this suggestion. The intuition to use a time dependent field is that parameterising the deformation as a time-dependent sequence of velocities allows the network to sequentially model larger movements first and detailed refinements thereafter. In our experiments we did find that the time-dependent parameterisation performed better, but did not run an in-depth analysis of performance between the two and therefore did not include these results in the paper. Nevertheless, we will try to perform additional experiments using stationary fields and provide the results as soon as they are available. \n\n(1b) We have analysed the computation time of our approach (see answer (4) to AnonReviewer4) and in addition also ran an experiment timing the difference between the stationary and time-dependent diffeomorphic fields. The time-dependent vector field is indeed slightly slower compared to a stationary field, although still small enough to be neglectible from a practical perspective.\n\nMethod                                                                                    | Inference time averaged over 20 full 3d volumes\n\nU-Net                                                                                       | 1.03 s      \nU-Net + non-diffeomorphic field                                        | 1.06 s      \nU-Net + stationary diffeomorphic field                             | 1.17 s      \nU-Net + time-dependent diffeomorphic field (ours)      | 1.19 s\n\n(2a) For the MNIST results, we tried to apply our method on an existing implementation without tuning hyperparameters in favor of our method to make the comparison as fair as possible. We do agree with the reviewer that the improvement is subtle and the experiments on the breast tissue segmentation might be more convincing as this is a real-world task. The MNIST experiments were repeated 20 times with different seeds to average out randomness in the optimization.\n\n(2b) We apologize for lack of clarity and thank the reviewer for pointing this out. We will update Table 2 and Section 5.2 and clarify that CNN + Field-STN is a spatial transformer network without performing integration of the velocity field to obtain a diffeomorphic field (ie. directly mapping the template with the field outputted by the neural network).\n\n(3a) The method of Lee et al. could not be applied to our task directly, because it relies on a task-specific shape template, which was unavailable for breast segmentation. We extended the method by Lee et al. with our analytical shape prior to act as a template and directly compared the extended approach with our other innovations. We understand this can be confusing and will update Section 5 to clarify this.\nFrom our results, we did find that integrating the velocity fields generated by the model resulted in slightly worse performance in terms of Dice score, but did clearly outperform the other methods in terms of Hausdorff Distance (HD), number of negative Jacobian determinants (<%JD) and number of connected components (CC). Also, we will update the importance of CC and HD for this task in Section 5 of the manuscript.\n\n(3b) In medical imaging tasks, robustness of the output is extremely important. We know beforehand that breast tissue is a single connected component and the edges should be smooth. By carefully choosing the shape prior to obey these properties, we enforce these properties - by definition of continuity of the diffeomorphic transformation - also to the output of the network. We are aware that we have missed some of these clinical motivations of the work and will add them in Section 1 and Section 5 of the manuscript.\n\n(4) Thank you for the advice. We will rewrite as suggested and make sure the template transformer is clearly introduced earlier in the paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper834/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper834/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diffeomorphic Template Transformers", "authorids": ["~Tycho_F.A._van_der_Ouderaa1", "~Ivana_Isgum1", "w.veldhuis@umcutrecht.nl", "~Bob_D._De_Vos2", "~Pim_Moeskops2"], "authors": ["Tycho F.A. van der Ouderaa", "Ivana Isgum", "Wouter B. Veldhuis", "Bob D. De Vos", "Pim Moeskops"], "keywords": [], "abstract": "In this paper we propose a spatial transformer network where the spatial transformations are limited to the group of diffeomorphisms. Diffeomorphic transformations are a kind of homeomorphism, which by definition preserve topology, a compelling property in certain applications.\nWe apply this diffemorphic spatial transformer to model the output of a neural network as a topology preserving mapping of a prior shape. By carefully choosing the prior shape we can enforce properties on the output of the network without requiring any changes to the loss function, such as smooth boundaries and a hard constraint on the number of connected components.\nThe diffeomorphic transformer networks outperform their non-diffeomorphic precursors when applied to learn data invariances in classification tasks. On a breast tissue segmentation task, we show that the approach is robust and flexible enough to deform simple artificial priors, such as Gaussian-shaped prior energies, into high-quality predictive probability densities. In addition to desirable topological properties, the segmentation maps have competitive quantitative fidelity compared to those obtained by direct estimation (i.e. plain U-Net).", "one-sentence_summary": "We propose a special kind of spatial transformer with diffeomorphic transformations and show that it can be particularly useful in certain classification and segmentation tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ouderaa|diffeomorphic_template_transformers", "pdf": "/pdf/06f15ed656485d193f3da689705f4ba986b595fc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Z2qRbmMONg", "_bibtex": "@misc{\nouderaa2021diffeomorphic,\ntitle={Diffeomorphic Template Transformers},\nauthor={Tycho F.A. van der Ouderaa and Ivana Isgum and Wouter B. Veldhuis and Bob D. De Vos and Pim Moeskops},\nyear={2021},\nurl={https://openreview.net/forum?id=_sCOYXNwaI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_sCOYXNwaI", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper834/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper834/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper834/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper834/Authors|ICLR.cc/2021/Conference/Paper834/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper834/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866698, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper834/-/Official_Comment"}}}, {"id": "OES7Ubqs0ZI", "original": null, "number": 6, "cdate": 1605790004429, "ddate": null, "tcdate": 1605790004429, "tmdate": 1605790990458, "tddate": null, "forum": "_sCOYXNwaI", "replyto": "KWotcs72mb", "invitation": "ICLR.cc/2021/Conference/Paper834/-/Official_Comment", "content": {"title": "Reply to AnonReviewer4", "comment": "We thank the reviewer for the insightful comments, suggestions and questions.\n\n(1) We apologize for the unclarity. We will clarify the motivations and intuitions behind the time-dependent vector field and the prior shape in Section 3 and Section 4.1 of the manuscript.\n\nThe main intuition about parameterizing a time-dependent vector field is increased flexibility. The network needs to deform a ball shaped object into the shape of the breast tissue and then obtain a fine detailed alignment of the shape around the edges of the shape. Parameterising the deformation as a time-dependent sequence of velocities allows the network to sequentially model larger movements first and detailed refinements thereafter.\n\nThe main intuition behind the prior shape is that it is required to construct an object such that a single connected component and smooth boundaries are guaranteed. By definition of continuity of the diffeomorphic transformation, this will also enforce these properties on the model output. In this paper we propose an analytic function that defines a gaussian-like ball shape to be used as a template. The motivation behind our proposed shape is twofold: First, we show that our method is flexible enough to deform even simple ball-shape functions into highly accurate output predictions of breast tissue. Second, the Gaussian ball is generic, making the prior shape practical from an engineering perspective. Nevertheless, any template that fits the specific problem at hand can be implemented.\n\n(2) The main novelties of our paper are: (I) using diffeomorphisms to enforce desirable properties on the output of a neural segmentation model, (II) an efficient time-dependent parameterization of the diffeomorphic field using the BCH formula and (III) the proposed analytic shape prior itself.\nLike any other approximation method, our approximation based on the BCH formula has some errors, but the results corroborate that the effect on model performance is likely limited. The benefit of using the BCH formula is that it allows us to use scaling-and-squaring to approximate a time-dependent field making it very efficient (please see answer (4) below). We will add a comparison of computational time measurements supporting this in the Supplementary Material.\n\n(3) We agree with the reviewer that compared to conventional spatial transformer networks the improvements on MNIST are subtle, but we did achieve a performance increase nevertheless. However, we would like to emphasize that for the comparison we used settings that were specifically designed for the conventional methods, and we did not tune any hyperparameters in favor of our proposed method.\nWe agree with the reviewer that our method did not perform best in terms of Dice score, but it outperformed the other methods in all other metrics. In medical tasks, robustness and consistency of methods is extremely important, since here downstream tasks may depend on correct continuous segmentation. These criteria are better reflected by including the other evaluation metrics: negative Jacobian determinants, Hausdorff Distance (HD), and connected components (CC). Moreover, close visual inspection of the results revealed that the outputs are qualitatively better.\n\n(4) We specifically have proposed to use the BCH formula to be able to approximate the time-dependent parameterisation using scaling-and-squaring, which is highly parallelizable and one of the most efficient approximations to calculate the matrix exponential. We will update our manuscript and add a comparison of computational time supporting this in the Supplementary Material: \n\nMethod                                                   | Inference time averaged over 20 full 3d volumes\n\nU-Net                                                       | 1.03 s      \nU-Net + non-diffeomorphic field        | 1.06 s      \nU-Net + diffeomorphic field (ours)     | 1.19 s    \n\n(5) Thank you for pointing this out. The scaling-and-squaring algorithm is very similar to Euler\u2019s method, where you would start from the identity Id and exponentiate by iteratively applying the field scaled down by the time fraction. In scaling-and-squaring, however, you start by \u2018scaling\u2019 down the field (v/2^T) and then self-composing (\u2018squaring\u2019) in order to find the solution more efficiently (see Moler and Van Loan, 2003). We understand how this can be confusing and will explain the difference between the two more clearly in the final version.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper834/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper834/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diffeomorphic Template Transformers", "authorids": ["~Tycho_F.A._van_der_Ouderaa1", "~Ivana_Isgum1", "w.veldhuis@umcutrecht.nl", "~Bob_D._De_Vos2", "~Pim_Moeskops2"], "authors": ["Tycho F.A. van der Ouderaa", "Ivana Isgum", "Wouter B. Veldhuis", "Bob D. De Vos", "Pim Moeskops"], "keywords": [], "abstract": "In this paper we propose a spatial transformer network where the spatial transformations are limited to the group of diffeomorphisms. Diffeomorphic transformations are a kind of homeomorphism, which by definition preserve topology, a compelling property in certain applications.\nWe apply this diffemorphic spatial transformer to model the output of a neural network as a topology preserving mapping of a prior shape. By carefully choosing the prior shape we can enforce properties on the output of the network without requiring any changes to the loss function, such as smooth boundaries and a hard constraint on the number of connected components.\nThe diffeomorphic transformer networks outperform their non-diffeomorphic precursors when applied to learn data invariances in classification tasks. On a breast tissue segmentation task, we show that the approach is robust and flexible enough to deform simple artificial priors, such as Gaussian-shaped prior energies, into high-quality predictive probability densities. In addition to desirable topological properties, the segmentation maps have competitive quantitative fidelity compared to those obtained by direct estimation (i.e. plain U-Net).", "one-sentence_summary": "We propose a special kind of spatial transformer with diffeomorphic transformations and show that it can be particularly useful in certain classification and segmentation tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ouderaa|diffeomorphic_template_transformers", "pdf": "/pdf/06f15ed656485d193f3da689705f4ba986b595fc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Z2qRbmMONg", "_bibtex": "@misc{\nouderaa2021diffeomorphic,\ntitle={Diffeomorphic Template Transformers},\nauthor={Tycho F.A. van der Ouderaa and Ivana Isgum and Wouter B. Veldhuis and Bob D. De Vos and Pim Moeskops},\nyear={2021},\nurl={https://openreview.net/forum?id=_sCOYXNwaI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_sCOYXNwaI", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper834/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper834/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper834/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper834/Authors|ICLR.cc/2021/Conference/Paper834/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper834/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866698, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper834/-/Official_Comment"}}}, {"id": "iz2hSWrlJ-D", "original": null, "number": 7, "cdate": 1605790092913, "ddate": null, "tcdate": 1605790092913, "tmdate": 1605790092913, "tddate": null, "forum": "_sCOYXNwaI", "replyto": "YY1YBzcEb_x", "invitation": "ICLR.cc/2021/Conference/Paper834/-/Official_Comment", "content": {"title": "Reply to AnonReviewer2", "comment": "We thank the reviewer for suggestions and questions.\n\n\nClarity and Presentation:\n\nThe ratio of negative Jacobian determinants is a standard method to evaluate presence of undesired image folding in deformation fields. Although we found that integrating the velocity fields generated by the network helped to reduce the number of negative Jacobians (see Table 2 on page 7), they still occur. Further reduction of these negative Jacobians (i.e. foldings) is an interesting open research question. \n\nExperimental Validation:\n\nBreast tissue segmentation is a particularly interesting problem for this task, because we know that the breast tissue is always a single connected component. Since we propose our diffeomorphic template transformer to enforce this property on the output of a segmentation model, it is a suitable task to evaluate performance. It would be very interesting to assess how our proposed method would perform on other tasks such as coronary artery tree segmentation, however we are not able to use the data used by Lee et. al since that data is not publicly available. We will try to better this in Section 6 on page 8.\n\nRegarding the experiments showing data invariances, we were able to perform better on an existing implementation without any tuning (please see our answer to AnonReviewer4-3), which shows the general applicability of diffeomorphic spatial transformers. It would indeed be interesting to see how well the approach can learn data invariances on other tasks, but chose to put more emphasis on the breast tissue experiments since diffeomorphisms lend themselves particularly to template transformer setting, to enforce properties on the output.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper834/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper834/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diffeomorphic Template Transformers", "authorids": ["~Tycho_F.A._van_der_Ouderaa1", "~Ivana_Isgum1", "w.veldhuis@umcutrecht.nl", "~Bob_D._De_Vos2", "~Pim_Moeskops2"], "authors": ["Tycho F.A. van der Ouderaa", "Ivana Isgum", "Wouter B. Veldhuis", "Bob D. De Vos", "Pim Moeskops"], "keywords": [], "abstract": "In this paper we propose a spatial transformer network where the spatial transformations are limited to the group of diffeomorphisms. Diffeomorphic transformations are a kind of homeomorphism, which by definition preserve topology, a compelling property in certain applications.\nWe apply this diffemorphic spatial transformer to model the output of a neural network as a topology preserving mapping of a prior shape. By carefully choosing the prior shape we can enforce properties on the output of the network without requiring any changes to the loss function, such as smooth boundaries and a hard constraint on the number of connected components.\nThe diffeomorphic transformer networks outperform their non-diffeomorphic precursors when applied to learn data invariances in classification tasks. On a breast tissue segmentation task, we show that the approach is robust and flexible enough to deform simple artificial priors, such as Gaussian-shaped prior energies, into high-quality predictive probability densities. In addition to desirable topological properties, the segmentation maps have competitive quantitative fidelity compared to those obtained by direct estimation (i.e. plain U-Net).", "one-sentence_summary": "We propose a special kind of spatial transformer with diffeomorphic transformations and show that it can be particularly useful in certain classification and segmentation tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ouderaa|diffeomorphic_template_transformers", "pdf": "/pdf/06f15ed656485d193f3da689705f4ba986b595fc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Z2qRbmMONg", "_bibtex": "@misc{\nouderaa2021diffeomorphic,\ntitle={Diffeomorphic Template Transformers},\nauthor={Tycho F.A. van der Ouderaa and Ivana Isgum and Wouter B. Veldhuis and Bob D. De Vos and Pim Moeskops},\nyear={2021},\nurl={https://openreview.net/forum?id=_sCOYXNwaI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_sCOYXNwaI", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper834/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper834/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper834/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper834/Authors|ICLR.cc/2021/Conference/Paper834/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper834/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866698, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper834/-/Official_Comment"}}}, {"id": "PLFWabOlZS", "original": null, "number": 2, "cdate": 1603919046123, "ddate": null, "tcdate": 1603919046123, "tmdate": 1605024594794, "tddate": null, "forum": "_sCOYXNwaI", "replyto": "_sCOYXNwaI", "invitation": "ICLR.cc/2021/Conference/Paper834/-/Official_Review", "content": {"title": "Interesting novelty but subtle and not backed up with experimental results", "review": "Authors present a spatial transformer layer that models\ndiffeomorphisms. Modelling diffeormophisms with neural networks is not very\nnew. Prior work successfully utilized stationary vector field prediction and\nfast solvers through scaling-and-squaring techniques. These prior work have been\nused to tackle registration and segmentation problems. The technical novelty\nhere is to use time-dependent vector fields for modeling\ndiffeomorphisms. Experiments on MNIST classification, using the proposed spatial\ntransformer as a layer, and breast tissue segmentation, using the proposed\ntransformer as a single network to deform a template, are presented and compared\nwith conventional spatial transformer layers using thin-plate-splines as\ntransformation models.\n\nWhile the introduction of time-dependency is interesting, more from a technical\nperspective than a conceptual perspective, I believe there are several aspects\nof the paper that needs improvement. \n\n1. Since the main innovation is integration of time-dependency in modelling\nvector fields with neural networks, I suggest directly comparing time-dependent\nwith stationary vector field approaches.\na. Experimental comparison can focus on the differences between stationary and\npiece-wise stationary vector field modelling. I could not see this comparison in\nthe experiments, only a single mention in the conclusion stating better\nperformance of the proposed model.\nb. Analysis of computational time would be interesting. Predicting and\nintegrating a stationary vector field will be faster I assume. But by how much?\n\n2. MNIST classification experiments raise some concerns:\na. The increase in classification accuracy of the proposed model compared to\nCNN + Field-STN is small. This difference may as well be due to randomness of\nthe optimization. This result is not motivating for the proposed\nmethod. Achieving lower number of negative Jacobian determinants is interesting\nbut its value is questionable.\nb. What is CNN + Field-STN? This is not defined.\n\n3. Segmentation results raise the following concerns:\na. The method proposed here does not seem to be better than the method\nproposed in Lee et al. 2019. The lower HD distance can be motivating but value\nof this may be better justified. If the proposed innovation was substantial,\nlower accuracy would have been completely fine. However, since the innovation is\nsubtle, such a lower accuracy lowers the enthusiasm for the paper.\nb. The proposed method achieves lower number of connected components but the\nvalue of this is not very well motivated from an application\nperspective. Providing such a motivation would be helpful.\n\n4. I suggest editing section 3 to present the proposed method much more\nclearly. The method only becomes clear in Section 4. \n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper834/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper834/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diffeomorphic Template Transformers", "authorids": ["~Tycho_F.A._van_der_Ouderaa1", "~Ivana_Isgum1", "w.veldhuis@umcutrecht.nl", "~Bob_D._De_Vos2", "~Pim_Moeskops2"], "authors": ["Tycho F.A. van der Ouderaa", "Ivana Isgum", "Wouter B. Veldhuis", "Bob D. De Vos", "Pim Moeskops"], "keywords": [], "abstract": "In this paper we propose a spatial transformer network where the spatial transformations are limited to the group of diffeomorphisms. Diffeomorphic transformations are a kind of homeomorphism, which by definition preserve topology, a compelling property in certain applications.\nWe apply this diffemorphic spatial transformer to model the output of a neural network as a topology preserving mapping of a prior shape. By carefully choosing the prior shape we can enforce properties on the output of the network without requiring any changes to the loss function, such as smooth boundaries and a hard constraint on the number of connected components.\nThe diffeomorphic transformer networks outperform their non-diffeomorphic precursors when applied to learn data invariances in classification tasks. On a breast tissue segmentation task, we show that the approach is robust and flexible enough to deform simple artificial priors, such as Gaussian-shaped prior energies, into high-quality predictive probability densities. In addition to desirable topological properties, the segmentation maps have competitive quantitative fidelity compared to those obtained by direct estimation (i.e. plain U-Net).", "one-sentence_summary": "We propose a special kind of spatial transformer with diffeomorphic transformations and show that it can be particularly useful in certain classification and segmentation tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ouderaa|diffeomorphic_template_transformers", "pdf": "/pdf/06f15ed656485d193f3da689705f4ba986b595fc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Z2qRbmMONg", "_bibtex": "@misc{\nouderaa2021diffeomorphic,\ntitle={Diffeomorphic Template Transformers},\nauthor={Tycho F.A. van der Ouderaa and Ivana Isgum and Wouter B. Veldhuis and Bob D. De Vos and Pim Moeskops},\nyear={2021},\nurl={https://openreview.net/forum?id=_sCOYXNwaI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_sCOYXNwaI", "replyto": "_sCOYXNwaI", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper834/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538133945, "tmdate": 1606915771902, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper834/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper834/-/Official_Review"}}}, {"id": "YY1YBzcEb_x", "original": null, "number": 3, "cdate": 1604018433783, "ddate": null, "tcdate": 1604018433783, "tmdate": 1605024594730, "tddate": null, "forum": "_sCOYXNwaI", "replyto": "_sCOYXNwaI", "invitation": "ICLR.cc/2021/Conference/Paper834/-/Official_Review", "content": {"title": "Official Blind Review #2", "review": "This paper propose a novel method to incorporate shape prior in neural networks based on Diffeomorphic transformation. This is useful as by design it preserves certain desirable properties of output such as smooth boundaries and connected components which are of interest in medical imaging applications.   The method is validated on Mnist for data invariance and a medical imaging task for segmentation.\n\nNovelty: The idea of  incorporating shape prior information into neural network based image segmentation is inspired by Lee et al. This method shows  how to use a diffeomorphic spatial transformer to warp a shape prior where warping is based on time-dependent parameterisation of multiple vector fields utilizing the Baker-Campbell-Hausdorff formula.\n\nClarity and Presentation: The paper is overall well written and motivated. Minor comment: As an outsider, I found the argument on negative Jacobian determinants hard to follow and it comes up several times. So it seems quite important.\n\nExperimental Validation: It is partially on weaker side in my opinion.  e.g. Lee et al. experiments were shown on segmentation of coronary lumen structures. Is there a good reason to instead choose breast tissue segmentation task only and not show any experiments on the former one? similarly, to prove data invariance, STN is validated on several benchmarks whereas this method only uses one, MNIST.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper834/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper834/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diffeomorphic Template Transformers", "authorids": ["~Tycho_F.A._van_der_Ouderaa1", "~Ivana_Isgum1", "w.veldhuis@umcutrecht.nl", "~Bob_D._De_Vos2", "~Pim_Moeskops2"], "authors": ["Tycho F.A. van der Ouderaa", "Ivana Isgum", "Wouter B. Veldhuis", "Bob D. De Vos", "Pim Moeskops"], "keywords": [], "abstract": "In this paper we propose a spatial transformer network where the spatial transformations are limited to the group of diffeomorphisms. Diffeomorphic transformations are a kind of homeomorphism, which by definition preserve topology, a compelling property in certain applications.\nWe apply this diffemorphic spatial transformer to model the output of a neural network as a topology preserving mapping of a prior shape. By carefully choosing the prior shape we can enforce properties on the output of the network without requiring any changes to the loss function, such as smooth boundaries and a hard constraint on the number of connected components.\nThe diffeomorphic transformer networks outperform their non-diffeomorphic precursors when applied to learn data invariances in classification tasks. On a breast tissue segmentation task, we show that the approach is robust and flexible enough to deform simple artificial priors, such as Gaussian-shaped prior energies, into high-quality predictive probability densities. In addition to desirable topological properties, the segmentation maps have competitive quantitative fidelity compared to those obtained by direct estimation (i.e. plain U-Net).", "one-sentence_summary": "We propose a special kind of spatial transformer with diffeomorphic transformations and show that it can be particularly useful in certain classification and segmentation tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ouderaa|diffeomorphic_template_transformers", "pdf": "/pdf/06f15ed656485d193f3da689705f4ba986b595fc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Z2qRbmMONg", "_bibtex": "@misc{\nouderaa2021diffeomorphic,\ntitle={Diffeomorphic Template Transformers},\nauthor={Tycho F.A. van der Ouderaa and Ivana Isgum and Wouter B. Veldhuis and Bob D. De Vos and Pim Moeskops},\nyear={2021},\nurl={https://openreview.net/forum?id=_sCOYXNwaI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_sCOYXNwaI", "replyto": "_sCOYXNwaI", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper834/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538133945, "tmdate": 1606915771902, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper834/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper834/-/Official_Review"}}}, {"id": "KWotcs72mb", "original": null, "number": 4, "cdate": 1604252173442, "ddate": null, "tcdate": 1604252173442, "tmdate": 1605024594669, "tddate": null, "forum": "_sCOYXNwaI", "replyto": "_sCOYXNwaI", "invitation": "ICLR.cc/2021/Conference/Paper834/-/Official_Review", "content": {"title": "Incremental methodological contribution and performance improvement", "review": "This submission proposes a diffeomorphic spatial transform network, which considers the specific transformation, i.e., diffeomorphism, in the data. The research topic is quite interesting, but the submission is at the preliminary stage and needs more work before publishing.\n\n1) The writing of the paper could be improved in terms of clarification. Most of the time, the authors present the solutions without explaining the motivations or underlying intuitions. For instance, why do we have to have the time-dependent vector field? Why do we need to use the prior shape? \n\n2) The novelty of the proposed method is limited. The main contribution of this paper is the use of BCH formula to approximate the piece-wise time-dependent sequence of the vector field. Very likely, this approximation is not very efficient, and we also don't know its approximation accuracy. \n\n3) The performance improvement is limited. In the MNIST experimental results, the accuracy improvement is subtle. Compared to CNN+Field-STN, the CNN+Diffeomorhpic-STN has a slightly increased mean accuracy with a larger standard deviation and a slightly decreased number of negative Jacobians with also a larger standard deviation. For the breast tissue segmentation task, if we consider the transformer only (without considering the shape prior), the Dice Score performance of the proposed method is downgraded. \n\n4) The efficiency of the proposed method is questionable. If we use this diffeomorphic transform network as a plugin module in another network, will it become the computational bottleneck of the whole model, since the estimated spatiotemporal vector field is really high-dimensional?\n\n5) Minor question: Is the identity map missing in the Algirhtm 1? phi_0 = Id + v/2^T?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper834/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper834/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diffeomorphic Template Transformers", "authorids": ["~Tycho_F.A._van_der_Ouderaa1", "~Ivana_Isgum1", "w.veldhuis@umcutrecht.nl", "~Bob_D._De_Vos2", "~Pim_Moeskops2"], "authors": ["Tycho F.A. van der Ouderaa", "Ivana Isgum", "Wouter B. Veldhuis", "Bob D. De Vos", "Pim Moeskops"], "keywords": [], "abstract": "In this paper we propose a spatial transformer network where the spatial transformations are limited to the group of diffeomorphisms. Diffeomorphic transformations are a kind of homeomorphism, which by definition preserve topology, a compelling property in certain applications.\nWe apply this diffemorphic spatial transformer to model the output of a neural network as a topology preserving mapping of a prior shape. By carefully choosing the prior shape we can enforce properties on the output of the network without requiring any changes to the loss function, such as smooth boundaries and a hard constraint on the number of connected components.\nThe diffeomorphic transformer networks outperform their non-diffeomorphic precursors when applied to learn data invariances in classification tasks. On a breast tissue segmentation task, we show that the approach is robust and flexible enough to deform simple artificial priors, such as Gaussian-shaped prior energies, into high-quality predictive probability densities. In addition to desirable topological properties, the segmentation maps have competitive quantitative fidelity compared to those obtained by direct estimation (i.e. plain U-Net).", "one-sentence_summary": "We propose a special kind of spatial transformer with diffeomorphic transformations and show that it can be particularly useful in certain classification and segmentation tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ouderaa|diffeomorphic_template_transformers", "pdf": "/pdf/06f15ed656485d193f3da689705f4ba986b595fc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Z2qRbmMONg", "_bibtex": "@misc{\nouderaa2021diffeomorphic,\ntitle={Diffeomorphic Template Transformers},\nauthor={Tycho F.A. van der Ouderaa and Ivana Isgum and Wouter B. Veldhuis and Bob D. De Vos and Pim Moeskops},\nyear={2021},\nurl={https://openreview.net/forum?id=_sCOYXNwaI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_sCOYXNwaI", "replyto": "_sCOYXNwaI", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper834/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538133945, "tmdate": 1606915771902, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper834/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper834/-/Official_Review"}}}], "count": 10}