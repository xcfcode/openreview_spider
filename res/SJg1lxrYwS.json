{"notes": [{"id": "SJg1lxrYwS", "original": "BkggJ3yKvr", "number": 2084, "cdate": 1569439719136, "ddate": null, "tcdate": 1569439719136, "tmdate": 1577168288389, "tddate": null, "forum": "SJg1lxrYwS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["aravind@cs.berkeley.edu", "pabbeel@cs.berkeley.edu"], "title": "PatchFormer: A neural architecture for self-supervised representation learning on images", "authors": ["Aravind Srinivas", "Pieter Abbeel"], "pdf": "/pdf/2ee479c1129ef1e86a8208c3e6278502f80c24da.pdf", "TL;DR": "Decoding pixels can still work for representation learning on images", "abstract": "Learning rich representations from predictive learning without labels has been a longstanding challenge in the field of machine learning. Generative pre-training has so far not been as successful as contrastive methods in modeling representations of raw images. In this paper, we propose a neural architecture for self-supervised representation learning on raw images called the PatchFormer which learns to model spatial dependencies across patches in a raw image. Our method learns to model the conditional probability distribution of missing patches given the context of surrounding patches. We evaluate the utility of the learned representations by fine-tuning the pre-trained model on low data-regime classification tasks. Specifically, we benchmark our model on semi-supervised ImageNet classification which has become a popular benchmark recently for semi-supervised and self-supervised learning methods. Our model is able to achieve 30.3% and 65.5% top-1 accuracies when trained only using 1% and 10% of the labels on ImageNet showing the promise for generative pre-training methods.", "keywords": ["Unsupervised Learning", "Representation Learning", "Transformers"], "paperhash": "srinivas|patchformer_a_neural_architecture_for_selfsupervised_representation_learning_on_images", "original_pdf": "/attachment/2ee479c1129ef1e86a8208c3e6278502f80c24da.pdf", "_bibtex": "@misc{\nsrinivas2020patchformer,\ntitle={PatchFormer: A neural architecture for self-supervised representation learning on images},\nauthor={Aravind Srinivas and Pieter Abbeel},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg1lxrYwS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "RG1EvURgtL", "original": null, "number": 1, "cdate": 1576798740084, "ddate": null, "tcdate": 1576798740084, "tmdate": 1576800896170, "tddate": null, "forum": "SJg1lxrYwS", "replyto": "SJg1lxrYwS", "invitation": "ICLR.cc/2020/Conference/Paper2084/-/Decision", "content": {"decision": "Reject", "comment": "The paper presents a generative approach to learn an image representation along a self-supervised scheme.  \n\nThe reviews state that the paper is premature for publication at ICLR 2020 for the following reasons:\n* the paper is unfinished (Rev#3); in particular the description of the approach is hardly reproducible (Rev#1);\n* the evaluation is limited to ImageNet and needs be strenghtened (all reviewers)\n* the novelty needs be better explained (Rev#1).\nIt might be interesting to discuss the approach w.r.t. \"Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles\", Noroozi and Favaro.\n\nI recommend the authors to rewrite and better structure the paper (claim, state of the art, high level overview of the approach, experimental setting, discussion of the results, discussion about the novelty and limitations of the approach). \n\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["aravind@cs.berkeley.edu", "pabbeel@cs.berkeley.edu"], "title": "PatchFormer: A neural architecture for self-supervised representation learning on images", "authors": ["Aravind Srinivas", "Pieter Abbeel"], "pdf": "/pdf/2ee479c1129ef1e86a8208c3e6278502f80c24da.pdf", "TL;DR": "Decoding pixels can still work for representation learning on images", "abstract": "Learning rich representations from predictive learning without labels has been a longstanding challenge in the field of machine learning. Generative pre-training has so far not been as successful as contrastive methods in modeling representations of raw images. In this paper, we propose a neural architecture for self-supervised representation learning on raw images called the PatchFormer which learns to model spatial dependencies across patches in a raw image. Our method learns to model the conditional probability distribution of missing patches given the context of surrounding patches. We evaluate the utility of the learned representations by fine-tuning the pre-trained model on low data-regime classification tasks. Specifically, we benchmark our model on semi-supervised ImageNet classification which has become a popular benchmark recently for semi-supervised and self-supervised learning methods. Our model is able to achieve 30.3% and 65.5% top-1 accuracies when trained only using 1% and 10% of the labels on ImageNet showing the promise for generative pre-training methods.", "keywords": ["Unsupervised Learning", "Representation Learning", "Transformers"], "paperhash": "srinivas|patchformer_a_neural_architecture_for_selfsupervised_representation_learning_on_images", "original_pdf": "/attachment/2ee479c1129ef1e86a8208c3e6278502f80c24da.pdf", "_bibtex": "@misc{\nsrinivas2020patchformer,\ntitle={PatchFormer: A neural architecture for self-supervised representation learning on images},\nauthor={Aravind Srinivas and Pieter Abbeel},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg1lxrYwS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJg1lxrYwS", "replyto": "SJg1lxrYwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795725599, "tmdate": 1576800277524, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2084/-/Decision"}}}, {"id": "B1xXZpgaFB", "original": null, "number": 1, "cdate": 1571781883257, "ddate": null, "tcdate": 1571781883257, "tmdate": 1572972385013, "tddate": null, "forum": "SJg1lxrYwS", "replyto": "SJg1lxrYwS", "invitation": "ICLR.cc/2020/Conference/Paper2084/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The motivation of this paper is to use the idea of Transformer-based NLP models in image data, which is appreciated. However, this seems to be a far unfinished paper. The introduction part is well written. But, the method is not well described.  It is very unclear how exactly the model is built. Moreover, the network structure in Figure 2 is not explained.  The experimental part is very brief, and unconvincing. Much more investigations and comparisons are needed.\n\nMinors:\ndeicisons? \nmodel the only the most significant few bits -> model only the most significant few bits\nposition position embedding -> position embedding\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2084/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2084/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["aravind@cs.berkeley.edu", "pabbeel@cs.berkeley.edu"], "title": "PatchFormer: A neural architecture for self-supervised representation learning on images", "authors": ["Aravind Srinivas", "Pieter Abbeel"], "pdf": "/pdf/2ee479c1129ef1e86a8208c3e6278502f80c24da.pdf", "TL;DR": "Decoding pixels can still work for representation learning on images", "abstract": "Learning rich representations from predictive learning without labels has been a longstanding challenge in the field of machine learning. Generative pre-training has so far not been as successful as contrastive methods in modeling representations of raw images. In this paper, we propose a neural architecture for self-supervised representation learning on raw images called the PatchFormer which learns to model spatial dependencies across patches in a raw image. Our method learns to model the conditional probability distribution of missing patches given the context of surrounding patches. We evaluate the utility of the learned representations by fine-tuning the pre-trained model on low data-regime classification tasks. Specifically, we benchmark our model on semi-supervised ImageNet classification which has become a popular benchmark recently for semi-supervised and self-supervised learning methods. Our model is able to achieve 30.3% and 65.5% top-1 accuracies when trained only using 1% and 10% of the labels on ImageNet showing the promise for generative pre-training methods.", "keywords": ["Unsupervised Learning", "Representation Learning", "Transformers"], "paperhash": "srinivas|patchformer_a_neural_architecture_for_selfsupervised_representation_learning_on_images", "original_pdf": "/attachment/2ee479c1129ef1e86a8208c3e6278502f80c24da.pdf", "_bibtex": "@misc{\nsrinivas2020patchformer,\ntitle={PatchFormer: A neural architecture for self-supervised representation learning on images},\nauthor={Aravind Srinivas and Pieter Abbeel},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg1lxrYwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJg1lxrYwS", "replyto": "SJg1lxrYwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2084/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2084/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575325607870, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2084/Reviewers"], "noninvitees": [], "tcdate": 1570237727918, "tmdate": 1575325607885, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2084/-/Official_Review"}}}, {"id": "S1gbpSTe9B", "original": null, "number": 2, "cdate": 1572029880714, "ddate": null, "tcdate": 1572029880714, "tmdate": 1572972384970, "tddate": null, "forum": "SJg1lxrYwS", "replyto": "SJg1lxrYwS", "invitation": "ICLR.cc/2020/Conference/Paper2084/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper attempts unsupervised representation learning, via a patch prediction task on ImageNet. The paper is sparse on details, but the method appears to be: (1) split the image into non-overlapping visible and masked patches, (2) from features extracted from the visible patches, predict the masked patches. Rather than predict RGB, they choose to predict 2-bit grayscale images. Also, rather than use the full patches, they use random crops of the input ones, and a center crop of the output ones.\n\nThe paper seems to be an early draft of something bigger, submitted with the hope of getting some feedback. The method description is mostly composed of tiny details, such as the number and sizes of the patches; I recommend rewriting this to focus on the big idea first, and pack the details into another sub section like \"Implementation Details\". The paper barely includes any evaluation. Also, the method does not appear to be very novel: I recommend the authors look at and compare against \"Unsupervised Visual Representation Learning by Context Prediction\" (ICCV 2015), which is conceptually very similar.\n\nThe evaluation right now is not good. \"Unknown\" is not a valid point of comparison. I understand the code for CPC++ might not be released yet, but the authors could at least implement their best approximation of it, and also find older works (which CPC compared against in their paper), to fill out the results and make a convincing argument.\n\nIn Table 2, the proposed model performs worse than CPC++, yet its values are bolded anyway. Please only put the best-performing result in bold. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2084/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2084/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["aravind@cs.berkeley.edu", "pabbeel@cs.berkeley.edu"], "title": "PatchFormer: A neural architecture for self-supervised representation learning on images", "authors": ["Aravind Srinivas", "Pieter Abbeel"], "pdf": "/pdf/2ee479c1129ef1e86a8208c3e6278502f80c24da.pdf", "TL;DR": "Decoding pixels can still work for representation learning on images", "abstract": "Learning rich representations from predictive learning without labels has been a longstanding challenge in the field of machine learning. Generative pre-training has so far not been as successful as contrastive methods in modeling representations of raw images. In this paper, we propose a neural architecture for self-supervised representation learning on raw images called the PatchFormer which learns to model spatial dependencies across patches in a raw image. Our method learns to model the conditional probability distribution of missing patches given the context of surrounding patches. We evaluate the utility of the learned representations by fine-tuning the pre-trained model on low data-regime classification tasks. Specifically, we benchmark our model on semi-supervised ImageNet classification which has become a popular benchmark recently for semi-supervised and self-supervised learning methods. Our model is able to achieve 30.3% and 65.5% top-1 accuracies when trained only using 1% and 10% of the labels on ImageNet showing the promise for generative pre-training methods.", "keywords": ["Unsupervised Learning", "Representation Learning", "Transformers"], "paperhash": "srinivas|patchformer_a_neural_architecture_for_selfsupervised_representation_learning_on_images", "original_pdf": "/attachment/2ee479c1129ef1e86a8208c3e6278502f80c24da.pdf", "_bibtex": "@misc{\nsrinivas2020patchformer,\ntitle={PatchFormer: A neural architecture for self-supervised representation learning on images},\nauthor={Aravind Srinivas and Pieter Abbeel},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg1lxrYwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJg1lxrYwS", "replyto": "SJg1lxrYwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2084/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2084/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575325607870, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2084/Reviewers"], "noninvitees": [], "tcdate": 1570237727918, "tmdate": 1575325607885, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2084/-/Official_Review"}}}, {"id": "ryxGPZfzqS", "original": null, "number": 3, "cdate": 1572114777808, "ddate": null, "tcdate": 1572114777808, "tmdate": 1572972384924, "tddate": null, "forum": "SJg1lxrYwS", "replyto": "SJg1lxrYwS", "invitation": "ICLR.cc/2020/Conference/Paper2084/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Contributions:\nThe paper aims to develop generative pre-training method for learning representations of images. Although representation learning for images has been widely investigated, the present work distinguishes itself by a combination of the following: \na) building on the use of transformers as a series of layers after initial convolutional layers; \nb) using self attention for aggregating context; \nc) learning spatial dependencies across patches;\nd) training on the task of predicting two bit gray scale version of randomly masked patches in an image\n\nResults: \nLimited experiments aim to compare against the CPC (Hefnaf et al 2019) and Selfie (Trinh et al 2019) algorithms both of which are contrastive unlike the generative approach adopted in the paper. After pre-training on unlabeled imageNet datasets the proposed approach is competitive with these algorithms with roughly similar results. \n\nEvaluation/Suggestions:\nOverall the paper combines ideas from several previous works in ways that are not sufficiently novel in the opinion of this reviewer and the experiments are very limited to the imageNet dataset with 1%, 10% and 20% of labels provided to downstream classification modeling, and evaluated on top-1 and top-5 accuracies. The paper could improve on its experimental evaluation bycomparing on multiple datasets, showing error bars when averaging across multiple samplings (eg for getting the 1% label set from the entire imageNet dataset) and also comparing with other approaches even when they dont directly aim to learn representation from unlabeled data (eg Image Transformers by Parmar et al). In addition the description is very high level and does not provide enough details for experimental reproducibility. For example the reviewer had to actually guess at some of specifics of the overall end-to-end architecture since it was not fully described precisely eg in a diagram. It would be relatively easy (but important) to provide suffcient detail for reproducibility"}, "signatures": ["ICLR.cc/2020/Conference/Paper2084/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2084/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["aravind@cs.berkeley.edu", "pabbeel@cs.berkeley.edu"], "title": "PatchFormer: A neural architecture for self-supervised representation learning on images", "authors": ["Aravind Srinivas", "Pieter Abbeel"], "pdf": "/pdf/2ee479c1129ef1e86a8208c3e6278502f80c24da.pdf", "TL;DR": "Decoding pixels can still work for representation learning on images", "abstract": "Learning rich representations from predictive learning without labels has been a longstanding challenge in the field of machine learning. Generative pre-training has so far not been as successful as contrastive methods in modeling representations of raw images. In this paper, we propose a neural architecture for self-supervised representation learning on raw images called the PatchFormer which learns to model spatial dependencies across patches in a raw image. Our method learns to model the conditional probability distribution of missing patches given the context of surrounding patches. We evaluate the utility of the learned representations by fine-tuning the pre-trained model on low data-regime classification tasks. Specifically, we benchmark our model on semi-supervised ImageNet classification which has become a popular benchmark recently for semi-supervised and self-supervised learning methods. Our model is able to achieve 30.3% and 65.5% top-1 accuracies when trained only using 1% and 10% of the labels on ImageNet showing the promise for generative pre-training methods.", "keywords": ["Unsupervised Learning", "Representation Learning", "Transformers"], "paperhash": "srinivas|patchformer_a_neural_architecture_for_selfsupervised_representation_learning_on_images", "original_pdf": "/attachment/2ee479c1129ef1e86a8208c3e6278502f80c24da.pdf", "_bibtex": "@misc{\nsrinivas2020patchformer,\ntitle={PatchFormer: A neural architecture for self-supervised representation learning on images},\nauthor={Aravind Srinivas and Pieter Abbeel},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg1lxrYwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJg1lxrYwS", "replyto": "SJg1lxrYwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2084/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2084/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575325607870, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2084/Reviewers"], "noninvitees": [], "tcdate": 1570237727918, "tmdate": 1575325607885, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2084/-/Official_Review"}}}], "count": 5}