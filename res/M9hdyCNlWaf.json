{"notes": [{"id": "M9hdyCNlWaf", "original": "4S4liBBuwe", "number": 3271, "cdate": 1601308363315, "ddate": null, "tcdate": 1601308363315, "tmdate": 1614985686196, "tddate": null, "forum": "M9hdyCNlWaf", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Sparse Uncertainty Representation in Deep Learning with Inducing Weights", "authorids": ["~Hippolyt_Ritter1", "~Martin_Kukla1", "~Cheng_Zhang1", "~Yingzhen_Li1"], "authors": ["Hippolyt Ritter", "Martin Kukla", "Cheng Zhang", "Yingzhen Li"], "keywords": ["Bayesian neural networks", "uncertainty estimation", "memory efficiency"], "abstract": "Bayesian neural networks and deep ensembles represent two modern paradigms of uncertainty quantification in deep learning. Yet these approaches struggle to scale mainly due to memory inefficiency issues, since they require parameter storage several times higher than their deterministic counterparts. To address this, we augment the weight matrix of each layer  with a small number of inducing weights, thereby projecting the uncertainty quantification into such low dimensional spaces. We further extend Matheron's conditional Gaussian sampling rule to enable fast weight sampling, whichenable our inference method to maintain reasonable run-time as compared with ensembles. Importantly, our approach achieves competitive performance to the state-of-the-art in prediction and uncertainty estimation tasks with fully connected neural networks and ResNets, while reducing the parameter size to $\\leq 47.9\\%$ of that of a single neural network. ", "one-sentence_summary": "We introduce a parameter-efficient uncertainty quantification framework for deep neural net, results show competitive performances, but the model size is reduced significantly to < half of a single network.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ritter|sparse_uncertainty_representation_in_deep_learning_with_inducing_weights", "supplementary_material": "/attachment/3f34d14de08a382bccc43dc3ee9761f316599de3.zip", "pdf": "/pdf/a3dd664869d916612de797754361eb503a27d4db.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=rw1jfllsOM", "_bibtex": "@misc{\nritter2021sparse,\ntitle={Sparse Uncertainty Representation in Deep Learning with Inducing Weights},\nauthor={Hippolyt Ritter and Martin Kukla and Cheng Zhang and Yingzhen Li},\nyear={2021},\nurl={https://openreview.net/forum?id=M9hdyCNlWaf}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 21, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "8r0pvO4u3yr", "original": null, "number": 1, "cdate": 1610040465221, "ddate": null, "tcdate": 1610040465221, "tmdate": 1610474068715, "tddate": null, "forum": "M9hdyCNlWaf", "replyto": "M9hdyCNlWaf", "invitation": "ICLR.cc/2021/Conference/Paper3271/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The scores for this paper have been borderline, however the decision has been greatly facilitated by the participation of the authors and reviewers to the discussion and, more importantly, by active private discussion among reviewers and AC. Specifically, from the private discussion it seems that the reviewers find interesting ideas in this paper, but are overall are not entirely convinced about its significance, at least in the way the paper is currently positioned and motivated. \n\nMore specifically, the reviewers found the main idea of using inducing weights interesting, both technically (e.g. associated sampling scheme) but also in terms of application (sparsity). The results are insightful from a theoretical perspective. That is, the inducing weights and their treatment does seem to result in interesting and potentially useful statistical properties for the model. On the other hand, it is important to note that the high-level idea of variational inducing weights, with usage of matrix normals in this setting, as well as connection to deep GPs has been studied before, as pointed out by R2 (refs [1,2]). Furthermore, even after discussions the motivation is still not entirely convincing, especially in conjunction with the experiments. Although various interesting ideas exist in the paper, both R2 and R3 in particular remain unconvinced about what is the main benefit (e.g.  pruning or runtime efficiency) stemming out of the proposed idea. Another reviewer agreed with this point in the private discussions.  Apart from overall clearer positioning of the paper, the claimed benefit would need to be supported by experiments tailored to illustrate this main point. The authors argued against some of the suggested comparisons (e.g. past pruning methods), and further discuss that there is no established experimental benchmark for the parameter efficiency of BNNs. I indeed sympathize with both of these arguments; however, I believe that if the reviewers' suggestions for extra experiments are rejected, it remains the responsibility of the authors to find a slightly different way of motivating their work and demonstrating its efficiency in some convincing, meaningful and more well-defined setting with the appropriate benchmarks.  \n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse Uncertainty Representation in Deep Learning with Inducing Weights", "authorids": ["~Hippolyt_Ritter1", "~Martin_Kukla1", "~Cheng_Zhang1", "~Yingzhen_Li1"], "authors": ["Hippolyt Ritter", "Martin Kukla", "Cheng Zhang", "Yingzhen Li"], "keywords": ["Bayesian neural networks", "uncertainty estimation", "memory efficiency"], "abstract": "Bayesian neural networks and deep ensembles represent two modern paradigms of uncertainty quantification in deep learning. Yet these approaches struggle to scale mainly due to memory inefficiency issues, since they require parameter storage several times higher than their deterministic counterparts. To address this, we augment the weight matrix of each layer  with a small number of inducing weights, thereby projecting the uncertainty quantification into such low dimensional spaces. We further extend Matheron's conditional Gaussian sampling rule to enable fast weight sampling, whichenable our inference method to maintain reasonable run-time as compared with ensembles. Importantly, our approach achieves competitive performance to the state-of-the-art in prediction and uncertainty estimation tasks with fully connected neural networks and ResNets, while reducing the parameter size to $\\leq 47.9\\%$ of that of a single neural network. ", "one-sentence_summary": "We introduce a parameter-efficient uncertainty quantification framework for deep neural net, results show competitive performances, but the model size is reduced significantly to < half of a single network.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ritter|sparse_uncertainty_representation_in_deep_learning_with_inducing_weights", "supplementary_material": "/attachment/3f34d14de08a382bccc43dc3ee9761f316599de3.zip", "pdf": "/pdf/a3dd664869d916612de797754361eb503a27d4db.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=rw1jfllsOM", "_bibtex": "@misc{\nritter2021sparse,\ntitle={Sparse Uncertainty Representation in Deep Learning with Inducing Weights},\nauthor={Hippolyt Ritter and Martin Kukla and Cheng Zhang and Yingzhen Li},\nyear={2021},\nurl={https://openreview.net/forum?id=M9hdyCNlWaf}\n}"}, "tags": [], "invitation": {"reply": {"forum": "M9hdyCNlWaf", "replyto": "M9hdyCNlWaf", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040465208, "tmdate": 1610474068698, "id": "ICLR.cc/2021/Conference/Paper3271/-/Decision"}}}, {"id": "tHp6Xlovbwi", "original": null, "number": 4, "cdate": 1603921154975, "ddate": null, "tcdate": 1603921154975, "tmdate": 1607087134986, "tddate": null, "forum": "M9hdyCNlWaf", "replyto": "M9hdyCNlWaf", "invitation": "ICLR.cc/2021/Conference/Paper3271/-/Official_Review", "content": {"title": "Official Blind Review #1 ", "review": "\n\n# summary\n\nThis paper proposed a method on uncertainty estimation in deep neural\nnetworks. Compared with BNN and deep ensemble, the proposed approach in this\nwork has a storage advantage. Furthermore, this work provides a better\ntrade-off between accuracy and calibration.\n\n\n# pros\n\n1.  The approach in this work is quite interesting. The idea of augmenting\n    weights with auxiliary low-dimensional latent variables in a deep neural\n    network seems natural at first sight, but this approach is novel as far as\n    my knowledge is concerned. Although VI with local latent variables is an\n    old technique, this application in deep neural network is novel.\n2.  The authors also proposed an efficient approach that can sample from the\n    variational approximation conditioning on the latent variable. Since the\n    original weight is large, such a sampling is necessary.\n3.  This paper provides extensive empirical results and sufficient theoretical\n    results. Experimental results show the proposed approach achieve a good\n    balance between accuracy, calibration and memory requirements.\n\n\n# cons\n\n1.  My major concern is all experiments are conducted on ResNet18, and this is\n    not a typical choice for practical problems. I think experiments on other\n    larger net such as ResNet56 on CIFAR10 will make this paper more\n    convincing.\n2.  It is not clear to me how the authors choose the \"mixture of delta\n    measures\", i.e. how to choose U<sup>k</sup>? Can the authors comment on this? In\n    this case, q(U) is a categorical distribution. It seems better if  q(U)\n    also depends on input data, however, the authors choose to use a fixed\n    q(U). Can the authors comment on this choice?\n\nOverall speaking, the idea and the presentation of this work are great in my\nopinion.\n\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3271/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse Uncertainty Representation in Deep Learning with Inducing Weights", "authorids": ["~Hippolyt_Ritter1", "~Martin_Kukla1", "~Cheng_Zhang1", "~Yingzhen_Li1"], "authors": ["Hippolyt Ritter", "Martin Kukla", "Cheng Zhang", "Yingzhen Li"], "keywords": ["Bayesian neural networks", "uncertainty estimation", "memory efficiency"], "abstract": "Bayesian neural networks and deep ensembles represent two modern paradigms of uncertainty quantification in deep learning. Yet these approaches struggle to scale mainly due to memory inefficiency issues, since they require parameter storage several times higher than their deterministic counterparts. To address this, we augment the weight matrix of each layer  with a small number of inducing weights, thereby projecting the uncertainty quantification into such low dimensional spaces. We further extend Matheron's conditional Gaussian sampling rule to enable fast weight sampling, whichenable our inference method to maintain reasonable run-time as compared with ensembles. Importantly, our approach achieves competitive performance to the state-of-the-art in prediction and uncertainty estimation tasks with fully connected neural networks and ResNets, while reducing the parameter size to $\\leq 47.9\\%$ of that of a single neural network. ", "one-sentence_summary": "We introduce a parameter-efficient uncertainty quantification framework for deep neural net, results show competitive performances, but the model size is reduced significantly to < half of a single network.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ritter|sparse_uncertainty_representation_in_deep_learning_with_inducing_weights", "supplementary_material": "/attachment/3f34d14de08a382bccc43dc3ee9761f316599de3.zip", "pdf": "/pdf/a3dd664869d916612de797754361eb503a27d4db.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=rw1jfllsOM", "_bibtex": "@misc{\nritter2021sparse,\ntitle={Sparse Uncertainty Representation in Deep Learning with Inducing Weights},\nauthor={Hippolyt Ritter and Martin Kukla and Cheng Zhang and Yingzhen Li},\nyear={2021},\nurl={https://openreview.net/forum?id=M9hdyCNlWaf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "M9hdyCNlWaf", "replyto": "M9hdyCNlWaf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3271/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538078824, "tmdate": 1606915790763, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3271/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3271/-/Official_Review"}}}, {"id": "rsttyd1OYq-", "original": null, "number": 17, "cdate": 1606260076486, "ddate": null, "tcdate": 1606260076486, "tmdate": 1606260076486, "tddate": null, "forum": "M9hdyCNlWaf", "replyto": "CrBfXBEgL9J", "invitation": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment", "content": {"title": "Parameter efficiency is a core research direction towards practical applicability of BNNs", "comment": "Thank you for engaging in the discussion. As previously stated, we highly appreciate your feedback for making our work more accessible to a broader machine learning audience.\n\nWhile we are indeed proposing an ultimately practically-minded approach for overcoming memory cost as one of the major obstacles to deploying BNNs in production systems, our paper still primarily aims to **advance core research in Bayesian deep learning**. To that end, we believe that demonstrating how to leverage inducing variables -- a widely popular technique for scaling GPs to large datasets -- for inference in deep networks is in itself a contribution of significant interest to many readers. Indeed, the other reviewers have expressed their excitement in their reviews, e.g. Reviewer4 writes: \u201c...given that the inducing points approach has achieved big successes for scalable GPs, I think the proposed method could be impactful for uncertainty modelling in deep learning models\u201d. \n\nOur approach achieves a parameter count below the arguably most important threshold for BNNs -- that of a deterministic network -- while maintaining competitive predictive performance against **strong** baselines on modern deep architectures. To our knowledge, our work is the first successful demonstration of applying inducing variable methods towards parameter-efficient uncertainty estimation for modern architectures, and we believe our approach is highly likely to inspire future research.\n\nPruning methods, in contrast, are mostly tied to variational mean-field and of independent interest for deterministic networks (hence ensembles), so a direct comparison would mostly be relevant to a particular combination of real-world dataset and network architecture rather than a more general research paper such as ours. Also, as an important note, we have achieved the milestone of reducing the parameter count compared to a deterministic network with the simplest scheme imaginable for setting the number of inducing rows and columns: a constant value for every layer across the network. We see multiple directions for future research towards further reducing the parameter count, e.g. designing an adaptive scheme for choosing the inducing weight dimensions, and a post-hoc pruning of the inducing weights. \n\nMore broadly, there is a pressing need for good uncertainty estimation to ensure robustness and safety in many practical settings (medical diagnosis, self-driving cars, ...). However, the increased parameter count of existing BNN/deep ensemble approaches is one of the core issues preventing most techniques from being used in practice, especially given the fact that modern deep learning models are still rapidly increasing in size. In this regard, we believe that our work is an important first step towards parametrically efficient uncertainty estimation in NNs, a direction that most DL users are interested in.\n\nWe hope this further clarifies where we see the contribution of our work. Thank you again for expanding on your review."}, "signatures": ["ICLR.cc/2021/Conference/Paper3271/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse Uncertainty Representation in Deep Learning with Inducing Weights", "authorids": ["~Hippolyt_Ritter1", "~Martin_Kukla1", "~Cheng_Zhang1", "~Yingzhen_Li1"], "authors": ["Hippolyt Ritter", "Martin Kukla", "Cheng Zhang", "Yingzhen Li"], "keywords": ["Bayesian neural networks", "uncertainty estimation", "memory efficiency"], "abstract": "Bayesian neural networks and deep ensembles represent two modern paradigms of uncertainty quantification in deep learning. Yet these approaches struggle to scale mainly due to memory inefficiency issues, since they require parameter storage several times higher than their deterministic counterparts. To address this, we augment the weight matrix of each layer  with a small number of inducing weights, thereby projecting the uncertainty quantification into such low dimensional spaces. We further extend Matheron's conditional Gaussian sampling rule to enable fast weight sampling, whichenable our inference method to maintain reasonable run-time as compared with ensembles. Importantly, our approach achieves competitive performance to the state-of-the-art in prediction and uncertainty estimation tasks with fully connected neural networks and ResNets, while reducing the parameter size to $\\leq 47.9\\%$ of that of a single neural network. ", "one-sentence_summary": "We introduce a parameter-efficient uncertainty quantification framework for deep neural net, results show competitive performances, but the model size is reduced significantly to < half of a single network.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ritter|sparse_uncertainty_representation_in_deep_learning_with_inducing_weights", "supplementary_material": "/attachment/3f34d14de08a382bccc43dc3ee9761f316599de3.zip", "pdf": "/pdf/a3dd664869d916612de797754361eb503a27d4db.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=rw1jfllsOM", "_bibtex": "@misc{\nritter2021sparse,\ntitle={Sparse Uncertainty Representation in Deep Learning with Inducing Weights},\nauthor={Hippolyt Ritter and Martin Kukla and Cheng Zhang and Yingzhen Li},\nyear={2021},\nurl={https://openreview.net/forum?id=M9hdyCNlWaf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "M9hdyCNlWaf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3271/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3271/Authors|ICLR.cc/2021/Conference/Paper3271/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839256, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment"}}}, {"id": "PWc8NzrHF4y", "original": null, "number": 16, "cdate": 1606259850822, "ddate": null, "tcdate": 1606259850822, "tmdate": 1606259850822, "tddate": null, "forum": "M9hdyCNlWaf", "replyto": "eKnzGij1VA", "invitation": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment", "content": {"title": "Re: response", "comment": "Thank you for further elaborating on your review. Scientifically, we still believe that comparing to the performance of mean-field pruning would not add much insight into **our** method to the paper and that the most important milestone to pass is having fewer parameters than a deterministic network. As the discussion period is almost over and this appears to be a core concern to you, we will investigate how different levels of pruning affect the predictive performance of FFG-W for the camera-ready revision. We hope this puts you more at ease with recommending acceptance.\n\nWe are primarily interested in reducing the parameter count at test time, so parallelising across GPUs and gradient checkpointing are not applicable, although they are of course compatible with our method for training.\n\nThank you again for your review and engaging in the discussion."}, "signatures": ["ICLR.cc/2021/Conference/Paper3271/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse Uncertainty Representation in Deep Learning with Inducing Weights", "authorids": ["~Hippolyt_Ritter1", "~Martin_Kukla1", "~Cheng_Zhang1", "~Yingzhen_Li1"], "authors": ["Hippolyt Ritter", "Martin Kukla", "Cheng Zhang", "Yingzhen Li"], "keywords": ["Bayesian neural networks", "uncertainty estimation", "memory efficiency"], "abstract": "Bayesian neural networks and deep ensembles represent two modern paradigms of uncertainty quantification in deep learning. Yet these approaches struggle to scale mainly due to memory inefficiency issues, since they require parameter storage several times higher than their deterministic counterparts. To address this, we augment the weight matrix of each layer  with a small number of inducing weights, thereby projecting the uncertainty quantification into such low dimensional spaces. We further extend Matheron's conditional Gaussian sampling rule to enable fast weight sampling, whichenable our inference method to maintain reasonable run-time as compared with ensembles. Importantly, our approach achieves competitive performance to the state-of-the-art in prediction and uncertainty estimation tasks with fully connected neural networks and ResNets, while reducing the parameter size to $\\leq 47.9\\%$ of that of a single neural network. ", "one-sentence_summary": "We introduce a parameter-efficient uncertainty quantification framework for deep neural net, results show competitive performances, but the model size is reduced significantly to < half of a single network.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ritter|sparse_uncertainty_representation_in_deep_learning_with_inducing_weights", "supplementary_material": "/attachment/3f34d14de08a382bccc43dc3ee9761f316599de3.zip", "pdf": "/pdf/a3dd664869d916612de797754361eb503a27d4db.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=rw1jfllsOM", "_bibtex": "@misc{\nritter2021sparse,\ntitle={Sparse Uncertainty Representation in Deep Learning with Inducing Weights},\nauthor={Hippolyt Ritter and Martin Kukla and Cheng Zhang and Yingzhen Li},\nyear={2021},\nurl={https://openreview.net/forum?id=M9hdyCNlWaf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "M9hdyCNlWaf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3271/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3271/Authors|ICLR.cc/2021/Conference/Paper3271/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839256, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment"}}}, {"id": "CrBfXBEgL9J", "original": null, "number": 15, "cdate": 1606223887455, "ddate": null, "tcdate": 1606223887455, "tmdate": 1606223887455, "tddate": null, "forum": "M9hdyCNlWaf", "replyto": "74BwCxlFDhE", "invitation": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment", "content": {"title": "Why is it important to the field?", "comment": "Thank you for your reply.\nMy rating has now been increased to reflect the value of the work carried out to improve this manuscript during the rebuttal. However, I keep it slightly below threshold because the text remains unclear regarding why this work is important to the field. \nTo my view, telling that this is the first time that inducing variables have been successfully used in DL is not enough.\nWhich kind of DL user might be interested in the solution proposed by the paper? As suggested in my initial comment, if the benefit is 'just' to give access to a lightweight formulation of a neural network providing uncertainty estimation, the authors should position their work with respect to alternative solutions, including pruned/quantized  networks (combined with proper uncertainty estimation approaches)."}, "signatures": ["ICLR.cc/2021/Conference/Paper3271/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse Uncertainty Representation in Deep Learning with Inducing Weights", "authorids": ["~Hippolyt_Ritter1", "~Martin_Kukla1", "~Cheng_Zhang1", "~Yingzhen_Li1"], "authors": ["Hippolyt Ritter", "Martin Kukla", "Cheng Zhang", "Yingzhen Li"], "keywords": ["Bayesian neural networks", "uncertainty estimation", "memory efficiency"], "abstract": "Bayesian neural networks and deep ensembles represent two modern paradigms of uncertainty quantification in deep learning. Yet these approaches struggle to scale mainly due to memory inefficiency issues, since they require parameter storage several times higher than their deterministic counterparts. To address this, we augment the weight matrix of each layer  with a small number of inducing weights, thereby projecting the uncertainty quantification into such low dimensional spaces. We further extend Matheron's conditional Gaussian sampling rule to enable fast weight sampling, whichenable our inference method to maintain reasonable run-time as compared with ensembles. Importantly, our approach achieves competitive performance to the state-of-the-art in prediction and uncertainty estimation tasks with fully connected neural networks and ResNets, while reducing the parameter size to $\\leq 47.9\\%$ of that of a single neural network. ", "one-sentence_summary": "We introduce a parameter-efficient uncertainty quantification framework for deep neural net, results show competitive performances, but the model size is reduced significantly to < half of a single network.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ritter|sparse_uncertainty_representation_in_deep_learning_with_inducing_weights", "supplementary_material": "/attachment/3f34d14de08a382bccc43dc3ee9761f316599de3.zip", "pdf": "/pdf/a3dd664869d916612de797754361eb503a27d4db.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=rw1jfllsOM", "_bibtex": "@misc{\nritter2021sparse,\ntitle={Sparse Uncertainty Representation in Deep Learning with Inducing Weights},\nauthor={Hippolyt Ritter and Martin Kukla and Cheng Zhang and Yingzhen Li},\nyear={2021},\nurl={https://openreview.net/forum?id=M9hdyCNlWaf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "M9hdyCNlWaf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3271/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3271/Authors|ICLR.cc/2021/Conference/Paper3271/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839256, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment"}}}, {"id": "KQIhPB1DX3s", "original": null, "number": 1, "cdate": 1603726888021, "ddate": null, "tcdate": 1603726888021, "tmdate": 1606222417369, "tddate": null, "forum": "M9hdyCNlWaf", "replyto": "M9hdyCNlWaf", "invitation": "ICLR.cc/2021/Conference/Paper3271/-/Official_Review", "content": {"title": "Unclear gain in knowledge", "review": "The paper proposes to define the weights approximate posterior of a NN with inducing variables.\n\nThe main benefit of the approach lies in the compactness of the description (Fig4, right). However, if memory size is the main issue, entropy should be considered instead of just a count of parameters. Moreover, if complexity and memory footprint is the main concern, MC dropout sounds like a reasonable alternative to Bayesian NN. Why do the authors do not compare their proposal to a conventional MC dropout approach?\n\nOverall, the contribution of the paper appears to be valid but relatively limited in scope compared to what exists in the literature (inducing variables are known, Bayesian NN are known, ensemble methods are known). What is the gain in knowledge brought by the paper? I might have missed a piece of the contribution, but the way the paper is written does not help. It is not self-contained (many previous works have to be read to follow the story), and remains relatively opaque to the non-expert. It lacks a clear and eye-bird picture of the approach (including both training and inference steps), to position and compare it to existing works.  \n\nThe overhead in optimizing (1) or (2), compared to training a deterministic NN, is not discussed. \n\nVariables d_in^l is used in Section 2, but only defined in Section 3. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper3271/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse Uncertainty Representation in Deep Learning with Inducing Weights", "authorids": ["~Hippolyt_Ritter1", "~Martin_Kukla1", "~Cheng_Zhang1", "~Yingzhen_Li1"], "authors": ["Hippolyt Ritter", "Martin Kukla", "Cheng Zhang", "Yingzhen Li"], "keywords": ["Bayesian neural networks", "uncertainty estimation", "memory efficiency"], "abstract": "Bayesian neural networks and deep ensembles represent two modern paradigms of uncertainty quantification in deep learning. Yet these approaches struggle to scale mainly due to memory inefficiency issues, since they require parameter storage several times higher than their deterministic counterparts. To address this, we augment the weight matrix of each layer  with a small number of inducing weights, thereby projecting the uncertainty quantification into such low dimensional spaces. We further extend Matheron's conditional Gaussian sampling rule to enable fast weight sampling, whichenable our inference method to maintain reasonable run-time as compared with ensembles. Importantly, our approach achieves competitive performance to the state-of-the-art in prediction and uncertainty estimation tasks with fully connected neural networks and ResNets, while reducing the parameter size to $\\leq 47.9\\%$ of that of a single neural network. ", "one-sentence_summary": "We introduce a parameter-efficient uncertainty quantification framework for deep neural net, results show competitive performances, but the model size is reduced significantly to < half of a single network.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ritter|sparse_uncertainty_representation_in_deep_learning_with_inducing_weights", "supplementary_material": "/attachment/3f34d14de08a382bccc43dc3ee9761f316599de3.zip", "pdf": "/pdf/a3dd664869d916612de797754361eb503a27d4db.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=rw1jfllsOM", "_bibtex": "@misc{\nritter2021sparse,\ntitle={Sparse Uncertainty Representation in Deep Learning with Inducing Weights},\nauthor={Hippolyt Ritter and Martin Kukla and Cheng Zhang and Yingzhen Li},\nyear={2021},\nurl={https://openreview.net/forum?id=M9hdyCNlWaf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "M9hdyCNlWaf", "replyto": "M9hdyCNlWaf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3271/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538078824, "tmdate": 1606915790763, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3271/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3271/-/Official_Review"}}}, {"id": "jZxhie4nLwS", "original": null, "number": 6, "cdate": 1605550266873, "ddate": null, "tcdate": 1605550266873, "tmdate": 1606170974230, "tddate": null, "forum": "M9hdyCNlWaf", "replyto": "tHp6Xlovbwi", "invitation": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment", "content": {"title": "Response to Reviewer1", "comment": "Thank you very much for your encouraging review. We are particularly pleased that you appreciate the novelty of the idea and the clarity of the presentation.\n\n**Architecture size** In order to avoid possible confusion regarding the architecture, we would like to emphasize that the Resnet18 architecture we use is based on the torchvision implementation, i.e. the smallest ImageNet-sized architecture (>11M parameters), **not** the CIFAR-sized Resnet20 architecture from the original Resnet paper (<1M parameters). The Resnet56 architecture you suggest was also developed for CIFAR (<1M parameters) -- did you mean Resnet50 (>22M parameters) or were you under the impression that we were using the smaller CIFAR architectures?\nWhile we fully agree that Resnet50 would be more representative of an architecture that a practitioner would use, we do believe that Resnet18 captures the most relevant properties of a modern deep neural network. For example, Osawa et al. (2019) also based their Bayesian deep learning experiments on Resnet18.\n\nWe are currently running some preliminary experiments for FFG-W, FFG-U and Ensemble-U on Resnet50 for CIFAR10. Those take slightly more than twice as long compared to the Resnet18 experiments due to the larger network architecture. Given the short discussion period, we wish to provide initial results in response to your question, and later on we will update the full set of experiments in the camera-ready version. Based on our first runs, we can confirm that the inducing weight method works on the larger architecture and accuracy increases as expected while maintaining a low ECE. The results we have available for Resnet50 at this point are as follows:\n\n|Method | Accuracy(%) | ECE(%) | #params | %params of det. net (including BatchNorm) | #seeds |\n|---|---|---|---|---|---|\n|Deterministic | 94.47 | 4.59 | 23,520,842 | 100 | 5 |\n|FFG-U (M=128)| 94.72 | 1.61 | 12,253,366 | 52.1 | 3 |\n|FFG-U (M=64) | 94.33 | 0.66 | 5,710,902 | 24.28 | 2 |\n|FFG-W | 93.24 | 0.66 | 46,988,564 | 199.8 | 3 |\n|Ensemble-U (M=128) | 95.28 | 1.86 | 14,907,574 | 63.4 | 3 |\n|Ensemble-W (K=5) | 95.58 | 1.32 | 117,604,210 | 500 | 1 (from det. seeds)\n\n**Ensemble-U** By \u201cmixture of delta measures\u201d we mean having $k$ sets of continuous \u201cparameter\u201d values in inducing space. You can think of this as equivalent to a classical ensemble of neural networks, except that the ensemble is in the lower-dimensional U space and then each set of inducing weights is (stochastically) projected into the original parameter space for a separate forward pass (the projection parameters are shared).\n\nHaving q(U) depend on the input would be a change of the model from globally shared parameters to parameters local to each data point as in the classical mixture density networks (Bishop, 1994). We have opted for the global weight model mostly to avoid an additional factor of variation between the two inference methods we compare. Introducing an input dependence would certainly be an interesting extension of our work and is a direction that has received attention recently e.g. in (Kristiadi et al., 2019) and (Karaletsos & Bui, 2020).\n\nWe hope our response has been helpful and are of course happy to further clarify any additional questions you may have.\n\n\nReferences:\n\nOsawa et al. Practical Deep Learning with Bayesian Principles. In NeurIPS 2019.\n\nBishop. Mixture density networks. 1994.\n\nKristiadi et al. Predictive Uncertainty Quantification with Compound Density Networks. arxiv preprint arXiv:1902.01080.\n\nKaraletsos & Bui. Hierarchical Gaussian Process Priors for Bayesian Neural Network Weights. NeurIPS 2020."}, "signatures": ["ICLR.cc/2021/Conference/Paper3271/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse Uncertainty Representation in Deep Learning with Inducing Weights", "authorids": ["~Hippolyt_Ritter1", "~Martin_Kukla1", "~Cheng_Zhang1", "~Yingzhen_Li1"], "authors": ["Hippolyt Ritter", "Martin Kukla", "Cheng Zhang", "Yingzhen Li"], "keywords": ["Bayesian neural networks", "uncertainty estimation", "memory efficiency"], "abstract": "Bayesian neural networks and deep ensembles represent two modern paradigms of uncertainty quantification in deep learning. Yet these approaches struggle to scale mainly due to memory inefficiency issues, since they require parameter storage several times higher than their deterministic counterparts. To address this, we augment the weight matrix of each layer  with a small number of inducing weights, thereby projecting the uncertainty quantification into such low dimensional spaces. We further extend Matheron's conditional Gaussian sampling rule to enable fast weight sampling, whichenable our inference method to maintain reasonable run-time as compared with ensembles. Importantly, our approach achieves competitive performance to the state-of-the-art in prediction and uncertainty estimation tasks with fully connected neural networks and ResNets, while reducing the parameter size to $\\leq 47.9\\%$ of that of a single neural network. ", "one-sentence_summary": "We introduce a parameter-efficient uncertainty quantification framework for deep neural net, results show competitive performances, but the model size is reduced significantly to < half of a single network.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ritter|sparse_uncertainty_representation_in_deep_learning_with_inducing_weights", "supplementary_material": "/attachment/3f34d14de08a382bccc43dc3ee9761f316599de3.zip", "pdf": "/pdf/a3dd664869d916612de797754361eb503a27d4db.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=rw1jfllsOM", "_bibtex": "@misc{\nritter2021sparse,\ntitle={Sparse Uncertainty Representation in Deep Learning with Inducing Weights},\nauthor={Hippolyt Ritter and Martin Kukla and Cheng Zhang and Yingzhen Li},\nyear={2021},\nurl={https://openreview.net/forum?id=M9hdyCNlWaf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "M9hdyCNlWaf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3271/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3271/Authors|ICLR.cc/2021/Conference/Paper3271/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839256, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment"}}}, {"id": "xtU1GbEgiSG", "original": null, "number": 7, "cdate": 1605550304807, "ddate": null, "tcdate": 1605550304807, "tmdate": 1606170952083, "tddate": null, "forum": "M9hdyCNlWaf", "replyto": "M9hdyCNlWaf", "invitation": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment", "content": {"title": "Summary of the response", "comment": "We thank all reviewers for their valuable feedback. We are pleased that reviewers 1, 2 and 4 find our work novel and interesting and its presentation clear. They further highlight that our extension of Matheron\u2019s rule for sampling conditional Gaussian variables can be of independent interest. We have addressed minor comments in the individual responses and will upload a revision of the paper in the next few days.\n\nReviewer 1 questioned whether Resnet18 was reflective of an architecture that would be used in a practical setting. While we believe this to be the case (see the response to reviewer 1), we have run initial experiments on the larger Resnet50 architecture and can confirm that our inducing weight method still works as expected on this deeper network. Accuracies, ECE and parameter counts on Resnet50 are as follows:\n\n|Method | Accuracy(%) | ECE(%) | #params | %params of det. net (including BatchNorm) | #seeds |\n|---|---|---|---|---|---|\n|Deterministic | 94.47 | 4.59 | 23,520,842 | 100 | 5 |\n|FFG-U (M=128)| 94.72 | 1.61 | 12,253,366 | 52.1 | 3 |\n|FFG-U (M=64) | 94.33 | 0.66 | 5,710,902 | 24.28 | 2 |\n|FFG-W | 93.24 | 0.66 | 46,988,564 | 199.8 | 3 |\n|Ensemble-U (M=128) | 95.28 | 1.86 | 14,907,574 | 63.4 | 3 |\n|Ensemble-W (K=5) | 95.58 | 1.32 | 117,604,210 | 500 | 1 (from det. seeds)\n\nWe will update the full set of experiments for the camera-ready version and do not anticipate any significant changes of the results or conclusions.\n\nReviewer 2 expressed concerns about a lack of comparison to baseline approaches for (post-hoc) network pruning, but we don\u2019t think this would give an apples-to-apples comparison (see response to reviewer 2). Nevertheless, we believe our approach has the potential to be pushed further for better memory savings. Indeed, we could have spent space in the manuscript discussing heuristics on choosing shape($U$) by adapting to the network architecture and the learning task. We decided not to push in this direction for the following reasons: \n\n1. The goal of our work is to perform **uncertainty estimation** in deep neural networks with **acceptable** memory consumptions. This is important especially for giant neural networks since maintaining even a single copy of them would require massive storage already. Our approach provides the first solution towards reducing BNN/deep ensemble memory to **below** the memory of their deterministic counterpart, and we believe our current experiments clearly and convincingly demonstrate the efficacy of our approach.\n2. We don\u2019t see that pushing the parameter count to the extremely small side (e.g. < 1%) for a specific architecture on CIFAR10/100 would have added much insight to the paper. Instead, we believe that it is more valuable to keep the hyperparameter search space for shape($U$) simple, since we wish to design an experimental protocol that is easy to follow for the reader and that lends itself to ablation studies (Fig. 5). The positive responses from the reviewers regarding clarity has reinforced this belief.\n3. To our knowledge, research on compressing neural networks has so far been focusing on maintaining **a minimum test accuracy** while reducing memory usage. Variational inference based techniques for network compression also focus on accuracy maintenance only, see e.g. (Louizos et al., 2017) and (Havasi et al., 2019). Investigating the maintenance of a minimum level of uncertainty calibration could be interesting future work, but again, we believe that adding this study to our paper is out of scope and would convolute the main message that we want to convey.\n\n\nAgain, we want to thank all of the reviewers and hope that we have now fully convinced all of them that our paper makes for a valuable contribution to the conference.\n\n\nReferences:\n\nLouizos et al. Bayesian compression for deep learning. NeurIPS 2017\n\nHavasi et al. Minimal random code learning: Getting bits back from compressed model parameters. ICLR 2019"}, "signatures": ["ICLR.cc/2021/Conference/Paper3271/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse Uncertainty Representation in Deep Learning with Inducing Weights", "authorids": ["~Hippolyt_Ritter1", "~Martin_Kukla1", "~Cheng_Zhang1", "~Yingzhen_Li1"], "authors": ["Hippolyt Ritter", "Martin Kukla", "Cheng Zhang", "Yingzhen Li"], "keywords": ["Bayesian neural networks", "uncertainty estimation", "memory efficiency"], "abstract": "Bayesian neural networks and deep ensembles represent two modern paradigms of uncertainty quantification in deep learning. Yet these approaches struggle to scale mainly due to memory inefficiency issues, since they require parameter storage several times higher than their deterministic counterparts. To address this, we augment the weight matrix of each layer  with a small number of inducing weights, thereby projecting the uncertainty quantification into such low dimensional spaces. We further extend Matheron's conditional Gaussian sampling rule to enable fast weight sampling, whichenable our inference method to maintain reasonable run-time as compared with ensembles. Importantly, our approach achieves competitive performance to the state-of-the-art in prediction and uncertainty estimation tasks with fully connected neural networks and ResNets, while reducing the parameter size to $\\leq 47.9\\%$ of that of a single neural network. ", "one-sentence_summary": "We introduce a parameter-efficient uncertainty quantification framework for deep neural net, results show competitive performances, but the model size is reduced significantly to < half of a single network.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ritter|sparse_uncertainty_representation_in_deep_learning_with_inducing_weights", "supplementary_material": "/attachment/3f34d14de08a382bccc43dc3ee9761f316599de3.zip", "pdf": "/pdf/a3dd664869d916612de797754361eb503a27d4db.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=rw1jfllsOM", "_bibtex": "@misc{\nritter2021sparse,\ntitle={Sparse Uncertainty Representation in Deep Learning with Inducing Weights},\nauthor={Hippolyt Ritter and Martin Kukla and Cheng Zhang and Yingzhen Li},\nyear={2021},\nurl={https://openreview.net/forum?id=M9hdyCNlWaf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "M9hdyCNlWaf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3271/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3271/Authors|ICLR.cc/2021/Conference/Paper3271/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839256, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment"}}}, {"id": "6eCa3b3TeVP", "original": null, "number": 14, "cdate": 1606169773062, "ddate": null, "tcdate": 1606169773062, "tmdate": 1606169773062, "tddate": null, "forum": "M9hdyCNlWaf", "replyto": "M9hdyCNlWaf", "invitation": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment", "content": {"title": "Updated results on Resnet50", "comment": "Dear reviewers, we have just updated our Resnet50 results for CIFAR10 in the \"Summary of the response\" below and the response to Reviewer1. We have added accuracy, ECE and parameter counts for deterministic networks, deterministic ensembles (Ensemble-W), our Ensemble-U method and FFG-U with a lower U dimension. In line with our results from the original submission for Resnet18, our inducing method achieves competitive predictive performance at a much reduced parameter count.\n\nWe will provide the full suite of Resnet50 results (additional random seeds; CIFAR100; uncertainty evaluation) in the camera-ready paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper3271/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse Uncertainty Representation in Deep Learning with Inducing Weights", "authorids": ["~Hippolyt_Ritter1", "~Martin_Kukla1", "~Cheng_Zhang1", "~Yingzhen_Li1"], "authors": ["Hippolyt Ritter", "Martin Kukla", "Cheng Zhang", "Yingzhen Li"], "keywords": ["Bayesian neural networks", "uncertainty estimation", "memory efficiency"], "abstract": "Bayesian neural networks and deep ensembles represent two modern paradigms of uncertainty quantification in deep learning. Yet these approaches struggle to scale mainly due to memory inefficiency issues, since they require parameter storage several times higher than their deterministic counterparts. To address this, we augment the weight matrix of each layer  with a small number of inducing weights, thereby projecting the uncertainty quantification into such low dimensional spaces. We further extend Matheron's conditional Gaussian sampling rule to enable fast weight sampling, whichenable our inference method to maintain reasonable run-time as compared with ensembles. Importantly, our approach achieves competitive performance to the state-of-the-art in prediction and uncertainty estimation tasks with fully connected neural networks and ResNets, while reducing the parameter size to $\\leq 47.9\\%$ of that of a single neural network. ", "one-sentence_summary": "We introduce a parameter-efficient uncertainty quantification framework for deep neural net, results show competitive performances, but the model size is reduced significantly to < half of a single network.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ritter|sparse_uncertainty_representation_in_deep_learning_with_inducing_weights", "supplementary_material": "/attachment/3f34d14de08a382bccc43dc3ee9761f316599de3.zip", "pdf": "/pdf/a3dd664869d916612de797754361eb503a27d4db.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=rw1jfllsOM", "_bibtex": "@misc{\nritter2021sparse,\ntitle={Sparse Uncertainty Representation in Deep Learning with Inducing Weights},\nauthor={Hippolyt Ritter and Martin Kukla and Cheng Zhang and Yingzhen Li},\nyear={2021},\nurl={https://openreview.net/forum?id=M9hdyCNlWaf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "M9hdyCNlWaf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3271/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3271/Authors|ICLR.cc/2021/Conference/Paper3271/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839256, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment"}}}, {"id": "eKnzGij1VA", "original": null, "number": 13, "cdate": 1606141567061, "ddate": null, "tcdate": 1606141567061, "tmdate": 1606141604835, "tddate": null, "forum": "M9hdyCNlWaf", "replyto": "Nhhw4yLjlV", "invitation": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment", "content": {"title": "Response to rebuttal", "comment": "I would like to thank you for addressing my comments and since the clarity has improved I will raise my score. \n\nNevertheless, I still cannot fully recommend acceptance, as I would still argue in favour of the baselines I mentioned. Both this framework and, e.g., FFG-W are doing inference under the same normal prior (i.e., they have the same model) and parameter reductions come essentially from the parametrisation / sparsity of the variational distribution. Furthermore, in my opinion, the primary motivation for reducing parameter count would be to facilitate for practical deployment of the BNN in cases where we also require uncertainty (e.g., a self driving car). As a result, since mean-field variational inference is known to produce pruning [1], it would be a valid baseline to include. In general, if  parameter count during training is an issue there are also alternative ways (which in a sense are simper) that can be employed, namely data parallelising across multiple gpu\u2019s and / or using gradient checkpointing. \n\n[1] Overpruning in Variational Bayesian Neural Networks, Brian Trippe, Richard Turner, 2017"}, "signatures": ["ICLR.cc/2021/Conference/Paper3271/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse Uncertainty Representation in Deep Learning with Inducing Weights", "authorids": ["~Hippolyt_Ritter1", "~Martin_Kukla1", "~Cheng_Zhang1", "~Yingzhen_Li1"], "authors": ["Hippolyt Ritter", "Martin Kukla", "Cheng Zhang", "Yingzhen Li"], "keywords": ["Bayesian neural networks", "uncertainty estimation", "memory efficiency"], "abstract": "Bayesian neural networks and deep ensembles represent two modern paradigms of uncertainty quantification in deep learning. Yet these approaches struggle to scale mainly due to memory inefficiency issues, since they require parameter storage several times higher than their deterministic counterparts. To address this, we augment the weight matrix of each layer  with a small number of inducing weights, thereby projecting the uncertainty quantification into such low dimensional spaces. We further extend Matheron's conditional Gaussian sampling rule to enable fast weight sampling, whichenable our inference method to maintain reasonable run-time as compared with ensembles. Importantly, our approach achieves competitive performance to the state-of-the-art in prediction and uncertainty estimation tasks with fully connected neural networks and ResNets, while reducing the parameter size to $\\leq 47.9\\%$ of that of a single neural network. ", "one-sentence_summary": "We introduce a parameter-efficient uncertainty quantification framework for deep neural net, results show competitive performances, but the model size is reduced significantly to < half of a single network.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ritter|sparse_uncertainty_representation_in_deep_learning_with_inducing_weights", "supplementary_material": "/attachment/3f34d14de08a382bccc43dc3ee9761f316599de3.zip", "pdf": "/pdf/a3dd664869d916612de797754361eb503a27d4db.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=rw1jfllsOM", "_bibtex": "@misc{\nritter2021sparse,\ntitle={Sparse Uncertainty Representation in Deep Learning with Inducing Weights},\nauthor={Hippolyt Ritter and Martin Kukla and Cheng Zhang and Yingzhen Li},\nyear={2021},\nurl={https://openreview.net/forum?id=M9hdyCNlWaf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "M9hdyCNlWaf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3271/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3271/Authors|ICLR.cc/2021/Conference/Paper3271/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839256, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment"}}}, {"id": "KUPFW1L8T_E", "original": null, "number": 2, "cdate": 1603729809310, "ddate": null, "tcdate": 1603729809310, "tmdate": 1606141587388, "tddate": null, "forum": "M9hdyCNlWaf", "replyto": "M9hdyCNlWaf", "invitation": "ICLR.cc/2021/Conference/Paper3271/-/Official_Review", "content": {"title": "Interesting work, some key baselines are missing", "review": "### Summary\nThis work proposes a specific parametrisation for the Gaussian prior and approximate posterior distribution in variational Bayesian neural networks in terms of inducing weights. The general idea is an instance of the sparse variational inference scheme for GPs proposed by Titsias back in 2009; for a given model with a prior p(W) perform variational inference on an extended model with a hierarchical prior p(U) p(W | U), that has the same marginal p(W) = \\int p(U)p(W | U)dU as the original model. The authors then consider \u201cU\u201d to be auxiliary weights that are jointly Gaussian with the actual weights \u201cW\u201d and then use the decomposition p(W|U)p(U), q(W|U)q(U) for the prior and approximate posterior (which can easily be computed via the conditional Gaussian rules). Furthermore, they \u201ctie\u201d (almost) all of the parameters between q(W|U) and p(W|U) (similarly to Titsias, 2009). The main benefit from these two things is that since the mean and covariance of the Gaussian distribution over W conditioned on U can be efficiently represented as functions of U, whenever dim(U) << dim(W) we get reductions in memory for storing the distributions over the parameters in the network. The authors furthermore, discuss how to efficiently parametrize the joint distribution over W, U, discuss different choices for q(U) (that can lead to either traditional VI or something like deep ensembles). In addition, they also discuss how more efficient sampling from q(W|U) can be realised via an extension of the Matheron\u2019s rule to the case of matrix random variables. Finally, they evaluate their method against traditional mean field variational Bayesian neural networks and deep ensembles on several tasks that include regression, classification, calibration and OOD performance. \n\n### Pros\n- The method provides a novel way to induce parameter efficiency in variational bayesian neural networks\n- It connects to the sparse GP literature and the trick seems to be general enough, in that it can be applied to any distribution that admits some parametrisation in terms of a Gaussian random variable.\n- The extension of the Matheron\u2019s rule can be of independent interest\n- Extensive set of experimental tasks, with a nice ablation study for lambda_max and sigma_max\n\n### Cons\n- Comparison against alternative approaches that induce parameter efficiency are missing\n- Results are mixed and sometimes not very convincing\n\n\n### Recommendation \nWhile I find the main idea very interesting and the presentation of it relatively clear, I unfortunately cannot recommend acceptance of this work as is. The main motivation behind improving parameter efficiency can also be performed with other, perhaps much simpler, ways and comparisons against such approaches is missing. Furthermore, the results, at least in their current state, are a bit mixed and thus not very convincing. \n\n### Detailed feedback\nOverall, this work is interesting and relatively easy to follow. Using inducing variables in neural networks in not a new concept, as it has been used before in, e.g., [1, 2], but the motivation of using them as a means towards reducing the number of parameters of each distribution is new. The authors explain the main idea behind them in a clear manner. Furthermore, they clearly explain the need for using matrix normal distributions, explaining how the Kronecker product factorisation of the covariance improves the parametrization efficiency, along with how efficient sampling from q(W|U) can be performed. In addition, performing parameter efficient deep ensembles in U space is a nice bonus of this formulation. Section 3.3 however is dense and can be a bit hard to follow (whereas Appendix D is clearer). I would therefore suggest that the authors briefly describe the main idea in a couple of sentences and refer the readers to Appendix D instead.\n\nMy main point for feedback is for the experimental section. The authors argue at the beginning of section 4 that \u201cthe goal is to demonstrate competitive performance \u2026. While remaining computationally efficient.\u201d Is that claim for the training or evaluation phases? From what I see, during the training phase, the inducing weights framework is not faster or more efficient than the baselines. The main advantage of the inducing weight is instead memory reduction in terms of parameters which translates to \u201cmemory efficiency\u201d and not \u201ccomputational \u201cefficiency\u201d. If then memory efficiency is the main target, I believe that some reasonable baselines are missing. For the case of a variational Bayesian neural network a simple baseline that performs pruning post-hoc (e.g., the one presented at the FFG-W paper or the one from [3]) in order to reduce the parameter count; remove weights by either setting them to exact zero or equal to the prior and then use the variational posterior for those that survive. It would be interesting to see how such an approach would fare not just on accuracy, but also on the ECE and OOD. Similarly for deep ensembles, a baseline where you perform pruning (e.g., simple magnitude based) would also serve as a better baseline for the Ensemble-U.  Both of these baselines would provide a more complete picture and would be a better signal in understanding whether the inducing weights framework is a better choice overall.\n\nAs for other points\n\n- At figure 3 you only show FCG-U and not the FFG-U which is used in all of the other experiments. How does the uncertainty look like with FFG-U? \n- Sometimes, both FFG-U and Ensemble-U underperform compared to FFG-W and Ensemble-W and it would be good to know if it is due to less free parameters to optimise or due to the model definition itself. How does FFG-U perform when increasing dim(U) such that the parameter count is the same as FFG-W? Similarly for Ensemble-U vs Ensemble-W.\n- The second point at the end of page 2 only makes sense in hindsight (i.e., after one reads section 3). Perhaps the authors could expand on them a bit better so that it is self-contained (e.g., explain what you mean with \u201cq(theta|alpha) can be efficiently parametrised\u201d).\n\n\n[1] Structured and efficient variational deep learning with matrix Gaussian posteriors, C. Louizos & M. Welling, 2016\n[2] Global inducing point variational posteriors for Bayesian neural networks and deep gaussian processes, S. Ober & L. Aitchison, 2020\n[3]\u0010 Practical Variational Inference for Neural Networks, A. Graves, 2011\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3271/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse Uncertainty Representation in Deep Learning with Inducing Weights", "authorids": ["~Hippolyt_Ritter1", "~Martin_Kukla1", "~Cheng_Zhang1", "~Yingzhen_Li1"], "authors": ["Hippolyt Ritter", "Martin Kukla", "Cheng Zhang", "Yingzhen Li"], "keywords": ["Bayesian neural networks", "uncertainty estimation", "memory efficiency"], "abstract": "Bayesian neural networks and deep ensembles represent two modern paradigms of uncertainty quantification in deep learning. Yet these approaches struggle to scale mainly due to memory inefficiency issues, since they require parameter storage several times higher than their deterministic counterparts. To address this, we augment the weight matrix of each layer  with a small number of inducing weights, thereby projecting the uncertainty quantification into such low dimensional spaces. We further extend Matheron's conditional Gaussian sampling rule to enable fast weight sampling, whichenable our inference method to maintain reasonable run-time as compared with ensembles. Importantly, our approach achieves competitive performance to the state-of-the-art in prediction and uncertainty estimation tasks with fully connected neural networks and ResNets, while reducing the parameter size to $\\leq 47.9\\%$ of that of a single neural network. ", "one-sentence_summary": "We introduce a parameter-efficient uncertainty quantification framework for deep neural net, results show competitive performances, but the model size is reduced significantly to < half of a single network.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ritter|sparse_uncertainty_representation_in_deep_learning_with_inducing_weights", "supplementary_material": "/attachment/3f34d14de08a382bccc43dc3ee9761f316599de3.zip", "pdf": "/pdf/a3dd664869d916612de797754361eb503a27d4db.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=rw1jfllsOM", "_bibtex": "@misc{\nritter2021sparse,\ntitle={Sparse Uncertainty Representation in Deep Learning with Inducing Weights},\nauthor={Hippolyt Ritter and Martin Kukla and Cheng Zhang and Yingzhen Li},\nyear={2021},\nurl={https://openreview.net/forum?id=M9hdyCNlWaf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "M9hdyCNlWaf", "replyto": "M9hdyCNlWaf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3271/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538078824, "tmdate": 1606915790763, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3271/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3271/-/Official_Review"}}}, {"id": "dRgmcm5bYDU", "original": null, "number": 12, "cdate": 1606084839508, "ddate": null, "tcdate": 1606084839508, "tmdate": 1606084839508, "tddate": null, "forum": "M9hdyCNlWaf", "replyto": "6DN6Vm01f4n", "invitation": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment", "content": {"title": "Response", "comment": "Thanks for your detailed responses ! Overall, I think your paper is very novel and interesting to read !"}, "signatures": ["ICLR.cc/2021/Conference/Paper3271/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse Uncertainty Representation in Deep Learning with Inducing Weights", "authorids": ["~Hippolyt_Ritter1", "~Martin_Kukla1", "~Cheng_Zhang1", "~Yingzhen_Li1"], "authors": ["Hippolyt Ritter", "Martin Kukla", "Cheng Zhang", "Yingzhen Li"], "keywords": ["Bayesian neural networks", "uncertainty estimation", "memory efficiency"], "abstract": "Bayesian neural networks and deep ensembles represent two modern paradigms of uncertainty quantification in deep learning. Yet these approaches struggle to scale mainly due to memory inefficiency issues, since they require parameter storage several times higher than their deterministic counterparts. To address this, we augment the weight matrix of each layer  with a small number of inducing weights, thereby projecting the uncertainty quantification into such low dimensional spaces. We further extend Matheron's conditional Gaussian sampling rule to enable fast weight sampling, whichenable our inference method to maintain reasonable run-time as compared with ensembles. Importantly, our approach achieves competitive performance to the state-of-the-art in prediction and uncertainty estimation tasks with fully connected neural networks and ResNets, while reducing the parameter size to $\\leq 47.9\\%$ of that of a single neural network. ", "one-sentence_summary": "We introduce a parameter-efficient uncertainty quantification framework for deep neural net, results show competitive performances, but the model size is reduced significantly to < half of a single network.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ritter|sparse_uncertainty_representation_in_deep_learning_with_inducing_weights", "supplementary_material": "/attachment/3f34d14de08a382bccc43dc3ee9761f316599de3.zip", "pdf": "/pdf/a3dd664869d916612de797754361eb503a27d4db.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=rw1jfllsOM", "_bibtex": "@misc{\nritter2021sparse,\ntitle={Sparse Uncertainty Representation in Deep Learning with Inducing Weights},\nauthor={Hippolyt Ritter and Martin Kukla and Cheng Zhang and Yingzhen Li},\nyear={2021},\nurl={https://openreview.net/forum?id=M9hdyCNlWaf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "M9hdyCNlWaf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3271/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3271/Authors|ICLR.cc/2021/Conference/Paper3271/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839256, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment"}}}, {"id": "6DN6Vm01f4n", "original": null, "number": 11, "cdate": 1605905414072, "ddate": null, "tcdate": 1605905414072, "tmdate": 1605905414072, "tddate": null, "forum": "M9hdyCNlWaf", "replyto": "BGU4-dbMcK", "invitation": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment", "content": {"title": "Bigger network architecture also reduces overhead", "comment": "To further elaborate on the point that the computational overhead decreases with batch size: this is also the case for a bigger architecture. We have just run our runtime benchmarks for Resnet50 and Resnet101 -- which besides being deeper, also have some wider layers compared to Resnet18 -- and the overhead for using FFG-U instead of FFG-W decreases from around $11\\times$ to $7\\times$ at a mini-batch size of $100$, and from around $4\\times$ to $2.5\\times$ at a batch a size of $500$ (all with $K=4$ samples and $M=128$). The overhead for sampling the weights becomes less and less significant as the computation time of the forward pass increases."}, "signatures": ["ICLR.cc/2021/Conference/Paper3271/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse Uncertainty Representation in Deep Learning with Inducing Weights", "authorids": ["~Hippolyt_Ritter1", "~Martin_Kukla1", "~Cheng_Zhang1", "~Yingzhen_Li1"], "authors": ["Hippolyt Ritter", "Martin Kukla", "Cheng Zhang", "Yingzhen Li"], "keywords": ["Bayesian neural networks", "uncertainty estimation", "memory efficiency"], "abstract": "Bayesian neural networks and deep ensembles represent two modern paradigms of uncertainty quantification in deep learning. Yet these approaches struggle to scale mainly due to memory inefficiency issues, since they require parameter storage several times higher than their deterministic counterparts. To address this, we augment the weight matrix of each layer  with a small number of inducing weights, thereby projecting the uncertainty quantification into such low dimensional spaces. We further extend Matheron's conditional Gaussian sampling rule to enable fast weight sampling, whichenable our inference method to maintain reasonable run-time as compared with ensembles. Importantly, our approach achieves competitive performance to the state-of-the-art in prediction and uncertainty estimation tasks with fully connected neural networks and ResNets, while reducing the parameter size to $\\leq 47.9\\%$ of that of a single neural network. ", "one-sentence_summary": "We introduce a parameter-efficient uncertainty quantification framework for deep neural net, results show competitive performances, but the model size is reduced significantly to < half of a single network.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ritter|sparse_uncertainty_representation_in_deep_learning_with_inducing_weights", "supplementary_material": "/attachment/3f34d14de08a382bccc43dc3ee9761f316599de3.zip", "pdf": "/pdf/a3dd664869d916612de797754361eb503a27d4db.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=rw1jfllsOM", "_bibtex": "@misc{\nritter2021sparse,\ntitle={Sparse Uncertainty Representation in Deep Learning with Inducing Weights},\nauthor={Hippolyt Ritter and Martin Kukla and Cheng Zhang and Yingzhen Li},\nyear={2021},\nurl={https://openreview.net/forum?id=M9hdyCNlWaf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "M9hdyCNlWaf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3271/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3271/Authors|ICLR.cc/2021/Conference/Paper3271/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839256, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment"}}}, {"id": "BGU4-dbMcK", "original": null, "number": 10, "cdate": 1605894742230, "ddate": null, "tcdate": 1605894742230, "tmdate": 1605894742230, "tddate": null, "forum": "M9hdyCNlWaf", "replyto": "Wa7YRCPg5Lw", "invitation": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment", "content": {"title": "Runtime overhead is mitigated with large batchsize", "comment": "Thank you for your response, glad to see that many of your previous concerns are resolved.\n\nRegarding runtime overhead: \n\nThe time complexity figures in Table 1 shows that compared with FFG-W, FFG-U has extra overhead (the $\\mathcal{O}(2 M^3_{in} + 2 M^3_{out} + K(d_{out} M_{out} M_{in} + M_{in} d_{out} d_{in}))$ term) required by computing the Cholesky decomposition and matrix multiplications. \n\nHowever, this extra cost dominates the computation **only when batch-size $N$ and number of samples $K$ are small**. We expect that when $N > M_{in}$ and $K$ is reasonably large, the dominate cost would be the regular $\\mathcal{O}(NK d_{in} d_{out})$ term that also appears in FFG-W time complexity. This is because:\n1. The Cholesky decompositions are only needed once (the $\\mathcal{O}(2 M^3_{in} + 2 M^3_{out})$ term).\n2. When $N > M_{in}$, we have $\\mathcal{O}(NK d_{in} d_{out}) > \\mathcal{O}(K(d_{out} M_{out} M_{in} + M_{in} d_{out} d_{in}))$, i.e. constructing $K$ set of network parameters becomes cheaper. \n3. If time budget is in concern, we can always cache both the Cholesky decomposions and the network parameter constructions. In this case inference time complexity reduces back to $\\mathcal{O}(NK d_{in} d_{out})$, but we need to pay the price of caching the full network parameters.\n\nYou can see in Figure 4 (left) that when $N=500$ and $K > 8$, the run time overhead of FFG-U as compared with FFG-W becomes smaller and smaller, converging towards a relative runtime ratio of 1. In our test time evaluations we used $N=500$, $K=20$, and $M_{in} \\leq 128$.\n\nWe would like to emphasize again that our goal is to develop a **parameter efficient** approach for uncertainty estimation in deep neural networks. Still we agree that it would be useful to address the run-time overhead for this method. As we stated in the conclusion part of the paper, we believe some vectorisation technique, e.g. see Wen et al. (2020), could be develop to improve the speed."}, "signatures": ["ICLR.cc/2021/Conference/Paper3271/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse Uncertainty Representation in Deep Learning with Inducing Weights", "authorids": ["~Hippolyt_Ritter1", "~Martin_Kukla1", "~Cheng_Zhang1", "~Yingzhen_Li1"], "authors": ["Hippolyt Ritter", "Martin Kukla", "Cheng Zhang", "Yingzhen Li"], "keywords": ["Bayesian neural networks", "uncertainty estimation", "memory efficiency"], "abstract": "Bayesian neural networks and deep ensembles represent two modern paradigms of uncertainty quantification in deep learning. Yet these approaches struggle to scale mainly due to memory inefficiency issues, since they require parameter storage several times higher than their deterministic counterparts. To address this, we augment the weight matrix of each layer  with a small number of inducing weights, thereby projecting the uncertainty quantification into such low dimensional spaces. We further extend Matheron's conditional Gaussian sampling rule to enable fast weight sampling, whichenable our inference method to maintain reasonable run-time as compared with ensembles. Importantly, our approach achieves competitive performance to the state-of-the-art in prediction and uncertainty estimation tasks with fully connected neural networks and ResNets, while reducing the parameter size to $\\leq 47.9\\%$ of that of a single neural network. ", "one-sentence_summary": "We introduce a parameter-efficient uncertainty quantification framework for deep neural net, results show competitive performances, but the model size is reduced significantly to < half of a single network.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ritter|sparse_uncertainty_representation_in_deep_learning_with_inducing_weights", "supplementary_material": "/attachment/3f34d14de08a382bccc43dc3ee9761f316599de3.zip", "pdf": "/pdf/a3dd664869d916612de797754361eb503a27d4db.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=rw1jfllsOM", "_bibtex": "@misc{\nritter2021sparse,\ntitle={Sparse Uncertainty Representation in Deep Learning with Inducing Weights},\nauthor={Hippolyt Ritter and Martin Kukla and Cheng Zhang and Yingzhen Li},\nyear={2021},\nurl={https://openreview.net/forum?id=M9hdyCNlWaf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "M9hdyCNlWaf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3271/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3271/Authors|ICLR.cc/2021/Conference/Paper3271/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839256, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment"}}}, {"id": "Wa7YRCPg5Lw", "original": null, "number": 9, "cdate": 1605892323681, "ddate": null, "tcdate": 1605892323681, "tmdate": 1605892323681, "tddate": null, "forum": "M9hdyCNlWaf", "replyto": "VSPZWoFSN3u", "invitation": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you so much for the responses. I think your responses resolve most of my concerns. However, as I mentioned in the **Experiments** section, I think the runtime-overhead is a major issue of the propose approach. Given that you used K=5 over your experiments, the proposed approach seems to be ~10x slower as shown by Figure 4. Could you please comment more on this ?"}, "signatures": ["ICLR.cc/2021/Conference/Paper3271/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse Uncertainty Representation in Deep Learning with Inducing Weights", "authorids": ["~Hippolyt_Ritter1", "~Martin_Kukla1", "~Cheng_Zhang1", "~Yingzhen_Li1"], "authors": ["Hippolyt Ritter", "Martin Kukla", "Cheng Zhang", "Yingzhen Li"], "keywords": ["Bayesian neural networks", "uncertainty estimation", "memory efficiency"], "abstract": "Bayesian neural networks and deep ensembles represent two modern paradigms of uncertainty quantification in deep learning. Yet these approaches struggle to scale mainly due to memory inefficiency issues, since they require parameter storage several times higher than their deterministic counterparts. To address this, we augment the weight matrix of each layer  with a small number of inducing weights, thereby projecting the uncertainty quantification into such low dimensional spaces. We further extend Matheron's conditional Gaussian sampling rule to enable fast weight sampling, whichenable our inference method to maintain reasonable run-time as compared with ensembles. Importantly, our approach achieves competitive performance to the state-of-the-art in prediction and uncertainty estimation tasks with fully connected neural networks and ResNets, while reducing the parameter size to $\\leq 47.9\\%$ of that of a single neural network. ", "one-sentence_summary": "We introduce a parameter-efficient uncertainty quantification framework for deep neural net, results show competitive performances, but the model size is reduced significantly to < half of a single network.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ritter|sparse_uncertainty_representation_in_deep_learning_with_inducing_weights", "supplementary_material": "/attachment/3f34d14de08a382bccc43dc3ee9761f316599de3.zip", "pdf": "/pdf/a3dd664869d916612de797754361eb503a27d4db.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=rw1jfllsOM", "_bibtex": "@misc{\nritter2021sparse,\ntitle={Sparse Uncertainty Representation in Deep Learning with Inducing Weights},\nauthor={Hippolyt Ritter and Martin Kukla and Cheng Zhang and Yingzhen Li},\nyear={2021},\nurl={https://openreview.net/forum?id=M9hdyCNlWaf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "M9hdyCNlWaf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3271/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3271/Authors|ICLR.cc/2021/Conference/Paper3271/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839256, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment"}}}, {"id": "VOfpbJfbFS", "original": null, "number": 8, "cdate": 1605873998055, "ddate": null, "tcdate": 1605873998055, "tmdate": 1605873998055, "tddate": null, "forum": "M9hdyCNlWaf", "replyto": "M9hdyCNlWaf", "invitation": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment", "content": {"title": "New revision", "comment": "Dear reviewers, we have just uploaded a new revision of our paper. To highlight the changes, we have marked any new text in red font. Please find a comprehensive list of the most important updates below, indicating in brackets whose questions or suggestion we\u2019re aiming to address:\n* Explained overhead of training a BNN based on eq. 1 (R3)\n* Defined variables d_in^l and d_out^l in Section 2 (R3)\n* Clarified property 2 for the inducing auxiliary variables (R2)\n* Expanded the introductory example of a multivariate Gaussian in Section 3.1\n* Clarified what we mean by a \u201cmixture of delta measures\u201d for Ensemble-U in Section 3.1 (R1, R4)\n* Significantly expanded Section 3.3 to make it less dense (R2)\n* Added figures for FFG-U (and FCG-W) to the toy regression example in Section 4.1 (R2)\n* Explained how we treat convolutional layers in Section 4.2 (R4)\n\nWe note that we have not included any preliminary results on Resnet50 in our current revision. We would like to finish our systematic run of all methods on both CIFAR10 and CIFAR100 with a sufficiently large number of random seeds before doing so, for which the discussion period has not been sufficiently long. We have updated the tables in our summary of the response and the reply to Reviewer1 to include additional seeds for FFG-U and FFG-W on CIFAR10 and we anticipate adding initial results for Ensemble-U when they are available. We believe that our preliminary results show that the inducing weight approach also works well on deeper networks such as Resnet50.\n\nThank you all for your valuable feedback. We will be happy to take any further suggestions into account."}, "signatures": ["ICLR.cc/2021/Conference/Paper3271/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse Uncertainty Representation in Deep Learning with Inducing Weights", "authorids": ["~Hippolyt_Ritter1", "~Martin_Kukla1", "~Cheng_Zhang1", "~Yingzhen_Li1"], "authors": ["Hippolyt Ritter", "Martin Kukla", "Cheng Zhang", "Yingzhen Li"], "keywords": ["Bayesian neural networks", "uncertainty estimation", "memory efficiency"], "abstract": "Bayesian neural networks and deep ensembles represent two modern paradigms of uncertainty quantification in deep learning. Yet these approaches struggle to scale mainly due to memory inefficiency issues, since they require parameter storage several times higher than their deterministic counterparts. To address this, we augment the weight matrix of each layer  with a small number of inducing weights, thereby projecting the uncertainty quantification into such low dimensional spaces. We further extend Matheron's conditional Gaussian sampling rule to enable fast weight sampling, whichenable our inference method to maintain reasonable run-time as compared with ensembles. Importantly, our approach achieves competitive performance to the state-of-the-art in prediction and uncertainty estimation tasks with fully connected neural networks and ResNets, while reducing the parameter size to $\\leq 47.9\\%$ of that of a single neural network. ", "one-sentence_summary": "We introduce a parameter-efficient uncertainty quantification framework for deep neural net, results show competitive performances, but the model size is reduced significantly to < half of a single network.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ritter|sparse_uncertainty_representation_in_deep_learning_with_inducing_weights", "supplementary_material": "/attachment/3f34d14de08a382bccc43dc3ee9761f316599de3.zip", "pdf": "/pdf/a3dd664869d916612de797754361eb503a27d4db.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=rw1jfllsOM", "_bibtex": "@misc{\nritter2021sparse,\ntitle={Sparse Uncertainty Representation in Deep Learning with Inducing Weights},\nauthor={Hippolyt Ritter and Martin Kukla and Cheng Zhang and Yingzhen Li},\nyear={2021},\nurl={https://openreview.net/forum?id=M9hdyCNlWaf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "M9hdyCNlWaf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3271/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3271/Authors|ICLR.cc/2021/Conference/Paper3271/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839256, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment"}}}, {"id": "OOXG9b2YMMl", "original": null, "number": 4, "cdate": 1605550225413, "ddate": null, "tcdate": 1605550225413, "tmdate": 1605873913115, "tddate": null, "forum": "M9hdyCNlWaf", "replyto": "KUPFW1L8T_E", "invitation": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment", "content": {"title": "Response to Reviewer2 (1/2)", "comment": "Thank you very much for your extensive and thorough view, in particular your feedback regarding the experimental section and suggesting specific baselines to compare against. We had indeed considered comparing to methods from the literature on pruning neural network weights but ultimately decided against this. \n\nOur reasoning is that such a comparison would mainly be needed if we were presenting our method as essentially ready-to-deploy for e.g. machine learning on mobile devices, which would indeed require pushing the memory requirement to be **as small as possible**. We believe that none of the existing BNN/deep ensemble approaches aim at solving this challenge, in this regard the focus of our work is no different to existing paradigms. For example, Swiatkowski et al. (2020) recently proposed an efficient parameterisation of the mean-field posterior that reduces the number of parameters to be slightly larger than that of a deterministic network (down from twice as many parameters) without a comparison to pruning methods, which would in all likelihood yield a greater reduction since a pruning rate of 50% would be sufficient to save more parameters that their method.\n\nHowever, we do notice that BNN/deep ensembles need to reduce their memory overhead to a **reasonable level** in order to be scaled up to giant neural networks such as very deep Resnets and Transformers. Therefore, we see our contribution mainly on the conceptual side, showing that we can efficiently perform inference in a lower-dimensional space while having a smaller parameter count than the dimensionality of the original space. We strongly believe that our experiments support this claim and our initial results on Resnet50 further confirm the efficacy of our approach (accuracy of 94.72% and ECE of 1.61% for FFG-U). As a side note, subspace methods have been shown to be effective in the literature before, e.g. (Izmailov et al., 2019), however all of these approaches pay the cost of **increasing** the number of parameters.\n\nThe variational mean-field pruning that you suggest is a reasonable approach to further reduce parameter count. However, in contrast to our inducing weight method, it **changes the model post-hoc**, whereas we perform inference in the original model, so it would not be an apples-to-apples comparison. Instead, pruning our inducing-weight based network post-hoc would be an interesting direction for a fair comparison in future work.\n\nOn another important note, we have not in any way pushed for the minimum attainable number of parameters with our inducing weight method. In fact, we have used the simplest scheme imaginable for setting the number of inducing rows and columns: a constant value for every layer across the network, as this allows for an illustrating ablation study and greatly simplifies the experimental setup. The alternative would be to devise a scheme for adapting shape($U_l$)  to the specific network architecture and the learning task. In this regard we see the potential to further reduce the memory requirements of our approach. However, we do not believe that this would add particularly important insights to the paper.\n\nFinally, there is no established experimental benchmark for the parameter efficiency of BNNs, and we do not think that it makes a significant difference to create such a protocol at this point given that existing methods are not routinely used for the purpose of on-device ML. Even if such a benchmark existed, the ICLR reviewer guidelines explicitly state papers do not need to achieve state-of-the-art results for acceptance, but need to bring \u201cnew, relevant, impactful knowledge\u201d to the community. Based on the feedback both from you and the other reviewers, we are confident that we have achieved this.\n\nWe hope you agree and will consider increasing your score, otherwise we are more than happy to engage in further discussion.\n\n\nReferences:\n\nSwiatkowski et al. The k-tied Normal Distribution: A Compact Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks. In ICML 2020.\n\nIzmailov et al. Subspace inference for Bayesian deep learning. In UAI 2019."}, "signatures": ["ICLR.cc/2021/Conference/Paper3271/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse Uncertainty Representation in Deep Learning with Inducing Weights", "authorids": ["~Hippolyt_Ritter1", "~Martin_Kukla1", "~Cheng_Zhang1", "~Yingzhen_Li1"], "authors": ["Hippolyt Ritter", "Martin Kukla", "Cheng Zhang", "Yingzhen Li"], "keywords": ["Bayesian neural networks", "uncertainty estimation", "memory efficiency"], "abstract": "Bayesian neural networks and deep ensembles represent two modern paradigms of uncertainty quantification in deep learning. Yet these approaches struggle to scale mainly due to memory inefficiency issues, since they require parameter storage several times higher than their deterministic counterparts. To address this, we augment the weight matrix of each layer  with a small number of inducing weights, thereby projecting the uncertainty quantification into such low dimensional spaces. We further extend Matheron's conditional Gaussian sampling rule to enable fast weight sampling, whichenable our inference method to maintain reasonable run-time as compared with ensembles. Importantly, our approach achieves competitive performance to the state-of-the-art in prediction and uncertainty estimation tasks with fully connected neural networks and ResNets, while reducing the parameter size to $\\leq 47.9\\%$ of that of a single neural network. ", "one-sentence_summary": "We introduce a parameter-efficient uncertainty quantification framework for deep neural net, results show competitive performances, but the model size is reduced significantly to < half of a single network.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ritter|sparse_uncertainty_representation_in_deep_learning_with_inducing_weights", "supplementary_material": "/attachment/3f34d14de08a382bccc43dc3ee9761f316599de3.zip", "pdf": "/pdf/a3dd664869d916612de797754361eb503a27d4db.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=rw1jfllsOM", "_bibtex": "@misc{\nritter2021sparse,\ntitle={Sparse Uncertainty Representation in Deep Learning with Inducing Weights},\nauthor={Hippolyt Ritter and Martin Kukla and Cheng Zhang and Yingzhen Li},\nyear={2021},\nurl={https://openreview.net/forum?id=M9hdyCNlWaf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "M9hdyCNlWaf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3271/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3271/Authors|ICLR.cc/2021/Conference/Paper3271/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839256, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment"}}}, {"id": "VSPZWoFSN3u", "original": null, "number": 5, "cdate": 1605550252607, "ddate": null, "tcdate": 1605550252607, "tmdate": 1605550571634, "tddate": null, "forum": "M9hdyCNlWaf", "replyto": "n4DQA_680-M", "invitation": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment", "content": {"title": "Response to Reviewer4", "comment": "Thank you very much for your review. We are pleased to see that you appreciate the novelty and the technical contributions of our paper. We will address your questions below:\n\n1. We treat convolutional weights as 2d matrices by \u2018flattening\u2019 the 4d tensors from shape (out_channels, in_channnels, height, width) to (out_channels, in_channels * height * width). This corresponds to the interpretation of convolutions as linear layers that are repeated across the image patches with tied weights, see e.g. (Grosse & Martens, 2016) for an equivalent use for second-order optimisation. We will clarify this in our next revision of the paper.\n\n2. We indeed use the latter interpretation, i.e. tie the groups across layers. Again, we will update the paper to better reflect this.\n\n3. Similar to previous work, e.g. (Ovadia et al., 2019), we found in preliminary experiments a small sample size of around 5 to be sufficient for capturing most of the performance gains over using a single sample. We used 20 samples for all variational methods, as we believe that this fairly represents the advantage over deterministic ensembles of being able to draw an arbitrary number of samples after training.\n\n4. This is an interesting question and we don\u2019t have a conclusive answer at this point. It has recently been argued in the BNN literature that a more concentrated \u2018cold posterior\u2019 (Wenzel et al., 2020) can give better predictive performance and we would hypothesise that our observations regarding the hyperparameters are related to this phenomenon. Further it has been observed that more expressive variational posteriors can lead to worse performance (Trippe & Turner, 2017), however since our approximate posterior has a low-rank mean, but a non-diagonal covariance, it is difficult to strictly classify it as more or less expressive than e.g. a mean-field posterior. Overall, we think that further research around the true posterior of deep neural networks is needed and that our low rank approach in the original model makes for an interesting contribution in that direction.\n\nWe are glad to see from your comments that the main message of our paper is clearly conveyed. Still we will revise our paper, especially for section 3.3, to further improve clarity.\n\n\nReferences:\n\nGrosse & Martens. A Kronecker-factored approximate Fisher matrix for convolution layers. In ICML 2016.\n\nOvadia et al. Can You Trust Your Model\u2019s Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift. In NeurIPS 2019.\n\nWenzel et al. How Good is the Bayes Posterior in Deep Neural Networks Really? In ICML 2020.\n\nTrippe & Turner. Overpruning in Variational Bayesian Neural Networks. In NeurIPS 2017 Workshop on Advances in Approximate Bayesian Inference."}, "signatures": ["ICLR.cc/2021/Conference/Paper3271/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse Uncertainty Representation in Deep Learning with Inducing Weights", "authorids": ["~Hippolyt_Ritter1", "~Martin_Kukla1", "~Cheng_Zhang1", "~Yingzhen_Li1"], "authors": ["Hippolyt Ritter", "Martin Kukla", "Cheng Zhang", "Yingzhen Li"], "keywords": ["Bayesian neural networks", "uncertainty estimation", "memory efficiency"], "abstract": "Bayesian neural networks and deep ensembles represent two modern paradigms of uncertainty quantification in deep learning. Yet these approaches struggle to scale mainly due to memory inefficiency issues, since they require parameter storage several times higher than their deterministic counterparts. To address this, we augment the weight matrix of each layer  with a small number of inducing weights, thereby projecting the uncertainty quantification into such low dimensional spaces. We further extend Matheron's conditional Gaussian sampling rule to enable fast weight sampling, whichenable our inference method to maintain reasonable run-time as compared with ensembles. Importantly, our approach achieves competitive performance to the state-of-the-art in prediction and uncertainty estimation tasks with fully connected neural networks and ResNets, while reducing the parameter size to $\\leq 47.9\\%$ of that of a single neural network. ", "one-sentence_summary": "We introduce a parameter-efficient uncertainty quantification framework for deep neural net, results show competitive performances, but the model size is reduced significantly to < half of a single network.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ritter|sparse_uncertainty_representation_in_deep_learning_with_inducing_weights", "supplementary_material": "/attachment/3f34d14de08a382bccc43dc3ee9761f316599de3.zip", "pdf": "/pdf/a3dd664869d916612de797754361eb503a27d4db.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=rw1jfllsOM", "_bibtex": "@misc{\nritter2021sparse,\ntitle={Sparse Uncertainty Representation in Deep Learning with Inducing Weights},\nauthor={Hippolyt Ritter and Martin Kukla and Cheng Zhang and Yingzhen Li},\nyear={2021},\nurl={https://openreview.net/forum?id=M9hdyCNlWaf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "M9hdyCNlWaf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3271/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3271/Authors|ICLR.cc/2021/Conference/Paper3271/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839256, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment"}}}, {"id": "Nhhw4yLjlV", "original": null, "number": 3, "cdate": 1605550201198, "ddate": null, "tcdate": 1605550201198, "tmdate": 1605550201198, "tddate": null, "forum": "M9hdyCNlWaf", "replyto": "KUPFW1L8T_E", "invitation": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment", "content": {"title": "Response to Reviewer2 (2/2)", "comment": "Regarding your other points:\n* We will add the corresponding figure to the appendix. The uncertainty is similar to FFG-W.\n* Overall we would say that the performance differences are rather small. The main factor that determines the number of parameters is the dimensionality of the inducing space. We only ran our ablation study for the dimensionality of the inducing space up to half the parameters of the deterministic network (or a quarter of the number of mean-field parameters), but given that both accuracy and ECE start to saturate (Figure 5; right column) at the value of M=128 that we use in our main figures and tables, we would expect that matching the number of parameters would at most give a marginal improvement for the performance of the inducing weight methods.\n* Thanks for pointing this out. We had to move more material than we would have liked to the appendix to be within the page limit, but will upload a revised version that makes use of the additional page that is now available. Similarly we will revise section 3.3 to make it clearer."}, "signatures": ["ICLR.cc/2021/Conference/Paper3271/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse Uncertainty Representation in Deep Learning with Inducing Weights", "authorids": ["~Hippolyt_Ritter1", "~Martin_Kukla1", "~Cheng_Zhang1", "~Yingzhen_Li1"], "authors": ["Hippolyt Ritter", "Martin Kukla", "Cheng Zhang", "Yingzhen Li"], "keywords": ["Bayesian neural networks", "uncertainty estimation", "memory efficiency"], "abstract": "Bayesian neural networks and deep ensembles represent two modern paradigms of uncertainty quantification in deep learning. Yet these approaches struggle to scale mainly due to memory inefficiency issues, since they require parameter storage several times higher than their deterministic counterparts. To address this, we augment the weight matrix of each layer  with a small number of inducing weights, thereby projecting the uncertainty quantification into such low dimensional spaces. We further extend Matheron's conditional Gaussian sampling rule to enable fast weight sampling, whichenable our inference method to maintain reasonable run-time as compared with ensembles. Importantly, our approach achieves competitive performance to the state-of-the-art in prediction and uncertainty estimation tasks with fully connected neural networks and ResNets, while reducing the parameter size to $\\leq 47.9\\%$ of that of a single neural network. ", "one-sentence_summary": "We introduce a parameter-efficient uncertainty quantification framework for deep neural net, results show competitive performances, but the model size is reduced significantly to < half of a single network.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ritter|sparse_uncertainty_representation_in_deep_learning_with_inducing_weights", "supplementary_material": "/attachment/3f34d14de08a382bccc43dc3ee9761f316599de3.zip", "pdf": "/pdf/a3dd664869d916612de797754361eb503a27d4db.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=rw1jfllsOM", "_bibtex": "@misc{\nritter2021sparse,\ntitle={Sparse Uncertainty Representation in Deep Learning with Inducing Weights},\nauthor={Hippolyt Ritter and Martin Kukla and Cheng Zhang and Yingzhen Li},\nyear={2021},\nurl={https://openreview.net/forum?id=M9hdyCNlWaf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "M9hdyCNlWaf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3271/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3271/Authors|ICLR.cc/2021/Conference/Paper3271/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839256, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment"}}}, {"id": "74BwCxlFDhE", "original": null, "number": 2, "cdate": 1605550158277, "ddate": null, "tcdate": 1605550158277, "tmdate": 1605550158277, "tddate": null, "forum": "M9hdyCNlWaf", "replyto": "KQIhPB1DX3s", "invitation": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment", "content": {"title": "Response to Reviewer3", "comment": "Thank you very much for your review. While the other reviewers have found the technical exposition novel and clear enough, it is valuable feedback that to the non-expert reader the paper is difficult to follow. Our aim is of course to make our work accessible to a more general machine learning audience than just the Bayesian deep learning research community. We appreciate that the write-up is dense in its current state and we already had to move some technical material to the appendix that we would have liked to include in the main text. Given that we now have an additional page available, we will include some more high-level discussion in our next revision. However, we hope you understand that it is not feasible to write a self-contained research paper within nine pages and we have to refer the reader to other works in some places.\n\nWe do not consider MC dropout as a baseline because it does not reduce the parameter count compared to a deterministic network. One would have to either store the full set of trained weights and perform dropout at test time, which would require storing the same number of parameters as a deterministic network, or one would have store an ensemble of smaller networks where the weights that have been zeroed-out by dropout have been removed, which, depending on the dropout rate, would quickly exceed the storage requirements of a deterministic network. Further, dropout is not used in all architectures, for example Resnets do not use dropout, limiting its applicability compared to deep ensembles.\n\nWhile our work builds on existing methods, we believe -- in which the other reviewers (as experts in Bayesian deep learning) also agreed -- that our approach is highly novel, and our contribution is significant. In addition to our empirical evaluations, we make the following original technical contributions:\n\n* An efficient approximate inference method for Bayesian neural networks that uses **fewer** parameters than a deterministic network. To our knowledge, this is the first such method that has been applied successfully to modern NN architectures.\n* An extension of Matheron\u2019s rule to efficiently sample from conditional Gaussian distributions that have a joint covariance matrix with Khatri-Rao product structure. This may be of use beyond variational inference and Bayesian neural networks.\n* An interpretation of the proposed inducing weight approach as related to function-space inference and (deep) Gaussian processes. To our knowledge, this is the first approach that explicitly introduces the inducing variable method (which has been hugely successful in Gaussian process literature) to deep neural networks, and our inducing weight method nicely connects weight-space and function-space methods for uncertainty estimation in deep learning.\n\n\nWe hope this has clarified the contribution of our work and that you will consider adjusting your score. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3271/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse Uncertainty Representation in Deep Learning with Inducing Weights", "authorids": ["~Hippolyt_Ritter1", "~Martin_Kukla1", "~Cheng_Zhang1", "~Yingzhen_Li1"], "authors": ["Hippolyt Ritter", "Martin Kukla", "Cheng Zhang", "Yingzhen Li"], "keywords": ["Bayesian neural networks", "uncertainty estimation", "memory efficiency"], "abstract": "Bayesian neural networks and deep ensembles represent two modern paradigms of uncertainty quantification in deep learning. Yet these approaches struggle to scale mainly due to memory inefficiency issues, since they require parameter storage several times higher than their deterministic counterparts. To address this, we augment the weight matrix of each layer  with a small number of inducing weights, thereby projecting the uncertainty quantification into such low dimensional spaces. We further extend Matheron's conditional Gaussian sampling rule to enable fast weight sampling, whichenable our inference method to maintain reasonable run-time as compared with ensembles. Importantly, our approach achieves competitive performance to the state-of-the-art in prediction and uncertainty estimation tasks with fully connected neural networks and ResNets, while reducing the parameter size to $\\leq 47.9\\%$ of that of a single neural network. ", "one-sentence_summary": "We introduce a parameter-efficient uncertainty quantification framework for deep neural net, results show competitive performances, but the model size is reduced significantly to < half of a single network.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ritter|sparse_uncertainty_representation_in_deep_learning_with_inducing_weights", "supplementary_material": "/attachment/3f34d14de08a382bccc43dc3ee9761f316599de3.zip", "pdf": "/pdf/a3dd664869d916612de797754361eb503a27d4db.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=rw1jfllsOM", "_bibtex": "@misc{\nritter2021sparse,\ntitle={Sparse Uncertainty Representation in Deep Learning with Inducing Weights},\nauthor={Hippolyt Ritter and Martin Kukla and Cheng Zhang and Yingzhen Li},\nyear={2021},\nurl={https://openreview.net/forum?id=M9hdyCNlWaf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "M9hdyCNlWaf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3271/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3271/Authors|ICLR.cc/2021/Conference/Paper3271/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839256, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3271/-/Official_Comment"}}}, {"id": "n4DQA_680-M", "original": null, "number": 3, "cdate": 1603765768853, "ddate": null, "tcdate": 1603765768853, "tmdate": 1605024032008, "tddate": null, "forum": "M9hdyCNlWaf", "replyto": "M9hdyCNlWaf", "invitation": "ICLR.cc/2021/Conference/Paper3271/-/Official_Review", "content": {"title": "A compact variational BNN posterior for memory efficiency", "review": "Bayesian deep learning attempts to incorporate uncertainty estimation in the modern neural network models. However, common approaches such as Bayesian neural networks and Deep Ensembles incur large memory overhead because of the increased parameter sizes. Motivated by the inducing points approach in the sparse variational Gaussian Processes area, this paper proposes the Gaussian variational posterior relying on the compact *inducing weights* $U$ and the conditional distribution $p(W|U)$. Because the variational parameter shifts from $W$ to the smaller $U$, the resulting model potentially uses even fewer parameters than a deterministic counterpart. In general, I think the paper proposes an interesting approach that could help addressing the storage issues of current Bayesian deep learning models. \n\n**Novelty**\nThe proposed approach is both novel and elegant. Given the high dimensionality of the parameters, many BNN approaches consider structured covariance for the variational posterior, which involves the balancing between computational costs and approximating capacity. And these approaches all maintain a \"mean parameter\", thus the memory costs are at least as large as the original network. In comparison, this paper turns to an augmented space using the joint Gaussian distribution, and instead model the variational posterior for the compact *inducing weights*. In consequence, the resulting model could potentially have even fewer parameters than a deterministic counterpart, while being able to conduct uncertainty quantification. Given that the inducing points approach has achieved big successes for scalable GPs, I think the proposed method could be impactful for uncertainty modelling in deep learning models. \n\n**Experiments**\nThis paper conducts experiments covering both image classification and out-of-distribution detection. Empirically, their model has increased calibration compared to the deterministic network while using $\\leq 47.9\\%$ of parameters. However, as shown in Figure 4, the proposed approach incurs large computational overheads especially when using small number of samples. \n\n**Questions**\n1. What are the inducing weights for the conv layer? Are they matrices of shape [128, 128, kern_size, kern_size]?\n2. The Ensemble-U approach uses a dirac measure $\\sum_k \\delta_{U^k_l}$ in each layer and drops the KL penalty. Are dirac measures of different layers independent, or like deep ensemble, the particles are tied into $K$ disjoint groups across layers? If it was the former, I would reckon that the particles of the same layer would converge to the same place; if it was the latter, it should be made clearer in the paper.\n3. How does the sample size $K$ influence the performance? In practice, I think only a small number of $K$ should be used.\n4. A hyperparameter $\\sigma_{max}^2$ is set for the variance of $q(W)$. And the similar hyperparameter $\\lambda_{\\max}$ is set for the variational posterior. Especially for $\\lambda_{max}$, increasing it from $0.1$ to $0.3$ observes large performance drop. Is this an issue due to the expressiveness of the proposed posterior ? \n\n**Clarity** \nIn spite of my questions regarding the model details, the paper well presents its main methods. Besides, I think Sec3.3 is worthy of more polishing, since it looks confusing when you start by studying $U_c$ instead of $U_r$ or $U$. And a few typos to be corrected, 1. Abstract: whichenable 2. Last paragraph of introduction: our approach achieve 3) Related works: function-space inference is appealing to ... ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3271/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3271/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparse Uncertainty Representation in Deep Learning with Inducing Weights", "authorids": ["~Hippolyt_Ritter1", "~Martin_Kukla1", "~Cheng_Zhang1", "~Yingzhen_Li1"], "authors": ["Hippolyt Ritter", "Martin Kukla", "Cheng Zhang", "Yingzhen Li"], "keywords": ["Bayesian neural networks", "uncertainty estimation", "memory efficiency"], "abstract": "Bayesian neural networks and deep ensembles represent two modern paradigms of uncertainty quantification in deep learning. Yet these approaches struggle to scale mainly due to memory inefficiency issues, since they require parameter storage several times higher than their deterministic counterparts. To address this, we augment the weight matrix of each layer  with a small number of inducing weights, thereby projecting the uncertainty quantification into such low dimensional spaces. We further extend Matheron's conditional Gaussian sampling rule to enable fast weight sampling, whichenable our inference method to maintain reasonable run-time as compared with ensembles. Importantly, our approach achieves competitive performance to the state-of-the-art in prediction and uncertainty estimation tasks with fully connected neural networks and ResNets, while reducing the parameter size to $\\leq 47.9\\%$ of that of a single neural network. ", "one-sentence_summary": "We introduce a parameter-efficient uncertainty quantification framework for deep neural net, results show competitive performances, but the model size is reduced significantly to < half of a single network.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ritter|sparse_uncertainty_representation_in_deep_learning_with_inducing_weights", "supplementary_material": "/attachment/3f34d14de08a382bccc43dc3ee9761f316599de3.zip", "pdf": "/pdf/a3dd664869d916612de797754361eb503a27d4db.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=rw1jfllsOM", "_bibtex": "@misc{\nritter2021sparse,\ntitle={Sparse Uncertainty Representation in Deep Learning with Inducing Weights},\nauthor={Hippolyt Ritter and Martin Kukla and Cheng Zhang and Yingzhen Li},\nyear={2021},\nurl={https://openreview.net/forum?id=M9hdyCNlWaf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "M9hdyCNlWaf", "replyto": "M9hdyCNlWaf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3271/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538078824, "tmdate": 1606915790763, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3271/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3271/-/Official_Review"}}}], "count": 22}