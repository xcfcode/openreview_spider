{"notes": [{"id": "iKQAk8a2kM0", "original": "RTXXssPhjz6", "number": 289, "cdate": 1601308040059, "ddate": null, "tcdate": 1601308040059, "tmdate": 1613874792754, "tddate": null, "forum": "iKQAk8a2kM0", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits", "authorids": ["~Jiawang_Bai2", "~Baoyuan_Wu1", "~Yong_Zhang6", "~Yiming_Li1", "~Zhifeng_Li5", "~Shu-Tao_Xia1"], "authors": ["Jiawang Bai", "Baoyuan Wu", "Yong Zhang", "Yiming Li", "Zhifeng Li", "Shu-Tao Xia"], "keywords": ["targeted attack", "bit-flip", "weight attack"], "abstract": "To explore the vulnerability of deep neural networks (DNNs), many attack paradigms have been well studied, such as the poisoning-based backdoor attack in the training stage and the adversarial attack in the inference stage. In this paper, we study a novel attack paradigm, which modifies model parameters in the deployment stage for malicious purposes. Specifically, our goal is to misclassify a specific sample into a target class without any sample modification, while not significantly reduce the prediction accuracy of other samples to ensure the stealthiness. To this end, we formulate this problem as a binary integer programming (BIP), since the parameters are stored as binary bits ($i.e.$, 0 and 1) in the memory. By utilizing the latest technique in integer programming, we equivalently reformulate this BIP problem as a continuous optimization problem, which can be effectively and efficiently solved using the alternating direction method of multipliers (ADMM) method. Consequently, the flipped critical bits can be easily determined through optimization, rather than using a heuristic strategy. Extensive experiments demonstrate the superiority of our method in attacking DNNs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bai|targeted_attack_against_deep_neural_networks_via_flipping_limited_weight_bits", "one-sentence_summary": "We propose a targeted attack method against the deployed DNN via flipping a few binary weight bits.", "supplementary_material": "/attachment/00af28d0b277b26bb58e197d38763a8c8c6a8807.zip", "pdf": "/pdf/ed4d75e28ae70ba28f4895cf7097cf634745d11a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbai2021targeted,\ntitle={Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits},\nauthor={Jiawang Bai and Baoyuan Wu and Yong Zhang and Yiming Li and Zhifeng Li and Shu-Tao Xia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iKQAk8a2kM0}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "QyjQ5xyXCx", "original": null, "number": 1, "cdate": 1610040496264, "ddate": null, "tcdate": 1610040496264, "tmdate": 1610474102596, "tddate": null, "forum": "iKQAk8a2kM0", "replyto": "iKQAk8a2kM0", "invitation": "ICLR.cc/2021/Conference/Paper289/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The major concerns about this paper are that (1) There are too many hyper-parameters, such as those needed for ADMM. I'd point out that there are adaptive variants of ADMM and heuristics methods for choosing optimization hyper-parameters, although it would be nice if the authors addressed these issues in the paper.  (2) Some reviewers are concerned that, compared to other related attacks, it\u2019s unclear why flipping fewer bits is an important objective - an attacker might only care about poisoning performance and clean data performance.  The authors respond that flipping fewer bits makes the attack more effective when bits are manipulated by a physical method such as manipulating memory.  Despite these criticisms, reviewers agree that the paper is a well thought-out approach that improves the state of the art by some metrics.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits", "authorids": ["~Jiawang_Bai2", "~Baoyuan_Wu1", "~Yong_Zhang6", "~Yiming_Li1", "~Zhifeng_Li5", "~Shu-Tao_Xia1"], "authors": ["Jiawang Bai", "Baoyuan Wu", "Yong Zhang", "Yiming Li", "Zhifeng Li", "Shu-Tao Xia"], "keywords": ["targeted attack", "bit-flip", "weight attack"], "abstract": "To explore the vulnerability of deep neural networks (DNNs), many attack paradigms have been well studied, such as the poisoning-based backdoor attack in the training stage and the adversarial attack in the inference stage. In this paper, we study a novel attack paradigm, which modifies model parameters in the deployment stage for malicious purposes. Specifically, our goal is to misclassify a specific sample into a target class without any sample modification, while not significantly reduce the prediction accuracy of other samples to ensure the stealthiness. To this end, we formulate this problem as a binary integer programming (BIP), since the parameters are stored as binary bits ($i.e.$, 0 and 1) in the memory. By utilizing the latest technique in integer programming, we equivalently reformulate this BIP problem as a continuous optimization problem, which can be effectively and efficiently solved using the alternating direction method of multipliers (ADMM) method. Consequently, the flipped critical bits can be easily determined through optimization, rather than using a heuristic strategy. Extensive experiments demonstrate the superiority of our method in attacking DNNs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bai|targeted_attack_against_deep_neural_networks_via_flipping_limited_weight_bits", "one-sentence_summary": "We propose a targeted attack method against the deployed DNN via flipping a few binary weight bits.", "supplementary_material": "/attachment/00af28d0b277b26bb58e197d38763a8c8c6a8807.zip", "pdf": "/pdf/ed4d75e28ae70ba28f4895cf7097cf634745d11a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbai2021targeted,\ntitle={Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits},\nauthor={Jiawang Bai and Baoyuan Wu and Yong Zhang and Yiming Li and Zhifeng Li and Shu-Tao Xia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iKQAk8a2kM0}\n}"}, "tags": [], "invitation": {"reply": {"forum": "iKQAk8a2kM0", "replyto": "iKQAk8a2kM0", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040496251, "tmdate": 1610474102581, "id": "ICLR.cc/2021/Conference/Paper289/-/Decision"}}}, {"id": "QQVm8hygpV", "original": null, "number": 9, "cdate": 1606303498350, "ddate": null, "tcdate": 1606303498350, "tmdate": 1606307508933, "tddate": null, "forum": "iKQAk8a2kM0", "replyto": "9kLdaVdxnj0", "invitation": "ICLR.cc/2021/Conference/Paper289/-/Official_Comment", "content": {"title": "Response to Reviewer 2 ", "comment": "Thank you for your additional feedback. We understand your concern about the hyper-parameters in our method and this concern is insightful. The introduced hyper-parameters can be divided into two parts, including (1) hyper-parameters in the $\\ell_p$-Box ADMM algorithm, and (2) attacker-specified hyper-parameters in our attack formulation. We would like to clarify that our method is very practical across different models and datasets by providing more details about how to easily select those hyper-parameters in practice, as follows:\n-  Tuning hyper-parameters in ADMM: In our method, inspired by the $\\ell_p$-Box ADMM [1] algorithm, our method also replaces the binary constraint in our attack formulation by the intersection of two continuous constraints, leading to more hyper-parameters ($i.e$., $\\rho_1, \\rho_2, \\rho_3$) in ADMM. However, in Section 3.2 of [1], there are very detailed studies about the sensitivity to the $\\rho$ parameters for convergence, and give a very specific and practical suggestion about how to tune the $\\rho$ parameters to get a good converged solution. Our experimental settings about the $\\rho$ parameters (see Appendix E.3 in our manuscript) exactly follow this suggestion. Here we simply repeat this suggestion, which consists of two parts: \n\\\n__(1)__ Initializing $\\rho$ from a small value $\\rho_0$, then gradually increasing $\\rho$ ($e.g$., $\\rho \\leftarrow 1.01 * \\rho$), until a user-defined upper bound $\\rho_{upper}$. The small initial value encourages more sufficient searches in the constraint space, while the gradual increase and the upper bound  ensure the final convergence.\n\\\n__(2)__ The tuning of the extra hyper-parameters $\\rho_0$ and $\\rho_{upper}$ can be easily adjusted according to the value of the corresponding constraint violation (each $\\rho$ corresponds to one constraint violation). If the violation value decreases very slowly along the iteration, then we can increase $\\rho_0$ to accelerate the convergence; if it decreases very quickly, then we can decrease $\\rho_0$ or $\\rho_{upper}$ to encourage more sufficient searches. \n\\\nIn [1], $\\ell_p$-Box ADMM has verified its effectiveness in image segmentation, graph matching and clustering. Moreover,  $\\ell_p$-Box  ADMM is utilized to solve many diverse tasks, including model compression [2], MAP inference [3], hash code learning [4], etc. In our experiments, we also follow the suggestion in [1], and get very satisfied results. And the specific values of these hyper-parameters used in our experiments (see Appendix E.3), as well as the codes, have been provided to ensure the reproduction of the reported results. All these previous works and our experiments have verified the practical effectiveness of $\\ell_p$-Box ADMM in many diverse tasks. Thus, we believe that there is no need to worry too much about the practical effectiveness of our method for different datasets and different DNN models. \n- Tuning attacker-specified hyper-parameters in our attack formulation: These parameters in the formulation correspond to the attacker\u2019s preference as follows.\n\\\n__(1)__ $\\lambda$ controls the trade-off between attack effectiveness and stealthiness; \n\\\n__(2)__ $k$ limits the maximum number of bit-flips; \n\\\n__(3)__ $\\delta$ manipulates the prediction probability of the attacked sample. \n\\\nUtilizing the ADMM algorithm as demonstrated above, our attack can identify the flipped bits to satisfy the attacker\u2019s preference by setting different attacker-specified hyper-parameters. The experiment results reported in the tables and ablation studies in Section 4.4 partially verify this point.\n\nIn summary, the proposed method is very practical and can be applied to attack different models on different datasets. Thank you again for raising this point! We hope that the above explanations could somewhat alleviate your concern about the hyper-parameters. \n\n[1] Wu B, Ghanem B. $\\ell_p$-Box ADMM: A Versatile Framework for Integer Programming[J]. IEEE transactions on pattern analysis and machine intelligence, 2018, 41(7): 1695-1708.\n\\\n[2] Li T, Wu B, Yang Y, et al. Compressing convolutional neural networks via factorized convolutional filters[C]. CVPR, 2019.\n\\\n[3] Wu B, Shen L, Zhang T, et al. MAP Inference Via $\\ell _2 $-Sphere Linear Program Reformulation[J]. International Journal of Computer Vision, 2020: 1-24.\n\\\n[4] Shen F, Xu Y, Liu L, et al. Unsupervised deep hashing with similarity-adaptive and discrete optimization[J]. IEEE transactions on pattern analysis and machine intelligence, 2018, 40(12): 3034-3044.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper289/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper289/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits", "authorids": ["~Jiawang_Bai2", "~Baoyuan_Wu1", "~Yong_Zhang6", "~Yiming_Li1", "~Zhifeng_Li5", "~Shu-Tao_Xia1"], "authors": ["Jiawang Bai", "Baoyuan Wu", "Yong Zhang", "Yiming Li", "Zhifeng Li", "Shu-Tao Xia"], "keywords": ["targeted attack", "bit-flip", "weight attack"], "abstract": "To explore the vulnerability of deep neural networks (DNNs), many attack paradigms have been well studied, such as the poisoning-based backdoor attack in the training stage and the adversarial attack in the inference stage. In this paper, we study a novel attack paradigm, which modifies model parameters in the deployment stage for malicious purposes. Specifically, our goal is to misclassify a specific sample into a target class without any sample modification, while not significantly reduce the prediction accuracy of other samples to ensure the stealthiness. To this end, we formulate this problem as a binary integer programming (BIP), since the parameters are stored as binary bits ($i.e.$, 0 and 1) in the memory. By utilizing the latest technique in integer programming, we equivalently reformulate this BIP problem as a continuous optimization problem, which can be effectively and efficiently solved using the alternating direction method of multipliers (ADMM) method. Consequently, the flipped critical bits can be easily determined through optimization, rather than using a heuristic strategy. Extensive experiments demonstrate the superiority of our method in attacking DNNs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bai|targeted_attack_against_deep_neural_networks_via_flipping_limited_weight_bits", "one-sentence_summary": "We propose a targeted attack method against the deployed DNN via flipping a few binary weight bits.", "supplementary_material": "/attachment/00af28d0b277b26bb58e197d38763a8c8c6a8807.zip", "pdf": "/pdf/ed4d75e28ae70ba28f4895cf7097cf634745d11a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbai2021targeted,\ntitle={Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits},\nauthor={Jiawang Bai and Baoyuan Wu and Yong Zhang and Yiming Li and Zhifeng Li and Shu-Tao Xia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iKQAk8a2kM0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iKQAk8a2kM0", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper289/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper289/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper289/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper289/Authors|ICLR.cc/2021/Conference/Paper289/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper289/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872605, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper289/-/Official_Comment"}}}, {"id": "9kLdaVdxnj0", "original": null, "number": 8, "cdate": 1606269512474, "ddate": null, "tcdate": 1606269512474, "tmdate": 1606269512474, "tddate": null, "forum": "iKQAk8a2kM0", "replyto": "i2hwyXOyVMm", "invitation": "ICLR.cc/2021/Conference/Paper289/-/Official_Comment", "content": {"title": "Final comment", "comment": "Thanks the the authors for responding to my answers. I'm still worried about an algorithm which has 4 or more hyper-parameters (with no clear way to figure them out). I just don't see how such an algorithm could have a reasonable out-of-the-box performance in the wild. The authors' response to this point that I raised in my review is unsatisfactory.\n\nI'm keeping my previous score of 6."}, "signatures": ["ICLR.cc/2021/Conference/Paper289/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper289/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits", "authorids": ["~Jiawang_Bai2", "~Baoyuan_Wu1", "~Yong_Zhang6", "~Yiming_Li1", "~Zhifeng_Li5", "~Shu-Tao_Xia1"], "authors": ["Jiawang Bai", "Baoyuan Wu", "Yong Zhang", "Yiming Li", "Zhifeng Li", "Shu-Tao Xia"], "keywords": ["targeted attack", "bit-flip", "weight attack"], "abstract": "To explore the vulnerability of deep neural networks (DNNs), many attack paradigms have been well studied, such as the poisoning-based backdoor attack in the training stage and the adversarial attack in the inference stage. In this paper, we study a novel attack paradigm, which modifies model parameters in the deployment stage for malicious purposes. Specifically, our goal is to misclassify a specific sample into a target class without any sample modification, while not significantly reduce the prediction accuracy of other samples to ensure the stealthiness. To this end, we formulate this problem as a binary integer programming (BIP), since the parameters are stored as binary bits ($i.e.$, 0 and 1) in the memory. By utilizing the latest technique in integer programming, we equivalently reformulate this BIP problem as a continuous optimization problem, which can be effectively and efficiently solved using the alternating direction method of multipliers (ADMM) method. Consequently, the flipped critical bits can be easily determined through optimization, rather than using a heuristic strategy. Extensive experiments demonstrate the superiority of our method in attacking DNNs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bai|targeted_attack_against_deep_neural_networks_via_flipping_limited_weight_bits", "one-sentence_summary": "We propose a targeted attack method against the deployed DNN via flipping a few binary weight bits.", "supplementary_material": "/attachment/00af28d0b277b26bb58e197d38763a8c8c6a8807.zip", "pdf": "/pdf/ed4d75e28ae70ba28f4895cf7097cf634745d11a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbai2021targeted,\ntitle={Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits},\nauthor={Jiawang Bai and Baoyuan Wu and Yong Zhang and Yiming Li and Zhifeng Li and Shu-Tao Xia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iKQAk8a2kM0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iKQAk8a2kM0", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper289/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper289/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper289/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper289/Authors|ICLR.cc/2021/Conference/Paper289/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper289/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872605, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper289/-/Official_Comment"}}}, {"id": "OjB7EVqtZ4K", "original": null, "number": 7, "cdate": 1605607795309, "ddate": null, "tcdate": 1605607795309, "tmdate": 1605607795309, "tddate": null, "forum": "iKQAk8a2kM0", "replyto": "iKQAk8a2kM0", "invitation": "ICLR.cc/2021/Conference/Paper289/-/Official_Comment", "content": {"title": "Summary of updates in the revised paper", "comment": "We thank all the reviewers for the valuable comments and helping us to improve the paper. We have updated our paper according to reviewers' suggestions and summarize the revisions as follows:\n- Added additional papers about ADMM to Section 1 as suggested.\n- Changed equation (6) from $\\hat{b} \\in \\mathcal{S}_b \\cap \\mathcal{S}_p$ to $\\hat{b} \\in (\\mathcal{S}_b \\cap \\mathcal{S}_p)$.\n- Added the explanation in Section 3.2 to clarify why reducing the number of flipped bits matters.\n- Added the results of the running time of different methods in Appendix C.\n- Added a new section (Appendix H) to study the trade-off between the three metrics for our method.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper289/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper289/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits", "authorids": ["~Jiawang_Bai2", "~Baoyuan_Wu1", "~Yong_Zhang6", "~Yiming_Li1", "~Zhifeng_Li5", "~Shu-Tao_Xia1"], "authors": ["Jiawang Bai", "Baoyuan Wu", "Yong Zhang", "Yiming Li", "Zhifeng Li", "Shu-Tao Xia"], "keywords": ["targeted attack", "bit-flip", "weight attack"], "abstract": "To explore the vulnerability of deep neural networks (DNNs), many attack paradigms have been well studied, such as the poisoning-based backdoor attack in the training stage and the adversarial attack in the inference stage. In this paper, we study a novel attack paradigm, which modifies model parameters in the deployment stage for malicious purposes. Specifically, our goal is to misclassify a specific sample into a target class without any sample modification, while not significantly reduce the prediction accuracy of other samples to ensure the stealthiness. To this end, we formulate this problem as a binary integer programming (BIP), since the parameters are stored as binary bits ($i.e.$, 0 and 1) in the memory. By utilizing the latest technique in integer programming, we equivalently reformulate this BIP problem as a continuous optimization problem, which can be effectively and efficiently solved using the alternating direction method of multipliers (ADMM) method. Consequently, the flipped critical bits can be easily determined through optimization, rather than using a heuristic strategy. Extensive experiments demonstrate the superiority of our method in attacking DNNs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bai|targeted_attack_against_deep_neural_networks_via_flipping_limited_weight_bits", "one-sentence_summary": "We propose a targeted attack method against the deployed DNN via flipping a few binary weight bits.", "supplementary_material": "/attachment/00af28d0b277b26bb58e197d38763a8c8c6a8807.zip", "pdf": "/pdf/ed4d75e28ae70ba28f4895cf7097cf634745d11a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbai2021targeted,\ntitle={Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits},\nauthor={Jiawang Bai and Baoyuan Wu and Yong Zhang and Yiming Li and Zhifeng Li and Shu-Tao Xia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iKQAk8a2kM0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iKQAk8a2kM0", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper289/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper289/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper289/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper289/Authors|ICLR.cc/2021/Conference/Paper289/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper289/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872605, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper289/-/Official_Comment"}}}, {"id": "i2hwyXOyVMm", "original": null, "number": 3, "cdate": 1605607226164, "ddate": null, "tcdate": 1605607226164, "tmdate": 1605607678259, "tddate": null, "forum": "iKQAk8a2kM0", "replyto": "wIrPpEa32DA", "invitation": "ICLR.cc/2021/Conference/Paper289/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thank you for your recognition of this work and all valuable comments and suggestions. We respond to your concerns below.\n\n**Q1**: The hyper-parameter tuning is not clearly outlined / explained.\n\\\n**A1**: We are sorry about this unclarity. Due to the space limit, the detailed hyper-parameter settings were originally provided in Appendix E.3. We re-clarify the hyper-parameter settings in our method here.  \n- For the parameters in our objective function, $\\lambda$ and $k$ affect the attack stealthiness and the attack success rate. We adopt a strategy for jointly searching $\\lambda$ and $k$, which is specified in Appendix E.3. The slack variable $\\delta$ can control the prediction probability on the attacked sample. The $\\delta$ is fixed as 10 on CIFAR-10, and it is set to 3 and increased to 10 if the attack fails on ImageNet. \n- In our experiments, we increase ($\\rho_1, \\rho_2, \\rho_3$) after each iteration to accelerate the practical convergence following the general setting. Other parameters of ADMM are fixed for all the experiments.\n\n\n**Q2**: Estimation of the complexity (running time, number of flops, etc.) of the proposed method, as a function of the maximum number of bits to flip.\n\\\n**A2**: Thanks for your valuable suggestion.\n- The theoretical complexity analysis was originally provided in Appendix C. According to the overall computational cost $O\\big(T_{outer} [ 2(N+1)CQ \\cdot (C+T_{inner}) ] \\big)$, there is no relationship between the computational complexity and the maximum number of bits to flip ($i.e.$, $k$). However, $k$ influences the constraint space, which may have a further impact on the running time. Therefore, to further study it, we evaluated the running time and the number of iterations on CIFAR-10 under different $k$, as shown in the following table. These results demonstrate that the running time and the number of iterations are not related to the parameter $k$ to some extent in our experiments. \n|   $k$   \t|       5       \t|       10       \t|       15       \t|       20       \t|       25       \t|       30       \t|\n|:-------:\t|:-------------:\t|:--------------:\t|:--------------:\t|:--------------:\t|:--------------:\t|:--------------:\t|\n| time(s) \t|  144.53\u00b119.74 \t|  147.24\u00b114.09  \t|  147.81\u00b117.91  \t|  150.57\u00b114.43  \t|  144.01\u00b114.13  \t|  139.55\u00b130.64  \t|\n| #iters  \t| 1435.10\u00b139.46 \t| 1404.30\u00b1137.34 \t| 1457.50\u00b1176.53 \t| 1419.60\u00b1135.33 \t| 1417.20\u00b1139.67 \t| 1483.60\u00b1263.58 \t|\n- Besides, we have reported the results of the running time of different methods in Table 3 and added the discussion in Appendix C for your reference. \n\n\n**Q3**: Small issues.\n\\\n**A3**: We greatly appreciate you for pointing out the early key papers about ADMM, and we have added them to the third paragraph in Section 1 in our revised version. \n\n\n**Q4**: Errors.\n\\\n**A4**: Sorry for the confusion and it has been modified to $\\hat{b} \\in (\\mathcal{S}_b \\cap \\mathcal{S}_p)$ in the revised paper. Accordingly, $\\hat{b}$ is the element of the intersection of the box constraint $\\mathcal{S}_b$ and the $\\ell_2$-sphere constraint $\\mathcal{S}_p$. Thank you for pointing it out."}, "signatures": ["ICLR.cc/2021/Conference/Paper289/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper289/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits", "authorids": ["~Jiawang_Bai2", "~Baoyuan_Wu1", "~Yong_Zhang6", "~Yiming_Li1", "~Zhifeng_Li5", "~Shu-Tao_Xia1"], "authors": ["Jiawang Bai", "Baoyuan Wu", "Yong Zhang", "Yiming Li", "Zhifeng Li", "Shu-Tao Xia"], "keywords": ["targeted attack", "bit-flip", "weight attack"], "abstract": "To explore the vulnerability of deep neural networks (DNNs), many attack paradigms have been well studied, such as the poisoning-based backdoor attack in the training stage and the adversarial attack in the inference stage. In this paper, we study a novel attack paradigm, which modifies model parameters in the deployment stage for malicious purposes. Specifically, our goal is to misclassify a specific sample into a target class without any sample modification, while not significantly reduce the prediction accuracy of other samples to ensure the stealthiness. To this end, we formulate this problem as a binary integer programming (BIP), since the parameters are stored as binary bits ($i.e.$, 0 and 1) in the memory. By utilizing the latest technique in integer programming, we equivalently reformulate this BIP problem as a continuous optimization problem, which can be effectively and efficiently solved using the alternating direction method of multipliers (ADMM) method. Consequently, the flipped critical bits can be easily determined through optimization, rather than using a heuristic strategy. Extensive experiments demonstrate the superiority of our method in attacking DNNs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bai|targeted_attack_against_deep_neural_networks_via_flipping_limited_weight_bits", "one-sentence_summary": "We propose a targeted attack method against the deployed DNN via flipping a few binary weight bits.", "supplementary_material": "/attachment/00af28d0b277b26bb58e197d38763a8c8c6a8807.zip", "pdf": "/pdf/ed4d75e28ae70ba28f4895cf7097cf634745d11a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbai2021targeted,\ntitle={Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits},\nauthor={Jiawang Bai and Baoyuan Wu and Yong Zhang and Yiming Li and Zhifeng Li and Shu-Tao Xia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iKQAk8a2kM0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iKQAk8a2kM0", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper289/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper289/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper289/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper289/Authors|ICLR.cc/2021/Conference/Paper289/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper289/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872605, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper289/-/Official_Comment"}}}, {"id": "Szk6vBJRKFy", "original": null, "number": 5, "cdate": 1605607467057, "ddate": null, "tcdate": 1605607467057, "tmdate": 1605607647531, "tddate": null, "forum": "iKQAk8a2kM0", "replyto": "MmHRoSJzQNK", "invitation": "ICLR.cc/2021/Conference/Paper289/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thank you for your recognition of this work and all valuable comments and suggestions. We respond to your concerns below.\n\n**Q1**: I suggest the authors to add some pareto frontier figures. \n\\\n**A1**: Thanks for your insightful suggestion. We have added a new section (Appendix H) to study the trade-off between the three metrics for our method in the revised paper.\n- For our proposed method, we can trade-off the three metrics by tuning the hyper-parameters. Therefore, following your suggestion, we study the trade-off between PA-ACC and $\\mathrm{N_{flip}}$ and the trade-off between PA-ACC and ASR (the absence of the trade-off between $\\mathrm{N_{flip}}$ and ASR is because the fixed PA-ACC is difficult to implement). A numerical example of the trade-off between PA-ACC and $\\mathrm{N_{flip}}$ on CIFAR-10 is shown below, and all visualized curves and detailed descriptions have been added in Appendix H.\n| $\\mathrm{N_{flip}}$ \t|  4.70 \t|  5.33 \t|  5.56 \t|  9.94 \t| 15.02 \t| 29.90 \t| 39.00 \t| 43.07 \t|\n|:-------------------:\t|:-----:\t|:-----:\t|:-----:\t|:-----:\t|:-----:\t|:-----:\t|-------\t|-------\t|\n| PA-ACC              \t| 85.49 \t| 86.66 \t| 88.07 \t| 89.25 \t| 89.58 \t| 89.18 \t| 88.87 \t| 88.45 \t|\n- Although the suggested way can present the results clearly, the trade-off curves of other methods could not be presented due to the below reasons. 1) T-BFA and GDA  greedily search the flipped bits until their attack goals are achieved, so there is no trade-off between PA-ACC, ASR, and $\\mathrm{N_{flip}}$ for these two methods. 2) For TBT and FSA, they do not have the corresponding hyper-parameters to control the degree of meeting the three metrics, $e.g.$, ASR for TBT and $\\mathrm{N_{flip}}$ for FSA. \n\n**Q2**: What are the time costs of different attacking methods?\n\\\n**A2**: Thanks for your insightful suggestion. Following your suggestion, we have compared the results of the running time in Table 3 and added the discussion in Appendix C. Besides, the theoretical computational cost is $O\\big(T_{outer} [ 2(N+1)CQ \\cdot (C+T_{inner}) ] \\big)$, which was originally analyzed in Appendix C."}, "signatures": ["ICLR.cc/2021/Conference/Paper289/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper289/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits", "authorids": ["~Jiawang_Bai2", "~Baoyuan_Wu1", "~Yong_Zhang6", "~Yiming_Li1", "~Zhifeng_Li5", "~Shu-Tao_Xia1"], "authors": ["Jiawang Bai", "Baoyuan Wu", "Yong Zhang", "Yiming Li", "Zhifeng Li", "Shu-Tao Xia"], "keywords": ["targeted attack", "bit-flip", "weight attack"], "abstract": "To explore the vulnerability of deep neural networks (DNNs), many attack paradigms have been well studied, such as the poisoning-based backdoor attack in the training stage and the adversarial attack in the inference stage. In this paper, we study a novel attack paradigm, which modifies model parameters in the deployment stage for malicious purposes. Specifically, our goal is to misclassify a specific sample into a target class without any sample modification, while not significantly reduce the prediction accuracy of other samples to ensure the stealthiness. To this end, we formulate this problem as a binary integer programming (BIP), since the parameters are stored as binary bits ($i.e.$, 0 and 1) in the memory. By utilizing the latest technique in integer programming, we equivalently reformulate this BIP problem as a continuous optimization problem, which can be effectively and efficiently solved using the alternating direction method of multipliers (ADMM) method. Consequently, the flipped critical bits can be easily determined through optimization, rather than using a heuristic strategy. Extensive experiments demonstrate the superiority of our method in attacking DNNs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bai|targeted_attack_against_deep_neural_networks_via_flipping_limited_weight_bits", "one-sentence_summary": "We propose a targeted attack method against the deployed DNN via flipping a few binary weight bits.", "supplementary_material": "/attachment/00af28d0b277b26bb58e197d38763a8c8c6a8807.zip", "pdf": "/pdf/ed4d75e28ae70ba28f4895cf7097cf634745d11a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbai2021targeted,\ntitle={Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits},\nauthor={Jiawang Bai and Baoyuan Wu and Yong Zhang and Yiming Li and Zhifeng Li and Shu-Tao Xia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iKQAk8a2kM0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iKQAk8a2kM0", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper289/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper289/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper289/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper289/Authors|ICLR.cc/2021/Conference/Paper289/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper289/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872605, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper289/-/Official_Comment"}}}, {"id": "m4m67DpFxx", "original": null, "number": 6, "cdate": 1605607550345, "ddate": null, "tcdate": 1605607550345, "tmdate": 1605607611420, "tddate": null, "forum": "iKQAk8a2kM0", "replyto": "-QEZB02xvjw", "invitation": "ICLR.cc/2021/Conference/Paper289/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "Thank you for all your valuable comments and suggestions. We respond to your concerns below.\n\n**Q1&Q2**: Why the number of flipped bits seems to matter. It is not really clear why this work is preferred over prior methods, i.e., does reducing the number of flipped bits matter in the attack?\n\\\n**A1&A2**: Thank you for pointing out this concern. Why it matters for the attack can be explained from three aspects as follows and we have added the explanation to the third paragraph in Section 3.2 in the revised paper.\n- The constraint on the number of flipped bits was originally introduced in Section 3.2 for ensuring the stealthiness. The curves in Appendix H verify that flipping too many bits hurts the PA-ACC.\n- Physical bit flipping techniques can be time consuming as discussed in [1,2]. Moreover, such techniques lead to abnormal behaviors in the attacked system ($e.g.$, suspicious cache activity of processes), which may be detected by some physical  detection-based defenses  [3]. Therefore, minimizing the number of flipped bits is essential to make the attack more efficient and practical. Besides, several recent works [2,4,5] also considered a minimum modification on the weights when performing the weight attack.\n- ASR, PA-ACC, and $\\mathrm{N_{flip}}$ are three evaluation dimensions, which should be integrated to measure the attack performance, as commented by reviewer 3. Our method outperforms or at least is comparable with other methods in terms of ASR and PA-ACC and achieves less $\\mathrm{N_{flip}}$, which demonstrates that the proposed method is overall a better attack.\n\n[1] Van Der Veen, Victor, et al. Drammer: Deterministic rowhammer attacks on mobile platforms. CCS, 2016.\n\\\n[2] Zhao, Pu, et al. Fault Sneaking Attack: a Stealthy Framework for Misleading Deep Neural Networks. DAC, 2019.\n\\\n[3] Gruss, Daniel, et al. Another Flip in the Wall of Rowhammer Defenses. IEEE S&P, 2018.\n\\\n[4] Rakin, Adnan Siraj, Zhezhi He, and Deliang Fan. Bit-Flip Attack: Crushing Neural Network with Progressive Bit Search. ICCV, 2019.\n\\\n[5] Rakin, Adnan Siraj, Zhezhi He, and Deliang Fan. TBT: Targeted Neural Network Attack with Bit Trojan. CVPR, 2020.\n\n**Q3**: Why the current \"single image\" attack is preferred over other works.\n\\\n**A3**: Thank you for your insightful comment. We agree that \"single image\" attack is a special case of attacking more images to the target class. Nevertheless, studying this threat scenario is still very meaningful, as follows:\n- In some real attack scenarios, attacking a specific sample may meet the attacker\u2019s requirement. For example, the attacker wants to manipulate the behavior of a face recognition engine on a specific input. The investigation of such a special case (\"single image\" attack) may yield a more stealthy attack method, therefore it is worthy of further exploration. \n- In addition, one of our contributions is to formulate the bit-flip based weight attack as a BIP problem, which can also be extended to attack more images. We will study it further in our future works. \n\n**Q4**: Evaluation with the defense which applied at the inference phase.\n\\\n**A4**: Thanks for your valuable suggestion. The suggested defense at the inference phase is really insightful and deserves attention. In our paper, the experiment on the models with defense aims to verify the superiority of our optimization-based method in attacking more robust models. The robustness of model parameters is not well explored yet, especially for defense. If the reference [1] (it is missing in the current review) is pointed out, we would like to try to evaluate attacks under such defense. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper289/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper289/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits", "authorids": ["~Jiawang_Bai2", "~Baoyuan_Wu1", "~Yong_Zhang6", "~Yiming_Li1", "~Zhifeng_Li5", "~Shu-Tao_Xia1"], "authors": ["Jiawang Bai", "Baoyuan Wu", "Yong Zhang", "Yiming Li", "Zhifeng Li", "Shu-Tao Xia"], "keywords": ["targeted attack", "bit-flip", "weight attack"], "abstract": "To explore the vulnerability of deep neural networks (DNNs), many attack paradigms have been well studied, such as the poisoning-based backdoor attack in the training stage and the adversarial attack in the inference stage. In this paper, we study a novel attack paradigm, which modifies model parameters in the deployment stage for malicious purposes. Specifically, our goal is to misclassify a specific sample into a target class without any sample modification, while not significantly reduce the prediction accuracy of other samples to ensure the stealthiness. To this end, we formulate this problem as a binary integer programming (BIP), since the parameters are stored as binary bits ($i.e.$, 0 and 1) in the memory. By utilizing the latest technique in integer programming, we equivalently reformulate this BIP problem as a continuous optimization problem, which can be effectively and efficiently solved using the alternating direction method of multipliers (ADMM) method. Consequently, the flipped critical bits can be easily determined through optimization, rather than using a heuristic strategy. Extensive experiments demonstrate the superiority of our method in attacking DNNs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bai|targeted_attack_against_deep_neural_networks_via_flipping_limited_weight_bits", "one-sentence_summary": "We propose a targeted attack method against the deployed DNN via flipping a few binary weight bits.", "supplementary_material": "/attachment/00af28d0b277b26bb58e197d38763a8c8c6a8807.zip", "pdf": "/pdf/ed4d75e28ae70ba28f4895cf7097cf634745d11a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbai2021targeted,\ntitle={Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits},\nauthor={Jiawang Bai and Baoyuan Wu and Yong Zhang and Yiming Li and Zhifeng Li and Shu-Tao Xia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iKQAk8a2kM0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iKQAk8a2kM0", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper289/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper289/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper289/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper289/Authors|ICLR.cc/2021/Conference/Paper289/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper289/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872605, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper289/-/Official_Comment"}}}, {"id": "08TL6WuK8YF", "original": null, "number": 4, "cdate": 1605607417585, "ddate": null, "tcdate": 1605607417585, "tmdate": 1605607417585, "tddate": null, "forum": "iKQAk8a2kM0", "replyto": "0MrD1z4id3", "invitation": "ICLR.cc/2021/Conference/Paper289/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Thanks for your recognition of this work and all valuable comments. Your comments are a great encouragement to us!"}, "signatures": ["ICLR.cc/2021/Conference/Paper289/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper289/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits", "authorids": ["~Jiawang_Bai2", "~Baoyuan_Wu1", "~Yong_Zhang6", "~Yiming_Li1", "~Zhifeng_Li5", "~Shu-Tao_Xia1"], "authors": ["Jiawang Bai", "Baoyuan Wu", "Yong Zhang", "Yiming Li", "Zhifeng Li", "Shu-Tao Xia"], "keywords": ["targeted attack", "bit-flip", "weight attack"], "abstract": "To explore the vulnerability of deep neural networks (DNNs), many attack paradigms have been well studied, such as the poisoning-based backdoor attack in the training stage and the adversarial attack in the inference stage. In this paper, we study a novel attack paradigm, which modifies model parameters in the deployment stage for malicious purposes. Specifically, our goal is to misclassify a specific sample into a target class without any sample modification, while not significantly reduce the prediction accuracy of other samples to ensure the stealthiness. To this end, we formulate this problem as a binary integer programming (BIP), since the parameters are stored as binary bits ($i.e.$, 0 and 1) in the memory. By utilizing the latest technique in integer programming, we equivalently reformulate this BIP problem as a continuous optimization problem, which can be effectively and efficiently solved using the alternating direction method of multipliers (ADMM) method. Consequently, the flipped critical bits can be easily determined through optimization, rather than using a heuristic strategy. Extensive experiments demonstrate the superiority of our method in attacking DNNs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bai|targeted_attack_against_deep_neural_networks_via_flipping_limited_weight_bits", "one-sentence_summary": "We propose a targeted attack method against the deployed DNN via flipping a few binary weight bits.", "supplementary_material": "/attachment/00af28d0b277b26bb58e197d38763a8c8c6a8807.zip", "pdf": "/pdf/ed4d75e28ae70ba28f4895cf7097cf634745d11a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbai2021targeted,\ntitle={Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits},\nauthor={Jiawang Bai and Baoyuan Wu and Yong Zhang and Yiming Li and Zhifeng Li and Shu-Tao Xia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iKQAk8a2kM0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iKQAk8a2kM0", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper289/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper289/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper289/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper289/Authors|ICLR.cc/2021/Conference/Paper289/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper289/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872605, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper289/-/Official_Comment"}}}, {"id": "wIrPpEa32DA", "original": null, "number": 4, "cdate": 1605033988886, "ddate": null, "tcdate": 1605033988886, "tmdate": 1605033988886, "tddate": null, "forum": "iKQAk8a2kM0", "replyto": "iKQAk8a2kM0", "invitation": "ICLR.cc/2021/Conference/Paper289/-/Official_Review", "content": {"title": "Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits", "review": "The paper proposes an optimization-based algorithm for bit-flipping a limited number\nof bits in a quantized / binarized deep-learning model, so that the prediction on a\ntarget input example is flipped while the prediction on the other examples is as\nuntouched as possible. The problem is formulated as a binary integer programming (BIP)\nproblem, which is then solved using a recent ADMM-based technique. Experiments CIFAR-10\nand ImageNet show that the proposed method outperforms the SOTA.\n\n**Weak points:**\n- The main shortcoming of this paper is the limitedness of the technical contributions.\n\n- The hyper-parameter tuning is not clearly outlined / explained. This is problematic\n  since there are quite a number of hyper-parameters. For example, I can count 4\n  hyper-parameters in equation (9) alone (including ADMM stepsizes rho_i).\n\n- It would be nice to have a back-of-envelop estimation of the complexity (running time,\n  number of flops, etc.) of the proposed method, as a function of the maximum number of\n  bits to flip (say).\n\n**Small issues:**\n- S. Boyd and co-workers have done a great job in popularizing ADMM. However, this method\n  has been around at least since the 70s. Key papers to reference when talking about ADMM\n  include:\n  * Glowinski and Marroco (1975) \"Sur l'approximation, par \u00e9l\u00e9ments finis d'ordre un, et\n    la r\u00e9solution par p\u00e9nalisation-dualit\u00e9 d'une classe de probl\u00e8mes de Dirichlet non\n    lin\u00e9aires dualite d'une classe de problemes de Dirichlet non lin\u00e9aires\"\n  * Gabay and Mercier (1976) \"A dual algorithm for the solution of nonlinear variational\n    problems via finite element approximation\"\n\n**Strong points:**\n- The strongest point in favor of this paper is that unlike the SOTA methods, the proposed\n  method only flips to a very limited number of bits in the binarized DNN model, while\n  achieving the same or higher accuracy.\n- The experiments are very detailed and well-presented.\n\n**Errors:**\n\n- The equivalence in (6) doesn't seem to make sense. In the definition of $S_p$, $\\hat{b}$ is an element of what ?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper289/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper289/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits", "authorids": ["~Jiawang_Bai2", "~Baoyuan_Wu1", "~Yong_Zhang6", "~Yiming_Li1", "~Zhifeng_Li5", "~Shu-Tao_Xia1"], "authors": ["Jiawang Bai", "Baoyuan Wu", "Yong Zhang", "Yiming Li", "Zhifeng Li", "Shu-Tao Xia"], "keywords": ["targeted attack", "bit-flip", "weight attack"], "abstract": "To explore the vulnerability of deep neural networks (DNNs), many attack paradigms have been well studied, such as the poisoning-based backdoor attack in the training stage and the adversarial attack in the inference stage. In this paper, we study a novel attack paradigm, which modifies model parameters in the deployment stage for malicious purposes. Specifically, our goal is to misclassify a specific sample into a target class without any sample modification, while not significantly reduce the prediction accuracy of other samples to ensure the stealthiness. To this end, we formulate this problem as a binary integer programming (BIP), since the parameters are stored as binary bits ($i.e.$, 0 and 1) in the memory. By utilizing the latest technique in integer programming, we equivalently reformulate this BIP problem as a continuous optimization problem, which can be effectively and efficiently solved using the alternating direction method of multipliers (ADMM) method. Consequently, the flipped critical bits can be easily determined through optimization, rather than using a heuristic strategy. Extensive experiments demonstrate the superiority of our method in attacking DNNs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bai|targeted_attack_against_deep_neural_networks_via_flipping_limited_weight_bits", "one-sentence_summary": "We propose a targeted attack method against the deployed DNN via flipping a few binary weight bits.", "supplementary_material": "/attachment/00af28d0b277b26bb58e197d38763a8c8c6a8807.zip", "pdf": "/pdf/ed4d75e28ae70ba28f4895cf7097cf634745d11a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbai2021targeted,\ntitle={Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits},\nauthor={Jiawang Bai and Baoyuan Wu and Yong Zhang and Yiming Li and Zhifeng Li and Shu-Tao Xia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iKQAk8a2kM0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "iKQAk8a2kM0", "replyto": "iKQAk8a2kM0", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper289/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538146428, "tmdate": 1606915799134, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper289/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper289/-/Official_Review"}}}, {"id": "-QEZB02xvjw", "original": null, "number": 1, "cdate": 1603864652412, "ddate": null, "tcdate": 1603864652412, "tmdate": 1605024722938, "tddate": null, "forum": "iKQAk8a2kM0", "replyto": "iKQAk8a2kM0", "invitation": "ICLR.cc/2021/Conference/Paper289/-/Official_Review", "content": {"title": "Unclear motivation and improvement over prior works", "review": "The paper proposes a bit-flip attack where model parameters weights are altered such that a certain sample is misclassified to a target class.  While the utilized optimization strategy and the combination of techniques seems interesting, a major concern is the motivation behind the proposed method and why it stands out against prior works. \n- Comparisons in the experiment section show that prior methods are performing on par with the proposed method in terms of the attack success rate and benign accuracy. The major difference seems to be the number of flipped bits. However, the paper in its current form does not state \"why\" the number of flipped bits seems to matter. \n- The method is also performing on par with prior works in terms of the resiliency against defense mechanisms. Therefore, it is not really clear why this work is preferred over prior methods, i.e., does reducing the number of flipped bits matter in the attack?\n- In addition to motivating the number of flipped bits, the authors need to also clarify why the current \"single image\" attack is preferred over other works where all images from a certain class are mapped to the attack target class. It seems like the proposed approach is in fact a special case of the latter scenario which is studied in prior works.\n- The evaluated defense strategies are all passive, i.e., they are performed before the attack and therefore are not aware of the attack strategy. For a comprehensive examination, the authors should also compare with the defense in [1] which applied the defense at the inference phase, assuming a bit-flip attack may have occurred on the model parameters.\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper289/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper289/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits", "authorids": ["~Jiawang_Bai2", "~Baoyuan_Wu1", "~Yong_Zhang6", "~Yiming_Li1", "~Zhifeng_Li5", "~Shu-Tao_Xia1"], "authors": ["Jiawang Bai", "Baoyuan Wu", "Yong Zhang", "Yiming Li", "Zhifeng Li", "Shu-Tao Xia"], "keywords": ["targeted attack", "bit-flip", "weight attack"], "abstract": "To explore the vulnerability of deep neural networks (DNNs), many attack paradigms have been well studied, such as the poisoning-based backdoor attack in the training stage and the adversarial attack in the inference stage. In this paper, we study a novel attack paradigm, which modifies model parameters in the deployment stage for malicious purposes. Specifically, our goal is to misclassify a specific sample into a target class without any sample modification, while not significantly reduce the prediction accuracy of other samples to ensure the stealthiness. To this end, we formulate this problem as a binary integer programming (BIP), since the parameters are stored as binary bits ($i.e.$, 0 and 1) in the memory. By utilizing the latest technique in integer programming, we equivalently reformulate this BIP problem as a continuous optimization problem, which can be effectively and efficiently solved using the alternating direction method of multipliers (ADMM) method. Consequently, the flipped critical bits can be easily determined through optimization, rather than using a heuristic strategy. Extensive experiments demonstrate the superiority of our method in attacking DNNs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bai|targeted_attack_against_deep_neural_networks_via_flipping_limited_weight_bits", "one-sentence_summary": "We propose a targeted attack method against the deployed DNN via flipping a few binary weight bits.", "supplementary_material": "/attachment/00af28d0b277b26bb58e197d38763a8c8c6a8807.zip", "pdf": "/pdf/ed4d75e28ae70ba28f4895cf7097cf634745d11a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbai2021targeted,\ntitle={Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits},\nauthor={Jiawang Bai and Baoyuan Wu and Yong Zhang and Yiming Li and Zhifeng Li and Shu-Tao Xia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iKQAk8a2kM0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "iKQAk8a2kM0", "replyto": "iKQAk8a2kM0", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper289/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538146428, "tmdate": 1606915799134, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper289/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper289/-/Official_Review"}}}, {"id": "MmHRoSJzQNK", "original": null, "number": 2, "cdate": 1603938697448, "ddate": null, "tcdate": 1603938697448, "tmdate": 1605024722860, "tddate": null, "forum": "iKQAk8a2kM0", "replyto": "iKQAk8a2kM0", "invitation": "ICLR.cc/2021/Conference/Paper289/-/Official_Review", "content": {"title": "Review", "review": "This paper proposes an ADMM based optimization method to conduct adversarial weight attack, and achieves superior or at least comparable performance compared with previous heuristic methods.\n\nPros:\n1. Adversarial weight attack is an interesting research direction with important practical importance and deserve more studies. \n2. The proposed method is mathematically sound. And it empirically outperforms or at least is comparable with previous state-of-the-art methods on undefended models, and consistently outperforms previous methods on defended models.\n\nCons:\nI think this paper as an necessary step towards stronger adversarial weight attacks, which could be used as an evaluation method to benchmark future defense methods.  \n\nSome comments:\n1. Table 1 and 2 may not be the best way to present the results. Considering there are three evaluation dimensions (PA-ACC, ASR and Nflip), I suggest the authors to add some pareto frontier figures. For example, fixing PA-ACC, plot the tradeoff curves between ASR and Nflip of different methods.\n2. What are the time costs of different attacking methods?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper289/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper289/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits", "authorids": ["~Jiawang_Bai2", "~Baoyuan_Wu1", "~Yong_Zhang6", "~Yiming_Li1", "~Zhifeng_Li5", "~Shu-Tao_Xia1"], "authors": ["Jiawang Bai", "Baoyuan Wu", "Yong Zhang", "Yiming Li", "Zhifeng Li", "Shu-Tao Xia"], "keywords": ["targeted attack", "bit-flip", "weight attack"], "abstract": "To explore the vulnerability of deep neural networks (DNNs), many attack paradigms have been well studied, such as the poisoning-based backdoor attack in the training stage and the adversarial attack in the inference stage. In this paper, we study a novel attack paradigm, which modifies model parameters in the deployment stage for malicious purposes. Specifically, our goal is to misclassify a specific sample into a target class without any sample modification, while not significantly reduce the prediction accuracy of other samples to ensure the stealthiness. To this end, we formulate this problem as a binary integer programming (BIP), since the parameters are stored as binary bits ($i.e.$, 0 and 1) in the memory. By utilizing the latest technique in integer programming, we equivalently reformulate this BIP problem as a continuous optimization problem, which can be effectively and efficiently solved using the alternating direction method of multipliers (ADMM) method. Consequently, the flipped critical bits can be easily determined through optimization, rather than using a heuristic strategy. Extensive experiments demonstrate the superiority of our method in attacking DNNs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bai|targeted_attack_against_deep_neural_networks_via_flipping_limited_weight_bits", "one-sentence_summary": "We propose a targeted attack method against the deployed DNN via flipping a few binary weight bits.", "supplementary_material": "/attachment/00af28d0b277b26bb58e197d38763a8c8c6a8807.zip", "pdf": "/pdf/ed4d75e28ae70ba28f4895cf7097cf634745d11a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbai2021targeted,\ntitle={Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits},\nauthor={Jiawang Bai and Baoyuan Wu and Yong Zhang and Yiming Li and Zhifeng Li and Shu-Tao Xia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iKQAk8a2kM0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "iKQAk8a2kM0", "replyto": "iKQAk8a2kM0", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper289/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538146428, "tmdate": 1606915799134, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper289/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper289/-/Official_Review"}}}, {"id": "0MrD1z4id3", "original": null, "number": 3, "cdate": 1604002707054, "ddate": null, "tcdate": 1604002707054, "tmdate": 1605024722788, "tddate": null, "forum": "iKQAk8a2kM0", "replyto": "iKQAk8a2kM0", "invitation": "ICLR.cc/2021/Conference/Paper289/-/Official_Review", "content": {"title": "Review", "review": "The paper describes a bit-flipping white-box attack on deployed neural network classifiers: given a model with quantized parameters, find a perturbation of the parameters bits such that the model with misclassify one specific example, while maintaining high accuracy on other examples.\n\nThe attack is formulated as a binary programmig problem where the parameter bits are the optimization variables and the objective function is an additive tradeoff between an effectiveness term (misclassification loss on the selected example) and a stealthness loss (classification loss on a batch of training examples), a constraint on the number of bit flips is also included. The optimization problem is solved by continuous relaxation using the Lp-box ADMM solver.\n\nThe paper reports experiments on various standard classifiers trained on CIFAR-10 or ImageNet, with different level of quantization. The proposed attack is compared to other weight attacks in the literature, and it achieves comparable or better attack success rate (a measure of effectiveness) and post-attack accuracy (a measure of stealthness).\nThere are also experiments on different values of hyperparameters and on more robust models (obtained either by a defense technique or by making the model bigger).\n\nOverall I find this a valid contribution.\n", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper289/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper289/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits", "authorids": ["~Jiawang_Bai2", "~Baoyuan_Wu1", "~Yong_Zhang6", "~Yiming_Li1", "~Zhifeng_Li5", "~Shu-Tao_Xia1"], "authors": ["Jiawang Bai", "Baoyuan Wu", "Yong Zhang", "Yiming Li", "Zhifeng Li", "Shu-Tao Xia"], "keywords": ["targeted attack", "bit-flip", "weight attack"], "abstract": "To explore the vulnerability of deep neural networks (DNNs), many attack paradigms have been well studied, such as the poisoning-based backdoor attack in the training stage and the adversarial attack in the inference stage. In this paper, we study a novel attack paradigm, which modifies model parameters in the deployment stage for malicious purposes. Specifically, our goal is to misclassify a specific sample into a target class without any sample modification, while not significantly reduce the prediction accuracy of other samples to ensure the stealthiness. To this end, we formulate this problem as a binary integer programming (BIP), since the parameters are stored as binary bits ($i.e.$, 0 and 1) in the memory. By utilizing the latest technique in integer programming, we equivalently reformulate this BIP problem as a continuous optimization problem, which can be effectively and efficiently solved using the alternating direction method of multipliers (ADMM) method. Consequently, the flipped critical bits can be easily determined through optimization, rather than using a heuristic strategy. Extensive experiments demonstrate the superiority of our method in attacking DNNs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bai|targeted_attack_against_deep_neural_networks_via_flipping_limited_weight_bits", "one-sentence_summary": "We propose a targeted attack method against the deployed DNN via flipping a few binary weight bits.", "supplementary_material": "/attachment/00af28d0b277b26bb58e197d38763a8c8c6a8807.zip", "pdf": "/pdf/ed4d75e28ae70ba28f4895cf7097cf634745d11a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbai2021targeted,\ntitle={Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits},\nauthor={Jiawang Bai and Baoyuan Wu and Yong Zhang and Yiming Li and Zhifeng Li and Shu-Tao Xia},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iKQAk8a2kM0}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "iKQAk8a2kM0", "replyto": "iKQAk8a2kM0", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper289/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538146428, "tmdate": 1606915799134, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper289/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper289/-/Official_Review"}}}], "count": 13}