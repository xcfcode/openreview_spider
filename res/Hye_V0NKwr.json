{"notes": [{"id": "Hye_V0NKwr", "original": "ryliyrUuDB", "number": 1080, "cdate": 1569439279811, "ddate": null, "tcdate": 1569439279811, "tmdate": 1583912027641, "tddate": null, "forum": "Hye_V0NKwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["tristan.sylvain@gmail.com", "lindapetrini@gmail.com", "devon.hjelm@microsoft.com"], "title": "Locality and Compositionality in Zero-Shot Learning", "authors": ["Tristan Sylvain", "Linda Petrini", "Devon Hjelm"], "pdf": "/pdf/ae21cd01627929186c1673ccc2f9b1c9cc1dfc2e.pdf", "TL;DR": "An analysis of the effects of compositionality and locality on representation learning for zero-shot learning.", "abstract": "In this work we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). \nIn order to well-isolate the importance of these properties in learned representations, we impose the additional constraint that, differently from most recent work in ZSL, no pre-training on different datasets (e.g. ImageNet) is performed.\nThe results of our experiment show how locality, in terms of small parts of the input, and compositionality, i.e. how well can the learned representations be expressed as a function of a smaller vocabulary, are both deeply related to generalization and motivate the focus on more local-aware models in future research directions for representation learning.", "keywords": ["Zero-shot learning", "Compositionality", "Locality", "Deep Learning"], "paperhash": "sylvain|locality_and_compositionality_in_zeroshot_learning", "_bibtex": "@inproceedings{\nsylvain2020locality,\ntitle={Locality and Compositionality in Zero-Shot Learning},\nauthor={Tristan Sylvain and Linda Petrini and Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye_V0NKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d55a6ecb5843fdb65af828aff26c08d11c2d2ce0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "u0GJlWsq_", "original": null, "number": 1, "cdate": 1576798714011, "ddate": null, "tcdate": 1576798714011, "tmdate": 1576800922468, "tddate": null, "forum": "Hye_V0NKwr", "replyto": "Hye_V0NKwr", "invitation": "ICLR.cc/2020/Conference/Paper1080/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper investigates the role of locality (ability to encode only information specific to locations of interest) and compositionality (ability to be expressed as a combination of simpler parts) in Zero-Shot Learning (ZSL). Main contributions of the paper are (i) compared to previous ZSL frameworks, the proposed approach is that the model is not allowed to be pretrained on another dataset (ii) a thorough evaluation of existing methods.\n\nFollowing discussions, weaknesses are (i) the proposed method (CMDIM) isn't sufficiently different or interesting compared to existing methods (ii) the paper does not do an in-depth discussion of locality and compositionality. The empirical evaluation being extensive, the accept decision is chosen.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tristan.sylvain@gmail.com", "lindapetrini@gmail.com", "devon.hjelm@microsoft.com"], "title": "Locality and Compositionality in Zero-Shot Learning", "authors": ["Tristan Sylvain", "Linda Petrini", "Devon Hjelm"], "pdf": "/pdf/ae21cd01627929186c1673ccc2f9b1c9cc1dfc2e.pdf", "TL;DR": "An analysis of the effects of compositionality and locality on representation learning for zero-shot learning.", "abstract": "In this work we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). \nIn order to well-isolate the importance of these properties in learned representations, we impose the additional constraint that, differently from most recent work in ZSL, no pre-training on different datasets (e.g. ImageNet) is performed.\nThe results of our experiment show how locality, in terms of small parts of the input, and compositionality, i.e. how well can the learned representations be expressed as a function of a smaller vocabulary, are both deeply related to generalization and motivate the focus on more local-aware models in future research directions for representation learning.", "keywords": ["Zero-shot learning", "Compositionality", "Locality", "Deep Learning"], "paperhash": "sylvain|locality_and_compositionality_in_zeroshot_learning", "_bibtex": "@inproceedings{\nsylvain2020locality,\ntitle={Locality and Compositionality in Zero-Shot Learning},\nauthor={Tristan Sylvain and Linda Petrini and Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye_V0NKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d55a6ecb5843fdb65af828aff26c08d11c2d2ce0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Hye_V0NKwr", "replyto": "Hye_V0NKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795711028, "tmdate": 1576800260142, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1080/-/Decision"}}}, {"id": "SJeARAZTYr", "original": null, "number": 2, "cdate": 1571786454284, "ddate": null, "tcdate": 1571786454284, "tmdate": 1574493851809, "tddate": null, "forum": "Hye_V0NKwr", "replyto": "Hye_V0NKwr", "invitation": "ICLR.cc/2020/Conference/Paper1080/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "UPDATE: My recommendation has been borderline because the discussion of the paper about the nature of locality and compositionality seems to be less in-depth than I would have expected, but if the authors will revise the submission to shift the focus of the paper to more focused on analysis and evaluation and weaken the claims that their model exhibits locality and compositionality, I would lean towards an accept as the empirical evaluation is extensive.\n\n----\nSummary: This paper aims to investigate the role of locality and compositionality in zero-shot learning. The authors propose a novel evaluation setup that differs from the original zero-shot learning framework in that the model is not allowed to be pretrained on another dataset. The authors propose several methods for learning and visualizing local representations.\n\nThe overall problem the paper tackles is quite ambitious which involve many parts: defining locality and compositionality, defining constraints for enforcing locality and compositionality, evaluating the existence of locality and compositionality, and evaluating model performance. Because of the extensive nature of the research problem, the coverage of each piece of this research problem in this paper could have been more thorough: for example, I would be really excited for a more in-depth analysis on methods for enforcing locality, but the authors consider only one method, which is the auxiliary loss, and this method does not seem to be applicable for datasets that do not have attribute labels. Given the formalism in section 2.1, I would furthermore be excited about a more in-depth analysis on methods for enforcing compositionality, but the authors only investigate compositionality from the perspective of a weighted average. As a result, the claims about whether enforcing locality and compositionality helps with generalization seem weak, as the authors only do not seem to consider different ways of achieving locality and compositionality -- it is not clear whether their method for enforcing locality (with auxiliary losses) or compositionality (by explicitly performing a weighted average) is representative of enforcing locality and compositionality in general. Furthermore, locality seems to be more of a statement about *ignoring* information (discussed below), which the authors do not explore. As a result, my recommendation is borderline. Perhaps the paper could benefit by reframing it with a more specific focus, with more emphasis on the evaluation, which the paper seems to do well. I would recommend that the authors either (1) perform a more extensive analysis of methods for enforcing locality and compositionality or (2) shift the focus of the paper to more focused on analysis and evaluation and weaken the claims that their model exhibits locality and compositionality, as there is not enough evidence to tell whether their method for enforcing locality and compositionality is the best way of doing so.\n\n\nStrengths:\n- The paper is well-motivated and tackles an important problem.\n- The empirical evaluation is extensive.\n- The framework of not relying on pretrained features is a novel contribution.\n\nWeaknesses:\n- There does not seem to be a comparison between a compositional model and a non-compositional model in section 5.3. Would the authors be able to provide such an analysis? Otherwise it is difficult to tell exactly how and to what extent compositionality helps.\n- I would have assumed that enforcing locality should actually be a problem of *ignoring* information, rather than *predicting* labeled information. However, the auxiliary loss introduction in section 4.1 only seems to enforce that local features (which by default are local from the CNN) are predictive of attributes/class, rather than enforcing that these features ignore information that is not local to the particular image patch they are modeling. Would the authors be able to comment on this point, as well as provide an empirical analysis of how *unpredictive* the features of their model are on image patches elsewhere?", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1080/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1080/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tristan.sylvain@gmail.com", "lindapetrini@gmail.com", "devon.hjelm@microsoft.com"], "title": "Locality and Compositionality in Zero-Shot Learning", "authors": ["Tristan Sylvain", "Linda Petrini", "Devon Hjelm"], "pdf": "/pdf/ae21cd01627929186c1673ccc2f9b1c9cc1dfc2e.pdf", "TL;DR": "An analysis of the effects of compositionality and locality on representation learning for zero-shot learning.", "abstract": "In this work we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). \nIn order to well-isolate the importance of these properties in learned representations, we impose the additional constraint that, differently from most recent work in ZSL, no pre-training on different datasets (e.g. ImageNet) is performed.\nThe results of our experiment show how locality, in terms of small parts of the input, and compositionality, i.e. how well can the learned representations be expressed as a function of a smaller vocabulary, are both deeply related to generalization and motivate the focus on more local-aware models in future research directions for representation learning.", "keywords": ["Zero-shot learning", "Compositionality", "Locality", "Deep Learning"], "paperhash": "sylvain|locality_and_compositionality_in_zeroshot_learning", "_bibtex": "@inproceedings{\nsylvain2020locality,\ntitle={Locality and Compositionality in Zero-Shot Learning},\nauthor={Tristan Sylvain and Linda Petrini and Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye_V0NKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d55a6ecb5843fdb65af828aff26c08d11c2d2ce0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hye_V0NKwr", "replyto": "Hye_V0NKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1080/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1080/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575767944395, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1080/Reviewers"], "noninvitees": [], "tcdate": 1570237742652, "tmdate": 1575767944411, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1080/-/Official_Review"}}}, {"id": "rkx4AoO0Yr", "original": null, "number": 3, "cdate": 1571879884004, "ddate": null, "tcdate": 1571879884004, "tmdate": 1574398519926, "tddate": null, "forum": "Hye_V0NKwr", "replyto": "Hye_V0NKwr", "invitation": "ICLR.cc/2020/Conference/Paper1080/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "###  Summary \n- The paper tries to understand what learning principles can lead to representations that allow zero-shot learning/generalization. \n- It explores the impact of two properties -- locality and compositionality -- on ZSL performance. \n- To encourage locality (and also compositionality since they can overlap), the paper uses an auxiliary loss at an earlier layer (when the receptive field of features is small) to predict local attributes. \n- For interpreting local features, the paper computes MI between the global feature map of one image and local features from a different image of the same class and visualizes it using a heatmap.  \n- It uses normalized TRE to measure compositionality (Normalized to take into account that some methods are biased towards learning more compositional representations.)\n- It compares supervised, unsupervised, and self-supervised representation learning methods and studies the impact of locality and compositionality on ZSL performance for these methods. \n\n### Decision and reasons\nI vote for a weak accept. However, I think it's a good paper and does a lot of things right. I'm willing to increase my score if the authors can convincingly answer my questions.  \n\nPositives: \n1- The proposed zero-shot learning from scratch setting is a step in the right direction for focusing on uncovering general learning principles. \n\n2- Locality and compositionality are sensible goals for good representations. The paper defines both and explores their importance with clear and novel experiment-designs and metrics. The experiments are also well conducted.\n\nNegatives: \n\n1- The paper makes a lot of observations, but sometimes does not even try to explain some unexpected observations. \n2- CMDIM is presented as a self-supervised algorithm but seems to require class labels. \n\n### Supporting arguments for the reasons for the decision.\n\nPositives: \n\n1- I agree with the paper that ZSL setting is more about uncovering learning principles and less about constructing practical systems that can do well in zero-shot settings. In a practical setting, it doesn't make much sense to zero-shot learning anyway. I also agree that current ZSL work is focusing too much on pushing the state-of-the-art using pre-trained imagenet features and missing the actual goal of the problem setting (I wouldn't say we shouldn't do that at all, however. Some learning principles may require a significant amount of data to learn good representations for zero-shot learning, and imagenet pretraining is a good proxy for that). This paper acts as a good reminder that ZSL research should be done keeping in mind the goal of ZSL.\n\n2- The paper first defines locality and compositionality. It then uses interesting and meaningful metrics for empirically testing if these properties correlate with zero-shot performance. I found the experiment designs to be clever (such as computing parts F-1 score using boolean maps, visualizing MI between local and global features of images from the same class, etc).  I think this paper will act as an important reference for motivating future work on learning more modular representations. \n\nNegatives: \n\n1- The paper doesn't try to explain the possible reasons behind some observations. For example, why does locality not correlate with ZSL performance of generative models? Why does LC loss hurt performance for CMDIM? \n2- CMDIM draws positive samples from other images of the same class. As a result, it's not a truly unsupervised learning method. The direct comparison of CMDIM to DIM seems unfair. \n\n### Questions\n\nQ1-  Computation of the local classification and attribute auxiliary loss is a bit unclear. How is it assured that a certain attribute is present in the local feature when computing the auxiliary attribute-loss? \n\nQ2- Did the authors try a baseline in which AC is also trained on the global representation in the context of Figure 2? The extra information about the parts may be the reason behind better ZSL performance, and a global AC could improve ZSL performance without improving Parts F1 score (Although a more likely outcome is that a global AC would increase both the parts F1 score and the ZSL accuracy.)\n\nQ3- For VAEs, the local F-1 score does not correlate with ZSL performance. This seems to contradict the idea locality leads to better ZSL performance. Is there an explanation for this? The paper just glances over this by calling this 'interesting.' \n\nQ4- Can CMDIM be considered a self-supervised algorithm? Doesn't it need class labels to draw positive samples from the same class? \n\n\n### Other minor remarks \n\n- \"Formally, f(x) \\elem R is compositional if it can be expressed as a combination of the elements of ...\"\n\nThe definition would be more clear if the authors could mention some reasonable combination operators here (such as weighted average etc).\n\n- \"However, this choice in architecture does not guarantee locality, as CNN representations could only contain \u201cglobal\u201d information, such as the class or color of the object, despite having a limited receptive field. \"\n\nI'm not sure what this means. Why are CNN representations restricted to 'only' global information?\n\n### Update after author's response\n\nThe author's response fixes some minor clarity issues. I think this work is a good contribution and should be accepted. I've updated my score accordingly. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1080/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1080/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tristan.sylvain@gmail.com", "lindapetrini@gmail.com", "devon.hjelm@microsoft.com"], "title": "Locality and Compositionality in Zero-Shot Learning", "authors": ["Tristan Sylvain", "Linda Petrini", "Devon Hjelm"], "pdf": "/pdf/ae21cd01627929186c1673ccc2f9b1c9cc1dfc2e.pdf", "TL;DR": "An analysis of the effects of compositionality and locality on representation learning for zero-shot learning.", "abstract": "In this work we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). \nIn order to well-isolate the importance of these properties in learned representations, we impose the additional constraint that, differently from most recent work in ZSL, no pre-training on different datasets (e.g. ImageNet) is performed.\nThe results of our experiment show how locality, in terms of small parts of the input, and compositionality, i.e. how well can the learned representations be expressed as a function of a smaller vocabulary, are both deeply related to generalization and motivate the focus on more local-aware models in future research directions for representation learning.", "keywords": ["Zero-shot learning", "Compositionality", "Locality", "Deep Learning"], "paperhash": "sylvain|locality_and_compositionality_in_zeroshot_learning", "_bibtex": "@inproceedings{\nsylvain2020locality,\ntitle={Locality and Compositionality in Zero-Shot Learning},\nauthor={Tristan Sylvain and Linda Petrini and Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye_V0NKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d55a6ecb5843fdb65af828aff26c08d11c2d2ce0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hye_V0NKwr", "replyto": "Hye_V0NKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1080/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1080/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575767944395, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1080/Reviewers"], "noninvitees": [], "tcdate": 1570237742652, "tmdate": 1575767944411, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1080/-/Official_Review"}}}, {"id": "rkg4oKg3ir", "original": null, "number": 1, "cdate": 1573812636321, "ddate": null, "tcdate": 1573812636321, "tmdate": 1573844462599, "tddate": null, "forum": "Hye_V0NKwr", "replyto": "rkx4AoO0Yr", "invitation": "ICLR.cc/2020/Conference/Paper1080/-/Official_Comment", "content": {"title": "Author response ", "comment": "We thank the reviewer for their supportive comments and helpful suggestions. In what follows we will attempt to address their concerns, grouped by category.\n\nGeneral observations\nWhy does locality not correlate with ZSL performance for generative models?\nWe will address this question in our answer to Q3 below.\n\nWhy does LC loss hurt CMDIM performance?\n\tWe hypothesize this is due to the fact that both CMDIM and the LC loss focus on discriminating classes at the local level, and that the LC objective is inherently less effective than the CMDIM formulation for this task (in terms of downstream ZSL performance). As a result, forcing the model to account for both terms lowers downstream performance. A more detailed explanation has been added to 5.1.\n\n\nQuestions.\nQ1. We do not have ground truth values for the local presence or absence of an attribute (this might be feasible on CUB by combining part information with attributes which sometimes map to specific parts such as tail color, but impossible on SUN and AWA2 by construction). As a result, we consider the ground-truth value for each local feature (attribute or class) to be that of its class. This means that some uninformative features are counted in the loss term (e.g. a local feature corresponding to pure background will not be able to predict the class or the attributes) but on average, the loss term encourages local features to capture class/attribute information when possible. More complex setups in the field of attribute detection tend to use attention mechanisms to mitigate the effect of taking into account uninformative local features (such as a feature mapping to the background of the image) [1] but are outside the scope of this work.\n\nQ2. \nWe initially ran experiments where we considered versions of AC and LC on the global representation. For AC this is equivalent to multi-task learning combining the model and a ProtoNet. For LC the same applies for a supervised classifier. \nThe addition of the global loss changed learning dynamics (e.g. VAEs tended to ignore the reconstruction term) and overall hyperparameter selection resulted in either:\nWeaker (in terms of ZSL accuracy) models focusing on the AC and LC at the expense of their own loss term\nStronger models mainly ignoring AC and LC.\nThe part accuracy overall changed in a similar fashion: either staying close to the original model (ignoring AC/LC) or converging towards that of FC/PN. As this did not provide useful insights into the original model, we dropped this line of investigation. We believe local AC/LC behave differently as local versions of PN and FC do not dominate the performance of other models.\n\n\nQ3.\nThis observation is actually more generally true for all models that include a reconstruction objective, such as VAEs but also AAEs, as outlined in figure 2. We hypothesize that this is due to the fact that such models already have a good understanding of local information (essential for reconstructing) which is not really improved by the local objectives. The main issue with those models is that they might be \u201ctoo local\u201d: focusing on capturing non-essential (from a class-discriminative point of view) local information, such as an image background, at the expense of more important aspects of an image. \nIn addition to this, these models score poorly in terms of the measures of compositionality we have introduced. While we have considered locality and compositionality separately, they interact, and a model that is aware of local parts, but unable to combine them effectively will be weaker at ZSL.\n\nQ4. CMDIM is indeed a (weakly) supervised learning algorithm and does draw positive samples from the same class. Overall the goal of introducing this model is to show the improvement one can achieve by changing the nature of positive/negative samples in DIM to learn class-discriminative rather than instance-discriminative features. We have clarified the category name in 4.1 to highlight this (we put it in the same category as DIM/AMDIM due to conceptual similarities). A detailed description of CMDIM has been added to section G of the appendix.\n\nOther remarks.\n\u201cFormally \u2026\u201d. We have updated section 2.1 with examples of common combination operators, referencing more complex ones as found in e.g. [2]\n\u201cHowever \u2026 field.\u201d. We realize that this sentence was not clear and could lead to a misunderstanding. We clarified section 2.2 accordingly.\n\n\nBibliography\n[1] Liu, Xiao, et al. \"Localizing by describing: Attribute-guided attention localization for fine-grained recognition.\" Thirty-First AAAI Conference on Artificial Intelligence. 2017.\n\n[2] Higgins, Irina, et al. \"Scan: Learning hierarchical compositional visual concepts.\" arXiv preprint arXiv:1707.03389 (2017)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1080/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1080/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tristan.sylvain@gmail.com", "lindapetrini@gmail.com", "devon.hjelm@microsoft.com"], "title": "Locality and Compositionality in Zero-Shot Learning", "authors": ["Tristan Sylvain", "Linda Petrini", "Devon Hjelm"], "pdf": "/pdf/ae21cd01627929186c1673ccc2f9b1c9cc1dfc2e.pdf", "TL;DR": "An analysis of the effects of compositionality and locality on representation learning for zero-shot learning.", "abstract": "In this work we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). \nIn order to well-isolate the importance of these properties in learned representations, we impose the additional constraint that, differently from most recent work in ZSL, no pre-training on different datasets (e.g. ImageNet) is performed.\nThe results of our experiment show how locality, in terms of small parts of the input, and compositionality, i.e. how well can the learned representations be expressed as a function of a smaller vocabulary, are both deeply related to generalization and motivate the focus on more local-aware models in future research directions for representation learning.", "keywords": ["Zero-shot learning", "Compositionality", "Locality", "Deep Learning"], "paperhash": "sylvain|locality_and_compositionality_in_zeroshot_learning", "_bibtex": "@inproceedings{\nsylvain2020locality,\ntitle={Locality and Compositionality in Zero-Shot Learning},\nauthor={Tristan Sylvain and Linda Petrini and Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye_V0NKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d55a6ecb5843fdb65af828aff26c08d11c2d2ce0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hye_V0NKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1080/Authors", "ICLR.cc/2020/Conference/Paper1080/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1080/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1080/Reviewers", "ICLR.cc/2020/Conference/Paper1080/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1080/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1080/Authors|ICLR.cc/2020/Conference/Paper1080/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161570, "tmdate": 1576860556079, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1080/Authors", "ICLR.cc/2020/Conference/Paper1080/Reviewers", "ICLR.cc/2020/Conference/Paper1080/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1080/-/Official_Comment"}}}, {"id": "r1grNcg3iH", "original": null, "number": 3, "cdate": 1573812780719, "ddate": null, "tcdate": 1573812780719, "tmdate": 1573835802914, "tddate": null, "forum": "Hye_V0NKwr", "replyto": "HJgLUdTnYB", "invitation": "ICLR.cc/2020/Conference/Paper1080/-/Official_Comment", "content": {"title": "Author response", "comment": "Thank you for your thorough and helpful review. \n\nRegarding the encoder backbone, we do freeze the pre-trained encoder when training Protonet, as you have indicated is common practice in ZSL studies. We apologize that this was not clear, and we clarified this for future readers in the revision by adding information to section 4.5. Our comment on the size of the encoders was only meant to hedge against any readers who might feel the results were too low compared to Imagenet-pretrained results from larger architectures (an example of such an improvement due to the size of the encoder is [1]).\n\nWe also clarified CMDIM in our revision, and this can be found in section G of the appendix. To clarify here: CMDIM only differs from DIM / AMDIM in that positive samples are drawn from a mixture of patches from the same image and other images with the same label. As such, CMDIM is not strictly self-supervised, as one reviewer notes, and this can be thought of as training the model to be good at retrieving patches from a query with the same label. We added CMDIM following our observations in Section 5.2 to improve the ability of DIM to recognize similar parts between different images.\n\nBibliography\n[1] Xian, Yongqin, Bernt Schiele, and Zeynep Akata. \"Zero-shot learning-the good, the bad and the ugly.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1080/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1080/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tristan.sylvain@gmail.com", "lindapetrini@gmail.com", "devon.hjelm@microsoft.com"], "title": "Locality and Compositionality in Zero-Shot Learning", "authors": ["Tristan Sylvain", "Linda Petrini", "Devon Hjelm"], "pdf": "/pdf/ae21cd01627929186c1673ccc2f9b1c9cc1dfc2e.pdf", "TL;DR": "An analysis of the effects of compositionality and locality on representation learning for zero-shot learning.", "abstract": "In this work we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). \nIn order to well-isolate the importance of these properties in learned representations, we impose the additional constraint that, differently from most recent work in ZSL, no pre-training on different datasets (e.g. ImageNet) is performed.\nThe results of our experiment show how locality, in terms of small parts of the input, and compositionality, i.e. how well can the learned representations be expressed as a function of a smaller vocabulary, are both deeply related to generalization and motivate the focus on more local-aware models in future research directions for representation learning.", "keywords": ["Zero-shot learning", "Compositionality", "Locality", "Deep Learning"], "paperhash": "sylvain|locality_and_compositionality_in_zeroshot_learning", "_bibtex": "@inproceedings{\nsylvain2020locality,\ntitle={Locality and Compositionality in Zero-Shot Learning},\nauthor={Tristan Sylvain and Linda Petrini and Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye_V0NKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d55a6ecb5843fdb65af828aff26c08d11c2d2ce0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hye_V0NKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1080/Authors", "ICLR.cc/2020/Conference/Paper1080/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1080/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1080/Reviewers", "ICLR.cc/2020/Conference/Paper1080/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1080/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1080/Authors|ICLR.cc/2020/Conference/Paper1080/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161570, "tmdate": 1576860556079, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1080/Authors", "ICLR.cc/2020/Conference/Paper1080/Reviewers", "ICLR.cc/2020/Conference/Paper1080/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1080/-/Official_Comment"}}}, {"id": "SygcZ5xhiS", "original": null, "number": 2, "cdate": 1573812738157, "ddate": null, "tcdate": 1573812738157, "tmdate": 1573813721072, "tddate": null, "forum": "Hye_V0NKwr", "replyto": "SJeARAZTYr", "invitation": "ICLR.cc/2020/Conference/Paper1080/-/Official_Comment", "content": {"title": "Author response", "comment": "We thank the reviewer for the positive comments. In what follows, we will attempt to address their remarks.\n\nYour concerns are completely fair, and I think the issue is that we didn't make it clear enough that the paper is an evaluation paper (and we can correct that). Rather, the methods we introduce are all meant to be different tools we felt would test / evaluate the effect of compositionality and locality. It is true that the paper would have benefited from trying more local / compositional methods, but we think that as establishing a framework for discussing generalization in ZSL, our work has high value, and we hope that future works explore these factors in more depth under ZFS.\n\nComments on section 5.3. The perspective we have taken in this work is to consider most CNN-based architectures to be compositional in one way or another (given that they arise from combining part representations e.g. feature maps). In this context, it is complicated to introduce a non-compositional model, as it would require a different architecture, hence the use of the TRE proxy and reasoning in terms of degrees of compositionality. Our second experiment in that section (fig. 6) is an attempt to contrast a more compositional model (averaging part representations) with an ensemble (averaging predictions).\n\nComments on \u201cignoring\u201d information. We agree that a good criteria for a local representation would be that it ignores information from e.g. other patches. This fits with our proposed definition of a local representation as one that contains information specific to a patch. The reasoning behind the local losses was that as a local feature can only at best be predictive of the class information or attribute information that is present at the given location, the loss would encourage the model to account for this local information. An example would be: a patch corresponding to the beak of a bird cannot predict the tail color, but having an AC loss will encourage it to take into account beak shape, and color. One way to evaluate whether a local representation ignores local information is to estimate the mutual information between different local features corresponding to the same image.  We are working on evaluating this across models and datasets.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1080/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1080/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tristan.sylvain@gmail.com", "lindapetrini@gmail.com", "devon.hjelm@microsoft.com"], "title": "Locality and Compositionality in Zero-Shot Learning", "authors": ["Tristan Sylvain", "Linda Petrini", "Devon Hjelm"], "pdf": "/pdf/ae21cd01627929186c1673ccc2f9b1c9cc1dfc2e.pdf", "TL;DR": "An analysis of the effects of compositionality and locality on representation learning for zero-shot learning.", "abstract": "In this work we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). \nIn order to well-isolate the importance of these properties in learned representations, we impose the additional constraint that, differently from most recent work in ZSL, no pre-training on different datasets (e.g. ImageNet) is performed.\nThe results of our experiment show how locality, in terms of small parts of the input, and compositionality, i.e. how well can the learned representations be expressed as a function of a smaller vocabulary, are both deeply related to generalization and motivate the focus on more local-aware models in future research directions for representation learning.", "keywords": ["Zero-shot learning", "Compositionality", "Locality", "Deep Learning"], "paperhash": "sylvain|locality_and_compositionality_in_zeroshot_learning", "_bibtex": "@inproceedings{\nsylvain2020locality,\ntitle={Locality and Compositionality in Zero-Shot Learning},\nauthor={Tristan Sylvain and Linda Petrini and Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye_V0NKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d55a6ecb5843fdb65af828aff26c08d11c2d2ce0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hye_V0NKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1080/Authors", "ICLR.cc/2020/Conference/Paper1080/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1080/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1080/Reviewers", "ICLR.cc/2020/Conference/Paper1080/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1080/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1080/Authors|ICLR.cc/2020/Conference/Paper1080/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161570, "tmdate": 1576860556079, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1080/Authors", "ICLR.cc/2020/Conference/Paper1080/Reviewers", "ICLR.cc/2020/Conference/Paper1080/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1080/-/Official_Comment"}}}, {"id": "HJgLUdTnYB", "original": null, "number": 1, "cdate": 1571768397571, "ddate": null, "tcdate": 1571768397571, "tmdate": 1572972515238, "tddate": null, "forum": "Hye_V0NKwr", "replyto": "Hye_V0NKwr", "invitation": "ICLR.cc/2020/Conference/Paper1080/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes an evaluation framework for Zero-Shot Learning (ZSL) methods called zero-shot learning from scratch (ZFS) where the model is not allowed to be pretrained on other datasets such as ImageNet. The main motivation of this approach is that it is difficult to understand what is useful for generalization in ZSL since most state-of-the-art methods use features pretrained on large datasets such as ImageNet.\nMost state-of-the-art approaches exploit models pretrained on ImageNet. Instead, the paper proposes to randomly initialize model parameters to have a better understanding of what's happening in ZSL.\nZFS adds one constraint: model parameters should not contain information about data outside that from the training split of the target dataset.\nTwo main criteria are studied to study neural networks:\n- compositionality (ability to be expressed as a combination of simpler parts)\n- locality (ability to encode only information specific to locations of interest)\nTo provide a better understanding of their claims, the authors use MTurk annotations to construct boolean map for each local part labelled in the CUB dataset.\n\n\nAlthough the paper is experimental, I vote for borderline accept for the following reasons:\n- The paper is well-written and the contributions are clear.\n- The evaluation metrics to evaluate how models generalize for both criteria are well motivated theoretically (e.g. Tree Reconstruction Error is used to evaluate compositionality), and different types of encoders (e.g. variations of DCGANS) are studied.\n- The analysis of models pretrained in different contexts (e.g. supervised classification, reconstruction etc...), with only global or local information, and their impact on generalization is clear (see conclusion for a summary of results). Some conclusions are very intuitive but useful to know (e.g. VAEs and reconstruction models are not well suited to learn representations that generalize in ZFS).\n\n\nThe weaknesses I found in the paper are the following:\n- The paper claims that the models used are smaller than the \u201cstandard\u201d backbones common in state-of-the-art Imagenet-pretrained ZSL methods. However, few-shot learning method (e.g. ProtoNet) do not retrain the whole model: Protonet freezes most layers and fine-tunes only the last layers to avoid overfitting.\n- The paper introduces \"Class-Matching Deep Informax (CMDIM)\" which draws positive samples from other images from the same class to extract information that discriminative between categories instead of individual samples. However, I did not understand its exact formulation and how it is exactly different from other DIM approaches.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1080/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1080/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tristan.sylvain@gmail.com", "lindapetrini@gmail.com", "devon.hjelm@microsoft.com"], "title": "Locality and Compositionality in Zero-Shot Learning", "authors": ["Tristan Sylvain", "Linda Petrini", "Devon Hjelm"], "pdf": "/pdf/ae21cd01627929186c1673ccc2f9b1c9cc1dfc2e.pdf", "TL;DR": "An analysis of the effects of compositionality and locality on representation learning for zero-shot learning.", "abstract": "In this work we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). \nIn order to well-isolate the importance of these properties in learned representations, we impose the additional constraint that, differently from most recent work in ZSL, no pre-training on different datasets (e.g. ImageNet) is performed.\nThe results of our experiment show how locality, in terms of small parts of the input, and compositionality, i.e. how well can the learned representations be expressed as a function of a smaller vocabulary, are both deeply related to generalization and motivate the focus on more local-aware models in future research directions for representation learning.", "keywords": ["Zero-shot learning", "Compositionality", "Locality", "Deep Learning"], "paperhash": "sylvain|locality_and_compositionality_in_zeroshot_learning", "_bibtex": "@inproceedings{\nsylvain2020locality,\ntitle={Locality and Compositionality in Zero-Shot Learning},\nauthor={Tristan Sylvain and Linda Petrini and Devon Hjelm},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye_V0NKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d55a6ecb5843fdb65af828aff26c08d11c2d2ce0.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hye_V0NKwr", "replyto": "Hye_V0NKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1080/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1080/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575767944395, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1080/Reviewers"], "noninvitees": [], "tcdate": 1570237742652, "tmdate": 1575767944411, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1080/-/Official_Review"}}}], "count": 8}