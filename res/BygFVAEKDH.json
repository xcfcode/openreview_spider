{"notes": [{"id": "BygFVAEKDH", "original": "Byxf3HLuvB", "number": 1083, "cdate": 1569439281126, "ddate": null, "tcdate": 1569439281126, "tmdate": 1583912033757, "tddate": null, "forum": "BygFVAEKDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["chuntinz@andrew.cmu.edu", "jgu@fb.com", "gneubig@cs.cmu.edu"], "title": "Understanding Knowledge Distillation in Non-autoregressive Machine Translation", "authors": ["Chunting Zhou", "Jiatao Gu", "Graham Neubig"], "pdf": "/pdf/a057d93f5526ce4ceb6751adc88766291d440cd7.pdf", "TL;DR": "We systematically examine why knowledge distillation is crucial to the training of non-autoregressive translation (NAT) models, and propose methods to further improve the distilled data to best match the capacity of an NAT model.", "abstract": "Non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models. Existing NAT models usually rely on the technique of knowledge distillation, which creates the training data from a pretrained autoregressive model for better performance. Knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear. In this paper, we first design systematic experiments to investigate why knowledge distillation is crucial to NAT training. We find that knowledge distillation can reduce the complexity of data sets and help NAT to model the variations in the output data. Furthermore, a strong correlation is observed between the capacity of an NAT model and the optimal complexity of the distilled data for the best translation quality. Based on these findings, we further propose several approaches that can alter the complexity of data sets to improve the performance of NAT models. We achieve the state-of-the-art performance for the NAT-based models, and close the gap with the autoregressive baseline on WMT14 En-De benchmark.", "keywords": ["knowledge distillation", "non-autoregressive neural machine translation"], "paperhash": "zhou|understanding_knowledge_distillation_in_nonautoregressive_machine_translation", "_bibtex": "@inproceedings{\nZhou2020Understanding,\ntitle={Understanding Knowledge Distillation in Non-autoregressive Machine Translation},\nauthor={Chunting Zhou and Jiatao Gu and Graham Neubig},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BygFVAEKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e0654412a5ccd8c30e0e75ccdf0f3464a42d638c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "6bhtf8p77", "original": null, "number": 5, "cdate": 1580686672397, "ddate": null, "tcdate": 1580686672397, "tmdate": 1580686672397, "tddate": null, "forum": "BygFVAEKDH", "replyto": "m4Y6N_YFSB", "invitation": "ICLR.cc/2020/Conference/Paper1083/-/Public_Comment", "content": {"title": "Thanks for your patience and answers", "comment": "My problem is addressed. Thanks for your time."}, "signatures": ["~Xuanli_He2"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Xuanli_He2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chuntinz@andrew.cmu.edu", "jgu@fb.com", "gneubig@cs.cmu.edu"], "title": "Understanding Knowledge Distillation in Non-autoregressive Machine Translation", "authors": ["Chunting Zhou", "Jiatao Gu", "Graham Neubig"], "pdf": "/pdf/a057d93f5526ce4ceb6751adc88766291d440cd7.pdf", "TL;DR": "We systematically examine why knowledge distillation is crucial to the training of non-autoregressive translation (NAT) models, and propose methods to further improve the distilled data to best match the capacity of an NAT model.", "abstract": "Non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models. Existing NAT models usually rely on the technique of knowledge distillation, which creates the training data from a pretrained autoregressive model for better performance. Knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear. In this paper, we first design systematic experiments to investigate why knowledge distillation is crucial to NAT training. We find that knowledge distillation can reduce the complexity of data sets and help NAT to model the variations in the output data. Furthermore, a strong correlation is observed between the capacity of an NAT model and the optimal complexity of the distilled data for the best translation quality. Based on these findings, we further propose several approaches that can alter the complexity of data sets to improve the performance of NAT models. We achieve the state-of-the-art performance for the NAT-based models, and close the gap with the autoregressive baseline on WMT14 En-De benchmark.", "keywords": ["knowledge distillation", "non-autoregressive neural machine translation"], "paperhash": "zhou|understanding_knowledge_distillation_in_nonautoregressive_machine_translation", "_bibtex": "@inproceedings{\nZhou2020Understanding,\ntitle={Understanding Knowledge Distillation in Non-autoregressive Machine Translation},\nauthor={Chunting Zhou and Jiatao Gu and Graham Neubig},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BygFVAEKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e0654412a5ccd8c30e0e75ccdf0f3464a42d638c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygFVAEKDH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504199884, "tmdate": 1576860567815, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1083/Authors", "ICLR.cc/2020/Conference/Paper1083/Reviewers", "ICLR.cc/2020/Conference/Paper1083/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1083/-/Public_Comment"}}}, {"id": "m4Y6N_YFSB", "original": null, "number": 11, "cdate": 1580525076668, "ddate": null, "tcdate": 1580525076668, "tmdate": 1580525076668, "tddate": null, "forum": "BygFVAEKDH", "replyto": "zWXC_hqwHT", "invitation": "ICLR.cc/2020/Conference/Paper1083/-/Official_Comment", "content": {"title": "We use fast_align", "comment": "Yes, you are right, we use fast align to compute the alignments, it basically outputs the target tokens that each source token is aligned, and then we can just count and normalize to compute the alignment distribution."}, "signatures": ["ICLR.cc/2020/Conference/Paper1083/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1083/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chuntinz@andrew.cmu.edu", "jgu@fb.com", "gneubig@cs.cmu.edu"], "title": "Understanding Knowledge Distillation in Non-autoregressive Machine Translation", "authors": ["Chunting Zhou", "Jiatao Gu", "Graham Neubig"], "pdf": "/pdf/a057d93f5526ce4ceb6751adc88766291d440cd7.pdf", "TL;DR": "We systematically examine why knowledge distillation is crucial to the training of non-autoregressive translation (NAT) models, and propose methods to further improve the distilled data to best match the capacity of an NAT model.", "abstract": "Non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models. Existing NAT models usually rely on the technique of knowledge distillation, which creates the training data from a pretrained autoregressive model for better performance. Knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear. In this paper, we first design systematic experiments to investigate why knowledge distillation is crucial to NAT training. We find that knowledge distillation can reduce the complexity of data sets and help NAT to model the variations in the output data. Furthermore, a strong correlation is observed between the capacity of an NAT model and the optimal complexity of the distilled data for the best translation quality. Based on these findings, we further propose several approaches that can alter the complexity of data sets to improve the performance of NAT models. We achieve the state-of-the-art performance for the NAT-based models, and close the gap with the autoregressive baseline on WMT14 En-De benchmark.", "keywords": ["knowledge distillation", "non-autoregressive neural machine translation"], "paperhash": "zhou|understanding_knowledge_distillation_in_nonautoregressive_machine_translation", "_bibtex": "@inproceedings{\nZhou2020Understanding,\ntitle={Understanding Knowledge Distillation in Non-autoregressive Machine Translation},\nauthor={Chunting Zhou and Jiatao Gu and Graham Neubig},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BygFVAEKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e0654412a5ccd8c30e0e75ccdf0f3464a42d638c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygFVAEKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1083/Authors", "ICLR.cc/2020/Conference/Paper1083/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1083/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1083/Reviewers", "ICLR.cc/2020/Conference/Paper1083/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1083/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1083/Authors|ICLR.cc/2020/Conference/Paper1083/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161527, "tmdate": 1576860534171, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1083/Authors", "ICLR.cc/2020/Conference/Paper1083/Reviewers", "ICLR.cc/2020/Conference/Paper1083/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1083/-/Official_Comment"}}}, {"id": "zWXC_hqwHT", "original": null, "number": 4, "cdate": 1580449921408, "ddate": null, "tcdate": 1580449921408, "tmdate": 1580449921408, "tddate": null, "forum": "BygFVAEKDH", "replyto": "KVW0YmPtO8", "invitation": "ICLR.cc/2020/Conference/Paper1083/-/Public_Comment", "content": {"title": "Still unclear for the alignment distribution", "comment": "Sorry, let me elaborate on my words. In equation (4), $y$s and $x$s are target words and source words respectively. My question here is how you align the source word $x$ to the target word $y$, did you also use fast_align or any other toolkit? or according to the attention scores? After you finished the alignments, how could you compute the alignment distribution $p(y|x)$"}, "signatures": ["~Xuanli_He2"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Xuanli_He2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chuntinz@andrew.cmu.edu", "jgu@fb.com", "gneubig@cs.cmu.edu"], "title": "Understanding Knowledge Distillation in Non-autoregressive Machine Translation", "authors": ["Chunting Zhou", "Jiatao Gu", "Graham Neubig"], "pdf": "/pdf/a057d93f5526ce4ceb6751adc88766291d440cd7.pdf", "TL;DR": "We systematically examine why knowledge distillation is crucial to the training of non-autoregressive translation (NAT) models, and propose methods to further improve the distilled data to best match the capacity of an NAT model.", "abstract": "Non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models. Existing NAT models usually rely on the technique of knowledge distillation, which creates the training data from a pretrained autoregressive model for better performance. Knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear. In this paper, we first design systematic experiments to investigate why knowledge distillation is crucial to NAT training. We find that knowledge distillation can reduce the complexity of data sets and help NAT to model the variations in the output data. Furthermore, a strong correlation is observed between the capacity of an NAT model and the optimal complexity of the distilled data for the best translation quality. Based on these findings, we further propose several approaches that can alter the complexity of data sets to improve the performance of NAT models. We achieve the state-of-the-art performance for the NAT-based models, and close the gap with the autoregressive baseline on WMT14 En-De benchmark.", "keywords": ["knowledge distillation", "non-autoregressive neural machine translation"], "paperhash": "zhou|understanding_knowledge_distillation_in_nonautoregressive_machine_translation", "_bibtex": "@inproceedings{\nZhou2020Understanding,\ntitle={Understanding Knowledge Distillation in Non-autoregressive Machine Translation},\nauthor={Chunting Zhou and Jiatao Gu and Graham Neubig},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BygFVAEKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e0654412a5ccd8c30e0e75ccdf0f3464a42d638c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygFVAEKDH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504199884, "tmdate": 1576860567815, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1083/Authors", "ICLR.cc/2020/Conference/Paper1083/Reviewers", "ICLR.cc/2020/Conference/Paper1083/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1083/-/Public_Comment"}}}, {"id": "KVW0YmPtO8", "original": null, "number": 10, "cdate": 1580355589493, "ddate": null, "tcdate": 1580355589493, "tmdate": 1580355589493, "tddate": null, "forum": "BygFVAEKDH", "replyto": "2VBfMeN7ni", "invitation": "ICLR.cc/2020/Conference/Paper1083/-/Official_Comment", "content": {"title": "Thanks for the comment!", "comment": "Thanks for pointing out the reference!\n\nWhen we compute the alignment distribution for $p(y|x)$, we set the probabilities of target words y to be a very small value if it doesn't exist in the alignment distribution given a x."}, "signatures": ["ICLR.cc/2020/Conference/Paper1083/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1083/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chuntinz@andrew.cmu.edu", "jgu@fb.com", "gneubig@cs.cmu.edu"], "title": "Understanding Knowledge Distillation in Non-autoregressive Machine Translation", "authors": ["Chunting Zhou", "Jiatao Gu", "Graham Neubig"], "pdf": "/pdf/a057d93f5526ce4ceb6751adc88766291d440cd7.pdf", "TL;DR": "We systematically examine why knowledge distillation is crucial to the training of non-autoregressive translation (NAT) models, and propose methods to further improve the distilled data to best match the capacity of an NAT model.", "abstract": "Non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models. Existing NAT models usually rely on the technique of knowledge distillation, which creates the training data from a pretrained autoregressive model for better performance. Knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear. In this paper, we first design systematic experiments to investigate why knowledge distillation is crucial to NAT training. We find that knowledge distillation can reduce the complexity of data sets and help NAT to model the variations in the output data. Furthermore, a strong correlation is observed between the capacity of an NAT model and the optimal complexity of the distilled data for the best translation quality. Based on these findings, we further propose several approaches that can alter the complexity of data sets to improve the performance of NAT models. We achieve the state-of-the-art performance for the NAT-based models, and close the gap with the autoregressive baseline on WMT14 En-De benchmark.", "keywords": ["knowledge distillation", "non-autoregressive neural machine translation"], "paperhash": "zhou|understanding_knowledge_distillation_in_nonautoregressive_machine_translation", "_bibtex": "@inproceedings{\nZhou2020Understanding,\ntitle={Understanding Knowledge Distillation in Non-autoregressive Machine Translation},\nauthor={Chunting Zhou and Jiatao Gu and Graham Neubig},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BygFVAEKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e0654412a5ccd8c30e0e75ccdf0f3464a42d638c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygFVAEKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1083/Authors", "ICLR.cc/2020/Conference/Paper1083/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1083/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1083/Reviewers", "ICLR.cc/2020/Conference/Paper1083/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1083/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1083/Authors|ICLR.cc/2020/Conference/Paper1083/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161527, "tmdate": 1576860534171, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1083/Authors", "ICLR.cc/2020/Conference/Paper1083/Reviewers", "ICLR.cc/2020/Conference/Paper1083/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1083/-/Official_Comment"}}}, {"id": "2VBfMeN7ni", "original": null, "number": 3, "cdate": 1580353823326, "ddate": null, "tcdate": 1580353823326, "tmdate": 1580353823326, "tddate": null, "forum": "BygFVAEKDH", "replyto": "BygFVAEKDH", "invitation": "ICLR.cc/2020/Conference/Paper1083/-/Public_Comment", "content": {"title": "How to measure the faithfulness", "comment": "It's a very insightful study. I have one question related to the measure of faithfulness. From the paper, I couldn't figure out how you align x and y in your $P_r(y|x)$ and $P_d(y|x)$ in the equ (4).\n\nAnd you might miss a related work He et al. 2018 [1] which suggested the diverse translation with MOE earlier than Shen et al. 2019.\n\n[1] Sequence to Sequence Mixture Model for Diverse Machine Translation, He et al. 2018"}, "signatures": ["~Xuanli_He2"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Xuanli_He2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chuntinz@andrew.cmu.edu", "jgu@fb.com", "gneubig@cs.cmu.edu"], "title": "Understanding Knowledge Distillation in Non-autoregressive Machine Translation", "authors": ["Chunting Zhou", "Jiatao Gu", "Graham Neubig"], "pdf": "/pdf/a057d93f5526ce4ceb6751adc88766291d440cd7.pdf", "TL;DR": "We systematically examine why knowledge distillation is crucial to the training of non-autoregressive translation (NAT) models, and propose methods to further improve the distilled data to best match the capacity of an NAT model.", "abstract": "Non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models. Existing NAT models usually rely on the technique of knowledge distillation, which creates the training data from a pretrained autoregressive model for better performance. Knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear. In this paper, we first design systematic experiments to investigate why knowledge distillation is crucial to NAT training. We find that knowledge distillation can reduce the complexity of data sets and help NAT to model the variations in the output data. Furthermore, a strong correlation is observed between the capacity of an NAT model and the optimal complexity of the distilled data for the best translation quality. Based on these findings, we further propose several approaches that can alter the complexity of data sets to improve the performance of NAT models. We achieve the state-of-the-art performance for the NAT-based models, and close the gap with the autoregressive baseline on WMT14 En-De benchmark.", "keywords": ["knowledge distillation", "non-autoregressive neural machine translation"], "paperhash": "zhou|understanding_knowledge_distillation_in_nonautoregressive_machine_translation", "_bibtex": "@inproceedings{\nZhou2020Understanding,\ntitle={Understanding Knowledge Distillation in Non-autoregressive Machine Translation},\nauthor={Chunting Zhou and Jiatao Gu and Graham Neubig},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BygFVAEKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e0654412a5ccd8c30e0e75ccdf0f3464a42d638c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygFVAEKDH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504199884, "tmdate": 1576860567815, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1083/Authors", "ICLR.cc/2020/Conference/Paper1083/Reviewers", "ICLR.cc/2020/Conference/Paper1083/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1083/-/Public_Comment"}}}, {"id": "d1m4vN94go", "original": null, "number": 1, "cdate": 1576798714101, "ddate": null, "tcdate": 1576798714101, "tmdate": 1576800922373, "tddate": null, "forum": "BygFVAEKDH", "replyto": "BygFVAEKDH", "invitation": "ICLR.cc/2020/Conference/Paper1083/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "Main content:\n\nBlind review #3 summarized it well, as follows:\n\nThis paper studies knowledge distillation in the context of non-autoregressive translation. In particular, it is well known that in order to make NAT competitive with AT, one needs to train the NAT system on a distilled dataset from the teacher model. Using initial experiments on EN=>ES/FR/DE, the authors argue that this necessity arises from the overly-multimodal nature of the output distribution, and that the AT teacher model produces a less multimodal distribution that is easier to model with NAT. \n\nBased on this, the authors propose two quantities that estimate the complexity (conditional entropy) and faithfulness (cross entropy vs real data), and derive approximations to these based on independence assumptions and an alignment model.  The translations from the teacher output are indeed found to be less complex, thereby facilitating easier training for the NAT student model.\n\n--\n\nDiscussion:\n\nQuestions were mostly about how robust the results were on other language pairs and random starting points. Authors addressed questions reasonably.\n\nOne low review came from a reviewer who admitted not knowing the field, and I agree with the other two reviewers.\n\n--\n\nRecommendation and justification:\n\nI think papers that offer empirically support for scientific insight (giving an \"a-ha!\" reaction), rather than massive engineering efforts to beat the state of the art, are very worthwhile in scientific conferences. This paper meets that criteria for acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chuntinz@andrew.cmu.edu", "jgu@fb.com", "gneubig@cs.cmu.edu"], "title": "Understanding Knowledge Distillation in Non-autoregressive Machine Translation", "authors": ["Chunting Zhou", "Jiatao Gu", "Graham Neubig"], "pdf": "/pdf/a057d93f5526ce4ceb6751adc88766291d440cd7.pdf", "TL;DR": "We systematically examine why knowledge distillation is crucial to the training of non-autoregressive translation (NAT) models, and propose methods to further improve the distilled data to best match the capacity of an NAT model.", "abstract": "Non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models. Existing NAT models usually rely on the technique of knowledge distillation, which creates the training data from a pretrained autoregressive model for better performance. Knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear. In this paper, we first design systematic experiments to investigate why knowledge distillation is crucial to NAT training. We find that knowledge distillation can reduce the complexity of data sets and help NAT to model the variations in the output data. Furthermore, a strong correlation is observed between the capacity of an NAT model and the optimal complexity of the distilled data for the best translation quality. Based on these findings, we further propose several approaches that can alter the complexity of data sets to improve the performance of NAT models. We achieve the state-of-the-art performance for the NAT-based models, and close the gap with the autoregressive baseline on WMT14 En-De benchmark.", "keywords": ["knowledge distillation", "non-autoregressive neural machine translation"], "paperhash": "zhou|understanding_knowledge_distillation_in_nonautoregressive_machine_translation", "_bibtex": "@inproceedings{\nZhou2020Understanding,\ntitle={Understanding Knowledge Distillation in Non-autoregressive Machine Translation},\nauthor={Chunting Zhou and Jiatao Gu and Graham Neubig},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BygFVAEKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e0654412a5ccd8c30e0e75ccdf0f3464a42d638c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BygFVAEKDH", "replyto": "BygFVAEKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795707552, "tmdate": 1576800255773, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1083/-/Decision"}}}, {"id": "B1gjfhYBOr", "original": null, "number": 1, "cdate": 1570245650653, "ddate": null, "tcdate": 1570245650653, "tmdate": 1573938959512, "tddate": null, "forum": "BygFVAEKDH", "replyto": "BygFVAEKDH", "invitation": "ICLR.cc/2020/Conference/Paper1083/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "[EDIT: Thanks for the response! I am updating my score to 8 given the rebuttal to my and other reviewers' questions]\n\n--------------------------------------------\nSummary:\n--------------------------------------------\nThis paper studies knowledge distillation in the context of non-autoregressive translation. In particular, it is well known that in order to make NAT competitive with AT, one needs to train the NAT system on a distilled dataset from the teacher model. Using initial experiments on EN=>ES/FR/DE, the authors argue that this necessity arises from the overly-multimodal nature of the output distribution, and that the AT teacher model produces a less multimodal distribution that is easier to model with NAT. \n\nBased on this, the authors propose two quantities that estimate the complexity (conditional entropy) and faithfulness (cross entropy vs real data), and derive approximations to these based on independence assumptions and an alignment model.  The translations from the teacher output are indeed found to be less complex, thereby facilitating easier training for the NAT student model.\n\n--------------------------------------------\nStrengths:\n--------------------------------------------\n- Careful initial experiments to motivate the study.\n\n- The metrics under consideration (entropy/faithfulness) are carefully derived, and while some of the assumptions may be overly simplifying (e.g. conditional independence assumptions), they are reasonable enough such that the directionality is likely to be correct.\n\n- Thorough comparison of various NAT models. This will benefit the community greatly since to my knowledge, there has been no empirical head-to-head comparison of the different NAT methods that exist today.\n\n- Extensive experiments across various settings, e.g. varying the teacher model size, varying the decoding strategies, investigating BAN/MoE/Sequence-level interpolation, etc. \n\n--------------------------------------------\nWeaknesses:\n--------------------------------------------\n- It would have been interesting to consider a synthetic data setting such that one has access to the true underlying data distribution, such that approximations are not necessary.\n\n- While translation is an important application of non-autoregressive generation, it would have also been interesting to study this in other seq2seq regimes such as summarization where the conditional entropy would presumably be even higher. (However this would complicate things like calculation of alignment probabilities, etc.)\n\n--------------------------------------------\nOther Questions/Comments:\n--------------------------------------------\n- p(y_t | l_i) coming from a token frequency distribution seems a bit too simple. Do the plots change if you model p(y | l_i) with a full language model that conditions on l_i?\n\n- It's interesting to note that in Figures 1b,1c,1d, there is much more overlap between the romance languages es/fr, which are more closely related to each other. \n\n- I am not sure I agree with \"C(d) reflects the level of multi-modality of a parallel corpus\". One can certainly imagine a distribution which is unimodal but has high entropy...\n\n- It seems like that we want a teacher model with low complexity and high faithfulness. Have the authors tried training a teacher model to directly target this? The usual MLE objective obviously targets faithfulness, but perhaps one could use RL techniques to optimize for low complexity as well.\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1083/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1083/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chuntinz@andrew.cmu.edu", "jgu@fb.com", "gneubig@cs.cmu.edu"], "title": "Understanding Knowledge Distillation in Non-autoregressive Machine Translation", "authors": ["Chunting Zhou", "Jiatao Gu", "Graham Neubig"], "pdf": "/pdf/a057d93f5526ce4ceb6751adc88766291d440cd7.pdf", "TL;DR": "We systematically examine why knowledge distillation is crucial to the training of non-autoregressive translation (NAT) models, and propose methods to further improve the distilled data to best match the capacity of an NAT model.", "abstract": "Non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models. Existing NAT models usually rely on the technique of knowledge distillation, which creates the training data from a pretrained autoregressive model for better performance. Knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear. In this paper, we first design systematic experiments to investigate why knowledge distillation is crucial to NAT training. We find that knowledge distillation can reduce the complexity of data sets and help NAT to model the variations in the output data. Furthermore, a strong correlation is observed between the capacity of an NAT model and the optimal complexity of the distilled data for the best translation quality. Based on these findings, we further propose several approaches that can alter the complexity of data sets to improve the performance of NAT models. We achieve the state-of-the-art performance for the NAT-based models, and close the gap with the autoregressive baseline on WMT14 En-De benchmark.", "keywords": ["knowledge distillation", "non-autoregressive neural machine translation"], "paperhash": "zhou|understanding_knowledge_distillation_in_nonautoregressive_machine_translation", "_bibtex": "@inproceedings{\nZhou2020Understanding,\ntitle={Understanding Knowledge Distillation in Non-autoregressive Machine Translation},\nauthor={Chunting Zhou and Jiatao Gu and Graham Neubig},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BygFVAEKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e0654412a5ccd8c30e0e75ccdf0f3464a42d638c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BygFVAEKDH", "replyto": "BygFVAEKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1083/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1083/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575880942381, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1083/Reviewers"], "noninvitees": [], "tcdate": 1570237742607, "tmdate": 1575880942394, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1083/-/Official_Review"}}}, {"id": "rJeOiuN2jr", "original": null, "number": 7, "cdate": 1573828767705, "ddate": null, "tcdate": 1573828767705, "tmdate": 1573828767705, "tddate": null, "forum": "BygFVAEKDH", "replyto": "BygmuCAKjB", "invitation": "ICLR.cc/2020/Conference/Paper1083/-/Official_Comment", "content": {"title": "Thank you for your comments! We update the paper accordingly.", "comment": "Thanks for your insightful reviews and we appreciate your valuable suggestions!\nWe have revised the paper and reflected most of your suggestions. \n\nWe address your concerns and questions as follows:\nQuestion [sampling top-k decoding]: Yes, we renormalize the top-k decoding probabilities and do sampling.\nQuestion [model selection according to validation loss]: We used the default setting of fairseq (the framework we used for implementing the model) - valid loss for validation. For FlowSeq, we used the open source implementation which used valid BLEU to select model. We have revised the paper to make it more clear.\n\nIn the latest revision, we also included figures of NAT model performance measured by other metrics (e.g. METEOR, TER, RIBES, ChrF, BEER), and we found that all the metrics are correlated with the original BLEU scores quite well showing the similar trend."}, "signatures": ["ICLR.cc/2020/Conference/Paper1083/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1083/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chuntinz@andrew.cmu.edu", "jgu@fb.com", "gneubig@cs.cmu.edu"], "title": "Understanding Knowledge Distillation in Non-autoregressive Machine Translation", "authors": ["Chunting Zhou", "Jiatao Gu", "Graham Neubig"], "pdf": "/pdf/a057d93f5526ce4ceb6751adc88766291d440cd7.pdf", "TL;DR": "We systematically examine why knowledge distillation is crucial to the training of non-autoregressive translation (NAT) models, and propose methods to further improve the distilled data to best match the capacity of an NAT model.", "abstract": "Non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models. Existing NAT models usually rely on the technique of knowledge distillation, which creates the training data from a pretrained autoregressive model for better performance. Knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear. In this paper, we first design systematic experiments to investigate why knowledge distillation is crucial to NAT training. We find that knowledge distillation can reduce the complexity of data sets and help NAT to model the variations in the output data. Furthermore, a strong correlation is observed between the capacity of an NAT model and the optimal complexity of the distilled data for the best translation quality. Based on these findings, we further propose several approaches that can alter the complexity of data sets to improve the performance of NAT models. We achieve the state-of-the-art performance for the NAT-based models, and close the gap with the autoregressive baseline on WMT14 En-De benchmark.", "keywords": ["knowledge distillation", "non-autoregressive neural machine translation"], "paperhash": "zhou|understanding_knowledge_distillation_in_nonautoregressive_machine_translation", "_bibtex": "@inproceedings{\nZhou2020Understanding,\ntitle={Understanding Knowledge Distillation in Non-autoregressive Machine Translation},\nauthor={Chunting Zhou and Jiatao Gu and Graham Neubig},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BygFVAEKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e0654412a5ccd8c30e0e75ccdf0f3464a42d638c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygFVAEKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1083/Authors", "ICLR.cc/2020/Conference/Paper1083/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1083/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1083/Reviewers", "ICLR.cc/2020/Conference/Paper1083/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1083/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1083/Authors|ICLR.cc/2020/Conference/Paper1083/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161527, "tmdate": 1576860534171, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1083/Authors", "ICLR.cc/2020/Conference/Paper1083/Reviewers", "ICLR.cc/2020/Conference/Paper1083/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1083/-/Official_Comment"}}}, {"id": "rJlBYihKiH", "original": null, "number": 6, "cdate": 1573665660798, "ddate": null, "tcdate": 1573665660798, "tmdate": 1573682408003, "tddate": null, "forum": "BygFVAEKDH", "replyto": "B1gjfhYBOr", "invitation": "ICLR.cc/2020/Conference/Paper1083/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "Thanks for your insightful comments! We address your concerns and questions as follows:\n\n- Weakness 1 [Synthetic data with known data distribution]: \nWe have conducted experiments with synthetic data generated from a hidden markov model (HMM) and due to space limitations, we didn\u2019t include it in the main paper. \n - In our revised version, we have added this case study in the Appendix C.\n - Tl; dr: we use constructed HMM as our data generator and train a bi-LSTM sequence labeling model (independent predictions). We find that the best distilled data from the HMM model correlates with the evaluation metric of sequence labeling (derived from the Bayesian decision theory), i.e. the best distilled data for sequence-level accuracy is generated by finding the most probable sequence out of the HMM model: $argmax_{y \\in \\mathcal{Y}} P(y|x)$ and the best distilled data for token-level accuracy is generated by finding the most probable label at each time step: $argmax_{y \\in \\mathcal{Y}} \\prod_{t=1}^{T} P(y_t|x)$.\n\n- Weakness 2 [Other Seq2Seq tasks]: \nTo make the experiments throughout the paper more consistent, we focus on the machine translation task. However, we have actually conducted experiments on other sequence-to-sequence tasks as well including summarization and automatic post-editing and find that knowledge distillation is consistently a key ingredient to improve the performance of non-autoregressive generation. But we ran these as preliminary experiments, we don\u2019t have numbers now and will consider adding them to our final version. \n\n- Question [Do the plots change if you model p(y | l_i) with a full language model that conditions on l_i?]:\nIn our initial experiments, we also tried to model the sentence probability instead of the average of tokens in the figure. However, since the sentence probability is basically a chain product of token level probabilities, it is very sensitive to small values for the less frequent words resulting in close to 0 values.\n\n- Question [Have the authors tried training a teacher model to directly target at the complexity and faithfulness?]:\nWe haven\u2019t tried this yet, but will try to resolve it in our future work. One of our major concerns is that the complexity of a data set is defined on all data samples, how to directly optimize the complexity over a data set for the teacher model is not easy at first glance. Batch-level learning and RL might be a reasonable solution.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1083/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1083/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chuntinz@andrew.cmu.edu", "jgu@fb.com", "gneubig@cs.cmu.edu"], "title": "Understanding Knowledge Distillation in Non-autoregressive Machine Translation", "authors": ["Chunting Zhou", "Jiatao Gu", "Graham Neubig"], "pdf": "/pdf/a057d93f5526ce4ceb6751adc88766291d440cd7.pdf", "TL;DR": "We systematically examine why knowledge distillation is crucial to the training of non-autoregressive translation (NAT) models, and propose methods to further improve the distilled data to best match the capacity of an NAT model.", "abstract": "Non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models. Existing NAT models usually rely on the technique of knowledge distillation, which creates the training data from a pretrained autoregressive model for better performance. Knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear. In this paper, we first design systematic experiments to investigate why knowledge distillation is crucial to NAT training. We find that knowledge distillation can reduce the complexity of data sets and help NAT to model the variations in the output data. Furthermore, a strong correlation is observed between the capacity of an NAT model and the optimal complexity of the distilled data for the best translation quality. Based on these findings, we further propose several approaches that can alter the complexity of data sets to improve the performance of NAT models. We achieve the state-of-the-art performance for the NAT-based models, and close the gap with the autoregressive baseline on WMT14 En-De benchmark.", "keywords": ["knowledge distillation", "non-autoregressive neural machine translation"], "paperhash": "zhou|understanding_knowledge_distillation_in_nonautoregressive_machine_translation", "_bibtex": "@inproceedings{\nZhou2020Understanding,\ntitle={Understanding Knowledge Distillation in Non-autoregressive Machine Translation},\nauthor={Chunting Zhou and Jiatao Gu and Graham Neubig},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BygFVAEKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e0654412a5ccd8c30e0e75ccdf0f3464a42d638c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygFVAEKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1083/Authors", "ICLR.cc/2020/Conference/Paper1083/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1083/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1083/Reviewers", "ICLR.cc/2020/Conference/Paper1083/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1083/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1083/Authors|ICLR.cc/2020/Conference/Paper1083/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161527, "tmdate": 1576860534171, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1083/Authors", "ICLR.cc/2020/Conference/Paper1083/Reviewers", "ICLR.cc/2020/Conference/Paper1083/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1083/-/Official_Comment"}}}, {"id": "Skl0PthYoB", "original": null, "number": 5, "cdate": 1573665125536, "ddate": null, "tcdate": 1573665125536, "tmdate": 1573682374249, "tddate": null, "forum": "BygFVAEKDH", "replyto": "r1eumhbnYH", "invitation": "ICLR.cc/2020/Conference/Paper1083/-/Official_Comment", "content": {"title": "Understanding Knowledge Distillation is Important to Non-autoregressive Generation", "comment": "Thanks for your reviews!\nThough the results of the synthetic experiment can be expected given the properties of MLE, this experiment is only used as an introductory and explicit example to motivate our study of (1) how it is hard for a non-autoregressive student model (NAT) to capture modes by training with MLE; (2) how the distillation data set from an autoregressive teacher model (AT) removes the modes and can affect the performance of NAT. We are interested in understanding to what extent, and why, distillation helps in the learning of NAT models which has not been explained in prior works.\n\nAs the reviewer has indicated that they are not very familiar with the area, we would just like to re-iterate that knowledge distillation is both (1) very important to building better NAT models (used in all state-of-the-art models), but (2) at the same time almost entirely glazed over in previous work both theoretically and empirically.\n\nOur work contributes by both improving understanding, and using this understanding to improve final results, sometimes by a great margin:\n- We first systematically studied why and how (mode reduction) non-autoregressive generation models benefit from the knowledge distillation technique. We then propose two metrics to measure the complexity and faithfulness of a given training data set and show how they correlate with the performance of an NAT model.\n- We conduct extensive experiments over different AT teacher models and NAT student models to reveal the correlation between the capacity of an NAT model and the optimal (in terms of the complexity) distillation data from the AT model.\n- We further propose several techniques that can be used to adjust the complexity of the distilled data to match the student model\u2019s capacity. We have achieved state-of-the-art performance for NAT and almost closes the gap between NAT and AT models.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1083/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1083/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chuntinz@andrew.cmu.edu", "jgu@fb.com", "gneubig@cs.cmu.edu"], "title": "Understanding Knowledge Distillation in Non-autoregressive Machine Translation", "authors": ["Chunting Zhou", "Jiatao Gu", "Graham Neubig"], "pdf": "/pdf/a057d93f5526ce4ceb6751adc88766291d440cd7.pdf", "TL;DR": "We systematically examine why knowledge distillation is crucial to the training of non-autoregressive translation (NAT) models, and propose methods to further improve the distilled data to best match the capacity of an NAT model.", "abstract": "Non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models. Existing NAT models usually rely on the technique of knowledge distillation, which creates the training data from a pretrained autoregressive model for better performance. Knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear. In this paper, we first design systematic experiments to investigate why knowledge distillation is crucial to NAT training. We find that knowledge distillation can reduce the complexity of data sets and help NAT to model the variations in the output data. Furthermore, a strong correlation is observed between the capacity of an NAT model and the optimal complexity of the distilled data for the best translation quality. Based on these findings, we further propose several approaches that can alter the complexity of data sets to improve the performance of NAT models. We achieve the state-of-the-art performance for the NAT-based models, and close the gap with the autoregressive baseline on WMT14 En-De benchmark.", "keywords": ["knowledge distillation", "non-autoregressive neural machine translation"], "paperhash": "zhou|understanding_knowledge_distillation_in_nonautoregressive_machine_translation", "_bibtex": "@inproceedings{\nZhou2020Understanding,\ntitle={Understanding Knowledge Distillation in Non-autoregressive Machine Translation},\nauthor={Chunting Zhou and Jiatao Gu and Graham Neubig},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BygFVAEKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e0654412a5ccd8c30e0e75ccdf0f3464a42d638c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygFVAEKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1083/Authors", "ICLR.cc/2020/Conference/Paper1083/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1083/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1083/Reviewers", "ICLR.cc/2020/Conference/Paper1083/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1083/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1083/Authors|ICLR.cc/2020/Conference/Paper1083/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161527, "tmdate": 1576860534171, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1083/Authors", "ICLR.cc/2020/Conference/Paper1083/Reviewers", "ICLR.cc/2020/Conference/Paper1083/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1083/-/Official_Comment"}}}, {"id": "BygmuCAKjB", "original": null, "number": 3, "cdate": 1573674603477, "ddate": null, "tcdate": 1573674603477, "tmdate": 1573674603477, "tddate": null, "forum": "BygFVAEKDH", "replyto": "BygFVAEKDH", "invitation": "ICLR.cc/2020/Conference/Paper1083/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #5", "review": "In this paper, the authors investigate non-autoregressive translation (NAT). They specifically look into how using different auto-regressive translation (AT) models for knowledge distillation impacts the quality of NAT models. The paper is well organised and the experiments are sound and interesting, shedding light on an aspect of NAT models that's been rather dismissed as secondary until now: the impact of varying AT knowledge distillation.\n\nFirst, so as to better please those out there who are more \"state-of-the-art\" inclined, I suggest the authors to better emphasise their improvements. Results obtained by their analysis can lead to improvements as stated in the last paragraph of Section 4. This could be better stressed in the introduction and it would make the main take-away messages from the paper stronger.\n\nOn a more general note, I would like to know how robust these models are. I understand this is a problem NAT and machine translation papers have in general, but I would still like to suggest the authors train each model from scratch multiple times using different random seeds, and report mean and variance of their results (i.e. BLEU). Although I understand that training each AT model 4 times and each NAT model 4x4 times (multiple times for each AT model trained) is unfeasible, you could still report mean and variance separately for AT and NAT, and simply choose one out of the 4 trained AT models per architecture to perform distillation. I recommend training each model at least 3 times and reporting mean BLEU and variance.\n\nI would also recommend using other MT metrics (e.g. chrF, METEOR, BEER, etc.), since BLEU is a comparatively weak metric in terms of correlations with human judgements. For translations into German, look into characTER, chrF and BEER. For more information on the different metrics available, how they correlate with human judgements, and which better apply to different target languages / language pairs, please refer to the WMT evaluation/metrics shared tasks. [1]\n\nI have a few general comments and suggestions:\n- In Section 3.1, when introducing the NAT model (simplified version of Gu et al., (2019)), be more specific. I would like to see an actual description of what the modifications consist of in more detail: is there a sentence length prediction step? what happens when source length is different from source length? etc.\n- In Section 3.2, right after Equation (3), the authors refer to \"x and y\" in an inverted manner, please fix it.\n- In Section 4.3, you mention that two of the decoding methods used involve \"random sampling\", which I find misleading. You probably mean sampling according to the model distribution p(y_t | y_{<t}) for all t, which is not random. I suggest you simply remove the word \"random\" and mention that you use \"sampling\" and \"sampling within the top-10 candidates\". Also, when you sampling using the top-10 candidates, do you simply re-normalise the probability mass to include only the top-10 candidate tokens?\n- As a suggestion, Tables and Figures could be more self-contained. There is always a compromise between conciseness and clarity, added by the fact that the page limit makes things even harder. However, I would recommend including more information in Table/Figure captions, especially in Figure 3 and Tables 3 and 4. Try to at least mention the training/evaluation data (dev or test?). Ideally one should understand it from reading the abstract and carefully reading the caption.\n- In Appendix A, you mention that you select models according to validation loss. Is that really the case? If so, why? I am not sure whether validation loss (i.e. word cross-entropy on the validation set) should correlate so well with translation quality (or sentence-level metrics such as BLEU).\n\n[1] http://www.statmt.org/wmt19/metrics-task.html", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1083/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1083/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chuntinz@andrew.cmu.edu", "jgu@fb.com", "gneubig@cs.cmu.edu"], "title": "Understanding Knowledge Distillation in Non-autoregressive Machine Translation", "authors": ["Chunting Zhou", "Jiatao Gu", "Graham Neubig"], "pdf": "/pdf/a057d93f5526ce4ceb6751adc88766291d440cd7.pdf", "TL;DR": "We systematically examine why knowledge distillation is crucial to the training of non-autoregressive translation (NAT) models, and propose methods to further improve the distilled data to best match the capacity of an NAT model.", "abstract": "Non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models. Existing NAT models usually rely on the technique of knowledge distillation, which creates the training data from a pretrained autoregressive model for better performance. Knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear. In this paper, we first design systematic experiments to investigate why knowledge distillation is crucial to NAT training. We find that knowledge distillation can reduce the complexity of data sets and help NAT to model the variations in the output data. Furthermore, a strong correlation is observed between the capacity of an NAT model and the optimal complexity of the distilled data for the best translation quality. Based on these findings, we further propose several approaches that can alter the complexity of data sets to improve the performance of NAT models. We achieve the state-of-the-art performance for the NAT-based models, and close the gap with the autoregressive baseline on WMT14 En-De benchmark.", "keywords": ["knowledge distillation", "non-autoregressive neural machine translation"], "paperhash": "zhou|understanding_knowledge_distillation_in_nonautoregressive_machine_translation", "_bibtex": "@inproceedings{\nZhou2020Understanding,\ntitle={Understanding Knowledge Distillation in Non-autoregressive Machine Translation},\nauthor={Chunting Zhou and Jiatao Gu and Graham Neubig},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BygFVAEKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e0654412a5ccd8c30e0e75ccdf0f3464a42d638c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BygFVAEKDH", "replyto": "BygFVAEKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1083/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1083/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575880942381, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1083/Reviewers"], "noninvitees": [], "tcdate": 1570237742607, "tmdate": 1575880942394, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1083/-/Official_Review"}}}, {"id": "r1eumhbnYH", "original": null, "number": 2, "cdate": 1571720223963, "ddate": null, "tcdate": 1571720223963, "tmdate": 1572972514819, "tddate": null, "forum": "BygFVAEKDH", "replyto": "BygFVAEKDH", "invitation": "ICLR.cc/2020/Conference/Paper1083/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper analyses recent distillation techniques for non-autoregressive machine translation models (NAT). These models use a autoregressive teacher (AT), which typically perform better. However, AT models can not be parallelized that easily such as the NAT models. The distillation has the effect of removing modes from the dataset which helps the NAT models as they suffer from the averaging effect of maximum likelihood solutions. The authors analyze why such distillation is needed and what the effect of the complexity of the training set is and further propose 3 methods to adjust the complexity of the teacher to the complexity of the NAT model.\n\nThe paper is well written and easy to understand and the experiments are exhaustive and well done. I think the analysis using the synthetic dataset is nice, but I am not sure how surprising the results are. I think most of it has to be expected given the properties of maximum likelihood estimation. Hence, I think the contribution of the paper is a bit limited. I am, however, not an expert in the field to fully judge the contribution and will therefore put low confidence on my review. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1083/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1083/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chuntinz@andrew.cmu.edu", "jgu@fb.com", "gneubig@cs.cmu.edu"], "title": "Understanding Knowledge Distillation in Non-autoregressive Machine Translation", "authors": ["Chunting Zhou", "Jiatao Gu", "Graham Neubig"], "pdf": "/pdf/a057d93f5526ce4ceb6751adc88766291d440cd7.pdf", "TL;DR": "We systematically examine why knowledge distillation is crucial to the training of non-autoregressive translation (NAT) models, and propose methods to further improve the distilled data to best match the capacity of an NAT model.", "abstract": "Non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models. Existing NAT models usually rely on the technique of knowledge distillation, which creates the training data from a pretrained autoregressive model for better performance. Knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear. In this paper, we first design systematic experiments to investigate why knowledge distillation is crucial to NAT training. We find that knowledge distillation can reduce the complexity of data sets and help NAT to model the variations in the output data. Furthermore, a strong correlation is observed between the capacity of an NAT model and the optimal complexity of the distilled data for the best translation quality. Based on these findings, we further propose several approaches that can alter the complexity of data sets to improve the performance of NAT models. We achieve the state-of-the-art performance for the NAT-based models, and close the gap with the autoregressive baseline on WMT14 En-De benchmark.", "keywords": ["knowledge distillation", "non-autoregressive neural machine translation"], "paperhash": "zhou|understanding_knowledge_distillation_in_nonautoregressive_machine_translation", "_bibtex": "@inproceedings{\nZhou2020Understanding,\ntitle={Understanding Knowledge Distillation in Non-autoregressive Machine Translation},\nauthor={Chunting Zhou and Jiatao Gu and Graham Neubig},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BygFVAEKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e0654412a5ccd8c30e0e75ccdf0f3464a42d638c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BygFVAEKDH", "replyto": "BygFVAEKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1083/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1083/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575880942381, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1083/Reviewers"], "noninvitees": [], "tcdate": 1570237742607, "tmdate": 1575880942394, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1083/-/Official_Review"}}}, {"id": "rJeXUeVstr", "original": null, "number": 3, "cdate": 1571663947412, "ddate": null, "tcdate": 1571663947412, "tmdate": 1571663947412, "tddate": null, "forum": "BygFVAEKDH", "replyto": "rklYWJn9FS", "invitation": "ICLR.cc/2020/Conference/Paper1083/-/Official_Comment", "content": {"comment": "Thanks for your interest!\nThe fuzzy reordering score is defined as $FRD = 1 - \\frac{C-1}{M-1}$, where $C$ is the number of chunks of contiguously aligned words and $M$ is the number of words in the source sentence, which is easy to compute with several lines of code. We will release code afterwards.", "title": "Equation of fuzzy reordering score"}, "signatures": ["ICLR.cc/2020/Conference/Paper1083/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1083/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chuntinz@andrew.cmu.edu", "jgu@fb.com", "gneubig@cs.cmu.edu"], "title": "Understanding Knowledge Distillation in Non-autoregressive Machine Translation", "authors": ["Chunting Zhou", "Jiatao Gu", "Graham Neubig"], "pdf": "/pdf/a057d93f5526ce4ceb6751adc88766291d440cd7.pdf", "TL;DR": "We systematically examine why knowledge distillation is crucial to the training of non-autoregressive translation (NAT) models, and propose methods to further improve the distilled data to best match the capacity of an NAT model.", "abstract": "Non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models. Existing NAT models usually rely on the technique of knowledge distillation, which creates the training data from a pretrained autoregressive model for better performance. Knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear. In this paper, we first design systematic experiments to investigate why knowledge distillation is crucial to NAT training. We find that knowledge distillation can reduce the complexity of data sets and help NAT to model the variations in the output data. Furthermore, a strong correlation is observed between the capacity of an NAT model and the optimal complexity of the distilled data for the best translation quality. Based on these findings, we further propose several approaches that can alter the complexity of data sets to improve the performance of NAT models. We achieve the state-of-the-art performance for the NAT-based models, and close the gap with the autoregressive baseline on WMT14 En-De benchmark.", "keywords": ["knowledge distillation", "non-autoregressive neural machine translation"], "paperhash": "zhou|understanding_knowledge_distillation_in_nonautoregressive_machine_translation", "_bibtex": "@inproceedings{\nZhou2020Understanding,\ntitle={Understanding Knowledge Distillation in Non-autoregressive Machine Translation},\nauthor={Chunting Zhou and Jiatao Gu and Graham Neubig},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BygFVAEKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e0654412a5ccd8c30e0e75ccdf0f3464a42d638c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygFVAEKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1083/Authors", "ICLR.cc/2020/Conference/Paper1083/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1083/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1083/Reviewers", "ICLR.cc/2020/Conference/Paper1083/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1083/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1083/Authors|ICLR.cc/2020/Conference/Paper1083/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161527, "tmdate": 1576860534171, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1083/Authors", "ICLR.cc/2020/Conference/Paper1083/Reviewers", "ICLR.cc/2020/Conference/Paper1083/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1083/-/Official_Comment"}}}, {"id": "rklYWJn9FS", "original": null, "number": 2, "cdate": 1571630849225, "ddate": null, "tcdate": 1571630849225, "tmdate": 1571630849225, "tddate": null, "forum": "BygFVAEKDH", "replyto": "SkxKyddAOr", "invitation": "ICLR.cc/2020/Conference/Paper1083/-/Public_Comment", "content": {"comment": "Thanks a lot for your reply! \n\nA small question. I think the analysis in Section 4.2 is very interesting and instructive, could you provide more details about computing the fuzzy reordering score? I find that the cited paper (Talbot et.al., 2011) did not provide the source code.", "title": "fuzzy reordering score"}, "signatures": ["~Kaaliya_Budhil2"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Kaaliya_Budhil2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chuntinz@andrew.cmu.edu", "jgu@fb.com", "gneubig@cs.cmu.edu"], "title": "Understanding Knowledge Distillation in Non-autoregressive Machine Translation", "authors": ["Chunting Zhou", "Jiatao Gu", "Graham Neubig"], "pdf": "/pdf/a057d93f5526ce4ceb6751adc88766291d440cd7.pdf", "TL;DR": "We systematically examine why knowledge distillation is crucial to the training of non-autoregressive translation (NAT) models, and propose methods to further improve the distilled data to best match the capacity of an NAT model.", "abstract": "Non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models. Existing NAT models usually rely on the technique of knowledge distillation, which creates the training data from a pretrained autoregressive model for better performance. Knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear. In this paper, we first design systematic experiments to investigate why knowledge distillation is crucial to NAT training. We find that knowledge distillation can reduce the complexity of data sets and help NAT to model the variations in the output data. Furthermore, a strong correlation is observed between the capacity of an NAT model and the optimal complexity of the distilled data for the best translation quality. Based on these findings, we further propose several approaches that can alter the complexity of data sets to improve the performance of NAT models. We achieve the state-of-the-art performance for the NAT-based models, and close the gap with the autoregressive baseline on WMT14 En-De benchmark.", "keywords": ["knowledge distillation", "non-autoregressive neural machine translation"], "paperhash": "zhou|understanding_knowledge_distillation_in_nonautoregressive_machine_translation", "_bibtex": "@inproceedings{\nZhou2020Understanding,\ntitle={Understanding Knowledge Distillation in Non-autoregressive Machine Translation},\nauthor={Chunting Zhou and Jiatao Gu and Graham Neubig},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BygFVAEKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e0654412a5ccd8c30e0e75ccdf0f3464a42d638c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygFVAEKDH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504199884, "tmdate": 1576860567815, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1083/Authors", "ICLR.cc/2020/Conference/Paper1083/Reviewers", "ICLR.cc/2020/Conference/Paper1083/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1083/-/Public_Comment"}}}, {"id": "SkxKyddAOr", "original": null, "number": 2, "cdate": 1570830305165, "ddate": null, "tcdate": 1570830305165, "tmdate": 1570830305165, "tddate": null, "forum": "BygFVAEKDH", "replyto": "rygAQB6a_H", "invitation": "ICLR.cc/2020/Conference/Paper1083/-/Official_Comment", "content": {"comment": "Thanks for your comments! \n\nActually, we have done experiments on other language pairs and directions, e.g. de-en, ro-en. We had the same conclusions for these different language pairs. Due to space limitation, we didn't include them in our paper. \n\nFor example, for ro-en, we find that the conditional entropy C(d) of different distillation data from AT models of different capacity is very close to each other in contrast with the en-de case (shown in Figure 7 Appendix B.), and the gap between the real data and the distillation data is not as large as that of en-de as well. Accordingly, we observe that distilling from different AT models of different capacity doesn't show improvements especially for bigger-capacity NAT models, e.g. with LevT we get 32.22 vs 33.1 vs 33.26 with real data, small AT distillation and base AT distillation data respectively.\n\nWe will definitely add experiments on more language pairs in the final version for completeness.", "title": "We have done experiments for other language pairs and the conclusion is the same."}, "signatures": ["ICLR.cc/2020/Conference/Paper1083/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1083/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chuntinz@andrew.cmu.edu", "jgu@fb.com", "gneubig@cs.cmu.edu"], "title": "Understanding Knowledge Distillation in Non-autoregressive Machine Translation", "authors": ["Chunting Zhou", "Jiatao Gu", "Graham Neubig"], "pdf": "/pdf/a057d93f5526ce4ceb6751adc88766291d440cd7.pdf", "TL;DR": "We systematically examine why knowledge distillation is crucial to the training of non-autoregressive translation (NAT) models, and propose methods to further improve the distilled data to best match the capacity of an NAT model.", "abstract": "Non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models. Existing NAT models usually rely on the technique of knowledge distillation, which creates the training data from a pretrained autoregressive model for better performance. Knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear. In this paper, we first design systematic experiments to investigate why knowledge distillation is crucial to NAT training. We find that knowledge distillation can reduce the complexity of data sets and help NAT to model the variations in the output data. Furthermore, a strong correlation is observed between the capacity of an NAT model and the optimal complexity of the distilled data for the best translation quality. Based on these findings, we further propose several approaches that can alter the complexity of data sets to improve the performance of NAT models. We achieve the state-of-the-art performance for the NAT-based models, and close the gap with the autoregressive baseline on WMT14 En-De benchmark.", "keywords": ["knowledge distillation", "non-autoregressive neural machine translation"], "paperhash": "zhou|understanding_knowledge_distillation_in_nonautoregressive_machine_translation", "_bibtex": "@inproceedings{\nZhou2020Understanding,\ntitle={Understanding Knowledge Distillation in Non-autoregressive Machine Translation},\nauthor={Chunting Zhou and Jiatao Gu and Graham Neubig},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BygFVAEKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e0654412a5ccd8c30e0e75ccdf0f3464a42d638c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygFVAEKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1083/Authors", "ICLR.cc/2020/Conference/Paper1083/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1083/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1083/Reviewers", "ICLR.cc/2020/Conference/Paper1083/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1083/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1083/Authors|ICLR.cc/2020/Conference/Paper1083/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161527, "tmdate": 1576860534171, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1083/Authors", "ICLR.cc/2020/Conference/Paper1083/Reviewers", "ICLR.cc/2020/Conference/Paper1083/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1083/-/Official_Comment"}}}, {"id": "rygAQB6a_H", "original": null, "number": 1, "cdate": 1570784550272, "ddate": null, "tcdate": 1570784550272, "tmdate": 1570784550272, "tddate": null, "forum": "BygFVAEKDH", "replyto": "BygFVAEKDH", "invitation": "ICLR.cc/2020/Conference/Paper1083/-/Public_Comment", "content": {"comment": "Very complete and impressive work! \n\nI have a question about language pairs. As shown by previous works such as the original NAT and mask-predict, knowledge distillation has more impact on WMT14 En-De/De-En. However, it conducts minor effects on WMT16 En-Ro/Ro-En. So I think the performance of knowledge distillation is related to specific language pairs. While you only analyze the translation from English to German, have you conducted experiments on other language pairs or directions (de-en)? Are the conclusions the same as that on en-de?", "title": "other language pairs"}, "signatures": ["~Kaaliya_Budhil2"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Kaaliya_Budhil2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chuntinz@andrew.cmu.edu", "jgu@fb.com", "gneubig@cs.cmu.edu"], "title": "Understanding Knowledge Distillation in Non-autoregressive Machine Translation", "authors": ["Chunting Zhou", "Jiatao Gu", "Graham Neubig"], "pdf": "/pdf/a057d93f5526ce4ceb6751adc88766291d440cd7.pdf", "TL;DR": "We systematically examine why knowledge distillation is crucial to the training of non-autoregressive translation (NAT) models, and propose methods to further improve the distilled data to best match the capacity of an NAT model.", "abstract": "Non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models. Existing NAT models usually rely on the technique of knowledge distillation, which creates the training data from a pretrained autoregressive model for better performance. Knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear. In this paper, we first design systematic experiments to investigate why knowledge distillation is crucial to NAT training. We find that knowledge distillation can reduce the complexity of data sets and help NAT to model the variations in the output data. Furthermore, a strong correlation is observed between the capacity of an NAT model and the optimal complexity of the distilled data for the best translation quality. Based on these findings, we further propose several approaches that can alter the complexity of data sets to improve the performance of NAT models. We achieve the state-of-the-art performance for the NAT-based models, and close the gap with the autoregressive baseline on WMT14 En-De benchmark.", "keywords": ["knowledge distillation", "non-autoregressive neural machine translation"], "paperhash": "zhou|understanding_knowledge_distillation_in_nonautoregressive_machine_translation", "_bibtex": "@inproceedings{\nZhou2020Understanding,\ntitle={Understanding Knowledge Distillation in Non-autoregressive Machine Translation},\nauthor={Chunting Zhou and Jiatao Gu and Graham Neubig},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BygFVAEKDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e0654412a5ccd8c30e0e75ccdf0f3464a42d638c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygFVAEKDH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504199884, "tmdate": 1576860567815, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1083/Authors", "ICLR.cc/2020/Conference/Paper1083/Reviewers", "ICLR.cc/2020/Conference/Paper1083/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1083/-/Public_Comment"}}}], "count": 17}