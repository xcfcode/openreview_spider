{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521582959669, "tcdate": 1520140508088, "number": 1, "cdate": 1520140508088, "id": "SyEkWWKdG", "invitation": "ICLR.cc/2018/Workshop/-/Paper26/Official_Review", "forum": "Bkl1FEdLM", "replyto": "Bkl1FEdLM", "signatures": ["ICLR.cc/2018/Workshop/Paper26/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper26/AnonReviewer2"], "content": {"title": "Lack of convergence discussion", "rating": "4: Ok but not good enough - rejection", "review": "This paper introduces a simple method to adjust the size of gradients for alleviating the perturbation problem in ASGD. This method emphasizes the gradient which has different phenomenon as compared with other gradients. As a result, experimental results show that their approach boost performances of asynchronous stochastic optimizers.\n\nThe concern of this paper is the lack of the discussion of convergences. I could not understand whether this method is guaranteed to converge to some local minimum or oscillates around the local minimum. Even if this method could not have a theoretical guarantee, I think the discussion about the convergence is needed either theoretically or experimentally.\n\n* Pros\n- This paper proposes a simple method to alleviate the perturbation problem in the asynchronous SGD training due to gradient delay.\n\n* Cons\n- This paper lacks the convergence proof of this method and related discussions.", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Boosting Gradient-based Optimizers for Asynchronous Parallelism", "abstract": "Stochastic gradient descent methods have been broadly used in training deep neural network models. However, the classic approaches may suffer from gradient delay and thus perturb the training under asynchronous parallelism. In this paper, we present an approach tackling this challenge by adaptively adjusting the size of each optimizing step. We demonstrate that our approach significantly boost SGD, AdaGrad and Momentum optimizers for two very different tasks: image classification and click through rate prediction.", "pdf": "/pdf/39d2de3a9e42b74e9d27ac7d764773a072bd7e2f.pdf", "paperhash": "li|boosting_gradientbased_optimizers_for_asynchronous_parallelism", "keywords": [], "authors": ["Shuai Li", "Yi Ren", "Dongchang Xu", "Lin Guo", "Hang Xiang", "Di Zhang", "Jinhui Li"], "authorids": ["voolc.li@alibaba-inc.com", "hengrui.ry@alibaba-inc.com", "dongchang.xu@alibaba-inc.com", "lin.gl@alibaba-inc.com", "xingzhi.xh@alibaba-inc.com", "di.zhangd@alibaba-inc.com", "jinhui.li@alibaba-inc.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582959476, "id": "ICLR.cc/2018/Workshop/-/Paper26/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper26/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper26/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper26/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper26/AnonReviewer1"], "reply": {"forum": "Bkl1FEdLM", "replyto": "Bkl1FEdLM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper26/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper26/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582959476}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582758995, "tcdate": 1520645701197, "number": 2, "cdate": 1520645701197, "id": "H16rLnlFz", "invitation": "ICLR.cc/2018/Workshop/-/Paper26/Official_Review", "forum": "Bkl1FEdLM", "replyto": "Bkl1FEdLM", "signatures": ["ICLR.cc/2018/Workshop/Paper26/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper26/AnonReviewer3"], "content": {"title": "Promising ideas, unconvincing evaluations", "rating": "5: Marginally below acceptance threshold", "review": "Summary\nAdapting learning rates to account for delayed gradients in Asynchronous update methods. The idea is applied to three different optimizers for two different tasks and shown to improve over raw baseline for those methods.\n\nPros\n- The idea is interesting, simple and relevant. It is easy to scale up asynchronous methods in cloud like environments and having ways for them to be more effective is quite useful.\n- The evaluation is applied to two fairly different models and consistently works across each of the optimizers\n\nCons\n- The work needed for each of the optimizers is different, hence one would need to do the same for every new optimizer. The good thing is that the basic ideas are simple enough and similar.\n- Eval only compares a single baseline (no adaptation) to their boosting. No comparisons even with papers cited that offer solutions to delays in gradients e.g. DC-ASGD.\n\nMinor clarification. Table #1 and Table #2 have parallel num that appears to number of parallel workers/updates. Will be great to clarify that.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Boosting Gradient-based Optimizers for Asynchronous Parallelism", "abstract": "Stochastic gradient descent methods have been broadly used in training deep neural network models. However, the classic approaches may suffer from gradient delay and thus perturb the training under asynchronous parallelism. In this paper, we present an approach tackling this challenge by adaptively adjusting the size of each optimizing step. We demonstrate that our approach significantly boost SGD, AdaGrad and Momentum optimizers for two very different tasks: image classification and click through rate prediction.", "pdf": "/pdf/39d2de3a9e42b74e9d27ac7d764773a072bd7e2f.pdf", "paperhash": "li|boosting_gradientbased_optimizers_for_asynchronous_parallelism", "keywords": [], "authors": ["Shuai Li", "Yi Ren", "Dongchang Xu", "Lin Guo", "Hang Xiang", "Di Zhang", "Jinhui Li"], "authorids": ["voolc.li@alibaba-inc.com", "hengrui.ry@alibaba-inc.com", "dongchang.xu@alibaba-inc.com", "lin.gl@alibaba-inc.com", "xingzhi.xh@alibaba-inc.com", "di.zhangd@alibaba-inc.com", "jinhui.li@alibaba-inc.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582959476, "id": "ICLR.cc/2018/Workshop/-/Paper26/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper26/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper26/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper26/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper26/AnonReviewer1"], "reply": {"forum": "Bkl1FEdLM", "replyto": "Bkl1FEdLM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper26/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper26/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582959476}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582599390, "tcdate": 1520977504340, "number": 3, "cdate": 1520977504340, "id": "H1dw8TrKf", "invitation": "ICLR.cc/2018/Workshop/-/Paper26/Official_Review", "forum": "Bkl1FEdLM", "replyto": "Bkl1FEdLM", "signatures": ["ICLR.cc/2018/Workshop/Paper26/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper26/AnonReviewer1"], "content": {"title": "Possibly worthwhile exploration of adaptive step size based SGD.", "rating": "5: Marginally below acceptance threshold", "review": "The paper presents a reasonable idea for adapting the step size during SGD. The others relate their proposal to existing adaptive step size methods such as Adagrad. The weakness of the work is the messy and very often ungrammatical writing style. In my view this puts it slightly below the acceptance threshold. It should be proof-read by a native speaker of english. \"This produces an adorable performance for Adagrad\"??", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Boosting Gradient-based Optimizers for Asynchronous Parallelism", "abstract": "Stochastic gradient descent methods have been broadly used in training deep neural network models. However, the classic approaches may suffer from gradient delay and thus perturb the training under asynchronous parallelism. In this paper, we present an approach tackling this challenge by adaptively adjusting the size of each optimizing step. We demonstrate that our approach significantly boost SGD, AdaGrad and Momentum optimizers for two very different tasks: image classification and click through rate prediction.", "pdf": "/pdf/39d2de3a9e42b74e9d27ac7d764773a072bd7e2f.pdf", "paperhash": "li|boosting_gradientbased_optimizers_for_asynchronous_parallelism", "keywords": [], "authors": ["Shuai Li", "Yi Ren", "Dongchang Xu", "Lin Guo", "Hang Xiang", "Di Zhang", "Jinhui Li"], "authorids": ["voolc.li@alibaba-inc.com", "hengrui.ry@alibaba-inc.com", "dongchang.xu@alibaba-inc.com", "lin.gl@alibaba-inc.com", "xingzhi.xh@alibaba-inc.com", "di.zhangd@alibaba-inc.com", "jinhui.li@alibaba-inc.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582959476, "id": "ICLR.cc/2018/Workshop/-/Paper26/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper26/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper26/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper26/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper26/AnonReviewer1"], "reply": {"forum": "Bkl1FEdLM", "replyto": "Bkl1FEdLM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper26/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper26/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582959476}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573589014, "tcdate": 1521573589014, "number": 197, "cdate": 1521573588675, "id": "SyaR0CCtG", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "Bkl1FEdLM", "replyto": "Bkl1FEdLM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Boosting Gradient-based Optimizers for Asynchronous Parallelism", "abstract": "Stochastic gradient descent methods have been broadly used in training deep neural network models. However, the classic approaches may suffer from gradient delay and thus perturb the training under asynchronous parallelism. In this paper, we present an approach tackling this challenge by adaptively adjusting the size of each optimizing step. We demonstrate that our approach significantly boost SGD, AdaGrad and Momentum optimizers for two very different tasks: image classification and click through rate prediction.", "pdf": "/pdf/39d2de3a9e42b74e9d27ac7d764773a072bd7e2f.pdf", "paperhash": "li|boosting_gradientbased_optimizers_for_asynchronous_parallelism", "keywords": [], "authors": ["Shuai Li", "Yi Ren", "Dongchang Xu", "Lin Guo", "Hang Xiang", "Di Zhang", "Jinhui Li"], "authorids": ["voolc.li@alibaba-inc.com", "hengrui.ry@alibaba-inc.com", "dongchang.xu@alibaba-inc.com", "lin.gl@alibaba-inc.com", "xingzhi.xh@alibaba-inc.com", "di.zhangd@alibaba-inc.com", "jinhui.li@alibaba-inc.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1518319144447, "tcdate": 1517992152259, "number": 26, "cdate": 1517992152259, "id": "Bkl1FEdLM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "Bkl1FEdLM", "signatures": ["~shuai_li4"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Boosting Gradient-based Optimizers for Asynchronous Parallelism", "abstract": "Stochastic gradient descent methods have been broadly used in training deep neural network models. However, the classic approaches may suffer from gradient delay and thus perturb the training under asynchronous parallelism. In this paper, we present an approach tackling this challenge by adaptively adjusting the size of each optimizing step. We demonstrate that our approach significantly boost SGD, AdaGrad and Momentum optimizers for two very different tasks: image classification and click through rate prediction.", "pdf": "/pdf/39d2de3a9e42b74e9d27ac7d764773a072bd7e2f.pdf", "paperhash": "li|boosting_gradientbased_optimizers_for_asynchronous_parallelism", "keywords": [], "authors": ["Shuai Li", "Yi Ren", "Dongchang Xu", "Lin Guo", "Hang Xiang", "Di Zhang", "Jinhui Li"], "authorids": ["voolc.li@alibaba-inc.com", "hengrui.ry@alibaba-inc.com", "dongchang.xu@alibaba-inc.com", "lin.gl@alibaba-inc.com", "xingzhi.xh@alibaba-inc.com", "di.zhangd@alibaba-inc.com", "jinhui.li@alibaba-inc.com"]}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}], "count": 5}