{"notes": [{"id": "BJg9DoR9t7", "original": "rklvqGzYYX", "number": 287, "cdate": 1538087777732, "ddate": null, "tcdate": 1538087777732, "tmdate": 1569425369978, "tddate": null, "forum": "BJg9DoR9t7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Max-MIG: an Information Theoretic Approach for Joint Learning from Crowds", "abstract": "Eliciting labels from crowds is a potential way to obtain large labeled data. Despite a variety of methods developed for learning from crowds, a key challenge remains unsolved: \\emph{learning from crowds without knowing the information structure among the crowds a priori, when some people of the crowds make highly correlated mistakes and some of them label effortlessly (e.g. randomly)}. We propose an information theoretic approach, Max-MIG, for joint learning from crowds, with a common assumption: the crowdsourced labels and the data are independent conditioning on the ground truth. Max-MIG simultaneously aggregates the crowdsourced labels and learns an accurate data classifier. Furthermore, we devise an accurate data-crowds forecaster that employs both the data and the crowdsourced labels to forecast the ground truth. To the best of our knowledge, this is the first algorithm that solves the aforementioned challenge of learning from crowds. In addition to the theoretical validation, we also empirically show that our algorithm achieves the new state-of-the-art results in most settings, including the real-world data, and is the first algorithm that is robust to various information structures. Codes are available at https://github.com/Newbeeer/Max-MIG .\n", "keywords": ["crowdsourcing", "information theory"], "authorids": ["caopeng2016@pku.edu.cn", "xuyilun@pku.edu.cn", "yuqing.kong@pku.edu.cn", "yizhou.wang@pku.edu.cn"], "authors": ["Peng Cao*", "Yilun Xu*", "Yuqing Kong", "Yizhou  Wang"], "pdf": "/pdf/6d826c78b6d8b90396fcb156ac1799e0ffe56737.pdf", "paperhash": "cao|maxmig_an_information_theoretic_approach_for_joint_learning_from_crowds", "_bibtex": "@inproceedings{\ncao2018maxmig,\ntitle={Max-{MIG}: an Information Theoretic Approach for Joint Learning from Crowds},\nauthor={Peng Cao and Yilun Xu and Yuqing Kong and Yizhou  Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJg9DoR9t7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SyxZBh5Ve4", "original": null, "number": 1, "cdate": 1545018425396, "ddate": null, "tcdate": 1545018425396, "tmdate": 1545354472609, "tddate": null, "forum": "BJg9DoR9t7", "replyto": "BJg9DoR9t7", "invitation": "ICLR.cc/2019/Conference/-/Paper287/Meta_Review", "content": {"metareview": "This paper proposes an interesting approach to leveraging crowd-sourced labels, along with an ML model learned from the data itself. \n\nThe reviewers were unanimous in their vote to accept.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Interesting idea for interpreting crowd-sourced labels"}, "signatures": ["ICLR.cc/2019/Conference/Paper287/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper287/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Max-MIG: an Information Theoretic Approach for Joint Learning from Crowds", "abstract": "Eliciting labels from crowds is a potential way to obtain large labeled data. Despite a variety of methods developed for learning from crowds, a key challenge remains unsolved: \\emph{learning from crowds without knowing the information structure among the crowds a priori, when some people of the crowds make highly correlated mistakes and some of them label effortlessly (e.g. randomly)}. We propose an information theoretic approach, Max-MIG, for joint learning from crowds, with a common assumption: the crowdsourced labels and the data are independent conditioning on the ground truth. Max-MIG simultaneously aggregates the crowdsourced labels and learns an accurate data classifier. Furthermore, we devise an accurate data-crowds forecaster that employs both the data and the crowdsourced labels to forecast the ground truth. To the best of our knowledge, this is the first algorithm that solves the aforementioned challenge of learning from crowds. In addition to the theoretical validation, we also empirically show that our algorithm achieves the new state-of-the-art results in most settings, including the real-world data, and is the first algorithm that is robust to various information structures. Codes are available at https://github.com/Newbeeer/Max-MIG .\n", "keywords": ["crowdsourcing", "information theory"], "authorids": ["caopeng2016@pku.edu.cn", "xuyilun@pku.edu.cn", "yuqing.kong@pku.edu.cn", "yizhou.wang@pku.edu.cn"], "authors": ["Peng Cao*", "Yilun Xu*", "Yuqing Kong", "Yizhou  Wang"], "pdf": "/pdf/6d826c78b6d8b90396fcb156ac1799e0ffe56737.pdf", "paperhash": "cao|maxmig_an_information_theoretic_approach_for_joint_learning_from_crowds", "_bibtex": "@inproceedings{\ncao2018maxmig,\ntitle={Max-{MIG}: an Information Theoretic Approach for Joint Learning from Crowds},\nauthor={Peng Cao and Yilun Xu and Yuqing Kong and Yizhou  Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJg9DoR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper287/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353270449, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJg9DoR9t7", "replyto": "BJg9DoR9t7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper287/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper287/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper287/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353270449}}}, {"id": "ryxhjH0zkE", "original": null, "number": 12, "cdate": 1543853476068, "ddate": null, "tcdate": 1543853476068, "tmdate": 1543853476068, "tddate": null, "forum": "BJg9DoR9t7", "replyto": "ryeXwploA7", "invitation": "ICLR.cc/2019/Conference/-/Paper287/Official_Comment", "content": {"title": "Accept", "comment": "I also consider this paper as an accept (as indicated in my updated review on Nov 26).\n\nThe literature on crowdsourced labeling assumes models with independent answers given by workers. In contrast, this paper permits correlations in workers' answers. It has some theoretical justification and strong performance in practice. I do think this is a good enough for accept (hence a 7). I do not give an 8 or above because:\n- The model is weaker (better) in the sense that it allows for correlations, but worse in the sense that it assumes all questions have equal difficulty as compared to the state of the art. The correlations are also somewhat restrictive. I would have given a higher score if these restrictions were absent.\n- The theory only says that this is a good heuristic. I would have given a much higher score if the theory also showed guarantees on sample complexity etc.\n\nOverall I think this is a good paper and worthy of acceptance."}, "signatures": ["ICLR.cc/2019/Conference/Paper287/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper287/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper287/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Max-MIG: an Information Theoretic Approach for Joint Learning from Crowds", "abstract": "Eliciting labels from crowds is a potential way to obtain large labeled data. Despite a variety of methods developed for learning from crowds, a key challenge remains unsolved: \\emph{learning from crowds without knowing the information structure among the crowds a priori, when some people of the crowds make highly correlated mistakes and some of them label effortlessly (e.g. randomly)}. We propose an information theoretic approach, Max-MIG, for joint learning from crowds, with a common assumption: the crowdsourced labels and the data are independent conditioning on the ground truth. Max-MIG simultaneously aggregates the crowdsourced labels and learns an accurate data classifier. Furthermore, we devise an accurate data-crowds forecaster that employs both the data and the crowdsourced labels to forecast the ground truth. To the best of our knowledge, this is the first algorithm that solves the aforementioned challenge of learning from crowds. In addition to the theoretical validation, we also empirically show that our algorithm achieves the new state-of-the-art results in most settings, including the real-world data, and is the first algorithm that is robust to various information structures. Codes are available at https://github.com/Newbeeer/Max-MIG .\n", "keywords": ["crowdsourcing", "information theory"], "authorids": ["caopeng2016@pku.edu.cn", "xuyilun@pku.edu.cn", "yuqing.kong@pku.edu.cn", "yizhou.wang@pku.edu.cn"], "authors": ["Peng Cao*", "Yilun Xu*", "Yuqing Kong", "Yizhou  Wang"], "pdf": "/pdf/6d826c78b6d8b90396fcb156ac1799e0ffe56737.pdf", "paperhash": "cao|maxmig_an_information_theoretic_approach_for_joint_learning_from_crowds", "_bibtex": "@inproceedings{\ncao2018maxmig,\ntitle={Max-{MIG}: an Information Theoretic Approach for Joint Learning from Crowds},\nauthor={Peng Cao and Yilun Xu and Yuqing Kong and Yizhou  Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJg9DoR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper287/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618384, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJg9DoR9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper287/Authors", "ICLR.cc/2019/Conference/Paper287/Reviewers", "ICLR.cc/2019/Conference/Paper287/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper287/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper287/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper287/Authors|ICLR.cc/2019/Conference/Paper287/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper287/Reviewers", "ICLR.cc/2019/Conference/Paper287/Authors", "ICLR.cc/2019/Conference/Paper287/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618384}}}, {"id": "rygf8BMF37", "original": null, "number": 2, "cdate": 1541117258125, "ddate": null, "tcdate": 1541117258125, "tmdate": 1543331686341, "tddate": null, "forum": "BJg9DoR9t7", "replyto": "BJg9DoR9t7", "invitation": "ICLR.cc/2019/Conference/-/Paper287/Official_Review", "content": {"title": "More details on actual learning are required", "review": "Update after feedback: I would like to thank the authors for their detailed answers, it would be great to see some revisions in the paper also though (except new experimental results).\nEspecially thank you for providing details of a training procedure which I was missing in the initial draft. I hope to see them in the paper (at least some of them).\n\nI have increased the rating to 6. Given new experimental results both on real data and forecaster comparison I would like to increase the rating to 7. However, I am not sure that this is fair to other authors who would might not be physically able to provide new experimental results due to computational constraints, please note that the experiments in this paper are rather 'light' in the standards of modern deep learning experiments and can be done within the rebuttal period.  \n====================================================\n\n\nThe paper finds a practical implementation of ideas from Kong & Schoenebeck (2018) for the learning with crowd problem. It proofs the claims from Kong & Schoenebeck (2018) for the specific family of data classifiers and crowd aggregators. From the general perspective, the papers proposes a method for joint training a classifier and a crowd label aggregator with particular consideration of correlated crowd labels. \n\nThe paper is fairly well-written and well-balanced between theoretical and empirical justification of the method. I see 1 major and 1 big issues with the paper.\n\nMajor issue: I am missing details of the actual procedure of training the model. Is MIG set as a loss function for the data classifier NN? Is crowd aggregator trained also as an NN with MIG as a loss function? How do the authors find the optimal p? Also, in order all the provided theory to work all the found data classifier NN, the aggregator and p should be exact maximisers of MIG as far as I understand. How do the author ensure that they find the exact maximisers? Also related to understanding how training works: on p.15 the authors claim \u201cNote that our method can handle this simple correlated mistakes case and will give all useless experts weight zero based on Theorem 3.4.\u201d I have trouble understanding why the proposed method should find these zero weights rather than it is just able to find them?\n\nI am willing to change my judgement if the authors provide convincing details on the training procedure.\n\nBig issue: Experimental settings. \na) Though it is interesting to see the analysis of the method under controlled environments of synthetic crowd labels with different properties that show benefits of the proposed method (such as dealing with correlated crowd labels), it would be also appealing to see the results with real labels, for example, Rodrigues & Pereira (2017) provide Amazon MTurk crowd labels for the LabelMe data\nb) Is the proposed data-crowd forecaster the only method that uses crowd labels on the test data? While it can be argued that it is not straightforward in the test regime to include crowd labels into Crowd Layer, for example, without retraining the neural net, AggNet can use crowd labels without retraining the neural net part. In the presented format, it is unfair to compare the forecaster with the other methods because it uses more information, and essentially, the forecaster is not compared with anything (that uses the same information). It can be compared, at least, with pure Majority Voting, or more advanced pure crowdsourcing aggregation methods. Yes, they won\u2019t use image data, but at least they can use the same amount of crowd label information, which would make a nice comparison with the presented related work and proposed NN: this is what you can get using just image data during test (Crowd Layer, Max-MIG, and others from the current paper), this is what you can get using just crowd labels during test (Majority Voting or, preferably, more advanced pure crowdsourcing aggregators), and this is what you can get using both image and crowd labels during test (the proposed forecaster and AggNet, for example)\n\nQuestions out of curiosity: \ni). Does Max-MIG handle missing crowd labels for some data points? Did the author use missing labels in the experiments?\nii). Both the Dogs vs. Cats and CIFAR-10 datasets have more or less balanced data, i.e., the number of data points belonging to each ground truth class is similar between classes. Is this true for the LUNA16 dataset? If yes, have the authors tried their method with heavily imbalanced data? In my experience, some crowdsourcing methods may suffer with imbalanced data, for example, Crowd Layer does so on some data. This tendency of Crowd Layer is kind of confirmed on the provided Dogs vs. Cats in the na\u00efve majority case, where based on crowd labels the first class dominates the second.\n\nOther questions/issues/suggestions:\n1. Until the formal introduction of the forecaster on page 4, it is not entirely clear what is the difference between the data classifier and data-crowd forecaster. It should be explained more clearly at the beginning that the 3 concepts (data classifier, crowd label aggregator and \"data-crowd forecaster\") are separated. Also some motivation why we should care about forecaster would be beneficial because one can argue that if we could train a NN that would make good enough predictions why we should waste resources on crowd labels. For example, the provided empirical results can be used as an argument for this.\n2. From the introduction it is unclear that there are methods in crowdsourcing that do not rely on the assumption that data and crowd labels are independent given the ground truth labels. As mentioned in related works there are methods dealing with difficulty of data points, where models assume that crowd labels maybe biased on some data points due to their difficulty, e.g., if images are blurred, which violates this assumption.\nAlso the note that considering image difficulty violates the independence assumption could be added on page 3 around \"[we] do not consider the image difficulty\"\n3. The beginning of page 4. I think it would be more clear to replace \"5 experts' labels:\" by $y^{[5]}=$\n4. I suggest to move the caption of Figure 3 into the main text.  \n5. p.3 \"However, these works are still not robust to correlated mistakes\" - Why? \n6. Data-crowds forecaster equation. It would be good to add some intuition about this choice. The product between the classifier and aggregator predictions seems reasonable, division on p_c is not that obvious. This expression presumably maximises the information gain introduced below. Some link between this equation and the gain introduction would be nice. Also, minor point \u2013 it is better to enlarge inner brackets ()_c\n7. The formulation \u201cTo the best of our knowledge, our approach is a very early algorithm\u201d, and namely \u201ca very early algorithm\u201d is unclear for me\n8. Dual usage of \u201cinformation intersection\u201d as an assumption and as something that Max-MIG finds is confusing\n9. Any comments how the learning rates were chosen are always beneficial\n10. Proof of Proposition C.3: \u201cBased on the result of Lemma C.2, by assuming that h \u2217 \u2208 H_{NN} , we can see (h \u2217 , g\u2217 ,p \u2217 ) is a maximizer of max_{h\u2208H_{NN} ,g\u2208G_{W A},p\u2208\u2206_C} MIGf (h, g,p)\u201d \u2013 is expectation missing in the max equation? Is this shown below on page 13? If yes, then the authors should paraphrase this sentence as it does not imply that this is actually shown below\n11. p.12 (and below) \u2013 what is $\\mathbf{C}^m$? Is it $\\mathbf{W}^m$?\n12. p.15 (at the end of proof) $p \\log q$ and $p \\log p$ are not formally defined\n\nMinor:\n1. p.1 \"of THE data-driven-based machine learning paradigm\"\n2. \"crowds aggregator\" -> \"crowd aggregator\"?\n3. p.2 (and below) \"between the data and crowdsourced labels i.e. the ground truth labelS\"\n4. Rodrigues & Pereira (2017) has a published version (AAAI) of their paper\n5. p.2 \"that model multiple experts individually and explicitly in A neural network\"\n6. p.3 \"model the crowds by A Gaussian process\"\n7. p.3 \"We model the crowds via confusion matriCES\"\n8. p.3 \"only provide A theoretic framework and assume AN extremely high model complexity\"\n9. p.4 \"forecast\" for h and g -> \"prediction\"?\n10. p.6 \u201cbetween the data and the crowdsourced labelS\u201d?\n11. p.6 \u201cHowever, in practice, with A finite number of datapoints\u201d\n12. p.6 \u201cthe experiment section will show that our picked H_{NN} and G_{W A} are sufficientLY simple to avoid over-fitting\u201d\n13. p.6 \u201cWe call them A Bayesian posterior data classifier / crowds aggregator / data-crowds forecaster, RESPECTEVILY\u201d\n14. p.6 \u201cTheorem 3.4. With assumptionS 3.1, 3.3\u201d\n15. p.7 \u201cDoctOr Net, the method proposed by Guan et al. (2017)\u201d\n16. p.7 \u201cincluding the naive majority case since naive expert is independent with everything\u201d \u2013 rephrasing is required, unclear what \u201cindependent with everything\u201d means and who is \u201cna\u00efve expert\u201d\n17. Please capitalised names of conferences and journals in References\n18. p.10 \u201cshe labels the image as \u201cdog\u201d/\u201ccat\u201d with THE probability 0.6/0.8 respectively\u201d, \u201c(e.g. B labels the image as \u201ccat\u201d with THE probability 0.5 and \u201cdog\u201d with THE probability 0.5 when the image has cats or dogs)\u201d\n19. p.12 \u201cLemma C.2. (Kong & Schoenebeck, 2018) With assumptionS 3.1, 3.3\u201d, \u201cProposition C.3. [Independent mistakes] With assumptionS 3.1, 3.3\u201d\n\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper287/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Max-MIG: an Information Theoretic Approach for Joint Learning from Crowds", "abstract": "Eliciting labels from crowds is a potential way to obtain large labeled data. Despite a variety of methods developed for learning from crowds, a key challenge remains unsolved: \\emph{learning from crowds without knowing the information structure among the crowds a priori, when some people of the crowds make highly correlated mistakes and some of them label effortlessly (e.g. randomly)}. We propose an information theoretic approach, Max-MIG, for joint learning from crowds, with a common assumption: the crowdsourced labels and the data are independent conditioning on the ground truth. Max-MIG simultaneously aggregates the crowdsourced labels and learns an accurate data classifier. Furthermore, we devise an accurate data-crowds forecaster that employs both the data and the crowdsourced labels to forecast the ground truth. To the best of our knowledge, this is the first algorithm that solves the aforementioned challenge of learning from crowds. In addition to the theoretical validation, we also empirically show that our algorithm achieves the new state-of-the-art results in most settings, including the real-world data, and is the first algorithm that is robust to various information structures. Codes are available at https://github.com/Newbeeer/Max-MIG .\n", "keywords": ["crowdsourcing", "information theory"], "authorids": ["caopeng2016@pku.edu.cn", "xuyilun@pku.edu.cn", "yuqing.kong@pku.edu.cn", "yizhou.wang@pku.edu.cn"], "authors": ["Peng Cao*", "Yilun Xu*", "Yuqing Kong", "Yizhou  Wang"], "pdf": "/pdf/6d826c78b6d8b90396fcb156ac1799e0ffe56737.pdf", "paperhash": "cao|maxmig_an_information_theoretic_approach_for_joint_learning_from_crowds", "_bibtex": "@inproceedings{\ncao2018maxmig,\ntitle={Max-{MIG}: an Information Theoretic Approach for Joint Learning from Crowds},\nauthor={Peng Cao and Yilun Xu and Yuqing Kong and Yizhou  Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJg9DoR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper287/Official_Review", "cdate": 1542234496125, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJg9DoR9t7", "replyto": "BJg9DoR9t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper287/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335690606, "tmdate": 1552335690606, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper287/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BklLgntDhX", "original": null, "number": 1, "cdate": 1541016558182, "ddate": null, "tcdate": 1541016558182, "tmdate": 1543252400822, "tddate": null, "forum": "BJg9DoR9t7", "replyto": "BJg9DoR9t7", "invitation": "ICLR.cc/2019/Conference/-/Paper287/Official_Review", "content": {"title": "Updated review", "review": "EDIT: I thank the authors for providing all clarifications. I think this paper is a useful contribution. It will be of interest to the audience in the conference.\n\nSummary:\nThis paper provides a method to jointly learn from crowdsourced worker labels and the actual data. The key claimed difference is that previous works on crowdsourced worker labels ignored the data. At a higher level, the algorithm comprises maximizing the mutual information gain between the worker labels and the output of a neural network (or more generally any ML model) on the data. \n\nEvaluation:\nI like the idea behind the algorithm. However there are several issues on which I ask the authors to provide some clarity. I will provide a formal \"evaluation\" after that. (For the moment, please ignore the \"rating\". I will provide one after the rebuttal.) \n\n(1) As the authors clarified, one key aspect of the \"information intersection\" assumption is that the crowdsourced labels are statistically independent from the data when conditioned on the ground truth. How strongly does this coincide with reality? Since the work is primary empirical, is there any evidence on this front?\n\n(2) In the abstract, introduction etc., what does it mean to say that the algorithm is an \"early algorithm\"?\n-- Thanks for the clarification. I would suggest using the term \"first algorithm\" in such cases. However, is this the first algorithm towards this goal? See point (3).\n\n(3) The submitted paper misses an extremely relevant piece of literature: \"Learning From Noisy Singly-labeled Data\" (arXiv:1712.04577). This paper also aims to solve the label + features problem together. How do the results of this paper compare to that of this submission?\n\n(4) \"Model and assumptions\" Is the i.i.d. assumption across the values of \"i\"? Then does that not violate the earlier claim of accommodating correlated mistakes?\n\n(5) Recent papers on crowdsourcing (such as Achieving budget-optimality with adaptive schemes in crowdsourcing arXiv:1602.03481 and  A Permutation-based Model for Crowd Labeling: Optimal Estimation and Robustness arXiv:1606.09632) go beyond restricting workers to have a common confusion matrix for all questions. In this respect, these are better aligned with the realistic scenario where the error in labeling may depend on the closeness to the decision boundary. How do these settings and algorithms relate to the submission?\n\n(6) Page 5: \"Later we will show....\"   Later where? Please provide a reference.\n\n(7) Theorem 3.4, The assumption of existence of experts such that Y^S is a sufficient statistic for Y: For instance, suppose there are 10 experts who all have a 0.999 probability of correctness (assume symmetric confusion matrices) and there are 5 non-experts who have a 0.001 probability of correctness and even if we suppose all are mutually independent given the true label, then does this satisfy this sufficient statistic assumption? This appears to be a very strong assumption, but perhaps the authors have better intuition?\n\n(8) The experiments comprise only some simulations. The main point of experiments (particularly in the absence of any theoretical results) towards bolstering the paper is to ensure that the assumptions are at least somewhat reasonable. I believe there are several datasets collected from Amazon Mechanical Turk available online? Otherwise, would it be possible to run realistic experiments on some crowdsourcing platforms?\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper287/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Max-MIG: an Information Theoretic Approach for Joint Learning from Crowds", "abstract": "Eliciting labels from crowds is a potential way to obtain large labeled data. Despite a variety of methods developed for learning from crowds, a key challenge remains unsolved: \\emph{learning from crowds without knowing the information structure among the crowds a priori, when some people of the crowds make highly correlated mistakes and some of them label effortlessly (e.g. randomly)}. We propose an information theoretic approach, Max-MIG, for joint learning from crowds, with a common assumption: the crowdsourced labels and the data are independent conditioning on the ground truth. Max-MIG simultaneously aggregates the crowdsourced labels and learns an accurate data classifier. Furthermore, we devise an accurate data-crowds forecaster that employs both the data and the crowdsourced labels to forecast the ground truth. To the best of our knowledge, this is the first algorithm that solves the aforementioned challenge of learning from crowds. In addition to the theoretical validation, we also empirically show that our algorithm achieves the new state-of-the-art results in most settings, including the real-world data, and is the first algorithm that is robust to various information structures. Codes are available at https://github.com/Newbeeer/Max-MIG .\n", "keywords": ["crowdsourcing", "information theory"], "authorids": ["caopeng2016@pku.edu.cn", "xuyilun@pku.edu.cn", "yuqing.kong@pku.edu.cn", "yizhou.wang@pku.edu.cn"], "authors": ["Peng Cao*", "Yilun Xu*", "Yuqing Kong", "Yizhou  Wang"], "pdf": "/pdf/6d826c78b6d8b90396fcb156ac1799e0ffe56737.pdf", "paperhash": "cao|maxmig_an_information_theoretic_approach_for_joint_learning_from_crowds", "_bibtex": "@inproceedings{\ncao2018maxmig,\ntitle={Max-{MIG}: an Information Theoretic Approach for Joint Learning from Crowds},\nauthor={Peng Cao and Yilun Xu and Yuqing Kong and Yizhou  Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJg9DoR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper287/Official_Review", "cdate": 1542234496125, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJg9DoR9t7", "replyto": "BJg9DoR9t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper287/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335690606, "tmdate": 1552335690606, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper287/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1g5uv1N07", "original": null, "number": 9, "cdate": 1542874994088, "ddate": null, "tcdate": 1542874994088, "tmdate": 1542877115807, "tddate": null, "forum": "BJg9DoR9t7", "replyto": "rygf8BMF37", "invitation": "ICLR.cc/2019/Conference/-/Paper287/Official_Comment", "content": {"title": "The training procedure of Max-MIG", "comment": "The training procedure of Max-MIG are illustrated below.\n\nDenote data classifier as h and crowds aggregator as g. The parameters of h (resp. g) is \\theta_h (resp. \\theta_g); the learning rate of h(resp. g) is \\alpha_h (resp. \\alpha_g). The crowdsourced dataset is denoted as D. (X, Y^{M}) pair denotes a batch of images and their corresponding crowdsourced labels Y^{M} from M experts. We tune the prior p as a hyperparameter.\n\nThe implementation details, such as batch size, learning rate and network architecture for different datasets, are illustrated in Page 12-13 in our paper.\n\nStep 1: Initialization for the experts' parameters in the crowds aggregator   (Please refer to Page 13 in our paper for more details)\n\nStep 2 :\n\n    For t in 1, 2, ..., T\n\n\t    sample mini-batch (X, Y^{M}))  from D\n\n\t    left_output = h(X)\n\n\t    right_output= g(Y^{M})\n\n\t    Loss = - MIG(left_output,right_output,p)   (Please refer to Page 5 in our paper for more details)\n\n\t    \\theta_h = \\theta_h - \\alpha_h * \\nabla{\\theta_h}Loss\n\n\t    \\theta_g = \\theta_g - \\alpha_g * \\nabla{\\theta_g}Loss\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper287/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper287/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper287/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Max-MIG: an Information Theoretic Approach for Joint Learning from Crowds", "abstract": "Eliciting labels from crowds is a potential way to obtain large labeled data. Despite a variety of methods developed for learning from crowds, a key challenge remains unsolved: \\emph{learning from crowds without knowing the information structure among the crowds a priori, when some people of the crowds make highly correlated mistakes and some of them label effortlessly (e.g. randomly)}. We propose an information theoretic approach, Max-MIG, for joint learning from crowds, with a common assumption: the crowdsourced labels and the data are independent conditioning on the ground truth. Max-MIG simultaneously aggregates the crowdsourced labels and learns an accurate data classifier. Furthermore, we devise an accurate data-crowds forecaster that employs both the data and the crowdsourced labels to forecast the ground truth. To the best of our knowledge, this is the first algorithm that solves the aforementioned challenge of learning from crowds. In addition to the theoretical validation, we also empirically show that our algorithm achieves the new state-of-the-art results in most settings, including the real-world data, and is the first algorithm that is robust to various information structures. Codes are available at https://github.com/Newbeeer/Max-MIG .\n", "keywords": ["crowdsourcing", "information theory"], "authorids": ["caopeng2016@pku.edu.cn", "xuyilun@pku.edu.cn", "yuqing.kong@pku.edu.cn", "yizhou.wang@pku.edu.cn"], "authors": ["Peng Cao*", "Yilun Xu*", "Yuqing Kong", "Yizhou  Wang"], "pdf": "/pdf/6d826c78b6d8b90396fcb156ac1799e0ffe56737.pdf", "paperhash": "cao|maxmig_an_information_theoretic_approach_for_joint_learning_from_crowds", "_bibtex": "@inproceedings{\ncao2018maxmig,\ntitle={Max-{MIG}: an Information Theoretic Approach for Joint Learning from Crowds},\nauthor={Peng Cao and Yilun Xu and Yuqing Kong and Yizhou  Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJg9DoR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper287/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618384, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJg9DoR9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper287/Authors", "ICLR.cc/2019/Conference/Paper287/Reviewers", "ICLR.cc/2019/Conference/Paper287/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper287/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper287/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper287/Authors|ICLR.cc/2019/Conference/Paper287/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper287/Reviewers", "ICLR.cc/2019/Conference/Paper287/Authors", "ICLR.cc/2019/Conference/Paper287/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618384}}}, {"id": "B1xLvvU0pQ", "original": null, "number": 8, "cdate": 1542510429771, "ddate": null, "tcdate": 1542510429771, "tmdate": 1542510429771, "tddate": null, "forum": "BJg9DoR9t7", "replyto": "B1e5ewz9pQ", "invitation": "ICLR.cc/2019/Conference/-/Paper287/Official_Comment", "content": {"title": "Reply", "comment": "Thank you for your questions.\n\nQ: My question is about Theorem 3.4. I repeat my question: Does the example in my question satisfy the sufficient statistic condition? If so, then is there an easy way to see that it does?\nA: Yes. The short explanation is: in your example, we can make the set of senior experts S consist of both the experts and the non-experts. \n\nThe long explanation is: in the example of your question, all experts are mutually independent conditioning on the ground truth. All independent mistakes cases satisfy SSC since (1) SSC requires that there EXISTs a subset of experts S (we call them senior experts), whose identities are unknown, such that the experts in S have mutually independent labeling biases and it is sufficient to only use the experts in S\u2019 information to predict the ground truth label; (2) in the independent mistakes case, we can make S = M, where M is the set of all experts. \n\nQ: Unfortunately, this apparently important requirement is quite hidden within all notation etc. In the revision, please clarify the meaning and implications of this condition (in the main text or appendix).\nA: Thanks for your suggestion. In addition to the explanation of this requirement in the last paragraph of our intro, in our revised version, we have also clarified it after the statement of our main theorem.  \n\nQ:  The simple setting of (5), which is highly prevalent in practice, does NOT satisfy the \"information intersection\" assumption. I am fine with this assumption since it appears often in earlier works in crowdsourced labeling but the paper needs to be very clear in the benefits as well as the limitations of this assumption. In the revision, please make a very careful comparison of pros and cons with respect to the references in (3) and (5).\n\nA: The \"information intersection\" assumption involves both the crowdsourced labels and the datapoints while both settings of (5),  Khetan & Oh 2016 and Shah et al. 2016, are for pure crowdsourcing methods. Thus, we assume that this question means the settings of (5), i.e. Khetan & Oh 2016 and Shah et al. 2016, do not use the original Dawid-Skene model to model the experts while our crowdsourcing part uses the original Dawid-Skene model to model the experts. \n\nKhetan & Oh 2016 and Shah et al. 2016 employ the generalized Dawid-Skene model, which considers the task difficulty, while we do not as we use the original Dawid-Skene model to model the experts. However, by employing the information from the datapoints, our results are robust to the correlated mistakes cases while they do not. We agree that combining the generalized Dawid-Skene model with our Max-MIG framework is an important future direction to explore. We will add this comparison, and the comparison with (3) in our revised version. \n\n\nKhetan, Ashish, and Sewoong Oh. \"Achieving budget-optimality with adaptive schemes in crowdsourcing.\" Advances in Neural Information Processing Systems. 2016.\n\nShah, Nihar B., Sivaraman Balakrishnan, and Martin J. Wainwright. \"A permutation-based model for crowd labeling: Optimal estimation and robustness.\" arXiv preprint arXiv:1606.09632 (2016)."}, "signatures": ["ICLR.cc/2019/Conference/Paper287/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper287/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper287/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Max-MIG: an Information Theoretic Approach for Joint Learning from Crowds", "abstract": "Eliciting labels from crowds is a potential way to obtain large labeled data. Despite a variety of methods developed for learning from crowds, a key challenge remains unsolved: \\emph{learning from crowds without knowing the information structure among the crowds a priori, when some people of the crowds make highly correlated mistakes and some of them label effortlessly (e.g. randomly)}. We propose an information theoretic approach, Max-MIG, for joint learning from crowds, with a common assumption: the crowdsourced labels and the data are independent conditioning on the ground truth. Max-MIG simultaneously aggregates the crowdsourced labels and learns an accurate data classifier. Furthermore, we devise an accurate data-crowds forecaster that employs both the data and the crowdsourced labels to forecast the ground truth. To the best of our knowledge, this is the first algorithm that solves the aforementioned challenge of learning from crowds. In addition to the theoretical validation, we also empirically show that our algorithm achieves the new state-of-the-art results in most settings, including the real-world data, and is the first algorithm that is robust to various information structures. Codes are available at https://github.com/Newbeeer/Max-MIG .\n", "keywords": ["crowdsourcing", "information theory"], "authorids": ["caopeng2016@pku.edu.cn", "xuyilun@pku.edu.cn", "yuqing.kong@pku.edu.cn", "yizhou.wang@pku.edu.cn"], "authors": ["Peng Cao*", "Yilun Xu*", "Yuqing Kong", "Yizhou  Wang"], "pdf": "/pdf/6d826c78b6d8b90396fcb156ac1799e0ffe56737.pdf", "paperhash": "cao|maxmig_an_information_theoretic_approach_for_joint_learning_from_crowds", "_bibtex": "@inproceedings{\ncao2018maxmig,\ntitle={Max-{MIG}: an Information Theoretic Approach for Joint Learning from Crowds},\nauthor={Peng Cao and Yilun Xu and Yuqing Kong and Yizhou  Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJg9DoR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper287/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618384, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJg9DoR9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper287/Authors", "ICLR.cc/2019/Conference/Paper287/Reviewers", "ICLR.cc/2019/Conference/Paper287/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper287/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper287/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper287/Authors|ICLR.cc/2019/Conference/Paper287/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper287/Reviewers", "ICLR.cc/2019/Conference/Paper287/Authors", "ICLR.cc/2019/Conference/Paper287/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618384}}}, {"id": "B1e5ewz9pQ", "original": null, "number": 7, "cdate": 1542231793735, "ddate": null, "tcdate": 1542231793735, "tmdate": 1542231793735, "tddate": null, "forum": "BJg9DoR9t7", "replyto": "SJg2zPgOpQ", "invitation": "ICLR.cc/2019/Conference/-/Paper287/Official_Comment", "content": {"title": "Follow-up questions", "comment": "Thank you for your response. The experimental results are indeed very positive. I have two follow-up comments:\n\n- Regarding earlier question (7): My question is about Theorem 3.4. I repeat my question: Does the example in my question satisfy the sufficient statistic condition? If so, then is there an easy way to see that it does? If not, then is the main theorem missing a very important case (and perhaps calls for the later proposition to be brought into the main text)? If not, also then what is the implication and/or meaning of this sufficient statistic condition? Unfortunately, this apparently important requirement is quite hidden within all notation etc. In the revision, please clarify the meaning and implications of this condition (in the main text or appendix).\n\n- Regarding earlier questions (1), (3), (5): The simple setting of (5), which is highly prevalent in practice, does NOT satisfy the \"information intersection\" assumption. I am fine with this assumption since it appears often in earlier works in crowdsourced labeling but the paper needs to be very clear in the benefits as well as the limitations of this assumption. In the revision, please make a very careful comparison of pros and cons with respect to the references in (3) and (5).\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper287/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper287/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper287/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Max-MIG: an Information Theoretic Approach for Joint Learning from Crowds", "abstract": "Eliciting labels from crowds is a potential way to obtain large labeled data. Despite a variety of methods developed for learning from crowds, a key challenge remains unsolved: \\emph{learning from crowds without knowing the information structure among the crowds a priori, when some people of the crowds make highly correlated mistakes and some of them label effortlessly (e.g. randomly)}. We propose an information theoretic approach, Max-MIG, for joint learning from crowds, with a common assumption: the crowdsourced labels and the data are independent conditioning on the ground truth. Max-MIG simultaneously aggregates the crowdsourced labels and learns an accurate data classifier. Furthermore, we devise an accurate data-crowds forecaster that employs both the data and the crowdsourced labels to forecast the ground truth. To the best of our knowledge, this is the first algorithm that solves the aforementioned challenge of learning from crowds. In addition to the theoretical validation, we also empirically show that our algorithm achieves the new state-of-the-art results in most settings, including the real-world data, and is the first algorithm that is robust to various information structures. Codes are available at https://github.com/Newbeeer/Max-MIG .\n", "keywords": ["crowdsourcing", "information theory"], "authorids": ["caopeng2016@pku.edu.cn", "xuyilun@pku.edu.cn", "yuqing.kong@pku.edu.cn", "yizhou.wang@pku.edu.cn"], "authors": ["Peng Cao*", "Yilun Xu*", "Yuqing Kong", "Yizhou  Wang"], "pdf": "/pdf/6d826c78b6d8b90396fcb156ac1799e0ffe56737.pdf", "paperhash": "cao|maxmig_an_information_theoretic_approach_for_joint_learning_from_crowds", "_bibtex": "@inproceedings{\ncao2018maxmig,\ntitle={Max-{MIG}: an Information Theoretic Approach for Joint Learning from Crowds},\nauthor={Peng Cao and Yilun Xu and Yuqing Kong and Yizhou  Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJg9DoR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper287/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618384, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJg9DoR9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper287/Authors", "ICLR.cc/2019/Conference/Paper287/Reviewers", "ICLR.cc/2019/Conference/Paper287/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper287/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper287/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper287/Authors|ICLR.cc/2019/Conference/Paper287/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper287/Reviewers", "ICLR.cc/2019/Conference/Paper287/Authors", "ICLR.cc/2019/Conference/Paper287/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618384}}}, {"id": "Syxqx4gdp7", "original": null, "number": 5, "cdate": 1542091762178, "ddate": null, "tcdate": 1542091762178, "tmdate": 1542171862286, "tddate": null, "forum": "BJg9DoR9t7", "replyto": "rygf8BMF37", "invitation": "ICLR.cc/2019/Conference/-/Paper287/Official_Comment", "content": {"title": "Thank you for your careful review.", "comment": "Thanks for your comments and questions. We will release our code after review. \n\nQ: >>Is MIG set as a loss function for the data classifier NN? Is crowd aggregator trained also as an NN with MIG as a loss function? \nA: <<We train the data classifier and the crowd aggregator together using -MIG(data classifier, crowd aggregator,p) as the loss function, i.e. they share the loss function. \n\nQ: >>How do the authors find the optimal p? \nA: <<We tune p as a hyperparameter and to maximize MIG(data classifier, crowd aggregator,p). \n\nQ: >>Also, in order all the provided theory to work all the found data classifier NN, the aggregator and p should be exact maximisers of MIG as far as I understand. How do the author ensure that they find the exact maximisers? \nA: <<We are not sure we understand this question. If this question asks about the robustness our algorithm, then our empirical results show that our algorithm is robust. \n\n\nQ: >>Also related to understanding how training works: on p.15 the authors claim \u201cNote that our method can handle this simple correlated mistakes case and will give all useless experts weight zero based on Theorem 3.4.\u201d I have trouble understanding why the proposed method should find these zero weights rather than it is just able to find them?\nA: <<Theoretically, our method should and is able to give the useless experts weight zero. In the simple correlated mistakes case, the best crowd aggregator gives the only useful expert all weight and other useless experts zero weights. During the training process, in order to maximize the mutual information between the classifier and aggregator, the SGD process of our algorithm will increase the weights of the useful experts and decrease the weights of the useless experts to a relatively small number, such that the trained aggregator approximates the best crowd aggregator. We will clarify it in our final version.\n\nQ: >>Real crowdsourced data\nA: <<Thanks for your suggestion. See our top comments. \n\nQ: >>Compare our data-crowd forecaster with AggNet\nA: <<Thanks for your suggestion. We compared our data-crowd forecaster with AggNet. The results still match our theory. When there are no correlated mistakes, we outperform AggNet or have very similar performances. When there are correlated mistakes, we outperform AggNet a lot (e.g. +30%). We have revised our paper and added this result. \n\nQ: >>Compare our crowd aggregator with pure crowdsourcing methods (Majority Voting or, preferably, more advanced pure crowdsourcing aggregators)\nA: <<This is still an unfair comparison. Although our crowd aggregator only takes the crowdsourced labels as input, the training process of our crowd aggregator incorporated the information from the images. \n\nQ: >>Does Max-MIG handle missing crowd labels for some data points? Did the author use missing labels in the experiments?\nA: <<The LabelMe data is in the missing label setting, the empirical results (our top comments) show that our algorithm handles this setting. \n\nQ: >> Both the Dogs vs. Cats and CIFAR-10 datasets have more or less balanced data, i.e., the number of data points belonging to each ground truth class is similar between classes. Is this true for the LUNA16 dataset? If yes, have the authors tried their method with heavily imbalanced data? In my experience, some crowdsourcing methods may suffer with imbalanced data, for example, Crowd Layer does so on some data. This tendency of Crowd Layer is kind of confirmed on the provided Dogs vs. Cats in the na\u00efve majority case, where based on crowd labels the first class dominates the second.\nA: <<LUNA16 is highly imbalanced (85%, 15%). We will clarify it in our final version. \n\nWe thank you for your careful review and will follow your suggestions on our writings and fix the typos. "}, "signatures": ["ICLR.cc/2019/Conference/Paper287/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper287/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper287/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Max-MIG: an Information Theoretic Approach for Joint Learning from Crowds", "abstract": "Eliciting labels from crowds is a potential way to obtain large labeled data. Despite a variety of methods developed for learning from crowds, a key challenge remains unsolved: \\emph{learning from crowds without knowing the information structure among the crowds a priori, when some people of the crowds make highly correlated mistakes and some of them label effortlessly (e.g. randomly)}. We propose an information theoretic approach, Max-MIG, for joint learning from crowds, with a common assumption: the crowdsourced labels and the data are independent conditioning on the ground truth. Max-MIG simultaneously aggregates the crowdsourced labels and learns an accurate data classifier. Furthermore, we devise an accurate data-crowds forecaster that employs both the data and the crowdsourced labels to forecast the ground truth. To the best of our knowledge, this is the first algorithm that solves the aforementioned challenge of learning from crowds. In addition to the theoretical validation, we also empirically show that our algorithm achieves the new state-of-the-art results in most settings, including the real-world data, and is the first algorithm that is robust to various information structures. Codes are available at https://github.com/Newbeeer/Max-MIG .\n", "keywords": ["crowdsourcing", "information theory"], "authorids": ["caopeng2016@pku.edu.cn", "xuyilun@pku.edu.cn", "yuqing.kong@pku.edu.cn", "yizhou.wang@pku.edu.cn"], "authors": ["Peng Cao*", "Yilun Xu*", "Yuqing Kong", "Yizhou  Wang"], "pdf": "/pdf/6d826c78b6d8b90396fcb156ac1799e0ffe56737.pdf", "paperhash": "cao|maxmig_an_information_theoretic_approach_for_joint_learning_from_crowds", "_bibtex": "@inproceedings{\ncao2018maxmig,\ntitle={Max-{MIG}: an Information Theoretic Approach for Joint Learning from Crowds},\nauthor={Peng Cao and Yilun Xu and Yuqing Kong and Yizhou  Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJg9DoR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper287/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618384, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJg9DoR9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper287/Authors", "ICLR.cc/2019/Conference/Paper287/Reviewers", "ICLR.cc/2019/Conference/Paper287/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper287/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper287/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper287/Authors|ICLR.cc/2019/Conference/Paper287/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper287/Reviewers", "ICLR.cc/2019/Conference/Paper287/Authors", "ICLR.cc/2019/Conference/Paper287/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618384}}}, {"id": "SJg2zPgOpQ", "original": null, "number": 6, "cdate": 1542092564448, "ddate": null, "tcdate": 1542092564448, "tmdate": 1542092564448, "tddate": null, "forum": "BJg9DoR9t7", "replyto": "BklLgntDhX", "invitation": "ICLR.cc/2019/Conference/-/Paper287/Official_Comment", "content": {"title": "Thanks for your review and here are our clarifications. ", "comment": "Thank you for your review and comments. \n\nQ: >>As the authors clarified, one key aspect of the \"information intersection\" assumption is that the crowdsourced labels are statistically independent from the data when conditioned on the ground truth. How strongly does this coincide with reality? Since the work is primary empirical, is there any evidence on this front?\n\nA: <<1) Let's consider the case where we ask the turkers to label \"dogs vs cats\". This assumption says that the turkers' labels are the noisy version of the ground truth class and the noise is independent with other aspects of the images (e.g. the image scene is indoor or outdoor). When the assumption is violated in the sense that the turkers' noises are highly correlated with other aspects of the images (e.g. the image scene is indoor or outdoor), without other assumptions, no algorithm can train a classifier here to avoid the influence of the ``indoor or outdoor'' information. \n2)This assumption is commonly used in most crowd-learning literature  ( Dawid & Skene (1979), Raykar et al. (2010),  Albarqouni et al. (2016),  Guan et al. (2017) , Rodrigues & Pereira (2017) ).\n\nQ: >>is this the first algorithm towards this goal? \nA: <<It's not the first algorithm to ``joint'' learn (Raykar et al. 2010 is the first). It is the first algorithm that is robust to various information structures theoretically and experimentally. Learning From Noisy Singly-labeled Data is not robust to correlated mistakes (see following detailed comparison).  \n\nQ: >>The submitted paper misses an extremely relevant piece of literature: \"Learning From Noisy Singly-labeled Data\" (arXiv:1712.04577). This paper also aims to solve the label + features problem together. How do the results of this paper compare to that of this submission?\nA: <<Thanks for your information. We will cite this ICLR 18 paper. Theoretically, this paper still requires the experts to be mutually conditional independent while we do not. Empirically, we tested this method on LabelMe data which has the real Amazon MTurk crowd labels and our method still outperforms this method: Max-MIG 86.42 +/- 0.36,  MBEM(ICLR 18) 81.24 +/- 1.60.\n\nQ: >>\"Model and assumptions\" Is the i.i.d. assumption across the values of \"i\"? Then does that not violate the earlier claim of accommodating correlated mistakes?\nA: <<It means {\uff08x_1,y_1^1,...,y_1^M),\uff08x_2,y_2^1,...,y_2^M),...} =\uff08x_i,y_i^1,...,y_i^M)_i=are i.i.d. samples of the joint random variables (X,Y^1,....,Y^M). A non-i.i.d example is that all\uff08x_i,y_i^1,...,y_i^M)_i are the same. Experts make correlated mistakes means the random variables Y^1,...,Y^M are correlated even conditioning on the ground truth. There is no contradiction here. \n\nQ: >>Recent papers on crowdsourcing (such as Achieving budget-optimality with adaptive schemes in crowdsourcing arXiv:1602.03481 and  A Permutation-based Model for Crowd Labeling: Optimal Estimation and Robustness arXiv:1606.09632) go beyond restricting workers to have a common confusion matrix for all questions. In this respect, these are better aligned with the realistic scenario where the error in labeling may depend on the closeness to the decision boundary. How do these settings and algorithms relate to the submission?\nA: <<Thanks for your information. One possible direction is making both the ground truth and image difficulty as the information intersection and finds them. We agree that taking account of image difficulty is an interesting direction to explore in future work and we will try to combine our framework with relevant papers in the future. \n\nQ: >>Page 5: \"Later we will show....\"   Later where? Please provide a reference.\nA: <<The formal statement is in Appendix C, Theorem 3.4 (this is a detailed statement compared with the Theorem 3.4 in the main body). We will clarify it in our revised paper. \n\nQ: >>Theorem 3.4, The assumption of existence of experts such that Y^S is a sufficient statistic for Y: For instance, suppose there are 10 experts who all have a 0.999 probability of correctness (assume symmetric confusion matrices) and there are 5 non-experts who have a 0.001 probability of correctness and even if we suppose all are mutually independent given the true label, then does this satisfy this sufficient statistic assumption? This appears to be a very strong assumption, but perhaps the authors have better intuition?\nA: <<1) Our algorithm can handle all mutually independent cases, which includes the above .999 example since the mutually independent case satisfies our assumption automatically where all experts can be seen as senior experts (see Proposition C.3. for detail). We will clarify this in our revised paper. 2)We test our algorithm in real data (see our top comments) and the results show that our algorithm is robust to the real case. \n\n\nQ: >>Realistic experiments on some crowdsourcing platforms?\nA: <<See our top comments. "}, "signatures": ["ICLR.cc/2019/Conference/Paper287/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper287/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper287/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Max-MIG: an Information Theoretic Approach for Joint Learning from Crowds", "abstract": "Eliciting labels from crowds is a potential way to obtain large labeled data. Despite a variety of methods developed for learning from crowds, a key challenge remains unsolved: \\emph{learning from crowds without knowing the information structure among the crowds a priori, when some people of the crowds make highly correlated mistakes and some of them label effortlessly (e.g. randomly)}. We propose an information theoretic approach, Max-MIG, for joint learning from crowds, with a common assumption: the crowdsourced labels and the data are independent conditioning on the ground truth. Max-MIG simultaneously aggregates the crowdsourced labels and learns an accurate data classifier. Furthermore, we devise an accurate data-crowds forecaster that employs both the data and the crowdsourced labels to forecast the ground truth. To the best of our knowledge, this is the first algorithm that solves the aforementioned challenge of learning from crowds. In addition to the theoretical validation, we also empirically show that our algorithm achieves the new state-of-the-art results in most settings, including the real-world data, and is the first algorithm that is robust to various information structures. Codes are available at https://github.com/Newbeeer/Max-MIG .\n", "keywords": ["crowdsourcing", "information theory"], "authorids": ["caopeng2016@pku.edu.cn", "xuyilun@pku.edu.cn", "yuqing.kong@pku.edu.cn", "yizhou.wang@pku.edu.cn"], "authors": ["Peng Cao*", "Yilun Xu*", "Yuqing Kong", "Yizhou  Wang"], "pdf": "/pdf/6d826c78b6d8b90396fcb156ac1799e0ffe56737.pdf", "paperhash": "cao|maxmig_an_information_theoretic_approach_for_joint_learning_from_crowds", "_bibtex": "@inproceedings{\ncao2018maxmig,\ntitle={Max-{MIG}: an Information Theoretic Approach for Joint Learning from Crowds},\nauthor={Peng Cao and Yilun Xu and Yuqing Kong and Yizhou  Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJg9DoR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper287/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618384, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJg9DoR9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper287/Authors", "ICLR.cc/2019/Conference/Paper287/Reviewers", "ICLR.cc/2019/Conference/Paper287/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper287/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper287/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper287/Authors|ICLR.cc/2019/Conference/Paper287/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper287/Reviewers", "ICLR.cc/2019/Conference/Paper287/Authors", "ICLR.cc/2019/Conference/Paper287/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618384}}}, {"id": "SkgHhklOTQ", "original": null, "number": 3, "cdate": 1542090669324, "ddate": null, "tcdate": 1542090669324, "tmdate": 1542091836897, "tddate": null, "forum": "BJg9DoR9t7", "replyto": "BJg9DoR9t7", "invitation": "ICLR.cc/2019/Conference/-/Paper287/Official_Comment", "content": {"title": "Our method achieves the state-of-the-art results on real-world dataset, LabelMe", "comment": "To all reviewers:\n\nThank you all for suggesting experiments on real Amazon MTurk data. We follow the second reviewer's suggestion and run our algorithm using LabelMe data, which has the real Amazon MTurk crowd labels (it is also the missing label setting with 59 annotators and each image was labeled by an average of 2.547 workers), from Rodrigues and Pereira (2017). We also achieve the state of art in this real data case. Here is the result: Max-Mig 86.42 +/- 0.36, Majority vote 80.41 +/- 0.56, Crowd Layer 83.65 +/- 0.50, Doctor Net 80.56 +/- 0.59, AggNet 85.20 +/- 0.26. \n\nWe have revised our paper and added this result. We will also release our code after review. "}, "signatures": ["ICLR.cc/2019/Conference/Paper287/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper287/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper287/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Max-MIG: an Information Theoretic Approach for Joint Learning from Crowds", "abstract": "Eliciting labels from crowds is a potential way to obtain large labeled data. Despite a variety of methods developed for learning from crowds, a key challenge remains unsolved: \\emph{learning from crowds without knowing the information structure among the crowds a priori, when some people of the crowds make highly correlated mistakes and some of them label effortlessly (e.g. randomly)}. We propose an information theoretic approach, Max-MIG, for joint learning from crowds, with a common assumption: the crowdsourced labels and the data are independent conditioning on the ground truth. Max-MIG simultaneously aggregates the crowdsourced labels and learns an accurate data classifier. Furthermore, we devise an accurate data-crowds forecaster that employs both the data and the crowdsourced labels to forecast the ground truth. To the best of our knowledge, this is the first algorithm that solves the aforementioned challenge of learning from crowds. In addition to the theoretical validation, we also empirically show that our algorithm achieves the new state-of-the-art results in most settings, including the real-world data, and is the first algorithm that is robust to various information structures. Codes are available at https://github.com/Newbeeer/Max-MIG .\n", "keywords": ["crowdsourcing", "information theory"], "authorids": ["caopeng2016@pku.edu.cn", "xuyilun@pku.edu.cn", "yuqing.kong@pku.edu.cn", "yizhou.wang@pku.edu.cn"], "authors": ["Peng Cao*", "Yilun Xu*", "Yuqing Kong", "Yizhou  Wang"], "pdf": "/pdf/6d826c78b6d8b90396fcb156ac1799e0ffe56737.pdf", "paperhash": "cao|maxmig_an_information_theoretic_approach_for_joint_learning_from_crowds", "_bibtex": "@inproceedings{\ncao2018maxmig,\ntitle={Max-{MIG}: an Information Theoretic Approach for Joint Learning from Crowds},\nauthor={Peng Cao and Yilun Xu and Yuqing Kong and Yizhou  Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJg9DoR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper287/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618384, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJg9DoR9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper287/Authors", "ICLR.cc/2019/Conference/Paper287/Reviewers", "ICLR.cc/2019/Conference/Paper287/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper287/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper287/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper287/Authors|ICLR.cc/2019/Conference/Paper287/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper287/Reviewers", "ICLR.cc/2019/Conference/Paper287/Authors", "ICLR.cc/2019/Conference/Paper287/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618384}}}, {"id": "HylQebguTX", "original": null, "number": 4, "cdate": 1542090986980, "ddate": null, "tcdate": 1542090986980, "tmdate": 1542090986980, "tddate": null, "forum": "BJg9DoR9t7", "replyto": "HygvOxIC3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper287/Official_Comment", "content": {"title": "Our algorithm learns from crowds without knowing the information structure among the crowds a priori.", "comment": "Thanks for your comments and questions. There might be some misunderstanding and we want to clarify it here: our algorithm is not ad hoc and it is independent of any prior knowledge about the information structures and identities of the senior/junior expert. i.e. our algorithm learns from crowds without knowing the information structure among the crowds a priori.\n\nQ: >>If the labels are collected from an unknown setup (e.g. on AMT), where it is hard to establish the dependency structure of the experts, how can we use such approaches effectively? \n>>So it is not surprising that the proposed approach outperforms other approaches. It's also interesting to see that AggNet isn't that bad in general compared to the proposed approach (except on LUNA16). What if we combine all experts in one setting and apply the proposed approach without prior knowledge of who are senior/junior?\n\nA: <<Our algorithm does not need to know the dependency structure of the experts nor the identity of the senior or junior experts. In detail, our algorithm's input is (datapoints and crowdsourced labels) and the initialization is also independent of the dependency structure of the experts nor the identity of the senior or junior experts.\n\nQ: >>(Top cons)Hard-to-check assumption for Theorem 3.4 for real-world problems, on the sufficiency of senior expert's info to predict the true class label\n>>Even if there exists a clear line between senior/junior experts in the labeling process, how do we know or check that the senior experts' opinion can sufficiently estimate the true labels? \nA: <<To implement our algorithm, we also do not need to check the sufficient statistic assumption. \n\nQ: >>(Top cons)Fairly strong assumption on the existence of mutually independent senior experts in the labeling process\nA: <<1)The general MAX-MIG framework does not need this assumption and the assumption can be relaxed by providing a more complicated aggregator model (see the last paragraph of the conclusion section). We agree that this can be an interesting direction to explore in future work. 2) Our results on real data show that our current implementation of MAX-MIG, with weighted average aggregator model, is still robust to the real situation empirically (see our top comments). \n\nQ: >>Did you require all experts to label ALL the data points or only a subset of training data points? \nA: <<The LabelMe data is in the missing label setting, the empirical results (our top comments) show that our algorithm handles this setting. \n\nQ: >>I don't believe \"Naive majority\" is an interesting setting - we can easily detect those junior experts that always label cases with one class, and remove these experts from the system, in practice. \nA: <<In our experiments, we make the naive majority always label 1 to show that other methods (e.g. majority vote) cannot handle this setting. However, in fact, if the naive majority label 1 with prob 0.9 and 0 with prob 0.1, we cannot easily remove them and other methods still cannot handle this setting, while our algorithm will not be affected by these kinds of naive majority based on our theory.  \n\nQ: >>The term ``early''\nA: <<We will revise this term. "}, "signatures": ["ICLR.cc/2019/Conference/Paper287/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper287/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper287/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Max-MIG: an Information Theoretic Approach for Joint Learning from Crowds", "abstract": "Eliciting labels from crowds is a potential way to obtain large labeled data. Despite a variety of methods developed for learning from crowds, a key challenge remains unsolved: \\emph{learning from crowds without knowing the information structure among the crowds a priori, when some people of the crowds make highly correlated mistakes and some of them label effortlessly (e.g. randomly)}. We propose an information theoretic approach, Max-MIG, for joint learning from crowds, with a common assumption: the crowdsourced labels and the data are independent conditioning on the ground truth. Max-MIG simultaneously aggregates the crowdsourced labels and learns an accurate data classifier. Furthermore, we devise an accurate data-crowds forecaster that employs both the data and the crowdsourced labels to forecast the ground truth. To the best of our knowledge, this is the first algorithm that solves the aforementioned challenge of learning from crowds. In addition to the theoretical validation, we also empirically show that our algorithm achieves the new state-of-the-art results in most settings, including the real-world data, and is the first algorithm that is robust to various information structures. Codes are available at https://github.com/Newbeeer/Max-MIG .\n", "keywords": ["crowdsourcing", "information theory"], "authorids": ["caopeng2016@pku.edu.cn", "xuyilun@pku.edu.cn", "yuqing.kong@pku.edu.cn", "yizhou.wang@pku.edu.cn"], "authors": ["Peng Cao*", "Yilun Xu*", "Yuqing Kong", "Yizhou  Wang"], "pdf": "/pdf/6d826c78b6d8b90396fcb156ac1799e0ffe56737.pdf", "paperhash": "cao|maxmig_an_information_theoretic_approach_for_joint_learning_from_crowds", "_bibtex": "@inproceedings{\ncao2018maxmig,\ntitle={Max-{MIG}: an Information Theoretic Approach for Joint Learning from Crowds},\nauthor={Peng Cao and Yilun Xu and Yuqing Kong and Yizhou  Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJg9DoR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper287/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618384, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJg9DoR9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper287/Authors", "ICLR.cc/2019/Conference/Paper287/Reviewers", "ICLR.cc/2019/Conference/Paper287/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper287/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper287/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper287/Authors|ICLR.cc/2019/Conference/Paper287/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper287/Reviewers", "ICLR.cc/2019/Conference/Paper287/Authors", "ICLR.cc/2019/Conference/Paper287/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618384}}}, {"id": "HygvOxIC3Q", "original": null, "number": 3, "cdate": 1541460078698, "ddate": null, "tcdate": 1541460078698, "tmdate": 1541534122997, "tddate": null, "forum": "BJg9DoR9t7", "replyto": "BJg9DoR9t7", "invitation": "ICLR.cc/2019/Conference/-/Paper287/Official_Review", "content": {"title": "Generally well-written paper on a new information theoretical approach for learning from crowdsourced data.", "review": "Top pros:\n- Well motivated approach with good examples from clinical setting\n- Sound proof on why information theoretical approach is better than MLE based approaches\n- Experiments on diversified data sets to show their approach's performance, with good implementation details. \n\nTop cons:\n- Fairly strong assumption on the existence of mutually independent senior experts in the labeling process\n- Hard-to-check assumption for Theorem 3.4 for real world problems, on the sufficiency of senior expert's info to predict the true class label\n\nThe paper is in general well written, and builds upon existing work on crowdsourced data mining and co-training. I believe this line of work will benefit the community in taking a more information theoretical approach with relaxed assumptions on the data collection process. My main feedback is how to check the existence of senior experts in real-world applications. In particular,\n- If the labels are collected from an unknown setup (e.g. on AMT), where it is hard to establish the dependency structure of the experts, how can we use such approaches effectively? \n- Even if there exists a clear line between senior/junior experts in the labeling process, how do we know or check that the senior experts' opinion can sufficiently estimate the true labels? \n\nIn the experiment section, the label data was collected with a build-in assumption of senior/junior labelers, and we also know exactly who are senior/junior experts. So it is not surprising that the proposed approach outperforms other approaches. It's also interesting to see that AggNet isn't that bad in general compared to the proposed approach (except on LUNA16). What if we combine all experts in one setting and apply the proposed approach without prior knowledge of who are senior/junior? Also, did you require all experts to label ALL the data points or only a subset of training data points? \n\nMinor points:\n- I don't believe \"Naive majority\" is an interesting setting - we can easily detect those junior experts that always label cases with one class, and remove these experts from the system, in practice. \n- I wouldn't call this an \"early\" algorithm as it indicates it's somewhat pre-mature. Just call this a novel approach that is in the early phase, and more sophisticated approach can be further developed. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper287/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Max-MIG: an Information Theoretic Approach for Joint Learning from Crowds", "abstract": "Eliciting labels from crowds is a potential way to obtain large labeled data. Despite a variety of methods developed for learning from crowds, a key challenge remains unsolved: \\emph{learning from crowds without knowing the information structure among the crowds a priori, when some people of the crowds make highly correlated mistakes and some of them label effortlessly (e.g. randomly)}. We propose an information theoretic approach, Max-MIG, for joint learning from crowds, with a common assumption: the crowdsourced labels and the data are independent conditioning on the ground truth. Max-MIG simultaneously aggregates the crowdsourced labels and learns an accurate data classifier. Furthermore, we devise an accurate data-crowds forecaster that employs both the data and the crowdsourced labels to forecast the ground truth. To the best of our knowledge, this is the first algorithm that solves the aforementioned challenge of learning from crowds. In addition to the theoretical validation, we also empirically show that our algorithm achieves the new state-of-the-art results in most settings, including the real-world data, and is the first algorithm that is robust to various information structures. Codes are available at https://github.com/Newbeeer/Max-MIG .\n", "keywords": ["crowdsourcing", "information theory"], "authorids": ["caopeng2016@pku.edu.cn", "xuyilun@pku.edu.cn", "yuqing.kong@pku.edu.cn", "yizhou.wang@pku.edu.cn"], "authors": ["Peng Cao*", "Yilun Xu*", "Yuqing Kong", "Yizhou  Wang"], "pdf": "/pdf/6d826c78b6d8b90396fcb156ac1799e0ffe56737.pdf", "paperhash": "cao|maxmig_an_information_theoretic_approach_for_joint_learning_from_crowds", "_bibtex": "@inproceedings{\ncao2018maxmig,\ntitle={Max-{MIG}: an Information Theoretic Approach for Joint Learning from Crowds},\nauthor={Peng Cao and Yilun Xu and Yuqing Kong and Yizhou  Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJg9DoR9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper287/Official_Review", "cdate": 1542234496125, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJg9DoR9t7", "replyto": "BJg9DoR9t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper287/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335690606, "tmdate": 1552335690606, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper287/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 13}