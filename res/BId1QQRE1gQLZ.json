{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392730680000, "tcdate": 1392730680000, "number": 1, "id": "kJUPkk5LtOkjZ", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "BId1QQRE1gQLZ", "replyto": "gJViuDG4cAJN1", "signatures": ["Bojan Pepikj"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "We would like to thank reviewer 138a for his valuable comments.\r\n\r\nThe reviewer expresses his concern about one of the main assumptions\r\nin the paper - the use of a separate detector for each subclass and\r\nview. Regarding this, we would like to point out two things:\r\n\r\nFirst, a fair number of state-of-the-art methods for viewpoint\r\nestimation and object localization belongs to this category. For\r\nexample, see references [25,28,34,40], as well as Tab. 1. They are all\r\ndetectors which assume a dedicated detector per viewpoint.\r\n\r\nSecond, our results confirm that having a detector per subclass and\r\nper view (more specific detectors) indeed leads to better performance\r\n(Tab. 3, results for the baseline SVM (KITTI)): the car-type SVM\r\ndetector is consistently better than the base SVM detector. Thus the\r\nusage of more specific detectors is justified. Naturally, as we go\r\nmore specific, the data distribution across viewpoints becomes scarce\r\n- to compensate for that, we successfully employ stronger and\r\nstructured regularization (SVM-Sigma). This is confirmed in Tab. 3:\r\nSVM-Sigma is better than SVM in all comparable settings.\r\n\r\nAnother concern is the usage of prior geometric knowledge. In this\r\nwork we model the geometric structure for a reason. We assume and\r\nbelieve that there is an underlying cause (object geometry) which\r\ndrives the appearance variation and changes of the object. Rather than\r\nletting the method discover this structure, we explicitly model it (to\r\na certain degree) in our work. How one would apply this to deep NNs\r\nis indeed an open and interesting question.\r\n\r\nRegarding the C parameter, for the baseline (SVM) we followed the\r\nsuggestions of [12] and [13] and used the value C = 0.002. For the\r\nproposed method, we ran experiments with varying amount of data per\r\nviewpoint and different values of C and we observed two things:\r\nfirstly, C = 0.002 is optimal value in most of the cases or it is very\r\nclose to the optimal and secondly, the performance is stable in the\r\nrange C*[0.1, 10], therefore we chose the same value for all the\r\nmodels. We are happy to include those results in the supplemental\r\nmaterial. Obviously one should do k-fold cross validation on all the\r\ntunable parameters jointly, but due to the scarce training data, large\r\nsearch space, costly training time and extensive experiments, doing\r\nthe proper k-fold cross validation is time consuming and prohibitive."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-View Priors for Learning Detectors from Sparse Viewpoint Data", "decision": "submitted, no decision", "abstract": "While the majority of today's object class models provide only 2D bounding boxes, far richer output hypotheses are desirable including viewpoint, fine-grained category, and 3D geometry estimate. However, models trained to provide richer output require larger amounts of training data, preferably well covering the relevant aspects such as viewpoint and fine-grained categories. In this paper, we address this issue from the perspective of transfer learning, and design an object class model that explicitly leverages correlations between visual features. Specifically, our model represents prior distributions over permissible multi-view detectors in a parametric way -- the priors are learned once from training data of a source object class, and can later be used to facilitate the learning of a detector for a target class. As we show in our experiments, this transfer is not only beneficial for detectors based on basic-level category representations, but also enables the robust learning of detectors that represent classes at finer levels of granularity, where training data is typically even scarcer and more unbalanced. As a result, we report largely improved performance in simultaneous 2D object localization and viewpoint estimation on a recent dataset of challenging street scenes.", "pdf": "https://arxiv.org/abs/1312.6095", "paperhash": "pepik|multiview_priors_for_learning_detectors_from_sparse_viewpoint_data", "keywords": [], "conflicts": [], "authors": ["Bojan Pepik", "Michael Stark", "Peter Gehler", "Bernt Schiele"], "authorids": ["bojan@mpi-inf.mpg.de", "mst@stanford.edu", "schiele@mpi-inf.mpg.de", "pgehler@tuebingen.mpg.de"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392730560000, "tcdate": 1392730560000, "number": 1, "id": "igPNrjZACmi9Q", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "BId1QQRE1gQLZ", "replyto": "MxgLMMu8Ldj3S", "signatures": ["Bojan Pepikj"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "We would like to thank the reviewer for the valuable comments.  We\r\nuploaded a new paper version and we included the prior work [a].\r\n\r\nRegarding the questions:\r\n\r\n-  Indeed we used the car class from the 3D Object classes dataset to learn\r\n   the 3D object priors that are later on used to train target class\r\n   detectors on KITTI.\r\n\r\n- On both KITTI and 3D Object Classes, we use CAD data to establish the\r\n  pairs of corresponding cells across views which are used in the case\r\n  of SVM-MV.\r\n\r\n- 'Bootstrapping' refers to the method from classical statistics\r\n  where the data is re-sampled multiple times in order to provide an\r\n  estimate of the underlying distribution. Specifically, we train N\r\n  source models by sub-sampling K positive training examples from the\r\n  training set for each of the source models.\r\n\r\n- Regarding the C parameter, for the baseline (SVM) we followed the\r\n  suggestions of [12] and [13] and used the value C = 0.002. For the\r\n  proposed method, we ran experiments with varying amount of data per\r\n  viewpoint and different values of C and we observed two things:\r\n  firstly, C = 0.002 is optimal value in most of the cases or it is\r\n  very close to the optimal and secondly, the performance is stable in\r\n  the range C*[0.1, 10], therefore we chose the same value for all the\r\n  models. We are happy to include those results in the supplemental\r\n  material. Obviously one should do k-fold cross validation on all the\r\n  tunable parameters jointly, but due to the scarce training data,\r\n  large search space, costly training time and extensive experiments,\r\n  doing the proper k-fold cross validation is time consuming and\r\n  prohibitive.\r\n\r\n- Table 2 is correct. The models are trained differently, but\r\n  they result in very similar performance."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-View Priors for Learning Detectors from Sparse Viewpoint Data", "decision": "submitted, no decision", "abstract": "While the majority of today's object class models provide only 2D bounding boxes, far richer output hypotheses are desirable including viewpoint, fine-grained category, and 3D geometry estimate. However, models trained to provide richer output require larger amounts of training data, preferably well covering the relevant aspects such as viewpoint and fine-grained categories. In this paper, we address this issue from the perspective of transfer learning, and design an object class model that explicitly leverages correlations between visual features. Specifically, our model represents prior distributions over permissible multi-view detectors in a parametric way -- the priors are learned once from training data of a source object class, and can later be used to facilitate the learning of a detector for a target class. As we show in our experiments, this transfer is not only beneficial for detectors based on basic-level category representations, but also enables the robust learning of detectors that represent classes at finer levels of granularity, where training data is typically even scarcer and more unbalanced. As a result, we report largely improved performance in simultaneous 2D object localization and viewpoint estimation on a recent dataset of challenging street scenes.", "pdf": "https://arxiv.org/abs/1312.6095", "paperhash": "pepik|multiview_priors_for_learning_detectors_from_sparse_viewpoint_data", "keywords": [], "conflicts": [], "authors": ["Bojan Pepik", "Michael Stark", "Peter Gehler", "Bernt Schiele"], "authorids": ["bojan@mpi-inf.mpg.de", "mst@stanford.edu", "schiele@mpi-inf.mpg.de", "pgehler@tuebingen.mpg.de"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392730440000, "tcdate": 1392730440000, "number": 1, "id": "qZMRBqtNdeuCz", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "BId1QQRE1gQLZ", "replyto": "OaWGaOhCfpaFT", "signatures": ["Bojan Pepikj"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "We would like to thank the reviewer for his valuable comments. We've\r\nincluded all suggestions made by the reviewer; they can be found in\r\nthe newest paper version on arXiv. Specifically, we added model\r\nvisualizations in the supplemental material section (Fig. 7), as well\r\nas we improved Fig. 3 and introduced bars visually explaining the\r\ntraining setups. Additionally, we incorporated the technical\r\nsuggestions proposed by the reviewer.\r\n\r\nAnswers to specific questions:\r\n\r\nIn Sec. 4.1 (3D object classes dataset) each class consists of 10\r\ninstances, depicted from 8 different viewpoints, 3 different scales\r\nand heights. We use 5 instances for training, 5 for testing, resulting\r\nin 360 images per train and test set. During source model training,\r\nfrom the training set, we sample 15 images per viewpoint. During\r\ntarget model training, we sample K = {1, 5, 10} images per\r\nviewpoint. This is done for each class separately. We didn't strictly\r\nenforce the training data to be non-overlapping among the source and\r\ntarget models.\r\n\r\nIn Sec 4.2 (KITTI), when learning the priors from 3D Object Classes,\r\nwe used only the car class. As KITTI and 3D Object Classes are\r\ndifferent datasets, there was no data overlap among the training sets\r\nfor the source and target models. The viewpoint annotations had to be\r\nmapped among the two datasets, which is rather trivial to do. When\r\nusing KITTI data only, the source and the target training data are at\r\na different level in the class hierarchy (e.g. target data at car type\r\nlevel, while source data at car class level), therefore it might\r\nhappen that the source and the target data overlap.\r\n\r\nRegarding the 3D Object Classes dataset, it actually comes with 10\r\nclasses, all previous work excludes the monitor class from the\r\nexperiments, while the head class is included in more recent work,\r\nthus the 9 classes.\r\n\r\nIn the experiments section, k = all means that all training data for\r\nthe subordinate category has been used. The amount of training data\r\nvaries across the subordinate classes. Figures 4, 5 and 6 provide\r\ntraining data distributions per class."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-View Priors for Learning Detectors from Sparse Viewpoint Data", "decision": "submitted, no decision", "abstract": "While the majority of today's object class models provide only 2D bounding boxes, far richer output hypotheses are desirable including viewpoint, fine-grained category, and 3D geometry estimate. However, models trained to provide richer output require larger amounts of training data, preferably well covering the relevant aspects such as viewpoint and fine-grained categories. In this paper, we address this issue from the perspective of transfer learning, and design an object class model that explicitly leverages correlations between visual features. Specifically, our model represents prior distributions over permissible multi-view detectors in a parametric way -- the priors are learned once from training data of a source object class, and can later be used to facilitate the learning of a detector for a target class. As we show in our experiments, this transfer is not only beneficial for detectors based on basic-level category representations, but also enables the robust learning of detectors that represent classes at finer levels of granularity, where training data is typically even scarcer and more unbalanced. As a result, we report largely improved performance in simultaneous 2D object localization and viewpoint estimation on a recent dataset of challenging street scenes.", "pdf": "https://arxiv.org/abs/1312.6095", "paperhash": "pepik|multiview_priors_for_learning_detectors_from_sparse_viewpoint_data", "keywords": [], "conflicts": [], "authors": ["Bojan Pepik", "Michael Stark", "Peter Gehler", "Bernt Schiele"], "authorids": ["bojan@mpi-inf.mpg.de", "mst@stanford.edu", "schiele@mpi-inf.mpg.de", "pgehler@tuebingen.mpg.de"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391819880000, "tcdate": 1391819880000, "number": 3, "id": "gJViuDG4cAJN1", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "BId1QQRE1gQLZ", "replyto": "BId1QQRE1gQLZ", "signatures": ["anonymous reviewer 138a"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Multi-View Priors for Learning Detectors from Sparse Viewpoint Data", "review": "The paper presents a method to learn a quadratic regularizer that improves the performance of multi-view object detectors when very little training data is available for each object or view.  The regularizer is computed using a sparse correlation matrix to identify similarity amongst feature weights in the detectors for different views of the same object (in some cases relying on a 3D model to help establish correspondence between features).  The similarity matrix is then used to define a Laplace regularization term for a standard SVM, thus requiring that a new multi-view detector trained with the regularizer share similar structure across views.  Results are presented on several detection tasks using the KITTI  (driving/urban) dataset and 3D Object Classes dataset.  It is shown that the main advantage is that the method can learn to detect views of novel target objects even when some views of these objects have few or no training examples.\r\n\r\nOverall the paper is clearly written and the experiments are extensive.  The authors broke out all of the different cases [number of examples per view, and which views are missing] to make their point.  In the regime where the method is intended to help (where very few examples are available for a particular view of a target object) the regularizer does help for the multi-view DPM model considered in the paper.\r\n\r\nThe main caveat here is that the method seems to assume the use of separate detectors for every subclass and view (and thus each detector requires its own dataset).  That really exacerbates the problem of too little data where other methods might not have an issue.  This is a nice trick, but since the one-detector-per-object-per-view approach has multiple scalability issues, it looks like the proposed solution also suffers from those barriers.  If this approach to multi-view detection is not workable in the near future, what is the high-level idea that we should take from this work?  I do not see it.\r\n\r\nAlso, the \u201cdense\u201d sparsity pattern appears to be by far the best performer.  This is somewhat interesting by itself, but also detracts from the \u201csparse\u201d proposals in the paper (and, of course, the dense approach is not very scalable).  Some candid discussion on the consequences of this result might clarify what parts of the system are important.\r\n\r\nPros:\r\nA relatively simple idea to exploit knowledge of regularity across views in multi-view detectors.\r\nPrimary value is in cases where there are few or no examples for a particular view, in which case it does appear to help over baseline approaches.\r\nCons:\r\nRelies on a fair amount of prior geometric knowledge.\r\nIt is unclear how to apply this to cases where the number of features is much larger than DPM-style models [e.g., deep neural nets] or has deeper difficult-to-interpret layers where geometric information cannot be leveraged.\r\nOther:\r\nThe SVM regularization penalty might need to be tuned for complete fairness of comparisons.  Since the regularizer is fundamental to performance and the number of training examples is varied, the penalty setting could alter the testing numbers.  (It is possible that the experiments/implementation are set up in such a way that this does not matter much;  a note to this effect, if true, would be helpful.)"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-View Priors for Learning Detectors from Sparse Viewpoint Data", "decision": "submitted, no decision", "abstract": "While the majority of today's object class models provide only 2D bounding boxes, far richer output hypotheses are desirable including viewpoint, fine-grained category, and 3D geometry estimate. However, models trained to provide richer output require larger amounts of training data, preferably well covering the relevant aspects such as viewpoint and fine-grained categories. In this paper, we address this issue from the perspective of transfer learning, and design an object class model that explicitly leverages correlations between visual features. Specifically, our model represents prior distributions over permissible multi-view detectors in a parametric way -- the priors are learned once from training data of a source object class, and can later be used to facilitate the learning of a detector for a target class. As we show in our experiments, this transfer is not only beneficial for detectors based on basic-level category representations, but also enables the robust learning of detectors that represent classes at finer levels of granularity, where training data is typically even scarcer and more unbalanced. As a result, we report largely improved performance in simultaneous 2D object localization and viewpoint estimation on a recent dataset of challenging street scenes.", "pdf": "https://arxiv.org/abs/1312.6095", "paperhash": "pepik|multiview_priors_for_learning_detectors_from_sparse_viewpoint_data", "keywords": [], "conflicts": [], "authors": ["Bojan Pepik", "Michael Stark", "Peter Gehler", "Bernt Schiele"], "authorids": ["bojan@mpi-inf.mpg.de", "mst@stanford.edu", "schiele@mpi-inf.mpg.de", "pgehler@tuebingen.mpg.de"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391575620000, "tcdate": 1391575620000, "number": 2, "id": "MxgLMMu8Ldj3S", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "BId1QQRE1gQLZ", "replyto": "BId1QQRE1gQLZ", "signatures": ["anonymous reviewer 8fcf"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Multi-View Priors for Learning Detectors from Sparse Viewpoint Data", "review": "Summary\r\n\r\nThis paper proposes to improve multi-view object detection and viewpoint estimation, particularly in the case where some viewpoints are undersampled, by introducing a multi-view prior into the standard SVM training framework used to learn many HOG-based object detectors. The work extends Gao et al. [16], which considers a specific case of the more general form of priors considered in this paper. (A citation is missing for a very relevant paper by Hariharan et al. [a] which also estimates covariances between HOG cells.) The paper presents an extensive empirical study of two newly proposed priors (SVM-Sigma and SVM-MV) compared with the SVM-SV prior of Gao et al. [16] and a standard SVM with no multi-view prior.\r\n\r\nThe experimental results are dense and at times hard to parse, but SVM-Sigma shows a clear advantage on several benchmarks.\r\n\r\nPros\r\n+ The topic is quite interesting and relevant to researchers working on object detection and coarse viewpoint estimation.\r\n+ The outline of the approach is clear.\r\n+ The experimental results look quite good.\r\n\r\nCons / Questions for author feedback\r\n- While the outline of the approach is clear, some of the details are hard to follow. A main confusion throughout the paper is what data is used to estimate the prior? For example, when the target is KITTI and the prior comes from 3D-Objects, are only the car objects used from the 3D-Objects dataset (I would assume so, but this was unclear to me).\r\n- For the MV prior \u201c...we first establish pairs of cells in the target model which satisfy a certain relation type ~n.\u201d It\u2019s clear how these pairs would be established when CAD data is used. How are these correspondences established in the case of KITTI data?\r\n- Sec. 3.2: I think it would be good to clarify what is meant by \u201cbootstrapping\u201d (i.e. training multiple models on bootstrapped samples) to avoid confusion with the ill-named hard negative \u201cbootstrapping\u201d for training SVMs.\r\n- Is setting a single value for C reasonable? The regularization (via the prior) is changing quite a bit and it\u2019s not clear that keeping C constant is reasonable. That said, searching over C can only improve the already good results.\r\n- In Table 2 the \u2018base\u2019 SVM-Sigma results are the same for 3D-Objects and KITTI priors---perhaps a bug in the table?\r\n\r\n[a] Bharath Hariharan, Jitendra Malik and Deva Ramanan. Discriminative decorrelation for clustering and classification. In ECCV 2012."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-View Priors for Learning Detectors from Sparse Viewpoint Data", "decision": "submitted, no decision", "abstract": "While the majority of today's object class models provide only 2D bounding boxes, far richer output hypotheses are desirable including viewpoint, fine-grained category, and 3D geometry estimate. However, models trained to provide richer output require larger amounts of training data, preferably well covering the relevant aspects such as viewpoint and fine-grained categories. In this paper, we address this issue from the perspective of transfer learning, and design an object class model that explicitly leverages correlations between visual features. Specifically, our model represents prior distributions over permissible multi-view detectors in a parametric way -- the priors are learned once from training data of a source object class, and can later be used to facilitate the learning of a detector for a target class. As we show in our experiments, this transfer is not only beneficial for detectors based on basic-level category representations, but also enables the robust learning of detectors that represent classes at finer levels of granularity, where training data is typically even scarcer and more unbalanced. As a result, we report largely improved performance in simultaneous 2D object localization and viewpoint estimation on a recent dataset of challenging street scenes.", "pdf": "https://arxiv.org/abs/1312.6095", "paperhash": "pepik|multiview_priors_for_learning_detectors_from_sparse_viewpoint_data", "keywords": [], "conflicts": [], "authors": ["Bojan Pepik", "Michael Stark", "Peter Gehler", "Bernt Schiele"], "authorids": ["bojan@mpi-inf.mpg.de", "mst@stanford.edu", "schiele@mpi-inf.mpg.de", "pgehler@tuebingen.mpg.de"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391504340000, "tcdate": 1391504340000, "number": 1, "id": "OaWGaOhCfpaFT", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "BId1QQRE1gQLZ", "replyto": "BId1QQRE1gQLZ", "signatures": ["anonymous reviewer 42a4"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Multi-View Priors for Learning Detectors from Sparse Viewpoint Data", "review": "This work aims to perform simultaneous detection and viewpoint estimation in the face of having few training examples for many classes or viewpoints, but where many examples are available for a smaller or related set of source instances.  To accomplish this, two new types of weight regularization are described, for use with deformable parts models.  Both regularizers form a quadratic penalty on the weights by means of a covariance matrix, constructed using models trained on the sources.  The first, SVM-MV, constructs the covariance by averaging across all pairs of HoG cells that overlap between adjacent object viewpoints; overlaps are found by projecting to a (provided or guessed) 3D object model.  The second, SVM-Sigma, constructs an explicit all-to-all covariance of the weights by averaging across different model instances.  SVM-MV extends the ideas of Gao et al. to work across views, while SVM-Sigma breaks from these careful constructions and uses all pairwise interactions that arise.  The authors evaluate their effectiveness using two datasets, 3D-objects and KITTI, concluding that such regularization enables good performance in these tasks, with SVM-Sigma generally outperforming the other methods.\r\n\r\nUnfortunately, the exposition is rather dry and can be hard to follow.  Illustrative examples and diagrams would help a lot here, though, particularly sketches of the projections and overlaps for SVM-MV.  I think it also would help to instantiate model early on, using one of the datasets from the experiments (i.e., in sec 3, linking sim_n, w^s, w^t to concrete instances).\r\n\r\nI'm also a bit confused on what exactly comprised the source vs target data.  For sec 4.1 (3D-objects), how was the data divided between sources (used for the priors) and the targets?  Was there any overlap between these, either by datapoint or by object instance?  For sec 4.2, p.8 para 1 seems to say that for KITTI, the priors were trained from source data drawn from either from 3D-objects or KITTI (i.e. there are two different cases).  In the former case, did viewpoints need to be mapped to transfer between the datasets, and which object classes were used?  In the latter case, did the prior data overlap the target data?\r\n\r\n\r\nPros:\r\n\r\n- Presents new regularizers that exploit structure relations, learned from the data in cases where there are dense subsets or aggregates\r\n\r\n- Experiments are detailed\r\n\r\nCons:\r\n\r\n- Dry and hard to grasp\r\n- Could use more illustrations of the method and problem setup\r\n- Results presentation confusing at times\r\n\r\n\r\n\r\nMinor comments:\r\n\r\n- Fig 1 right, TD2ND:  the labels along the rows (y-axis) appear swapped:  The text indicates the block with ones should connect with-data to no-data.\r\n\r\n- I found the italics on occurrences of 'target' and 'source' somewhat distracting; it tended to take my eyes away from the parts of paragraphs I wanted to concentrate on much of the time.\r\n\r\n- p.5 last para:  says there are 9 object classes, but the webpage for this dataset says there are 8?\r\n\r\n- p.6 (Experiments sec):  k in {1,5,10,all} -- would be nice if this said how many 'all' is as well.\r\n\r\n- Fig 2:  figures could use titles\r\n\r\n- Fig 3:  I'm confused about which views were included/excluded for each plot -- are the included views progressive subsets?  It looks like the differences are more than that.  Maybe a key with on/off bitmap listing each view would help."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-View Priors for Learning Detectors from Sparse Viewpoint Data", "decision": "submitted, no decision", "abstract": "While the majority of today's object class models provide only 2D bounding boxes, far richer output hypotheses are desirable including viewpoint, fine-grained category, and 3D geometry estimate. However, models trained to provide richer output require larger amounts of training data, preferably well covering the relevant aspects such as viewpoint and fine-grained categories. In this paper, we address this issue from the perspective of transfer learning, and design an object class model that explicitly leverages correlations between visual features. Specifically, our model represents prior distributions over permissible multi-view detectors in a parametric way -- the priors are learned once from training data of a source object class, and can later be used to facilitate the learning of a detector for a target class. As we show in our experiments, this transfer is not only beneficial for detectors based on basic-level category representations, but also enables the robust learning of detectors that represent classes at finer levels of granularity, where training data is typically even scarcer and more unbalanced. As a result, we report largely improved performance in simultaneous 2D object localization and viewpoint estimation on a recent dataset of challenging street scenes.", "pdf": "https://arxiv.org/abs/1312.6095", "paperhash": "pepik|multiview_priors_for_learning_detectors_from_sparse_viewpoint_data", "keywords": [], "conflicts": [], "authors": ["Bojan Pepik", "Michael Stark", "Peter Gehler", "Bernt Schiele"], "authorids": ["bojan@mpi-inf.mpg.de", "mst@stanford.edu", "schiele@mpi-inf.mpg.de", "pgehler@tuebingen.mpg.de"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387805400000, "tcdate": 1387805400000, "number": 32, "id": "BId1QQRE1gQLZ", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "BId1QQRE1gQLZ", "signatures": ["bojan@mpi-inf.mpg.de"], "readers": ["everyone"], "content": {"title": "Multi-View Priors for Learning Detectors from Sparse Viewpoint Data", "decision": "submitted, no decision", "abstract": "While the majority of today's object class models provide only 2D bounding boxes, far richer output hypotheses are desirable including viewpoint, fine-grained category, and 3D geometry estimate. However, models trained to provide richer output require larger amounts of training data, preferably well covering the relevant aspects such as viewpoint and fine-grained categories. In this paper, we address this issue from the perspective of transfer learning, and design an object class model that explicitly leverages correlations between visual features. Specifically, our model represents prior distributions over permissible multi-view detectors in a parametric way -- the priors are learned once from training data of a source object class, and can later be used to facilitate the learning of a detector for a target class. As we show in our experiments, this transfer is not only beneficial for detectors based on basic-level category representations, but also enables the robust learning of detectors that represent classes at finer levels of granularity, where training data is typically even scarcer and more unbalanced. As a result, we report largely improved performance in simultaneous 2D object localization and viewpoint estimation on a recent dataset of challenging street scenes.", "pdf": "https://arxiv.org/abs/1312.6095", "paperhash": "pepik|multiview_priors_for_learning_detectors_from_sparse_viewpoint_data", "keywords": [], "conflicts": [], "authors": ["Bojan Pepik", "Michael Stark", "Peter Gehler", "Bernt Schiele"], "authorids": ["bojan@mpi-inf.mpg.de", "mst@stanford.edu", "schiele@mpi-inf.mpg.de", "pgehler@tuebingen.mpg.de"]}, "writers": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 7}