{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488400943838, "tcdate": 1478298398743, "number": 519, "id": "r1LXit5ee", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "r1LXit5ee", "signatures": ["~Gabriel_Synnaeve1"], "readers": ["everyone"], "content": {"title": "Episodic Exploration for Deep Deterministic Policies for StarCraft Micromanagement", "abstract": "We consider scenarios from the real-time strategy game StarCraft as benchmarks for reinforcement learning algorithms. We focus on micromanagement, that is, the short-term, low-level control of team members during a battle. We propose several scenarios that are challenging for reinforcement learning algorithms because the state- action space is very large, and there is no obvious feature representation for the value functions. We describe our approach to tackle the micromanagement scenarios with deep neural network controllers from raw state features given by the game engine. We also present a heuristic reinforcement learning algorithm which combines direct exploration in the policy space and backpropagation. This algorithm collects traces for learning using deterministic policies, which appears much more efficient than, e.g., \u03b5-greedy exploration. Experiments show that this algorithm allows to successfully learn non-trivial strategies for scenarios with armies of up to 15 agents, where both Q-learning and REINFORCE struggle.", "pdf": "/pdf/a436dd86fa64bf2053b3323a072669d6b505e675.pdf", "TL;DR": "We propose a new reinforcement learning algorithm based on zero order optimization, that we evaluate on StarCraft micromanagement scenarios.", "paperhash": "usunier|episodic_exploration_for_deep_deterministic_policies_for_starcraft_micromanagement", "conflicts": ["fb.com"], "keywords": ["Deep learning", "Reinforcement Learning", "Games"], "authors": ["Nicolas Usunier", "Gabriel Synnaeve", "Zeming Lin", "Soumith Chintala"], "authorids": ["usunier@fb.com", "gab@fb.com", "zlin@fb.com", "soumith@fb.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396653584, "tcdate": 1486396653584, "number": 1, "id": "HJIgazL_l", "invitation": "ICLR.cc/2017/conference/-/paper519/acceptance", "forum": "r1LXit5ee", "replyto": "r1LXit5ee", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The paper presents an approach to structured exploration in StarCraft micromanagement policies (essentially small tasks in the game). From an application standpoint, this is a notable advance, since the authors are tackling a challenging domain and in doing so develop a novel exploration algorithm that seems to work quite well here.\n \n The main downside of the paper is that the proposed algorithm does seem fairly ad-hoc: certain approximations in the algorithm, like ignoring the argmax term in computing the backprop, are not really justified except in that it \"seems to work well in practice\" (a quote from the paper), so it's really unclear whether these represent general techniques or just a method that happens to work well on the target domain for poorly understood reasons.\n \n Despite this, however, I think the strength of the application is sufficient here. Deep learning has been alternatively pushed forward by more algorithmic/mathematical advances and more applied advances, with many of the major breakthroughs coming from seemingly ad-hoc strategies applied to challenging problems. This paper falls in that later category: the ZO algorithm may or may not lead to something slightly more disciplined in the future, but for now the compelling results on StarCraft are I believe enough to warrant accepting the paper.\n \n Pros:\n + Substantial performance improvement (over basic techniques like Q-learning) on a challenging task\n + Nice intuitive justification of a new exploration approach\n \n Cons:\n - Proposed algorithm seems rather ad-hoc, making some (admittedly not theoretically justified) approximations simply because they seem to work well in practice", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Episodic Exploration for Deep Deterministic Policies for StarCraft Micromanagement", "abstract": "We consider scenarios from the real-time strategy game StarCraft as benchmarks for reinforcement learning algorithms. We focus on micromanagement, that is, the short-term, low-level control of team members during a battle. We propose several scenarios that are challenging for reinforcement learning algorithms because the state- action space is very large, and there is no obvious feature representation for the value functions. We describe our approach to tackle the micromanagement scenarios with deep neural network controllers from raw state features given by the game engine. We also present a heuristic reinforcement learning algorithm which combines direct exploration in the policy space and backpropagation. This algorithm collects traces for learning using deterministic policies, which appears much more efficient than, e.g., \u03b5-greedy exploration. Experiments show that this algorithm allows to successfully learn non-trivial strategies for scenarios with armies of up to 15 agents, where both Q-learning and REINFORCE struggle.", "pdf": "/pdf/a436dd86fa64bf2053b3323a072669d6b505e675.pdf", "TL;DR": "We propose a new reinforcement learning algorithm based on zero order optimization, that we evaluate on StarCraft micromanagement scenarios.", "paperhash": "usunier|episodic_exploration_for_deep_deterministic_policies_for_starcraft_micromanagement", "conflicts": ["fb.com"], "keywords": ["Deep learning", "Reinforcement Learning", "Games"], "authors": ["Nicolas Usunier", "Gabriel Synnaeve", "Zeming Lin", "Soumith Chintala"], "authorids": ["usunier@fb.com", "gab@fb.com", "zlin@fb.com", "soumith@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396654092, "id": "ICLR.cc/2017/conference/-/paper519/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "r1LXit5ee", "replyto": "r1LXit5ee", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396654092}}}, {"tddate": null, "tmdate": 1485189454258, "tcdate": 1482177826415, "number": 3, "id": "SJcQahSEg", "invitation": "ICLR.cc/2017/conference/-/paper519/official/review", "forum": "r1LXit5ee", "replyto": "r1LXit5ee", "signatures": ["ICLR.cc/2017/conference/paper519/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper519/AnonReviewer2"], "content": {"title": "Final Review: Nice new application of zeroth order optimization for structured exploration. Complex domain, and good results", "rating": "7: Good paper, accept", "review": "The paper presents a learning algorithm for micromanagement of battle scenarios in real-time strategy games. It focuses on a complex sub-problem of the full RTS problem. The assumptions and restrictions made (greedy MDP, distance-based action encoding, etc.) are clear and make sense for this problem.\n\nThe main contribution of this paper is the zero-order optimization algorithm and how it is used for structured exploration. This is a nice new application of zero-order optimization meets deep learning for RL, quite well-motivated using similar arguments as DPG. The results show clear wins over vanilla Q-learning and REINFORCE, which is not hard to believe. Although RTS is a very interesting and challenging domain (certainly worthy as a domain of focused research!), it would have been nice to see results on other domains, mainly because it seems that this algorithm could be more generally applicable than just RTS games. Also, evaluation on such a complex domain makes it difficult to predict what other kinds of domains would benefit from this zero-order approach. Maybe the authors could add some text to clarify/motivate this.\n\nThere are a few seemingly arbitrary choices that are justified only by \"it worked in practice\". For example, using only the sign of w / Psi_{theta}(s^k, a^k). Again later: \"Also we neglected the argmax operation that chooses the actions\". I suppose this and dividing by t could keep things nicely within or close to [-1,1] ? It might make sense to try truncating/normalizing w/Psi; it seems that much information must be lost when only taking the sign. Also lines such as \"We did not extensively experiment with the structure of the network, but we found the maxpooling and tanh nonlinearity to be particularly important\" and claiming the importance of adagrad over RMSprop without elaboration or providing any details feels somewhat unsatisfactory and leaves the reader wondering why.. e.g. could these only be true in the RTS setup in this paper?\n\nThe presentation of the paper can be improved, as some ideas are presented without any context making it unnecessarily confusing. For example, when defining f(\\tilde{s}, c) at the top of page 5, the w vector is not explained at all, so the reader is left wondering where it comes from or what its use is. This is explained later, of course, but one sentence on its role here would help contextualize its purpose (maybe refer later to the section where it is described fully). Also page 7: \"because we neglected that a single u is sampled for an entire episode\"; actually, no, you did mention this in the text above and it's clear from the pseudo-code too.\n\n\"perturbated\" -> \"perturbed\"\n\n--- After response period: \n\nNo rebuttal entered, therefore review remains unchanged.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Episodic Exploration for Deep Deterministic Policies for StarCraft Micromanagement", "abstract": "We consider scenarios from the real-time strategy game StarCraft as benchmarks for reinforcement learning algorithms. We focus on micromanagement, that is, the short-term, low-level control of team members during a battle. We propose several scenarios that are challenging for reinforcement learning algorithms because the state- action space is very large, and there is no obvious feature representation for the value functions. We describe our approach to tackle the micromanagement scenarios with deep neural network controllers from raw state features given by the game engine. We also present a heuristic reinforcement learning algorithm which combines direct exploration in the policy space and backpropagation. This algorithm collects traces for learning using deterministic policies, which appears much more efficient than, e.g., \u03b5-greedy exploration. Experiments show that this algorithm allows to successfully learn non-trivial strategies for scenarios with armies of up to 15 agents, where both Q-learning and REINFORCE struggle.", "pdf": "/pdf/a436dd86fa64bf2053b3323a072669d6b505e675.pdf", "TL;DR": "We propose a new reinforcement learning algorithm based on zero order optimization, that we evaluate on StarCraft micromanagement scenarios.", "paperhash": "usunier|episodic_exploration_for_deep_deterministic_policies_for_starcraft_micromanagement", "conflicts": ["fb.com"], "keywords": ["Deep learning", "Reinforcement Learning", "Games"], "authors": ["Nicolas Usunier", "Gabriel Synnaeve", "Zeming Lin", "Soumith Chintala"], "authorids": ["usunier@fb.com", "gab@fb.com", "zlin@fb.com", "soumith@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512556035, "id": "ICLR.cc/2017/conference/-/paper519/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper519/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper519/AnonReviewer1", "ICLR.cc/2017/conference/paper519/AnonReviewer3", "ICLR.cc/2017/conference/paper519/AnonReviewer2"], "reply": {"forum": "r1LXit5ee", "replyto": "r1LXit5ee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper519/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper519/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512556035}}}, {"tddate": null, "tmdate": 1482175080908, "tcdate": 1482175080908, "number": 2, "id": "H1WuMnSVx", "invitation": "ICLR.cc/2017/conference/-/paper519/official/review", "forum": "r1LXit5ee", "replyto": "r1LXit5ee", "signatures": ["ICLR.cc/2017/conference/paper519/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper519/AnonReviewer3"], "content": {"title": "", "rating": "7: Good paper, accept", "review": "This is a very interesting and timely paper, with multiple contributions. \n- it proposes a setup for dealing with combinatorial perception and action-spaces that generalizes to an arbitrary number of units and opponent units,\n- it establishes some deep RL baseline results on a collection of Starcraft subdomains,\n- it proposes a new algorithm that is a hybrid between black-box optimization REINFORCE, and which facilitates consistent exploration.\n\n\nAs mentioned in an earlier comment, I don\u2019t see why the \u201cgradient of the average cumulative reward\u201d is a reasonable choice, as compared to just the average reward? This over-weights late rewards at the expense of early ones, so the updates are not matching the measured objective. The authors state that they \u201cdid not observe a large difference in preliminary experiments\u201d -- so if that is the case, then why not choose the correct objective?\n\nDPQ is characterized incorrectly: despite its name, it does not \u201ccollect traces by following deterministic policies\u201d, instead it follows a stochastic behavior policy and learns off-policy about the deterministic policy. Please revise this. \n\nGradient-free optimization is also characterized incorrectly (\u201cit only scales to few parameters\u201d), recent work has shown that this can be overcome (e.g. the TORCS paper by Koutnik et al, 2013). This also suggests that your \u201cpreliminary experiments with direct exploration in the parameter space\u201d may not have followed best practices in neuroevolution? Did you try out some of the recent variants of NEAT for example, which have been applied to similar domains in the past?\n\nOn the specific results, I\u2019m wondering about the DQN transfer from m15v16 to m5v5, obtaining the best win rate of 96% in transfer, despite only reaching 13% (the worst) on the training domain? Is this a typo, or how can you explain that?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Episodic Exploration for Deep Deterministic Policies for StarCraft Micromanagement", "abstract": "We consider scenarios from the real-time strategy game StarCraft as benchmarks for reinforcement learning algorithms. We focus on micromanagement, that is, the short-term, low-level control of team members during a battle. We propose several scenarios that are challenging for reinforcement learning algorithms because the state- action space is very large, and there is no obvious feature representation for the value functions. We describe our approach to tackle the micromanagement scenarios with deep neural network controllers from raw state features given by the game engine. We also present a heuristic reinforcement learning algorithm which combines direct exploration in the policy space and backpropagation. This algorithm collects traces for learning using deterministic policies, which appears much more efficient than, e.g., \u03b5-greedy exploration. Experiments show that this algorithm allows to successfully learn non-trivial strategies for scenarios with armies of up to 15 agents, where both Q-learning and REINFORCE struggle.", "pdf": "/pdf/a436dd86fa64bf2053b3323a072669d6b505e675.pdf", "TL;DR": "We propose a new reinforcement learning algorithm based on zero order optimization, that we evaluate on StarCraft micromanagement scenarios.", "paperhash": "usunier|episodic_exploration_for_deep_deterministic_policies_for_starcraft_micromanagement", "conflicts": ["fb.com"], "keywords": ["Deep learning", "Reinforcement Learning", "Games"], "authors": ["Nicolas Usunier", "Gabriel Synnaeve", "Zeming Lin", "Soumith Chintala"], "authorids": ["usunier@fb.com", "gab@fb.com", "zlin@fb.com", "soumith@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512556035, "id": "ICLR.cc/2017/conference/-/paper519/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper519/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper519/AnonReviewer1", "ICLR.cc/2017/conference/paper519/AnonReviewer3", "ICLR.cc/2017/conference/paper519/AnonReviewer2"], "reply": {"forum": "r1LXit5ee", "replyto": "r1LXit5ee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper519/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper519/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512556035}}}, {"tddate": null, "tmdate": 1481963597868, "tcdate": 1481963597868, "number": 1, "id": "rJULd_G4x", "invitation": "ICLR.cc/2017/conference/-/paper519/official/review", "forum": "r1LXit5ee", "replyto": "r1LXit5ee", "signatures": ["ICLR.cc/2017/conference/paper519/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper519/AnonReviewer1"], "content": {"title": "Topically relevant work, likely of significant interest", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This work introduces some StarCraft micro-management tasks (controlling individual units during a battle). These tasks are difficult for recent DeepRL methods due to high-dimensional, variable action spaces (the action space is the task of each unit, the number of units may vary). In such large action spaces, simple exploration strategies (such as epsilon-greedy) perform poorly.\n\nThey introduce a novel algorithm ZO to tackle this problem. This algorithm combines ideas from policy gradient, deep networks trained with backpropagation for state embedding and gradient free optimization. The algorithm is well explained and is compared to some existing baselines. Due to the gradient free optimization providing for much better structured exploration, it performs far better.\n\nThis is a well-written paper and a novel algorithm which is applied to a very relevant problem. After the success of DeepRL approaches at learning in large state spaces such as visual environment, there is significant interest in applying RL to more structured state and action spaces. The tasks introduced here are interesting environments for these sorts of problems.\n\nIt would be helpful if the authors were able to share the source code / specifications for their tasks, to allow other groups to compare against this work.\n\nI found section 5 (the details of the raw inputs and feature encodings) somewhat difficult to understand. In addition to clarifying, the authors might wish to consider whether they could provide the source code to their algorithm or at least the encoder to allow careful comparisons by other work.\n\nAlthough discussed, there is no baseline comparison with valued based approaches with attempt to do better exploration by modeling uncertainty (such as Bootstrapped DQN). It would useful to understand how such approaches, which also promise better exploration, compare.\n\nIt would also be interesting to discuss whether action embedding models such as energy-based approaches (e.g. https://arxiv.org/pdf/1512.07679v2.pdf, http://www.jmlr.org/papers/volume5/sallans04a/sallans04a.pdf ) or continuous action embeddings (https://arxiv.org/pdf/1512.07679v2.pdf ) would provide an alternative approach for structured exploration in these action spaces.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Episodic Exploration for Deep Deterministic Policies for StarCraft Micromanagement", "abstract": "We consider scenarios from the real-time strategy game StarCraft as benchmarks for reinforcement learning algorithms. We focus on micromanagement, that is, the short-term, low-level control of team members during a battle. We propose several scenarios that are challenging for reinforcement learning algorithms because the state- action space is very large, and there is no obvious feature representation for the value functions. We describe our approach to tackle the micromanagement scenarios with deep neural network controllers from raw state features given by the game engine. We also present a heuristic reinforcement learning algorithm which combines direct exploration in the policy space and backpropagation. This algorithm collects traces for learning using deterministic policies, which appears much more efficient than, e.g., \u03b5-greedy exploration. Experiments show that this algorithm allows to successfully learn non-trivial strategies for scenarios with armies of up to 15 agents, where both Q-learning and REINFORCE struggle.", "pdf": "/pdf/a436dd86fa64bf2053b3323a072669d6b505e675.pdf", "TL;DR": "We propose a new reinforcement learning algorithm based on zero order optimization, that we evaluate on StarCraft micromanagement scenarios.", "paperhash": "usunier|episodic_exploration_for_deep_deterministic_policies_for_starcraft_micromanagement", "conflicts": ["fb.com"], "keywords": ["Deep learning", "Reinforcement Learning", "Games"], "authors": ["Nicolas Usunier", "Gabriel Synnaeve", "Zeming Lin", "Soumith Chintala"], "authorids": ["usunier@fb.com", "gab@fb.com", "zlin@fb.com", "soumith@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512556035, "id": "ICLR.cc/2017/conference/-/paper519/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper519/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper519/AnonReviewer1", "ICLR.cc/2017/conference/paper519/AnonReviewer3", "ICLR.cc/2017/conference/paper519/AnonReviewer2"], "reply": {"forum": "r1LXit5ee", "replyto": "r1LXit5ee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper519/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper519/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512556035}}}, {"tddate": null, "tmdate": 1481833700490, "tcdate": 1481833700482, "number": 1, "id": "Byh1TugVe", "invitation": "ICLR.cc/2017/conference/-/paper519/official/comment", "forum": "r1LXit5ee", "replyto": "ryLbDa0Qg", "signatures": ["ICLR.cc/2017/conference/paper519/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper519/AnonReviewer3"], "content": {"title": "follow-up", "comment": "Thanks!\n\nOne more clarification about the w-gradient, is the following correct:\n\n\\hat{g} = u * 1/t * sum[r_k * k]\n\nIn other words, the u-vector scaled by a kind of inversely discounted return (final rewards are weighted much more than early ones)? Or should I really only be looking at the normalized variant (in which case, only put that one into the pseudo-code)?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Episodic Exploration for Deep Deterministic Policies for StarCraft Micromanagement", "abstract": "We consider scenarios from the real-time strategy game StarCraft as benchmarks for reinforcement learning algorithms. We focus on micromanagement, that is, the short-term, low-level control of team members during a battle. We propose several scenarios that are challenging for reinforcement learning algorithms because the state- action space is very large, and there is no obvious feature representation for the value functions. We describe our approach to tackle the micromanagement scenarios with deep neural network controllers from raw state features given by the game engine. We also present a heuristic reinforcement learning algorithm which combines direct exploration in the policy space and backpropagation. This algorithm collects traces for learning using deterministic policies, which appears much more efficient than, e.g., \u03b5-greedy exploration. Experiments show that this algorithm allows to successfully learn non-trivial strategies for scenarios with armies of up to 15 agents, where both Q-learning and REINFORCE struggle.", "pdf": "/pdf/a436dd86fa64bf2053b3323a072669d6b505e675.pdf", "TL;DR": "We propose a new reinforcement learning algorithm based on zero order optimization, that we evaluate on StarCraft micromanagement scenarios.", "paperhash": "usunier|episodic_exploration_for_deep_deterministic_policies_for_starcraft_micromanagement", "conflicts": ["fb.com"], "keywords": ["Deep learning", "Reinforcement Learning", "Games"], "authors": ["Nicolas Usunier", "Gabriel Synnaeve", "Zeming Lin", "Soumith Chintala"], "authorids": ["usunier@fb.com", "gab@fb.com", "zlin@fb.com", "soumith@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287539816, "id": "ICLR.cc/2017/conference/-/paper519/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "r1LXit5ee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper519/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper519/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper519/reviewers", "ICLR.cc/2017/conference/paper519/areachairs"], "cdate": 1485287539816}}}, {"tddate": null, "tmdate": 1481721597776, "tcdate": 1481721597769, "number": 2, "id": "ryLbDa0Qg", "invitation": "ICLR.cc/2017/conference/-/paper519/public/comment", "forum": "r1LXit5ee", "replyto": "Syv17Wy7l", "signatures": ["~Nicolas_Usunier1"], "readers": ["everyone"], "writers": ["~Nicolas_Usunier1"], "content": {"title": "Re: coordinated exploration, and algorithm clarifications", "comment": "We thank the reviewer for these constructive comments. We updated section 6 and the algorithm significantly to clarify the zero-order + backprop heuristic.\n\nAbout the combinatorial action space, it scales exponentially with the number of units. In practice, good strategies should implement some form of local consistency (both in space and time), which should make exploration feasible.\n\nAbout homogenous unit behavior: maps large number of units (e.g. m15v16 and w15v17) specifically test for the units to be coordinated around a strategy, but not take exactly the same action: we can see that focus firing with overkill gives poor results. Zero-order correctly learns not to overkill in such maps. As to formations, we did not observe such behavior. This is likely because the scenarios do not require this kind of strategies, but also because the model does not seem to be well-suited to learn coordinated movements (as we mentioned in the conclusion). For \"large\" groups of units (e.g. 15+ here), learning formations directly on atomic moves through RL appears to be a real challenge (e.g., because bad moves are extremely detrimental and there is no immediate feedback to guide exploration)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Episodic Exploration for Deep Deterministic Policies for StarCraft Micromanagement", "abstract": "We consider scenarios from the real-time strategy game StarCraft as benchmarks for reinforcement learning algorithms. We focus on micromanagement, that is, the short-term, low-level control of team members during a battle. We propose several scenarios that are challenging for reinforcement learning algorithms because the state- action space is very large, and there is no obvious feature representation for the value functions. We describe our approach to tackle the micromanagement scenarios with deep neural network controllers from raw state features given by the game engine. We also present a heuristic reinforcement learning algorithm which combines direct exploration in the policy space and backpropagation. This algorithm collects traces for learning using deterministic policies, which appears much more efficient than, e.g., \u03b5-greedy exploration. Experiments show that this algorithm allows to successfully learn non-trivial strategies for scenarios with armies of up to 15 agents, where both Q-learning and REINFORCE struggle.", "pdf": "/pdf/a436dd86fa64bf2053b3323a072669d6b505e675.pdf", "TL;DR": "We propose a new reinforcement learning algorithm based on zero order optimization, that we evaluate on StarCraft micromanagement scenarios.", "paperhash": "usunier|episodic_exploration_for_deep_deterministic_policies_for_starcraft_micromanagement", "conflicts": ["fb.com"], "keywords": ["Deep learning", "Reinforcement Learning", "Games"], "authors": ["Nicolas Usunier", "Gabriel Synnaeve", "Zeming Lin", "Soumith Chintala"], "authorids": ["usunier@fb.com", "gab@fb.com", "zlin@fb.com", "soumith@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287539989, "id": "ICLR.cc/2017/conference/-/paper519/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1LXit5ee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper519/reviewers", "ICLR.cc/2017/conference/paper519/areachairs"], "cdate": 1485287539989}}}, {"tddate": null, "tmdate": 1481720184830, "tcdate": 1481720184824, "number": 1, "id": "SJWYWpCXx", "invitation": "ICLR.cc/2017/conference/-/paper519/public/comment", "forum": "r1LXit5ee", "replyto": "B1VKKmeXl", "signatures": ["~Nicolas_Usunier1"], "readers": ["everyone"], "writers": ["~Nicolas_Usunier1"], "content": {"title": "Re: Table 2 uncertainty", "comment": "We thank the reviewer for the helpful comment. We removed the bold .79 for ZO in m15v16, which is, indeed, not comparable to .81 for the baseline heuristics. All the algorithms (heuristics and RL) were tested on 1000 battles. The numbers .99 and 1. (m5v5 DQN and ZO respectively) are a difference of 6 games. The standard deviation is about 10 games out of 1000."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Episodic Exploration for Deep Deterministic Policies for StarCraft Micromanagement", "abstract": "We consider scenarios from the real-time strategy game StarCraft as benchmarks for reinforcement learning algorithms. We focus on micromanagement, that is, the short-term, low-level control of team members during a battle. We propose several scenarios that are challenging for reinforcement learning algorithms because the state- action space is very large, and there is no obvious feature representation for the value functions. We describe our approach to tackle the micromanagement scenarios with deep neural network controllers from raw state features given by the game engine. We also present a heuristic reinforcement learning algorithm which combines direct exploration in the policy space and backpropagation. This algorithm collects traces for learning using deterministic policies, which appears much more efficient than, e.g., \u03b5-greedy exploration. Experiments show that this algorithm allows to successfully learn non-trivial strategies for scenarios with armies of up to 15 agents, where both Q-learning and REINFORCE struggle.", "pdf": "/pdf/a436dd86fa64bf2053b3323a072669d6b505e675.pdf", "TL;DR": "We propose a new reinforcement learning algorithm based on zero order optimization, that we evaluate on StarCraft micromanagement scenarios.", "paperhash": "usunier|episodic_exploration_for_deep_deterministic_policies_for_starcraft_micromanagement", "conflicts": ["fb.com"], "keywords": ["Deep learning", "Reinforcement Learning", "Games"], "authors": ["Nicolas Usunier", "Gabriel Synnaeve", "Zeming Lin", "Soumith Chintala"], "authorids": ["usunier@fb.com", "gab@fb.com", "zlin@fb.com", "soumith@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287539989, "id": "ICLR.cc/2017/conference/-/paper519/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1LXit5ee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper519/reviewers", "ICLR.cc/2017/conference/paper519/areachairs"], "cdate": 1485287539989}}}, {"tddate": null, "tmdate": 1480763771763, "tcdate": 1480763771759, "number": 2, "id": "B1VKKmeXl", "invitation": "ICLR.cc/2017/conference/-/paper519/pre-review/question", "forum": "r1LXit5ee", "replyto": "r1LXit5ee", "signatures": ["ICLR.cc/2017/conference/paper519/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper519/AnonReviewer2"], "content": {"title": "Table 2 uncertainty", "question": "Why are multiple values bolded per row? I suppose these are statistically insignificant differences? If so: how was this decided... and can the details be noted (i.e. 95% confidence intervals of +/- value) ?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Episodic Exploration for Deep Deterministic Policies for StarCraft Micromanagement", "abstract": "We consider scenarios from the real-time strategy game StarCraft as benchmarks for reinforcement learning algorithms. We focus on micromanagement, that is, the short-term, low-level control of team members during a battle. We propose several scenarios that are challenging for reinforcement learning algorithms because the state- action space is very large, and there is no obvious feature representation for the value functions. We describe our approach to tackle the micromanagement scenarios with deep neural network controllers from raw state features given by the game engine. We also present a heuristic reinforcement learning algorithm which combines direct exploration in the policy space and backpropagation. This algorithm collects traces for learning using deterministic policies, which appears much more efficient than, e.g., \u03b5-greedy exploration. Experiments show that this algorithm allows to successfully learn non-trivial strategies for scenarios with armies of up to 15 agents, where both Q-learning and REINFORCE struggle.", "pdf": "/pdf/a436dd86fa64bf2053b3323a072669d6b505e675.pdf", "TL;DR": "We propose a new reinforcement learning algorithm based on zero order optimization, that we evaluate on StarCraft micromanagement scenarios.", "paperhash": "usunier|episodic_exploration_for_deep_deterministic_policies_for_starcraft_micromanagement", "conflicts": ["fb.com"], "keywords": ["Deep learning", "Reinforcement Learning", "Games"], "authors": ["Nicolas Usunier", "Gabriel Synnaeve", "Zeming Lin", "Soumith Chintala"], "authorids": ["usunier@fb.com", "gab@fb.com", "zlin@fb.com", "soumith@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959235301, "id": "ICLR.cc/2017/conference/-/paper519/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper519/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper519/AnonReviewer3", "ICLR.cc/2017/conference/paper519/AnonReviewer2"], "reply": {"forum": "r1LXit5ee", "replyto": "r1LXit5ee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper519/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper519/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959235301}}}, {"tddate": null, "tmdate": 1480689501520, "tcdate": 1480688350737, "number": 1, "id": "Syv17Wy7l", "invitation": "ICLR.cc/2017/conference/-/paper519/pre-review/question", "forum": "r1LXit5ee", "replyto": "r1LXit5ee", "signatures": ["ICLR.cc/2017/conference/paper519/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper519/AnonReviewer3"], "content": {"title": "coordinated exploration, and algorithm clarifications", "question": "I'm intrigued by your comments about how the combinatorial action space (controlling many units) affects exploration, in particular when coordination is necessary for meaningful feedback. My hunch is that in a pure form, this difficulty would scale exponentially with the number of units? Do you have more insights on this?\n\nI like your solution of turning the slate-action into a (longer) sequence of single-unit actions, it encourages coordinated behavior in a very direct way. I'm just worried that it might be almost too strong as a bias toward homogeneous unit behavior, because something like forming sub-formations that execute a flanking attack is very unlikely to emerge this way? Have you observed such behavior, or could you augment your set of tasks to specifically test for non-homogeneous strategy?\n\n[edited because I did not realize I only get one question]\n\nI would also like to understand the actual algorithm better: in its current form (revision of Nov 23) it is scattered across sections 4, 6, pseudocode and appendix, with inconsistent notation (e.g. g-hat is not used in the text, R and n-bar might refer to the same? etc). The clearest I found was the normalized-reward-with-adaptive-discount formulation from the appendix: I would embrace that to simplify everything else. Also, make the pseudo-code complete by inserting all the (currently mismatched) reference equations. The use of \"u\" in the pseudo-code is also rather odd: u is a constant across the episode, so g-hat could just be computed as u * return at the final step, instead of incrementally at each time-step? Actually, I'm still confused about the ZO algorithm: I'd strongly suggest rewriting section 6 and the pseudo-code."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Episodic Exploration for Deep Deterministic Policies for StarCraft Micromanagement", "abstract": "We consider scenarios from the real-time strategy game StarCraft as benchmarks for reinforcement learning algorithms. We focus on micromanagement, that is, the short-term, low-level control of team members during a battle. We propose several scenarios that are challenging for reinforcement learning algorithms because the state- action space is very large, and there is no obvious feature representation for the value functions. We describe our approach to tackle the micromanagement scenarios with deep neural network controllers from raw state features given by the game engine. We also present a heuristic reinforcement learning algorithm which combines direct exploration in the policy space and backpropagation. This algorithm collects traces for learning using deterministic policies, which appears much more efficient than, e.g., \u03b5-greedy exploration. Experiments show that this algorithm allows to successfully learn non-trivial strategies for scenarios with armies of up to 15 agents, where both Q-learning and REINFORCE struggle.", "pdf": "/pdf/a436dd86fa64bf2053b3323a072669d6b505e675.pdf", "TL;DR": "We propose a new reinforcement learning algorithm based on zero order optimization, that we evaluate on StarCraft micromanagement scenarios.", "paperhash": "usunier|episodic_exploration_for_deep_deterministic_policies_for_starcraft_micromanagement", "conflicts": ["fb.com"], "keywords": ["Deep learning", "Reinforcement Learning", "Games"], "authors": ["Nicolas Usunier", "Gabriel Synnaeve", "Zeming Lin", "Soumith Chintala"], "authorids": ["usunier@fb.com", "gab@fb.com", "zlin@fb.com", "soumith@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959235301, "id": "ICLR.cc/2017/conference/-/paper519/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper519/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper519/AnonReviewer3", "ICLR.cc/2017/conference/paper519/AnonReviewer2"], "reply": {"forum": "r1LXit5ee", "replyto": "r1LXit5ee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper519/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper519/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959235301}}}], "count": 10}