{"notes": [{"id": "XG1Drw7VbLJ", "original": "tYPascUvLLi", "number": 1737, "cdate": 1601308191869, "ddate": null, "tcdate": 1601308191869, "tmdate": 1614985719584, "tddate": null, "forum": "XG1Drw7VbLJ", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Defining Benchmarks for Continual Few-Shot Learning", "authorids": ["~Antreas_Antoniou2", "~Massimiliano_Patacchiola1", "~Mateusz_Ochal1", "~Amos_Storkey1"], "authors": ["Antreas Antoniou", "Massimiliano Patacchiola", "Mateusz Ochal", "Amos Storkey"], "keywords": ["few-shot learning", "continual learning", "benchmark"], "abstract": "In recent years there has been substantial progress in few-shot learning, where a model is trained on a small labeled dataset related to a specific task, and in continual learning, where a model has to retain knowledge acquired on a sequence of datasets. Both of these fields are different abstractions of the same real world scenario, where a learner has to adapt to limited information from different changing sources and be able to generalize in and from each of them. Combining these two paradigms, where a model is trained on several sequential few-shot tasks, and then tested on a validation set stemming from all those tasks, helps by explicitly defining the competing requirements for both efficient integration and continuity. In this paper we propose such a setting, naming it Continual Few-Shot Learning (CFSL). We first define a theoretical framework for CFSL, then we propose a range of flexible benchmarks to unify the evaluation criteria. As part of the benchmark, we introduce a compact variant of ImageNet, called SlimageNet64, which retains all original 1000 classes but only contains 200 instances of each one (a total of 200K data-points) downscaled to 64 by 64 pixels. We provide baselines for the proposed benchmarks using a number of popular few-shot and continual learning methods, exposing previously unknown strengths and weaknesses of those algorithms. The dataloader and dataset will be released with an open-source license.", "one-sentence_summary": "The paper propose a benchmark for bridging the gap between few-shot and continual learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "antoniou|defining_benchmarks_for_continual_fewshot_learning", "supplementary_material": "/attachment/ea1c16d1968b9b0d78a8239a457a308f32ebc496.zip", "pdf": "/pdf/9d040a3a2bfb3fa9f9f1e9c64fe193c40efcbe90.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wFjeqqwzd", "_bibtex": "@misc{\nantoniou2021defining,\ntitle={Defining Benchmarks for Continual Few-Shot Learning},\nauthor={Antreas Antoniou and Massimiliano Patacchiola and Mateusz Ochal and Amos Storkey},\nyear={2021},\nurl={https://openreview.net/forum?id=XG1Drw7VbLJ}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "qW2WWvozscG", "original": null, "number": 1, "cdate": 1610040422355, "ddate": null, "tcdate": 1610040422355, "tmdate": 1610474021179, "tddate": null, "forum": "XG1Drw7VbLJ", "replyto": "XG1Drw7VbLJ", "invitation": "ICLR.cc/2021/Conference/Paper1737/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The reviewers appreciate the steps taken to combine continual learning with few-shot learning, this is an interesting intersection with many potential applications. However, the reviewers generally outlined a number of concerns with the benchmark and paper in its current form. They largely feel that this benchmark doesn\u2019t differentiate itself well enough from other incremental learning benchmarks, nor does it experiment with a wide enough variety of settings (additional episode configurations, more FSL/CL approaches). As such, it is difficult to determine at this point what major insights can be gained from this benchmark. I understand that there is a tradeoff: computation is limited, and there is merit in keeping things simple. However, the general consensus is that more work needs to be done in order to fully realize the potential of this benchmark.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defining Benchmarks for Continual Few-Shot Learning", "authorids": ["~Antreas_Antoniou2", "~Massimiliano_Patacchiola1", "~Mateusz_Ochal1", "~Amos_Storkey1"], "authors": ["Antreas Antoniou", "Massimiliano Patacchiola", "Mateusz Ochal", "Amos Storkey"], "keywords": ["few-shot learning", "continual learning", "benchmark"], "abstract": "In recent years there has been substantial progress in few-shot learning, where a model is trained on a small labeled dataset related to a specific task, and in continual learning, where a model has to retain knowledge acquired on a sequence of datasets. Both of these fields are different abstractions of the same real world scenario, where a learner has to adapt to limited information from different changing sources and be able to generalize in and from each of them. Combining these two paradigms, where a model is trained on several sequential few-shot tasks, and then tested on a validation set stemming from all those tasks, helps by explicitly defining the competing requirements for both efficient integration and continuity. In this paper we propose such a setting, naming it Continual Few-Shot Learning (CFSL). We first define a theoretical framework for CFSL, then we propose a range of flexible benchmarks to unify the evaluation criteria. As part of the benchmark, we introduce a compact variant of ImageNet, called SlimageNet64, which retains all original 1000 classes but only contains 200 instances of each one (a total of 200K data-points) downscaled to 64 by 64 pixels. We provide baselines for the proposed benchmarks using a number of popular few-shot and continual learning methods, exposing previously unknown strengths and weaknesses of those algorithms. The dataloader and dataset will be released with an open-source license.", "one-sentence_summary": "The paper propose a benchmark for bridging the gap between few-shot and continual learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "antoniou|defining_benchmarks_for_continual_fewshot_learning", "supplementary_material": "/attachment/ea1c16d1968b9b0d78a8239a457a308f32ebc496.zip", "pdf": "/pdf/9d040a3a2bfb3fa9f9f1e9c64fe193c40efcbe90.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wFjeqqwzd", "_bibtex": "@misc{\nantoniou2021defining,\ntitle={Defining Benchmarks for Continual Few-Shot Learning},\nauthor={Antreas Antoniou and Massimiliano Patacchiola and Mateusz Ochal and Amos Storkey},\nyear={2021},\nurl={https://openreview.net/forum?id=XG1Drw7VbLJ}\n}"}, "tags": [], "invitation": {"reply": {"forum": "XG1Drw7VbLJ", "replyto": "XG1Drw7VbLJ", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040422341, "tmdate": 1610474021163, "id": "ICLR.cc/2021/Conference/Paper1737/-/Decision"}}}, {"id": "lZegbEne9Qo", "original": null, "number": 7, "cdate": 1606181654249, "ddate": null, "tcdate": 1606181654249, "tmdate": 1606181654249, "tddate": null, "forum": "XG1Drw7VbLJ", "replyto": "FRzCq0rqL7v", "invitation": "ICLR.cc/2021/Conference/Paper1737/-/Official_Comment", "content": {"title": "Rebuttal Comment 2", "comment": ">> We do not restrict people to not use pretrained representation. Furthermore, setting C represents a situation where the model is learning a class of superclasses. For example, a learner starts learning only cars but then motorcycles and boats are added to the class. The model should be able to learn the general idea of human-made modes of transportation.\nSetting C has been previously used in the Continual Learning literature (Farquhar & Gal, 2018) and represents the rather common case where the number of total classes is not known in advance.\n\nFrom an implementation point of view, the proposed benchmark seems no different from a few newly defined mini-batch samplers for regular few-shot learning episodes. If it is no different from a few newly defined mini-batch samplers, then what prevents a model from storing all the examples in the episode? Since this is low-data, it doesn\u2019t seem to have a huge memory cost. Let\u2019s say for each class we store a fixed constant K exemplars, then the overall memory storage is still on the same order as ProtoNet, which stores a single mean vector per class. At the very least, the author should provide results for such a model, as an offline oracle.\n\n>>What stops them is the fact that we explicitly state that our benchmark prohibits that.\nNote that, even though the number of datapoints for each task is limited, the cost of storing those samples would grow linearly with the length of the episode becoming soon unmanageable. Note that, this is the first benchmark to carefully consider different types of memory metrics. We have explicitly defined various metrics to account for memory costs in Section 3.3 and we have performed several experiments (including on ProtoNet) which are reported in Appendix C.\n\nA lack of models evaluated.\na) If it can potentially include models that store some of the examples, then there seems no reason to exclude other baselines such as MatchingNet (Vinyals et al., 2016), infinite mixture prototypes (Allen et al., 2019), which can probably address the challenges that ProtoNet faces when dealing with type C sequences.\nb) There have also been a bunch of incremental class learning baselines that can be covered (which doesn\u2019t require meta-learning). For example, iCaRL (Rebuffi et al., 2017), LwF (Li & Hoiem, 2018), BiC (Wu et al., 2019), PODNet (Douillard et al., 2020). Currently I only see EWC as a pretty weak baseline in continual learning and it wasn\u2019t entirely designed for class incremental learning.\nPrevious comment on compute.\nc) I do notice that there are Pre. & Tune and EWC-Pre. baselines that use pretrained representation. This is good. However, it doesn\u2019t explain whether their failures are due to the sequential gradient descent steps or the lack of generalizability of the representation. I suspect it is probably the first case, and have the authors considered using pretrained representation and then using ProtoNet or some other FSL methods at test time?\n\n>>We agree that more baselines would be beneficial, however, due to compute constraints we can only add 1 or at most 2 more continual learning baselines. We are considering OML and iCaRL, but that can\u2019t happen within the review timeframe, but would be done should the paper be accepted.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1737/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1737/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defining Benchmarks for Continual Few-Shot Learning", "authorids": ["~Antreas_Antoniou2", "~Massimiliano_Patacchiola1", "~Mateusz_Ochal1", "~Amos_Storkey1"], "authors": ["Antreas Antoniou", "Massimiliano Patacchiola", "Mateusz Ochal", "Amos Storkey"], "keywords": ["few-shot learning", "continual learning", "benchmark"], "abstract": "In recent years there has been substantial progress in few-shot learning, where a model is trained on a small labeled dataset related to a specific task, and in continual learning, where a model has to retain knowledge acquired on a sequence of datasets. Both of these fields are different abstractions of the same real world scenario, where a learner has to adapt to limited information from different changing sources and be able to generalize in and from each of them. Combining these two paradigms, where a model is trained on several sequential few-shot tasks, and then tested on a validation set stemming from all those tasks, helps by explicitly defining the competing requirements for both efficient integration and continuity. In this paper we propose such a setting, naming it Continual Few-Shot Learning (CFSL). We first define a theoretical framework for CFSL, then we propose a range of flexible benchmarks to unify the evaluation criteria. As part of the benchmark, we introduce a compact variant of ImageNet, called SlimageNet64, which retains all original 1000 classes but only contains 200 instances of each one (a total of 200K data-points) downscaled to 64 by 64 pixels. We provide baselines for the proposed benchmarks using a number of popular few-shot and continual learning methods, exposing previously unknown strengths and weaknesses of those algorithms. The dataloader and dataset will be released with an open-source license.", "one-sentence_summary": "The paper propose a benchmark for bridging the gap between few-shot and continual learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "antoniou|defining_benchmarks_for_continual_fewshot_learning", "supplementary_material": "/attachment/ea1c16d1968b9b0d78a8239a457a308f32ebc496.zip", "pdf": "/pdf/9d040a3a2bfb3fa9f9f1e9c64fe193c40efcbe90.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wFjeqqwzd", "_bibtex": "@misc{\nantoniou2021defining,\ntitle={Defining Benchmarks for Continual Few-Shot Learning},\nauthor={Antreas Antoniou and Massimiliano Patacchiola and Mateusz Ochal and Amos Storkey},\nyear={2021},\nurl={https://openreview.net/forum?id=XG1Drw7VbLJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XG1Drw7VbLJ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1737/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1737/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1737/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1737/Authors|ICLR.cc/2021/Conference/Paper1737/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1737/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856286, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1737/-/Official_Comment"}}}, {"id": "us_5wv38RZc", "original": null, "number": 6, "cdate": 1606181636883, "ddate": null, "tcdate": 1606181636883, "tmdate": 1606181636883, "tddate": null, "forum": "XG1Drw7VbLJ", "replyto": "FRzCq0rqL7v", "invitation": "ICLR.cc/2021/Conference/Paper1737/-/Official_Comment", "content": {"title": "Rebuttal Comment 1", "comment": "Thank you for your time and effort in providing this review, especially given the instability of the current world-landscape.\n\nWeaknesses\nLimitations in the setup.\na) Although the paper motivates the application of continual few-shot learning, the proposed benchmark seems rather far away from application. The two applications mentioned in the paper are 1) online shopping for user preference and 2) human-robot interaction for task learning. Why not directly target these settings?\n\n>>In our opinion a benchmark represents an abstraction with respect to real world problems. We want to stress the fact that each one of the four settings we have described in the paper have been carefully built by considering the literature on few-shot and continual learning. Given the space constraints we could not detail the similarities between our tasks and previous work in the main body of the paper, but we included a detailed analysis in the supplementary material (Appendix D). Our benchmark is unifying two fields, and each task provides a setting for studying specific problems. Additional details about each task are given in the answers below. We are open to improve the clarity of the paper on this point, if the reviewer thinks this is necessary.\n\n\nb) Although the paper is named for defining the benchmark for continual few-shot learning, it is mainly targeting object classification, but not learning new tasks (which is motivated in the human-robot interaction).\n\n>> We used image few-shot learning as the general prototype of tasks and then defined different tasks based around the idea of different classes/samples that were to be learned in sequence. The motivating examples we have given are just possible use-cases of our benchmark; this represents an abstraction and can be easily reframed to tackle specific applications.\n\nc) The setups in Figure 2 don't seem very natural. Why wouldn\u2019t the model see any old classes in B? Why would the sequence completely overwrite all the class definitions in C? C doesn\u2019t fully capture another very interesting yet unexplored area of domain adaptation, e.g. change the image style from real to synthetic images over time. I can see it is partially addressed in OSAKA between meta-training and meta-test but not in the sequence.\n\n>> We have carefully defined the various setups by following the literature on few-shot and continual learning. Regarding setting B, the idea is to have a sort of \u201cimplicit context\u201d which can be presented multiple times to the learner. This is rather common in a continual setting, where the learner may be exposed to the same set of classes periodically through time. Setting C has been previously used in the Continual Learning literature (Farquhar & Gal, 2018). This setting corresponds to the shared-head condition mentioned by Farquhar & Gal (2018), which makes the task much more difficult since there is overlapping between classes. The motivation for this setting is to have a case that explicitly accounts for limits in the output space of the classifier, since the total number of classes may be an information that is not available in advance in a continual setting. We are keen to improve these points in the paper. Reference: Farquhar, S., & Gal, Y. (2018). \u201cTowards robust evaluations of continual learning\u201d. arXiv preprint arXiv:1805.09733.\n\n\nd) Why is the query set only happening at the very end but not in the middle of the sequence?\n\n>> We care about the performance of the system on previously unseen instances of all previously learned tasks. One could have multiple query sets after every support set, or at the middle as you suggested, but we decided to use the simplest variant which is the one most people care about. Since one can vary the number of support sets before evaluation, someone that cares about performance after only N tasks can achieve that by choosing a suitable NSS.\n\nA lack of new components in the benchmark. The benchmark seems very similar to previous benchmarks (e.g. class incremental learning and CORe50) with the main difference being low-data. According to Table 3, the only setting missing in CORe50 is setting C, which is a rather limited setting since it is extremely unnatural that we need to completely overwrite a label for another class (see my first comment). So what prevents people from using less number of images per class in CORe50, with a possibly pretrained representation from somewhere else, if they want to study a low-shot setting? Pretrained representation from supervised classification seems like a pretty robust approach for few-shot learning based on recent studies (Chen et al., 2019).\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1737/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1737/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defining Benchmarks for Continual Few-Shot Learning", "authorids": ["~Antreas_Antoniou2", "~Massimiliano_Patacchiola1", "~Mateusz_Ochal1", "~Amos_Storkey1"], "authors": ["Antreas Antoniou", "Massimiliano Patacchiola", "Mateusz Ochal", "Amos Storkey"], "keywords": ["few-shot learning", "continual learning", "benchmark"], "abstract": "In recent years there has been substantial progress in few-shot learning, where a model is trained on a small labeled dataset related to a specific task, and in continual learning, where a model has to retain knowledge acquired on a sequence of datasets. Both of these fields are different abstractions of the same real world scenario, where a learner has to adapt to limited information from different changing sources and be able to generalize in and from each of them. Combining these two paradigms, where a model is trained on several sequential few-shot tasks, and then tested on a validation set stemming from all those tasks, helps by explicitly defining the competing requirements for both efficient integration and continuity. In this paper we propose such a setting, naming it Continual Few-Shot Learning (CFSL). We first define a theoretical framework for CFSL, then we propose a range of flexible benchmarks to unify the evaluation criteria. As part of the benchmark, we introduce a compact variant of ImageNet, called SlimageNet64, which retains all original 1000 classes but only contains 200 instances of each one (a total of 200K data-points) downscaled to 64 by 64 pixels. We provide baselines for the proposed benchmarks using a number of popular few-shot and continual learning methods, exposing previously unknown strengths and weaknesses of those algorithms. The dataloader and dataset will be released with an open-source license.", "one-sentence_summary": "The paper propose a benchmark for bridging the gap between few-shot and continual learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "antoniou|defining_benchmarks_for_continual_fewshot_learning", "supplementary_material": "/attachment/ea1c16d1968b9b0d78a8239a457a308f32ebc496.zip", "pdf": "/pdf/9d040a3a2bfb3fa9f9f1e9c64fe193c40efcbe90.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wFjeqqwzd", "_bibtex": "@misc{\nantoniou2021defining,\ntitle={Defining Benchmarks for Continual Few-Shot Learning},\nauthor={Antreas Antoniou and Massimiliano Patacchiola and Mateusz Ochal and Amos Storkey},\nyear={2021},\nurl={https://openreview.net/forum?id=XG1Drw7VbLJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XG1Drw7VbLJ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1737/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1737/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1737/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1737/Authors|ICLR.cc/2021/Conference/Paper1737/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1737/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856286, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1737/-/Official_Comment"}}}, {"id": "L180LK-ekAX", "original": null, "number": 4, "cdate": 1606181426292, "ddate": null, "tcdate": 1606181426292, "tmdate": 1606181493166, "tddate": null, "forum": "XG1Drw7VbLJ", "replyto": "dPyk9VhHK1i", "invitation": "ICLR.cc/2021/Conference/Paper1737/-/Official_Comment", "content": {"title": "Rebuttal Comment 2", "comment": "(1) The benchmark has significant limitations:\nThe benchmark focuses on classification accuracy, memory usage, and computational complexity metrics, but has no metrics for other fundamental continual learning metrics such as positive forward transfer, positive backward transfer, forgetting over time, and intransigence (i.e. the inability of an algorithm to learn new tasks) [3,4]. These metrics would have to be measured within a task over the course of learning. The proposed benchmark only does evaluation at the end of a task, which is a limitation.\n\n>>We are in no way implying that others cannot discuss and report other evaluation measures and metrics while using the benchmark. However many of the other measures are hypothesised as important as proxies for understanding performance. However there is always some discussion as to whether such additional metrics are good measures. We believe the core evaluation of a benchmark should stick to evaluating the task at hand, while of course allowing any other metrics people may wish to compute.\n\n\nSection 3.1 implies that shot and way are fixed across support sets. This is a limitation as realistic few-shot and continual learning scenarios require varying shot and way, including some of the motivating scenarios in Section 1.1.\n\n>>Another fair point. We wanted our scenarios to be more structured and simple, both for computational feasibility and easier interpretations of the results. Future work can propose new benchmarks that contain a stochastic sampling of shot and way. Note that, those settings could be easily defined starting from our benchmark.\n\nCertain guidance about the benchmark is not mentioned. For example, what is the guidance on pre-training the learner via meta-learning or large-scale supervised classification on large datasets outside the benchmark? For example, [5] demonstrates promising continual learning results on a few-shot variant of split MNIST and split CIFAR using a few-shot learner that has been meta-trained on meta-dataset [2].\n\n>>Researchers are allowed to do whatever they want with their model training scheme as long as they are not training on the test set. If another dataset was used for pre-training, it should be stated in the relevant paper.\n\nThe 64 x 64 pixel image size choice in the SlimageNet64 dataset is short-sighted. I realize that the intention is to keep the dataset size small, but in the era of low-end smartphones producing 5 million or more pixel images and the ready availability of pre-trained networks at 224 x 224 pixels implies that the benchmark will not have longevity. \n\n>>Producing the table in our paper required 200+ GPU days with the 64x64 images. We want the benchmark to be applicable to people with just a few GPUs such as PhD students, and not only large corporations. That is why we used the 64x64 size. Someone with more compute is always free to create or use a larger dataset. \n\n(2) The experiments are limited:\nAll the experiments were carried out with one fixed configuration (5-way, 1-shot). It would be more revealing if a greater number of configurations were explored, including support sets with random shot and random way.\n\n>> An excellent point and suggestion. We appreciate the excellent suggestion of the reviewer. We will improve the presentations of the results in the tables following the reviewer\u2019s recommendations. The results for the 5-way, 1-shot case have taken 200+ GPU-days to be collected. There is a computational barrier in repeating the experiments for all the same set of conditions and methods.\n\n(3) Much detail required for reproducibility of the experiments is missing:\nNo detail is given on how the various approaches in the experiments utilize memory within a task. Section 3.3 says: \u201cMost learners will be compressing a given support set, but this is not strictly the case.\u201d and \u201c(e.g. embedding vectors in ProtoNets, and inner loop parameters for MAML)\u201d. It would be beneficial to describe precisely how memory is used in each of the methods.\n\n>>Those are described in the original papers. We could explicitly state them in our paper but unfortunately, our text space budget would not allow that. Given this emphasis by the reviewer we now include more details in the appendix.\n\nNo details were provided on how the baseline methods were carried out. For example, how was the fine tuning done in terms of network architecture, learning rate, optimization method, number of iterations, any memory that is used, were all weights in the network optimized or just the top layer, etc. The same goes for the other methods.\n\n>> We shall add those details in the paper. Due to space constraints we could not include those details in the main body of the paper and we had included some details in Appendix A. \n\nHow is |Gx| measured? i.e. Is it measured in its tensor form (i.e. 4-byte per channel float) or in its native form (1 unsigned byte per channel)?\n\n>> It is measured in its tensor form, we now clarity this in the paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1737/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1737/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defining Benchmarks for Continual Few-Shot Learning", "authorids": ["~Antreas_Antoniou2", "~Massimiliano_Patacchiola1", "~Mateusz_Ochal1", "~Amos_Storkey1"], "authors": ["Antreas Antoniou", "Massimiliano Patacchiola", "Mateusz Ochal", "Amos Storkey"], "keywords": ["few-shot learning", "continual learning", "benchmark"], "abstract": "In recent years there has been substantial progress in few-shot learning, where a model is trained on a small labeled dataset related to a specific task, and in continual learning, where a model has to retain knowledge acquired on a sequence of datasets. Both of these fields are different abstractions of the same real world scenario, where a learner has to adapt to limited information from different changing sources and be able to generalize in and from each of them. Combining these two paradigms, where a model is trained on several sequential few-shot tasks, and then tested on a validation set stemming from all those tasks, helps by explicitly defining the competing requirements for both efficient integration and continuity. In this paper we propose such a setting, naming it Continual Few-Shot Learning (CFSL). We first define a theoretical framework for CFSL, then we propose a range of flexible benchmarks to unify the evaluation criteria. As part of the benchmark, we introduce a compact variant of ImageNet, called SlimageNet64, which retains all original 1000 classes but only contains 200 instances of each one (a total of 200K data-points) downscaled to 64 by 64 pixels. We provide baselines for the proposed benchmarks using a number of popular few-shot and continual learning methods, exposing previously unknown strengths and weaknesses of those algorithms. The dataloader and dataset will be released with an open-source license.", "one-sentence_summary": "The paper propose a benchmark for bridging the gap between few-shot and continual learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "antoniou|defining_benchmarks_for_continual_fewshot_learning", "supplementary_material": "/attachment/ea1c16d1968b9b0d78a8239a457a308f32ebc496.zip", "pdf": "/pdf/9d040a3a2bfb3fa9f9f1e9c64fe193c40efcbe90.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wFjeqqwzd", "_bibtex": "@misc{\nantoniou2021defining,\ntitle={Defining Benchmarks for Continual Few-Shot Learning},\nauthor={Antreas Antoniou and Massimiliano Patacchiola and Mateusz Ochal and Amos Storkey},\nyear={2021},\nurl={https://openreview.net/forum?id=XG1Drw7VbLJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XG1Drw7VbLJ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1737/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1737/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1737/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1737/Authors|ICLR.cc/2021/Conference/Paper1737/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1737/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856286, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1737/-/Official_Comment"}}}, {"id": "HHiq_pLDY7B", "original": null, "number": 5, "cdate": 1606181480117, "ddate": null, "tcdate": 1606181480117, "tmdate": 1606181480117, "tddate": null, "forum": "XG1Drw7VbLJ", "replyto": "dPyk9VhHK1i", "invitation": "ICLR.cc/2021/Conference/Paper1737/-/Official_Comment", "content": {"title": "Rebuttal Comment 1", "comment": "Thank you for your time and effort in providing this review, especially given the instability of the current world-landscape.\n\nAll of your minor comments will be incorporated into the paper shortly."}, "signatures": ["ICLR.cc/2021/Conference/Paper1737/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1737/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defining Benchmarks for Continual Few-Shot Learning", "authorids": ["~Antreas_Antoniou2", "~Massimiliano_Patacchiola1", "~Mateusz_Ochal1", "~Amos_Storkey1"], "authors": ["Antreas Antoniou", "Massimiliano Patacchiola", "Mateusz Ochal", "Amos Storkey"], "keywords": ["few-shot learning", "continual learning", "benchmark"], "abstract": "In recent years there has been substantial progress in few-shot learning, where a model is trained on a small labeled dataset related to a specific task, and in continual learning, where a model has to retain knowledge acquired on a sequence of datasets. Both of these fields are different abstractions of the same real world scenario, where a learner has to adapt to limited information from different changing sources and be able to generalize in and from each of them. Combining these two paradigms, where a model is trained on several sequential few-shot tasks, and then tested on a validation set stemming from all those tasks, helps by explicitly defining the competing requirements for both efficient integration and continuity. In this paper we propose such a setting, naming it Continual Few-Shot Learning (CFSL). We first define a theoretical framework for CFSL, then we propose a range of flexible benchmarks to unify the evaluation criteria. As part of the benchmark, we introduce a compact variant of ImageNet, called SlimageNet64, which retains all original 1000 classes but only contains 200 instances of each one (a total of 200K data-points) downscaled to 64 by 64 pixels. We provide baselines for the proposed benchmarks using a number of popular few-shot and continual learning methods, exposing previously unknown strengths and weaknesses of those algorithms. The dataloader and dataset will be released with an open-source license.", "one-sentence_summary": "The paper propose a benchmark for bridging the gap between few-shot and continual learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "antoniou|defining_benchmarks_for_continual_fewshot_learning", "supplementary_material": "/attachment/ea1c16d1968b9b0d78a8239a457a308f32ebc496.zip", "pdf": "/pdf/9d040a3a2bfb3fa9f9f1e9c64fe193c40efcbe90.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wFjeqqwzd", "_bibtex": "@misc{\nantoniou2021defining,\ntitle={Defining Benchmarks for Continual Few-Shot Learning},\nauthor={Antreas Antoniou and Massimiliano Patacchiola and Mateusz Ochal and Amos Storkey},\nyear={2021},\nurl={https://openreview.net/forum?id=XG1Drw7VbLJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XG1Drw7VbLJ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1737/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1737/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1737/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1737/Authors|ICLR.cc/2021/Conference/Paper1737/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1737/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856286, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1737/-/Official_Comment"}}}, {"id": "rJlJPIzONJ2", "original": null, "number": 3, "cdate": 1606181283480, "ddate": null, "tcdate": 1606181283480, "tmdate": 1606181283480, "tddate": null, "forum": "XG1Drw7VbLJ", "replyto": "e7f96JGjpah", "invitation": "ICLR.cc/2021/Conference/Paper1737/-/Official_Comment", "content": {"title": "Rebuttal Comment", "comment": "Thank you for your time and effort in providing this review, especially given the instability of the current world-landscape.\n\nCons:\n\nTable 1 is very hard to read. Please consider either removing some entries to make it bigger (as some baselines perform very poorly), or using another way of presenting it.\n\n>>We have spent a lot of time trying to improve this table, and admittedly, it\u2019s not ideal yet. We\u2019ll try to revise, but we are not entirely sure what more can be done. Feel free to suggest specific ideas and we\u2019d be happy to look into them. We have considered breaking it up into two parts, one smaller for the main paper and one larger for the appendix, but that would mean omitting results that some readers would be interested in, from the main text.\n\nThe evaluation protocol has very little CL baselines, I'm not sure why that is. I don't consider EWC to be a good representative of CL methods, there should at least some replay algorithm. Also please consider adding OML [1] as a baseline, as it seems appropriate to this setting.\n\n>> We tried to choose a few representative methods from each setting, admittedly leaning more into simpler methods that also required less compute to benchmark. We agree that the addition of 1-2 more continual learning methods would be a good idea for our work. We\u2019ll work on adding the OML algorithm if this work is accepted since we do not have enough time until the review process is over.\n\nThe memory and MAC results are barely mentioned in the text. I understand this is hard due to space constraints, however a whole page is spent motivating it.\n\n>>Very good point. We have now added additional discussions on our memory and MAC results. However, as you have recognized, the availability of space is a problem.\n\n\nEdit : After reading the appendix, it is mentioned that each model is trained for 250 epochs of 500 steps, where each step is done on a single continual learning task. Here a CL task is a support set (or multiple support sets) from the same distribution ? I just want to double check that a given sample is **only seen during this single continual learning task **, i.e. once this task is over the sample is discarded. Moreover I want to double check that once a task / class is seen, it is never revisited again, even when changing epochs. I think this is the case, however my whole understanding of the paper rests on this assumption.\n\n>>This is an issue of the approach used to test the models on the benchmark, but not particularly an issue of the benchmark itself which is agnostic of the approach taken. Your understanding is mostly right. During a continual few-shot learning task, a given sample is only seen once and then discarded, as you noted. During meta-training, at each iteration, we randomly sample the classes in each support set in a manner consistent with the task. Then we randomly sample the individual data points (without replacement within the individual meta-training iteration) from the core dataset, conditioned on each class. Likewise the target dataset is always unseen data points.\n\n>>For a subsequent step of meta-training, this process is repeated. It is possible for the same data point to turn up a second time in a future meta-training iteration, though it is not a strong feature of the process - in SlimageNet64, for each class we are sampling from 200 samples, but there 1000 classes. So it is even rare for individual classes to turn up in a sequence, and even rarer for individual data points. We also do not consider this to be a big issue - it is much more important that there is no replacement within a run. Do let us know if there is some concern you have here that you would like us to address.\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1737/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1737/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defining Benchmarks for Continual Few-Shot Learning", "authorids": ["~Antreas_Antoniou2", "~Massimiliano_Patacchiola1", "~Mateusz_Ochal1", "~Amos_Storkey1"], "authors": ["Antreas Antoniou", "Massimiliano Patacchiola", "Mateusz Ochal", "Amos Storkey"], "keywords": ["few-shot learning", "continual learning", "benchmark"], "abstract": "In recent years there has been substantial progress in few-shot learning, where a model is trained on a small labeled dataset related to a specific task, and in continual learning, where a model has to retain knowledge acquired on a sequence of datasets. Both of these fields are different abstractions of the same real world scenario, where a learner has to adapt to limited information from different changing sources and be able to generalize in and from each of them. Combining these two paradigms, where a model is trained on several sequential few-shot tasks, and then tested on a validation set stemming from all those tasks, helps by explicitly defining the competing requirements for both efficient integration and continuity. In this paper we propose such a setting, naming it Continual Few-Shot Learning (CFSL). We first define a theoretical framework for CFSL, then we propose a range of flexible benchmarks to unify the evaluation criteria. As part of the benchmark, we introduce a compact variant of ImageNet, called SlimageNet64, which retains all original 1000 classes but only contains 200 instances of each one (a total of 200K data-points) downscaled to 64 by 64 pixels. We provide baselines for the proposed benchmarks using a number of popular few-shot and continual learning methods, exposing previously unknown strengths and weaknesses of those algorithms. The dataloader and dataset will be released with an open-source license.", "one-sentence_summary": "The paper propose a benchmark for bridging the gap between few-shot and continual learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "antoniou|defining_benchmarks_for_continual_fewshot_learning", "supplementary_material": "/attachment/ea1c16d1968b9b0d78a8239a457a308f32ebc496.zip", "pdf": "/pdf/9d040a3a2bfb3fa9f9f1e9c64fe193c40efcbe90.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wFjeqqwzd", "_bibtex": "@misc{\nantoniou2021defining,\ntitle={Defining Benchmarks for Continual Few-Shot Learning},\nauthor={Antreas Antoniou and Massimiliano Patacchiola and Mateusz Ochal and Amos Storkey},\nyear={2021},\nurl={https://openreview.net/forum?id=XG1Drw7VbLJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XG1Drw7VbLJ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1737/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1737/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1737/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1737/Authors|ICLR.cc/2021/Conference/Paper1737/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1737/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856286, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1737/-/Official_Comment"}}}, {"id": "CzKqEi5bO39", "original": null, "number": 2, "cdate": 1606181227972, "ddate": null, "tcdate": 1606181227972, "tmdate": 1606181227972, "tddate": null, "forum": "XG1Drw7VbLJ", "replyto": "u4viPbfkYTC", "invitation": "ICLR.cc/2021/Conference/Paper1737/-/Official_Comment", "content": {"title": "Rebuttal Comment", "comment": "Thank you for your time and effort in providing this review, especially given the instability of the current world-landscape. \n\nReply to Reviewer #4:\n\nThe overall idea is interesting (few-shot + sequential observations); however, it's not clear should one take home after reading this draft. The conclusions seem intuitive and reasonable, but leave the reader with questions about the main findings of the work. \n\n>>We thank the reviewer for showing their interest in the overall idea. We are keen to improve the clarity of the paper, we would appreciate it if the reviewer could provide more detailed feedback on this point to facilitate our work.\n\nI would take issue with the way the word \"continual learning\" is used. In practice, the author(s) use existing datasets where the instances arrive in a sequential manner (streaming observations). However, this is not quite what they motivate: \"Consider a user in a fast-changing environment who must learn from the many scenarios that are encountered.\" since, in the described sequential setting all the instances belong to the same underlying distribution (the original dataset), even though they're observed sequentially.\n\n>>Our benchmark provides a wide range of task types, some of which explicitly \u2018overwrite\u2019 classes, therefore ensuring that the distribution changes as learning is happening. Furthermore, even in the non overwriting settings, since the examples are observed in a single-shot manner, each observed support set will usually contain the same class in a very different situation, i.e. dog in park vs dog on the pier vs dog on the floor at a house. Furthermore, it\u2019s worth stressing that our benchmarks remain a bit more abstract, and general in order to establish generalization metrics that can be used to judge the performance of the system in a wide variety of contexts. Then, once a model is judged on a general benchmark, it can be used in a specific application setting to establish a more specific, application-directed generalization metric. In a sense, we want benchmarks to be general.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1737/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1737/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defining Benchmarks for Continual Few-Shot Learning", "authorids": ["~Antreas_Antoniou2", "~Massimiliano_Patacchiola1", "~Mateusz_Ochal1", "~Amos_Storkey1"], "authors": ["Antreas Antoniou", "Massimiliano Patacchiola", "Mateusz Ochal", "Amos Storkey"], "keywords": ["few-shot learning", "continual learning", "benchmark"], "abstract": "In recent years there has been substantial progress in few-shot learning, where a model is trained on a small labeled dataset related to a specific task, and in continual learning, where a model has to retain knowledge acquired on a sequence of datasets. Both of these fields are different abstractions of the same real world scenario, where a learner has to adapt to limited information from different changing sources and be able to generalize in and from each of them. Combining these two paradigms, where a model is trained on several sequential few-shot tasks, and then tested on a validation set stemming from all those tasks, helps by explicitly defining the competing requirements for both efficient integration and continuity. In this paper we propose such a setting, naming it Continual Few-Shot Learning (CFSL). We first define a theoretical framework for CFSL, then we propose a range of flexible benchmarks to unify the evaluation criteria. As part of the benchmark, we introduce a compact variant of ImageNet, called SlimageNet64, which retains all original 1000 classes but only contains 200 instances of each one (a total of 200K data-points) downscaled to 64 by 64 pixels. We provide baselines for the proposed benchmarks using a number of popular few-shot and continual learning methods, exposing previously unknown strengths and weaknesses of those algorithms. The dataloader and dataset will be released with an open-source license.", "one-sentence_summary": "The paper propose a benchmark for bridging the gap between few-shot and continual learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "antoniou|defining_benchmarks_for_continual_fewshot_learning", "supplementary_material": "/attachment/ea1c16d1968b9b0d78a8239a457a308f32ebc496.zip", "pdf": "/pdf/9d040a3a2bfb3fa9f9f1e9c64fe193c40efcbe90.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wFjeqqwzd", "_bibtex": "@misc{\nantoniou2021defining,\ntitle={Defining Benchmarks for Continual Few-Shot Learning},\nauthor={Antreas Antoniou and Massimiliano Patacchiola and Mateusz Ochal and Amos Storkey},\nyear={2021},\nurl={https://openreview.net/forum?id=XG1Drw7VbLJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XG1Drw7VbLJ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1737/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1737/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1737/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1737/Authors|ICLR.cc/2021/Conference/Paper1737/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1737/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856286, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1737/-/Official_Comment"}}}, {"id": "FRzCq0rqL7v", "original": null, "number": 1, "cdate": 1603507448481, "ddate": null, "tcdate": 1603507448481, "tmdate": 1605024369812, "tddate": null, "forum": "XG1Drw7VbLJ", "replyto": "XG1Drw7VbLJ", "invitation": "ICLR.cc/2021/Conference/Paper1737/-/Official_Review", "content": {"title": "Review", "review": "----------------------------------\n**Summary**\n\nThis paper proposes a benchmark for a new task called continual few-shot learning. The benchmark is based on the ImageNet dataset. Basically, the model looks at a part of the support set one after another sequentially, and it is then evaluated on the query set that contains balanced samples from each part of the support set. Under the benchmark, there are four types of challenges, which differs in how they sub-partition the support set. A suite of models have been run and evaluated, including MAML, ProtoNet and SCA. SCA is found to have the best performance, whereas ProtoNet is found to be the most resource efficient.\n\n----------------------------------\n**Strengths**\n\n1. I agree that continual few-shot learning is a very useful way of framing few-shot learning problems, especially in the domain of general purpose robotics.\n\n2. Well defined metrics that consider both accuracy and computation resources.\n\n3. There is a significant amount of effort in terms of defining four different scenarios and coming up with another split of the ImageNet dataset and running a number of models on the benchmark.\n\n----------------------------------\n**Weaknesses**\n\n1. Limitations in the setup. \n\n    a) Although the paper motivates the application of continual few-shot learning, the proposed benchmark seems rather far away from application. The two applications mentioned in the paper are 1) online shopping for user preference and 2) human-robot interaction for task learning. Why not directly target these settings? \n\n    b) Although the paper is named for defining the benchmark for continual few-shot learning, it is mainly targeting object classification, but not learning new tasks (which is motivated in the human-robot interaction). \n\n    c) The setups in Figure 2 don't seem very natural. Why wouldn\u2019t the model see any old classes in B? Why would the sequence completely overwrite all the class definitions in C? C doesn\u2019t fully capture another very interesting yet unexplored area of domain adaptation, e.g. change the image style from real to synthetic images over time. I can see it is partially addressed in OSAKA between meta-training and meta-test but not in the sequence.\n\n    d) Why is the query set only happening at the very end but not in the middle of the sequence?\n\n2. A lack of new components in the benchmark. The benchmark seems very similar to previous benchmarks (e.g. class incremental learning and CORe50) with the main difference being low-data. According to Table 3, the only setting missing in CORe50 is setting C, which is a rather limited setting since it is extremely unnatural that we need to completely overwrite a label for another class (see my first comment). So what prevents people from using less number of images per class in CORe50, with a possibly pretrained representation from somewhere else, if they want to study a low-shot setting? Pretrained representation from supervised classification seems like a pretty robust approach for few-shot learning based on recent studies (Chen et al., 2019).\n\n3. From an implementation point of view, the proposed benchmark seems no different from a few newly defined mini-batch samplers for regular few-shot learning episodes. If it is no different from a few newly defined mini-batch samplers, then what prevents a model from storing all the examples in the episode? Since this is low-data, it doesn\u2019t seem to have a huge memory cost. Let\u2019s say for each class we store a fixed constant K exemplars, then the overall memory storage is still on the same order as ProtoNet, which stores a single mean vector per class. At the very least, the author should provide results for such a model, as an offline oracle.\n\n4. A lack of models evaluated. \n\n    a) If it can potentially include models that store some of the examples, then there seems no reason to exclude other baselines such as MatchingNet (Vinyals et al., 2016), infinite mixture prototypes (Allen et al., 2019), which can probably address the challenges that ProtoNet faces when dealing with type C sequences. \n\n    b) There have also been a bunch of incremental class learning baselines that can be covered (which doesn\u2019t require meta-learning). For example, iCaRL (Rebuffi et al., 2017), LwF (Li & Hoiem, 2018), BiC (Wu et al., 2019), PODNet (Douillard et al., 2020). Currently I only see EWC as a pretty weak baseline in continual learning and it wasn\u2019t entirely designed for class incremental learning.\n\n    c) I do notice that there are Pre. & Tune and EWC-Pre. baselines that use pretrained representation. This is good. However, it doesn\u2019t explain whether their failures are due to the sequential gradient descent steps or the lack of generalizability of the representation. I suspect it is probably the first case, and have the authors considered using pretrained representation and then using ProtoNet or some other FSL methods at test time?\n\n----------------------------------\n**Minor comments**\n\n1. 64x64 seems rather small resolution. I would suggest at least 84x84 for ImageNet images.\n2. The memory utilization results should go into the main paper, along with other baselines I suggested that use some example-based storage.\n3. Section 2.3, Page 4, Line 13: thie line -> this line.\n\n----------------------------------\n**Conclusion**\n\nIn conclusion, based on the weaknesses I mentioned above, my score is 5. The paper is one step towards a more useful type of few-shot learning, and I am glad to see the field progresses towards it and I appreciate the effort that the authors have contributed. However, the way it samples new classes and the datasets that the paper study are still in a very limited sense, which prevent the paper from getting direct applications. On the other hand, it is very similar to some other incremental class learning benchmarks and it is not clear what different kinds of conclusions we can draw (e.g. different models that may work better) by using this benchmark instead of using previous ones. Therefore I am leaning towards rejection.\n\n----------------------------------\n**References**\n\n- Li, Zhizhong and Hoiem, Derek. Learning without forgetting. In ECCV 2016.\n- Vinyals, Oriol, Blundell, Charles, Lillicrap, timothy, Kavukcuoglu, Koray and Wierstra, Daan. Matching networks for one shot learning. In NIPS 2016.\n- Rebuffi, Sylvestre-Alvise, Kolesnikov, Alexander, Sperlm, Georg and Lampert, Christoph H. iCaRL: Incremental classifier and representation learning. In CVPR 2017.\n- Chen, Wei-Yu, Liu, Yen-Cheng, Kira, Zsolt, Wang, Yu-Chiang and  Huang, Jia-Bin. A closer look at few-shot classification. In ICLR 2019.\n- Wu, Yue, Chen, Yinpeng, Wang, Lijuan, Ye, Yuancheng, Liu, Zicheng, Guo, Yandong and Fu, Yun. Large scale incremental learning. In CVPR 2019.\n- Allenn, Kelsey R., Shelhamer, Evan, Shin, Hanul and Tenenbaum, Joshua B. Infinite mixture prototypes for few-shot learning. In ICML 2019.\n- Douillard, Arthur, Cord, Matthieu, Ollion, Charles, Robert, Thomas and Valle, Eduardo. PODNet: Pooled outputs distillation for small-tasks incremental learning. In ECCV 2020.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1737/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1737/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defining Benchmarks for Continual Few-Shot Learning", "authorids": ["~Antreas_Antoniou2", "~Massimiliano_Patacchiola1", "~Mateusz_Ochal1", "~Amos_Storkey1"], "authors": ["Antreas Antoniou", "Massimiliano Patacchiola", "Mateusz Ochal", "Amos Storkey"], "keywords": ["few-shot learning", "continual learning", "benchmark"], "abstract": "In recent years there has been substantial progress in few-shot learning, where a model is trained on a small labeled dataset related to a specific task, and in continual learning, where a model has to retain knowledge acquired on a sequence of datasets. Both of these fields are different abstractions of the same real world scenario, where a learner has to adapt to limited information from different changing sources and be able to generalize in and from each of them. Combining these two paradigms, where a model is trained on several sequential few-shot tasks, and then tested on a validation set stemming from all those tasks, helps by explicitly defining the competing requirements for both efficient integration and continuity. In this paper we propose such a setting, naming it Continual Few-Shot Learning (CFSL). We first define a theoretical framework for CFSL, then we propose a range of flexible benchmarks to unify the evaluation criteria. As part of the benchmark, we introduce a compact variant of ImageNet, called SlimageNet64, which retains all original 1000 classes but only contains 200 instances of each one (a total of 200K data-points) downscaled to 64 by 64 pixels. We provide baselines for the proposed benchmarks using a number of popular few-shot and continual learning methods, exposing previously unknown strengths and weaknesses of those algorithms. The dataloader and dataset will be released with an open-source license.", "one-sentence_summary": "The paper propose a benchmark for bridging the gap between few-shot and continual learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "antoniou|defining_benchmarks_for_continual_fewshot_learning", "supplementary_material": "/attachment/ea1c16d1968b9b0d78a8239a457a308f32ebc496.zip", "pdf": "/pdf/9d040a3a2bfb3fa9f9f1e9c64fe193c40efcbe90.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wFjeqqwzd", "_bibtex": "@misc{\nantoniou2021defining,\ntitle={Defining Benchmarks for Continual Few-Shot Learning},\nauthor={Antreas Antoniou and Massimiliano Patacchiola and Mateusz Ochal and Amos Storkey},\nyear={2021},\nurl={https://openreview.net/forum?id=XG1Drw7VbLJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "XG1Drw7VbLJ", "replyto": "XG1Drw7VbLJ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1737/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538111797, "tmdate": 1606915778779, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1737/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1737/-/Official_Review"}}}, {"id": "dPyk9VhHK1i", "original": null, "number": 2, "cdate": 1603525060336, "ddate": null, "tcdate": 1603525060336, "tmdate": 1605024369739, "tddate": null, "forum": "XG1Drw7VbLJ", "replyto": "XG1Drw7VbLJ", "invitation": "ICLR.cc/2021/Conference/Paper1737/-/Official_Review", "content": {"title": "Review for \"Defining Benchmarks for Continual Few-Shot Learning\"", "review": "This paper proposes a new machine learning setting called \u201cContinual Few-Shot Learning\u201d which fuses the up until now disparate paradigms of continual learning and few-shot learning. To evaluate methods in this new setting, a new benchmark and dataset called SlimageNet64 are defined. Various methods are evaluated on the new benchmark establishing a set of baseline results for the new setting.\n\n**Pros:**   \n- The idea of fusing the previously separate domains of few-shot learning and continual learning in the form of a new benchmark is fantastic.\n- The benchmarks include metrics for memory usage and computational complexity which are critical in practice and almost always omitted in other benchmarks.\n- The paper is well written and straightforward to understand.\n\n**Concerns:**\n\n(1) The benchmark has significant limitations:\n- The benchmark focuses on classification accuracy, memory usage, and computational complexity metrics, but has no metrics for other fundamental continual learning metrics such as positive forward transfer, positive backward transfer, forgetting over time, and intransigence (i.e. the inability of an algorithm to learn new tasks) [3,4]. These metrics would have to be measured within a task over the course of learning. The proposed benchmark only does evaluation at the end of a task, which is a limitation.\n- Section 3.1 implies that shot and way are fixed across support sets. This is a limitation as realistic few-shot and continual learning scenarios require varying shot and way, including some of the motivating scenarios in Section 1.1.\n- Certain guidance about the benchmark is not mentioned. For example, what is the guidance on pre-training the learner via meta-learning or large-scale supervised classification on large datasets outside the benchmark? For example, [5] demonstrates promising continual learning results on a few-shot variant of split MNIST and split CIFAR using a few-shot learner that has been meta-trained on meta-dataset [2].\n- The 64 x 64 pixel image size choice in the SlimageNet64 dataset is short-sighted. I realize that the intention is to keep the dataset size small, but in the era of low-end smartphones producing 5 million or more pixel images and the ready availability of pre-trained networks at 224 x 224 pixels implies that the benchmark will not have longevity. The images should be available at their natural size and the allow the learner to scale the images down if they are operating in a constrained computational environment.\n\n(2) The experiments are limited:\n- All the experiments were carried out with one fixed configuration (5-way, 1-shot). It would be more revealing if a greater number of configurations were explored, including support sets with random shot and random way.\n- The tasks consisted of a very small number of support sets (NSS between 3 and 10) in the accuracy experiments. However, the memory cost tests used NSS=640 (a more realistic number), why not report accuracy results on those runs?\n- It is peculiar to compare the accuracy of various methods that use very different network backbones in the same table without clearly delineating the differences in the table (i.e. with a horizontal line or column indicating the backbone). I realize that the purpose of these experiments is to establish baselines with various types and levels of learners but the fact that SCA is bolded as the best in the accuracy tables implies that there is a direct (and unfair) comparison being made between methods that should be made within several categories (say low, medium, and high tiers). In any case, dividing up the table into categories and adding detail on architectures used for the various methods would make the presentation of the results more transparent and insightful.\n\n(3) Much detail required for reproducibility of the experiments is missing:\n- No detail is given on how the various approaches in the experiments utilize memory within a task. Section 3.3 says: \u201cMost learners will be compressing a given support set, but this is not strictly the case.\u201d and \u201c(e.g. embedding vectors in ProtoNets, and inner loop parameters for MAML)\u201d. It would be beneficial to describe precisely how memory is used in each of the methods.\n- No details were provided on how the baseline methods were carried out. For example, how was the fine tuning done in terms of network architecture, learning rate, optimization method, number of iterations, any memory that is used, were all weights in the network optimized or just the top layer, etc. The same goes for the other methods.\n- How is $|\\mathcal{G}^x|$ measured? i.e. Is it measured in its tensor form (i.e. 4-byte per channel float) or in its native form (1 unsigned byte per channel)?\n\n**Minor Comments:**  \n- In the introduction, you should mention meta-dataset [2] along with the other few-shot learning benchmarks listed as it is arguably the most challenging/meaningful one at the moment.\n- Section 2.1 should cite [1] as a thorough few-shot learning survey.\n- Section 3.2, the acronym NI is used in the sentence before it is defined.\n- In Section 3.3, Multiply-Addition operations(MACs) paragraph, it says \u201c\u2026it measures the memory footprint that the model\u201d. Should it be something like \u201c\u2026computational complexity of the model\u201d?\n-  Section 3.3, 1st sentence \u2013 should it say: \u201call the task types of interest.\u201d instead of \u201call the tasks of interest\u201d?\n- Should the title of Figure 1 be \u201cHigh level overview of a CFSL task\u201d instead of \u201cHigh level overview of the proposed benchmark\u201d? i.e. \u2018benchmark\u2019 is a bit too general as there is no other mention of key parts of the benchmark including memory and computational complexity metrics.\n\n**References:**  \n[1] Hospedales, Timothy, et al. \"Meta-learning in neural networks: A survey.\" arXiv preprint arXiv:2004.05439 (2020).  \n[2] Triantafillou, Eleni, et al. \"Meta-dataset: A dataset of datasets for learning to learn from few examples.\" arXiv preprint arXiv:1903.03096 (2019).  \n[3] Schwarz, Jonathan, et al. \"Progress & compress: A scalable framework for continual learning.\" arXiv preprint arXiv:1805.06370 (2018).  \n[4] Chaudhry, Arslan, et al. \"Riemannian walk for incremental learning: Understanding forgetting and intransigence.\" Proceedings of the European Conference on Computer Vision (ECCV). 2018.  \n[5] Requeima, James, et al. \"Fast and flexible multi-task classification using conditional neural adaptive processes.\" Advances in Neural Information Processing Systems. 2019.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1737/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1737/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defining Benchmarks for Continual Few-Shot Learning", "authorids": ["~Antreas_Antoniou2", "~Massimiliano_Patacchiola1", "~Mateusz_Ochal1", "~Amos_Storkey1"], "authors": ["Antreas Antoniou", "Massimiliano Patacchiola", "Mateusz Ochal", "Amos Storkey"], "keywords": ["few-shot learning", "continual learning", "benchmark"], "abstract": "In recent years there has been substantial progress in few-shot learning, where a model is trained on a small labeled dataset related to a specific task, and in continual learning, where a model has to retain knowledge acquired on a sequence of datasets. Both of these fields are different abstractions of the same real world scenario, where a learner has to adapt to limited information from different changing sources and be able to generalize in and from each of them. Combining these two paradigms, where a model is trained on several sequential few-shot tasks, and then tested on a validation set stemming from all those tasks, helps by explicitly defining the competing requirements for both efficient integration and continuity. In this paper we propose such a setting, naming it Continual Few-Shot Learning (CFSL). We first define a theoretical framework for CFSL, then we propose a range of flexible benchmarks to unify the evaluation criteria. As part of the benchmark, we introduce a compact variant of ImageNet, called SlimageNet64, which retains all original 1000 classes but only contains 200 instances of each one (a total of 200K data-points) downscaled to 64 by 64 pixels. We provide baselines for the proposed benchmarks using a number of popular few-shot and continual learning methods, exposing previously unknown strengths and weaknesses of those algorithms. The dataloader and dataset will be released with an open-source license.", "one-sentence_summary": "The paper propose a benchmark for bridging the gap between few-shot and continual learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "antoniou|defining_benchmarks_for_continual_fewshot_learning", "supplementary_material": "/attachment/ea1c16d1968b9b0d78a8239a457a308f32ebc496.zip", "pdf": "/pdf/9d040a3a2bfb3fa9f9f1e9c64fe193c40efcbe90.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wFjeqqwzd", "_bibtex": "@misc{\nantoniou2021defining,\ntitle={Defining Benchmarks for Continual Few-Shot Learning},\nauthor={Antreas Antoniou and Massimiliano Patacchiola and Mateusz Ochal and Amos Storkey},\nyear={2021},\nurl={https://openreview.net/forum?id=XG1Drw7VbLJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "XG1Drw7VbLJ", "replyto": "XG1Drw7VbLJ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1737/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538111797, "tmdate": 1606915778779, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1737/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1737/-/Official_Review"}}}, {"id": "e7f96JGjpah", "original": null, "number": 3, "cdate": 1603853051589, "ddate": null, "tcdate": 1603853051589, "tmdate": 1605024369668, "tddate": null, "forum": "XG1Drw7VbLJ", "replyto": "XG1Drw7VbLJ", "invitation": "ICLR.cc/2021/Conference/Paper1737/-/Official_Review", "content": {"title": "Paper Review", "review": "Summary\n--------------------\nThe authors propose a new evaluation protocol which generalizes continual learning and few-shot learning. By controlling the 1) number of support sets per task and 2) the rate at witch tasks change, the authors can span a wide variety of settings previously explored in the literature. \nThe authors proceed to evaluate several meta learning algorithms under this new protocol and provide a detailed analysis of the results.\n\n\nPros\n-------------------\n+ The overall evaluation protocol is well explained. Figure 2 is a beautiful summary of it. \n+ The authors monitor both memory and computation. To me this is one of the key novelties of the paper : it's often overlooked in the literature, yet is very important as one can solve catastrophic forgetting given enough compute\n+ The authors release clear and concise code (and installation instructions). This is a great first step to encourage other practitioners to use this dataset\n+ The results give interesting insights into popular few-shot learning methods like MAML and ProtoNet\n\nCons\n-------------------\n- Table 1 is very hard to read. Please consider either removing some entries to make it bigger (as some baselines perform very poorly), or using another way of presenting it. \n- The evaluation protocol has very little CL baselines, I'm not sure why that is. I don't consider EWC to be a good representative of CL methods, there should at least some replay algorithm. Also please consider adding OML [1] as a baseline, as it seems appropriate to this setting. \n- The memory and MAC results are barely mentioned in the text. I understand this is hard due to space constraints, however a whole page is spent motivating it. \n\nSmall suggestion : The paper would be stronger if it had a wide range of baselines, making it also an empirical study. You constructed a nice benchmark, please show us how (more) known methods perform in it! I'm willing to increase my score if this request is met. \n\nEdit : After reading the appendix, it is mentioned that each model is trained for 250 epochs of 500 steps, where each step is done on a single continual learning task. Here a CL task is a support set (or multiple support sets) from the same distribution ? I just want to double check that a given sample is **only seen during this single continual learning task **, i.e. once this task is over the sample is discarded. Moreover I want to double check that once a task / class is seen, it is never revisited again, even when changing epochs.  I think this is the case, however my whole understanding of the paper rests on this assumption. \n\n\n[1] Javed, Khurram, and Martha White. \"Meta-learning representations for continual learning.\" Advances in Neural Information Processing Systems. 2019.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1737/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1737/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defining Benchmarks for Continual Few-Shot Learning", "authorids": ["~Antreas_Antoniou2", "~Massimiliano_Patacchiola1", "~Mateusz_Ochal1", "~Amos_Storkey1"], "authors": ["Antreas Antoniou", "Massimiliano Patacchiola", "Mateusz Ochal", "Amos Storkey"], "keywords": ["few-shot learning", "continual learning", "benchmark"], "abstract": "In recent years there has been substantial progress in few-shot learning, where a model is trained on a small labeled dataset related to a specific task, and in continual learning, where a model has to retain knowledge acquired on a sequence of datasets. Both of these fields are different abstractions of the same real world scenario, where a learner has to adapt to limited information from different changing sources and be able to generalize in and from each of them. Combining these two paradigms, where a model is trained on several sequential few-shot tasks, and then tested on a validation set stemming from all those tasks, helps by explicitly defining the competing requirements for both efficient integration and continuity. In this paper we propose such a setting, naming it Continual Few-Shot Learning (CFSL). We first define a theoretical framework for CFSL, then we propose a range of flexible benchmarks to unify the evaluation criteria. As part of the benchmark, we introduce a compact variant of ImageNet, called SlimageNet64, which retains all original 1000 classes but only contains 200 instances of each one (a total of 200K data-points) downscaled to 64 by 64 pixels. We provide baselines for the proposed benchmarks using a number of popular few-shot and continual learning methods, exposing previously unknown strengths and weaknesses of those algorithms. The dataloader and dataset will be released with an open-source license.", "one-sentence_summary": "The paper propose a benchmark for bridging the gap between few-shot and continual learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "antoniou|defining_benchmarks_for_continual_fewshot_learning", "supplementary_material": "/attachment/ea1c16d1968b9b0d78a8239a457a308f32ebc496.zip", "pdf": "/pdf/9d040a3a2bfb3fa9f9f1e9c64fe193c40efcbe90.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wFjeqqwzd", "_bibtex": "@misc{\nantoniou2021defining,\ntitle={Defining Benchmarks for Continual Few-Shot Learning},\nauthor={Antreas Antoniou and Massimiliano Patacchiola and Mateusz Ochal and Amos Storkey},\nyear={2021},\nurl={https://openreview.net/forum?id=XG1Drw7VbLJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "XG1Drw7VbLJ", "replyto": "XG1Drw7VbLJ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1737/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538111797, "tmdate": 1606915778779, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1737/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1737/-/Official_Review"}}}, {"id": "u4viPbfkYTC", "original": null, "number": 4, "cdate": 1603941364615, "ddate": null, "tcdate": 1603941364615, "tmdate": 1605024369597, "tddate": null, "forum": "XG1Drw7VbLJ", "replyto": "XG1Drw7VbLJ", "invitation": "ICLR.cc/2021/Conference/Paper1737/-/Official_Review", "content": {"title": "A benchmak for continual few-shot learning ", "review": "The work proposes Continual Few-Shot learning -- a setting to study tasks (1) with a small labeled dataset, and (2) retain knowledge acquired on a sequence of instances.  Additionally, the authors build a compact variant of ImageNet which retains all original 1000 classes but only contains 200 instances of each one (a total of 200K data-points) downscaled to 64 \u00d7 64 pixels. \n\nBy evaluating baselines on the proposed benchmark, the authors observe that embedding-based models tend to perform better when \"incoming tasks contain different classes from one another\" and gradient-based methods tend to perform better when the task classes \"form super-classes of randomly combined categories.\" \n\nThe overall idea is interesting (few-shot + sequential observations); however, it's not clear should one take home after reading this draft. The conclusions seem intuitive and reasonable, but leave the reader with questions about the main findings of the work. \n\nI would take issue with the way the word \"continual learning\" is used. In practice, the author(s) use existing datasets where the instances arrive in a sequential manner (streaming observations). However, this is not quite what they motivate: \"Consider a user in a fast-changing environment who must learn from the many scenarios that are encountered.\" since, in the described sequential setting all the instances belong to the same underlying distribution (the original dataset), even though they're observed sequentially.  \n\nNote that a realistic temporal observations are much more challenging (e.g., the language of search queries over time because of the change in the functionality of search engines, or the changing distribution of images over time because of various social changes.)\n\nThe overall direction is promising: clearly, we need to move towards more data-efficiency and build better frameworks for measuring the generalization of our models. However, I am not convinced if the presented work is a significant step toward that goal (or, at least, I don't see it). Happy to change my mind, if I am missing anything. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1737/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1737/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defining Benchmarks for Continual Few-Shot Learning", "authorids": ["~Antreas_Antoniou2", "~Massimiliano_Patacchiola1", "~Mateusz_Ochal1", "~Amos_Storkey1"], "authors": ["Antreas Antoniou", "Massimiliano Patacchiola", "Mateusz Ochal", "Amos Storkey"], "keywords": ["few-shot learning", "continual learning", "benchmark"], "abstract": "In recent years there has been substantial progress in few-shot learning, where a model is trained on a small labeled dataset related to a specific task, and in continual learning, where a model has to retain knowledge acquired on a sequence of datasets. Both of these fields are different abstractions of the same real world scenario, where a learner has to adapt to limited information from different changing sources and be able to generalize in and from each of them. Combining these two paradigms, where a model is trained on several sequential few-shot tasks, and then tested on a validation set stemming from all those tasks, helps by explicitly defining the competing requirements for both efficient integration and continuity. In this paper we propose such a setting, naming it Continual Few-Shot Learning (CFSL). We first define a theoretical framework for CFSL, then we propose a range of flexible benchmarks to unify the evaluation criteria. As part of the benchmark, we introduce a compact variant of ImageNet, called SlimageNet64, which retains all original 1000 classes but only contains 200 instances of each one (a total of 200K data-points) downscaled to 64 by 64 pixels. We provide baselines for the proposed benchmarks using a number of popular few-shot and continual learning methods, exposing previously unknown strengths and weaknesses of those algorithms. The dataloader and dataset will be released with an open-source license.", "one-sentence_summary": "The paper propose a benchmark for bridging the gap between few-shot and continual learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "antoniou|defining_benchmarks_for_continual_fewshot_learning", "supplementary_material": "/attachment/ea1c16d1968b9b0d78a8239a457a308f32ebc496.zip", "pdf": "/pdf/9d040a3a2bfb3fa9f9f1e9c64fe193c40efcbe90.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=wFjeqqwzd", "_bibtex": "@misc{\nantoniou2021defining,\ntitle={Defining Benchmarks for Continual Few-Shot Learning},\nauthor={Antreas Antoniou and Massimiliano Patacchiola and Mateusz Ochal and Amos Storkey},\nyear={2021},\nurl={https://openreview.net/forum?id=XG1Drw7VbLJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "XG1Drw7VbLJ", "replyto": "XG1Drw7VbLJ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1737/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538111797, "tmdate": 1606915778779, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1737/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1737/-/Official_Review"}}}], "count": 12}