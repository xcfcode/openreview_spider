{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396680333, "tcdate": 1486396680333, "number": 1, "id": "Skgf6MIOl", "invitation": "ICLR.cc/2017/conference/-/paper565/acceptance", "forum": "ryaFG5ige", "replyto": "ryaFG5ige", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The reviewers agree that the paper pursues an interesting direction to explore active example selection for CNN training, but have unanimously raised serious concerns with regards to overall presentation which needs further improvement (I still see spelling/grammatical errors/sloppy notation in the latest draft). Some sections in the paper are hard to follow. With regards to technical motivation, the link between depth and need for active example selection is alluded to, but not properly explained in the paper. The PCs think that this paper has too many areas in need of improvement to be accepted to the conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introducing Active Learning for CNN under the light of Variational Inference", "abstract": "One main concern of the deep learning community is to increase the capacity of\nrepresentation of deep networks by increasing their depth. This requires to scale\nup the size of the training database accordingly. Indeed a major intuition lies in\nthe fact that the depth of the network and the size of the training set are strongly\ncorrelated. However recent works tend to show that deep learning may be handled\nwith smaller dataset as long as the training samples are carefully selected (let us\nmention for instance curriculum learning). In this context we introduce a scalable\nand efficient active learning method that can be applied to most neural networks,\nespecially Convolutional Neural Networks (CNN). To the best of our knowledge,\nthis paper is the first of its kind to design an active learning selection scheme based\non a variational inference for neural networks. We also deduced a formulation of\nthe posterior and prior distributions of the weights using statistical knowledge on\nthe Maximum Likelihood Estimator.\nWe describe our strategy to come up with our active learning criterion. We assess its\nconsistency by checking the accuracy obtained by successive active learning steps\non two benchmark datasets MNIST and USPS. We also demonstrate its scalability\ntowards increasing training set size.", "pdf": "/pdf/799e4a649a22a4da58454b5223b37fdbabec0955.pdf", "TL;DR": "Building automatically the labeled training set with active learning for CNN. The criterion is developed on a variational inference for NN and a kronecker approximation of Fisher matrices for CNN", "paperhash": "ducoffe|introducing_active_learning_for_cnn_under_the_light_of_variational_inference", "conflicts": ["unice.fr"], "keywords": ["Deep learning", "Supervised Learning", "Optimization"], "authors": ["Melanie Ducoffe", "Frederic Precioso"], "authorids": ["ducoffe@i3s.unice.fr", "precioso@i3s.unice.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396680845, "id": "ICLR.cc/2017/conference/-/paper565/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "ryaFG5ige", "replyto": "ryaFG5ige", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396680845}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484389343937, "tcdate": 1478365829347, "number": 565, "id": "ryaFG5ige", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "ryaFG5ige", "signatures": ["~M\u00e9lanie_Ducoffe1"], "readers": ["everyone"], "content": {"title": "Introducing Active Learning for CNN under the light of Variational Inference", "abstract": "One main concern of the deep learning community is to increase the capacity of\nrepresentation of deep networks by increasing their depth. This requires to scale\nup the size of the training database accordingly. Indeed a major intuition lies in\nthe fact that the depth of the network and the size of the training set are strongly\ncorrelated. However recent works tend to show that deep learning may be handled\nwith smaller dataset as long as the training samples are carefully selected (let us\nmention for instance curriculum learning). In this context we introduce a scalable\nand efficient active learning method that can be applied to most neural networks,\nespecially Convolutional Neural Networks (CNN). To the best of our knowledge,\nthis paper is the first of its kind to design an active learning selection scheme based\non a variational inference for neural networks. We also deduced a formulation of\nthe posterior and prior distributions of the weights using statistical knowledge on\nthe Maximum Likelihood Estimator.\nWe describe our strategy to come up with our active learning criterion. We assess its\nconsistency by checking the accuracy obtained by successive active learning steps\non two benchmark datasets MNIST and USPS. We also demonstrate its scalability\ntowards increasing training set size.", "pdf": "/pdf/799e4a649a22a4da58454b5223b37fdbabec0955.pdf", "TL;DR": "Building automatically the labeled training set with active learning for CNN. The criterion is developed on a variational inference for NN and a kronecker approximation of Fisher matrices for CNN", "paperhash": "ducoffe|introducing_active_learning_for_cnn_under_the_light_of_variational_inference", "conflicts": ["unice.fr"], "keywords": ["Deep learning", "Supervised Learning", "Optimization"], "authors": ["Melanie Ducoffe", "Frederic Precioso"], "authorids": ["ducoffe@i3s.unice.fr", "precioso@i3s.unice.fr"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 20, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1484388609099, "tcdate": 1484388609099, "number": 10, "id": "rktZK_DUe", "invitation": "ICLR.cc/2017/conference/-/paper565/public/comment", "forum": "ryaFG5ige", "replyto": "B1qrOdwIx", "signatures": ["~M\u00e9lanie_Ducoffe1"], "readers": ["everyone"], "writers": ["~M\u00e9lanie_Ducoffe1"], "content": {"title": "Supplementary experiments", "comment": "Another experiments demonstrating that sampling a larger subset D does not affect the accuracy is available in the apendix of the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introducing Active Learning for CNN under the light of Variational Inference", "abstract": "One main concern of the deep learning community is to increase the capacity of\nrepresentation of deep networks by increasing their depth. This requires to scale\nup the size of the training database accordingly. Indeed a major intuition lies in\nthe fact that the depth of the network and the size of the training set are strongly\ncorrelated. However recent works tend to show that deep learning may be handled\nwith smaller dataset as long as the training samples are carefully selected (let us\nmention for instance curriculum learning). In this context we introduce a scalable\nand efficient active learning method that can be applied to most neural networks,\nespecially Convolutional Neural Networks (CNN). To the best of our knowledge,\nthis paper is the first of its kind to design an active learning selection scheme based\non a variational inference for neural networks. We also deduced a formulation of\nthe posterior and prior distributions of the weights using statistical knowledge on\nthe Maximum Likelihood Estimator.\nWe describe our strategy to come up with our active learning criterion. We assess its\nconsistency by checking the accuracy obtained by successive active learning steps\non two benchmark datasets MNIST and USPS. We also demonstrate its scalability\ntowards increasing training set size.", "pdf": "/pdf/799e4a649a22a4da58454b5223b37fdbabec0955.pdf", "TL;DR": "Building automatically the labeled training set with active learning for CNN. The criterion is developed on a variational inference for NN and a kronecker approximation of Fisher matrices for CNN", "paperhash": "ducoffe|introducing_active_learning_for_cnn_under_the_light_of_variational_inference", "conflicts": ["unice.fr"], "keywords": ["Deep learning", "Supervised Learning", "Optimization"], "authors": ["Melanie Ducoffe", "Frederic Precioso"], "authorids": ["ducoffe@i3s.unice.fr", "precioso@i3s.unice.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287518846, "id": "ICLR.cc/2017/conference/-/paper565/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryaFG5ige", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper565/reviewers", "ICLR.cc/2017/conference/paper565/areachairs"], "cdate": 1485287518846}}}, {"tddate": null, "tmdate": 1484388417878, "tcdate": 1484388417878, "number": 9, "id": "B1qrOdwIx", "invitation": "ICLR.cc/2017/conference/-/paper565/public/comment", "forum": "ryaFG5ige", "replyto": "BJv2ZLQVe", "signatures": ["~M\u00e9lanie_Ducoffe1"], "readers": ["everyone"], "writers": ["~M\u00e9lanie_Ducoffe1"], "content": {"title": "Proof of concept for active learning with CNNs", "comment": "We are grateful to AnonReview3 for the interest and feedback. We have revised the paper to both improve the overall organization and stress the distinction between our contributions and previous works like the one from Graves which is now described in the related works. We have also revised the notations and mathematical formulations. The new version is now available on openreview. \nWe have validated our method on traditional datasets for the active learning community which are USPS and MNIST. Those are relatively small datasets compared to ImageNet, thereby not requiring networks as deep as for bigger datasets. The theory and the scalability of our method however holds for deeper networks. \nThe experiments made appear that curriculum learning is not a good active learning strategy for both tested datasets. As for the uncertainty selection, it works really well on MNIST while it fails on USPS.  While MNIST is a pretty clean database, USPS contains more outliers and noisy samples rendering it more difficult in terms of accuracy even though both databases are designed to assess digit classification. As other works we mentioned in the related work section, we are led to explain uncertainty selection to select useless samples with the amount of outliers and noisy samples in USPS."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introducing Active Learning for CNN under the light of Variational Inference", "abstract": "One main concern of the deep learning community is to increase the capacity of\nrepresentation of deep networks by increasing their depth. This requires to scale\nup the size of the training database accordingly. Indeed a major intuition lies in\nthe fact that the depth of the network and the size of the training set are strongly\ncorrelated. However recent works tend to show that deep learning may be handled\nwith smaller dataset as long as the training samples are carefully selected (let us\nmention for instance curriculum learning). In this context we introduce a scalable\nand efficient active learning method that can be applied to most neural networks,\nespecially Convolutional Neural Networks (CNN). To the best of our knowledge,\nthis paper is the first of its kind to design an active learning selection scheme based\non a variational inference for neural networks. We also deduced a formulation of\nthe posterior and prior distributions of the weights using statistical knowledge on\nthe Maximum Likelihood Estimator.\nWe describe our strategy to come up with our active learning criterion. We assess its\nconsistency by checking the accuracy obtained by successive active learning steps\non two benchmark datasets MNIST and USPS. We also demonstrate its scalability\ntowards increasing training set size.", "pdf": "/pdf/799e4a649a22a4da58454b5223b37fdbabec0955.pdf", "TL;DR": "Building automatically the labeled training set with active learning for CNN. The criterion is developed on a variational inference for NN and a kronecker approximation of Fisher matrices for CNN", "paperhash": "ducoffe|introducing_active_learning_for_cnn_under_the_light_of_variational_inference", "conflicts": ["unice.fr"], "keywords": ["Deep learning", "Supervised Learning", "Optimization"], "authors": ["Melanie Ducoffe", "Frederic Precioso"], "authorids": ["ducoffe@i3s.unice.fr", "precioso@i3s.unice.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287518846, "id": "ICLR.cc/2017/conference/-/paper565/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryaFG5ige", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper565/reviewers", "ICLR.cc/2017/conference/paper565/areachairs"], "cdate": 1485287518846}}}, {"tddate": null, "tmdate": 1484388383455, "tcdate": 1484388383455, "number": 8, "id": "BJPXuuDUx", "invitation": "ICLR.cc/2017/conference/-/paper565/public/comment", "forum": "ryaFG5ige", "replyto": "r1HGpiBNg", "signatures": ["~M\u00e9lanie_Ducoffe1"], "readers": ["everyone"], "writers": ["~M\u00e9lanie_Ducoffe1"], "content": {"title": "Interesting ideas but not well explained", "comment": "We are grateful to AnonReview4 for the interest and feedback. We have corrected the typos and grammatical errors and modified the structure of the text: we have trimmed the descriptions of unrelated active learning methods in the related work section and extended the discussion on Bayesian inference for active learning. We also included a detailed description of Graves' work on variational inference for neural networks. We have modified the paper to stress out the distinction between previous works (Graves) and our contributions, in particular to highlight our own proposition to get prior and posterior distributions of the weights from statistical assumption on the Maximum Likelihood Estimator. The new version is now available on openreview. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introducing Active Learning for CNN under the light of Variational Inference", "abstract": "One main concern of the deep learning community is to increase the capacity of\nrepresentation of deep networks by increasing their depth. This requires to scale\nup the size of the training database accordingly. Indeed a major intuition lies in\nthe fact that the depth of the network and the size of the training set are strongly\ncorrelated. However recent works tend to show that deep learning may be handled\nwith smaller dataset as long as the training samples are carefully selected (let us\nmention for instance curriculum learning). In this context we introduce a scalable\nand efficient active learning method that can be applied to most neural networks,\nespecially Convolutional Neural Networks (CNN). To the best of our knowledge,\nthis paper is the first of its kind to design an active learning selection scheme based\non a variational inference for neural networks. We also deduced a formulation of\nthe posterior and prior distributions of the weights using statistical knowledge on\nthe Maximum Likelihood Estimator.\nWe describe our strategy to come up with our active learning criterion. We assess its\nconsistency by checking the accuracy obtained by successive active learning steps\non two benchmark datasets MNIST and USPS. We also demonstrate its scalability\ntowards increasing training set size.", "pdf": "/pdf/799e4a649a22a4da58454b5223b37fdbabec0955.pdf", "TL;DR": "Building automatically the labeled training set with active learning for CNN. The criterion is developed on a variational inference for NN and a kronecker approximation of Fisher matrices for CNN", "paperhash": "ducoffe|introducing_active_learning_for_cnn_under_the_light_of_variational_inference", "conflicts": ["unice.fr"], "keywords": ["Deep learning", "Supervised Learning", "Optimization"], "authors": ["Melanie Ducoffe", "Frederic Precioso"], "authorids": ["ducoffe@i3s.unice.fr", "precioso@i3s.unice.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287518846, "id": "ICLR.cc/2017/conference/-/paper565/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryaFG5ige", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper565/reviewers", "ICLR.cc/2017/conference/paper565/areachairs"], "cdate": 1485287518846}}}, {"tddate": null, "tmdate": 1484388237485, "tcdate": 1484388237485, "number": 7, "id": "S1Hqv_vUg", "invitation": "ICLR.cc/2017/conference/-/paper565/public/comment", "forum": "ryaFG5ige", "replyto": "HyRPClKNl", "signatures": ["~M\u00e9lanie_Ducoffe1"], "readers": ["everyone"], "writers": ["~M\u00e9lanie_Ducoffe1"], "content": {"title": "Very interesting but hard to follow", "comment": "We are grateful to AnonReview2 for the interest and feedback. We have paid a great attention to rewriting the paper, in particular straightening up the mathematical notations and restructuring the presentation of several sections. We specifically focused on introduction and related works so as to make clearly appear the novelty of our contribution. The new version is now available on openreview. As we focus on an active learning strategy, assessing the benefit of this very strategy on a dataset pre-requires to have the complete implementation of the training process on the full dataset for the right deep architecture. It will be very interesting to assess how the strategy performs on bigger datasets. Owing to the limited time frame, we could not get these results yet."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introducing Active Learning for CNN under the light of Variational Inference", "abstract": "One main concern of the deep learning community is to increase the capacity of\nrepresentation of deep networks by increasing their depth. This requires to scale\nup the size of the training database accordingly. Indeed a major intuition lies in\nthe fact that the depth of the network and the size of the training set are strongly\ncorrelated. However recent works tend to show that deep learning may be handled\nwith smaller dataset as long as the training samples are carefully selected (let us\nmention for instance curriculum learning). In this context we introduce a scalable\nand efficient active learning method that can be applied to most neural networks,\nespecially Convolutional Neural Networks (CNN). To the best of our knowledge,\nthis paper is the first of its kind to design an active learning selection scheme based\non a variational inference for neural networks. We also deduced a formulation of\nthe posterior and prior distributions of the weights using statistical knowledge on\nthe Maximum Likelihood Estimator.\nWe describe our strategy to come up with our active learning criterion. We assess its\nconsistency by checking the accuracy obtained by successive active learning steps\non two benchmark datasets MNIST and USPS. We also demonstrate its scalability\ntowards increasing training set size.", "pdf": "/pdf/799e4a649a22a4da58454b5223b37fdbabec0955.pdf", "TL;DR": "Building automatically the labeled training set with active learning for CNN. The criterion is developed on a variational inference for NN and a kronecker approximation of Fisher matrices for CNN", "paperhash": "ducoffe|introducing_active_learning_for_cnn_under_the_light_of_variational_inference", "conflicts": ["unice.fr"], "keywords": ["Deep learning", "Supervised Learning", "Optimization"], "authors": ["Melanie Ducoffe", "Frederic Precioso"], "authorids": ["ducoffe@i3s.unice.fr", "precioso@i3s.unice.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287518846, "id": "ICLR.cc/2017/conference/-/paper565/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryaFG5ige", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper565/reviewers", "ICLR.cc/2017/conference/paper565/areachairs"], "cdate": 1485287518846}}}, {"tddate": null, "tmdate": 1482391141842, "tcdate": 1482391141842, "number": 3, "id": "HyRPClKNl", "invitation": "ICLR.cc/2017/conference/-/paper565/official/review", "forum": "ryaFG5ige", "replyto": "ryaFG5ige", "signatures": ["ICLR.cc/2017/conference/paper565/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper565/AnonReviewer2"], "content": {"title": "Very interesting but hard to follow.", "rating": "6: Marginally above acceptance threshold", "review": "The paper proposes to perform active learning using pool selection of deep learning mini-batches using an approximation of the bayesian posterior. Several terms are in turn approximated.\n\nThe Maximum Likelihood Estimation (MLE) bayesian inference approach to active learning, the various approximations, and more generally the theoretical framework is very interesting but difficult to follow.\n\nThe paper is written in poor English and is sometimes a bit painful to read.\n\nAlternative Active learning strategies and techniques do not need to be described with such detail. On the other hand, the proposed approach has a lot of complex approximations which would benefit from a more detailed/structured presentation. Another dataset would be a big plus (both datasets concern gray digits and USPS and are arguably somewhat similar).", "confidence": "1: The reviewer's evaluation is an educated guess"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introducing Active Learning for CNN under the light of Variational Inference", "abstract": "One main concern of the deep learning community is to increase the capacity of\nrepresentation of deep networks by increasing their depth. This requires to scale\nup the size of the training database accordingly. Indeed a major intuition lies in\nthe fact that the depth of the network and the size of the training set are strongly\ncorrelated. However recent works tend to show that deep learning may be handled\nwith smaller dataset as long as the training samples are carefully selected (let us\nmention for instance curriculum learning). In this context we introduce a scalable\nand efficient active learning method that can be applied to most neural networks,\nespecially Convolutional Neural Networks (CNN). To the best of our knowledge,\nthis paper is the first of its kind to design an active learning selection scheme based\non a variational inference for neural networks. We also deduced a formulation of\nthe posterior and prior distributions of the weights using statistical knowledge on\nthe Maximum Likelihood Estimator.\nWe describe our strategy to come up with our active learning criterion. We assess its\nconsistency by checking the accuracy obtained by successive active learning steps\non two benchmark datasets MNIST and USPS. We also demonstrate its scalability\ntowards increasing training set size.", "pdf": "/pdf/799e4a649a22a4da58454b5223b37fdbabec0955.pdf", "TL;DR": "Building automatically the labeled training set with active learning for CNN. The criterion is developed on a variational inference for NN and a kronecker approximation of Fisher matrices for CNN", "paperhash": "ducoffe|introducing_active_learning_for_cnn_under_the_light_of_variational_inference", "conflicts": ["unice.fr"], "keywords": ["Deep learning", "Supervised Learning", "Optimization"], "authors": ["Melanie Ducoffe", "Frederic Precioso"], "authorids": ["ducoffe@i3s.unice.fr", "precioso@i3s.unice.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512539032, "id": "ICLR.cc/2017/conference/-/paper565/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper565/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper565/AnonReviewer3", "ICLR.cc/2017/conference/paper565/AnonReviewer4", "ICLR.cc/2017/conference/paper565/AnonReviewer2"], "reply": {"forum": "ryaFG5ige", "replyto": "ryaFG5ige", "writers": {"values-regex": "ICLR.cc/2017/conference/paper565/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper565/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512539032}}}, {"tddate": null, "tmdate": 1482173708798, "tcdate": 1482173708798, "number": 2, "id": "r1HGpiBNg", "invitation": "ICLR.cc/2017/conference/-/paper565/official/review", "forum": "ryaFG5ige", "replyto": "ryaFG5ige", "signatures": ["ICLR.cc/2017/conference/paper565/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper565/AnonReviewer4"], "content": {"title": "Interesting ideas but not well explained", "rating": "6: Marginally above acceptance threshold", "review": "Quality:\nThe paper initiates a framework to incorporate active learning into the deep learning framework, mainly addressing challenges such as scalability that accompanies the training of a deep neural network. \nHowever, I think the paper is not well polished; there are quite a lot of grammatical and typing errors.\n\nClarity:\nThe paper needs major improvements in terms of clarity. The motivations in the introduction, i.e., why it is difficult to do active learning in deep architectures, could be better explained, and tied to the explanation in Section 3 of the paper. For example, the authors motivated the need of (mini)batch label queries, but never mention it again in Section 3, when they describe their main methodology. \nThe related work section, although appearing systematic and thorough, is a little detached from the main body of the paper (related work section should not be a survey of the literature, but help readers locate your work in the relevant literature, and highlight the pros and cons. In this perspective, maybe the authors could shorten some explanations over the related work that are not directly related, while spending more time on discussing/comparing with works that are most related to your current work, e.g., that of Graves '11.\n\nOriginality & Significance:\nThe authors proposed an active learning training framework. The idea is to treat the network parameter optimization problem as a Bayesian inference problem (which is proposed previously by Graves) and formulate the active learning problem as that of sampling the most informative data, where the informativeness is defined by the variational free energy, which depends on the Fisher information. To reconcile the computational burden of computing the inverse of Fisher Information matrix, the authors proposed techniques to approximate it (which seems to be novel)\n\nI think that this paper initiates an interesting direction: one that adapts deep learning to label-expensive problems, via active learning. But the paper needs to be improved in terms of presentation.\n\n", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introducing Active Learning for CNN under the light of Variational Inference", "abstract": "One main concern of the deep learning community is to increase the capacity of\nrepresentation of deep networks by increasing their depth. This requires to scale\nup the size of the training database accordingly. Indeed a major intuition lies in\nthe fact that the depth of the network and the size of the training set are strongly\ncorrelated. However recent works tend to show that deep learning may be handled\nwith smaller dataset as long as the training samples are carefully selected (let us\nmention for instance curriculum learning). In this context we introduce a scalable\nand efficient active learning method that can be applied to most neural networks,\nespecially Convolutional Neural Networks (CNN). To the best of our knowledge,\nthis paper is the first of its kind to design an active learning selection scheme based\non a variational inference for neural networks. We also deduced a formulation of\nthe posterior and prior distributions of the weights using statistical knowledge on\nthe Maximum Likelihood Estimator.\nWe describe our strategy to come up with our active learning criterion. We assess its\nconsistency by checking the accuracy obtained by successive active learning steps\non two benchmark datasets MNIST and USPS. We also demonstrate its scalability\ntowards increasing training set size.", "pdf": "/pdf/799e4a649a22a4da58454b5223b37fdbabec0955.pdf", "TL;DR": "Building automatically the labeled training set with active learning for CNN. The criterion is developed on a variational inference for NN and a kronecker approximation of Fisher matrices for CNN", "paperhash": "ducoffe|introducing_active_learning_for_cnn_under_the_light_of_variational_inference", "conflicts": ["unice.fr"], "keywords": ["Deep learning", "Supervised Learning", "Optimization"], "authors": ["Melanie Ducoffe", "Frederic Precioso"], "authorids": ["ducoffe@i3s.unice.fr", "precioso@i3s.unice.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512539032, "id": "ICLR.cc/2017/conference/-/paper565/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper565/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper565/AnonReviewer3", "ICLR.cc/2017/conference/paper565/AnonReviewer4", "ICLR.cc/2017/conference/paper565/AnonReviewer2"], "reply": {"forum": "ryaFG5ige", "replyto": "ryaFG5ige", "writers": {"values-regex": "ICLR.cc/2017/conference/paper565/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper565/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512539032}}}, {"tddate": null, "tmdate": 1482019247146, "tcdate": 1482019247146, "number": 1, "id": "BJv2ZLQVe", "invitation": "ICLR.cc/2017/conference/-/paper565/official/review", "forum": "ryaFG5ige", "replyto": "ryaFG5ige", "signatures": ["ICLR.cc/2017/conference/paper565/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper565/AnonReviewer3"], "content": {"title": "Proof of concept for active learning with CNNs", "rating": "6: Marginally above acceptance threshold", "review": "This paper introduces a mechanism for active learning with convolutional neural networks (CNNs). I would not go as far as the authors in calling these \"deep\", seeing that they seem to have only 2 hidden layers with only 20 filters each. The active learning criterion is a greedy selection scheme based on variational free energy and a series of approximations.\n\nThe paper is sometimes hard to read, due to (a) many grammatical errors and (b) sloppy notation in some places (e.g., on page 5, line 1, f is used but never introduced before). Overall, I give an accepting score, but a weak one because of the grammatical errors. If the paper is accepted, these should be fixed for the final version, optimally by a native speaker. \n\nThe paper's topic is interesting, and the paper appears to succeed in its goal of showing a proof of concept for active learning in CNNs (if only on toy datasets). I'm surprised by the new results on uncertainty sampling and curriculum learning the authors added: why do these methods both break for USPS? In particular, uncertainty sampling did very well (in fact, better than the authors' new method) on MNIST, but apparently horribly on USPS; some explanation for this would be useful.\n\nI have one more question: why is it necessary to first sample a larger subset D \\subset U, from which we select using active learning? Is this merely done for reasons of computational efficiency, or can it actually somehow improve results? (If so, it would be instrumental to see the worse results when this is not done.)\n", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introducing Active Learning for CNN under the light of Variational Inference", "abstract": "One main concern of the deep learning community is to increase the capacity of\nrepresentation of deep networks by increasing their depth. This requires to scale\nup the size of the training database accordingly. Indeed a major intuition lies in\nthe fact that the depth of the network and the size of the training set are strongly\ncorrelated. However recent works tend to show that deep learning may be handled\nwith smaller dataset as long as the training samples are carefully selected (let us\nmention for instance curriculum learning). In this context we introduce a scalable\nand efficient active learning method that can be applied to most neural networks,\nespecially Convolutional Neural Networks (CNN). To the best of our knowledge,\nthis paper is the first of its kind to design an active learning selection scheme based\non a variational inference for neural networks. We also deduced a formulation of\nthe posterior and prior distributions of the weights using statistical knowledge on\nthe Maximum Likelihood Estimator.\nWe describe our strategy to come up with our active learning criterion. We assess its\nconsistency by checking the accuracy obtained by successive active learning steps\non two benchmark datasets MNIST and USPS. We also demonstrate its scalability\ntowards increasing training set size.", "pdf": "/pdf/799e4a649a22a4da58454b5223b37fdbabec0955.pdf", "TL;DR": "Building automatically the labeled training set with active learning for CNN. The criterion is developed on a variational inference for NN and a kronecker approximation of Fisher matrices for CNN", "paperhash": "ducoffe|introducing_active_learning_for_cnn_under_the_light_of_variational_inference", "conflicts": ["unice.fr"], "keywords": ["Deep learning", "Supervised Learning", "Optimization"], "authors": ["Melanie Ducoffe", "Frederic Precioso"], "authorids": ["ducoffe@i3s.unice.fr", "precioso@i3s.unice.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512539032, "id": "ICLR.cc/2017/conference/-/paper565/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper565/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper565/AnonReviewer3", "ICLR.cc/2017/conference/paper565/AnonReviewer4", "ICLR.cc/2017/conference/paper565/AnonReviewer2"], "reply": {"forum": "ryaFG5ige", "replyto": "ryaFG5ige", "writers": {"values-regex": "ICLR.cc/2017/conference/paper565/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper565/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512539032}}}, {"tddate": null, "tmdate": 1481892290617, "tcdate": 1481884082114, "number": 5, "id": "Sy5hbH-Vg", "invitation": "ICLR.cc/2017/conference/-/paper565/public/comment", "forum": "ryaFG5ige", "replyto": "Bk1NfRJXe", "signatures": ["~M\u00e9lanie_Ducoffe1"], "readers": ["everyone"], "writers": ["~M\u00e9lanie_Ducoffe1"], "content": {"title": "Experiments", "comment": "We thank warmly AnonReview2 for his feedbacks and interests!\n\nFor a lack of time before the deadline, we run 5 experiments for each baseline\non MNIST while we run 10 experiments for USPS. We update the papers so the\nbaselines on MNIST are compared on a basis of 10 runs. Figures correspond\nto the curves averages over several tests.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introducing Active Learning for CNN under the light of Variational Inference", "abstract": "One main concern of the deep learning community is to increase the capacity of\nrepresentation of deep networks by increasing their depth. This requires to scale\nup the size of the training database accordingly. Indeed a major intuition lies in\nthe fact that the depth of the network and the size of the training set are strongly\ncorrelated. However recent works tend to show that deep learning may be handled\nwith smaller dataset as long as the training samples are carefully selected (let us\nmention for instance curriculum learning). In this context we introduce a scalable\nand efficient active learning method that can be applied to most neural networks,\nespecially Convolutional Neural Networks (CNN). To the best of our knowledge,\nthis paper is the first of its kind to design an active learning selection scheme based\non a variational inference for neural networks. We also deduced a formulation of\nthe posterior and prior distributions of the weights using statistical knowledge on\nthe Maximum Likelihood Estimator.\nWe describe our strategy to come up with our active learning criterion. We assess its\nconsistency by checking the accuracy obtained by successive active learning steps\non two benchmark datasets MNIST and USPS. We also demonstrate its scalability\ntowards increasing training set size.", "pdf": "/pdf/799e4a649a22a4da58454b5223b37fdbabec0955.pdf", "TL;DR": "Building automatically the labeled training set with active learning for CNN. The criterion is developed on a variational inference for NN and a kronecker approximation of Fisher matrices for CNN", "paperhash": "ducoffe|introducing_active_learning_for_cnn_under_the_light_of_variational_inference", "conflicts": ["unice.fr"], "keywords": ["Deep learning", "Supervised Learning", "Optimization"], "authors": ["Melanie Ducoffe", "Frederic Precioso"], "authorids": ["ducoffe@i3s.unice.fr", "precioso@i3s.unice.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287518846, "id": "ICLR.cc/2017/conference/-/paper565/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryaFG5ige", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper565/reviewers", "ICLR.cc/2017/conference/paper565/areachairs"], "cdate": 1485287518846}}}, {"tddate": null, "tmdate": 1481884281002, "tcdate": 1481884281002, "number": 6, "id": "rJ-tMSbEe", "invitation": "ICLR.cc/2017/conference/-/paper565/public/comment", "forum": "ryaFG5ige", "replyto": "BJctfh1Xg", "signatures": ["~M\u00e9lanie_Ducoffe1"], "readers": ["everyone"], "writers": ["~M\u00e9lanie_Ducoffe1"], "content": {"title": "Approximation of Q(\\beta)", "comment": "We thank warmly AnonReview4 for his feedbacks and interests!\n\nThe sampling distribution is initialized randomly, we also sampled the initial training set randomly for every runs.\n\nIndeed the approximation of Q(\\beta) is crucial for the importance sampling of the data. \nPapers proposing active learning with second order information (1, 2, 3) assume that the current MLE estimator w hat, is already\na good approximation to the true parameter and we follow that guide of line.\n\n\nreferences\n(1) Zhang, T., & Oles, F. (2000). The value of unlabeled data for classifica-\ntion problems. In Proceedings of the Seventeenth International Conference on\nMachine Learning,(Langley, P., ed.) (pp. 1191-1198).\n(2) Hoi, S. C., Jin, R., Zhu, J., & Lyu, M. R. (2006, June). Batch mode active\nlearning and its application to medical image classification. In Proceedings of\nthe 23rd international conference on Machine learning (pp. 417-424). ACM.\n(3) Chaudhuri, K., Kakade, S. M., Netrapalli, P., & Sanghavi, S. (2015). Conver-\ngence rates of active learning for maximum likelihood estimation. In Advances\nin Neural Information Processing Systems (pp. 1090-1098)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introducing Active Learning for CNN under the light of Variational Inference", "abstract": "One main concern of the deep learning community is to increase the capacity of\nrepresentation of deep networks by increasing their depth. This requires to scale\nup the size of the training database accordingly. Indeed a major intuition lies in\nthe fact that the depth of the network and the size of the training set are strongly\ncorrelated. However recent works tend to show that deep learning may be handled\nwith smaller dataset as long as the training samples are carefully selected (let us\nmention for instance curriculum learning). In this context we introduce a scalable\nand efficient active learning method that can be applied to most neural networks,\nespecially Convolutional Neural Networks (CNN). To the best of our knowledge,\nthis paper is the first of its kind to design an active learning selection scheme based\non a variational inference for neural networks. We also deduced a formulation of\nthe posterior and prior distributions of the weights using statistical knowledge on\nthe Maximum Likelihood Estimator.\nWe describe our strategy to come up with our active learning criterion. We assess its\nconsistency by checking the accuracy obtained by successive active learning steps\non two benchmark datasets MNIST and USPS. We also demonstrate its scalability\ntowards increasing training set size.", "pdf": "/pdf/799e4a649a22a4da58454b5223b37fdbabec0955.pdf", "TL;DR": "Building automatically the labeled training set with active learning for CNN. The criterion is developed on a variational inference for NN and a kronecker approximation of Fisher matrices for CNN", "paperhash": "ducoffe|introducing_active_learning_for_cnn_under_the_light_of_variational_inference", "conflicts": ["unice.fr"], "keywords": ["Deep learning", "Supervised Learning", "Optimization"], "authors": ["Melanie Ducoffe", "Frederic Precioso"], "authorids": ["ducoffe@i3s.unice.fr", "precioso@i3s.unice.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287518846, "id": "ICLR.cc/2017/conference/-/paper565/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryaFG5ige", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper565/reviewers", "ICLR.cc/2017/conference/paper565/areachairs"], "cdate": 1485287518846}}}, {"tddate": null, "tmdate": 1481883972381, "tcdate": 1481883972381, "number": 4, "id": "ry3rbSbEg", "invitation": "ICLR.cc/2017/conference/-/paper565/public/comment", "forum": "ryaFG5ige", "replyto": "BJWRG6xmx", "signatures": ["~M\u00e9lanie_Ducoffe1"], "readers": ["everyone"], "writers": ["~M\u00e9lanie_Ducoffe1"], "content": {"title": "Naive baseline", "comment": "For a sake of concision, as the\ninitial submission is already two pages more that a standard submission should\nbe, we did not present further comparisons with simple approach. We updated\nour paper with comparison with two naive baselines : uncertainty sampling and\ncurriculum sampling both based on the likelihood score of a sample (because\nthe true label of a sample is unknown, the likelihood is computed based on the\nprediction of the current network). Uncertainty will select samples on which the\nclassifier is uncertain about, that means samples with the highest negative log\nlikelihood. On the contrary, curriculum sampling selects the sample on which\nthe classifier is the more certain about. Figures 1 and 2 have been updated accordingly."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introducing Active Learning for CNN under the light of Variational Inference", "abstract": "One main concern of the deep learning community is to increase the capacity of\nrepresentation of deep networks by increasing their depth. This requires to scale\nup the size of the training database accordingly. Indeed a major intuition lies in\nthe fact that the depth of the network and the size of the training set are strongly\ncorrelated. However recent works tend to show that deep learning may be handled\nwith smaller dataset as long as the training samples are carefully selected (let us\nmention for instance curriculum learning). In this context we introduce a scalable\nand efficient active learning method that can be applied to most neural networks,\nespecially Convolutional Neural Networks (CNN). To the best of our knowledge,\nthis paper is the first of its kind to design an active learning selection scheme based\non a variational inference for neural networks. We also deduced a formulation of\nthe posterior and prior distributions of the weights using statistical knowledge on\nthe Maximum Likelihood Estimator.\nWe describe our strategy to come up with our active learning criterion. We assess its\nconsistency by checking the accuracy obtained by successive active learning steps\non two benchmark datasets MNIST and USPS. We also demonstrate its scalability\ntowards increasing training set size.", "pdf": "/pdf/799e4a649a22a4da58454b5223b37fdbabec0955.pdf", "TL;DR": "Building automatically the labeled training set with active learning for CNN. The criterion is developed on a variational inference for NN and a kronecker approximation of Fisher matrices for CNN", "paperhash": "ducoffe|introducing_active_learning_for_cnn_under_the_light_of_variational_inference", "conflicts": ["unice.fr"], "keywords": ["Deep learning", "Supervised Learning", "Optimization"], "authors": ["Melanie Ducoffe", "Frederic Precioso"], "authorids": ["ducoffe@i3s.unice.fr", "precioso@i3s.unice.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287518846, "id": "ICLR.cc/2017/conference/-/paper565/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryaFG5ige", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper565/reviewers", "ICLR.cc/2017/conference/paper565/areachairs"], "cdate": 1485287518846}}}, {"tddate": null, "tmdate": 1481883866748, "tcdate": 1481883866748, "number": 3, "id": "HkX1WHZNl", "invitation": "ICLR.cc/2017/conference/-/paper565/public/comment", "forum": "ryaFG5ige", "replyto": "S1RBQaxQx", "signatures": ["~M\u00e9lanie_Ducoffe1"], "readers": ["everyone"], "writers": ["~M\u00e9lanie_Ducoffe1"], "content": {"title": "Time complexity", "comment": "The time required for using a minibatch to update the model by backprop is\nmuch faster: the mean time for processing a minibatch (forward and backprop\npass) on USPS (8 samples) is 9.65475 ms with a Titan-X (GTX 980 M) with 8\nGB RAM GPU memory. Because finding the optimal subset is a combinatorial\noptimization problem and it is necessary to test every solution to check\nwhether one solution is optimal, the original problem is EXPTIME complete. We\npropose a polynomial heuristic, which runs approximately in Nlog(N) given N\nthe number of unlabelled samples evaluated as potential queries."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introducing Active Learning for CNN under the light of Variational Inference", "abstract": "One main concern of the deep learning community is to increase the capacity of\nrepresentation of deep networks by increasing their depth. This requires to scale\nup the size of the training database accordingly. Indeed a major intuition lies in\nthe fact that the depth of the network and the size of the training set are strongly\ncorrelated. However recent works tend to show that deep learning may be handled\nwith smaller dataset as long as the training samples are carefully selected (let us\nmention for instance curriculum learning). In this context we introduce a scalable\nand efficient active learning method that can be applied to most neural networks,\nespecially Convolutional Neural Networks (CNN). To the best of our knowledge,\nthis paper is the first of its kind to design an active learning selection scheme based\non a variational inference for neural networks. We also deduced a formulation of\nthe posterior and prior distributions of the weights using statistical knowledge on\nthe Maximum Likelihood Estimator.\nWe describe our strategy to come up with our active learning criterion. We assess its\nconsistency by checking the accuracy obtained by successive active learning steps\non two benchmark datasets MNIST and USPS. We also demonstrate its scalability\ntowards increasing training set size.", "pdf": "/pdf/799e4a649a22a4da58454b5223b37fdbabec0955.pdf", "TL;DR": "Building automatically the labeled training set with active learning for CNN. The criterion is developed on a variational inference for NN and a kronecker approximation of Fisher matrices for CNN", "paperhash": "ducoffe|introducing_active_learning_for_cnn_under_the_light_of_variational_inference", "conflicts": ["unice.fr"], "keywords": ["Deep learning", "Supervised Learning", "Optimization"], "authors": ["Melanie Ducoffe", "Frederic Precioso"], "authorids": ["ducoffe@i3s.unice.fr", "precioso@i3s.unice.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287518846, "id": "ICLR.cc/2017/conference/-/paper565/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryaFG5ige", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper565/reviewers", "ICLR.cc/2017/conference/paper565/areachairs"], "cdate": 1485287518846}}}, {"tddate": null, "tmdate": 1481883623537, "tcdate": 1481883623537, "number": 2, "id": "BJ1eeHWEg", "invitation": "ICLR.cc/2017/conference/-/paper565/public/comment", "forum": "ryaFG5ige", "replyto": "H1gu7Te7g", "signatures": ["~M\u00e9lanie_Ducoffe1"], "readers": ["everyone"], "writers": ["~M\u00e9lanie_Ducoffe1"], "content": {"title": "Groundtruth meaning", "comment": "We call groundtruth the average accuracy on 10 runs of the test error obtained\nwith the full annotated data shuffled randomly. Note that we do not optimize\nthe hyperparameters specifically for the size of the current annotated training\nset."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introducing Active Learning for CNN under the light of Variational Inference", "abstract": "One main concern of the deep learning community is to increase the capacity of\nrepresentation of deep networks by increasing their depth. This requires to scale\nup the size of the training database accordingly. Indeed a major intuition lies in\nthe fact that the depth of the network and the size of the training set are strongly\ncorrelated. However recent works tend to show that deep learning may be handled\nwith smaller dataset as long as the training samples are carefully selected (let us\nmention for instance curriculum learning). In this context we introduce a scalable\nand efficient active learning method that can be applied to most neural networks,\nespecially Convolutional Neural Networks (CNN). To the best of our knowledge,\nthis paper is the first of its kind to design an active learning selection scheme based\non a variational inference for neural networks. We also deduced a formulation of\nthe posterior and prior distributions of the weights using statistical knowledge on\nthe Maximum Likelihood Estimator.\nWe describe our strategy to come up with our active learning criterion. We assess its\nconsistency by checking the accuracy obtained by successive active learning steps\non two benchmark datasets MNIST and USPS. We also demonstrate its scalability\ntowards increasing training set size.", "pdf": "/pdf/799e4a649a22a4da58454b5223b37fdbabec0955.pdf", "TL;DR": "Building automatically the labeled training set with active learning for CNN. The criterion is developed on a variational inference for NN and a kronecker approximation of Fisher matrices for CNN", "paperhash": "ducoffe|introducing_active_learning_for_cnn_under_the_light_of_variational_inference", "conflicts": ["unice.fr"], "keywords": ["Deep learning", "Supervised Learning", "Optimization"], "authors": ["Melanie Ducoffe", "Frederic Precioso"], "authorids": ["ducoffe@i3s.unice.fr", "precioso@i3s.unice.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287518846, "id": "ICLR.cc/2017/conference/-/paper565/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryaFG5ige", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper565/reviewers", "ICLR.cc/2017/conference/paper565/areachairs"], "cdate": 1485287518846}}}, {"tddate": null, "tmdate": 1481883560519, "tcdate": 1481883560519, "number": 1, "id": "HJenJHZEg", "invitation": "ICLR.cc/2017/conference/-/paper565/public/comment", "forum": "ryaFG5ige", "replyto": "SJuxNag7e", "signatures": ["~M\u00e9lanie_Ducoffe1"], "readers": ["everyone"], "writers": ["~M\u00e9lanie_Ducoffe1"], "content": {"title": "Is our code available ?", "comment": "We thank warmly AnonReview3 for his feedbacks and interests !\n\nOur code is now available on the github account mducoffe, it is developed with\nTheano and Blocks. If the approximations mentionned are related to the KFAC\nrepresentation, the author\u2019s version is supposed to be also published soon."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introducing Active Learning for CNN under the light of Variational Inference", "abstract": "One main concern of the deep learning community is to increase the capacity of\nrepresentation of deep networks by increasing their depth. This requires to scale\nup the size of the training database accordingly. Indeed a major intuition lies in\nthe fact that the depth of the network and the size of the training set are strongly\ncorrelated. However recent works tend to show that deep learning may be handled\nwith smaller dataset as long as the training samples are carefully selected (let us\nmention for instance curriculum learning). In this context we introduce a scalable\nand efficient active learning method that can be applied to most neural networks,\nespecially Convolutional Neural Networks (CNN). To the best of our knowledge,\nthis paper is the first of its kind to design an active learning selection scheme based\non a variational inference for neural networks. We also deduced a formulation of\nthe posterior and prior distributions of the weights using statistical knowledge on\nthe Maximum Likelihood Estimator.\nWe describe our strategy to come up with our active learning criterion. We assess its\nconsistency by checking the accuracy obtained by successive active learning steps\non two benchmark datasets MNIST and USPS. We also demonstrate its scalability\ntowards increasing training set size.", "pdf": "/pdf/799e4a649a22a4da58454b5223b37fdbabec0955.pdf", "TL;DR": "Building automatically the labeled training set with active learning for CNN. The criterion is developed on a variational inference for NN and a kronecker approximation of Fisher matrices for CNN", "paperhash": "ducoffe|introducing_active_learning_for_cnn_under_the_light_of_variational_inference", "conflicts": ["unice.fr"], "keywords": ["Deep learning", "Supervised Learning", "Optimization"], "authors": ["Melanie Ducoffe", "Frederic Precioso"], "authorids": ["ducoffe@i3s.unice.fr", "precioso@i3s.unice.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287518846, "id": "ICLR.cc/2017/conference/-/paper565/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryaFG5ige", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper565/reviewers", "ICLR.cc/2017/conference/paper565/areachairs"], "cdate": 1485287518846}}}, {"tddate": null, "tmdate": 1480803311862, "tcdate": 1480803311857, "number": 3, "id": "SJuxNag7e", "invitation": "ICLR.cc/2017/conference/-/paper565/official/comment", "forum": "ryaFG5ige", "replyto": "ryaFG5ige", "signatures": ["ICLR.cc/2017/conference/paper565/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper565/AnonReviewer3"], "content": {"title": "Code available?", "comment": "Some of the approximations are quite complex; is your code available?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introducing Active Learning for CNN under the light of Variational Inference", "abstract": "One main concern of the deep learning community is to increase the capacity of\nrepresentation of deep networks by increasing their depth. This requires to scale\nup the size of the training database accordingly. Indeed a major intuition lies in\nthe fact that the depth of the network and the size of the training set are strongly\ncorrelated. However recent works tend to show that deep learning may be handled\nwith smaller dataset as long as the training samples are carefully selected (let us\nmention for instance curriculum learning). In this context we introduce a scalable\nand efficient active learning method that can be applied to most neural networks,\nespecially Convolutional Neural Networks (CNN). To the best of our knowledge,\nthis paper is the first of its kind to design an active learning selection scheme based\non a variational inference for neural networks. We also deduced a formulation of\nthe posterior and prior distributions of the weights using statistical knowledge on\nthe Maximum Likelihood Estimator.\nWe describe our strategy to come up with our active learning criterion. We assess its\nconsistency by checking the accuracy obtained by successive active learning steps\non two benchmark datasets MNIST and USPS. We also demonstrate its scalability\ntowards increasing training set size.", "pdf": "/pdf/799e4a649a22a4da58454b5223b37fdbabec0955.pdf", "TL;DR": "Building automatically the labeled training set with active learning for CNN. The criterion is developed on a variational inference for NN and a kronecker approximation of Fisher matrices for CNN", "paperhash": "ducoffe|introducing_active_learning_for_cnn_under_the_light_of_variational_inference", "conflicts": ["unice.fr"], "keywords": ["Deep learning", "Supervised Learning", "Optimization"], "authors": ["Melanie Ducoffe", "Frederic Precioso"], "authorids": ["ducoffe@i3s.unice.fr", "precioso@i3s.unice.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287518712, "id": "ICLR.cc/2017/conference/-/paper565/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "ryaFG5ige", "writers": {"values-regex": "ICLR.cc/2017/conference/paper565/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper565/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper565/reviewers", "ICLR.cc/2017/conference/paper565/areachairs"], "cdate": 1485287518712}}}, {"tddate": null, "tmdate": 1480803176063, "tcdate": 1480803176058, "number": 2, "id": "H1gu7Te7g", "invitation": "ICLR.cc/2017/conference/-/paper565/official/comment", "forum": "ryaFG5ige", "replyto": "ryaFG5ige", "signatures": ["ICLR.cc/2017/conference/paper565/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper565/AnonReviewer3"], "content": {"title": "Groundtruth?", "comment": "In Figures 1 and 2, what do you mean by groundtruth?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introducing Active Learning for CNN under the light of Variational Inference", "abstract": "One main concern of the deep learning community is to increase the capacity of\nrepresentation of deep networks by increasing their depth. This requires to scale\nup the size of the training database accordingly. Indeed a major intuition lies in\nthe fact that the depth of the network and the size of the training set are strongly\ncorrelated. However recent works tend to show that deep learning may be handled\nwith smaller dataset as long as the training samples are carefully selected (let us\nmention for instance curriculum learning). In this context we introduce a scalable\nand efficient active learning method that can be applied to most neural networks,\nespecially Convolutional Neural Networks (CNN). To the best of our knowledge,\nthis paper is the first of its kind to design an active learning selection scheme based\non a variational inference for neural networks. We also deduced a formulation of\nthe posterior and prior distributions of the weights using statistical knowledge on\nthe Maximum Likelihood Estimator.\nWe describe our strategy to come up with our active learning criterion. We assess its\nconsistency by checking the accuracy obtained by successive active learning steps\non two benchmark datasets MNIST and USPS. We also demonstrate its scalability\ntowards increasing training set size.", "pdf": "/pdf/799e4a649a22a4da58454b5223b37fdbabec0955.pdf", "TL;DR": "Building automatically the labeled training set with active learning for CNN. The criterion is developed on a variational inference for NN and a kronecker approximation of Fisher matrices for CNN", "paperhash": "ducoffe|introducing_active_learning_for_cnn_under_the_light_of_variational_inference", "conflicts": ["unice.fr"], "keywords": ["Deep learning", "Supervised Learning", "Optimization"], "authors": ["Melanie Ducoffe", "Frederic Precioso"], "authorids": ["ducoffe@i3s.unice.fr", "precioso@i3s.unice.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287518712, "id": "ICLR.cc/2017/conference/-/paper565/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "ryaFG5ige", "writers": {"values-regex": "ICLR.cc/2017/conference/paper565/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper565/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper565/reviewers", "ICLR.cc/2017/conference/paper565/areachairs"], "cdate": 1485287518712}}}, {"tddate": null, "tmdate": 1480803142225, "tcdate": 1480803142220, "number": 1, "id": "S1RBQaxQx", "invitation": "ICLR.cc/2017/conference/-/paper565/official/comment", "forum": "ryaFG5ige", "replyto": "ryaFG5ige", "signatures": ["ICLR.cc/2017/conference/paper565/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper565/AnonReviewer3"], "content": {"title": "Overhead of active learning?", "comment": "Section 5.2 studies the time complexity of your approach -- up to 30s to select the elements of one minibatch. How does this compare to the time required for using that minibatch to update the model by backprop?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introducing Active Learning for CNN under the light of Variational Inference", "abstract": "One main concern of the deep learning community is to increase the capacity of\nrepresentation of deep networks by increasing their depth. This requires to scale\nup the size of the training database accordingly. Indeed a major intuition lies in\nthe fact that the depth of the network and the size of the training set are strongly\ncorrelated. However recent works tend to show that deep learning may be handled\nwith smaller dataset as long as the training samples are carefully selected (let us\nmention for instance curriculum learning). In this context we introduce a scalable\nand efficient active learning method that can be applied to most neural networks,\nespecially Convolutional Neural Networks (CNN). To the best of our knowledge,\nthis paper is the first of its kind to design an active learning selection scheme based\non a variational inference for neural networks. We also deduced a formulation of\nthe posterior and prior distributions of the weights using statistical knowledge on\nthe Maximum Likelihood Estimator.\nWe describe our strategy to come up with our active learning criterion. We assess its\nconsistency by checking the accuracy obtained by successive active learning steps\non two benchmark datasets MNIST and USPS. We also demonstrate its scalability\ntowards increasing training set size.", "pdf": "/pdf/799e4a649a22a4da58454b5223b37fdbabec0955.pdf", "TL;DR": "Building automatically the labeled training set with active learning for CNN. The criterion is developed on a variational inference for NN and a kronecker approximation of Fisher matrices for CNN", "paperhash": "ducoffe|introducing_active_learning_for_cnn_under_the_light_of_variational_inference", "conflicts": ["unice.fr"], "keywords": ["Deep learning", "Supervised Learning", "Optimization"], "authors": ["Melanie Ducoffe", "Frederic Precioso"], "authorids": ["ducoffe@i3s.unice.fr", "precioso@i3s.unice.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287518712, "id": "ICLR.cc/2017/conference/-/paper565/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "ryaFG5ige", "writers": {"values-regex": "ICLR.cc/2017/conference/paper565/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper565/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper565/reviewers", "ICLR.cc/2017/conference/paper565/areachairs"], "cdate": 1485287518712}}}, {"tddate": null, "tmdate": 1480803017264, "tcdate": 1480803017260, "number": 3, "id": "BJWRG6xmx", "invitation": "ICLR.cc/2017/conference/-/paper565/pre-review/question", "forum": "ryaFG5ige", "replyto": "ryaFG5ige", "signatures": ["ICLR.cc/2017/conference/paper565/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper565/AnonReviewer3"], "content": {"title": "Simple active learning baseline?", "question": "You mention existing work on the simple approach of uncertainty sampling. Did you try this as a baseline?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introducing Active Learning for CNN under the light of Variational Inference", "abstract": "One main concern of the deep learning community is to increase the capacity of\nrepresentation of deep networks by increasing their depth. This requires to scale\nup the size of the training database accordingly. Indeed a major intuition lies in\nthe fact that the depth of the network and the size of the training set are strongly\ncorrelated. However recent works tend to show that deep learning may be handled\nwith smaller dataset as long as the training samples are carefully selected (let us\nmention for instance curriculum learning). In this context we introduce a scalable\nand efficient active learning method that can be applied to most neural networks,\nespecially Convolutional Neural Networks (CNN). To the best of our knowledge,\nthis paper is the first of its kind to design an active learning selection scheme based\non a variational inference for neural networks. We also deduced a formulation of\nthe posterior and prior distributions of the weights using statistical knowledge on\nthe Maximum Likelihood Estimator.\nWe describe our strategy to come up with our active learning criterion. We assess its\nconsistency by checking the accuracy obtained by successive active learning steps\non two benchmark datasets MNIST and USPS. We also demonstrate its scalability\ntowards increasing training set size.", "pdf": "/pdf/799e4a649a22a4da58454b5223b37fdbabec0955.pdf", "TL;DR": "Building automatically the labeled training set with active learning for CNN. The criterion is developed on a variational inference for NN and a kronecker approximation of Fisher matrices for CNN", "paperhash": "ducoffe|introducing_active_learning_for_cnn_under_the_light_of_variational_inference", "conflicts": ["unice.fr"], "keywords": ["Deep learning", "Supervised Learning", "Optimization"], "authors": ["Melanie Ducoffe", "Frederic Precioso"], "authorids": ["ducoffe@i3s.unice.fr", "precioso@i3s.unice.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959211673, "id": "ICLR.cc/2017/conference/-/paper565/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper565/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper565/AnonReviewer4", "ICLR.cc/2017/conference/paper565/AnonReviewer2", "ICLR.cc/2017/conference/paper565/AnonReviewer3"], "reply": {"forum": "ryaFG5ige", "replyto": "ryaFG5ige", "writers": {"values-regex": "ICLR.cc/2017/conference/paper565/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper565/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959211673}}}, {"tddate": null, "tmdate": 1480741415511, "tcdate": 1480741415508, "number": 2, "id": "Bk1NfRJXe", "invitation": "ICLR.cc/2017/conference/-/paper565/pre-review/question", "forum": "ryaFG5ige", "replyto": "ryaFG5ige", "signatures": ["ICLR.cc/2017/conference/paper565/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper565/AnonReviewer2"], "content": {"title": "Experiments", "question": "In the Experiments of figure 2 what criterion did you use to decide to use 5 or 10 runs of experiments ? Does figure 2 correspond to only one active learning test or are the curves averages over several tests ?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introducing Active Learning for CNN under the light of Variational Inference", "abstract": "One main concern of the deep learning community is to increase the capacity of\nrepresentation of deep networks by increasing their depth. This requires to scale\nup the size of the training database accordingly. Indeed a major intuition lies in\nthe fact that the depth of the network and the size of the training set are strongly\ncorrelated. However recent works tend to show that deep learning may be handled\nwith smaller dataset as long as the training samples are carefully selected (let us\nmention for instance curriculum learning). In this context we introduce a scalable\nand efficient active learning method that can be applied to most neural networks,\nespecially Convolutional Neural Networks (CNN). To the best of our knowledge,\nthis paper is the first of its kind to design an active learning selection scheme based\non a variational inference for neural networks. We also deduced a formulation of\nthe posterior and prior distributions of the weights using statistical knowledge on\nthe Maximum Likelihood Estimator.\nWe describe our strategy to come up with our active learning criterion. We assess its\nconsistency by checking the accuracy obtained by successive active learning steps\non two benchmark datasets MNIST and USPS. We also demonstrate its scalability\ntowards increasing training set size.", "pdf": "/pdf/799e4a649a22a4da58454b5223b37fdbabec0955.pdf", "TL;DR": "Building automatically the labeled training set with active learning for CNN. The criterion is developed on a variational inference for NN and a kronecker approximation of Fisher matrices for CNN", "paperhash": "ducoffe|introducing_active_learning_for_cnn_under_the_light_of_variational_inference", "conflicts": ["unice.fr"], "keywords": ["Deep learning", "Supervised Learning", "Optimization"], "authors": ["Melanie Ducoffe", "Frederic Precioso"], "authorids": ["ducoffe@i3s.unice.fr", "precioso@i3s.unice.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959211673, "id": "ICLR.cc/2017/conference/-/paper565/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper565/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper565/AnonReviewer4", "ICLR.cc/2017/conference/paper565/AnonReviewer2", "ICLR.cc/2017/conference/paper565/AnonReviewer3"], "reply": {"forum": "ryaFG5ige", "replyto": "ryaFG5ige", "writers": {"values-regex": "ICLR.cc/2017/conference/paper565/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper565/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959211673}}}, {"tddate": null, "tmdate": 1480733409557, "tcdate": 1480733313786, "number": 1, "id": "BJctfh1Xg", "invitation": "ICLR.cc/2017/conference/-/paper565/pre-review/question", "forum": "ryaFG5ige", "replyto": "ryaFG5ige", "signatures": ["ICLR.cc/2017/conference/paper565/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper565/AnonReviewer4"], "content": {"title": "A strong assumption?", "question": "In the approximation of Q(\\beta), which is crucial for the importance sampling of the data in the authors' framework of active learning, they assumed the current MLE estimator w_hat, is already a good approximation to the true parameter \\theta_Y^*; how can this be justified? How is the sampling distribution initialized?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Introducing Active Learning for CNN under the light of Variational Inference", "abstract": "One main concern of the deep learning community is to increase the capacity of\nrepresentation of deep networks by increasing their depth. This requires to scale\nup the size of the training database accordingly. Indeed a major intuition lies in\nthe fact that the depth of the network and the size of the training set are strongly\ncorrelated. However recent works tend to show that deep learning may be handled\nwith smaller dataset as long as the training samples are carefully selected (let us\nmention for instance curriculum learning). In this context we introduce a scalable\nand efficient active learning method that can be applied to most neural networks,\nespecially Convolutional Neural Networks (CNN). To the best of our knowledge,\nthis paper is the first of its kind to design an active learning selection scheme based\non a variational inference for neural networks. We also deduced a formulation of\nthe posterior and prior distributions of the weights using statistical knowledge on\nthe Maximum Likelihood Estimator.\nWe describe our strategy to come up with our active learning criterion. We assess its\nconsistency by checking the accuracy obtained by successive active learning steps\non two benchmark datasets MNIST and USPS. We also demonstrate its scalability\ntowards increasing training set size.", "pdf": "/pdf/799e4a649a22a4da58454b5223b37fdbabec0955.pdf", "TL;DR": "Building automatically the labeled training set with active learning for CNN. The criterion is developed on a variational inference for NN and a kronecker approximation of Fisher matrices for CNN", "paperhash": "ducoffe|introducing_active_learning_for_cnn_under_the_light_of_variational_inference", "conflicts": ["unice.fr"], "keywords": ["Deep learning", "Supervised Learning", "Optimization"], "authors": ["Melanie Ducoffe", "Frederic Precioso"], "authorids": ["ducoffe@i3s.unice.fr", "precioso@i3s.unice.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959211673, "id": "ICLR.cc/2017/conference/-/paper565/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper565/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper565/AnonReviewer4", "ICLR.cc/2017/conference/paper565/AnonReviewer2", "ICLR.cc/2017/conference/paper565/AnonReviewer3"], "reply": {"forum": "ryaFG5ige", "replyto": "ryaFG5ige", "writers": {"values-regex": "ICLR.cc/2017/conference/paper565/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper565/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959211673}}}], "count": 21}