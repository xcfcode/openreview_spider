{"notes": [{"id": "SkgzYiRqtX", "original": "r1xQtHcqY7", "number": 424, "cdate": 1538087801831, "ddate": null, "tcdate": 1538087801831, "tmdate": 1545355427883, "tddate": null, "forum": "SkgzYiRqtX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Graph Neural Networks with Generated Parameters for Relation Extraction", "abstract": "Recently, progress has been made towards improving relational reasoning in machine learning field. Among existing models, graph neural networks (GNNs) is one of the most effective approaches for multi-hop relational reasoning. In fact, multi-hop relational reasoning is indispensable in many natural language processing tasks such as relation extraction. In this paper, we propose to generate the parameters of graph neural networks (GP-GNNs) according to natural language sentences, which enables GNNs to process relational reasoning on unstructured text inputs. We verify GP-GNNs in relation extraction from text. Experimental results on a human-annotated dataset and two distantly supervised datasets show that our model achieves significant improvements compared to the baselines. We also perform a qualitative analysis to demonstrate that our model could discover more accurate relations by multi-hop relational reasoning.", "keywords": ["Graph Neural Networks", "Relational Reasoning"], "authorids": ["prokilchu@gmail.com", "linyk14@mails.tsinghua.edu.cn", "liuzy@tsinghua.edu.cn", "full.jeffrey@gmail.com", "chuats@comp.nus.edu.sg", "sms@tsinghua.edu.cn"], "authors": ["Hao Zhu", "Yankai Lin", "Zhiyuan Liu", "Jie Fu", "Tat-seng Chua and Maosong Sun"], "TL;DR": "A graph neural network model with parameters generated from natural languages, which can perform multi-hop reasoning. ", "pdf": "/pdf/b666e5c11e849ceec046b669f30df1b32ef7c3c6.pdf", "paperhash": "zhu|graph_neural_networks_with_generated_parameters_for_relation_extraction", "_bibtex": "@misc{\nzhu2019graph,\ntitle={Graph Neural Networks with Generated Parameters for Relation Extraction},\nauthor={Hao Zhu and Yankai Lin and Zhiyuan Liu and Jie Fu and Tat-seng Chua and Maosong Sun},\nyear={2019},\nurl={https://openreview.net/forum?id=SkgzYiRqtX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BkxJXB4Vx4", "original": null, "number": 1, "cdate": 1544992022837, "ddate": null, "tcdate": 1544992022837, "tmdate": 1545354488138, "tddate": null, "forum": "SkgzYiRqtX", "replyto": "SkgzYiRqtX", "invitation": "ICLR.cc/2019/Conference/-/Paper424/Meta_Review", "content": {"metareview": "+ experiments on an interesting task: inferring relations which are not necessarily explicitly mentioned in a sentence but need to be induced relying on other relations\n+ the idea to frame the relation prediction task as an inference task on a graph is interesting \n\n- the paper is not very well written, and it is hard to understand what exactly the contribution is. E.g., the authors contrast with previous work saying that previous work was relying on pre-defined graphs rather than inducing them. However, here they actually rely on predefined full graphs as well (i.e. full graphs connecting all entities).   (See questions from R1)\n\n- the idea of predicting edge embeddings from the sentence is an interesting one. However, I do not see results studying alternative architectures (e.g., fixed transition matrices + gates / attention), or careful ablation studies. It is hard to say if this modification is indeed necessary / beneficial.  (See also R3, agreeing that experiments look preliminary)\n\n- Extra baselines? E.g., what about layers of multi-head self-attention across entities? (as in Transformer). What about the number of parameters for the proposed model? Is there chance that it works better simply because it is a larger model? (See also R3)\n\n- evaluation only one dataset (not clear if any other datasets of this kind exist though)\n\nOverall, though I find the direction and certain aspects of the model quite interesting, the paper is not ready for publication.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "an interesting direction but not ready for publication yet"}, "signatures": ["ICLR.cc/2019/Conference/Paper424/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper424/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Neural Networks with Generated Parameters for Relation Extraction", "abstract": "Recently, progress has been made towards improving relational reasoning in machine learning field. Among existing models, graph neural networks (GNNs) is one of the most effective approaches for multi-hop relational reasoning. In fact, multi-hop relational reasoning is indispensable in many natural language processing tasks such as relation extraction. In this paper, we propose to generate the parameters of graph neural networks (GP-GNNs) according to natural language sentences, which enables GNNs to process relational reasoning on unstructured text inputs. We verify GP-GNNs in relation extraction from text. Experimental results on a human-annotated dataset and two distantly supervised datasets show that our model achieves significant improvements compared to the baselines. We also perform a qualitative analysis to demonstrate that our model could discover more accurate relations by multi-hop relational reasoning.", "keywords": ["Graph Neural Networks", "Relational Reasoning"], "authorids": ["prokilchu@gmail.com", "linyk14@mails.tsinghua.edu.cn", "liuzy@tsinghua.edu.cn", "full.jeffrey@gmail.com", "chuats@comp.nus.edu.sg", "sms@tsinghua.edu.cn"], "authors": ["Hao Zhu", "Yankai Lin", "Zhiyuan Liu", "Jie Fu", "Tat-seng Chua and Maosong Sun"], "TL;DR": "A graph neural network model with parameters generated from natural languages, which can perform multi-hop reasoning. ", "pdf": "/pdf/b666e5c11e849ceec046b669f30df1b32ef7c3c6.pdf", "paperhash": "zhu|graph_neural_networks_with_generated_parameters_for_relation_extraction", "_bibtex": "@misc{\nzhu2019graph,\ntitle={Graph Neural Networks with Generated Parameters for Relation Extraction},\nauthor={Hao Zhu and Yankai Lin and Zhiyuan Liu and Jie Fu and Tat-seng Chua and Maosong Sun},\nyear={2019},\nurl={https://openreview.net/forum?id=SkgzYiRqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper424/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353222291, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkgzYiRqtX", "replyto": "SkgzYiRqtX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper424/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper424/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper424/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353222291}}}, {"id": "ryxJBSl5CQ", "original": null, "number": 1, "cdate": 1543271734728, "ddate": null, "tcdate": 1543271734728, "tmdate": 1543272139255, "tddate": null, "forum": "SkgzYiRqtX", "replyto": "HkekMdOihm", "invitation": "ICLR.cc/2019/Conference/-/Paper424/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thank the thoughtful advice and the following is our response.\n\n\n> - \u201cl\u201d is not defined -- I assume it denotes the number of tokens in the sentence but |s| is used in other places. Are \u201centires\u201d and \u201centities\u201d the same? a series of tokens => a sequence of tokens.\n\nWe have changed the notations and words accordingly. The original thought of making these distinction is to highlight the difference between GP-GNNs model and relation extraction application. The new version is more consistent.\n\n> Equation (5): \u201cn\u201d is not defined on the right of equation. Does this mean that different layers have LSTMs/MLPs with separate sets of parameters? If it is the case, why do you need the word embeddings at all the layers to construct the transition matrices? Please clarify. \n\nThanks to indicating the minor mistake in our equation, and we have added \u201cn\u201d to the RHS of Eq. 5. In our experiments, we have tried using different and the same set of parameters in LSTMs and MLPs among layers and the results show that using different sets is better. Word embeddings in these layers are shared.\n\n> Does BiLSTM return one vector or a sequence of l vectors? Even MLP needs to be defined.\n\nWe concatenated the tail hidden state of the forward LSTM and the head hidden state of the backward LSTM together to form a single vector instead of a sequence of vectors. We clarified it in the revised version to make it easier to understand. MLP is also formally defined.\n\n> In general I find the concept of \u201cgenerated parameters\u201d even confusing. How would traditional GNNs work in this context? Isn\u2019t the novelty that parameterizing word/positional information in the transition matrix which enables a graph-based neural network working?\nIt would be very important to explain the intuition of this model and make the presentation clear and understandable. I don\u2019t recommend this paper to be accepted in the current format.\n\n\nThe reason we proposed to generate parameters (transition matrices) of GNNs is because \u201ctraditional\u201d GNNs cannot be directly applied in the reasoning tasks of natural languages. Existing attempts to encode relational information in transition matrices (e.g. Schlichtkrull et al.) only learn matrices for a fixed set of relations in a predefined graph. However, for relational reasoning in natural languages (or images), the graph to be reasoned is not predefined and the relations are extracted from natural language sentences, these methods still cannot be applied. Therefore, yes, \u201cparameterizing word/positional information in the transition matrices\u201d is the novelty. \n\n> The empirical results also make me wonder whether this model outperforms other models because the other models work on a single pair of entities while this model attempts to work on all pairs of entities at the same time so that it enables some level of reasoning at the entity level (e.g., language + cast member -> language spoken in Figure 1). If this is the real contribution, the paper has to make it clear enough. \n\n\nOur model is not the only one working on all of the pairs at the same time. In fact, Context-Aware RE, one of the baseline methods, also work on all the pairs simultaneously, but our model still outperforms them including Context-Aware RE.  We thus attribute its success to the superior relational reasoning ability of Graph Neural Networks.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper424/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper424/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper424/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Neural Networks with Generated Parameters for Relation Extraction", "abstract": "Recently, progress has been made towards improving relational reasoning in machine learning field. Among existing models, graph neural networks (GNNs) is one of the most effective approaches for multi-hop relational reasoning. In fact, multi-hop relational reasoning is indispensable in many natural language processing tasks such as relation extraction. In this paper, we propose to generate the parameters of graph neural networks (GP-GNNs) according to natural language sentences, which enables GNNs to process relational reasoning on unstructured text inputs. We verify GP-GNNs in relation extraction from text. Experimental results on a human-annotated dataset and two distantly supervised datasets show that our model achieves significant improvements compared to the baselines. We also perform a qualitative analysis to demonstrate that our model could discover more accurate relations by multi-hop relational reasoning.", "keywords": ["Graph Neural Networks", "Relational Reasoning"], "authorids": ["prokilchu@gmail.com", "linyk14@mails.tsinghua.edu.cn", "liuzy@tsinghua.edu.cn", "full.jeffrey@gmail.com", "chuats@comp.nus.edu.sg", "sms@tsinghua.edu.cn"], "authors": ["Hao Zhu", "Yankai Lin", "Zhiyuan Liu", "Jie Fu", "Tat-seng Chua and Maosong Sun"], "TL;DR": "A graph neural network model with parameters generated from natural languages, which can perform multi-hop reasoning. ", "pdf": "/pdf/b666e5c11e849ceec046b669f30df1b32ef7c3c6.pdf", "paperhash": "zhu|graph_neural_networks_with_generated_parameters_for_relation_extraction", "_bibtex": "@misc{\nzhu2019graph,\ntitle={Graph Neural Networks with Generated Parameters for Relation Extraction},\nauthor={Hao Zhu and Yankai Lin and Zhiyuan Liu and Jie Fu and Tat-seng Chua and Maosong Sun},\nyear={2019},\nurl={https://openreview.net/forum?id=SkgzYiRqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper424/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608278, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkgzYiRqtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper424/Authors", "ICLR.cc/2019/Conference/Paper424/Reviewers", "ICLR.cc/2019/Conference/Paper424/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper424/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper424/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper424/Authors|ICLR.cc/2019/Conference/Paper424/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper424/Reviewers", "ICLR.cc/2019/Conference/Paper424/Authors", "ICLR.cc/2019/Conference/Paper424/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608278}}}, {"id": "HkeucHlq07", "original": null, "number": 2, "cdate": 1543271824410, "ddate": null, "tcdate": 1543271824410, "tmdate": 1543272122474, "tddate": null, "forum": "SkgzYiRqtX", "replyto": "BJelhSgoh7", "invitation": "ICLR.cc/2019/Conference/-/Paper424/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We thank the thoughtful advice and the following is our response.\n\n\n> Why not using mono sentence evaluation on the two distantly labeled datasets? \n\nMono sentence evaluation is usually not performed on the distantly supervised datasets as many instances in the distantly supervised dataset are not correct. The conventional practice is to perform bag-level evaluation on it. \n\n> The performance of the two CNN baselines on the hand labeled dataset are extremely low, what is the reason for that?\n\nAs Zhang et al. 2015 pointed out, \u201cfor sentences with long-distance relations, the RNN model exhibits clear advantages\u201d. Sentences in Wikipedia are much longer than the ones from New York Times corpus (where CNN-based approaches mostly experiment on). For bag-level RE, as long as one instance in the bag is recognized correctly and get the high probability, the bag is recognized correctly. Instead of tackling long-term dependencies, CNNs could achieve a decent result by recognizing short sentencing correctly. However, for instance-level RE, if CNNs make mistakes on longer sentences, there are no shorter sentences it can rely on.\n\n> The improved performance of the proposed model w.r.t. the baselines is attributed to the \u201creasoning\u201d potential of the model, but this explanation falls short. There is no fact in the paper to support this idea, and the Context-aware RE model making use of sentence attention has also the potential of exploiting contextual information and thus might also infer more complex relations. The reason for the improvement has to be found somewhere else.\n\nContext-Aware RE is prone to errors in utilizing contextual information. Consider two relation chains in a sentence: A--R1-->B--R2-->C, D--R3-->C. When applying Context-Aware RE to reason the relationship between A and C, it attends to R1, R2 and R3, despite it has nothing to do with the reasoning chain between A and C. If exists R4, (R1, R3) \u21d2 R4, the relationship between A and C is easily mistaken as R4. We have also shown in the case study that CARE is also influenced by unrelated relations. This problem limits the ability of CARE to do reasoning. We also show that by adding more layers, the performance is significantly improved, which means the multi-hop reasoning mechanism does help in our model.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper424/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper424/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper424/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Neural Networks with Generated Parameters for Relation Extraction", "abstract": "Recently, progress has been made towards improving relational reasoning in machine learning field. Among existing models, graph neural networks (GNNs) is one of the most effective approaches for multi-hop relational reasoning. In fact, multi-hop relational reasoning is indispensable in many natural language processing tasks such as relation extraction. In this paper, we propose to generate the parameters of graph neural networks (GP-GNNs) according to natural language sentences, which enables GNNs to process relational reasoning on unstructured text inputs. We verify GP-GNNs in relation extraction from text. Experimental results on a human-annotated dataset and two distantly supervised datasets show that our model achieves significant improvements compared to the baselines. We also perform a qualitative analysis to demonstrate that our model could discover more accurate relations by multi-hop relational reasoning.", "keywords": ["Graph Neural Networks", "Relational Reasoning"], "authorids": ["prokilchu@gmail.com", "linyk14@mails.tsinghua.edu.cn", "liuzy@tsinghua.edu.cn", "full.jeffrey@gmail.com", "chuats@comp.nus.edu.sg", "sms@tsinghua.edu.cn"], "authors": ["Hao Zhu", "Yankai Lin", "Zhiyuan Liu", "Jie Fu", "Tat-seng Chua and Maosong Sun"], "TL;DR": "A graph neural network model with parameters generated from natural languages, which can perform multi-hop reasoning. ", "pdf": "/pdf/b666e5c11e849ceec046b669f30df1b32ef7c3c6.pdf", "paperhash": "zhu|graph_neural_networks_with_generated_parameters_for_relation_extraction", "_bibtex": "@misc{\nzhu2019graph,\ntitle={Graph Neural Networks with Generated Parameters for Relation Extraction},\nauthor={Hao Zhu and Yankai Lin and Zhiyuan Liu and Jie Fu and Tat-seng Chua and Maosong Sun},\nyear={2019},\nurl={https://openreview.net/forum?id=SkgzYiRqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper424/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608278, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkgzYiRqtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper424/Authors", "ICLR.cc/2019/Conference/Paper424/Reviewers", "ICLR.cc/2019/Conference/Paper424/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper424/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper424/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper424/Authors|ICLR.cc/2019/Conference/Paper424/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper424/Reviewers", "ICLR.cc/2019/Conference/Paper424/Authors", "ICLR.cc/2019/Conference/Paper424/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608278}}}, {"id": "H1loPUeqRQ", "original": null, "number": 3, "cdate": 1543272034788, "ddate": null, "tcdate": 1543272034788, "tmdate": 1543272111988, "tddate": null, "forum": "SkgzYiRqtX", "replyto": "BJlOlHzcnm", "invitation": "ICLR.cc/2019/Conference/-/Paper424/Official_Comment", "content": {"title": "Response to Reviewer 2 (1/2)", "comment": "We thank the thoughtful advice and the following is our response.\n\n> unclear how/why the specific result metrics and their particular range of precision are chosen. Why is F1/Acc only reported for the human-labelled part, but not for the distantly labelled part?\n\nThese specific metrics (PR curve, P@N%, Acc and Macro F1) are directly following previous papers (Zeng et. al 2014, 2015, Lin et. al 2015, Feng et. al 2018) in relation extraction. For relation extraction, only when the precision is high, the model is useful in practice, therefore, only high-precision part is shown.\n\n> task varies from previous versions of the task, posing a potential problem with comparability. What is the motivation behind augmenting the dataset with \u201cNA\u201d labels? Why is the task from previous work altered to predicting relationships between _every_ entity pair? Also unclear: is predicting \u201cNA\u201d actually part of the training loss, and of the evaluation? Is training of the previous \u201cbaseline\u201d models adapted accordingly?\n\n\nIn real-world applications, we do not know which pair of entities have relationships, and we need to examine every pair of entities to verify if they have relationships. It is not realistic to let the model only attend to a subset of entity pairs. Therefore, we regard our setting more realistic and correct. \u201cNA\u201d is a part of loss and evaluation, and baselines are trained accordingly.\n\n> There is similar prior work to this, most prominently Schlichtkrull et al. 2017 [https://arxiv.org/pdf/1703.06103.pdf] who evaluate on Fb15k-237, but also De Cao et al. 2018 [https://arxiv.org/pdf/1808.09920.pdf, published within 1 month before ICLR submission deadline] who evaluate on Wikihop. These previous methods, in fact, do the following: \u201cMost existing GNNs only process multi-hop relational reasoning on pre-defined graphs and cannot be directly applied in natural language relational reasoning.\u201d It would be good to work out differences to these previous models in more detail, especially to Schlichtkrull et al. 2017.\n\n\nWe have added more discussions about Schlichtkrull et al. 2017 and De Cao et al. 2018 to our latest PDF version. Note that although these approaches apply GNNs to natural languages processing tasks, they still apply GNN to predefined graphs: knowledge graphs (Schlichtkrull et al. 2017), co-occurrence and co-reference graphs (De Cao et al. 2018). In sharp contrast, our model could apply directly to natural language sentences instead of predefined graphs.\n\n> \u201cstate-of-the-art baselines\u201d. SOTA defines the best previous work. How can there be several baselines (plural) which are all best? What does SOTA mean when a slight redefinition of the task is proposed, as in this work?\n\n\nSOTA here means the best models on other relation extraction tasks. We admit that this might cause confusion, and we have removed SOTA statement in abstract and introduction in our latest PDF version.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper424/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper424/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper424/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Neural Networks with Generated Parameters for Relation Extraction", "abstract": "Recently, progress has been made towards improving relational reasoning in machine learning field. Among existing models, graph neural networks (GNNs) is one of the most effective approaches for multi-hop relational reasoning. In fact, multi-hop relational reasoning is indispensable in many natural language processing tasks such as relation extraction. In this paper, we propose to generate the parameters of graph neural networks (GP-GNNs) according to natural language sentences, which enables GNNs to process relational reasoning on unstructured text inputs. We verify GP-GNNs in relation extraction from text. Experimental results on a human-annotated dataset and two distantly supervised datasets show that our model achieves significant improvements compared to the baselines. We also perform a qualitative analysis to demonstrate that our model could discover more accurate relations by multi-hop relational reasoning.", "keywords": ["Graph Neural Networks", "Relational Reasoning"], "authorids": ["prokilchu@gmail.com", "linyk14@mails.tsinghua.edu.cn", "liuzy@tsinghua.edu.cn", "full.jeffrey@gmail.com", "chuats@comp.nus.edu.sg", "sms@tsinghua.edu.cn"], "authors": ["Hao Zhu", "Yankai Lin", "Zhiyuan Liu", "Jie Fu", "Tat-seng Chua and Maosong Sun"], "TL;DR": "A graph neural network model with parameters generated from natural languages, which can perform multi-hop reasoning. ", "pdf": "/pdf/b666e5c11e849ceec046b669f30df1b32ef7c3c6.pdf", "paperhash": "zhu|graph_neural_networks_with_generated_parameters_for_relation_extraction", "_bibtex": "@misc{\nzhu2019graph,\ntitle={Graph Neural Networks with Generated Parameters for Relation Extraction},\nauthor={Hao Zhu and Yankai Lin and Zhiyuan Liu and Jie Fu and Tat-seng Chua and Maosong Sun},\nyear={2019},\nurl={https://openreview.net/forum?id=SkgzYiRqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper424/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608278, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkgzYiRqtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper424/Authors", "ICLR.cc/2019/Conference/Paper424/Reviewers", "ICLR.cc/2019/Conference/Paper424/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper424/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper424/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper424/Authors|ICLR.cc/2019/Conference/Paper424/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper424/Reviewers", "ICLR.cc/2019/Conference/Paper424/Authors", "ICLR.cc/2019/Conference/Paper424/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608278}}}, {"id": "ByeiKLgq0m", "original": null, "number": 4, "cdate": 1543272066954, "ddate": null, "tcdate": 1543272066954, "tmdate": 1543272066954, "tddate": null, "forum": "SkgzYiRqtX", "replyto": "BJlOlHzcnm", "invitation": "ICLR.cc/2019/Conference/-/Paper424/Official_Comment", "content": {"title": "Response to Review 2 (2/2)", "comment": "> not a big issue, though this does sound somewhat contradictory: 1) \u201cthe number of layers K is chosen to be of the order of the graph diameter\u201d. 2) \u201cWe treat K as a hyperparameter\u201d\n\nIn general graph, if K >= order of the graph diameter, then all nodes could receive global message. In our fully-connected graph, K >= 1 is enough. We treat K as a hyperparameter and show that as K grows, the reasoning ability is improved. \n\n> not clear: is LSTM indeed exactly the same as GP-GNN with K=1? I assume there is a difference, as the LSTM encodes the entire sentence at once, conditioning entity representations on their local context, whereas in GP-GNN this would not be the case.\n\n\nAs we let the head entity and the tail entities carry special initial embeddings, the embeddings of head and tail entities are conditioned only on the LSTM output encoding their relationship. Hence, GP-GNN (K=1) is the same model as LSTM.\n\n\n> The distinction to Context-Aware RE (CARE) is not clear. The authors argue that CARE models co-occurrence of multiple relations, but is this not what a multi-hop relation extraction _should_ learn? It is also not clear how GP-GNN differs in this respect.\n\nContext-Aware RE is prone to errors in utilizing contextual information. Consider two relation chains in a sentence: A--R1-->B--R2-->C, D--R3-->C. When applying Context-Aware RE to reason the relationship between A and C, it attends to R1, R2 and R3, despite it has nothing to do with the reasoning chain between A and C. If exists R4, (R1, R3) \u21d2 R4, the relationship between A and C is easily mistaken as R4. We have also shown in the case study that CARE is also influenced by unrelated relations. This problem limits the ability of CARE to do reasoning. It is true that multi-hop reasoning learns also the co-occurrence of relations. However, our model models the co-occurrence along reasoning chains.  \n\n> It would be interesting to compare with a model which does not use language to define relation matrices (A), but learns them directly as parameters (independently from the text). \n\nThe reason why we proposed to generate parameters (transition matrices) of GNNs is because \u201ctraditional\u201d GNNs cannot be directly applied in the reasoning tasks of natural languages. Existing attempts to encode relational information in transition matrices (e.g. Schlichtkrull et al.) only learn matrices for a fixed set of relations in a predefined graph. However, for relational reasoning in natural languages (or images), the graph to be reasoned is not predefined and the relations are extracted from natural language sentences, these methods still cannot be applied. \n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper424/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper424/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper424/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Neural Networks with Generated Parameters for Relation Extraction", "abstract": "Recently, progress has been made towards improving relational reasoning in machine learning field. Among existing models, graph neural networks (GNNs) is one of the most effective approaches for multi-hop relational reasoning. In fact, multi-hop relational reasoning is indispensable in many natural language processing tasks such as relation extraction. In this paper, we propose to generate the parameters of graph neural networks (GP-GNNs) according to natural language sentences, which enables GNNs to process relational reasoning on unstructured text inputs. We verify GP-GNNs in relation extraction from text. Experimental results on a human-annotated dataset and two distantly supervised datasets show that our model achieves significant improvements compared to the baselines. We also perform a qualitative analysis to demonstrate that our model could discover more accurate relations by multi-hop relational reasoning.", "keywords": ["Graph Neural Networks", "Relational Reasoning"], "authorids": ["prokilchu@gmail.com", "linyk14@mails.tsinghua.edu.cn", "liuzy@tsinghua.edu.cn", "full.jeffrey@gmail.com", "chuats@comp.nus.edu.sg", "sms@tsinghua.edu.cn"], "authors": ["Hao Zhu", "Yankai Lin", "Zhiyuan Liu", "Jie Fu", "Tat-seng Chua and Maosong Sun"], "TL;DR": "A graph neural network model with parameters generated from natural languages, which can perform multi-hop reasoning. ", "pdf": "/pdf/b666e5c11e849ceec046b669f30df1b32ef7c3c6.pdf", "paperhash": "zhu|graph_neural_networks_with_generated_parameters_for_relation_extraction", "_bibtex": "@misc{\nzhu2019graph,\ntitle={Graph Neural Networks with Generated Parameters for Relation Extraction},\nauthor={Hao Zhu and Yankai Lin and Zhiyuan Liu and Jie Fu and Tat-seng Chua and Maosong Sun},\nyear={2019},\nurl={https://openreview.net/forum?id=SkgzYiRqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper424/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621608278, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkgzYiRqtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper424/Authors", "ICLR.cc/2019/Conference/Paper424/Reviewers", "ICLR.cc/2019/Conference/Paper424/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper424/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper424/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper424/Authors|ICLR.cc/2019/Conference/Paper424/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper424/Reviewers", "ICLR.cc/2019/Conference/Paper424/Authors", "ICLR.cc/2019/Conference/Paper424/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621608278}}}, {"id": "HkekMdOihm", "original": null, "number": 3, "cdate": 1541273607381, "ddate": null, "tcdate": 1541273607381, "tmdate": 1541534007268, "tddate": null, "forum": "SkgzYiRqtX", "replyto": "SkgzYiRqtX", "invitation": "ICLR.cc/2019/Conference/-/Paper424/Official_Review", "content": {"title": "Review", "review": "\nThis paper proposes a new model based on graph neural networks for relation extraction and evaluates it on multiple benchmarks and demonstrates its ability to do some level of \u201cmulti-hop relational reasoning\u201d.\n\nAlthough the empirical results look good and the paper might present some interesting and effective ideas, I find the paper very difficult to follow and many concepts are confusing (and even misleading).\n\nSection 3-4:\n\n- \u201cl\u201d is not defined -- I assume it denotes the number of tokens in the sentence but |s| is used in other places.\n- Are \u201centires\u201d and \u201centities\u201d the same?\n- a series of tokens => a sequence of tokens.\n- Equation (5): \u201cn\u201d is not defined on the right of equation. Does this mean that different layers have LSTMs/MLPs with separate sets of parameters? If it is the case, why do you need the word embeddings at all the layers to construct the transition matrices? Please clarify. \n- Does BiLSTM return one vector or a sequence of l vectors? Even MLP needs to be defined.\n\nIn general I find the concept of \u201cgenerated parameters\u201d even confusing. How would traditional GNNs work in this context? Isn\u2019t the novelty that parameterizing word/positional information in the transition matrix which enables a graph-based neural network working?\n\nIt would be very important to explain the intuition of this model and make the presentation clear and understandable. I don\u2019t recommend this paper to be accepted in the current format.\n\nThe empirical results also make me wonder whether this model outperforms other models because the other models work on a single pair of entities while this model attempts to work on all pairs of entities at the same time so that it enables some level of reasoning at the entity level (e.g., language + cast member -> language spoken in Figure 1). If this is the real contribution, the paper has to make it clear enough. \n\nAnother related paper that needs to be cited:\n\n- Zhang et al, 2018: Graph Convolution over Pruned Dependency Trees Improves Relation Extraction. EMNLP.\n\n\n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper424/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Neural Networks with Generated Parameters for Relation Extraction", "abstract": "Recently, progress has been made towards improving relational reasoning in machine learning field. Among existing models, graph neural networks (GNNs) is one of the most effective approaches for multi-hop relational reasoning. In fact, multi-hop relational reasoning is indispensable in many natural language processing tasks such as relation extraction. In this paper, we propose to generate the parameters of graph neural networks (GP-GNNs) according to natural language sentences, which enables GNNs to process relational reasoning on unstructured text inputs. We verify GP-GNNs in relation extraction from text. Experimental results on a human-annotated dataset and two distantly supervised datasets show that our model achieves significant improvements compared to the baselines. We also perform a qualitative analysis to demonstrate that our model could discover more accurate relations by multi-hop relational reasoning.", "keywords": ["Graph Neural Networks", "Relational Reasoning"], "authorids": ["prokilchu@gmail.com", "linyk14@mails.tsinghua.edu.cn", "liuzy@tsinghua.edu.cn", "full.jeffrey@gmail.com", "chuats@comp.nus.edu.sg", "sms@tsinghua.edu.cn"], "authors": ["Hao Zhu", "Yankai Lin", "Zhiyuan Liu", "Jie Fu", "Tat-seng Chua and Maosong Sun"], "TL;DR": "A graph neural network model with parameters generated from natural languages, which can perform multi-hop reasoning. ", "pdf": "/pdf/b666e5c11e849ceec046b669f30df1b32ef7c3c6.pdf", "paperhash": "zhu|graph_neural_networks_with_generated_parameters_for_relation_extraction", "_bibtex": "@misc{\nzhu2019graph,\ntitle={Graph Neural Networks with Generated Parameters for Relation Extraction},\nauthor={Hao Zhu and Yankai Lin and Zhiyuan Liu and Jie Fu and Tat-seng Chua and Maosong Sun},\nyear={2019},\nurl={https://openreview.net/forum?id=SkgzYiRqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper424/Official_Review", "cdate": 1542234464771, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkgzYiRqtX", "replyto": "SkgzYiRqtX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper424/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335720959, "tmdate": 1552335720959, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper424/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJelhSgoh7", "original": null, "number": 2, "cdate": 1541240231976, "ddate": null, "tcdate": 1541240231976, "tmdate": 1541534007064, "tddate": null, "forum": "SkgzYiRqtX", "replyto": "SkgzYiRqtX", "invitation": "ICLR.cc/2019/Conference/-/Paper424/Official_Review", "content": {"title": "The paper describes a Graph NN method for information extraction from sentences. Some interesting innovations in the paper. The evaluation analysis and the comparison with other models are still preliminary.", "review": "The paper describes a Graph NN method for information extraction from sentences. The objective is to label couples of entities (token or multiple tokens)  according to a given set of relations. The GNN processes token representations through one or more layers and a final classification layer scores the relations between entity couples in the sentence. Parameters of GNN \u2013 transition matrices between layers operating on node representations \u2013 are learned from the data.  Experiments are performed on different variants of an extraction corpus, for the task consisting in extracting all the relations between identified token couples in a sentence. \n\nThe description is clear. The main original contribution is the scheme used for learning the transition matrices between layers from the text input.  Overall, the proposed system is a new mechanism for classifying relations between items in a sentence using GNN. Note that the current title is misleading w.r.t. this goal.\nThe authors claim that the model allows relational reasoning on text. This is somewhat exaggerated, the model performs relation classification and that\u2019s it. There is nothing indicating any form of reasoning and this argument could be removed without reducing the value of the paper.\n\nThe experiments show that the proposed model outperforms, on this classification task baselines, including a recent model. The analysis here should be developed and improved: as it is, it does not provide much information.\nBelow are some questions concerning this evaluation.\nWhy not using mono sentence evaluation on the two distantly labeled datasets? \nThe performance of the two CNN baselines on the hand labeled dataset are extremely low, what is the reason for that?\nThe improved performance of the proposed model w.r.t. the baselines is attributed to the \u201creasoning\u201d potential of the model, but this explanation falls short. There is no fact in the paper to support this idea, and the Context-aware RE model making use of sentence attention has also the potential of exploiting contextual information and thus might also infer more complex relations. The reason for the improvement has to be found somewhere else.\nOverall, there is some interesting innovation in the paper. The evaluation analysis and the comparison with other models are still preliminary.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper424/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Neural Networks with Generated Parameters for Relation Extraction", "abstract": "Recently, progress has been made towards improving relational reasoning in machine learning field. Among existing models, graph neural networks (GNNs) is one of the most effective approaches for multi-hop relational reasoning. In fact, multi-hop relational reasoning is indispensable in many natural language processing tasks such as relation extraction. In this paper, we propose to generate the parameters of graph neural networks (GP-GNNs) according to natural language sentences, which enables GNNs to process relational reasoning on unstructured text inputs. We verify GP-GNNs in relation extraction from text. Experimental results on a human-annotated dataset and two distantly supervised datasets show that our model achieves significant improvements compared to the baselines. We also perform a qualitative analysis to demonstrate that our model could discover more accurate relations by multi-hop relational reasoning.", "keywords": ["Graph Neural Networks", "Relational Reasoning"], "authorids": ["prokilchu@gmail.com", "linyk14@mails.tsinghua.edu.cn", "liuzy@tsinghua.edu.cn", "full.jeffrey@gmail.com", "chuats@comp.nus.edu.sg", "sms@tsinghua.edu.cn"], "authors": ["Hao Zhu", "Yankai Lin", "Zhiyuan Liu", "Jie Fu", "Tat-seng Chua and Maosong Sun"], "TL;DR": "A graph neural network model with parameters generated from natural languages, which can perform multi-hop reasoning. ", "pdf": "/pdf/b666e5c11e849ceec046b669f30df1b32ef7c3c6.pdf", "paperhash": "zhu|graph_neural_networks_with_generated_parameters_for_relation_extraction", "_bibtex": "@misc{\nzhu2019graph,\ntitle={Graph Neural Networks with Generated Parameters for Relation Extraction},\nauthor={Hao Zhu and Yankai Lin and Zhiyuan Liu and Jie Fu and Tat-seng Chua and Maosong Sun},\nyear={2019},\nurl={https://openreview.net/forum?id=SkgzYiRqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper424/Official_Review", "cdate": 1542234464771, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkgzYiRqtX", "replyto": "SkgzYiRqtX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper424/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335720959, "tmdate": 1552335720959, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper424/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJlOlHzcnm", "original": null, "number": 1, "cdate": 1541182703829, "ddate": null, "tcdate": 1541182703829, "tmdate": 1541534006836, "tddate": null, "forum": "SkgzYiRqtX", "replyto": "SkgzYiRqtX", "invitation": "ICLR.cc/2019/Conference/-/Paper424/Official_Review", "content": {"title": "Good results, some questions", "review": "This work proposes a method for parametrising relation matrices in graph neural networks (GNNs) using text. The model is applied in a relation extraction task, and specific dataset subsections are identified to test and analyse \u201chopping\u201d behaviour: a model\u2019s ability to combine multiple relations for inferring a new one.\n\nStrengths\n- strong results\n- testing on both bag-level / single-level relation extraction\n- Insights via multiple ablations \u2014 variation of the number of layers, exploring densely connected data subsections with cycles to identify examples for multi-hop inference\n- evaluation on new, human-annotated test set\n\n\nIssues\n- evaluation only on one task, and one dataset (although, with more detail).\n- unclear how/why the specific result metrics and their particular range of precision are chosen. Why is F1/Acc only reported for the human-labelled part, but not for the distantly labelled part? Why is precision cut off at ~25% recall? A comprehensive aggregate measure across all levels of recall would be more informative, e.g. PR-AUC., and consistently applied in all experiments.\n- task varies from previous versions of the task, posing a potential problem with comparability. What is the motivation behind augmenting the dataset with \u201cNA\u201d labels? Why is the task from previous work altered to predicting relationships between _every_ entity pair? Also unclear: is predicting \u201cNA\u201d actually part of the training loss, and of the evaluation? Is training of the previous \u201cbaseline\u201d models adapted accordingly?\n- some claims appear too bold and vague \u2014 see below.\n- comparatively small modelling innovation\n- There is similar prior work to this, most prominently Schlichtkrull et al. 2017 [https://arxiv.org/pdf/1703.06103.pdf] who evaluate on Fb15k-237, but also De Cao et al. 2018 [https://arxiv.org/pdf/1808.09920.pdf, published within 1 month before ICLR submission deadline] who evaluate on Wikihop. These previous methods in fact do the following: \u201cMost existing GNNs only process multi-hop relational reasoning on pre-defined graphs and cannot be directly applied in natural language relational reasoning.\u201d It would be good to work out differences to these previous models in more detail, especially to Schlichtkrull et al. 2017.\n- unclear how big the specific contribution of the language-generated matrices is. I would normally not obsess about such a baseline, but it seems that the \u201cgenerate using NL\u201d aspect is a core to the paper (even in the title), and this isn\u2019t worked out clearly.\n\n\nMore comments / questions:\n- \u201cmulti-hop relational reasoning is indispensable in many natural language processing tasks such as relation extraction\u201d. This isn\u2019t clear to me, \u201cindispensable\u201d is a very strong wording.\n- \u201cstate-of-the-art baselines\u201d. SOTA defines the best previous work. How can there be several baselines (plural) which are all best? What does SOTA mean when a slight redefinition of the task is proposed, as in this work?\n- \u201cRelation extraction from text is a classic natural language relational reasoning task\u201d \u2014 reference would be useful. \n- not a big issue, though this does sound somewhat contradictory: 1) \u201cthe number of layers K is chosen to be of the order of the graph diameter\u201d. 2) \u201cWe treat K as a hyperparameter\u201d\n- not clear: is LSTM indeed exactly the same as GP-GNN with K=1? I assume there is a difference, as the LSTM encodes the entire sentence at once, conditioning entity representations on their local context, whereas in GP-GNN this would not be the case.\n- The distinction to Context-Aware RE (CARE) is not clear. The authors argue that CARE models co-occurrence of multiple relations, but is this not what a multi-hop relation extraction _should_ learn? It is also not clear how GP-GNN differs in this respect.\n- It would be interesting to compare with a model which does not use language to define relation matrices (A), but learns them directly as parameters (independently from the text). \n- It would be interesting to see an analysis of the matrices A_{i,j}. What does the text generate, and how can one find this out? ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper424/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Neural Networks with Generated Parameters for Relation Extraction", "abstract": "Recently, progress has been made towards improving relational reasoning in machine learning field. Among existing models, graph neural networks (GNNs) is one of the most effective approaches for multi-hop relational reasoning. In fact, multi-hop relational reasoning is indispensable in many natural language processing tasks such as relation extraction. In this paper, we propose to generate the parameters of graph neural networks (GP-GNNs) according to natural language sentences, which enables GNNs to process relational reasoning on unstructured text inputs. We verify GP-GNNs in relation extraction from text. Experimental results on a human-annotated dataset and two distantly supervised datasets show that our model achieves significant improvements compared to the baselines. We also perform a qualitative analysis to demonstrate that our model could discover more accurate relations by multi-hop relational reasoning.", "keywords": ["Graph Neural Networks", "Relational Reasoning"], "authorids": ["prokilchu@gmail.com", "linyk14@mails.tsinghua.edu.cn", "liuzy@tsinghua.edu.cn", "full.jeffrey@gmail.com", "chuats@comp.nus.edu.sg", "sms@tsinghua.edu.cn"], "authors": ["Hao Zhu", "Yankai Lin", "Zhiyuan Liu", "Jie Fu", "Tat-seng Chua and Maosong Sun"], "TL;DR": "A graph neural network model with parameters generated from natural languages, which can perform multi-hop reasoning. ", "pdf": "/pdf/b666e5c11e849ceec046b669f30df1b32ef7c3c6.pdf", "paperhash": "zhu|graph_neural_networks_with_generated_parameters_for_relation_extraction", "_bibtex": "@misc{\nzhu2019graph,\ntitle={Graph Neural Networks with Generated Parameters for Relation Extraction},\nauthor={Hao Zhu and Yankai Lin and Zhiyuan Liu and Jie Fu and Tat-seng Chua and Maosong Sun},\nyear={2019},\nurl={https://openreview.net/forum?id=SkgzYiRqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper424/Official_Review", "cdate": 1542234464771, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkgzYiRqtX", "replyto": "SkgzYiRqtX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper424/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335720959, "tmdate": 1552335720959, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper424/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 9}