{"notes": [{"id": "pg9c6etTWXR", "original": "CzFRatJekrW", "number": 590, "cdate": 1601308071436, "ddate": null, "tcdate": 1601308071436, "tmdate": 1614985709885, "tddate": null, "forum": "pg9c6etTWXR", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Learnable Uncertainty under Laplace Approximations", "authorids": ["~Agustinus_Kristiadi1", "~Matthias_Hein2", "~Philipp_Hennig1"], "authors": ["Agustinus Kristiadi", "Matthias Hein", "Philipp Hennig"], "keywords": ["Bayesian deep learning", "Laplace approximations", "uncertainty quantification"], "abstract": "Laplace approximations are classic, computationally lightweight means to construct Bayesian neural networks (BNNs). As in other approximate BNNs, one cannot necessarily expect the induced predictive uncertainty to be calibrated. Here we develop a formalism to explicitly \"train\" the uncertainty in a decoupled way to the prediction itself. To this end we introduce uncertainty units for Laplace-approximated networks: Hidden units with zero weights that can be added to any pre-trained, point-estimated network. Since these units are inactive, they do not affect the predictions. But their presence changes the geometry (in particular the Hessian) of the loss landscape around the point estimate, thereby affecting the network's uncertainty estimates under a Laplace approximation. We show that such units can be trained via an uncertainty-aware objective, making the Laplace approximation competitive with more expensive alternative uncertainty-quantification frameworks.", "one-sentence_summary": "A new form of hidden units for Bayesian deep learning that only affects uncertainty, not the prediction, and which can be trained post-hoc at low cost.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kristiadi|learnable_uncertainty_under_laplace_approximations", "pdf": "/pdf/9322f1ce4a51da0b3a75c6eb90411a04be39cd11.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LgfpwqInwx", "_bibtex": "@misc{\nkristiadi2021learnable,\ntitle={Learnable Uncertainty under Laplace Approximations},\nauthor={Agustinus Kristiadi and Matthias Hein and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=pg9c6etTWXR}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "3T_5mkzPyqG", "original": null, "number": 1, "cdate": 1610040433354, "ddate": null, "tcdate": 1610040433354, "tmdate": 1610474033660, "tddate": null, "forum": "pg9c6etTWXR", "replyto": "pg9c6etTWXR", "invitation": "ICLR.cc/2021/Conference/Paper590/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper proposes a simple modification to Laplace approximation  to improve the quality of uncertainty estimates in neural networks.\n\nThe key idea is to add \u201cuncertainty units\u201d which do not affect the predictions but change the Hessian of the loss landscape, thereby improving the quality of uncertainty estimates. The \u201cuncertainty units\u201d are themselves trained by minimizing a non-Bayesian objective that minimizes variance on in-distribution data and maximizes variance on known out-of-distribution data. Unlike previous work on outlier exposure and prior networks, the known out-of-distribution data is used only post-hoc.\n\nWhile the idea is interesting and intriguing, the reviewers felt that the current version of the paper falls a bit short of the acceptance threshold (see detailed comments by R3 and R2\u2019s concerns about Bayesian justification for this idea). I encourage the authors to revise and resubmit to a different venue.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learnable Uncertainty under Laplace Approximations", "authorids": ["~Agustinus_Kristiadi1", "~Matthias_Hein2", "~Philipp_Hennig1"], "authors": ["Agustinus Kristiadi", "Matthias Hein", "Philipp Hennig"], "keywords": ["Bayesian deep learning", "Laplace approximations", "uncertainty quantification"], "abstract": "Laplace approximations are classic, computationally lightweight means to construct Bayesian neural networks (BNNs). As in other approximate BNNs, one cannot necessarily expect the induced predictive uncertainty to be calibrated. Here we develop a formalism to explicitly \"train\" the uncertainty in a decoupled way to the prediction itself. To this end we introduce uncertainty units for Laplace-approximated networks: Hidden units with zero weights that can be added to any pre-trained, point-estimated network. Since these units are inactive, they do not affect the predictions. But their presence changes the geometry (in particular the Hessian) of the loss landscape around the point estimate, thereby affecting the network's uncertainty estimates under a Laplace approximation. We show that such units can be trained via an uncertainty-aware objective, making the Laplace approximation competitive with more expensive alternative uncertainty-quantification frameworks.", "one-sentence_summary": "A new form of hidden units for Bayesian deep learning that only affects uncertainty, not the prediction, and which can be trained post-hoc at low cost.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kristiadi|learnable_uncertainty_under_laplace_approximations", "pdf": "/pdf/9322f1ce4a51da0b3a75c6eb90411a04be39cd11.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LgfpwqInwx", "_bibtex": "@misc{\nkristiadi2021learnable,\ntitle={Learnable Uncertainty under Laplace Approximations},\nauthor={Agustinus Kristiadi and Matthias Hein and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=pg9c6etTWXR}\n}"}, "tags": [], "invitation": {"reply": {"forum": "pg9c6etTWXR", "replyto": "pg9c6etTWXR", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040433340, "tmdate": 1610474033644, "id": "ICLR.cc/2021/Conference/Paper590/-/Decision"}}}, {"id": "L4N4qpUBoAe", "original": null, "number": 4, "cdate": 1604014020934, "ddate": null, "tcdate": 1604014020934, "tmdate": 1607311466925, "tddate": null, "forum": "pg9c6etTWXR", "replyto": "pg9c6etTWXR", "invitation": "ICLR.cc/2021/Conference/Paper590/-/Official_Review", "content": {"title": "Simple, yet powerful idea to tune uncertainty.", "review": "POST DISCUSSION UPDATE\n------------------------------------\nI like the proposed method and I will keep my score.\n\nEND OF UPDATE\n------------------------------------\nThe paper proposes a new method to learn uncertainty under Laplace approximations. The method relies on uncertainty units that do not change the predictions but increase the dimensionality of the parameters and help learn better uncertainty by the proposed loss function.\n\nStrong points:\n\n- The paper proposes an ad-hoc method that can be applied to any MAP trained network.\n- The proposed method is simple, powerful, and not expensive relative to the training time.\n- Good experimental analysis.\n\n\nClearly state your recommendation (accept or reject) with one or two key reasons for this choice.\n- I recommend to accept this paper.\n- This type of work is definitely needed to enable more powerful models that have accurate predictions and better uncertainty estimates with small additional cost.\n\n\nQuestions:\n- What are the hyperparameters that the method is sensitive for? For example, the size of OOD, number of epochs, learning rate,..,etc\n- How was the number of LULA units chosen? Why did you choose 64? Why not try 32 or less?\n- Were the hyperparametrs of LULA tuned on a separate validation set than the one used to tune the uncertainty?\n\nMinor comments:\n- Table 1 is very hard to read. It would be great if the best numbers are in bold as in Table 2.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper590/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper590/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learnable Uncertainty under Laplace Approximations", "authorids": ["~Agustinus_Kristiadi1", "~Matthias_Hein2", "~Philipp_Hennig1"], "authors": ["Agustinus Kristiadi", "Matthias Hein", "Philipp Hennig"], "keywords": ["Bayesian deep learning", "Laplace approximations", "uncertainty quantification"], "abstract": "Laplace approximations are classic, computationally lightweight means to construct Bayesian neural networks (BNNs). As in other approximate BNNs, one cannot necessarily expect the induced predictive uncertainty to be calibrated. Here we develop a formalism to explicitly \"train\" the uncertainty in a decoupled way to the prediction itself. To this end we introduce uncertainty units for Laplace-approximated networks: Hidden units with zero weights that can be added to any pre-trained, point-estimated network. Since these units are inactive, they do not affect the predictions. But their presence changes the geometry (in particular the Hessian) of the loss landscape around the point estimate, thereby affecting the network's uncertainty estimates under a Laplace approximation. We show that such units can be trained via an uncertainty-aware objective, making the Laplace approximation competitive with more expensive alternative uncertainty-quantification frameworks.", "one-sentence_summary": "A new form of hidden units for Bayesian deep learning that only affects uncertainty, not the prediction, and which can be trained post-hoc at low cost.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kristiadi|learnable_uncertainty_under_laplace_approximations", "pdf": "/pdf/9322f1ce4a51da0b3a75c6eb90411a04be39cd11.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LgfpwqInwx", "_bibtex": "@misc{\nkristiadi2021learnable,\ntitle={Learnable Uncertainty under Laplace Approximations},\nauthor={Agustinus Kristiadi and Matthias Hein and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=pg9c6etTWXR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "pg9c6etTWXR", "replyto": "pg9c6etTWXR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper590/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538139714, "tmdate": 1606915781851, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper590/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper590/-/Official_Review"}}}, {"id": "6WjZqHZQRnR", "original": null, "number": 2, "cdate": 1603831115817, "ddate": null, "tcdate": 1603831115817, "tmdate": 1607095408758, "tddate": null, "forum": "pg9c6etTWXR", "replyto": "pg9c6etTWXR", "invitation": "ICLR.cc/2021/Conference/Paper590/-/Official_Review", "content": {"title": "An Interesting Predictive Variance Module in form of Laplacian approximation", "review": "#################\nPost-Rebuttal Reviews: Thank the authors for the detailed responses. The proposed approach is an interesting add-up for the Laplacian approximations. However, I think the paper still deserves more works. As far as I am concerned, applying the approach to multi-layers instead of only the output-layer is important. I will keep my score for now.\n\n##################\n\nBayesian deep learning aims to bring uncertainty quantification to modern neural network models. Laplacian approximation, which transforms a trained deterministic neural network into stochastic by a second-order Taylor approximation, is especially appealing due to its pos-hoc nature. However, since the network parameters are trained and fixed, few parameters can be optimized in Laplacian approximation. To enhance the capacity of Laplacian approximation, this paper proposes the *learnable uncertainty units (LULA)*. The LULA units do not affect the network prediction, but the Hessians of the LULA units are nonzero. In consequence, training LULA units increases the capacity of Laplacian approximation by adapting the Hessians. \n\nIn general I think the paper proposes an interesting module for parameterizing the predictive variances. The proposed $v(x)$ in Eq(5) is similar to the expression of the neural tangent kernel (Jacot et. al., \u200e2018), thus might be suitable for parameterizing the predictive variance. However, the current version of the paper leaves me several questions regarding the methods. \n\n1. It seems to me that the weight matrix does not affect the prediction as long as it is in the form of $\\tilde{W}^{(l)} = [ W_{map}^{(l)} \\; 0 \\;  \\backslash \\backslash  \\; \\hat{W}_1^{(l)} \\; \\hat{W}_2^{(l)} ]$. However, the proposed method sets $\\hat{W}_2^{(l)}$ as zero. Setting $\\hat{W}_2^{(l)}=0$ blocks the backpropagation through the augmented hidden units, which does not seem reasonable to me. \n2. The approximate Hessian $\\hat{\\Sigma}$ depends on the augmented weights, do you backprop though it when computing the gradients ? \n3. The predictive variance is $J^\\top \\Sigma J$. For multi-output networks like in classification, how do you compute it efficiently ?\n\n**Experiments**\nThis paper conducts experiments for out-of-distribution detection in order to test the performance. However, in my perspective, the experimental results are not strong enough to prove its superiority.\n1. More experiments on uncertainty quantification can be conducted to support the proposed methods. For example, the adversarial example experiment (Ritter et al, 2018) and the calibration experiment (Guo et. al., 2017). These experiments are only my suggestions. I think any experiment regarding to uncertainty quantification would be helpful.\n2. The paper uses Laplacian approximation only for the last layer, and all previous layers act as feature extractor. In this scenario, the proposed method is not that different with a parameterized variance neural network $f_{var}(x)$. Then the Laplacian approximation formulation in the paper is not very useful. Moreover, using only the last layer is not a fair comparison with KFL (Ritter et. al. 2018) either.\n3. Compared with KFL+LL and KFL+OOD, the vanilla KFL seems to be better based on Table 1 and Table 2. However, KFL+LL and KFL+OOD are fine-tuned versions of KFL, it is weird that tuning the prior variance hurts the performance. \n\n**A few typos**\nEq(2) misses the const $\\log p(D)$; \ntwo paragraphs before prop1, \"where d it the resulting number\"\n\n**References**\nJacot et. al., \u200e2018, Neural Tangent Kernel: Convergence and Generalization in Neural Networks\nRitter et. al. 2018, A scalable laplace approximation for neural networks\nGuo et. al., 2017, On Calibration of Modern Neural Networks", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper590/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper590/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learnable Uncertainty under Laplace Approximations", "authorids": ["~Agustinus_Kristiadi1", "~Matthias_Hein2", "~Philipp_Hennig1"], "authors": ["Agustinus Kristiadi", "Matthias Hein", "Philipp Hennig"], "keywords": ["Bayesian deep learning", "Laplace approximations", "uncertainty quantification"], "abstract": "Laplace approximations are classic, computationally lightweight means to construct Bayesian neural networks (BNNs). As in other approximate BNNs, one cannot necessarily expect the induced predictive uncertainty to be calibrated. Here we develop a formalism to explicitly \"train\" the uncertainty in a decoupled way to the prediction itself. To this end we introduce uncertainty units for Laplace-approximated networks: Hidden units with zero weights that can be added to any pre-trained, point-estimated network. Since these units are inactive, they do not affect the predictions. But their presence changes the geometry (in particular the Hessian) of the loss landscape around the point estimate, thereby affecting the network's uncertainty estimates under a Laplace approximation. We show that such units can be trained via an uncertainty-aware objective, making the Laplace approximation competitive with more expensive alternative uncertainty-quantification frameworks.", "one-sentence_summary": "A new form of hidden units for Bayesian deep learning that only affects uncertainty, not the prediction, and which can be trained post-hoc at low cost.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kristiadi|learnable_uncertainty_under_laplace_approximations", "pdf": "/pdf/9322f1ce4a51da0b3a75c6eb90411a04be39cd11.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LgfpwqInwx", "_bibtex": "@misc{\nkristiadi2021learnable,\ntitle={Learnable Uncertainty under Laplace Approximations},\nauthor={Agustinus Kristiadi and Matthias Hein and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=pg9c6etTWXR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "pg9c6etTWXR", "replyto": "pg9c6etTWXR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper590/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538139714, "tmdate": 1606915781851, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper590/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper590/-/Official_Review"}}}, {"id": "bOZR-qQoK2l", "original": null, "number": 1, "cdate": 1603610396704, "ddate": null, "tcdate": 1603610396704, "tmdate": 1606803316383, "tddate": null, "forum": "pg9c6etTWXR", "replyto": "pg9c6etTWXR", "invitation": "ICLR.cc/2021/Conference/Paper590/-/Official_Review", "content": {"title": "Not a correct way of doing Bayesian inference", "review": "The paper proposes a post-hoc uncertainty tuning pipeline for Bayesian neural networks. After getting the point estimate, it adds extra dimensions to the weight matrices and hidden layers, which has no effect on the network output, with the hope that it would influence the variance of the original network weights under the Laplacian approximation. More specifically, it tunes the extra weights by optimizing another objective borrowed from the non-Bayesian robust learning literature, which encourages low uncertainty over real (extra, validation) data, and high uncertainty over manually constructed, out-of-distribution data.\n\nUsing Eq (7) and (8) to quantify posterior uncertainty doesn't seem correct to me. Instead of manually building some fake OOD data and forcing their posterior variance to be large, plus forcing all real data points' posterior variance to be small, one should focus more on the different posterior uncertainty *within* the real observed data. For example, data in highly uncertain areas should naturally have higher posterior variance. But in the proposed method, they are all forced to have small variances as long as they are real data.\n\nAnother confusion I have is more related to the math behind. The extra network weights and hidden dimensions are designed not to have any connection to generate the output. If so, does it mean that no matter what values the extra network weights take, their curvature to the output prediction should be entirely flat? How would these values affect the uncertainty if their Hessian values are zero? It would be beneficial if the authors could explain more about why that's not the case.\n\n\n--------------------------\nPOST DISCUSSION UPDATE\n\nThe central part of this work about how the extra network weights only affect the curvature still confuses me. But I'm more motivated by the proposed objective function that borrows idea from non-Bayesian robust learning literature.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper590/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper590/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learnable Uncertainty under Laplace Approximations", "authorids": ["~Agustinus_Kristiadi1", "~Matthias_Hein2", "~Philipp_Hennig1"], "authors": ["Agustinus Kristiadi", "Matthias Hein", "Philipp Hennig"], "keywords": ["Bayesian deep learning", "Laplace approximations", "uncertainty quantification"], "abstract": "Laplace approximations are classic, computationally lightweight means to construct Bayesian neural networks (BNNs). As in other approximate BNNs, one cannot necessarily expect the induced predictive uncertainty to be calibrated. Here we develop a formalism to explicitly \"train\" the uncertainty in a decoupled way to the prediction itself. To this end we introduce uncertainty units for Laplace-approximated networks: Hidden units with zero weights that can be added to any pre-trained, point-estimated network. Since these units are inactive, they do not affect the predictions. But their presence changes the geometry (in particular the Hessian) of the loss landscape around the point estimate, thereby affecting the network's uncertainty estimates under a Laplace approximation. We show that such units can be trained via an uncertainty-aware objective, making the Laplace approximation competitive with more expensive alternative uncertainty-quantification frameworks.", "one-sentence_summary": "A new form of hidden units for Bayesian deep learning that only affects uncertainty, not the prediction, and which can be trained post-hoc at low cost.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kristiadi|learnable_uncertainty_under_laplace_approximations", "pdf": "/pdf/9322f1ce4a51da0b3a75c6eb90411a04be39cd11.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LgfpwqInwx", "_bibtex": "@misc{\nkristiadi2021learnable,\ntitle={Learnable Uncertainty under Laplace Approximations},\nauthor={Agustinus Kristiadi and Matthias Hein and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=pg9c6etTWXR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "pg9c6etTWXR", "replyto": "pg9c6etTWXR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper590/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538139714, "tmdate": 1606915781851, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper590/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper590/-/Official_Review"}}}, {"id": "i12Xhg-_GJG", "original": null, "number": 3, "cdate": 1603883606785, "ddate": null, "tcdate": 1603883606785, "tmdate": 1606775305996, "tddate": null, "forum": "pg9c6etTWXR", "replyto": "pg9c6etTWXR", "invitation": "ICLR.cc/2021/Conference/Paper590/-/Official_Review", "content": {"title": "Interesting idea with some unclear points", "review": "***\nPOST DISCUSSION UPDATE\n***\nI am satisfied with the authors' response and will increase my score to an accept.\n***\nEND OF UPDATE\n***\n\nThe paper proposes an uncertainty estimation method for deep learning. The idea is, building on prior work on Laplace approximations for neural networks, to augment a pre-trained network with additional units such that predictions with point estimates remain unaffected, while the variance may change. The weights of those units are then trained on the validation set/out-of-distribution (OOD) data such as to minimise the variance on the in-distribution data and maximise it on OOD data.\n\nOverall, I think this is a novel and interesting method, which could be useful in practice considering that it builds on pre-trained networks. There are a couple of points which are unclear to me (see below for details) as well as some details regarding the experiments which could be improved, so that I would **not recommend acceptance** at this point, however I believe that these can be rectified over the discussion period.\n\nUnclear points:\n* How exactly are you training the LULA weights considering that the Hessian (and hence the variance of the corresponding weights and therefore the samples in the MC integral in eq 8) depends on them? I assume you are treating the Hessian as constant? Surely differentiating through its computation would be prohibitively expensive considering that it requires a pass over the entire training dataset? Would it be possible to construct an example (e.g. a reduced version of MNIST) where you can differentiate through the Hessian to see if this has much of an impact?\n* I'm somewhat confused by the discussion regarding the closed-form approximations for the predictive distribution around eq. 4. Isn't the point of those to avoid an MC approximation of the integral over the posterior? I understand that computing eq. 5 efficiently for a Laplace approximation over all weights is not generally supported in Pytorch and tensorflow, but wouldn't it be feasible for the last-layer approximation that you use? Or is the whole discussion mainly to motivate the LULA objective in eq. 7?\n* Are you using the multi-class equivalents of eq 4 and 5 for test-time prediction or do you do a MC integral over the approximate posterior? \n\nExperiments:\n* The setup seems slightly unfair to me in that proposed method is the only one (except KFL+OOD) to be tuned on out-of-distribution data. It would be good to see the DPNs included in Tab. 1 & 2.\n* In Tab. 1 it looks like the test predictions from the proposed method are somewhat underconfident. Could you also report test log likelihoods?\n* That being said, I find the evaluation a bit narrow in that it only looks at out-of-distribution detection. Could you add e.g. an evaluation of the robustness to data shift? For example, making the predictions using your trained networks for the corrupted CIFAR10 and CIFAR100 variants from (Hendrycks & Dietterich, 2019) shouldn't be too much trouble. See e.g. (Ovadia et al., 2019) for a recent use of those datasets in the literature. I understand that if you're making predictions using eq. 4 accuracies would be unaffected, but it would still be interesting to see log likelihoods and calibration errors.\n* Would it be possible to use a more recent architecture for CIFAR100? 50% test accuracy seem somewhat subpar e.g. compared to even small Resnets (which get over 70%).\n* Finally, as more of an optional request/suggestion, it would be nice to have some experiment in a domain other than image classification. Considering that you're only implementing inference for the last layer, I would imagine that your method would be compatible with any architecture (e.g. RNN, Transformer) that feeds into a fully-connected output layer? Having more variety in the empirical tasks would strengthen the paper.\n\nReferences:\nHendrycks & Dietterich. Benchmarking Neural Network Robustness to Common Corruptions and Perturbations. In ICLR 2019.\nOvadia et al. Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift. In Neurips 2019.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper590/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper590/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learnable Uncertainty under Laplace Approximations", "authorids": ["~Agustinus_Kristiadi1", "~Matthias_Hein2", "~Philipp_Hennig1"], "authors": ["Agustinus Kristiadi", "Matthias Hein", "Philipp Hennig"], "keywords": ["Bayesian deep learning", "Laplace approximations", "uncertainty quantification"], "abstract": "Laplace approximations are classic, computationally lightweight means to construct Bayesian neural networks (BNNs). As in other approximate BNNs, one cannot necessarily expect the induced predictive uncertainty to be calibrated. Here we develop a formalism to explicitly \"train\" the uncertainty in a decoupled way to the prediction itself. To this end we introduce uncertainty units for Laplace-approximated networks: Hidden units with zero weights that can be added to any pre-trained, point-estimated network. Since these units are inactive, they do not affect the predictions. But their presence changes the geometry (in particular the Hessian) of the loss landscape around the point estimate, thereby affecting the network's uncertainty estimates under a Laplace approximation. We show that such units can be trained via an uncertainty-aware objective, making the Laplace approximation competitive with more expensive alternative uncertainty-quantification frameworks.", "one-sentence_summary": "A new form of hidden units for Bayesian deep learning that only affects uncertainty, not the prediction, and which can be trained post-hoc at low cost.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kristiadi|learnable_uncertainty_under_laplace_approximations", "pdf": "/pdf/9322f1ce4a51da0b3a75c6eb90411a04be39cd11.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LgfpwqInwx", "_bibtex": "@misc{\nkristiadi2021learnable,\ntitle={Learnable Uncertainty under Laplace Approximations},\nauthor={Agustinus Kristiadi and Matthias Hein and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=pg9c6etTWXR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "pg9c6etTWXR", "replyto": "pg9c6etTWXR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper590/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538139714, "tmdate": 1606915781851, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper590/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper590/-/Official_Review"}}}, {"id": "UkNiBxyx40", "original": null, "number": 12, "cdate": 1606149718968, "ddate": null, "tcdate": 1606149718968, "tmdate": 1606149718968, "tddate": null, "forum": "pg9c6etTWXR", "replyto": "yh9ofRbgBvN", "invitation": "ICLR.cc/2021/Conference/Paper590/-/Official_Comment", "content": {"title": "Thanks a lot for your prompt response", "comment": "Thanks a lot for your prompt response.\n\nYes, you are right, we can maintain the original network function even if we remove the constraint $\\widehat{W}_2^{(l)} = 0$, as long as the last-layer LULA weight matrix deactivates the penultimate LULA units. Thank you for this very useful suggestion, which we will add to the text (and add a thankful note in the acknowledgments)!\n\nWe have applied a revision to the manuscript to reflect your suggestion, which consists of minor changes in eq. 6 and the proof of Prop. 1.\n\n(For clarity, especially to other readers: Since we focus on last-layer LULA, mostly due to computational constraints, this generalization does not affect our experimental results). "}, "signatures": ["ICLR.cc/2021/Conference/Paper590/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper590/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learnable Uncertainty under Laplace Approximations", "authorids": ["~Agustinus_Kristiadi1", "~Matthias_Hein2", "~Philipp_Hennig1"], "authors": ["Agustinus Kristiadi", "Matthias Hein", "Philipp Hennig"], "keywords": ["Bayesian deep learning", "Laplace approximations", "uncertainty quantification"], "abstract": "Laplace approximations are classic, computationally lightweight means to construct Bayesian neural networks (BNNs). As in other approximate BNNs, one cannot necessarily expect the induced predictive uncertainty to be calibrated. Here we develop a formalism to explicitly \"train\" the uncertainty in a decoupled way to the prediction itself. To this end we introduce uncertainty units for Laplace-approximated networks: Hidden units with zero weights that can be added to any pre-trained, point-estimated network. Since these units are inactive, they do not affect the predictions. But their presence changes the geometry (in particular the Hessian) of the loss landscape around the point estimate, thereby affecting the network's uncertainty estimates under a Laplace approximation. We show that such units can be trained via an uncertainty-aware objective, making the Laplace approximation competitive with more expensive alternative uncertainty-quantification frameworks.", "one-sentence_summary": "A new form of hidden units for Bayesian deep learning that only affects uncertainty, not the prediction, and which can be trained post-hoc at low cost.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kristiadi|learnable_uncertainty_under_laplace_approximations", "pdf": "/pdf/9322f1ce4a51da0b3a75c6eb90411a04be39cd11.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LgfpwqInwx", "_bibtex": "@misc{\nkristiadi2021learnable,\ntitle={Learnable Uncertainty under Laplace Approximations},\nauthor={Agustinus Kristiadi and Matthias Hein and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=pg9c6etTWXR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "pg9c6etTWXR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper590/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper590/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper590/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper590/Authors|ICLR.cc/2021/Conference/Paper590/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper590/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869321, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper590/-/Official_Comment"}}}, {"id": "yh9ofRbgBvN", "original": null, "number": 11, "cdate": 1606085262926, "ddate": null, "tcdate": 1606085262926, "tmdate": 1606085262926, "tddate": null, "forum": "pg9c6etTWXR", "replyto": "6i0-i4f2TG", "invitation": "ICLR.cc/2021/Conference/Paper590/-/Official_Comment", "content": {"title": "Reponse", "comment": "Thank you for your responses. \n\nFor the first point, my point is that even if you set $\\hat{W}_2=0$, the function still equals to the original function (**Am I wrong about this ?**). Therefore, we can add this freely in your model without any compromises. I think adding it will make the resulting model much more expressive, given that it passes more gradient information. "}, "signatures": ["ICLR.cc/2021/Conference/Paper590/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper590/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learnable Uncertainty under Laplace Approximations", "authorids": ["~Agustinus_Kristiadi1", "~Matthias_Hein2", "~Philipp_Hennig1"], "authors": ["Agustinus Kristiadi", "Matthias Hein", "Philipp Hennig"], "keywords": ["Bayesian deep learning", "Laplace approximations", "uncertainty quantification"], "abstract": "Laplace approximations are classic, computationally lightweight means to construct Bayesian neural networks (BNNs). As in other approximate BNNs, one cannot necessarily expect the induced predictive uncertainty to be calibrated. Here we develop a formalism to explicitly \"train\" the uncertainty in a decoupled way to the prediction itself. To this end we introduce uncertainty units for Laplace-approximated networks: Hidden units with zero weights that can be added to any pre-trained, point-estimated network. Since these units are inactive, they do not affect the predictions. But their presence changes the geometry (in particular the Hessian) of the loss landscape around the point estimate, thereby affecting the network's uncertainty estimates under a Laplace approximation. We show that such units can be trained via an uncertainty-aware objective, making the Laplace approximation competitive with more expensive alternative uncertainty-quantification frameworks.", "one-sentence_summary": "A new form of hidden units for Bayesian deep learning that only affects uncertainty, not the prediction, and which can be trained post-hoc at low cost.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kristiadi|learnable_uncertainty_under_laplace_approximations", "pdf": "/pdf/9322f1ce4a51da0b3a75c6eb90411a04be39cd11.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LgfpwqInwx", "_bibtex": "@misc{\nkristiadi2021learnable,\ntitle={Learnable Uncertainty under Laplace Approximations},\nauthor={Agustinus Kristiadi and Matthias Hein and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=pg9c6etTWXR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "pg9c6etTWXR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper590/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper590/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper590/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper590/Authors|ICLR.cc/2021/Conference/Paper590/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper590/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869321, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper590/-/Official_Comment"}}}, {"id": "6i0-i4f2TG", "original": null, "number": 10, "cdate": 1605961607556, "ddate": null, "tcdate": 1605961607556, "tmdate": 1605961607556, "tddate": null, "forum": "pg9c6etTWXR", "replyto": "_6vO55Y4MfA", "invitation": "ICLR.cc/2021/Conference/Paper590/-/Official_Comment", "content": {"title": "Thank you very much for the swift response.", "comment": "Thank you very much for the swift response.\n\nFor your first point: Indeed training $\\widehat{W}_2$, i.e. not forcing it to zero, would further increase the prior flexibility. However, crucially the main point of this paper is to _only_ train the uncertainty, without affecting the function that the MAP-trained network represents. The constraint $\\widehat{W}_2 = 0$ is a key factor for ensuring this. Please refer to Prop. 1 (and its proof in the appendix).\n\nRegarding all-layer Laplace: This issue seems to be nearly orthogonal to our paper in our view. Previous works have studied and compared last-layer (LLL) against all-layer Laplaces [1] (or more generally last-layer against all-layer Bayesian methods [2, 3, 4, etc.]) and concluded last-layer Bayesian methods are competitive to the all-layer counterparts, even though the latter is much more expensive. Thus, LLL is useful for cheaply applying Laplace approximations to very deep networks (e.g. DenseNet-121 in the appendix). Finally, by showing results on LLL, we essentially show that LULA can improve the UQ performance of even such a simple Bayesian method, making it much more competitive to more expensive, strong baselines such as Deep Ensemble.\n\nRefs.\n1. Kristiadi, et al. \"Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks.\" ICML 2020.\n2. Ovadia et al. \u201cCan You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift.\u201d In Neurips 2019.\n3. Brosse, et al. \"On Last-Layer Algorithms for Classification: Decoupling Representation from Uncertainty Estimation.\" arXiv 2020.\n4. Ober & Rasmussen. \"Benchmarking the Neural Linear Model for Regression.\" arXiv 2019."}, "signatures": ["ICLR.cc/2021/Conference/Paper590/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper590/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learnable Uncertainty under Laplace Approximations", "authorids": ["~Agustinus_Kristiadi1", "~Matthias_Hein2", "~Philipp_Hennig1"], "authors": ["Agustinus Kristiadi", "Matthias Hein", "Philipp Hennig"], "keywords": ["Bayesian deep learning", "Laplace approximations", "uncertainty quantification"], "abstract": "Laplace approximations are classic, computationally lightweight means to construct Bayesian neural networks (BNNs). As in other approximate BNNs, one cannot necessarily expect the induced predictive uncertainty to be calibrated. Here we develop a formalism to explicitly \"train\" the uncertainty in a decoupled way to the prediction itself. To this end we introduce uncertainty units for Laplace-approximated networks: Hidden units with zero weights that can be added to any pre-trained, point-estimated network. Since these units are inactive, they do not affect the predictions. But their presence changes the geometry (in particular the Hessian) of the loss landscape around the point estimate, thereby affecting the network's uncertainty estimates under a Laplace approximation. We show that such units can be trained via an uncertainty-aware objective, making the Laplace approximation competitive with more expensive alternative uncertainty-quantification frameworks.", "one-sentence_summary": "A new form of hidden units for Bayesian deep learning that only affects uncertainty, not the prediction, and which can be trained post-hoc at low cost.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kristiadi|learnable_uncertainty_under_laplace_approximations", "pdf": "/pdf/9322f1ce4a51da0b3a75c6eb90411a04be39cd11.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LgfpwqInwx", "_bibtex": "@misc{\nkristiadi2021learnable,\ntitle={Learnable Uncertainty under Laplace Approximations},\nauthor={Agustinus Kristiadi and Matthias Hein and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=pg9c6etTWXR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "pg9c6etTWXR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper590/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper590/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper590/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper590/Authors|ICLR.cc/2021/Conference/Paper590/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper590/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869321, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper590/-/Official_Comment"}}}, {"id": "_6vO55Y4MfA", "original": null, "number": 9, "cdate": 1605893540481, "ddate": null, "tcdate": 1605893540481, "tmdate": 1605893540481, "tddate": null, "forum": "pg9c6etTWXR", "replyto": "Tvds2wTQOf9", "invitation": "ICLR.cc/2021/Conference/Paper590/-/Official_Comment", "content": {"title": "Response", "comment": "Thanks for your responses.\n\n**Backpropagation through the augmented hidden units is blocked?** \nSorry I do not think your explanation is strong enough to support your design about setting $\\hat{W}_2 = 0$. Training $\\hat{W}_2 \\neq 0$ as well further increases the prior flexibility, so I don't see a reason why don't do that.\n\n**Only last-layer Laplace? Difference to networks with variance-output?** \nI agree that \"one can apply all-layer Laplace on a LULA-augmented network\". In fact, I think applying all-layer Laplace (LLL) is much more interesting than conducting only the last-layer Laplace. Therefore, I think LLL should be studied as a major part."}, "signatures": ["ICLR.cc/2021/Conference/Paper590/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper590/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learnable Uncertainty under Laplace Approximations", "authorids": ["~Agustinus_Kristiadi1", "~Matthias_Hein2", "~Philipp_Hennig1"], "authors": ["Agustinus Kristiadi", "Matthias Hein", "Philipp Hennig"], "keywords": ["Bayesian deep learning", "Laplace approximations", "uncertainty quantification"], "abstract": "Laplace approximations are classic, computationally lightweight means to construct Bayesian neural networks (BNNs). As in other approximate BNNs, one cannot necessarily expect the induced predictive uncertainty to be calibrated. Here we develop a formalism to explicitly \"train\" the uncertainty in a decoupled way to the prediction itself. To this end we introduce uncertainty units for Laplace-approximated networks: Hidden units with zero weights that can be added to any pre-trained, point-estimated network. Since these units are inactive, they do not affect the predictions. But their presence changes the geometry (in particular the Hessian) of the loss landscape around the point estimate, thereby affecting the network's uncertainty estimates under a Laplace approximation. We show that such units can be trained via an uncertainty-aware objective, making the Laplace approximation competitive with more expensive alternative uncertainty-quantification frameworks.", "one-sentence_summary": "A new form of hidden units for Bayesian deep learning that only affects uncertainty, not the prediction, and which can be trained post-hoc at low cost.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kristiadi|learnable_uncertainty_under_laplace_approximations", "pdf": "/pdf/9322f1ce4a51da0b3a75c6eb90411a04be39cd11.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LgfpwqInwx", "_bibtex": "@misc{\nkristiadi2021learnable,\ntitle={Learnable Uncertainty under Laplace Approximations},\nauthor={Agustinus Kristiadi and Matthias Hein and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=pg9c6etTWXR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "pg9c6etTWXR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper590/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper590/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper590/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper590/Authors|ICLR.cc/2021/Conference/Paper590/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper590/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869321, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper590/-/Official_Comment"}}}, {"id": "NQRduqLYAd-", "original": null, "number": 8, "cdate": 1605618633588, "ddate": null, "tcdate": 1605618633588, "tmdate": 1605618633588, "tddate": null, "forum": "pg9c6etTWXR", "replyto": "bOZR-qQoK2l", "invitation": "ICLR.cc/2021/Conference/Paper590/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "We appreciate the feedback. We hope the following could clear your concerns up.\n\n\n* **Using Eq (7) and (8) to quantify posterior uncertainty doesn't seem correct?  One should focus more on the different posterior uncertainty within the real observed data?** The idea behind the objective in eqs. 7 and 8 is a widely accepted standard procedure in non-Bayesian robust learning literature [1, 2, 3, etc.].  We believe that explicitly forcing this behavior using some OOD data is required since even if standard Bayesian NNs have been shown to be uncertain far from the data, their confidence estimates in these regions can still be rather high, or even be arbitrarily bad [4]. The OOD data used in LULA\u2019s training helps to calibrate uncertainty/confidence in these regions.\n\n* **Does it mean that no matter what values the extra network weights take, their curvature to the output prediction should be entirely flat? How can it affect uncertainty?** We believe that you have misunderstood this aspect, which is central to the paper. LULA works in the parameter space of a network, not in the output space. Intuitively, by adding LULA units, we obtain an augmented version of the network\u2019s loss landscape. The goal of LULA training is then to exploit the weight-space symmetry (i.e. different parameters but induce the same output) and pick a parameter that is symmetric to the original parameter but has \u201cbetter\u201d curvatures. Here, we define a \u201cgood curvature\u201d in terms of eqs. 7 and 8. These curvatures, then, when used in a Laplace approximation, yield better uncertainty estimates. We have added a discussion about this in Sec. 3.2.\n\n\nReferences:\n1. Hendrycks, et al. \"Deep anomaly detection with outlier exposure.\" ICLR 2018.\n2. Meinke and Hein. \"Towards neural networks that provably know when they don't know.\" ICLR 2020.\n3. Lee, et al. \"Training confidence-calibrated classifiers for detecting out-of-distribution samples.\" ICLR 2018.\n4. Kristiadi, et al. \"Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks.\" ICML 2020.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper590/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper590/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learnable Uncertainty under Laplace Approximations", "authorids": ["~Agustinus_Kristiadi1", "~Matthias_Hein2", "~Philipp_Hennig1"], "authors": ["Agustinus Kristiadi", "Matthias Hein", "Philipp Hennig"], "keywords": ["Bayesian deep learning", "Laplace approximations", "uncertainty quantification"], "abstract": "Laplace approximations are classic, computationally lightweight means to construct Bayesian neural networks (BNNs). As in other approximate BNNs, one cannot necessarily expect the induced predictive uncertainty to be calibrated. Here we develop a formalism to explicitly \"train\" the uncertainty in a decoupled way to the prediction itself. To this end we introduce uncertainty units for Laplace-approximated networks: Hidden units with zero weights that can be added to any pre-trained, point-estimated network. Since these units are inactive, they do not affect the predictions. But their presence changes the geometry (in particular the Hessian) of the loss landscape around the point estimate, thereby affecting the network's uncertainty estimates under a Laplace approximation. We show that such units can be trained via an uncertainty-aware objective, making the Laplace approximation competitive with more expensive alternative uncertainty-quantification frameworks.", "one-sentence_summary": "A new form of hidden units for Bayesian deep learning that only affects uncertainty, not the prediction, and which can be trained post-hoc at low cost.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kristiadi|learnable_uncertainty_under_laplace_approximations", "pdf": "/pdf/9322f1ce4a51da0b3a75c6eb90411a04be39cd11.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LgfpwqInwx", "_bibtex": "@misc{\nkristiadi2021learnable,\ntitle={Learnable Uncertainty under Laplace Approximations},\nauthor={Agustinus Kristiadi and Matthias Hein and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=pg9c6etTWXR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "pg9c6etTWXR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper590/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper590/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper590/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper590/Authors|ICLR.cc/2021/Conference/Paper590/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper590/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869321, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper590/-/Official_Comment"}}}, {"id": "Tvds2wTQOf9", "original": null, "number": 7, "cdate": 1605618538855, "ddate": null, "tcdate": 1605618538855, "tmdate": 1605618538855, "tddate": null, "forum": "pg9c6etTWXR", "replyto": "6WjZqHZQRnR", "invitation": "ICLR.cc/2021/Conference/Paper590/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Thank you for the feedback. We would like to address your concerns and questions below.\n\n* **Backpropagation through the augmented hidden units is blocked?** Thank you for bringing this up. Indeed, for lower layers (other than the last layer) $\\widehat{W}$ has gradient zero. But this is by design: The purpose of lower layers\u2019 augmented weights are to provide a priori flexibility---if one would like to make the curvature of the augmented loss landscape behave in a certain way, it can be done by setting these lower layers\u2019 $\\widehat{W}$\u2019s. In practice, when we do not use last-layer LULA, we set these weights randomly. We found---as Prop. 2 shows---that a priori, these weights could already improve Laplace approximations\u2019 uncertainty estimates. Fine-tuning the last-layer LULA\u2019s weights using our objective improves them further. We have clarified this in Sec. 3.2 and provided an empirical evidence in Appendix C.1.\n\n* **Backpropagation through the Hessian?** Please refer to our answer to AnonReviewer1, point 1.\n\n* **Computing predictive variance?** Indeed in general the computation of the Jacobian is infeasible. However, as we have stated in Sec. 5.1., we used Monte Carlo estimation in our experiments. We only used linearized predictive distribution in our theoretical analysis Prop. 2.\n\n* **More experiments?** We have added calibration results in Tabs. 9 and 11 (the first row of each in-distribution data). Furthermore, we have added results from dataset shift experiments (CIFAR-10-C) [2], in Tab. 3. Please refer also to our response regarding this experiment to AnonReviewer1.\n\n* **Only last-layer Laplace? Difference to networks with variance-output?** While our uncertainty tuning is done in the last-layer of a network, one can apply all-layer Laplace on a LULA-augmented network. We show the results of these all-layer Laplaces in our toy examples (Fig. 1) and UCI regression experiments (Appendix C.2). Finally, LULA with last-layer Laplace (LLL) differs from networks with variance-output in that it is based on a Bayesian approximation [1], and so it quantifies _epistemic_ uncertainty while networks with variance output quantifies _aleatoric_ uncertainty.\n\n* **KFL+LL and KFL+OOD are worse than KFL?** Thank you for noticing this. Indeed this seems to be the behavior of these tuning methods in smaller networks. We have added results for larger networks (20-layer CNN and DenseNet-121) in the Tabs. 9-13 in the appendix. We observed there that KFL+LL and KFL+OOD tend to be better than the vanilla KFL. However, our conclusion stays the same: KFL+LULA achieve the best results among these tuning methods.\n\n\nReferences:\n1. Kristiadi, et al. \"Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks.\" ICML 2020.\n2.  Ovadia et al. \u201cCan You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift.\u201d In Neurips 2019.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper590/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper590/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learnable Uncertainty under Laplace Approximations", "authorids": ["~Agustinus_Kristiadi1", "~Matthias_Hein2", "~Philipp_Hennig1"], "authors": ["Agustinus Kristiadi", "Matthias Hein", "Philipp Hennig"], "keywords": ["Bayesian deep learning", "Laplace approximations", "uncertainty quantification"], "abstract": "Laplace approximations are classic, computationally lightweight means to construct Bayesian neural networks (BNNs). As in other approximate BNNs, one cannot necessarily expect the induced predictive uncertainty to be calibrated. Here we develop a formalism to explicitly \"train\" the uncertainty in a decoupled way to the prediction itself. To this end we introduce uncertainty units for Laplace-approximated networks: Hidden units with zero weights that can be added to any pre-trained, point-estimated network. Since these units are inactive, they do not affect the predictions. But their presence changes the geometry (in particular the Hessian) of the loss landscape around the point estimate, thereby affecting the network's uncertainty estimates under a Laplace approximation. We show that such units can be trained via an uncertainty-aware objective, making the Laplace approximation competitive with more expensive alternative uncertainty-quantification frameworks.", "one-sentence_summary": "A new form of hidden units for Bayesian deep learning that only affects uncertainty, not the prediction, and which can be trained post-hoc at low cost.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kristiadi|learnable_uncertainty_under_laplace_approximations", "pdf": "/pdf/9322f1ce4a51da0b3a75c6eb90411a04be39cd11.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LgfpwqInwx", "_bibtex": "@misc{\nkristiadi2021learnable,\ntitle={Learnable Uncertainty under Laplace Approximations},\nauthor={Agustinus Kristiadi and Matthias Hein and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=pg9c6etTWXR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "pg9c6etTWXR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper590/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper590/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper590/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper590/Authors|ICLR.cc/2021/Conference/Paper590/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper590/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869321, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper590/-/Official_Comment"}}}, {"id": "_dplKs3vFe5", "original": null, "number": 6, "cdate": 1605618458559, "ddate": null, "tcdate": 1605618458559, "tmdate": 1605618458559, "tddate": null, "forum": "pg9c6etTWXR", "replyto": "i12Xhg-_GJG", "invitation": "ICLR.cc/2021/Conference/Paper590/-/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "Thank you for your feedback. We will address your concerns and questions below. \n\n* **How exactly are you training the LULA weights considering that the Hessian depends on them?** In practice, we use a stochastic version of Alg. 1: For each minibatch, we compute the last-layer Hessian (we use diagonal Fisher approximation) and treat it as constant, as you have mentioned, to compute LULA\u2019s loss.\n\n* **Surely differentiating through its computation would be prohibitively expensive considering that it requires a pass over the entire training dataset?** By using the stochastic training above, a pass over the entire training set is not required.\n\n* **Would it be possible to construct an example where you can differentiate through the Hessian to see if this has much of an impact?** In our stochastic training, backpropagation through the last-layer Hessian poses no problem for any dataset. (The Fisher approximation of the Hessian only depends on the gradients over the minibatch and modern frameworks---e.g. PyTorch---support double backprop.)\n\n* **Discussion re. eq. 4? Isn't its point to avoid MC-integral?** Indeed it is arguably a better alternative to MC-integrated predictive distribution [1, 2]. However, in this paper, we mainly use it for our theoretical analysis (Prop. 2).\n\n* **Eq. 4 is feasible for the last-layer approximation? Or is it to motivate the LULA objective in eq. 7?** We use the linearization used in eq. 4 to obtain results in Figure 1 and UCI regression in Appendix C.2. But you are right that it can alternatively be used to compute each summand in the first line of eq. 8 in the case of general last-layer training. \n\n* **Are you using the multi-class equivalents of eq 4 and 5 for test-time prediction?** We use Monte Carlo estimation during test-time.\n\n* **DPN in Tabs. 1 and 2?** We show the OOD-detection comparison a la Tabs. 1 and 2 between KFL+LULA and DPN on MNIST in Tab. 7 in the appendix. We found that KFL+LULA outperformed DPN. Unfortunately, after many tries, we were unable to train DPN on larger datasets and networks: We keep getting validation accuracy of ~10% and ~1% for CIFAR-10, SVHN and CIFAR-100, respectively, throughout the training process even when using the hyperparameter values suggested in the DPN\u2019s paper.\n\n* **Underconfident results?** Considering the accuracies of the networks, KFL+LULA is not grossly underconfident, e.g. 88.8% and 44.4% MMCs on CIFAR-10 and CIFAR-100 are close to their accuracies of 90% and 50%, respectively. This indicates LULA does not destroy models\u2019 calibrations. To give empirical evidence: We have added calibration results, in terms of ECE, in Table 9 and 10 (the first line of each in-distribution dataset) in the appendix---LULA yield competitive calibration performance.\n\n* **Dataset shift experiments?** Thank you for the suggestion. We have added results on the dataset shift experiment in Table 3 (Sec. 5.1) as suggested. From these results we observed that LULA provides the best performance compared to other tuning methods for Laplace approximations. Furthermore, we found that it outperformed Deep Ensemble in both calibration error and log-likelihood.\n\n* **Would it be possible to use a more recent architecture for CIFAR100?** Sure, we have added additional results using larger networks---20-layer self-normalizing network and DenseNet-121---in Table 9-13 in the appendix. These results do not change our conclusion that LULA is effective and efficient for improving the UQ performance of these networks.\n\n* **Experiments on other domains?** The results for non-image experiments, in the form of UCI regressions, are presented in Appendix C.2. We agree that our method and last-layer Laplace in general can be applied to broader range domains. However, for the context of this paper, we believe both UCI regression and image classification have already shown the competitiveness of LULA.\n\n\nRefs.\n1. Foong, et al. \"In-Between Uncertainty in Bayesian Neural Networks.\" arXiv 2019.\n2. Immer, et al. \"Improving predictions of Bayesian neural networks via local linearization.\" arXiv 2020.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper590/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper590/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learnable Uncertainty under Laplace Approximations", "authorids": ["~Agustinus_Kristiadi1", "~Matthias_Hein2", "~Philipp_Hennig1"], "authors": ["Agustinus Kristiadi", "Matthias Hein", "Philipp Hennig"], "keywords": ["Bayesian deep learning", "Laplace approximations", "uncertainty quantification"], "abstract": "Laplace approximations are classic, computationally lightweight means to construct Bayesian neural networks (BNNs). As in other approximate BNNs, one cannot necessarily expect the induced predictive uncertainty to be calibrated. Here we develop a formalism to explicitly \"train\" the uncertainty in a decoupled way to the prediction itself. To this end we introduce uncertainty units for Laplace-approximated networks: Hidden units with zero weights that can be added to any pre-trained, point-estimated network. Since these units are inactive, they do not affect the predictions. But their presence changes the geometry (in particular the Hessian) of the loss landscape around the point estimate, thereby affecting the network's uncertainty estimates under a Laplace approximation. We show that such units can be trained via an uncertainty-aware objective, making the Laplace approximation competitive with more expensive alternative uncertainty-quantification frameworks.", "one-sentence_summary": "A new form of hidden units for Bayesian deep learning that only affects uncertainty, not the prediction, and which can be trained post-hoc at low cost.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kristiadi|learnable_uncertainty_under_laplace_approximations", "pdf": "/pdf/9322f1ce4a51da0b3a75c6eb90411a04be39cd11.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LgfpwqInwx", "_bibtex": "@misc{\nkristiadi2021learnable,\ntitle={Learnable Uncertainty under Laplace Approximations},\nauthor={Agustinus Kristiadi and Matthias Hein and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=pg9c6etTWXR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "pg9c6etTWXR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper590/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper590/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper590/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper590/Authors|ICLR.cc/2021/Conference/Paper590/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper590/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869321, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper590/-/Official_Comment"}}}, {"id": "Hcogrs6mjh3", "original": null, "number": 5, "cdate": 1605618338947, "ddate": null, "tcdate": 1605618338947, "tmdate": 1605618338947, "tddate": null, "forum": "pg9c6etTWXR", "replyto": "L4N4qpUBoAe", "invitation": "ICLR.cc/2021/Conference/Paper590/-/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "Thank you very much for your supportive review. We have made the best numbers in Table 1 bold as suggested. We would like to address your questions below.\n\n* **What are the hyperparameters that the method is sensitive for?** The number of LULA units is the primary choice affecting uncertainty quantification performance. Larger number of LULA units tends to make the network underconfident. We present the sensitivity analysis in Sec. C.3 and Table 6 in the appendix.\n\n* **How was the number of LULA units chosen?** We use grid search over the set $\\\\{ 64, 128, 256, 512 \\\\}$ to pick the number that balances the in- and out-distribution MMCs (we use 64 units for our experiments in the main text). We have added the detail in Appendix B, in particular eq. 9. We have also added smaller numbers of LULA units (16, 32) in the sensitivity analysis in Table 6. We found that around 32 and 64 units is optimal. \n\n* **Separate validation set for hyperparameter search?** We use the training set to tune LULA\u2019s uncertainty (i.e. to carry out Alg. 1). We have corrected the sentence just below eq. 8: \u201c... we can simply set $\\mathcal{D}_\\text{in}$ to be the ~~validation~~ training set ...\u201d. To pick the hyperparameter, i.e. the number of LULA units, we use the validation set (see the newly added Appendix B and also Appendix. C.3)."}, "signatures": ["ICLR.cc/2021/Conference/Paper590/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper590/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learnable Uncertainty under Laplace Approximations", "authorids": ["~Agustinus_Kristiadi1", "~Matthias_Hein2", "~Philipp_Hennig1"], "authors": ["Agustinus Kristiadi", "Matthias Hein", "Philipp Hennig"], "keywords": ["Bayesian deep learning", "Laplace approximations", "uncertainty quantification"], "abstract": "Laplace approximations are classic, computationally lightweight means to construct Bayesian neural networks (BNNs). As in other approximate BNNs, one cannot necessarily expect the induced predictive uncertainty to be calibrated. Here we develop a formalism to explicitly \"train\" the uncertainty in a decoupled way to the prediction itself. To this end we introduce uncertainty units for Laplace-approximated networks: Hidden units with zero weights that can be added to any pre-trained, point-estimated network. Since these units are inactive, they do not affect the predictions. But their presence changes the geometry (in particular the Hessian) of the loss landscape around the point estimate, thereby affecting the network's uncertainty estimates under a Laplace approximation. We show that such units can be trained via an uncertainty-aware objective, making the Laplace approximation competitive with more expensive alternative uncertainty-quantification frameworks.", "one-sentence_summary": "A new form of hidden units for Bayesian deep learning that only affects uncertainty, not the prediction, and which can be trained post-hoc at low cost.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kristiadi|learnable_uncertainty_under_laplace_approximations", "pdf": "/pdf/9322f1ce4a51da0b3a75c6eb90411a04be39cd11.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=LgfpwqInwx", "_bibtex": "@misc{\nkristiadi2021learnable,\ntitle={Learnable Uncertainty under Laplace Approximations},\nauthor={Agustinus Kristiadi and Matthias Hein and Philipp Hennig},\nyear={2021},\nurl={https://openreview.net/forum?id=pg9c6etTWXR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "pg9c6etTWXR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper590/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper590/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper590/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper590/Authors|ICLR.cc/2021/Conference/Paper590/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper590/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869321, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper590/-/Official_Comment"}}}], "count": 14}