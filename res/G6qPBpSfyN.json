{"notes": [{"id": "G6qPBpSfyN", "original": "7KN84d8nd", "number": 35, "cdate": 1582750163245, "ddate": null, "tcdate": 1582750163245, "tmdate": 1587925113846, "tddate": null, "forum": "G6qPBpSfyN", "replyto": null, "invitation": "ICLR.cc/2020/Workshop/DeepDiffEq/-/Blind_Submission", "content": {"keywords": ["optimization", "gradient flow", "port-Hamiltonian"], "authors": ["Michael Poli", "Stefano Massaroli", "Atsushi Yamashita", "Hajime Asama", "Jinkyoo Park"], "title": "Port-Hamiltonian Gradient Flows", "pdf": "/pdf/86287a7e14af1f2dc42c064141fb02c2fdfe9a1e.pdf", "TL;DR": "We introduce an optimization framework that leverages the port-Hamiltonian dynamical system formalism.", "abstract": "In this paper we present a general framework for continuous--time gradient descent, often referred to as gradient flow. We extend Hamiltonian gradient flows, which ascribe mechanical dynamics to neural network parameters and constitute a natural continuous-time alternative to discrete momentum-based gradient descent approaches. The proposed Port-Hamiltonian Gradient Flow (PHGF)  casts neural network training into a system--theoretic framework: a fictitious physical system is coupled to the neural network by setting the loss function as an energy term of the system. As autonomous port--Hamiltonian systems naturally tend to dissipate energy towards one of its minima by construction, solving the system simultaneously trains the neural network. We show that general PHGFs are compatible with both continuous-time data--stream optimization, where the optimizer processes a continuous stream of data, as well as standard fixed-step optimization. In continuous-time, PHGFs allow for the embedding of black--box adaptive--step ODE solvers and are able to stick to the energy manifold, thus avoiding divergence due to large learning rates. In fixed-step optimization, on the other hand, PGHFs open the door to novel fixed-step approaches based on symplectic discretizations of the Port--Hamiltonian with similar memory footprint and computational complexity as momentum optimizers.", "authorids": ["poli_m@kaist.ac.kr", "massaroli@robot.t.u-tokyo.ac.jp", "yamashita@robot.t.u-tokyo.ac.jp", "asama@robot.t.u-tokyo.ac.jp", "jinkyoo.park@kaist.ac.kr"], "paperhash": "poli|porthamiltonian_gradient_flows", "_bibtex": "@inproceedings{\npoli2020porthamiltonian,\ntitle={Port-Hamiltonian Gradient Flows},\nauthor={Michael Poli and Stefano Massaroli and Atsushi Yamashita and Hajime Asama and Jinkyoo Park},\nbooktitle={ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations},\nyear={2020},\nurl={https://openreview.net/forum?id=G6qPBpSfyN}\n}"}, "signatures": ["ICLR.cc/2020/Workshop/DeepDiffEq"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Workshop/DeepDiffEq"], "details": {"replyCount": 1, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Workshop/DeepDiffEq"]}, "signatures": {"values": ["ICLR.cc/2020/Workshop/DeepDiffEq"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "signatures": ["ICLR.cc/2020/Workshop/DeepDiffEq"], "readers": ["everyone"], "writers": ["ICLR.cc/2020/Workshop/DeepDiffEq"], "invitees": ["~"], "tcdate": 1582750147213, "tmdate": 1587924718420, "id": "ICLR.cc/2020/Workshop/DeepDiffEq/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "6vsTAxWmVF", "original": null, "number": 1, "cdate": 1582774530267, "ddate": null, "tcdate": 1582774530267, "tmdate": 1582774530267, "tddate": null, "forum": "G6qPBpSfyN", "replyto": "G6qPBpSfyN", "invitation": "ICLR.cc/2020/Workshop/DeepDiffEq/Paper35/-/Decision", "content": {"decision": "Accept (Poster)", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Workshop/DeepDiffEq/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Workshop/DeepDiffEq/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"keywords": ["optimization", "gradient flow", "port-Hamiltonian"], "authors": ["Michael Poli", "Stefano Massaroli", "Atsushi Yamashita", "Hajime Asama", "Jinkyoo Park"], "title": "Port-Hamiltonian Gradient Flows", "pdf": "/pdf/86287a7e14af1f2dc42c064141fb02c2fdfe9a1e.pdf", "TL;DR": "We introduce an optimization framework that leverages the port-Hamiltonian dynamical system formalism.", "abstract": "In this paper we present a general framework for continuous--time gradient descent, often referred to as gradient flow. We extend Hamiltonian gradient flows, which ascribe mechanical dynamics to neural network parameters and constitute a natural continuous-time alternative to discrete momentum-based gradient descent approaches. The proposed Port-Hamiltonian Gradient Flow (PHGF)  casts neural network training into a system--theoretic framework: a fictitious physical system is coupled to the neural network by setting the loss function as an energy term of the system. As autonomous port--Hamiltonian systems naturally tend to dissipate energy towards one of its minima by construction, solving the system simultaneously trains the neural network. We show that general PHGFs are compatible with both continuous-time data--stream optimization, where the optimizer processes a continuous stream of data, as well as standard fixed-step optimization. In continuous-time, PHGFs allow for the embedding of black--box adaptive--step ODE solvers and are able to stick to the energy manifold, thus avoiding divergence due to large learning rates. In fixed-step optimization, on the other hand, PGHFs open the door to novel fixed-step approaches based on symplectic discretizations of the Port--Hamiltonian with similar memory footprint and computational complexity as momentum optimizers.", "authorids": ["poli_m@kaist.ac.kr", "massaroli@robot.t.u-tokyo.ac.jp", "yamashita@robot.t.u-tokyo.ac.jp", "asama@robot.t.u-tokyo.ac.jp", "jinkyoo.park@kaist.ac.kr"], "paperhash": "poli|porthamiltonian_gradient_flows", "_bibtex": "@inproceedings{\npoli2020porthamiltonian,\ntitle={Port-Hamiltonian Gradient Flows},\nauthor={Michael Poli and Stefano Massaroli and Atsushi Yamashita and Hajime Asama and Jinkyoo Park},\nbooktitle={ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations},\nyear={2020},\nurl={https://openreview.net/forum?id=G6qPBpSfyN}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"values": ["ICLR.cc/2020/Workshop/DeepDiffEq/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2020/Workshop/DeepDiffEq/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Paper Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject"], "description": "Decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}, "forum": "G6qPBpSfyN", "replyto": "G6qPBpSfyN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}}, "cdate": 1582156800000, "expdate": 1589155200000, "duedate": 1588291200000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Workshop/DeepDiffEq/Program_Chairs"], "tcdate": 1582771073879, "tmdate": 1587925024536, "super": "ICLR.cc/2020/Workshop/DeepDiffEq/-/Decision", "signatures": ["ICLR.cc/2020/Workshop/DeepDiffEq"], "writers": ["ICLR.cc/2020/Workshop/DeepDiffEq"], "id": "ICLR.cc/2020/Workshop/DeepDiffEq/Paper35/-/Decision"}}}], "count": 2}