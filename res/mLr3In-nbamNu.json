{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1364253000000, "tcdate": 1364253000000, "number": 1, "id": "c2pVc0PtwzcEK", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "mLr3In-nbamNu", "replyto": "mLr3In-nbamNu", "signatures": ["Nicolas Le Roux, Francis Bach"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "First, we would like to thank the reviewers for their comments.\r\n\r\nThe main complaint was that the experiments were limited to toy problems. Since it is always hard to evaluate unsupervised learning algorithms (what is the metric of performance), the experiments were designed as a proof of concept. Hence, we agree with the reviewers and would love to see LCA tried and evaluated on real problems.\r\n\r\nFor the comment about the required modifications to avoid overfitting, there is truly only one parameter to set, i.e., the lambda parameter. All the others can easily be set to default values."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Local Component Analysis", "decision": "conferencePoster-iclr2013-conference", "abstract": "Kernel density estimation, a.k.a. Parzen windows, is a popular density estimation method, which can be used for outlier detection or clustering. With multivariate data, its performance is heavily reliant on the metric used within the kernel. Most earlier work has focused on learning only the bandwidth of the kernel (i.e., a scalar multiplicative factor). In this paper, we propose to learn a full Euclidean metric through an expectation-minimization (EM) procedure, which can be seen as an unsupervised counterpart to neighbourhood component analysis (NCA). In order to avoid overfitting with a fully nonparametric density estimator in high dimensions, we also consider a semi-parametric Gaussian-Parzen density model, where some of the variables are modelled through a jointly Gaussian density, while others are modelled through Parzen windows. For these two models, EM leads to simple closed-form updates based on matrix inversions and eigenvalue decompositions. We show empirically that our method leads to density estimators with higher test-likelihoods than natural competing methods, and that the metrics may be used within most unsupervised learning techniques that rely on such metrics, such as spectral clustering or manifold learning methods. Finally, we present a stochastic approximation scheme which allows for the use of this method in a large-scale setting.", "pdf": "https://arxiv.org/abs/1109.0093", "paperhash": "roux|local_component_analysis", "keywords": [], "conflicts": [], "authors": ["Nicolas Le Roux", "Francis Bach"], "authorids": ["nicolas.le.roux@gmail.com", "francis.bach@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362491220000, "tcdate": 1362491220000, "number": 4, "id": "pRFvp6BDvn46c", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "mLr3In-nbamNu", "replyto": "mLr3In-nbamNu", "signatures": ["anonymous reviewer 61c0"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Local Component Analysis", "review": "Summary of contributions:\r\nThe paper presents a robust algorithm for density estimation. The main idea is to model the density into a product of two independent distributions: one from a Parzen windows estimation (for modeling a low dimensional manifold) and the other from a Gaussian distribution (for modeling noise). Specifically, leave-one-out log-likelihood is used as the objective function of Parzen window estimator, and the joint model can be optimized using Expectation Maximization algorithm. In addition, the paper presents an analytical solution for M-step using eigen-decomposition. The authors also propose several heuristics to address local optima problems and to improve computational efficiency. The experimental results on synthetic data show that the proposed algorithm is indeed robust to noise.\r\n\r\n\r\nAssessment on novelty and quality:\r\n\r\nNovelty: \r\nThis paper seems to be novel. The main ideas (using leave-one-out log-likelihood and decomposing the density as a product of Parzen windows estimator and a Gaussian distribution) are very interesting.\r\n\r\nQuality: \r\nThe paper is clearly written. The method is well motivated, and the technical solutions are quite elegant and clearly described. The paper also presents important practical tips on addressing local optima problems and speeding up the algorithm. \r\n\r\nIn experiments, the proposed algorithm works well when noise dimensions increase in the data. The experiments are reasonably convincing, but they are limited to very low-dimensional, toy data. Evaluation on more real-world datasets would have been much more compelling. Without such evaluation, it\u2019s unclear how the proposed method will perform on real data.\r\n\r\nAlthough interesting, the assumption about modeling the data density as a product of two independent distributions can be too strong and unrealistic. For example, how can this model handle the cases when noise are added to the low-dimensional manifold, not as orthogonal \u201cnoise dimension\u201d?\r\n\r\n\r\nOther comments:\r\n- Figure 1 is not very interesting since even NCA will learn near-isotropic covariance, and the baseline method seems to be PCA whitening, not PCA.\r\n\r\n\r\nPros and cons:\r\npros:\r\n- The paper seems sufficiently novel. \r\n- The main approach and solution are technically interesting.\r\n- The experiments show proof-of-concept (albeit limited) demonstration that the proposed method is robust to noise dimensions (or irrelevant features).\r\n\r\ncons:\r\n- The experiments are limited to very low-dimensional, toy datasets. Evaluation on more real-world datasets would have been much more compelling. Without such evaluation, it\u2019s unclear how the proposed method will perform on real data.\r\n- The assumption about modeling the data density as a product of two independent distributions can be too strong and unrealistic (see comments above)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Local Component Analysis", "decision": "conferencePoster-iclr2013-conference", "abstract": "Kernel density estimation, a.k.a. Parzen windows, is a popular density estimation method, which can be used for outlier detection or clustering. With multivariate data, its performance is heavily reliant on the metric used within the kernel. Most earlier work has focused on learning only the bandwidth of the kernel (i.e., a scalar multiplicative factor). In this paper, we propose to learn a full Euclidean metric through an expectation-minimization (EM) procedure, which can be seen as an unsupervised counterpart to neighbourhood component analysis (NCA). In order to avoid overfitting with a fully nonparametric density estimator in high dimensions, we also consider a semi-parametric Gaussian-Parzen density model, where some of the variables are modelled through a jointly Gaussian density, while others are modelled through Parzen windows. For these two models, EM leads to simple closed-form updates based on matrix inversions and eigenvalue decompositions. We show empirically that our method leads to density estimators with higher test-likelihoods than natural competing methods, and that the metrics may be used within most unsupervised learning techniques that rely on such metrics, such as spectral clustering or manifold learning methods. Finally, we present a stochastic approximation scheme which allows for the use of this method in a large-scale setting.", "pdf": "https://arxiv.org/abs/1109.0093", "paperhash": "roux|local_component_analysis", "keywords": [], "conflicts": [], "authors": ["Nicolas Le Roux", "Francis Bach"], "authorids": ["nicolas.le.roux@gmail.com", "francis.bach@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362428640000, "tcdate": 1362428640000, "number": 3, "id": "iGfW_jMjFAoZQ", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "mLr3In-nbamNu", "replyto": "mLr3In-nbamNu", "signatures": ["anonymous reviewer 18ca"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Local Component Analysis", "review": "Summary of contributions:\r\n1. The paper proposed an unsupervised local component analysis (LCA) framework that estimates the Parzen window covariance via maximizing the leave-one-out density. The basic algorithm is an EM procedure with closed form updates. \r\n\r\n2. One further extension of LCA was introduced, which assumes two multiplicative densities, one is Parzen window (non Gaussian) and the other is a global Gaussian distribution. \r\n\r\n3. Algorithms was designed to scale up the algorithms to large data sets. \r\n\r\nAssessment of novelty and quality:\r\nThe work looks quite reasonable. But the approach seems to be a bit straightforward.  The work is perhaps not very deep or inspiring.  \r\n\r\nMy major concern is, other than the described problem setting being tackled, mostly toy problems, I don't see the significance of the work for addressing major machine learning challenges. For example, the authors argued the approach might be a good preprocessing step, but in the experiments, there is nothing like improving machine learning (e.g. classification) via such a pre-processing of data. \r\n\r\nIt's disappointing to see that the authors didn't study the identifiability of the Parzen/Gaussian model. Addressing this issue should have been a  good chance to show some depth of the research."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Local Component Analysis", "decision": "conferencePoster-iclr2013-conference", "abstract": "Kernel density estimation, a.k.a. Parzen windows, is a popular density estimation method, which can be used for outlier detection or clustering. With multivariate data, its performance is heavily reliant on the metric used within the kernel. Most earlier work has focused on learning only the bandwidth of the kernel (i.e., a scalar multiplicative factor). In this paper, we propose to learn a full Euclidean metric through an expectation-minimization (EM) procedure, which can be seen as an unsupervised counterpart to neighbourhood component analysis (NCA). In order to avoid overfitting with a fully nonparametric density estimator in high dimensions, we also consider a semi-parametric Gaussian-Parzen density model, where some of the variables are modelled through a jointly Gaussian density, while others are modelled through Parzen windows. For these two models, EM leads to simple closed-form updates based on matrix inversions and eigenvalue decompositions. We show empirically that our method leads to density estimators with higher test-likelihoods than natural competing methods, and that the metrics may be used within most unsupervised learning techniques that rely on such metrics, such as spectral clustering or manifold learning methods. Finally, we present a stochastic approximation scheme which allows for the use of this method in a large-scale setting.", "pdf": "https://arxiv.org/abs/1109.0093", "paperhash": "roux|local_component_analysis", "keywords": [], "conflicts": [], "authors": ["Nicolas Le Roux", "Francis Bach"], "authorids": ["nicolas.le.roux@gmail.com", "francis.bach@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1361300640000, "tcdate": 1361300640000, "number": 2, "id": "D1cO7TgVjPGT9", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "mLr3In-nbamNu", "replyto": "mLr3In-nbamNu", "signatures": ["anonymous reviewer 71f4"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Local Component Analysis", "review": "In this paper, the authors consider unsupervised metric learning as a\r\ndensity estimation problem with a Parzen windows estimator based on \r\nEuclidean metric. They use maximum likelihood method and EM algorithm\r\nfor deriving a method that may be considered as an unsupervised counterpart to neighbourhood component analysis. Various versions of the method provide good results in the clustering problems considered.\r\n\r\n+ Good and interesting conference paper.\r\n+ Certainly novel enough.\r\n- Modifications are needed to combat the problems of overfitting,\r\nlocal minima, and computational load in the basic approach proposed.\r\nSome of these improvements are heuristic or seem to require hand-tuning.\r\n\r\n\r\nSpecific comments:\r\n\r\n- The authors should refer to the paper S. Kaski and J. Peltonen,\r\n'Informative discriminant analysis', in T. Fawcett and N. Mishna (Eds.),\r\nProc. of the 20th Int. Conf. on Machine Learning (ICML 2003), pp. 329-336,\r\nAAAI Press, Menlo Park, CA, 2003.\r\nIn this paper, essentially the same technique as Neighbourhood Component\r\nAnalysis is defined under the name Informative discriminant analysis\r\none year prior to the paper by Goldberger et al., your reference [16].\r\n\r\n- In the beginning of page 6 the authors state: 'Following [1, 2], the data\r\nis progressively corrupted by adding dimensions of white Gaussian noise,\r\nthen whitened.' In this case, whitening amplifies Gaussian noise, so that\r\nit has the same power as the underlying data. Obviously this is the reason\r\nwhy the experimental results approach to a random guess when the dimensions of the white noise increase sufficiently. The authors should mention that in real-world applications, one should not use whitening in this kind of situations,  but rather compress the data using for example principal component analysis (PCA) without whitening for getting rid of the extra dimensions corresponding to white Gaussian noise. Or at least use the data as such without any whitening."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Local Component Analysis", "decision": "conferencePoster-iclr2013-conference", "abstract": "Kernel density estimation, a.k.a. Parzen windows, is a popular density estimation method, which can be used for outlier detection or clustering. With multivariate data, its performance is heavily reliant on the metric used within the kernel. Most earlier work has focused on learning only the bandwidth of the kernel (i.e., a scalar multiplicative factor). In this paper, we propose to learn a full Euclidean metric through an expectation-minimization (EM) procedure, which can be seen as an unsupervised counterpart to neighbourhood component analysis (NCA). In order to avoid overfitting with a fully nonparametric density estimator in high dimensions, we also consider a semi-parametric Gaussian-Parzen density model, where some of the variables are modelled through a jointly Gaussian density, while others are modelled through Parzen windows. For these two models, EM leads to simple closed-form updates based on matrix inversions and eigenvalue decompositions. We show empirically that our method leads to density estimators with higher test-likelihoods than natural competing methods, and that the metrics may be used within most unsupervised learning techniques that rely on such metrics, such as spectral clustering or manifold learning methods. Finally, we present a stochastic approximation scheme which allows for the use of this method in a large-scale setting.", "pdf": "https://arxiv.org/abs/1109.0093", "paperhash": "roux|local_component_analysis", "keywords": [], "conflicts": [], "authors": ["Nicolas Le Roux", "Francis Bach"], "authorids": ["nicolas.le.roux@gmail.com", "francis.bach@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1357918200000, "tcdate": 1357918200000, "number": 64, "id": "mLr3In-nbamNu", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "mLr3In-nbamNu", "signatures": ["nicolas.le.roux@gmail.com"], "readers": ["everyone"], "content": {"title": "Local Component Analysis", "decision": "conferencePoster-iclr2013-conference", "abstract": "Kernel density estimation, a.k.a. Parzen windows, is a popular density estimation method, which can be used for outlier detection or clustering. With multivariate data, its performance is heavily reliant on the metric used within the kernel. Most earlier work has focused on learning only the bandwidth of the kernel (i.e., a scalar multiplicative factor). In this paper, we propose to learn a full Euclidean metric through an expectation-minimization (EM) procedure, which can be seen as an unsupervised counterpart to neighbourhood component analysis (NCA). In order to avoid overfitting with a fully nonparametric density estimator in high dimensions, we also consider a semi-parametric Gaussian-Parzen density model, where some of the variables are modelled through a jointly Gaussian density, while others are modelled through Parzen windows. For these two models, EM leads to simple closed-form updates based on matrix inversions and eigenvalue decompositions. We show empirically that our method leads to density estimators with higher test-likelihoods than natural competing methods, and that the metrics may be used within most unsupervised learning techniques that rely on such metrics, such as spectral clustering or manifold learning methods. Finally, we present a stochastic approximation scheme which allows for the use of this method in a large-scale setting.", "pdf": "https://arxiv.org/abs/1109.0093", "paperhash": "roux|local_component_analysis", "keywords": [], "conflicts": [], "authors": ["Nicolas Le Roux", "Francis Bach"], "authorids": ["nicolas.le.roux@gmail.com", "francis.bach@gmail.com"]}, "writers": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 5}