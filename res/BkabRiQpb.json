{"notes": [{"tddate": null, "ddate": null, "tmdate": 1519669692075, "tcdate": 1508257284829, "number": 10, "cdate": 1518730191884, "id": "BkabRiQpb", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "BkabRiQpb", "original": "H1n-RiQpW", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Consequentialist conditional cooperation in social dilemmas with imperfect information", "abstract": "Social dilemmas, where mutual cooperation can lead to high payoffs but participants face incentives to cheat, are ubiquitous in multi-agent interaction. We wish to construct agents that cooperate with pure cooperators, avoid exploitation by pure defectors, and incentivize cooperation from the rest. However, often the actions taken by a partner are (partially) unobserved or the consequences of individual actions are hard to predict. We show that in a large class of games good strategies can be constructed by conditioning one's behavior solely on outcomes (ie. one's past rewards). We call this consequentialist conditional cooperation. We show how to construct such strategies using deep reinforcement learning techniques and demonstrate, both analytically and experimentally, that they are effective in social dilemmas beyond simple matrix games. We also show the limitations of relying purely on consequences and discuss the need for understanding both the consequences of and the intentions behind an action.", "pdf": "/pdf/251416720d46c82d6db75a812110371d037ba986.pdf", "TL;DR": "We show how to use deep RL to construct agents that can solve social dilemmas beyond matrix games.", "paperhash": "peysakhovich|consequentialist_conditional_cooperation_in_social_dilemmas_with_imperfect_information", "_bibtex": "@inproceedings{\npeysakhovich2018consequentialist,\ntitle={Consequentialist conditional cooperation in social dilemmas with imperfect information},\nauthor={Alexander Peysakhovich and Adam Lerer},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BkabRiQpb},\n}", "keywords": ["deep reinforcement learning", "cooperation", "social dilemma", "multi-agent systems"], "authors": ["Alexander Peysakhovich", "Adam Lerer"], "authorids": ["alexpeys@gmail.com", "alerer@fb.com"]}, "nonreaders": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260095121, "tcdate": 1517249465267, "number": 231, "cdate": 1517249465256, "id": "HkW67J6BG", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "BkabRiQpb", "replyto": "BkabRiQpb", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "The reviewer reactions to the initial manuscript were generally positive.  They considered the paper to be well written and clear, providing an original contribution to learning to cooperate in multi-agent deep RL in imperfect domains.  The reviewers raised a number of specific issues to address, including improved definitions and descriptions, and proper citations of related work.  The authors have substantially  revised the manuscript to address most or all of these issues.  At this point, the only knock on this paper is that the findings seemed unsurprising from a game-theoretic or deep learning point of view.\n\nPros: algorithmic contribution, technical quality, clarity\nCons: no real surprises ", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Consequentialist conditional cooperation in social dilemmas with imperfect information", "abstract": "Social dilemmas, where mutual cooperation can lead to high payoffs but participants face incentives to cheat, are ubiquitous in multi-agent interaction. We wish to construct agents that cooperate with pure cooperators, avoid exploitation by pure defectors, and incentivize cooperation from the rest. However, often the actions taken by a partner are (partially) unobserved or the consequences of individual actions are hard to predict. We show that in a large class of games good strategies can be constructed by conditioning one's behavior solely on outcomes (ie. one's past rewards). We call this consequentialist conditional cooperation. We show how to construct such strategies using deep reinforcement learning techniques and demonstrate, both analytically and experimentally, that they are effective in social dilemmas beyond simple matrix games. We also show the limitations of relying purely on consequences and discuss the need for understanding both the consequences of and the intentions behind an action.", "pdf": "/pdf/251416720d46c82d6db75a812110371d037ba986.pdf", "TL;DR": "We show how to use deep RL to construct agents that can solve social dilemmas beyond matrix games.", "paperhash": "peysakhovich|consequentialist_conditional_cooperation_in_social_dilemmas_with_imperfect_information", "_bibtex": "@inproceedings{\npeysakhovich2018consequentialist,\ntitle={Consequentialist conditional cooperation in social dilemmas with imperfect information},\nauthor={Alexander Peysakhovich and Adam Lerer},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BkabRiQpb},\n}", "keywords": ["deep reinforcement learning", "cooperation", "social dilemma", "multi-agent systems"], "authors": ["Alexander Peysakhovich", "Adam Lerer"], "authorids": ["alexpeys@gmail.com", "alerer@fb.com"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "tmdate": 1515783141900, "tcdate": 1515783141900, "number": 4, "cdate": 1515783141900, "id": "r1CyEt8Nf", "invitation": "ICLR.cc/2018/Conference/-/Paper10/Official_Comment", "forum": "BkabRiQpb", "replyto": "rJYL9Uhzf", "signatures": ["ICLR.cc/2018/Conference/Paper10/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper10/AnonReviewer2"], "content": {"title": "Thank you for the response", "comment": "Thanks for clarifying some of the mentioned issues. With the introduced revision, you cover your ground very well. I believe that this paper offers a great basis for interesting further studies in this direction."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Consequentialist conditional cooperation in social dilemmas with imperfect information", "abstract": "Social dilemmas, where mutual cooperation can lead to high payoffs but participants face incentives to cheat, are ubiquitous in multi-agent interaction. We wish to construct agents that cooperate with pure cooperators, avoid exploitation by pure defectors, and incentivize cooperation from the rest. However, often the actions taken by a partner are (partially) unobserved or the consequences of individual actions are hard to predict. We show that in a large class of games good strategies can be constructed by conditioning one's behavior solely on outcomes (ie. one's past rewards). We call this consequentialist conditional cooperation. We show how to construct such strategies using deep reinforcement learning techniques and demonstrate, both analytically and experimentally, that they are effective in social dilemmas beyond simple matrix games. We also show the limitations of relying purely on consequences and discuss the need for understanding both the consequences of and the intentions behind an action.", "pdf": "/pdf/251416720d46c82d6db75a812110371d037ba986.pdf", "TL;DR": "We show how to use deep RL to construct agents that can solve social dilemmas beyond matrix games.", "paperhash": "peysakhovich|consequentialist_conditional_cooperation_in_social_dilemmas_with_imperfect_information", "_bibtex": "@inproceedings{\npeysakhovich2018consequentialist,\ntitle={Consequentialist conditional cooperation in social dilemmas with imperfect information},\nauthor={Alexander Peysakhovich and Adam Lerer},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BkabRiQpb},\n}", "keywords": ["deep reinforcement learning", "cooperation", "social dilemma", "multi-agent systems"], "authors": ["Alexander Peysakhovich", "Adam Lerer"], "authorids": ["alexpeys@gmail.com", "alerer@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825740751, "id": "ICLR.cc/2018/Conference/-/Paper10/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "BkabRiQpb", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper10/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper10/Authors|ICLR.cc/2018/Conference/Paper10/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper10/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper10/Authors|ICLR.cc/2018/Conference/Paper10/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper10/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper10/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper10/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper10/Reviewers", "ICLR.cc/2018/Conference/Paper10/Authors", "ICLR.cc/2018/Conference/Paper10/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825740751}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642375635, "tcdate": 1511790336014, "number": 1, "cdate": 1511790336014, "id": "r1uWD5txM", "invitation": "ICLR.cc/2018/Conference/-/Paper10/Official_Review", "forum": "BkabRiQpb", "replyto": "BkabRiQpb", "signatures": ["ICLR.cc/2018/Conference/Paper10/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Review on 'Consequentialist conditional cooperation in social dilemmas with imperfect information'", "rating": "7: Good paper, accept", "review": "This paper proposes a novel adaptive learning mechanism to improve results in ergodic cooperation games. The algorithm, tagged 'Consequentialist Conditional Cooperation', uses outcome-based accumulative rewards of different strategies established during prior training. Its core benefit is its adaptiveness towards diverse opposing player strategies (e.g. selfish, prosocial, CCC) while maintaining maximum reward.\n\nWhile the contribution is explored in all its technical complexity, fundamentally this algorithm exploits policies for selfish and prosocial strategies to determine expected rewards in a training phase. During operation it then switches its strategy depending on a dynamically-calculated threshold reward value (considering variation in agent-specific policies, initial game states and stochasticity of rewards) relative to the total reward of the played game instance. The work is contrasted to tit-for-tat approaches that require complete observability and operate based on expected future rewards. In addition to the observability, approximate Markov TFT (amTFT) methods are more processing-intense, since they fall back on a game's Q-function, as opposed to learned policies, making CCC a lightweight alternative. \n\nComments:\n\nThe findings suggest the effectiveness of that approach. In all experiments CCC-based agents fare better than agents operating based on a specific strategy. While performing worse than the amTFT approach and only working well for larger number of iterations, the outcome-based evaluation shows benefits. Specifically in the PPD game, the use of CCC produces interesting results; when paired with cooperate agents in the PPD game, CCC-based players produce higher overall reward than pairing cooperative players (see Figure 2, (d) & (e)). This should be explained. To improve the understanding of the CCC-based operation, it would further be worthwhile to provide an additional graph that shows the action choices of CCC agents over time to clarify behavioural characteristics and convergence performance.\n\nHowever, when paired with non-cooperative players in the risky PPD game, CCC players lead to an improvement of pay-offs by around 50 percent (see Figure 2, (e)), compared to payoff received between non-cooperative players (-28.4 vs. -18, relative to -5 for defection). This leads to the question: How much CCC perform compared to random policy selection? Given its reduction of processing-intensive and need for larger number of iterations, how much worse is the random choice (no processing, independent of iterations)? This is would be worthwhile to appreciate the benefit of the proposed approach.\n\nAnother point relates to the fishing game. The game is parameterized with the rewards of +1 and +3. What is the bases for these parameter choices? What would happen if the higher reward was +2, or more interestingly, if the game was extended to allow agents to fish medium-sized fish (+2), in addition to small and large fish. Here it would be interesting to see how CCC fares (in all combinations with cooperators and defectors).\n\nOverall, the paper is well-written and explores the technical details of the presented approach. The authors position the approach well within contemporary literature, both conceptually and using experimental evaluation, and are explicit about its strengths and limitations.\n\nPresentation aspects:\n- Minor typo: Page 2, last paragraph of Introduction: `... will act act identically.'\n- Figure 2 should be shifted to the next page, since it is not self-explanatory and requires more context.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Consequentialist conditional cooperation in social dilemmas with imperfect information", "abstract": "Social dilemmas, where mutual cooperation can lead to high payoffs but participants face incentives to cheat, are ubiquitous in multi-agent interaction. We wish to construct agents that cooperate with pure cooperators, avoid exploitation by pure defectors, and incentivize cooperation from the rest. However, often the actions taken by a partner are (partially) unobserved or the consequences of individual actions are hard to predict. We show that in a large class of games good strategies can be constructed by conditioning one's behavior solely on outcomes (ie. one's past rewards). We call this consequentialist conditional cooperation. We show how to construct such strategies using deep reinforcement learning techniques and demonstrate, both analytically and experimentally, that they are effective in social dilemmas beyond simple matrix games. We also show the limitations of relying purely on consequences and discuss the need for understanding both the consequences of and the intentions behind an action.", "pdf": "/pdf/251416720d46c82d6db75a812110371d037ba986.pdf", "TL;DR": "We show how to use deep RL to construct agents that can solve social dilemmas beyond matrix games.", "paperhash": "peysakhovich|consequentialist_conditional_cooperation_in_social_dilemmas_with_imperfect_information", "_bibtex": "@inproceedings{\npeysakhovich2018consequentialist,\ntitle={Consequentialist conditional cooperation in social dilemmas with imperfect information},\nauthor={Alexander Peysakhovich and Adam Lerer},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BkabRiQpb},\n}", "keywords": ["deep reinforcement learning", "cooperation", "social dilemma", "multi-agent systems"], "authors": ["Alexander Peysakhovich", "Adam Lerer"], "authorids": ["alexpeys@gmail.com", "alerer@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642375488, "id": "ICLR.cc/2018/Conference/-/Paper10/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper10/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper10/AnonReviewer2", "ICLR.cc/2018/Conference/Paper10/AnonReviewer1", "ICLR.cc/2018/Conference/Paper10/AnonReviewer3"], "reply": {"forum": "BkabRiQpb", "replyto": "BkabRiQpb", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper10/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642375488}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642375579, "tcdate": 1511818103261, "number": 2, "cdate": 1511818103261, "id": "HkCdXWqlM", "invitation": "ICLR.cc/2018/Conference/-/Paper10/Official_Review", "forum": "BkabRiQpb", "replyto": "BkabRiQpb", "signatures": ["ICLR.cc/2018/Conference/Paper10/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Learning to cooperate with incomplete information", "rating": "5: Marginally below acceptance threshold", "review": "This paper studies learning to play two-player general-sum games with state (Markov games) with imperfect information. The idea is to learn to cooperate (think prisoner's dilemma) but in more complex domains. Generally, in repeated prisoner's dilemma, one can punish one's opponent for noncooperation. In this paper, they design an apporach to learn to cooperate in a more complex game, like a hybrid pong meets prisoner's dilemma game. This is fun but I did not find it particularly surprising from a game-theoretic or from a deep learning point of view. \n\nFrom a game-theoretic point of view, the paper begins with a game-theoretic analysis of a cooperative strategy for these markov games with imperfect information. It is basically a straightforward generalization of the idea of punishing, which is common in \"folk theorems\" from game theory, to give a particular equilibrium for cooperating in Markov games. Many Markov games do not have a cooperative equilibrium, so this paper restricts attention to those that do. Even in games where there is a cooperative solution that maximizes the total welfare, it is not clear why players would choose to do so. When the game is symmetric, this might be \"the natural\" solution but in general it is far from clear why all players would want to maximize the total payoff. \n\nThe paper follows with some fun experiments implementing these new game theory notions. Unfortunately, since the game theory was not particularly well-motivated, I did not find the overall story compelling. It is perhaps interesting that one can make deep learning learn to cooperate with imperfect information, but one could have illustrated the game theory equally well with other techniques.\n\nIn contrast, the paper \"Coco-Q: Learning in Stochastic Games with Side Payments\" by Sodomka et. al. is an example where they took a well-motivated game theoretic cooperative solution concept and explored how to implement that with reinforcement learning. I would think that generalizing such solution concepts to stochastic games and/or deep learning might be more interesting.\n\nIt should also be noted that I was asked to review another ICLR submission entitled \"MAINTAINING COOPERATION IN COMPLEX SOCIAL DILEMMAS USING DEEP REINFORCEMENT LEARNING\" which amazingly introduced the same \"Pong Player\u2019s Dilemma\" game as in this paper. \n\nNotice the following suspiciously similar paragraphs from the two papers:\n\nFrom \"MAINTAINING COOPERATION IN COMPLEX SOCIAL DILEMMAS USING DEEP REINFORCEMENT LEARNING\":\nWe also look at an environment where strategies must be learned from raw pixels. We use the method\nof Tampuu et al. (2017) to alter the reward structure of Atari Pong so that whenever an agent scores a\npoint they receive a reward of 1 and the other player receives \u22122. We refer to this game as the Pong\nPlayer\u2019s Dilemma (PPD). In the PPD the only (jointly) winning move is not to play. However, a fully\ncooperative agent can be exploited by a defector.\n\nFrom \"CONSEQUENTIALIST CONDITIONAL COOPERATION IN SOCIAL DILEMMAS WITH IMPERFECT INFORMATION\":\nTo demonstrate this we follow the method of Tampuu et al. (2017) to construct a version of Atari Pong \nwhich makes the game into a social dilemma. In what we call the Pong Player\u2019s Dilemma (PPD) when an agent \nscores they gain a reward of 1 but the partner receives a reward of \u22122. Thus, in the PPD the only (jointly) winning\nmove is not to play, but selfish agents are again tempted to defect and try to score points even though\nthis decreases total social reward. We see that CCC is a successful, robust, and simple strategy in this\ngame.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Consequentialist conditional cooperation in social dilemmas with imperfect information", "abstract": "Social dilemmas, where mutual cooperation can lead to high payoffs but participants face incentives to cheat, are ubiquitous in multi-agent interaction. We wish to construct agents that cooperate with pure cooperators, avoid exploitation by pure defectors, and incentivize cooperation from the rest. However, often the actions taken by a partner are (partially) unobserved or the consequences of individual actions are hard to predict. We show that in a large class of games good strategies can be constructed by conditioning one's behavior solely on outcomes (ie. one's past rewards). We call this consequentialist conditional cooperation. We show how to construct such strategies using deep reinforcement learning techniques and demonstrate, both analytically and experimentally, that they are effective in social dilemmas beyond simple matrix games. We also show the limitations of relying purely on consequences and discuss the need for understanding both the consequences of and the intentions behind an action.", "pdf": "/pdf/251416720d46c82d6db75a812110371d037ba986.pdf", "TL;DR": "We show how to use deep RL to construct agents that can solve social dilemmas beyond matrix games.", "paperhash": "peysakhovich|consequentialist_conditional_cooperation_in_social_dilemmas_with_imperfect_information", "_bibtex": "@inproceedings{\npeysakhovich2018consequentialist,\ntitle={Consequentialist conditional cooperation in social dilemmas with imperfect information},\nauthor={Alexander Peysakhovich and Adam Lerer},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BkabRiQpb},\n}", "keywords": ["deep reinforcement learning", "cooperation", "social dilemma", "multi-agent systems"], "authors": ["Alexander Peysakhovich", "Adam Lerer"], "authorids": ["alexpeys@gmail.com", "alerer@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642375488, "id": "ICLR.cc/2018/Conference/-/Paper10/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper10/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper10/AnonReviewer2", "ICLR.cc/2018/Conference/Paper10/AnonReviewer1", "ICLR.cc/2018/Conference/Paper10/AnonReviewer3"], "reply": {"forum": "BkabRiQpb", "replyto": "BkabRiQpb", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper10/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642375488}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642375514, "tcdate": 1511848682115, "number": 3, "cdate": 1511848682115, "id": "B1GljO9xM", "invitation": "ICLR.cc/2018/Conference/-/Paper10/Official_Review", "forum": "BkabRiQpb", "replyto": "BkabRiQpb", "signatures": ["ICLR.cc/2018/Conference/Paper10/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "The paper makes a contribution to a challenging problem within multi-agent reinforcement learning. The paper is written clearly and it is easy to follow both the theoretical details and the general line of arguments. The paper would however, in my opinion, benefit from developing a number of areas of both the theoretical analysis and experimental section to solidify the contribution and validity. ", "rating": "6: Marginally above acceptance threshold", "review": "The main result specifies a (trigger) strategy (CCC) and corresponding algorithm that leads to an efficient outcome in social dilemmas, the theoretical basis of which is provided by theorem 1. This underscores an algorithm that uses a prosocial adjustment of the agents rewards to encourage efficient behaviour. The paper makes a useful contribution in demonstrating that convergence to efficient outcomes in social dilemmas without the need for agents to observe each other's actions. The paper is also clearly written and the theoretical result is accompanied by some supporting experiments. The numerical experiments show that using CCC strategy leads to an increase in the proportion of efficient equilibrium outcomes. However, in order to solidify the experimental validation, the authors could consider a broader range of experimental evaluations. There are also a number of items that could be added that I believe would strengthen the contribution and novelty, in particular:\n\nSome highly relevant references on (prosocial) reward shaping in social dilemmas are missing, such as Babes, Munoz de cote and Littman, 2008 and for the (iterated) prisoner's dilemma; Vassiliades and Christodoulou, 2010 which all provide important background material on the subject. In addition, it would be useful to see how the method put forward in the paper compares with other (reward-shaping) techniques within MARL (especially in the perfect information case in the pong players' dilemma (PPD) experiment) such as those already mentioned. The authors could, therefore, provide more detail in relating the contribution to these papers and other relevant past work and existing algorithms. \n\nThe paper also omits any formal discussion on the equilibrium concepts being used in the Markov game setting (e.g. Markov Perfect Equilibrium or Markov-Nash equilibrium) which leaves a notable gap in the theoretical analysis.  \n\nThere are also some questions that to me, remain unaddressed namely:\n\ni. the model of the experiments, particularly a description of the structure of the pong players' dilemma in terms of the elements of the partially observed Markov game described in definition 1. In particular, what are the state space and transitions?\n\nii. the equilibrium concepts being considered i.e. does the paper consider Markov perfect equilibria. Some analysis on the conditions that under which the continuation equilibria e.g. cooperation in the social dilemma is expected to arise would also be beneficial.\n\niii. Although the formal discussion is concerned with Markov games (i.e. repeated games with stochastic transitions with multiple states) the experiments (particularly the PPD) appear to apply to repeated games (this could very much be cleared up with a formal description of the games in the experimental sections and the equilibrium concept being used). \n\niv. In part 1 of the proof of the main theorem, it seems unclear why the sign of the main inequality has changed after application of Cauchy convergence in probability (equation at the top of the page). As this is an important component of the proof of the main result, the paper would benefit from an explanation of this step?\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Consequentialist conditional cooperation in social dilemmas with imperfect information", "abstract": "Social dilemmas, where mutual cooperation can lead to high payoffs but participants face incentives to cheat, are ubiquitous in multi-agent interaction. We wish to construct agents that cooperate with pure cooperators, avoid exploitation by pure defectors, and incentivize cooperation from the rest. However, often the actions taken by a partner are (partially) unobserved or the consequences of individual actions are hard to predict. We show that in a large class of games good strategies can be constructed by conditioning one's behavior solely on outcomes (ie. one's past rewards). We call this consequentialist conditional cooperation. We show how to construct such strategies using deep reinforcement learning techniques and demonstrate, both analytically and experimentally, that they are effective in social dilemmas beyond simple matrix games. We also show the limitations of relying purely on consequences and discuss the need for understanding both the consequences of and the intentions behind an action.", "pdf": "/pdf/251416720d46c82d6db75a812110371d037ba986.pdf", "TL;DR": "We show how to use deep RL to construct agents that can solve social dilemmas beyond matrix games.", "paperhash": "peysakhovich|consequentialist_conditional_cooperation_in_social_dilemmas_with_imperfect_information", "_bibtex": "@inproceedings{\npeysakhovich2018consequentialist,\ntitle={Consequentialist conditional cooperation in social dilemmas with imperfect information},\nauthor={Alexander Peysakhovich and Adam Lerer},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BkabRiQpb},\n}", "keywords": ["deep reinforcement learning", "cooperation", "social dilemma", "multi-agent systems"], "authors": ["Alexander Peysakhovich", "Adam Lerer"], "authorids": ["alexpeys@gmail.com", "alerer@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642375488, "id": "ICLR.cc/2018/Conference/-/Paper10/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper10/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper10/AnonReviewer2", "ICLR.cc/2018/Conference/Paper10/AnonReviewer1", "ICLR.cc/2018/Conference/Paper10/AnonReviewer3"], "reply": {"forum": "BkabRiQpb", "replyto": "BkabRiQpb", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper10/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642375488}}}, {"tddate": null, "ddate": null, "tmdate": 1514069298441, "tcdate": 1514068357952, "number": 1, "cdate": 1514068357952, "id": "Sy0tFIhMf", "invitation": "ICLR.cc/2018/Conference/-/Paper10/Official_Comment", "forum": "BkabRiQpb", "replyto": "B1GljO9xM", "signatures": ["ICLR.cc/2018/Conference/Paper10/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper10/Authors"], "content": {"title": "Reply", "comment": "We thank the reviewer for their comments. They have pointed out weaknesses in our presentation of our results. We have edited the text substantially and hope that our contributions and claims are much clearer.\n\n**>>> R3 asks about the relationship of our work to prior work on reward shaping in MARL. **\n \nWe are happy to add these references to the main text and discuss them. One important thing to note is that the prior work mentioned by the reviewer has dealt with perfectly observed games rather than partially observed ones. \n \nWe have added a longer discussion of how our work is related to existing work on MARL, equilibrium finding, and reward shaping. \n\nWe specifically discuss one of the examples the reviewer gives: Babes et al. 2008 use reward shaping in the repeated Prisoner's Dilemma to construct an agent that does well against fixed opponents as well as can lead learner \u201cfollowers\u201d to cooperate with it. However, in order to do this they first need to compute a value function for a known \u201cgood\u201d strategy (they use Pavlov, a variant of tit-for-tat) and use this for shaping. This is possible for the basic one (or multi-memory) PD but doesn't scale well to general Markov games (in particular partially observed ones). \n\nBy contrast, the CCC agent creates similar incentives by switching between two pre-computed strategies in a predictable way. The computation of these two strategies does not require anything other than standard self-play.\n\nCombining these ideas is an interesting direction for future research but beyond the scope of our paper.\n \n** **\n**>> R3 asks for formalization for the state spaces/transition functions/etc\u2026 in our games.**\n \nWe are happy to add more details of the games to the paper (as well as release the code upon publication). Our games do not permit a compact enumeration of the states and transitions (which is precisely why we are interested in moving beyond tabular methods to eg. deep RL). For example, in the PPD the full set of states is the set of RAM states in Atari Pong.\n\n**>> R3 asks about equilibrium concepts in our Markov game setting **\n\nWhile much existing work is framed in terms of finding good equilibria, our work is more related to questions raised by Axelrod (1984) who asks: if one is to enter a social dilemma with an (unknown) partner, how should one behave? \n\nThe work on tit-for-tat (TFT, and related strategies such as Win-Stay-Lose-Shift/Pavlov) comes up with the answer that one should play a strategy that is:\n\n    * simple\n    * nice (begins by cooperating)\n    * not exploitable\n    * forgiving (provides a way to return to cooperation after a defection)\n    * incentivizes cooperation from its partner (that is, a partner who commits to cooperation will get a higher payoff than a partner who commits to defection)\n\nOur main contribution is to find a way to construct a strategy which satisfies the Axelrod desiderata in *partially observed* Markov games which require deep RL for function approximation.\n\nNote that these desiderata are different from equilibrium desiderata. For example, tit-for-tat, one of the most heavily studied strategies in the Prisoner's Dilemma, is actually not a symmetric equilibrium strategy (because the best response to TFT is always cooperate). Rather, these desiderata are about agent design or about good strategies to commit to.\n\nWe do not claim that CCC forms an equilibrium with itself as there may be local deviations to improve one's payoff, however, our theorem shows that the partner of a CCC agent maximizes its asymptotic payoff by playing a policy that cooperates with the CCC agent (thus we preserve TFT-like incentive properties).\n\nWe have edited the text to make our problem statement / results clearer.\n\nWe only focus on equilibrium for computational reasons in the case of the D policy for which we have made an assumption that (D,D) forms an equilibrium in policies which only condition on agent's observations (this is related to the notion of a belief free equilibrium in repeated games Ely et al. 2005). \n\n\n**>> R3 asks \u201cit seems unclear why the sign of the main inequality has changed after application of Cauchy convergence in probability (equation at the top of the page)\u201d**\n** **\nWe apologize for any confusion. The equation at the top of the page uses the convention \n\nP(X) < epsilon \n \nwhile the next equations use the notation \n \nP(~X) > (1-epsilon)\n \nWe have changed these to both use P(~X)>(1-epsilon) so that it is more clear.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Consequentialist conditional cooperation in social dilemmas with imperfect information", "abstract": "Social dilemmas, where mutual cooperation can lead to high payoffs but participants face incentives to cheat, are ubiquitous in multi-agent interaction. We wish to construct agents that cooperate with pure cooperators, avoid exploitation by pure defectors, and incentivize cooperation from the rest. However, often the actions taken by a partner are (partially) unobserved or the consequences of individual actions are hard to predict. We show that in a large class of games good strategies can be constructed by conditioning one's behavior solely on outcomes (ie. one's past rewards). We call this consequentialist conditional cooperation. We show how to construct such strategies using deep reinforcement learning techniques and demonstrate, both analytically and experimentally, that they are effective in social dilemmas beyond simple matrix games. We also show the limitations of relying purely on consequences and discuss the need for understanding both the consequences of and the intentions behind an action.", "pdf": "/pdf/251416720d46c82d6db75a812110371d037ba986.pdf", "TL;DR": "We show how to use deep RL to construct agents that can solve social dilemmas beyond matrix games.", "paperhash": "peysakhovich|consequentialist_conditional_cooperation_in_social_dilemmas_with_imperfect_information", "_bibtex": "@inproceedings{\npeysakhovich2018consequentialist,\ntitle={Consequentialist conditional cooperation in social dilemmas with imperfect information},\nauthor={Alexander Peysakhovich and Adam Lerer},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BkabRiQpb},\n}", "keywords": ["deep reinforcement learning", "cooperation", "social dilemma", "multi-agent systems"], "authors": ["Alexander Peysakhovich", "Adam Lerer"], "authorids": ["alexpeys@gmail.com", "alerer@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825740751, "id": "ICLR.cc/2018/Conference/-/Paper10/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "BkabRiQpb", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper10/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper10/Authors|ICLR.cc/2018/Conference/Paper10/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper10/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper10/Authors|ICLR.cc/2018/Conference/Paper10/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper10/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper10/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper10/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper10/Reviewers", "ICLR.cc/2018/Conference/Paper10/Authors", "ICLR.cc/2018/Conference/Paper10/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825740751}}}, {"tddate": null, "ddate": null, "tmdate": 1514068561002, "tcdate": 1514068561002, "number": 3, "cdate": 1514068561002, "id": "rJYL9Uhzf", "invitation": "ICLR.cc/2018/Conference/-/Paper10/Official_Comment", "forum": "BkabRiQpb", "replyto": "r1uWD5txM", "signatures": ["ICLR.cc/2018/Conference/Paper10/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper10/Authors"], "content": {"title": "reply", "comment": "We have made several additions to the paper that were suggested by the reviewer (the paper updated paper can be viewed via the PDF link above). We think these suggestions make the contribution more clear. We thank the reviewer for these comments.\n\n>>> Specifically in the PPD game, the use of CCC produces interesting results; when paired with cooperate agents in the PPD game, CCC-based players produce higher overall reward than pairing cooperative players (see Figure 2, (d) & (e)). This should be explained. **\n \nThis is a good catch! Actually this is due to variance in the payoffs.\n \nThe 2 cooperator payoff is -.74 with a standard error (calculated assuming that each matchup is independent so using the empirical standard deviation / sqrt(n-1)) of +/- .56 while the 2 CCC payoff is -.22 with a CI of  +/- .36. Thus, these payoffs are statistically indistinguishable. \n \nOn the other hand 2 defectors get a payoff of -5.8 with a standard error of 2.2, so (D,D) is quite statistically distinguishable from (C,C) or (CCC,CCC). \n\nThis stochasticity only occurs in the rPPD because of the random nature of the defect-payoff. A defection in the standard PPD means the partner loses 2 points deterministically whereas in the rPPD the partner only realizes a large loss of -20 with a relatively small probability of .1. \n\nIn other words, the standard errors of the mean payoffs in the other games (eg. Fishery, standard PPD) are tiny and can be ignored but do need to be acknowledged in the rPPD.\n \nIn the current version we have relegated the full tournament results to the appendix and edited what we show in the text to better reflect our problem definition.\n \n**>>> worthwhile to provide an additional graph that shows the action choices of CCC agents over time to clarify behavioural characteristics and convergence performance.**\n\nThis is a good suggestion. We have added a figure that shows trajectories of behavior of a CCC agent with a C or D partner for the Fishery game.\n\n** >>> How much CCC perform compared to random policy selection? **\n \nWe note that while random policy selection will yield approximately a payoff of .5*(D,D) + .5*(C,D) when paired with a defect partner this random policy will no longer have the incentive properties of CCC are such that if our agent commits to CCC then their partner does better by cooperating than by defecting.\n\nBy comparison, if our agent commits to a random policy this incentive property no longer holds and so the best response for a partner is to always defect.\n \n**The game is parameterized with the rewards of +1 and +3. What is the bases for these parameter choices? What would happen if the higher reward was +2, or more interestingly, if the game was extended to allow agents to fish medium-sized fish (+2), in addition to small and large fish. Here it would be interesting to see how CCC fares (in all combinations with cooperators and defectors). **\n** **\nThe choice of +1/ +3 was made rather arbitrarily (in most behavioral studies of cooperation the key question is one of the benefit/cost ratio, typically a ratio of 2:1 or 3:1 is used, we simply copied that here). \n \nWe agree that more games are important for testing the robustness of CCC and other strategies for solving social dilemmas but we leave this for future work.\n\n**>> Typos/figure 2 clarity**\n** **\nWe thank the reviewer for these catches, we have fixed both of them.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Consequentialist conditional cooperation in social dilemmas with imperfect information", "abstract": "Social dilemmas, where mutual cooperation can lead to high payoffs but participants face incentives to cheat, are ubiquitous in multi-agent interaction. We wish to construct agents that cooperate with pure cooperators, avoid exploitation by pure defectors, and incentivize cooperation from the rest. However, often the actions taken by a partner are (partially) unobserved or the consequences of individual actions are hard to predict. We show that in a large class of games good strategies can be constructed by conditioning one's behavior solely on outcomes (ie. one's past rewards). We call this consequentialist conditional cooperation. We show how to construct such strategies using deep reinforcement learning techniques and demonstrate, both analytically and experimentally, that they are effective in social dilemmas beyond simple matrix games. We also show the limitations of relying purely on consequences and discuss the need for understanding both the consequences of and the intentions behind an action.", "pdf": "/pdf/251416720d46c82d6db75a812110371d037ba986.pdf", "TL;DR": "We show how to use deep RL to construct agents that can solve social dilemmas beyond matrix games.", "paperhash": "peysakhovich|consequentialist_conditional_cooperation_in_social_dilemmas_with_imperfect_information", "_bibtex": "@inproceedings{\npeysakhovich2018consequentialist,\ntitle={Consequentialist conditional cooperation in social dilemmas with imperfect information},\nauthor={Alexander Peysakhovich and Adam Lerer},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BkabRiQpb},\n}", "keywords": ["deep reinforcement learning", "cooperation", "social dilemma", "multi-agent systems"], "authors": ["Alexander Peysakhovich", "Adam Lerer"], "authorids": ["alexpeys@gmail.com", "alerer@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825740751, "id": "ICLR.cc/2018/Conference/-/Paper10/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "BkabRiQpb", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper10/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper10/Authors|ICLR.cc/2018/Conference/Paper10/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper10/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper10/Authors|ICLR.cc/2018/Conference/Paper10/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper10/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper10/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper10/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper10/Reviewers", "ICLR.cc/2018/Conference/Paper10/Authors", "ICLR.cc/2018/Conference/Paper10/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825740751}}}, {"tddate": null, "ddate": null, "tmdate": 1514068425208, "tcdate": 1514068425208, "number": 2, "cdate": 1514068425208, "id": "Byb0FUnfz", "invitation": "ICLR.cc/2018/Conference/-/Paper10/Official_Comment", "forum": "BkabRiQpb", "replyto": "HkCdXWqlM", "signatures": ["ICLR.cc/2018/Conference/Paper10/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper10/Authors"], "content": {"title": "Reply", "comment": "We thank the reviewer for their thorough review. We have made several changes in exposition and presentation. We hope these address the reviewer's concerns.\n\n>> The paper follows with some fun experiments implementing these new game theory notions. Unfortunately, since the game theory was not particularly well-motivated, I did not find the overall story compelling. It is perhaps interesting that one can make deep learning learn to cooperate with imperfect information, but one could have illustrated the game theory equally well with other techniques.\n\nFrom reading the reviewers' comments, we realize that we should have been much clearer front with our problem definition. We have edited the text substantially to make this clearer.\n\nOur goal is to consider a question of agent design: how should we build agents that can enter into social dilemmas and achieve high payoffs with partners that are themselves trying to achieve high payoffs?\n\nIn particular, we seek to answer this question for social dilemmas where actions of a partner are not observed.\n\nThis question is quite different from just making agents that cooperate all the time (since those agents are easily exploited by defectors). It is related to, but also different from, the question of computing cooperative equilibria. \n\nSee the reply to R3 above for a more in depth discussion about the desiderata from past literature that we seek to satisfy in order to construct a \u201cgood\u201d strategy for imperfect information social dilemmas.\n\n>> The reviewer asks whether maximizing the sum of the payoffs is the \u201cright\u201d solution\nWe agree with this criticism. While in symmetric games (eg. bargaining) behavioral experiments show that often view the symmetric sum of payoffs to be a natural focal point while in asymmetric games they do not (see eg. the chapter on bargaining in Kagel & Roth Handbook of Experimental Economics or more recent work on inequality in social dilemmas eg. Hauser, Kraft-Todd, Rand, Nowak & Norton 2016).\n \nThe question of how to choose the \u201ccorrect\u201d focal points for playing with humans comes down to asking what should the \u201cright\u201d utility function be for training the cooperative strategies (see Charness & Rabin (2002) for a generic utility function that can express many social goals). Note that CCC can then be applied using these new C strategies just as in the current work.\n\nHowever, figuring out the correct utility function to use here is far beyond the scope of this paper and is likely quite context dependent. This is an important direction for future research and we have made this point clear the paper.\n\n\n>> Similar paragraphs\nWe are also the authors of the other paper, it is earlier/related work to this paper (in the sense that it asks about designing agents that can solve social dilemmas), though it covers substantially different ground (perfectly observed games).\n\nWe apologize if there is something unclear from the current text. We do not mean to imply that this paper (CCC) introduces the PPD. Rather, it is the earlier paper (amTFT) that introduces the PPD as an environment and the CCC paper uses it for robustness checks.\n\nThe point of the PPD in this paper is to ask whether the other work is superceded by the CCC. Indeed, the techniques proposed in the amTFT paper can ONLY work in perfectly observed games. \n\nBy contrast, CCC is a good strategy for imperfectly observed games. Since any perfectly observed game is trivially also an imperfectly observed one one may think that CCC is just a strictly better strategy than amTFT (and thus the other paper is subsumed by this one).\n \nThe point of the PPD experiments in this paper is to show that there are classes of perfectly observed games where CCC performs similarly to amTFT (normal PPD) but there are also those where CCC fails but amTFT succeeds (risky PPD). \n\nWe have changed the text to make these points clearer and to attribute credit transparently.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Consequentialist conditional cooperation in social dilemmas with imperfect information", "abstract": "Social dilemmas, where mutual cooperation can lead to high payoffs but participants face incentives to cheat, are ubiquitous in multi-agent interaction. We wish to construct agents that cooperate with pure cooperators, avoid exploitation by pure defectors, and incentivize cooperation from the rest. However, often the actions taken by a partner are (partially) unobserved or the consequences of individual actions are hard to predict. We show that in a large class of games good strategies can be constructed by conditioning one's behavior solely on outcomes (ie. one's past rewards). We call this consequentialist conditional cooperation. We show how to construct such strategies using deep reinforcement learning techniques and demonstrate, both analytically and experimentally, that they are effective in social dilemmas beyond simple matrix games. We also show the limitations of relying purely on consequences and discuss the need for understanding both the consequences of and the intentions behind an action.", "pdf": "/pdf/251416720d46c82d6db75a812110371d037ba986.pdf", "TL;DR": "We show how to use deep RL to construct agents that can solve social dilemmas beyond matrix games.", "paperhash": "peysakhovich|consequentialist_conditional_cooperation_in_social_dilemmas_with_imperfect_information", "_bibtex": "@inproceedings{\npeysakhovich2018consequentialist,\ntitle={Consequentialist conditional cooperation in social dilemmas with imperfect information},\nauthor={Alexander Peysakhovich and Adam Lerer},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BkabRiQpb},\n}", "keywords": ["deep reinforcement learning", "cooperation", "social dilemma", "multi-agent systems"], "authors": ["Alexander Peysakhovich", "Adam Lerer"], "authorids": ["alexpeys@gmail.com", "alerer@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825740751, "id": "ICLR.cc/2018/Conference/-/Paper10/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "BkabRiQpb", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper10/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper10/Authors|ICLR.cc/2018/Conference/Paper10/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper10/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper10/Authors|ICLR.cc/2018/Conference/Paper10/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper10/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper10/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper10/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper10/Reviewers", "ICLR.cc/2018/Conference/Paper10/Authors", "ICLR.cc/2018/Conference/Paper10/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825740751}}}], "count": 9}