{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518865896510, "tcdate": 1518865896510, "number": 3, "cdate": 1518865896510, "id": "SJlgRKHDf", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "SJgJm52UM", "replyto": "SJgJm52UM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "This paper does not follow the formatting guidelines in https://iclr.cc/Conferences/2018/CallForWorkshops and is, thus, rejected."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Music Transcription with Convolutional  Sequence-to-Sequence models", "abstract": "Translating an audio sequence to a symbolic representation of music a fundamen- tal problem in Music Information Retrieval (MIR) refereed to Automatic Music Transcription (AMT). Recently, convolutional neural networks (CNNs) have been successfully applied to the task by translating frames of audioSigtia et al. (2016); Thickstun et al. (2017). However, those models can by their nature not model temporal relations and long time dependencies. Furthermore, it is extremely la- bor intense to get annotations for supervised learning in this setting. We propose a model that overcomes all these problems. The convolutional sequence to sequence (Cseq2seq) model applies a CNN to learn a low dimensional representation of au- dio frames and a sequential model to translate these learned features to a symbolic representation directly. Our approach has three advantages over other methods: (i) extracting audio frame representations and learning the sequential model is jointly trained end-to-end, (ii) the recurrent model can capture temporal features in musical pieces in order to improve transcription, and (iii) our model learns from entire sequences as opposed to temporally accurately annotated onsets and offsets for each note thus making it possible to train on large already existing corpora of music. For the purpose of testing our method we created our own dataset of 17K monophonic songs and respective MusicXML files. Initial experiments proof the validity of our approach.", "pdf": "/pdf/275a950d5688c9a0fd9f491b4d9b57cd7a66d566.pdf", "TL;DR": "Solving automatic music transcription without note level annotated data deep learning style.", "paperhash": "ullrich|music_transcription_with_convolutional_sequencetosequence_models", "keywords": ["automatic music transcription", "audio", "music", "deep learning", "sparse data regime"], "authors": ["Karen Ullrich", "Eelco van der Wel"], "authorids": ["karen.ullrich@uva.nl", "eelcovdw@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1518277336413, "tcdate": 1518277336413, "number": 56, "cdate": 1518277336413, "id": "SJgJm52UM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "SJgJm52UM", "signatures": ["~Karen_Ullrich1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Music Transcription with Convolutional  Sequence-to-Sequence models", "abstract": "Translating an audio sequence to a symbolic representation of music a fundamen- tal problem in Music Information Retrieval (MIR) refereed to Automatic Music Transcription (AMT). Recently, convolutional neural networks (CNNs) have been successfully applied to the task by translating frames of audioSigtia et al. (2016); Thickstun et al. (2017). However, those models can by their nature not model temporal relations and long time dependencies. Furthermore, it is extremely la- bor intense to get annotations for supervised learning in this setting. We propose a model that overcomes all these problems. The convolutional sequence to sequence (Cseq2seq) model applies a CNN to learn a low dimensional representation of au- dio frames and a sequential model to translate these learned features to a symbolic representation directly. Our approach has three advantages over other methods: (i) extracting audio frame representations and learning the sequential model is jointly trained end-to-end, (ii) the recurrent model can capture temporal features in musical pieces in order to improve transcription, and (iii) our model learns from entire sequences as opposed to temporally accurately annotated onsets and offsets for each note thus making it possible to train on large already existing corpora of music. For the purpose of testing our method we created our own dataset of 17K monophonic songs and respective MusicXML files. Initial experiments proof the validity of our approach.", "pdf": "/pdf/275a950d5688c9a0fd9f491b4d9b57cd7a66d566.pdf", "TL;DR": "Solving automatic music transcription without note level annotated data deep learning style.", "paperhash": "ullrich|music_transcription_with_convolutional_sequencetosequence_models", "keywords": ["automatic music transcription", "audio", "music", "deep learning", "sparse data regime"], "authors": ["Karen Ullrich", "Eelco van der Wel"], "authorids": ["karen.ullrich@uva.nl", "eelcovdw@gmail.com"]}, "nonreaders": [], "details": {"replyCount": 1, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}], "count": 2}