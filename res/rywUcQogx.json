{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396671895, "tcdate": 1486396671895, "number": 1, "id": "HkOW6MUde", "invitation": "ICLR.cc/2017/conference/-/paper550/acceptance", "forum": "rywUcQogx", "replyto": "rywUcQogx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The authors propose to use CCA as a transformation within a network that optimally correlates two views. The authors then back-propagate gradients through the CCA. Promising experimental results on for cross-modality retrieval experiments on two public image-to-text datasets are presented. \n \n The main concern with the paper is the clarity of the exposition. The novelty and motivation of the approach remains unclear, despite significant effort from the reviewers to understand. \n \n A major rewriting of the paper will generate a stronger submission to a future venue."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Canonical Correlation Analysis", "abstract": "Canonical Correlation Analysis (CCA) computes maximally-correlated \nlinear projections of two modalities. We propose Differentiable CCA, a \nformulation of CCA that can be cast as a layer within a multi-view \nneural network. Unlike Deep CCA, an earlier extension of CCA to \nnonlinear projections, our formulation enables gradient flow through the \ncomputation of the CCA projection matrices, and free choice of the final \noptimization target. We show the effectiveness of this approach in \ncross-modality retrieval experiments on two public image-to-text \ndatasets, surpassing both Deep CCA and a multi-view network with \nfreely-learned projections. We assume that Differentiable CCA could be a \nuseful building block for many multi-modality tasks.", "pdf": "/pdf/6820cff043fabaac08ad991cb9057cb0721e58c7.pdf", "TL;DR": "We propose Differentiable CCA a  formulation of CCA that enables gradient flow through the  computation of the CCA projection matrices.", "paperhash": "dorfer|differentiable_canonical_correlation_analysis", "keywords": ["Multi-modal learning"], "conflicts": ["jku.at", "ofai.at", "scch.at"], "authors": ["Matthias Dorfer", "Jan Schl\u00fcter", "Gerhard Widmer"], "authorids": ["matthias.dorfer@jku.at", "jan.schlueter@ofai.at", "gerhard.widmer@jku.at"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396672633, "id": "ICLR.cc/2017/conference/-/paper550/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rywUcQogx", "replyto": "rywUcQogx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396672633}}}, {"tddate": null, "tmdate": 1483518188771, "tcdate": 1483518188771, "number": 2, "id": "SyBeWVcSx", "invitation": "ICLR.cc/2017/conference/-/paper550/public/comment", "forum": "rywUcQogx", "replyto": "rywUcQogx", "signatures": ["~Matthias_Dorfer1"], "readers": ["everyone"], "writers": ["~Matthias_Dorfer1"], "content": {"title": "Response to reviewer comments", "comment": "Dear reviewers, thank you for your effort and valuable feedback! We \nunderstand that we did not manage to clearly present the central idea of \nour work.\nWe do *not* combine the CCA objective (i.e., maximizing the correlation \nbetween the two hidden representations, the so-called Trace Norm \nObjective in Deep CCA) with another objective. We use CCA as a \ntransformation anywhere within a network that optimally correlates two \nviews, but we do not optimize the network towards maximal correlation as \ndone in Deep CCA. Instead, we apply the CCA transformations in the \nforward pass (not done in Deep CCA) and compute an arbitrary loss \nfunction on the transformed data.\nOur use of CCA can be compared to Batch Normalization (BN): BN provides \na transformation applicable anywhere within a network that normalizes a \nsingle view, and allows backpropagation of gradients through the \nnormalization procedure. Similarly, we backpropagate gradients through \nthe CCA procedure, where the gradients can be derived from any loss \nfunction operating on the optimally-correlated views produced by CCA.\nWe are grateful for all your comments, and will rewrite our manuscript \nto submit it elsewhere."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Canonical Correlation Analysis", "abstract": "Canonical Correlation Analysis (CCA) computes maximally-correlated \nlinear projections of two modalities. We propose Differentiable CCA, a \nformulation of CCA that can be cast as a layer within a multi-view \nneural network. Unlike Deep CCA, an earlier extension of CCA to \nnonlinear projections, our formulation enables gradient flow through the \ncomputation of the CCA projection matrices, and free choice of the final \noptimization target. We show the effectiveness of this approach in \ncross-modality retrieval experiments on two public image-to-text \ndatasets, surpassing both Deep CCA and a multi-view network with \nfreely-learned projections. We assume that Differentiable CCA could be a \nuseful building block for many multi-modality tasks.", "pdf": "/pdf/6820cff043fabaac08ad991cb9057cb0721e58c7.pdf", "TL;DR": "We propose Differentiable CCA a  formulation of CCA that enables gradient flow through the  computation of the CCA projection matrices.", "paperhash": "dorfer|differentiable_canonical_correlation_analysis", "keywords": ["Multi-modal learning"], "conflicts": ["jku.at", "ofai.at", "scch.at"], "authors": ["Matthias Dorfer", "Jan Schl\u00fcter", "Gerhard Widmer"], "authorids": ["matthias.dorfer@jku.at", "jan.schlueter@ofai.at", "gerhard.widmer@jku.at"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287525364, "id": "ICLR.cc/2017/conference/-/paper550/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rywUcQogx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper550/reviewers", "ICLR.cc/2017/conference/paper550/areachairs"], "cdate": 1485287525364}}}, {"tddate": null, "tmdate": 1482325019569, "tcdate": 1482325019569, "number": 3, "id": "rkEX3x_Nx", "invitation": "ICLR.cc/2017/conference/-/paper550/official/review", "forum": "rywUcQogx", "replyto": "rywUcQogx", "signatures": ["ICLR.cc/2017/conference/paper550/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper550/AnonReviewer1"], "content": {"title": "Unclear about the contribution ", "rating": "3: Clear rejection", "review": "It is not clear to me at all what this paper is contributing. Deep CCA (Andrew et al, 2013) already gives the gradient derivation of the correlation objective with respect to the network outputs which are then back-propagated to update the network weights. Again, the paper gives the gradient of the correlation (i.e. the CCA objective) w.r.t. the network outputs, so it is confusing to me when authors say that their differentiable version enables them to back-propagate directly through the computation of CCA. \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Canonical Correlation Analysis", "abstract": "Canonical Correlation Analysis (CCA) computes maximally-correlated \nlinear projections of two modalities. We propose Differentiable CCA, a \nformulation of CCA that can be cast as a layer within a multi-view \nneural network. Unlike Deep CCA, an earlier extension of CCA to \nnonlinear projections, our formulation enables gradient flow through the \ncomputation of the CCA projection matrices, and free choice of the final \noptimization target. We show the effectiveness of this approach in \ncross-modality retrieval experiments on two public image-to-text \ndatasets, surpassing both Deep CCA and a multi-view network with \nfreely-learned projections. We assume that Differentiable CCA could be a \nuseful building block for many multi-modality tasks.", "pdf": "/pdf/6820cff043fabaac08ad991cb9057cb0721e58c7.pdf", "TL;DR": "We propose Differentiable CCA a  formulation of CCA that enables gradient flow through the  computation of the CCA projection matrices.", "paperhash": "dorfer|differentiable_canonical_correlation_analysis", "keywords": ["Multi-modal learning"], "conflicts": ["jku.at", "ofai.at", "scch.at"], "authors": ["Matthias Dorfer", "Jan Schl\u00fcter", "Gerhard Widmer"], "authorids": ["matthias.dorfer@jku.at", "jan.schlueter@ofai.at", "gerhard.widmer@jku.at"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512545446, "id": "ICLR.cc/2017/conference/-/paper550/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper550/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper550/AnonReviewer2", "ICLR.cc/2017/conference/paper550/AnonReviewer3", "ICLR.cc/2017/conference/paper550/AnonReviewer1"], "reply": {"forum": "rywUcQogx", "replyto": "rywUcQogx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper550/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper550/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512545446}}}, {"tddate": null, "tmdate": 1481907640614, "tcdate": 1481907640614, "number": 2, "id": "SJ-aT5ZNg", "invitation": "ICLR.cc/2017/conference/-/paper550/official/review", "forum": "rywUcQogx", "replyto": "rywUcQogx", "signatures": ["ICLR.cc/2017/conference/paper550/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper550/AnonReviewer3"], "content": {"title": "paper needs to be more explicit", "rating": "4: Ok but not good enough - rejection", "review": "After a second look of the paper, I am still confused what the authors are trying to achieve.\n\nThe CCA objective is not differentiable in the sense that the sum of singular values (trace norm) of T is not differentiable. It appears to me (from the title, and section 3), the authors are trying to solve this problem. However,\n\n-- Did the authors simply reformulate the CCA objective or change the objective? The authors need to be explicit here.\n\n-- What is the relationship between the retrieval objective and the \"CCA layer\"? I could imagine different ways of combining them, such as combination or bi-level optimization. And I could not find discussion about this in section 3. For this, equations would be helpful.\n\n-- Even though the CCA objective is not differentiable in the above sense, it has not caused major problem for training (e.g., in principle we need batch training, but empirically using large minibatches works fine). The authors need to justify why the original gradient computation is problematic for what the authors are trying to achieve. From the authors' response to my question 2, it seems they still use SVD of T, so I am not sure if the proposed method has advantage in computational efficiency.\n\nIn terms of paper organization, it is better to describe the retrieval objective earlier than in the experiments. And I still encourage the authors to conduct the comparison with contrastive loss that I mentioned in my previous comments. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Canonical Correlation Analysis", "abstract": "Canonical Correlation Analysis (CCA) computes maximally-correlated \nlinear projections of two modalities. We propose Differentiable CCA, a \nformulation of CCA that can be cast as a layer within a multi-view \nneural network. Unlike Deep CCA, an earlier extension of CCA to \nnonlinear projections, our formulation enables gradient flow through the \ncomputation of the CCA projection matrices, and free choice of the final \noptimization target. We show the effectiveness of this approach in \ncross-modality retrieval experiments on two public image-to-text \ndatasets, surpassing both Deep CCA and a multi-view network with \nfreely-learned projections. We assume that Differentiable CCA could be a \nuseful building block for many multi-modality tasks.", "pdf": "/pdf/6820cff043fabaac08ad991cb9057cb0721e58c7.pdf", "TL;DR": "We propose Differentiable CCA a  formulation of CCA that enables gradient flow through the  computation of the CCA projection matrices.", "paperhash": "dorfer|differentiable_canonical_correlation_analysis", "keywords": ["Multi-modal learning"], "conflicts": ["jku.at", "ofai.at", "scch.at"], "authors": ["Matthias Dorfer", "Jan Schl\u00fcter", "Gerhard Widmer"], "authorids": ["matthias.dorfer@jku.at", "jan.schlueter@ofai.at", "gerhard.widmer@jku.at"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512545446, "id": "ICLR.cc/2017/conference/-/paper550/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper550/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper550/AnonReviewer2", "ICLR.cc/2017/conference/paper550/AnonReviewer3", "ICLR.cc/2017/conference/paper550/AnonReviewer1"], "reply": {"forum": "rywUcQogx", "replyto": "rywUcQogx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper550/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper550/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512545446}}}, {"tddate": null, "tmdate": 1481785000896, "tcdate": 1481785000891, "number": 1, "id": "ry-2Cn1Eg", "invitation": "ICLR.cc/2017/conference/-/paper550/official/review", "forum": "rywUcQogx", "replyto": "rywUcQogx", "signatures": ["ICLR.cc/2017/conference/paper550/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper550/AnonReviewer2"], "content": {"title": "Needs significant work before it can be publishable", "rating": "3: Clear rejection", "review": "The authors propose to combine a CCA objective with a downstream loss.  This is a really nice and natural idea.  However, both the execution and presentation leave a lot to be desired in the current version of the paper.\n\nIt is not clear what the overall objective is.  This was asked in a pre-review question but the answer did not fully clarify it for me.  Is it the sum of the CCA objective and the final (top-layer) objective, including the CCA constraints?  Is there some interpolation of the two objectives?  \n\nBy saying that the top-layer objective is \"cosine distance\" or \"squared cosine distance\", do you really mean you are just minimizing this distance between the matched pairs in the two views?  If so, then of course that does not work out of the box without the intervening CCA layer:  You could minimize it by setting all of the projections to a single point.  A better comparison would be against a contrastive loss like the Hermann & Blunsom one mentioned in the reviewer question, which aims to both minimize the distance for matched pairs and separate mismatched ones (where \"mismatched\" ones can be uniformly drawn, or picked in some cleverer way).  But other discriminative top-layer objectives that are tailored to a downstream task could make sense.\n\nThere is some loose terminology in the paper.  The authors refer to the \"correlation\" and \"cross-correlation\" between two vectors.  \"Correlation\" normally applies to scalars, so you need to define what you mean here.  \"Cross-correlation\" typically refers to time series.  In eq. (2) you are taking the max of a matrix.  Finally I am not too sure in what way this approach is \"fully differentiable\" while regular CCA is not -- perhaps it is worth revisiting this term as well.\n\nAlso just a small note about the relationship between cosine distance and correlation:  they are related when we view the dimensions of each of the two vectors as samples of a single random variable.  In that case the cosine distance of the (mean-normalized) vectors is the same as the correlation between the two corresponding random variables.  In CCA we are viewing each dimension of the vectors as its own random variable.  So I fear the claim about cosine distance and correlation is a bit of a red herring here.\n\nA couple of typos:\n\n\"prosed\" --> \"proposed\"\n\"allong\" --> \"along\"\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Canonical Correlation Analysis", "abstract": "Canonical Correlation Analysis (CCA) computes maximally-correlated \nlinear projections of two modalities. We propose Differentiable CCA, a \nformulation of CCA that can be cast as a layer within a multi-view \nneural network. Unlike Deep CCA, an earlier extension of CCA to \nnonlinear projections, our formulation enables gradient flow through the \ncomputation of the CCA projection matrices, and free choice of the final \noptimization target. We show the effectiveness of this approach in \ncross-modality retrieval experiments on two public image-to-text \ndatasets, surpassing both Deep CCA and a multi-view network with \nfreely-learned projections. We assume that Differentiable CCA could be a \nuseful building block for many multi-modality tasks.", "pdf": "/pdf/6820cff043fabaac08ad991cb9057cb0721e58c7.pdf", "TL;DR": "We propose Differentiable CCA a  formulation of CCA that enables gradient flow through the  computation of the CCA projection matrices.", "paperhash": "dorfer|differentiable_canonical_correlation_analysis", "keywords": ["Multi-modal learning"], "conflicts": ["jku.at", "ofai.at", "scch.at"], "authors": ["Matthias Dorfer", "Jan Schl\u00fcter", "Gerhard Widmer"], "authorids": ["matthias.dorfer@jku.at", "jan.schlueter@ofai.at", "gerhard.widmer@jku.at"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512545446, "id": "ICLR.cc/2017/conference/-/paper550/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper550/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper550/AnonReviewer2", "ICLR.cc/2017/conference/paper550/AnonReviewer3", "ICLR.cc/2017/conference/paper550/AnonReviewer1"], "reply": {"forum": "rywUcQogx", "replyto": "rywUcQogx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper550/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper550/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512545446}}}, {"tddate": null, "tmdate": 1481656170413, "tcdate": 1481656170405, "number": 1, "id": "SkfODTamx", "invitation": "ICLR.cc/2017/conference/-/paper550/public/comment", "forum": "rywUcQogx", "replyto": "HJOcE7y7e", "signatures": ["~Matthias_Dorfer1"], "readers": ["everyone"], "writers": ["~Matthias_Dorfer1"], "content": {"title": "Answer to AnonReviewer3's initial questions", "comment": ">> 1. in equation 7, it should be tr((T'T)^0.5) instead of tr(T'T)^0.5 ?\n\nThank you, good catch. We took this from Equation (10) of Andrew et al. \n(2013), which has it wrong. We will fix this in the next revision.\n\n>> 2. in sec 3.1 gradient of CCA, it appears you are computing the\n>> derivative of tr(T'T) or tr(TT') with respect to the data. Maybe this\n>> gives you simpler gradient wrt eigenvectors for symmetric matrices, than\n>> gradient wrt singular vectors for non-square matrices. However, does it\n>> still require eigenvalue decompositions of T'T and TT'?\n\nIn contrast to Deep CCA, we do not compute the derivative of tr(T'T) \nwrt. the data (i.e., the trace norm objective), but the derivative of \nthe projected data wrt. the input data, which requires the derivative of \nthe eigenvectors wrt. the input data. We use the derivative of the eigen \ndecomposition due to its simpler form, and this requires the \neigenvectors and eigenvalues to be known. We do not need to compute the \neigenvalue decompositions of T'T and TT' for that, though, we can still \nuse the SVD of T in the forward pass. We will clarify this in our revision.\n\n>> 3. stochastic optimization for deep CCA have been done in\n>> Weiran Wang, Raman Arora, Nathan Srebro, and Karen Livescu. Stochastic\n>> Optimization for Deep CCA via Nonlinear Orthogonal Iterations. ALLERTON,\n>> 2015. This paper transform the deep CCA problem into a sequence of nonlinear\n>> least squares problem, for which gradients are trivial to derive.\n>> Have you compared with this paper? Also the idea of using running\n>> averages with small minibatches was explored in this paper.\n\nThe mentioned paper is a stochastic version of Deep CCA as introduced in \nAndrew et al. (2013). It tackles the stochastic optimization of the \ntrace norm objective, while our work replaces the trace norm objective \nwith a freely chosen loss function. Thus, this paper does not provide a \nuseful comparison point.\nHowever, we were not aware that Wang et al. also experimented with \nrunning averages and will include this reference in our revision. Thank \nyou for the hint! Note that we did not end up using running averages for \nour results. We found that stochastic optimization of CCA is feasible \nwith large mini-batches (like Wang et al. (2015a;b) cited in our work), \nand works better than running averages (see Appendix B).\n\n>> 4. What is the retrieval objective used in the paper? [...]\n\nThe retrieval objective in our case is cosine distance, or squared \ncosine distance, described in Section 4.2. We initially decided not to \ninclude contrastive or ranking losses, but show that our CCA layer \nallows us to improve over Deep CCA by replacing its trace norm objective \neven with a very simple alternative.\nHowever, our proposal is more than an extension of Deep CCA. Our CCA \nlayer can serve as a replacement for freely learned projections (i.e., \ndense layers) in any multi-view network. For example, we obtained good \nresults combining it with the pairwise ranking loss described in Kiros \net al., 2014. Their loss is also based on cosine distance and a natural \nfit to the proposed CCA layer. As this adds an interesting perspective \non our work, we will add these results in our revised manuscript.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Canonical Correlation Analysis", "abstract": "Canonical Correlation Analysis (CCA) computes maximally-correlated \nlinear projections of two modalities. We propose Differentiable CCA, a \nformulation of CCA that can be cast as a layer within a multi-view \nneural network. Unlike Deep CCA, an earlier extension of CCA to \nnonlinear projections, our formulation enables gradient flow through the \ncomputation of the CCA projection matrices, and free choice of the final \noptimization target. We show the effectiveness of this approach in \ncross-modality retrieval experiments on two public image-to-text \ndatasets, surpassing both Deep CCA and a multi-view network with \nfreely-learned projections. We assume that Differentiable CCA could be a \nuseful building block for many multi-modality tasks.", "pdf": "/pdf/6820cff043fabaac08ad991cb9057cb0721e58c7.pdf", "TL;DR": "We propose Differentiable CCA a  formulation of CCA that enables gradient flow through the  computation of the CCA projection matrices.", "paperhash": "dorfer|differentiable_canonical_correlation_analysis", "keywords": ["Multi-modal learning"], "conflicts": ["jku.at", "ofai.at", "scch.at"], "authors": ["Matthias Dorfer", "Jan Schl\u00fcter", "Gerhard Widmer"], "authorids": ["matthias.dorfer@jku.at", "jan.schlueter@ofai.at", "gerhard.widmer@jku.at"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287525364, "id": "ICLR.cc/2017/conference/-/paper550/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rywUcQogx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper550/reviewers", "ICLR.cc/2017/conference/paper550/areachairs"], "cdate": 1485287525364}}}, {"tddate": null, "tmdate": 1480696975626, "tcdate": 1480696975621, "number": 1, "id": "HJOcE7y7e", "invitation": "ICLR.cc/2017/conference/-/paper550/pre-review/question", "forum": "rywUcQogx", "replyto": "rywUcQogx", "signatures": ["ICLR.cc/2017/conference/paper550/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper550/AnonReviewer3"], "content": {"title": "some questions", "question": "I just took a very quick look at the paper (did not go beyond Sec 3).\n\n1. in equation 7, it should be tr((T'T)^0.5) instead of tr(T'T)^0.5 ?\n\n2. in sec 3.1 gradient of CCA, it appears you are computing the derivative of tr(T'T) or tr(TT') with respect to the data. Maybe this gives you simpler gradient wrt eigenvectors for symmetric matrices, than gradient wrt singular vectors for non-square matrices. However, does it still require eigenvalue decompositions of T'T and TT'?\n\n3. stochastic optimization for deep CCA have been done in \nWeiran Wang, Raman Arora, Nathan Srebro, and Karen Livescu. Stochastic Optimization for Deep CCA via Nonlinear Orthogonal Iterations. ALLERTON, 2015. \nThis paper transform the deep CCA problem into a sequence of nonlinear least squares problem, for which gradients are trivial to derive.\nHave you compared with this paper? Also the idea of using running averages with small minibatches was explored in this paper. \n\n4. What is the retrieval objective used in the paper? Another simple (easy to optimize) objective that works well for retrieval is the contrastive (large margin) loss, see for example\nMultilingual Distributed Representations without Word Alignment. Karl Moritz Hermann, Phil Blunsom.\nICLR 2014. \nIvan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and language. In ICLR 2016.\nHave you compared with this objective?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Differentiable Canonical Correlation Analysis", "abstract": "Canonical Correlation Analysis (CCA) computes maximally-correlated \nlinear projections of two modalities. We propose Differentiable CCA, a \nformulation of CCA that can be cast as a layer within a multi-view \nneural network. Unlike Deep CCA, an earlier extension of CCA to \nnonlinear projections, our formulation enables gradient flow through the \ncomputation of the CCA projection matrices, and free choice of the final \noptimization target. We show the effectiveness of this approach in \ncross-modality retrieval experiments on two public image-to-text \ndatasets, surpassing both Deep CCA and a multi-view network with \nfreely-learned projections. We assume that Differentiable CCA could be a \nuseful building block for many multi-modality tasks.", "pdf": "/pdf/6820cff043fabaac08ad991cb9057cb0721e58c7.pdf", "TL;DR": "We propose Differentiable CCA a  formulation of CCA that enables gradient flow through the  computation of the CCA projection matrices.", "paperhash": "dorfer|differentiable_canonical_correlation_analysis", "keywords": ["Multi-modal learning"], "conflicts": ["jku.at", "ofai.at", "scch.at"], "authors": ["Matthias Dorfer", "Jan Schl\u00fcter", "Gerhard Widmer"], "authorids": ["matthias.dorfer@jku.at", "jan.schlueter@ofai.at", "gerhard.widmer@jku.at"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959218653, "id": "ICLR.cc/2017/conference/-/paper550/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper550/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper550/AnonReviewer3"], "reply": {"forum": "rywUcQogx", "replyto": "rywUcQogx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper550/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper550/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959218653}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478366282448, "tcdate": 1478339151553, "number": 550, "id": "rywUcQogx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rywUcQogx", "signatures": ["~Matthias_Dorfer1"], "readers": ["everyone"], "content": {"title": "Differentiable Canonical Correlation Analysis", "abstract": "Canonical Correlation Analysis (CCA) computes maximally-correlated \nlinear projections of two modalities. We propose Differentiable CCA, a \nformulation of CCA that can be cast as a layer within a multi-view \nneural network. Unlike Deep CCA, an earlier extension of CCA to \nnonlinear projections, our formulation enables gradient flow through the \ncomputation of the CCA projection matrices, and free choice of the final \noptimization target. We show the effectiveness of this approach in \ncross-modality retrieval experiments on two public image-to-text \ndatasets, surpassing both Deep CCA and a multi-view network with \nfreely-learned projections. We assume that Differentiable CCA could be a \nuseful building block for many multi-modality tasks.", "pdf": "/pdf/6820cff043fabaac08ad991cb9057cb0721e58c7.pdf", "TL;DR": "We propose Differentiable CCA a  formulation of CCA that enables gradient flow through the  computation of the CCA projection matrices.", "paperhash": "dorfer|differentiable_canonical_correlation_analysis", "keywords": ["Multi-modal learning"], "conflicts": ["jku.at", "ofai.at", "scch.at"], "authors": ["Matthias Dorfer", "Jan Schl\u00fcter", "Gerhard Widmer"], "authorids": ["matthias.dorfer@jku.at", "jan.schlueter@ofai.at", "gerhard.widmer@jku.at"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 8}