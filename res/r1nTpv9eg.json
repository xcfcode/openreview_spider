{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488569486713, "tcdate": 1478290884353, "number": 443, "id": "r1nTpv9eg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "r1nTpv9eg", "signatures": ["~Misha_Denil2"], "readers": ["everyone"], "content": {"title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning", "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.  We also compare our learned experimentation policies to randomized baselines and show that the learned policies lead to better predictions.", "pdf": "/pdf/ebbdbda6bc6648f3ddd06a91a4229083302dd917.pdf", "TL;DR": "We train agents to conduct experiments in interactive simulated physical environments.", "paperhash": "denil|learning_to_perform_physics_experiments_via_deep_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Misha Denil", "Pulkit Agrawal", "Tejas D Kulkarni", "Tom Erez", "Peter Battaglia", "Nando de Freitas"], "authorids": ["mdenil@google.com", "pulkitag@berkeley.edu", "tkulkarni@google.com", "etom@google.com", "peterbattaglia@google.com", "nandodefreitas@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 20, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396588077, "tcdate": 1486396588077, "number": 1, "id": "BJN3hfI_e", "invitation": "ICLR.cc/2017/conference/-/paper443/acceptance", "forum": "r1nTpv9eg", "replyto": "r1nTpv9eg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The following statement best summarizes the contribution: \"This paper shows that model free RL methods can learn how to gather information about physical properties of objects, even when this information is not available to a passive observer, and use this information to make decisions.\" So this is not a paper about new theory or algorithms, but rather about solving the problem of acquiring knowledge about the physics of the world around us, which is important for many problems and helps explain human performance in many tasks. There are still some concerns about the depth-of-analysis of the paper, but on balance, it is seen as an unconventional but interesting paper. As per AnonReviewer6, the final version could still aim to better address \"What should other researchers focus on if they are trying to build agents that can understand physics intuitively (building off this work)?\"\n -- Area chair", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning", "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.  We also compare our learned experimentation policies to randomized baselines and show that the learned policies lead to better predictions.", "pdf": "/pdf/ebbdbda6bc6648f3ddd06a91a4229083302dd917.pdf", "TL;DR": "We train agents to conduct experiments in interactive simulated physical environments.", "paperhash": "denil|learning_to_perform_physics_experiments_via_deep_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Misha Denil", "Pulkit Agrawal", "Tejas D Kulkarni", "Tom Erez", "Peter Battaglia", "Nando de Freitas"], "authorids": ["mdenil@google.com", "pulkitag@berkeley.edu", "tkulkarni@google.com", "etom@google.com", "peterbattaglia@google.com", "nandodefreitas@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396588627, "id": "ICLR.cc/2017/conference/-/paper443/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "r1nTpv9eg", "replyto": "r1nTpv9eg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396588627}}}, {"tddate": null, "tmdate": 1484804492784, "tcdate": 1484783450460, "number": 3, "id": "BJzv1tp8g", "invitation": "ICLR.cc/2017/conference/-/paper443/official/comment", "forum": "r1nTpv9eg", "replyto": "ryJacsq8l", "signatures": ["ICLR.cc/2017/conference/paper443/AnonReviewer6"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper443/AnonReviewer6"], "content": {"title": "Response", "comment": "Thank you for the detailed response, and for the high-level comment describing the importance of the paper. \n\nRegarding Agarwal et al. (2016): indeed, I assumed the authors were familiar with the differences in these works, considering the fact that Agarwal is the second author on this paper. My opinion was more that, on my first read of the paper, it did not seem that this would be clear to readers with only slight familiarity with previous works. I think it would be worthwhile to mention a key difference (that you ensure the agent can't infer the physics from simply visual observations) earlier in the paper, perhaps in the introduction.\n\nRegarding paper motivation: I quite like the top-level comment that describes the importance of the paper. I completely agree that the usual paradigm (come up with a slightly changed algorithm, show slightly better results) is often not the most productive way forward for science. I think papers in the vein of \"laying foundations for exploring an important direction\" are less often published, but certainly no less valuable. This paper is along those lines; however, unlike many other papers that 'lay foundations', it is purely experimental. This is not necessarily a bad thing, but I think the paper has to be presented in the right way in order to have maximum impact.\n\nIn its current form, the paper (understandably) tries to walk the line between the usual format of papers in terms of layout, and the fairly unusual content (no algorithmic, model or theoretical contributions). Given that the authors view this content as perhaps 'more conducive to useful science' than traditional papers, I would encourage the authors to also do this for the tone of the paper. I found the top-level comment motivated the paper better than the introduction, so why not add some of these arguments into the main paper? Meta-comments like \"the point is not the algorithm/model we are using\" are rarely put into papers, but are often invaluable in terms of the reader's understanding. I think if the authors are more ambitious in this respect (given of course that you don't want to completely re-write the paper), I think the paper could benefit from this (although I understand why this wasn't originally done, since some reviewers may object to it).\n\nMost importantly, one of the claims of the authors in the introduction is that: \n\"We believe this work establishes an important foundation\nfor future research in grounded experimentation, intuitive physics and transfer of skills across tasks.\"\nIn my opinion, a crucial element of 'foundation' papers is that they provide clear directions for future research (a framework for 'building the house', following the metaphor). I think this should be more prominently featured in the paper. What should other researchers focus on if they are trying to build agents that can understand physics intuitively (building off this work)? The only part of the paper that can be interpreted in this light is the (very thorough) related work section. But I think it would be invaluable to highlight some of these works in particular as worthy of investigation (in the very near future), perhaps in a 'discussion' section (I realize the paper is long already). \n\nI have updated my score in light of the author's comments, and in anticipation of some of these comments being added to the main paper.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning", "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.  We also compare our learned experimentation policies to randomized baselines and show that the learned policies lead to better predictions.", "pdf": "/pdf/ebbdbda6bc6648f3ddd06a91a4229083302dd917.pdf", "TL;DR": "We train agents to conduct experiments in interactive simulated physical environments.", "paperhash": "denil|learning_to_perform_physics_experiments_via_deep_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Misha Denil", "Pulkit Agrawal", "Tejas D Kulkarni", "Tom Erez", "Peter Battaglia", "Nando de Freitas"], "authorids": ["mdenil@google.com", "pulkitag@berkeley.edu", "tkulkarni@google.com", "etom@google.com", "peterbattaglia@google.com", "nandodefreitas@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287573216, "id": "ICLR.cc/2017/conference/-/paper443/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "r1nTpv9eg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper443/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper443/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper443/reviewers", "ICLR.cc/2017/conference/paper443/areachairs"], "cdate": 1485287573216}}}, {"tddate": null, "tmdate": 1484783857330, "tcdate": 1481892960869, "number": 1, "id": "B1YP4Db4l", "invitation": "ICLR.cc/2017/conference/-/paper443/official/review", "forum": "r1nTpv9eg", "replyto": "r1nTpv9eg", "signatures": ["ICLR.cc/2017/conference/paper443/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper443/AnonReviewer3"], "content": {"title": "mixed opinion", "rating": "7: Good paper, accept", "review": "This paper presents interesting experimental findings that state-of-the-art deep reinforcement learning methods enable agent learning of latent (physical) properties in its environment. The paper formulates the problem of an agent labeling environmental properties after interacting with the environment based on its actions, and applies the deep reinforcement learning model to evaluate whether such learning is possible. The approach jointly learns the convolutional layers for pixel-based perception and its later layers for learning actions based on reinforcement signals.\n\nWe have a mixed opinion about this paper. The paper is written clearly and presents interesting experimental findings. It introduces and formulates a problem potentially important for many robotics applications. Simultaneously, the paper suffers from lacking algorithmic contributions and missing (some of) crucial experiments to confirm its true benefits.\n\nPros:\n\n+ This paper introduces a new problem of learning latent properties in the agent's environment.\n\n+ The paper presents a framework to appropriately combine existing tools to address the formulated problem.\n\n+ The paper tries reinforcement learning with image inputs and fist-like actuator actions. This will lead to its direct application to robots.\n\nCons:\n\n- Lacking algorithmic contribution: this paper applies existing tools/methods to solve the problem rather than developing something new or extending them. The approach essentially is training LSTMs with convolutional layers using the previous Asynchronous Advantage Actor Critic.\n\n- In the Towers experiment, the results of probably the most important setting, \"Fist Pixels\", are missing. This setting receiving pixel inputs and using the Fist actuator in a continuous space is the setting closest to real-world robots, and thus is very important to confirm whether the proposed approach will be directly applicable to real-world robots. However, Figure 5 is missing the results with this setting. Is there any reason behind this?\n\n- The paper lacks its comparison to any baseline methods. Without explicit baselines, it is difficult to see what the agent is really learning and what aspect of the proposed approach is benefitting the task. For instance, in the Towers task, how would an agent randomly pushing/hitting the tower (using 'Fist') a number of times and then passively observing its consequence to produce a label perform compared to this approach? That is, how would an approach with a fixed action policy (but with everything else) perform compared to the full deep reinforcement learning version?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning", "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.  We also compare our learned experimentation policies to randomized baselines and show that the learned policies lead to better predictions.", "pdf": "/pdf/ebbdbda6bc6648f3ddd06a91a4229083302dd917.pdf", "TL;DR": "We train agents to conduct experiments in interactive simulated physical environments.", "paperhash": "denil|learning_to_perform_physics_experiments_via_deep_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Misha Denil", "Pulkit Agrawal", "Tejas D Kulkarni", "Tom Erez", "Peter Battaglia", "Nando de Freitas"], "authorids": ["mdenil@google.com", "pulkitag@berkeley.edu", "tkulkarni@google.com", "etom@google.com", "peterbattaglia@google.com", "nandodefreitas@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483207736423, "id": "ICLR.cc/2017/conference/-/paper443/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper443/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper443/AnonReviewer3", "ICLR.cc/2017/conference/paper443/AnonReviewer6", "ICLR.cc/2017/conference/paper443/AnonReviewer5", "ICLR.cc/2017/conference/paper443/AnonReviewer7"], "reply": {"forum": "r1nTpv9eg", "replyto": "r1nTpv9eg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper443/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper443/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483207736423}}}, {"tddate": null, "tmdate": 1484783835171, "tcdate": 1484783835171, "number": 4, "id": "Sy71WYpUe", "invitation": "ICLR.cc/2017/conference/-/paper443/official/comment", "forum": "r1nTpv9eg", "replyto": "BJS1jjcLx", "signatures": ["ICLR.cc/2017/conference/paper443/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper443/AnonReviewer3"], "content": {"title": "added experimental results", "comment": "The paper has been appropriately revised to address my concerns. With the new experiments, I believe the paper is in a good shape. Will update the rating accordingly."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning", "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.  We also compare our learned experimentation policies to randomized baselines and show that the learned policies lead to better predictions.", "pdf": "/pdf/ebbdbda6bc6648f3ddd06a91a4229083302dd917.pdf", "TL;DR": "We train agents to conduct experiments in interactive simulated physical environments.", "paperhash": "denil|learning_to_perform_physics_experiments_via_deep_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Misha Denil", "Pulkit Agrawal", "Tejas D Kulkarni", "Tom Erez", "Peter Battaglia", "Nando de Freitas"], "authorids": ["mdenil@google.com", "pulkitag@berkeley.edu", "tkulkarni@google.com", "etom@google.com", "peterbattaglia@google.com", "nandodefreitas@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287573216, "id": "ICLR.cc/2017/conference/-/paper443/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "r1nTpv9eg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper443/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper443/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper443/reviewers", "ICLR.cc/2017/conference/paper443/areachairs"], "cdate": 1485287573216}}}, {"tddate": null, "tmdate": 1484783483333, "tcdate": 1481925104312, "number": 2, "id": "rkuxGkM4l", "invitation": "ICLR.cc/2017/conference/-/paper443/official/review", "forum": "r1nTpv9eg", "replyto": "r1nTpv9eg", "signatures": ["ICLR.cc/2017/conference/paper443/AnonReviewer6"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper443/AnonReviewer6"], "content": {"title": "Review", "rating": "7: Good paper, accept", "review": "This paper purports to investigate the ability of RL agents to perform \u2018physics experiments\u2019 in an environment, to infer physical properties about the objects in that environment. The problem is very well motivated; indeed, inferring the physical properties of objects is a crucial skill for intelligent agents, and there has been relatively little work in this direction, particularly in deep RL. The paper is also well-written.\n\nAs there are no architectural or theoretical contributions of the paper (and none are claimed), the main novelty comes in the task application \u2013 using a recurrent A3C model for two tasks that simulate an agent interacting with an environment to infer physical properties of objects. More specifically, two tasks are considered \u2013 moving blocks to determine their mass, and poking towers such that they fall to determine the number of rigid bodies they are composed of. These of course represent a very limited cross-section of the prerequisite abilities for an agent to understand physics. This in itself is not a bad thing, but since there is no comparison of different (simpler) RL agents on the tasks, it is difficult to determine if the tasks selected are challenging. As mentioned in the pre-review question, the \u2018Which is Heavier\u2019 task seems quite easy due to the actuator set-up, and the fact that the model simply must learn to take the difference between successive block positions (which are directly encoded as features in most experiments).  Thus, it is not particularly surprising that the RL agent can solve the proposed tasks. \n\nThe main claim beyond solving two proposed tasks related to physics simulation is that \u201cthe agents learn different strategies for these tasks that balance the cost of gathering information against the cost of making mistakes\u201d. The \u2018cost of gathering information\u2019 is implemented by multiplying the reward with a value of gamma < 1. This is somewhat interesting behaviour, but is hardly surprising given the problem setup.\n\nOne item the authors highlight is that their approach of learning about physical object properties through interaction is different from many previous approaches, which use visual cues. However, the authors also note that this in itself is not novel, and has been explored in other work (e.g. Agrawal et al. (2016)). I think it\u2019s crucial for the authors to discuss these approaches in more detail (potentially along with removing some other, less relevant information from the related work section), and specifically highlight why the proposed tasks in this paper are interesting compared to, for example, learning to move objects towards certain end positions by poking them.\n\nTo discern the level of contribution of the paper, one must ask the following questions: \n\n1)\thow much do these two tasks contribute (above previous work) to the goal of having agents learn the properties of objects by interaction; and\n2)\thow much do the results of the RL agent on these tasks contribute to our understanding of agents that interact with their environment to learn physical properties of objects? \n\nIt is difficult to know exactly, but due to the concerns outlined above, I am not convinced that the answers to (1) or (2) are \u201cto a significant extent\u201d. In particular, for (1), since the proposed agent is able to essentially solve both tasks, it is not clear that the tasks can be used to benchmark more advanced agents (e.g. it can\u2019t be used as a set of bAbI-like tasks). \n\nAnother possible concern, as pointed out by Reviewer 3, is that the description of the model is extremely concise. It would be nice to have, for example, a diagram illustrating the inputs and outputs to the model at each time step, to ease replication.\n\nOverall, it is important to make progress towards agents that can learn to discover physical properties of their environment, and the paper contributes in this direction. However, the technical contributions of this paper are rather limited \u2013 thus, it is not clear to what extent the paper pushes forward research in this direction beyond previous work that is mentioned. It would be nice, for example, to have some discussion about the future of agents that learn physics from interaction (speculation on more difficult versions of the tasks in this paper), and how the proposed approach fits into that picture.  \n\n---------------\nEDIT: score updated, see comments below\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning", "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.  We also compare our learned experimentation policies to randomized baselines and show that the learned policies lead to better predictions.", "pdf": "/pdf/ebbdbda6bc6648f3ddd06a91a4229083302dd917.pdf", "TL;DR": "We train agents to conduct experiments in interactive simulated physical environments.", "paperhash": "denil|learning_to_perform_physics_experiments_via_deep_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Misha Denil", "Pulkit Agrawal", "Tejas D Kulkarni", "Tom Erez", "Peter Battaglia", "Nando de Freitas"], "authorids": ["mdenil@google.com", "pulkitag@berkeley.edu", "tkulkarni@google.com", "etom@google.com", "peterbattaglia@google.com", "nandodefreitas@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483207736423, "id": "ICLR.cc/2017/conference/-/paper443/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper443/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper443/AnonReviewer3", "ICLR.cc/2017/conference/paper443/AnonReviewer6", "ICLR.cc/2017/conference/paper443/AnonReviewer5", "ICLR.cc/2017/conference/paper443/AnonReviewer7"], "reply": {"forum": "r1nTpv9eg", "replyto": "r1nTpv9eg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper443/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper443/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483207736423}}}, {"tddate": null, "tmdate": 1484598112741, "tcdate": 1484598112741, "number": 9, "id": "B1YPso9Ug", "invitation": "ICLR.cc/2017/conference/-/paper443/public/comment", "forum": "r1nTpv9eg", "replyto": "r1nTpv9eg", "signatures": ["~Misha_Denil2"], "readers": ["everyone"], "writers": ["~Misha_Denil2"], "content": {"title": "Why This Paper is Important", "comment": "The machine learning field has profited from hundreds of yearly publications that propose a new algorithm, which outperforms baseline algorithms and has a theoretical proof for some idealized form of the algorithm. This paper, however, is different. It does not propose a new algorithm. It incidentally uses an RL algorithm, but many other RL algorithms could have been used just as well. This paper is not about new models or algorithms.\n\nSo what is this paper about? It is an initial research step toward understanding objects and intuitive reasoning in physical worlds. Why is this important? Our best AI agents currently fail on simple control tasks and simple games, such as Montezuma\u2019s Revenge, because when they look at a screen that has a ladder, a key and a skull they don\u2019t immediately know that keys open doors, that skulls are probably hazardous and best avoided, that ladders allow us to defy gravity, etc. The understanding of physics, relations and objects enables children to solve seemingly simple problems that our best existing AI agents do not come close to begin to solve.   \n\nEndowing our agents with knowledge of objects would help enormously with planning, reasoning and exploration. Yet, this is far from trivial. First, what is an object? It turns out this is not an easy question to answer and has baffled psychologists and philosophers. To simplify matters, in this paper we restrict our attention to rigid bodies. Staring at a thing in one\u2019s hands is not enough to understand what it is. Vision or open loop perception in general is not enough. Trying to pull the thing apart to see whether it is a single entity might help. (Children do indeed love to tear things like paper apart to understand them.) Touching the thing to see what happens also helps --- perhaps it lights up and starts beeping. Further interaction might reveal that it enables us to remotely talk to someone else, to tweet, etc. Much of the knowledge gained is the result of interaction, that is perception by action. \n\nThis paper is also about designing tasks to understand how agents acquire intuitive reasoning and experimentation strategies in physical worlds. An abundant body of evidence in psychology --- see eg the works of Gerd Gigerenzer https://scholar.google.co.uk/citations?user=iw7cepUAAAAJ --- shows that people use intuitive rules to reason about physics, eg a baseball player catching a ball. How do agents acquire such intuition? In this paper, we show for the first time how this is possible with RL and the right type of task.  \n\nFor example, in the proposed Which is Heavier task we show that our agents learn a valuable intuitive reasoning ability. If the agent probes three blocks blocks and notices that one of them remains much closer to the ground than the other two it doesn\u2019t even bother to try the forth block. It simply points at the block that didn\u2019t lift and chooses to terminate. Through experience, the agent has learned a valuable rule of thumb that enables it to solve the task efficiently, terminating at an optimal stopping time. Our experiments show clearly that our agents have learned to act in this reasonable manner.\n\nThis paper shows that model free RL methods can learn how to gather information about physical properties of objects, even when this information is not available to a passive observer, and use this information to make decisions. \n\nPrevious approaches to this problem have relied on either explicit knowledge of the underlying structure of the environment (eg hard-wired physical laws) or on exploiting correlations between material appearance and physical properties, see eg the many convnets for intuitive physics research, Galileo, physics 101, learning to poke by poking, etc. (Incidentally, please note that the main author of the work on learning to poke by poking is also author here.)\n\nOne of the contributions of this paper is to show that our agents can still learn about properties of objects, even when the connection between material appearance and physical properties is broken. This setting allows us to show that our agents are not merely learning that blocks are heavy; they are learning how to check if blocks are heavy.\n\nNone of the previous approaches give a complete account of how agents could come to understand the physical properties of the world around them.  Specifying a model manually is difficult to scale, generalize and to ground in perception.  Making predictions from only visual properties will fail to distinguish between objects that look similar, and certainly it will be insufficient to answer whether a closed thermos is full or empty.  Our approach is a novel contribution to the field of learning intuitive physics that addresses an important gap that had not been filled by previous approaches.  "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning", "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.  We also compare our learned experimentation policies to randomized baselines and show that the learned policies lead to better predictions.", "pdf": "/pdf/ebbdbda6bc6648f3ddd06a91a4229083302dd917.pdf", "TL;DR": "We train agents to conduct experiments in interactive simulated physical environments.", "paperhash": "denil|learning_to_perform_physics_experiments_via_deep_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Misha Denil", "Pulkit Agrawal", "Tejas D Kulkarni", "Tom Erez", "Peter Battaglia", "Nando de Freitas"], "authorids": ["mdenil@google.com", "pulkitag@berkeley.edu", "tkulkarni@google.com", "etom@google.com", "peterbattaglia@google.com", "nandodefreitas@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287573344, "id": "ICLR.cc/2017/conference/-/paper443/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1nTpv9eg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper443/reviewers", "ICLR.cc/2017/conference/paper443/areachairs"], "cdate": 1485287573344}}}, {"tddate": null, "tmdate": 1484598080747, "tcdate": 1484598080747, "number": 8, "id": "S1Kroi5Le", "invitation": "ICLR.cc/2017/conference/-/paper443/public/comment", "forum": "r1nTpv9eg", "replyto": "r1nTpv9eg", "signatures": ["~Misha_Denil2"], "readers": ["everyone"], "writers": ["~Misha_Denil2"], "content": {"title": "Updated Environments", "comment": "We would like to highlight to all reviewers that the experiments have been updated substantially since the previous version of the paper.  In particular, we now include the missing results for Fist Pixels as requested by Reviewer 3, as well as additional experiments comparing our models to random baseline policies in both environments.\n\nIn the process of making these updates we have have made several minor changes to the environments themselves. These changes have made the tower environment harder to solve, but do not alter any conclusions drawn from the experiments."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning", "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.  We also compare our learned experimentation policies to randomized baselines and show that the learned policies lead to better predictions.", "pdf": "/pdf/ebbdbda6bc6648f3ddd06a91a4229083302dd917.pdf", "TL;DR": "We train agents to conduct experiments in interactive simulated physical environments.", "paperhash": "denil|learning_to_perform_physics_experiments_via_deep_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Misha Denil", "Pulkit Agrawal", "Tejas D Kulkarni", "Tom Erez", "Peter Battaglia", "Nando de Freitas"], "authorids": ["mdenil@google.com", "pulkitag@berkeley.edu", "tkulkarni@google.com", "etom@google.com", "peterbattaglia@google.com", "nandodefreitas@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287573344, "id": "ICLR.cc/2017/conference/-/paper443/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1nTpv9eg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper443/reviewers", "ICLR.cc/2017/conference/paper443/areachairs"], "cdate": 1485287573344}}}, {"tddate": null, "tmdate": 1484597981384, "tcdate": 1484597981384, "number": 7, "id": "BJS1jjcLx", "invitation": "ICLR.cc/2017/conference/-/paper443/public/comment", "forum": "r1nTpv9eg", "replyto": "B1YP4Db4l", "signatures": ["~Misha_Denil2"], "readers": ["everyone"], "writers": ["~Misha_Denil2"], "content": {"title": "Thank you for your feedback. ", "comment": "Thank you for your balanced feedback. \n\nIn addition to the specific issues addressed below, please also see our general reply which addresses the importance and positioning of this paper in spite of not not offering an algorithmic contribution.\n\nTo address your concern over the lack of the Fist Pixels setting for the Towers environment, we have conducted the experiment you requested. We have added the corresponding plots to the paper which show that the agents are also able to learn in this setting.  We have also modified the \"Waiting for information\" experiment to use the Fist Pixels agent (replacing the Fist Features agent we used for this experiment in the original version of the paper). Thank you for this suggestion to improve the paper.\n\nYour suggestion to compare against baselines that use a fixed (randomized) policy is a good one.  We have added experiments on both environments comparing the performance of our agents using learned vs randomized interaction policies (see the updated paper for full details).  These experiments show that when using the learned interaction policies agents are more accurate and often take less time to produce correct outputs as compared to randomized interactions.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning", "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.  We also compare our learned experimentation policies to randomized baselines and show that the learned policies lead to better predictions.", "pdf": "/pdf/ebbdbda6bc6648f3ddd06a91a4229083302dd917.pdf", "TL;DR": "We train agents to conduct experiments in interactive simulated physical environments.", "paperhash": "denil|learning_to_perform_physics_experiments_via_deep_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Misha Denil", "Pulkit Agrawal", "Tejas D Kulkarni", "Tom Erez", "Peter Battaglia", "Nando de Freitas"], "authorids": ["mdenil@google.com", "pulkitag@berkeley.edu", "tkulkarni@google.com", "etom@google.com", "peterbattaglia@google.com", "nandodefreitas@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287573344, "id": "ICLR.cc/2017/conference/-/paper443/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1nTpv9eg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper443/reviewers", "ICLR.cc/2017/conference/paper443/areachairs"], "cdate": 1485287573344}}}, {"tddate": null, "tmdate": 1484597943158, "tcdate": 1484597943158, "number": 6, "id": "ryJacsq8l", "invitation": "ICLR.cc/2017/conference/-/paper443/public/comment", "forum": "r1nTpv9eg", "replyto": "rJOpuGQVl", "signatures": ["~Misha_Denil2"], "readers": ["everyone"], "writers": ["~Misha_Denil2"], "content": {"title": "Thank you for your careful feedback.", "comment": "Thank you for your careful feedback.\n\nI think there is some confusion in comparing this paper to Agrawal et al. (2016), since the approach taken here is very different.  In that paper the model being learned is a model of the environment dynamics from vision.  Their model must learn to map static visual observations to dynamic properties of the objects by relying on correlations between appearance and material properties.  In this work we have carefully constructed our environments to eliminate these correlations between static appearance and physical properties, and because of this the model of Agrawal et al. (2016) would be unable to complete our tasks.\n\nThe relationship between appearance and physical properties is quite rich, but relying on vision alone gives an incomplete physical story. For example, the approach of Agrawal et al. (2016) would have trouble distinguishing between a full thermos and an empty thermos, or between a bag full of rocks and a bag full of tennis balls.\n\nThere are two different questions at play here:\n1. Given the appearance of an object, how will it move when I poke it?\n2. If I want to know how heavy an object is (or if it comes apart, etc), how should I poke it? \n\nBoth of these are questions are important. The work of Agrawal et al. (2016) targets the first question, whereas this work is focused on the second.\n\nThe purpose of analyzing the which is heavier experiments as a function of the mass gap is to show that the solutions found by the agents behave similarly to what we would expect from solving the underlying structured problem directly.  Indeed it does not seem important to have agents make fine distinctions between extremely similar masses, which is why we did not focus on ensuring that our agents obtain perfect solutions to the more difficult settings for this task.\n\nHowever, probing this axis of the problem allows us to better understand the solutions we obtain.  In particular, the fact that episode length is correlated with instance level difficulty shows that our agents learn an adaptive strategy for gathering information, that behaves as though the agents are reasoning using an implicitly learned model.  The fact that this effect persists at the instance level suggests the agents have learned more than simply to identify the generating distribution of problems.\n\nWe did not compare to other RL baselines because the choice of RL algorithm is not central to the paper. We chose to use A3C because we have an implementation of this algorithm that is robust and scalable, but we have no reason to think that there is anything unique about A3C that would preclude other algorithms from obtaining similar results, and introducing a comparison between different learning algorithms here would only distract from the message of the paper.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning", "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.  We also compare our learned experimentation policies to randomized baselines and show that the learned policies lead to better predictions.", "pdf": "/pdf/ebbdbda6bc6648f3ddd06a91a4229083302dd917.pdf", "TL;DR": "We train agents to conduct experiments in interactive simulated physical environments.", "paperhash": "denil|learning_to_perform_physics_experiments_via_deep_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Misha Denil", "Pulkit Agrawal", "Tejas D Kulkarni", "Tom Erez", "Peter Battaglia", "Nando de Freitas"], "authorids": ["mdenil@google.com", "pulkitag@berkeley.edu", "tkulkarni@google.com", "etom@google.com", "peterbattaglia@google.com", "nandodefreitas@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287573344, "id": "ICLR.cc/2017/conference/-/paper443/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1nTpv9eg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper443/reviewers", "ICLR.cc/2017/conference/paper443/areachairs"], "cdate": 1485287573344}}}, {"tddate": null, "tmdate": 1484597892521, "tcdate": 1484597892521, "number": 5, "id": "Sy3KcocUx", "invitation": "ICLR.cc/2017/conference/-/paper443/public/comment", "forum": "r1nTpv9eg", "replyto": "H1_VdJf4g", "signatures": ["~Misha_Denil2"], "readers": ["everyone"], "writers": ["~Misha_Denil2"], "content": {"title": "Thank you for your suggestions to improve the precision of the paper.", "comment": "Thank you for your suggestions to improve the precision of the paper.  In particular, our intent when describing the the Which is Heavier environment as a \"latent bandit\" seems to have caused quite a bit of confusion and this is connected to our imprecise formulation of our \"question answering\" framework which you identified earlier.  We will attempt to make these ideas more precise as we continue to update the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning", "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.  We also compare our learned experimentation policies to randomized baselines and show that the learned policies lead to better predictions.", "pdf": "/pdf/ebbdbda6bc6648f3ddd06a91a4229083302dd917.pdf", "TL;DR": "We train agents to conduct experiments in interactive simulated physical environments.", "paperhash": "denil|learning_to_perform_physics_experiments_via_deep_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Misha Denil", "Pulkit Agrawal", "Tejas D Kulkarni", "Tom Erez", "Peter Battaglia", "Nando de Freitas"], "authorids": ["mdenil@google.com", "pulkitag@berkeley.edu", "tkulkarni@google.com", "etom@google.com", "peterbattaglia@google.com", "nandodefreitas@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287573344, "id": "ICLR.cc/2017/conference/-/paper443/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1nTpv9eg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper443/reviewers", "ICLR.cc/2017/conference/paper443/areachairs"], "cdate": 1485287573344}}}, {"tddate": null, "tmdate": 1484597833208, "tcdate": 1484597833208, "number": 4, "id": "S1-89s5Ll", "invitation": "ICLR.cc/2017/conference/-/paper443/public/comment", "forum": "r1nTpv9eg", "replyto": "SJgSV_rBg", "signatures": ["~Misha_Denil2"], "readers": ["everyone"], "writers": ["~Misha_Denil2"], "content": {"title": "Review reply", "comment": "Thank you for your feedback.\n\nWith regards to the desire for more analysis of the learned representations, we have added some experiments comparing the learned policies to a randomized baselines.  These experiments show that when using the learned interaction policies agents are more accurate and often take less time to produce correct outputs as compared to randomized interactions.\n\nWe would also draw your attention, particularly for the which is heavier environment, to the experiments where we examine episode length as a function of difficulty.  These are intended to show that the learned representations include information both about the prior distribution of masses (the population level experiment) as well as information the agent's state of knowledge about the mass gap for individual instances.  We assess this by measuring the behavior of the agent, but the behavior is wholly determined by the representations.\n\n> \"why does one \"must\" interact with objects in order to learn about the properties? Can't we also learn through observation?\"\n\nThis is intended as a descriptive statement about our particular environments, not as a principle of how learning must happen in a general setting.  Our environments are designed so that mere passive observation is not sufficient to perform better than chance, and therefore agents \"must\" interact with the environment in order to learn.  In real world settings appearance will often provide strong clues as to physical properties, but for this paper we have deliberately removed such clues from our environments.\n\n> I am a bit confused why having a small mass gap makes the task harder (unless it's really close to 0). Shouldn't a machine be possible to distinguish even a pixel difference of speed? If not, isn't this just because of the network architecture?\n\n\nThe mass gap is frequently quite small in the more difficult settings (see Figure 1 (right)), when this is not the case the task is reliably solved perfectly. It is possible a different architecture or more training would allow the agents to achieve perfect performance on the more difficult settings as well; however, as we argue in our reply to Reviewer 6, achieving perfect performance on this task is not really the point, and the value of doing so is questionable.\n\n> \"For example, how does an agent really know when the instance is any more difficult? Doesn't this really depend on the empirically learned distribution of training samples (i.e. P(m_3 | m_1, m_2), where m_i indicates masses of object 1, 2, and 3)?\"\n\nThe agent is not told explicitly about the difficulty of any instance. It must come to know which instances are difficult through interaction, and the purpose of the experiments on the which is heavier environment are intended to show that the agent does in fact come to know this.  The reason for including both population and instance level experiments is to show that what is learned goes beyond simply the conditional distributions of masses: in the instance level experiments the conditional distributions you mention are fixed and we still see behavior that adapts to the difficulty of the individual instances.\n\n> Any baseline approach?\n\nWe have added baseline comparisons for both environments.  Please see the updated version of the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning", "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.  We also compare our learned experimentation policies to randomized baselines and show that the learned policies lead to better predictions.", "pdf": "/pdf/ebbdbda6bc6648f3ddd06a91a4229083302dd917.pdf", "TL;DR": "We train agents to conduct experiments in interactive simulated physical environments.", "paperhash": "denil|learning_to_perform_physics_experiments_via_deep_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Misha Denil", "Pulkit Agrawal", "Tejas D Kulkarni", "Tom Erez", "Peter Battaglia", "Nando de Freitas"], "authorids": ["mdenil@google.com", "pulkitag@berkeley.edu", "tkulkarni@google.com", "etom@google.com", "peterbattaglia@google.com", "nandodefreitas@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287573344, "id": "ICLR.cc/2017/conference/-/paper443/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1nTpv9eg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper443/reviewers", "ICLR.cc/2017/conference/paper443/areachairs"], "cdate": 1485287573344}}}, {"tddate": null, "tmdate": 1483207735675, "tcdate": 1483207735675, "number": 4, "id": "SJgSV_rBg", "invitation": "ICLR.cc/2017/conference/-/paper443/official/review", "forum": "r1nTpv9eg", "replyto": "r1nTpv9eg", "signatures": ["ICLR.cc/2017/conference/paper443/AnonReviewer7"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper443/AnonReviewer7"], "content": {"title": "Interesting ideas while lack of the details", "rating": "6: Marginally above acceptance threshold", "review": "This paper addresses the question of how to utilize physical interactions to answer questions about physical outcomes. This question falls into a popular stream in ML community -- understanding physics. The paper moved a step further and worked on experimental setups where there is no prior about the physical properties/rules and it uses a deep reinforcement learning (DRL) technique to address the problem. My overall opinion about this paper is: an interesting attempt and idea, yet without a clear contribution.\n\nThe experimental setups are quite interesting. The goal is to figure out which blocks are heavier or which blocks are glued together -- only by pushing and pulling objects around without any prior. The paper also shows reasonable performances on each task with detailed scenarios.\n\nWhile these experiments and results are interesting, the contribution is unclear. My main question is: does this result bring us any new insight? While the scenarios are interesting and focused on physical experiments, this is not any more different (potentially easier) than learning from playing games (e.g. Atari). In other words, are the tasks really different from other typical popular DRL tasks? To this end, I would have been more excited if authors showed some more new insights or experiments on learned representations and etc. Currently, the paper only discusses the factual outcome. For example, it describes the experimental setup and how much performances an agent could achieve. The authors could probably dissect the learned representations further, or discuss how the experimental results are linked to the human behavior or physical properties/laws.\n\nI am very in-between for my overall rating. I think the paper could have a deeper analysis. I however recommend the acceptance because of its merit of the idea.\n\n\n\nThe followings are some detailed questions (not directly impacting my overall rating):\n(1) Page 2 \"we assume that the agent has no prior knowledge about the physical properties of objects, or the laws of physics, and hence must interact with the objects in order to learn to answer questions about these properties.\": why does one \"must\" interact with objects in order to learn about the properties? Can't we also learn through observation?\n\n(2) Figure 1right is missing a Y-axis label.\n\n(3) Page 3: A relating to bandit is interesting, but the formal approach is all based on DRL.\n\n(4) Page 5 \"which makes distinguishing between the two heaviest blocks very difficult\": I am a bit confused why having a small mass gap makes the task harder (unless it's really close to 0). Shouldn't a machine be possible to distinguish even a pixel difference of speed? If not, isn't this just because of the network architecture?\n\n(5) Page 5 \"Since the agents exhibit similar performance using pixels and features we conduct the remaining experiments in this section using feature observations, since these agents are substantially faster to train.\": How about at least showing a correlation of performances at the instance level (rather than average performances)? Even so, I think this is a bit of big conclusion.\n\n(6) Throughout the papers, I felt that many conclusions (e.g. difficulty and etc) are based on a particularly chosen training distribution. For example, how does an agent really know when the instance is any more difficult? Doesn't this really depend on the empirically learned distribution of training samples (i.e. P(m_3 | m_1, m_2), where m_i indicates masses of object 1, 2, and 3)? In other words, does what's hard/easy matter much unless this is more thoroughly tested over various types of distributions?\n\n(7) Any baseline approach?", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning", "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.  We also compare our learned experimentation policies to randomized baselines and show that the learned policies lead to better predictions.", "pdf": "/pdf/ebbdbda6bc6648f3ddd06a91a4229083302dd917.pdf", "TL;DR": "We train agents to conduct experiments in interactive simulated physical environments.", "paperhash": "denil|learning_to_perform_physics_experiments_via_deep_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Misha Denil", "Pulkit Agrawal", "Tejas D Kulkarni", "Tom Erez", "Peter Battaglia", "Nando de Freitas"], "authorids": ["mdenil@google.com", "pulkitag@berkeley.edu", "tkulkarni@google.com", "etom@google.com", "peterbattaglia@google.com", "nandodefreitas@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483207736423, "id": "ICLR.cc/2017/conference/-/paper443/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper443/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper443/AnonReviewer3", "ICLR.cc/2017/conference/paper443/AnonReviewer6", "ICLR.cc/2017/conference/paper443/AnonReviewer5", "ICLR.cc/2017/conference/paper443/AnonReviewer7"], "reply": {"forum": "r1nTpv9eg", "replyto": "r1nTpv9eg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper443/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper443/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483207736423}}}, {"tddate": null, "tmdate": 1482004671696, "tcdate": 1482004671696, "number": 2, "id": "rJOpuGQVl", "invitation": "ICLR.cc/2017/conference/-/paper443/official/comment", "forum": "r1nTpv9eg", "replyto": "rkuxGkM4l", "signatures": ["ICLR.cc/2017/conference/paper443/AnonReviewer6"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper443/AnonReviewer6"], "content": {"title": "Review clarification", "comment": "I should clarify - I would be willing to modify the review if the authors address the following issues:\n\n1) More thorough comparison to the approaches taken in other papers for agents learning physical properties of their environment (e.g. Agrawal et al. (2016) and others). Why is the approach in this paper measuring something different (in an interesting way) than previous approaches? The current related work section has significant breadth, but needs more depth in this area.\n2) Similarly, the authors argue that the 'Which is Heavier' task becomes more challenging as the mass gap decreases. If the authors want to argue that this is an interesting axis of difficulty, they should explain why in more detail. For example, it is unlikely that humans would be able to tell two blocks apart if their mass gap was extremely small -- why is this necessary for artificial agents? Shouldn't we focus on one of the many other areas of physics understanding where humans outperform RL agents? More discussion of the general direction of physics-learning agents (and how this paper fits in) would greatly benefit the paper.\n3) Comparison of some simpler RL baselines on the tasks considered."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning", "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.  We also compare our learned experimentation policies to randomized baselines and show that the learned policies lead to better predictions.", "pdf": "/pdf/ebbdbda6bc6648f3ddd06a91a4229083302dd917.pdf", "TL;DR": "We train agents to conduct experiments in interactive simulated physical environments.", "paperhash": "denil|learning_to_perform_physics_experiments_via_deep_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Misha Denil", "Pulkit Agrawal", "Tejas D Kulkarni", "Tom Erez", "Peter Battaglia", "Nando de Freitas"], "authorids": ["mdenil@google.com", "pulkitag@berkeley.edu", "tkulkarni@google.com", "etom@google.com", "peterbattaglia@google.com", "nandodefreitas@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287573216, "id": "ICLR.cc/2017/conference/-/paper443/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "r1nTpv9eg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper443/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper443/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper443/reviewers", "ICLR.cc/2017/conference/paper443/areachairs"], "cdate": 1485287573216}}}, {"tddate": null, "tmdate": 1481926807475, "tcdate": 1481926703999, "number": 3, "id": "H1_VdJf4g", "invitation": "ICLR.cc/2017/conference/-/paper443/official/review", "forum": "r1nTpv9eg", "replyto": "r1nTpv9eg", "signatures": ["ICLR.cc/2017/conference/paper443/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper443/AnonReviewer5"], "content": {"title": "Conceptually interesting and valuable, but lacks commitment to precise definitions", "rating": "7: Good paper, accept", "review": "This paper investigates the question of gathering information (answering question)\nthrough direct interaction with the environment. In that sense, it is closely\nrelated to \"active learning\" in supervised learning, or to the fundamental\nproblem of exploration-exploitation in RL. The authors consider a specific \ninstance of this problem in a physics domain and learn\ninformation-seeking policies using recent deep RL methods.\n\nThe paper is mostly empirical and explores the effect of changing the\ncost of information (via the discount factor) on the structure of the learned\npolicies. It also shows that general-purpose deep policy gradient methods are\nsufficient powerful to learn such tasks. The proposed environment is, to my knowledge,\nnovel as well the task formulation in section 2. (And it would be very valuable to the\nthe community if the environment would be open-sourced)\n\nThe expression \"latent structure/dynamics\" is used throughout the text and the connection\nwith bandits is mentioned in section 4. It therefore seems that authors aspire\nfor more generality with their approach but the paper doesn't quite fully ground\nthe proposed approach formally in any existing framework nor does it provide a\nnew one completely.\n\nFor example: how does your approach formalize the concept of \"questions\" and \"answers\" ?\nWhat makes a question \"difficult\" ? How do you quantify \"difficulty\" ?\nHow do you define the \"cost of information\"? What are its units (bits, scalar reward), its semantics ?\nDo you you have an MDP or a POMDP ? What kind of MDP do you consider ?\nHow do you define your discounted MDP ? What is the state and action spaces ?\nSome important problem structure under the \"interaction/labeling/reward\"\nparagraph of section 2 would be worth expressing directly in your definition\nof the MDP: labeling actions can only occur during the \"labeling phase\" and that the transition\nand reward functions have a specific structure (positive/negative, lead to absorbing state).\nThe notion of \"phase\" could perhaps be implemented by considering an augmented state space : $\\tilde s = (s, phase)$\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning", "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.  We also compare our learned experimentation policies to randomized baselines and show that the learned policies lead to better predictions.", "pdf": "/pdf/ebbdbda6bc6648f3ddd06a91a4229083302dd917.pdf", "TL;DR": "We train agents to conduct experiments in interactive simulated physical environments.", "paperhash": "denil|learning_to_perform_physics_experiments_via_deep_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Misha Denil", "Pulkit Agrawal", "Tejas D Kulkarni", "Tom Erez", "Peter Battaglia", "Nando de Freitas"], "authorids": ["mdenil@google.com", "pulkitag@berkeley.edu", "tkulkarni@google.com", "etom@google.com", "peterbattaglia@google.com", "nandodefreitas@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483207736423, "id": "ICLR.cc/2017/conference/-/paper443/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper443/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper443/AnonReviewer3", "ICLR.cc/2017/conference/paper443/AnonReviewer6", "ICLR.cc/2017/conference/paper443/AnonReviewer5", "ICLR.cc/2017/conference/paper443/AnonReviewer7"], "reply": {"forum": "r1nTpv9eg", "replyto": "r1nTpv9eg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper443/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper443/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483207736423}}}, {"tddate": null, "tmdate": 1481808967247, "tcdate": 1481808967237, "number": 3, "id": "HykI2MeNe", "invitation": "ICLR.cc/2017/conference/-/paper443/public/comment", "forum": "r1nTpv9eg", "replyto": "SksLfo17x", "signatures": ["~Misha_Denil2"], "readers": ["everyone"], "writers": ["~Misha_Denil2"], "content": {"title": "reply", "comment": "1.1.\n\nThere are many approaches to analyzing how agents do and should make tradeoffs between acting in ignorance and the cost of acquiring more information, as you note.  In this paper the tradeoff that needs to be made is deliberately very simple.  The agent can either pay a fixed cost for gathering more information or can commit to a choice given their current knowledge.  \n\nHowever, much of the power of these approaches comes from giving a framework for balancing between several options with different costs.  For example, when the cost of actions are fixed the \"expected value of control\" framework reduces to simply linearly shifting the reward by the (fixed) cost of action.\n\nI think a more detailed development of the connection of cost of information to these other frameworks is best left for future work where we require agents to make more sophisticated information tradeoffs, where the tools they offer will have more power.\n\n1.2.\n\nThank you for pointing out the connection to Sutton's work. I agree this connection is one we should make explicit in the paper.  In Sutton's language our Section 2 might be better titled \"Encouraging Agents to Ask Questions\", and what we call the \"implicit question posed by the environment\" would be better called \"the question the environment encourages the agent to ask\".\n\nWe will add some discussion about how our setup is related to Sutton's framework to the final version of the paper.\n\n2.\n\nThe equivalence of which is heavier to a bandit problem is at the \"question\" level  (in our sense, not in Sutton's sense), rather than the behavioral level.  The underlying problem is a bandit in the sense that the return (mass) of each arm (block) does not change as a function of actions taken by the agent.  The instantiation of this problem in a physical environment (the physical lens) means that the agent needs to deal with the environment as an MDP, but the underlying structure of the problem to be solved is simpler.\n\n3. \n\nThe agent controls the length of the episode by choosing when to produce a label.  Taking a labeling action always immediately ends the episode.\n\nIf we interpret the discount factor as a probability of termination then it effectively controls the cost of information by forcing the agent to behave as if the episode might terminate prematurely at any step even though this event cannot actually occur.  I agree that the semantics of this seem sort of strange, but stochastic termination is not the only way to interpret the discount factor.  We can also interpret the discount factor deterministically as explicitly de-valuing rewards that take longer to obtain, and the semantics of this interpretation are less strange in our setting.\n\nThe cost of information must be extrinsic.  It can be imposed explicitly (as we do) or implicitly (through metabolic costs, time sensitive decisions, etc), but the only way for information to have a cost is for the cost to be imposed through properties of the environment."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning", "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.  We also compare our learned experimentation policies to randomized baselines and show that the learned policies lead to better predictions.", "pdf": "/pdf/ebbdbda6bc6648f3ddd06a91a4229083302dd917.pdf", "TL;DR": "We train agents to conduct experiments in interactive simulated physical environments.", "paperhash": "denil|learning_to_perform_physics_experiments_via_deep_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Misha Denil", "Pulkit Agrawal", "Tejas D Kulkarni", "Tom Erez", "Peter Battaglia", "Nando de Freitas"], "authorids": ["mdenil@google.com", "pulkitag@berkeley.edu", "tkulkarni@google.com", "etom@google.com", "peterbattaglia@google.com", "nandodefreitas@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287573344, "id": "ICLR.cc/2017/conference/-/paper443/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1nTpv9eg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper443/reviewers", "ICLR.cc/2017/conference/paper443/areachairs"], "cdate": 1485287573344}}}, {"tddate": null, "tmdate": 1481808711616, "tcdate": 1481808711611, "number": 2, "id": "HJgUsflNx", "invitation": "ICLR.cc/2017/conference/-/paper443/public/comment", "forum": "r1nTpv9eg", "replyto": "SklkQz1mg", "signatures": ["~Misha_Denil2"], "readers": ["everyone"], "writers": ["~Misha_Denil2"], "content": {"title": "which is heavier difficulty", "comment": "We have trained agents that include no-op actions in the \"direct\" action space (although they are not discussed in the paper)  We have also trained agents that use a completely different \"point and throw\" action space where there are (effectively) no-op actions available.  In both cases this does not affect the ability of agents to solve the task.\n\nThere are a few confounding factors that I think contribute to the no-op actions not being helpful:\n\nThere is always time pressure from the discount factor and waiting for a block to settle after poking it takes several frames.\nThe fact that small distinctions in mass are relevant is not explicitly made available to the learning algorithm, so it needs to learn to make these distinctions.  This can manifest as a capacity issue (learning to make more fine distinctions requires more representational power) or a learning issue (discovering that fine distinctions are relevant is hard).\n\nIt is true that in principle this task is perfectly solvable by measuring small differences in displacement of the blocks after poking them, but in practice these differences can be quite small in episodes where the mass gap is small. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning", "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.  We also compare our learned experimentation policies to randomized baselines and show that the learned policies lead to better predictions.", "pdf": "/pdf/ebbdbda6bc6648f3ddd06a91a4229083302dd917.pdf", "TL;DR": "We train agents to conduct experiments in interactive simulated physical environments.", "paperhash": "denil|learning_to_perform_physics_experiments_via_deep_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Misha Denil", "Pulkit Agrawal", "Tejas D Kulkarni", "Tom Erez", "Peter Battaglia", "Nando de Freitas"], "authorids": ["mdenil@google.com", "pulkitag@berkeley.edu", "tkulkarni@google.com", "etom@google.com", "peterbattaglia@google.com", "nandodefreitas@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287573344, "id": "ICLR.cc/2017/conference/-/paper443/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1nTpv9eg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper443/reviewers", "ICLR.cc/2017/conference/paper443/areachairs"], "cdate": 1485287573344}}}, {"tddate": null, "tmdate": 1481808651620, "tcdate": 1481808651615, "number": 1, "id": "HJEMizeNl", "invitation": "ICLR.cc/2017/conference/-/paper443/public/comment", "forum": "r1nTpv9eg", "replyto": "SySZztkQg", "signatures": ["~Misha_Denil2"], "readers": ["everyone"], "writers": ["~Misha_Denil2"], "content": {"title": ".", "comment": "The contributions of this paper are to show\n\nHow to construct environments that ask questions about latent physical properties by constructing environments with latent structure\nThat RL agents can perform well on these tasks without explicit access to this latent structure\nHow to analyze the behavior of these agents by relating their behavior to behaviors we would expect from solutions that exploit the latent structure\nThat RL agents exhibit these properties\n\nWe can add more pictures of two environments to the paper to visualize inputs, and we can also make some videos available that show trained agents behaving in the environments.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning", "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.  We also compare our learned experimentation policies to randomized baselines and show that the learned policies lead to better predictions.", "pdf": "/pdf/ebbdbda6bc6648f3ddd06a91a4229083302dd917.pdf", "TL;DR": "We train agents to conduct experiments in interactive simulated physical environments.", "paperhash": "denil|learning_to_perform_physics_experiments_via_deep_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Misha Denil", "Pulkit Agrawal", "Tejas D Kulkarni", "Tom Erez", "Peter Battaglia", "Nando de Freitas"], "authorids": ["mdenil@google.com", "pulkitag@berkeley.edu", "tkulkarni@google.com", "etom@google.com", "peterbattaglia@google.com", "nandodefreitas@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287573344, "id": "ICLR.cc/2017/conference/-/paper443/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1nTpv9eg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper443/reviewers", "ICLR.cc/2017/conference/paper443/areachairs"], "cdate": 1485287573344}}}, {"tddate": null, "tmdate": 1480729255601, "tcdate": 1480729171278, "number": 3, "id": "SksLfo17x", "invitation": "ICLR.cc/2017/conference/-/paper443/pre-review/question", "forum": "r1nTpv9eg", "replyto": "r1nTpv9eg", "signatures": ["ICLR.cc/2017/conference/paper443/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper443/AnonReviewer5"], "content": {"title": "Related work, bandit, discount factor", "question": "1. [Related work] I would appreciate if you could comment on the following :\n\n1.1 I think that there is a great opportunity to develop the connection of the \"cost of information\" with ideas from Bounded Rationality and the \"expected value of control\" (Botvinick).  There is also a great deal of related work on the value of information in economics and OR (Howard, Delage, etc). \n\n1.2 The partition between question/answer and its role in representation learning is an important concept of Sutton's \"Experience-oriented\" perspective on AI. This view underlies early work on PSRs, TD networks and more recently GVFs. \n\n2. [Bandit formulation] Analyzing your problem under the lens of Bandit theory could be relevant, but I think that the equivalence to a \"latent bandit\" that you put forward in section 4.1 seems incorrect. The way that the problem is posed from the very beginning is under the RL setting, thus assuming an MDP. The use of deep learning is to make state, and we're therefore not in the bandit setting. \n\n3. [Discount factor] The way that you use the discount factor is really much like a regularizer for the \"cost of information\". Since the discounted setting in MDPs is equivalent to a finite horizon MDP with random termination, I can see why this can encourage the agent to \"answer as quickly as possible\". However, I have the feeling that this termination event should be explicit, and not implicit through the discount factor. In other words: the agent should have direct control over this termination decision. In your setting, it seems that the cost of information is purely \"extrinsic\". What do you think is the right balance between intrinsic vs extrinsic ? Is it purely environmental or self-motivated ? \n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning", "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.  We also compare our learned experimentation policies to randomized baselines and show that the learned policies lead to better predictions.", "pdf": "/pdf/ebbdbda6bc6648f3ddd06a91a4229083302dd917.pdf", "TL;DR": "We train agents to conduct experiments in interactive simulated physical environments.", "paperhash": "denil|learning_to_perform_physics_experiments_via_deep_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Misha Denil", "Pulkit Agrawal", "Tejas D Kulkarni", "Tom Erez", "Peter Battaglia", "Nando de Freitas"], "authorids": ["mdenil@google.com", "pulkitag@berkeley.edu", "tkulkarni@google.com", "etom@google.com", "peterbattaglia@google.com", "nandodefreitas@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959276920, "id": "ICLR.cc/2017/conference/-/paper443/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper443/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper443/AnonReviewer6", "ICLR.cc/2017/conference/paper443/AnonReviewer3", "ICLR.cc/2017/conference/paper443/AnonReviewer5"], "reply": {"forum": "r1nTpv9eg", "replyto": "r1nTpv9eg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper443/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper443/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959276920}}}, {"tddate": null, "tmdate": 1480720892567, "tcdate": 1480720892559, "number": 2, "id": "SySZztkQg", "invitation": "ICLR.cc/2017/conference/-/paper443/pre-review/question", "forum": "r1nTpv9eg", "replyto": "r1nTpv9eg", "signatures": ["ICLR.cc/2017/conference/paper443/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper443/AnonReviewer3"], "content": {"title": "clarification of technical contributions", "question": "It will be great if the authors can clarify the technical contributions of the paper. Is the main contribution of the paper on applying \"LSTMs with convolutional layers using Asynchronous Advantage Actor Critic\" for the problem of physical property inference?\n\nAlso, having a figure visualizing exact inputs and outputs of LSTMs and how reinforcement signals influence them might help the understanding.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning", "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.  We also compare our learned experimentation policies to randomized baselines and show that the learned policies lead to better predictions.", "pdf": "/pdf/ebbdbda6bc6648f3ddd06a91a4229083302dd917.pdf", "TL;DR": "We train agents to conduct experiments in interactive simulated physical environments.", "paperhash": "denil|learning_to_perform_physics_experiments_via_deep_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Misha Denil", "Pulkit Agrawal", "Tejas D Kulkarni", "Tom Erez", "Peter Battaglia", "Nando de Freitas"], "authorids": ["mdenil@google.com", "pulkitag@berkeley.edu", "tkulkarni@google.com", "etom@google.com", "peterbattaglia@google.com", "nandodefreitas@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959276920, "id": "ICLR.cc/2017/conference/-/paper443/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper443/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper443/AnonReviewer6", "ICLR.cc/2017/conference/paper443/AnonReviewer3", "ICLR.cc/2017/conference/paper443/AnonReviewer5"], "reply": {"forum": "r1nTpv9eg", "replyto": "r1nTpv9eg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper443/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper443/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959276920}}}, {"tddate": null, "tmdate": 1480693599541, "tcdate": 1480692439768, "number": 1, "id": "SklkQz1mg", "invitation": "ICLR.cc/2017/conference/-/paper443/pre-review/question", "forum": "r1nTpv9eg", "replyto": "r1nTpv9eg", "signatures": ["ICLR.cc/2017/conference/paper443/AnonReviewer6"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper443/AnonReviewer6"], "content": {"title": "which is heavier experiment", "question": "I'm a bit confused as to why the agent is not able to obtain a perfect score in the 'which is heavier' experiment, when it is using the z coordinate features of each of the 4 blocks. It seems like the best strategy is simply to move each of the objects once and observe their maximum z-value, and report the smallest displacement as the heaviest block; I would have thought this would be learnable with the A3C architecture (even for low values of Beta). Is there stochasticity in the agent's force or in the z displacement? If not, once this rule is learned the game plays out similarly to a deterministic bandit, which is easily solvable. Is there something I'm missing, or am I underestimating the difficulty of the task? \n\nThanks!\n\nEDIT:\nAfter re-reading the section, it seems like simply moving a block and then waiting to observe its trajectory is harder than I guessed since there is no 'no-op' action (i.e. the agent must move a block at every time step). Could you provide your justification for requiring the agent to move a block at each time step? To me, having a 'no-op' action seems more natural as young humans (which you mention in order to motivate your paper) will often move things in their environment and observe the effects. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning", "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.  We also compare our learned experimentation policies to randomized baselines and show that the learned policies lead to better predictions.", "pdf": "/pdf/ebbdbda6bc6648f3ddd06a91a4229083302dd917.pdf", "TL;DR": "We train agents to conduct experiments in interactive simulated physical environments.", "paperhash": "denil|learning_to_perform_physics_experiments_via_deep_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Misha Denil", "Pulkit Agrawal", "Tejas D Kulkarni", "Tom Erez", "Peter Battaglia", "Nando de Freitas"], "authorids": ["mdenil@google.com", "pulkitag@berkeley.edu", "tkulkarni@google.com", "etom@google.com", "peterbattaglia@google.com", "nandodefreitas@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959276920, "id": "ICLR.cc/2017/conference/-/paper443/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper443/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper443/AnonReviewer6", "ICLR.cc/2017/conference/paper443/AnonReviewer3", "ICLR.cc/2017/conference/paper443/AnonReviewer5"], "reply": {"forum": "r1nTpv9eg", "replyto": "r1nTpv9eg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper443/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper443/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959276920}}}], "count": 21}