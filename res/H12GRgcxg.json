{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488024926623, "tcdate": 1478262292517, "number": 166, "id": "H12GRgcxg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "H12GRgcxg", "signatures": ["~Jacob_Goldberger1"], "readers": ["everyone"], "content": {"title": "Training deep neural-networks using a noise adaptation layer", "abstract": "The availability of large datsets has enabled neural networks to achieve impressive recognition results. However, the presence of inaccurate class labels is known  to deteriorate the performance of even the best classifiers in a broad range of classification problems. Noisy labels also  tend to be more harmful than noisy attributes. When the observed label is noisy, we can view the correct label as a latent random variable and model the noise processes by a communication channel with unknown parameters. Thus we can apply the EM algorithm to find the parameters of both the network and the noise  and to estimate the correct label. In this study we present a neural-network approach that optimizes  the same likelihood function as optimized by the EM algorithm. The noise is explicitly modeled by an additional softmax layer that connects the correct labels to the noisy ones. This scheme is then extended  to the case where the noisy labels are dependent  on the features in addition to the correct labels.  Experimental results demonstrate that this approach  outperforms previous methods.\n", "pdf": "/pdf/7cd7427b4f427d62bb5f3191d0726f76ad45cc03.pdf", "TL;DR": "Training neural network with noisy labels", "paperhash": "goldberger|training_deep_neuralnetworks_using_a_noise_adaptation_layer", "conflicts": ["eng.biu.ac.il"], "keywords": ["Deep learning", "Optimization"], "authors": ["Jacob Goldberger", "Ehud Ben-Reuven"], "authorids": ["jacob.goldberger@biu.ac.il", "udi.benreuven@gmail.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396407170, "tcdate": 1486396407170, "number": 1, "id": "BkyWhf8dl", "invitation": "ICLR.cc/2017/conference/-/paper166/acceptance", "forum": "H12GRgcxg", "replyto": "H12GRgcxg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "Reviewers agreed that the problem was important and the method was interesting and novel. The main (shared) concerns were preliminary nature of the experiments and questions around scalability to more classes. \n \n During the discussion phase, the authors provided additional CIFAR-100 results and introduced a new approximate but scalable method for performing inference. I engaged the reviewers in discussion, who were originally borderline, to see what they thought about the changes. R2 championed the paper, stating that the additional experiments and response re: scalability were an improvement. On the balance, I think the paper is a poster accept.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training deep neural-networks using a noise adaptation layer", "abstract": "The availability of large datsets has enabled neural networks to achieve impressive recognition results. However, the presence of inaccurate class labels is known  to deteriorate the performance of even the best classifiers in a broad range of classification problems. Noisy labels also  tend to be more harmful than noisy attributes. When the observed label is noisy, we can view the correct label as a latent random variable and model the noise processes by a communication channel with unknown parameters. Thus we can apply the EM algorithm to find the parameters of both the network and the noise  and to estimate the correct label. In this study we present a neural-network approach that optimizes  the same likelihood function as optimized by the EM algorithm. The noise is explicitly modeled by an additional softmax layer that connects the correct labels to the noisy ones. This scheme is then extended  to the case where the noisy labels are dependent  on the features in addition to the correct labels.  Experimental results demonstrate that this approach  outperforms previous methods.\n", "pdf": "/pdf/7cd7427b4f427d62bb5f3191d0726f76ad45cc03.pdf", "TL;DR": "Training neural network with noisy labels", "paperhash": "goldberger|training_deep_neuralnetworks_using_a_noise_adaptation_layer", "conflicts": ["eng.biu.ac.il"], "keywords": ["Deep learning", "Optimization"], "authors": ["Jacob Goldberger", "Ehud Ben-Reuven"], "authorids": ["jacob.goldberger@biu.ac.il", "udi.benreuven@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396407933, "id": "ICLR.cc/2017/conference/-/paper166/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "H12GRgcxg", "replyto": "H12GRgcxg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396407933}}}, {"tddate": null, "tmdate": 1484812634732, "tcdate": 1484812634732, "number": 5, "id": "HkmwWl08x", "invitation": "ICLR.cc/2017/conference/-/paper166/public/comment", "forum": "H12GRgcxg", "replyto": "ryRA5jeVl", "signatures": ["~Jacob_Goldberger1"], "readers": ["everyone"], "writers": ["~Jacob_Goldberger1"], "content": {"title": "case of many classes", "comment": "In the revised version we proposed a scaleble strategy and showed that it does not cause performance degradation."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training deep neural-networks using a noise adaptation layer", "abstract": "The availability of large datsets has enabled neural networks to achieve impressive recognition results. However, the presence of inaccurate class labels is known  to deteriorate the performance of even the best classifiers in a broad range of classification problems. Noisy labels also  tend to be more harmful than noisy attributes. When the observed label is noisy, we can view the correct label as a latent random variable and model the noise processes by a communication channel with unknown parameters. Thus we can apply the EM algorithm to find the parameters of both the network and the noise  and to estimate the correct label. In this study we present a neural-network approach that optimizes  the same likelihood function as optimized by the EM algorithm. The noise is explicitly modeled by an additional softmax layer that connects the correct labels to the noisy ones. This scheme is then extended  to the case where the noisy labels are dependent  on the features in addition to the correct labels.  Experimental results demonstrate that this approach  outperforms previous methods.\n", "pdf": "/pdf/7cd7427b4f427d62bb5f3191d0726f76ad45cc03.pdf", "TL;DR": "Training neural network with noisy labels", "paperhash": "goldberger|training_deep_neuralnetworks_using_a_noise_adaptation_layer", "conflicts": ["eng.biu.ac.il"], "keywords": ["Deep learning", "Optimization"], "authors": ["Jacob Goldberger", "Ehud Ben-Reuven"], "authorids": ["jacob.goldberger@biu.ac.il", "udi.benreuven@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287703698, "id": "ICLR.cc/2017/conference/-/paper166/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H12GRgcxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper166/reviewers", "ICLR.cc/2017/conference/paper166/areachairs"], "cdate": 1485287703698}}}, {"tddate": null, "tmdate": 1483339711177, "tcdate": 1483339711177, "number": 4, "id": "rJD6wuvrl", "invitation": "ICLR.cc/2017/conference/-/paper166/public/comment", "forum": "H12GRgcxg", "replyto": "H1eu-hlVl", "signatures": ["~Jacob_Goldberger1"], "readers": ["everyone"], "writers": ["~Jacob_Goldberger1"], "content": {"title": "reply to review", "comment": "1. We added a discussion about the scalability of the method and proposed a scalable variant with the same perfromance.\n\n2. The consecutive softmax layers are exact implementation of a probabilistic modeling that describes the relation between the corrrect and noisy labels. We compare our approach to methods that are based on  a compound objective with two losses and show better results.\n\n3. We added experiments on CNN network with several layers. \n\n4  We added experiment on CIFAR-100  and showed the our method is still better than previous approaches"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training deep neural-networks using a noise adaptation layer", "abstract": "The availability of large datsets has enabled neural networks to achieve impressive recognition results. However, the presence of inaccurate class labels is known  to deteriorate the performance of even the best classifiers in a broad range of classification problems. Noisy labels also  tend to be more harmful than noisy attributes. When the observed label is noisy, we can view the correct label as a latent random variable and model the noise processes by a communication channel with unknown parameters. Thus we can apply the EM algorithm to find the parameters of both the network and the noise  and to estimate the correct label. In this study we present a neural-network approach that optimizes  the same likelihood function as optimized by the EM algorithm. The noise is explicitly modeled by an additional softmax layer that connects the correct labels to the noisy ones. This scheme is then extended  to the case where the noisy labels are dependent  on the features in addition to the correct labels.  Experimental results demonstrate that this approach  outperforms previous methods.\n", "pdf": "/pdf/7cd7427b4f427d62bb5f3191d0726f76ad45cc03.pdf", "TL;DR": "Training neural network with noisy labels", "paperhash": "goldberger|training_deep_neuralnetworks_using_a_noise_adaptation_layer", "conflicts": ["eng.biu.ac.il"], "keywords": ["Deep learning", "Optimization"], "authors": ["Jacob Goldberger", "Ehud Ben-Reuven"], "authorids": ["jacob.goldberger@biu.ac.il", "udi.benreuven@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287703698, "id": "ICLR.cc/2017/conference/-/paper166/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H12GRgcxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper166/reviewers", "ICLR.cc/2017/conference/paper166/areachairs"], "cdate": 1485287703698}}}, {"tddate": null, "tmdate": 1483305320815, "tcdate": 1482869102781, "number": 3, "id": "HkPOYHeBl", "invitation": "ICLR.cc/2017/conference/-/paper166/public/comment", "forum": "H12GRgcxg", "replyto": "H12GRgcxg", "signatures": ["~Jacob_Goldberger1"], "readers": ["everyone"], "writers": ["~Jacob_Goldberger1"], "content": {"title": "Reply to reviews", "comment": "We thank the reviewers for the comments and suggestions\n\n1) We uploaded a revised version that contains experiments on CIFAR-100 dataset. We results on the CIFAR-100 are consistent with results on other data set and show that the performance of our method is better than previous methods.   \n\n2) In the uploaded revised version we also addressed the scalability issue of the method in case of many classes. In our approach we initialized the second soft-max layer using the confusion matrix of the baseline system. The confusion matrix is a good estimation of the label noise.  Assume the rows of the matrix correspond to the true labels and the matrix columns correspond to the noisy labels. The k largest elements in the i-th row are the most occurring noisy class values when the true class value is i.    We can thus connect  i-th elements in the first softmax layer only to its k most probable noisy class  candidates.  (Note that if we connect the i-label in the first softmax only to the i-label in the second softmax layer,  we obtain the standard baseline model.) Taking only k connections to the second softamx layer solves the scalability problem. In the revised version we show experiments on CIFAR-100  that show that by using this scalable approach there is no performance degradation.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training deep neural-networks using a noise adaptation layer", "abstract": "The availability of large datsets has enabled neural networks to achieve impressive recognition results. However, the presence of inaccurate class labels is known  to deteriorate the performance of even the best classifiers in a broad range of classification problems. Noisy labels also  tend to be more harmful than noisy attributes. When the observed label is noisy, we can view the correct label as a latent random variable and model the noise processes by a communication channel with unknown parameters. Thus we can apply the EM algorithm to find the parameters of both the network and the noise  and to estimate the correct label. In this study we present a neural-network approach that optimizes  the same likelihood function as optimized by the EM algorithm. The noise is explicitly modeled by an additional softmax layer that connects the correct labels to the noisy ones. This scheme is then extended  to the case where the noisy labels are dependent  on the features in addition to the correct labels.  Experimental results demonstrate that this approach  outperforms previous methods.\n", "pdf": "/pdf/7cd7427b4f427d62bb5f3191d0726f76ad45cc03.pdf", "TL;DR": "Training neural network with noisy labels", "paperhash": "goldberger|training_deep_neuralnetworks_using_a_noise_adaptation_layer", "conflicts": ["eng.biu.ac.il"], "keywords": ["Deep learning", "Optimization"], "authors": ["Jacob Goldberger", "Ehud Ben-Reuven"], "authorids": ["jacob.goldberger@biu.ac.il", "udi.benreuven@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287703698, "id": "ICLR.cc/2017/conference/-/paper166/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H12GRgcxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper166/reviewers", "ICLR.cc/2017/conference/paper166/areachairs"], "cdate": 1485287703698}}}, {"tddate": null, "tmdate": 1482128850997, "tcdate": 1482128850997, "number": 3, "id": "BksCTxHEe", "invitation": "ICLR.cc/2017/conference/-/paper166/official/review", "forum": "H12GRgcxg", "replyto": "H12GRgcxg", "signatures": ["ICLR.cc/2017/conference/paper166/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper166/AnonReviewer3"], "content": {"title": "This paper investigates how to make neural nets be more robust to noise in the labels", "rating": "5: Marginally below acceptance threshold", "review": "This paper looks at how to train if there are significant label noise present.\nThis is a good paper where two main methods are proposed, the first one is a latent variable model and training would require the EM algorithm, alternating between estimating the true label and maximizing the parameters given a true label.\n\nThe second directly integrates out the true label and simply optimizes the p(z|x).\n\nPros: the paper examines a training scenario which is a real concern for big dataset which are not carefully annotated.\nCons: the results on mnist is all synthetic and it's hard to tell if this would translate to a win on real datasets.\n\n- comments:\nEquation 11 should be expensive, what happens if you are training on imagenet with 1000 classes?\nIt would be nice to see how well you can recover the corrupting distribution parameter using either the EM or the integration method. \n\nOverall, this is an OK paper. However, the ideas are not novel as previous cited papers have tried to handle noise in the labels. I think the authors can make the paper better by either demonstrating state-of-the-art results on a dataset known to have label noise, or demonstrate that a method can reliably estimate the true label corrupting probabilities.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training deep neural-networks using a noise adaptation layer", "abstract": "The availability of large datsets has enabled neural networks to achieve impressive recognition results. However, the presence of inaccurate class labels is known  to deteriorate the performance of even the best classifiers in a broad range of classification problems. Noisy labels also  tend to be more harmful than noisy attributes. When the observed label is noisy, we can view the correct label as a latent random variable and model the noise processes by a communication channel with unknown parameters. Thus we can apply the EM algorithm to find the parameters of both the network and the noise  and to estimate the correct label. In this study we present a neural-network approach that optimizes  the same likelihood function as optimized by the EM algorithm. The noise is explicitly modeled by an additional softmax layer that connects the correct labels to the noisy ones. This scheme is then extended  to the case where the noisy labels are dependent  on the features in addition to the correct labels.  Experimental results demonstrate that this approach  outperforms previous methods.\n", "pdf": "/pdf/7cd7427b4f427d62bb5f3191d0726f76ad45cc03.pdf", "TL;DR": "Training neural network with noisy labels", "paperhash": "goldberger|training_deep_neuralnetworks_using_a_noise_adaptation_layer", "conflicts": ["eng.biu.ac.il"], "keywords": ["Deep learning", "Optimization"], "authors": ["Jacob Goldberger", "Ehud Ben-Reuven"], "authorids": ["jacob.goldberger@biu.ac.il", "udi.benreuven@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512676623, "id": "ICLR.cc/2017/conference/-/paper166/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper166/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper166/AnonReviewer1", "ICLR.cc/2017/conference/paper166/AnonReviewer2", "ICLR.cc/2017/conference/paper166/AnonReviewer3"], "reply": {"forum": "H12GRgcxg", "replyto": "H12GRgcxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper166/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper166/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512676623}}}, {"tddate": null, "tmdate": 1481928157693, "tcdate": 1481928157693, "number": 2, "id": "BkL101z4e", "invitation": "ICLR.cc/2017/conference/-/paper166/official/review", "forum": "H12GRgcxg", "replyto": "H12GRgcxg", "signatures": ["ICLR.cc/2017/conference/paper166/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper166/AnonReviewer2"], "content": {"title": "Interesting paper but lack of experiments", "rating": "7: Good paper, accept", "review": "The paper addressed the erroneous label problem for supervised training. The problem is well formulated and the presented solution is novel. \n\nThe experimental justification is limited. The effectiveness of the proposed method is hard to gauge, especially how to scale the proposed method to large number of classification targets and whether it is still effective.\n\nFor example, it would be interesting to see whether the proposed method is better than training with only less but high quality data. \n\nFrom Figure 2, it seems with more data, the proposed method tends to behave very well when the noise fraction is below a threshold and dramatically degrades once passing that threshold. Analysis and justification of this behavior whether it is just by chance or an expected one of the method would be very useful. \n\n ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training deep neural-networks using a noise adaptation layer", "abstract": "The availability of large datsets has enabled neural networks to achieve impressive recognition results. However, the presence of inaccurate class labels is known  to deteriorate the performance of even the best classifiers in a broad range of classification problems. Noisy labels also  tend to be more harmful than noisy attributes. When the observed label is noisy, we can view the correct label as a latent random variable and model the noise processes by a communication channel with unknown parameters. Thus we can apply the EM algorithm to find the parameters of both the network and the noise  and to estimate the correct label. In this study we present a neural-network approach that optimizes  the same likelihood function as optimized by the EM algorithm. The noise is explicitly modeled by an additional softmax layer that connects the correct labels to the noisy ones. This scheme is then extended  to the case where the noisy labels are dependent  on the features in addition to the correct labels.  Experimental results demonstrate that this approach  outperforms previous methods.\n", "pdf": "/pdf/7cd7427b4f427d62bb5f3191d0726f76ad45cc03.pdf", "TL;DR": "Training neural network with noisy labels", "paperhash": "goldberger|training_deep_neuralnetworks_using_a_noise_adaptation_layer", "conflicts": ["eng.biu.ac.il"], "keywords": ["Deep learning", "Optimization"], "authors": ["Jacob Goldberger", "Ehud Ben-Reuven"], "authorids": ["jacob.goldberger@biu.ac.il", "udi.benreuven@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512676623, "id": "ICLR.cc/2017/conference/-/paper166/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper166/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper166/AnonReviewer1", "ICLR.cc/2017/conference/paper166/AnonReviewer2", "ICLR.cc/2017/conference/paper166/AnonReviewer3"], "reply": {"forum": "H12GRgcxg", "replyto": "H12GRgcxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper166/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper166/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512676623}}}, {"tddate": null, "tmdate": 1481847143628, "tcdate": 1481847143623, "number": 1, "id": "H1eu-hlVl", "invitation": "ICLR.cc/2017/conference/-/paper166/official/review", "forum": "H12GRgcxg", "replyto": "H12GRgcxg", "signatures": ["ICLR.cc/2017/conference/paper166/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper166/AnonReviewer1"], "content": {"title": "Training with Noisy Labels", "rating": "5: Marginally below acceptance threshold", "review": "This work address the problem of supervised learning from strongly labeled data with label noise. This is a very practical and relevant problem in applied machine learning.  The authors note that using sampling approaches such as EM isn't effective, too slow and cannot be integrated into end-to-end training. Thus, they propose to simulate the effects of EM by a noisy adaptation layer, effectively a softmax, that is added to the architecture during training, and is omitted at inference time. The proposed algorithm is evaluated on MNIST and shows improvements over existing approaches that deal with noisy labeled data.\n\nA few comments.\n1. There is no discussion in the work about the increased complexity of training for the model with two softmaxes. \n\n2. What is the rationale for having consecutive (serialized) softmaxes, instead of having a compound objective with two losses, or a network with parallel losses and two sets of gradients?\n\n3. The proposed architecture with only two hidden layers isn't not representative of larger and deeper models that are practically used, and it is not clear that shown results will scale to bigger networks. \n\n4. Why is the approach only evaluated on MNIST, a dataset that is unrealistically simple.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training deep neural-networks using a noise adaptation layer", "abstract": "The availability of large datsets has enabled neural networks to achieve impressive recognition results. However, the presence of inaccurate class labels is known  to deteriorate the performance of even the best classifiers in a broad range of classification problems. Noisy labels also  tend to be more harmful than noisy attributes. When the observed label is noisy, we can view the correct label as a latent random variable and model the noise processes by a communication channel with unknown parameters. Thus we can apply the EM algorithm to find the parameters of both the network and the noise  and to estimate the correct label. In this study we present a neural-network approach that optimizes  the same likelihood function as optimized by the EM algorithm. The noise is explicitly modeled by an additional softmax layer that connects the correct labels to the noisy ones. This scheme is then extended  to the case where the noisy labels are dependent  on the features in addition to the correct labels.  Experimental results demonstrate that this approach  outperforms previous methods.\n", "pdf": "/pdf/7cd7427b4f427d62bb5f3191d0726f76ad45cc03.pdf", "TL;DR": "Training neural network with noisy labels", "paperhash": "goldberger|training_deep_neuralnetworks_using_a_noise_adaptation_layer", "conflicts": ["eng.biu.ac.il"], "keywords": ["Deep learning", "Optimization"], "authors": ["Jacob Goldberger", "Ehud Ben-Reuven"], "authorids": ["jacob.goldberger@biu.ac.il", "udi.benreuven@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512676623, "id": "ICLR.cc/2017/conference/-/paper166/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper166/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper166/AnonReviewer1", "ICLR.cc/2017/conference/paper166/AnonReviewer2", "ICLR.cc/2017/conference/paper166/AnonReviewer3"], "reply": {"forum": "H12GRgcxg", "replyto": "H12GRgcxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper166/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper166/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512676623}}}, {"tddate": null, "tmdate": 1481845461945, "tcdate": 1481845461940, "number": 1, "id": "ryRA5jeVl", "invitation": "ICLR.cc/2017/conference/-/paper166/official/comment", "forum": "H12GRgcxg", "replyto": "Sy52_tV7g", "signatures": ["ICLR.cc/2017/conference/paper166/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper166/AnonReviewer1"], "content": {"title": "case of many classes", "comment": "in practice, hierarchical softmax does not work, instead embeddings and NN search is the preferred approach. Can you comment on that?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training deep neural-networks using a noise adaptation layer", "abstract": "The availability of large datsets has enabled neural networks to achieve impressive recognition results. However, the presence of inaccurate class labels is known  to deteriorate the performance of even the best classifiers in a broad range of classification problems. Noisy labels also  tend to be more harmful than noisy attributes. When the observed label is noisy, we can view the correct label as a latent random variable and model the noise processes by a communication channel with unknown parameters. Thus we can apply the EM algorithm to find the parameters of both the network and the noise  and to estimate the correct label. In this study we present a neural-network approach that optimizes  the same likelihood function as optimized by the EM algorithm. The noise is explicitly modeled by an additional softmax layer that connects the correct labels to the noisy ones. This scheme is then extended  to the case where the noisy labels are dependent  on the features in addition to the correct labels.  Experimental results demonstrate that this approach  outperforms previous methods.\n", "pdf": "/pdf/7cd7427b4f427d62bb5f3191d0726f76ad45cc03.pdf", "TL;DR": "Training neural network with noisy labels", "paperhash": "goldberger|training_deep_neuralnetworks_using_a_noise_adaptation_layer", "conflicts": ["eng.biu.ac.il"], "keywords": ["Deep learning", "Optimization"], "authors": ["Jacob Goldberger", "Ehud Ben-Reuven"], "authorids": ["jacob.goldberger@biu.ac.il", "udi.benreuven@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287703571, "id": "ICLR.cc/2017/conference/-/paper166/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "H12GRgcxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper166/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper166/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper166/reviewers", "ICLR.cc/2017/conference/paper166/areachairs"], "cdate": 1485287703571}}}, {"tddate": null, "tmdate": 1481050289916, "tcdate": 1481050289913, "number": 2, "id": "Sy52_tV7g", "invitation": "ICLR.cc/2017/conference/-/paper166/public/comment", "forum": "H12GRgcxg", "replyto": "ByZM18kXg", "signatures": ["~Jacob_Goldberger1"], "readers": ["everyone"], "writers": ["~Jacob_Goldberger1"], "content": {"title": "case of many classes", "comment": "If there are many different classes there is indeed a scalability issue.  We can handle it using standard approaches that address soft-max with many labels such as hierarchical soft-max, NCE and importance sampling.  "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training deep neural-networks using a noise adaptation layer", "abstract": "The availability of large datsets has enabled neural networks to achieve impressive recognition results. However, the presence of inaccurate class labels is known  to deteriorate the performance of even the best classifiers in a broad range of classification problems. Noisy labels also  tend to be more harmful than noisy attributes. When the observed label is noisy, we can view the correct label as a latent random variable and model the noise processes by a communication channel with unknown parameters. Thus we can apply the EM algorithm to find the parameters of both the network and the noise  and to estimate the correct label. In this study we present a neural-network approach that optimizes  the same likelihood function as optimized by the EM algorithm. The noise is explicitly modeled by an additional softmax layer that connects the correct labels to the noisy ones. This scheme is then extended  to the case where the noisy labels are dependent  on the features in addition to the correct labels.  Experimental results demonstrate that this approach  outperforms previous methods.\n", "pdf": "/pdf/7cd7427b4f427d62bb5f3191d0726f76ad45cc03.pdf", "TL;DR": "Training neural network with noisy labels", "paperhash": "goldberger|training_deep_neuralnetworks_using_a_noise_adaptation_layer", "conflicts": ["eng.biu.ac.il"], "keywords": ["Deep learning", "Optimization"], "authors": ["Jacob Goldberger", "Ehud Ben-Reuven"], "authorids": ["jacob.goldberger@biu.ac.il", "udi.benreuven@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287703698, "id": "ICLR.cc/2017/conference/-/paper166/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H12GRgcxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper166/reviewers", "ICLR.cc/2017/conference/paper166/areachairs"], "cdate": 1485287703698}}}, {"tddate": null, "tmdate": 1481049923598, "tcdate": 1481049923593, "number": 1, "id": "BJnHvFN7l", "invitation": "ICLR.cc/2017/conference/-/paper166/public/comment", "forum": "H12GRgcxg", "replyto": "HJcIz8ZXe", "signatures": ["~Jacob_Goldberger1"], "readers": ["everyone"], "writers": ["~Jacob_Goldberger1"], "content": {"title": "EM behavior ", "comment": "First in our paper we provide an alternative to the EM approach. Unless the noise level is very high or there is not enough data, the EM algorithm will converge to a good estimation of the noise distribution. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training deep neural-networks using a noise adaptation layer", "abstract": "The availability of large datsets has enabled neural networks to achieve impressive recognition results. However, the presence of inaccurate class labels is known  to deteriorate the performance of even the best classifiers in a broad range of classification problems. Noisy labels also  tend to be more harmful than noisy attributes. When the observed label is noisy, we can view the correct label as a latent random variable and model the noise processes by a communication channel with unknown parameters. Thus we can apply the EM algorithm to find the parameters of both the network and the noise  and to estimate the correct label. In this study we present a neural-network approach that optimizes  the same likelihood function as optimized by the EM algorithm. The noise is explicitly modeled by an additional softmax layer that connects the correct labels to the noisy ones. This scheme is then extended  to the case where the noisy labels are dependent  on the features in addition to the correct labels.  Experimental results demonstrate that this approach  outperforms previous methods.\n", "pdf": "/pdf/7cd7427b4f427d62bb5f3191d0726f76ad45cc03.pdf", "TL;DR": "Training neural network with noisy labels", "paperhash": "goldberger|training_deep_neuralnetworks_using_a_noise_adaptation_layer", "conflicts": ["eng.biu.ac.il"], "keywords": ["Deep learning", "Optimization"], "authors": ["Jacob Goldberger", "Ehud Ben-Reuven"], "authorids": ["jacob.goldberger@biu.ac.il", "udi.benreuven@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287703698, "id": "ICLR.cc/2017/conference/-/paper166/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H12GRgcxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper166/reviewers", "ICLR.cc/2017/conference/paper166/areachairs"], "cdate": 1485287703698}}}, {"tddate": null, "tmdate": 1480839762007, "tcdate": 1480839762003, "number": 2, "id": "HJcIz8ZXe", "invitation": "ICLR.cc/2017/conference/-/paper166/pre-review/question", "forum": "H12GRgcxg", "replyto": "H12GRgcxg", "signatures": ["ICLR.cc/2017/conference/paper166/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper166/AnonReviewer3"], "content": {"title": "Recovering the latent p", "question": "In your experiments, how well can your EM algorithm recover the \"converted\neach label with probability p to a different label\", from the experiments section?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training deep neural-networks using a noise adaptation layer", "abstract": "The availability of large datsets has enabled neural networks to achieve impressive recognition results. However, the presence of inaccurate class labels is known  to deteriorate the performance of even the best classifiers in a broad range of classification problems. Noisy labels also  tend to be more harmful than noisy attributes. When the observed label is noisy, we can view the correct label as a latent random variable and model the noise processes by a communication channel with unknown parameters. Thus we can apply the EM algorithm to find the parameters of both the network and the noise  and to estimate the correct label. In this study we present a neural-network approach that optimizes  the same likelihood function as optimized by the EM algorithm. The noise is explicitly modeled by an additional softmax layer that connects the correct labels to the noisy ones. This scheme is then extended  to the case where the noisy labels are dependent  on the features in addition to the correct labels.  Experimental results demonstrate that this approach  outperforms previous methods.\n", "pdf": "/pdf/7cd7427b4f427d62bb5f3191d0726f76ad45cc03.pdf", "TL;DR": "Training neural network with noisy labels", "paperhash": "goldberger|training_deep_neuralnetworks_using_a_noise_adaptation_layer", "conflicts": ["eng.biu.ac.il"], "keywords": ["Deep learning", "Optimization"], "authors": ["Jacob Goldberger", "Ehud Ben-Reuven"], "authorids": ["jacob.goldberger@biu.ac.il", "udi.benreuven@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959428697, "id": "ICLR.cc/2017/conference/-/paper166/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper166/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper166/AnonReviewer2", "ICLR.cc/2017/conference/paper166/AnonReviewer3"], "reply": {"forum": "H12GRgcxg", "replyto": "H12GRgcxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper166/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper166/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959428697}}}, {"tddate": null, "tmdate": 1480707849503, "tcdate": 1480707849499, "number": 1, "id": "ByZM18kXg", "invitation": "ICLR.cc/2017/conference/-/paper166/pre-review/question", "forum": "H12GRgcxg", "replyto": "H12GRgcxg", "signatures": ["ICLR.cc/2017/conference/paper166/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper166/AnonReviewer2"], "content": {"title": "Scalability to large number of classes", "question": "Suppose there are N classes to predict, the proposed methods requires (N+1) sets of softmax with the size of N. For speech tasks, there are normally around 10k output classes or even more. Discussions on the scalability of the method and whether they still work well in those cases would very helpful.   "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training deep neural-networks using a noise adaptation layer", "abstract": "The availability of large datsets has enabled neural networks to achieve impressive recognition results. However, the presence of inaccurate class labels is known  to deteriorate the performance of even the best classifiers in a broad range of classification problems. Noisy labels also  tend to be more harmful than noisy attributes. When the observed label is noisy, we can view the correct label as a latent random variable and model the noise processes by a communication channel with unknown parameters. Thus we can apply the EM algorithm to find the parameters of both the network and the noise  and to estimate the correct label. In this study we present a neural-network approach that optimizes  the same likelihood function as optimized by the EM algorithm. The noise is explicitly modeled by an additional softmax layer that connects the correct labels to the noisy ones. This scheme is then extended  to the case where the noisy labels are dependent  on the features in addition to the correct labels.  Experimental results demonstrate that this approach  outperforms previous methods.\n", "pdf": "/pdf/7cd7427b4f427d62bb5f3191d0726f76ad45cc03.pdf", "TL;DR": "Training neural network with noisy labels", "paperhash": "goldberger|training_deep_neuralnetworks_using_a_noise_adaptation_layer", "conflicts": ["eng.biu.ac.il"], "keywords": ["Deep learning", "Optimization"], "authors": ["Jacob Goldberger", "Ehud Ben-Reuven"], "authorids": ["jacob.goldberger@biu.ac.il", "udi.benreuven@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959428697, "id": "ICLR.cc/2017/conference/-/paper166/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper166/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper166/AnonReviewer2", "ICLR.cc/2017/conference/paper166/AnonReviewer3"], "reply": {"forum": "H12GRgcxg", "replyto": "H12GRgcxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper166/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper166/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959428697}}}], "count": 13}