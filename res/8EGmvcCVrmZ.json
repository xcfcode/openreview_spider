{"notes": [{"id": "8EGmvcCVrmZ", "original": "S4IuPL5F8_K", "number": 1361, "cdate": 1601308151939, "ddate": null, "tcdate": 1601308151939, "tmdate": 1614985631932, "tddate": null, "forum": "8EGmvcCVrmZ", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Deep Learning is Singular, and That's Good", "authorids": ["~Daniel_Murfet1", "~Susan_Wei1", "~Mingming_Gong1", "huli2@student.unimelb.edu.au", "j.gell@unimelb.edu.au", "thomas.quella@unimelb.edu.au"], "authors": ["Daniel Murfet", "Susan Wei", "Mingming Gong", "Hui Li", "Jesse Gell-Redman", "Thomas Quella"], "keywords": ["deep learning theory", "effective degrees of freedom", "generalisation", "posterior predictive distribution", "real log canonical threshold", "singular learning theory"], "abstract": "In singular models, the optimal set of parameters forms an analytic set with singularities and classical statistical inference cannot be applied to such models. This is significant for deep learning as neural networks are singular and thus ``dividing\" by the determinant of the Hessian or employing the Laplace approximation are not appropriate. Despite its potential for addressing fundamental issues in deep learning, singular learning theory appears to have made little inroads into the developing canon of deep learning theory. Via a mix of theory and experiment, we present an invitation to singular learning theory as a vehicle for understanding deep learning and suggest important future work to make singular learning theory directly applicable to how deep learning is performed in practice. ", "one-sentence_summary": "An invitation to singular learning theory as a theory of deep learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "murfet|deep_learning_is_singular_and_thats_good", "supplementary_material": "/attachment/cdea96d33b68519a8fb04c435d1de90312a35fae.zip", "pdf": "/pdf/055b587f821423ecb63143818cafdc91a9a974af.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0L9nqwO3PE", "_bibtex": "@misc{\nmurfet2021deep,\ntitle={Deep Learning is Singular, and That's Good},\nauthor={Daniel Murfet and Susan Wei and Mingming Gong and Hui Li and Jesse Gell-Redman and Thomas Quella},\nyear={2021},\nurl={https://openreview.net/forum?id=8EGmvcCVrmZ}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "5zwoXKy1Hlz", "original": null, "number": 1, "cdate": 1610040528844, "ddate": null, "tcdate": 1610040528844, "tmdate": 1610474138177, "tddate": null, "forum": "8EGmvcCVrmZ", "replyto": "8EGmvcCVrmZ", "invitation": "ICLR.cc/2021/Conference/Paper1361/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper proposes to introduce ideas from singular theory to deep learning. All reviewers agree that the work is not yet ready for publication. The key issue seems to boil down to the fact that the paper does not propose nor verify any clearly motivated scientific hypothesis. Relatedly, the work includes many too broad or unscientific claims such as \"To understand why classical measures of capacity fail to say anything meaningful about DNNs\". Such statements should be given more precisely and with a proper citation.  \n\nBased on this I have to recommend rejecting the paper. At the same time, I would like to thank the Authors for submitting the work for consideration to ICLR. I hope the feedback will be useful for improving the work."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning is Singular, and That's Good", "authorids": ["~Daniel_Murfet1", "~Susan_Wei1", "~Mingming_Gong1", "huli2@student.unimelb.edu.au", "j.gell@unimelb.edu.au", "thomas.quella@unimelb.edu.au"], "authors": ["Daniel Murfet", "Susan Wei", "Mingming Gong", "Hui Li", "Jesse Gell-Redman", "Thomas Quella"], "keywords": ["deep learning theory", "effective degrees of freedom", "generalisation", "posterior predictive distribution", "real log canonical threshold", "singular learning theory"], "abstract": "In singular models, the optimal set of parameters forms an analytic set with singularities and classical statistical inference cannot be applied to such models. This is significant for deep learning as neural networks are singular and thus ``dividing\" by the determinant of the Hessian or employing the Laplace approximation are not appropriate. Despite its potential for addressing fundamental issues in deep learning, singular learning theory appears to have made little inroads into the developing canon of deep learning theory. Via a mix of theory and experiment, we present an invitation to singular learning theory as a vehicle for understanding deep learning and suggest important future work to make singular learning theory directly applicable to how deep learning is performed in practice. ", "one-sentence_summary": "An invitation to singular learning theory as a theory of deep learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "murfet|deep_learning_is_singular_and_thats_good", "supplementary_material": "/attachment/cdea96d33b68519a8fb04c435d1de90312a35fae.zip", "pdf": "/pdf/055b587f821423ecb63143818cafdc91a9a974af.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0L9nqwO3PE", "_bibtex": "@misc{\nmurfet2021deep,\ntitle={Deep Learning is Singular, and That's Good},\nauthor={Daniel Murfet and Susan Wei and Mingming Gong and Hui Li and Jesse Gell-Redman and Thomas Quella},\nyear={2021},\nurl={https://openreview.net/forum?id=8EGmvcCVrmZ}\n}"}, "tags": [], "invitation": {"reply": {"forum": "8EGmvcCVrmZ", "replyto": "8EGmvcCVrmZ", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040528832, "tmdate": 1610474138161, "id": "ICLR.cc/2021/Conference/Paper1361/-/Decision"}}}, {"id": "8mkwAlESXcs", "original": null, "number": 5, "cdate": 1605606962691, "ddate": null, "tcdate": 1605606962691, "tmdate": 1605606962691, "tddate": null, "forum": "8EGmvcCVrmZ", "replyto": "abyT4sGG61Q", "invitation": "ICLR.cc/2021/Conference/Paper1361/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "We are glad to hear that Reviewer 2 enjoyed the pedagogical aspect of our submission. \n\nRegarding the comments on the effective number of parameters. There are no assumptions on the model here: the calculation is meant only to exhibit that in regular models and a slightly larger class (what we term the minimally singular models) the RLCT agrees with a natural count of the effective number of parameters. The point of this calculation is to show that the RLCT is a generalisation of this quantity to general singular models (such as DNNs). While there may exist other generalisations, it is hard to argue they can be equally fundamental, given the role of the RLCT in the asymptotic formula for the Bayes generalisation error. We agree that these observations are elementary given a familiarity with singular learning theory, nonetheless they are important and have been missed.\n\nThe claim that the Bayes predictive distribution is superior to MAP or MLE for singular  models is on firm theoretical grounds. This is hard to demonstrate empirically in a complete way because the posterior distribution over DNN weights is intractable. However, we were encouraged by the fact that being Bayesian in the last layer (which can be viewed as an approximate Bayes predictive distribution) shows superiority over MAP. We agree that more experiments should be done to support the claim that the last-layer-Bayesian approach is a good scheme for approximating the Bayes predictive distribution."}, "signatures": ["ICLR.cc/2021/Conference/Paper1361/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1361/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning is Singular, and That's Good", "authorids": ["~Daniel_Murfet1", "~Susan_Wei1", "~Mingming_Gong1", "huli2@student.unimelb.edu.au", "j.gell@unimelb.edu.au", "thomas.quella@unimelb.edu.au"], "authors": ["Daniel Murfet", "Susan Wei", "Mingming Gong", "Hui Li", "Jesse Gell-Redman", "Thomas Quella"], "keywords": ["deep learning theory", "effective degrees of freedom", "generalisation", "posterior predictive distribution", "real log canonical threshold", "singular learning theory"], "abstract": "In singular models, the optimal set of parameters forms an analytic set with singularities and classical statistical inference cannot be applied to such models. This is significant for deep learning as neural networks are singular and thus ``dividing\" by the determinant of the Hessian or employing the Laplace approximation are not appropriate. Despite its potential for addressing fundamental issues in deep learning, singular learning theory appears to have made little inroads into the developing canon of deep learning theory. Via a mix of theory and experiment, we present an invitation to singular learning theory as a vehicle for understanding deep learning and suggest important future work to make singular learning theory directly applicable to how deep learning is performed in practice. ", "one-sentence_summary": "An invitation to singular learning theory as a theory of deep learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "murfet|deep_learning_is_singular_and_thats_good", "supplementary_material": "/attachment/cdea96d33b68519a8fb04c435d1de90312a35fae.zip", "pdf": "/pdf/055b587f821423ecb63143818cafdc91a9a974af.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0L9nqwO3PE", "_bibtex": "@misc{\nmurfet2021deep,\ntitle={Deep Learning is Singular, and That's Good},\nauthor={Daniel Murfet and Susan Wei and Mingming Gong and Hui Li and Jesse Gell-Redman and Thomas Quella},\nyear={2021},\nurl={https://openreview.net/forum?id=8EGmvcCVrmZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8EGmvcCVrmZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1361/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1361/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1361/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1361/Authors|ICLR.cc/2021/Conference/Paper1361/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1361/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860589, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1361/-/Official_Comment"}}}, {"id": "anxo7ZRHSN", "original": null, "number": 4, "cdate": 1605605031217, "ddate": null, "tcdate": 1605605031217, "tmdate": 1605605031217, "tddate": null, "forum": "8EGmvcCVrmZ", "replyto": "XBy1AjYYhaA", "invitation": "ICLR.cc/2021/Conference/Paper1361/-/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "Yes, the paper is one-part review of singular learning theory and other-part drawing the implications of singular learning theory for DNN. Thank you for noting the paper is clearly written and well-organized. \n\nRegarding the stated con, we do not agree with a scientific standard that puts novelty above all other values. If a problem is widely acknowledged to be important and has remained open despite much attention, then its solution is significant, even if that solution makes use of existing methods. For instance, the link between \"flatness\" and generalisation error, and how to measure the former, has occupied many published works. While short, the material in Section 4 definitively refutes this connection using singular learning theory, by explaining how the codimension of the set of true parameters (or more precisely, the RLCT) is much more important than curvature. If you wish to disagree with this conclusion or find our argument unconvincing, then we welcome that discussion, but given the amount of ink that has been spilled in the literature on this topic we cannot agree that resolving this confusion is insignificant."}, "signatures": ["ICLR.cc/2021/Conference/Paper1361/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1361/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning is Singular, and That's Good", "authorids": ["~Daniel_Murfet1", "~Susan_Wei1", "~Mingming_Gong1", "huli2@student.unimelb.edu.au", "j.gell@unimelb.edu.au", "thomas.quella@unimelb.edu.au"], "authors": ["Daniel Murfet", "Susan Wei", "Mingming Gong", "Hui Li", "Jesse Gell-Redman", "Thomas Quella"], "keywords": ["deep learning theory", "effective degrees of freedom", "generalisation", "posterior predictive distribution", "real log canonical threshold", "singular learning theory"], "abstract": "In singular models, the optimal set of parameters forms an analytic set with singularities and classical statistical inference cannot be applied to such models. This is significant for deep learning as neural networks are singular and thus ``dividing\" by the determinant of the Hessian or employing the Laplace approximation are not appropriate. Despite its potential for addressing fundamental issues in deep learning, singular learning theory appears to have made little inroads into the developing canon of deep learning theory. Via a mix of theory and experiment, we present an invitation to singular learning theory as a vehicle for understanding deep learning and suggest important future work to make singular learning theory directly applicable to how deep learning is performed in practice. ", "one-sentence_summary": "An invitation to singular learning theory as a theory of deep learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "murfet|deep_learning_is_singular_and_thats_good", "supplementary_material": "/attachment/cdea96d33b68519a8fb04c435d1de90312a35fae.zip", "pdf": "/pdf/055b587f821423ecb63143818cafdc91a9a974af.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0L9nqwO3PE", "_bibtex": "@misc{\nmurfet2021deep,\ntitle={Deep Learning is Singular, and That's Good},\nauthor={Daniel Murfet and Susan Wei and Mingming Gong and Hui Li and Jesse Gell-Redman and Thomas Quella},\nyear={2021},\nurl={https://openreview.net/forum?id=8EGmvcCVrmZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8EGmvcCVrmZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1361/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1361/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1361/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1361/Authors|ICLR.cc/2021/Conference/Paper1361/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1361/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860589, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1361/-/Official_Comment"}}}, {"id": "5iKlER1amIs", "original": null, "number": 3, "cdate": 1605604690329, "ddate": null, "tcdate": 1605604690329, "tmdate": 1605604756897, "tddate": null, "forum": "8EGmvcCVrmZ", "replyto": "CgWmojHjqXf", "invitation": "ICLR.cc/2021/Conference/Paper1361/-/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "We are glad the reviewer enjoyed the section on connections of RLCT with flatness. We agree that moving A.1-A.4 up to the main text would help the general exposition.\n\nRegarding the questions posed by the reviewer: \n\n\u201c\u2026the fact that one sample from the posterior distribution also results in good generalization for deep networks\u201d\n\nThis is a good question but we wish to reframe it slightly, since research on the connection between finding a solution by SGD and sampling from the posterior distribution is currently inconclusive. Regarding the question \u201cwhy does one run of SGD tend to produce a network that generalises well\u201d it is true that singular learning theory does not currently offer a conclusive answer. Progress will require a rigorous understanding of the connection between SGD and the posterior. \n\nHowever, this topic is addressed in Section 5 where the MAP is used a baseline to compare with our estimate of the Bayesian predictive distribution. The MAP is the solution reached by the SGD, and it is in general inferior to the approximate Bayesian predictive distribution.\n\n\u201c\u2026better experimental evidence is necessary in order to make a convincing case that it [singular learning theory] is a promising one\u201d\n\nWe are not aware of methods for estimating the RLCT for large DNNs. We hope to point out the importance of this quantity so that the plethora of talented people from the DL community can engage with this research. Also it is important to highlight the role of experiments in a work of this nature. In deep learning research where the theory is lacking, empirical evidence assumes a primary role. This is not the case here. We highlight a sound theoretical framework for studying deep learning but the challenge here is that many objects of interest (such as the RLCT) are difficult (but not impossible surely!) to estimate. \n\n\u201cCan you also investigate whether one obtains non-vacuous generalization bounds for large deep networks using an estimate for the RLCT\u201d\n\nThe relationship between the RLCT and (average) generalization error of the Bayes predictive distribution is not in the form of a bound. What would be interesting is to explore bounds on the RLCT itself.\n\n\u201cIs the true distribution of data really unrealizable? The reviewer is of the opinion that the fact that we can learn very good generative models for complex data indicates that it might be more viable to study the case when the true distribution is realizable for large-enough models.\u201d\n\nThe empirical success of deep generative models is mostly measured by its generative ability, e.g.. performance at drawing realistic looking images. The performance of deep generative models as density estimation is far less convincing. In fact, there are many emerging works that illustrate the phenomenon that a learned generative model can assign higher likelihood to Out-Of-Distribution instances than In-Distribution instances. This empirical observation casts serious doubt on whether generative models learn the underlying density well. It may learn it well enough to sample but that does not mean the generative model is estimating the density in a functionally accurate way. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1361/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1361/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning is Singular, and That's Good", "authorids": ["~Daniel_Murfet1", "~Susan_Wei1", "~Mingming_Gong1", "huli2@student.unimelb.edu.au", "j.gell@unimelb.edu.au", "thomas.quella@unimelb.edu.au"], "authors": ["Daniel Murfet", "Susan Wei", "Mingming Gong", "Hui Li", "Jesse Gell-Redman", "Thomas Quella"], "keywords": ["deep learning theory", "effective degrees of freedom", "generalisation", "posterior predictive distribution", "real log canonical threshold", "singular learning theory"], "abstract": "In singular models, the optimal set of parameters forms an analytic set with singularities and classical statistical inference cannot be applied to such models. This is significant for deep learning as neural networks are singular and thus ``dividing\" by the determinant of the Hessian or employing the Laplace approximation are not appropriate. Despite its potential for addressing fundamental issues in deep learning, singular learning theory appears to have made little inroads into the developing canon of deep learning theory. Via a mix of theory and experiment, we present an invitation to singular learning theory as a vehicle for understanding deep learning and suggest important future work to make singular learning theory directly applicable to how deep learning is performed in practice. ", "one-sentence_summary": "An invitation to singular learning theory as a theory of deep learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "murfet|deep_learning_is_singular_and_thats_good", "supplementary_material": "/attachment/cdea96d33b68519a8fb04c435d1de90312a35fae.zip", "pdf": "/pdf/055b587f821423ecb63143818cafdc91a9a974af.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0L9nqwO3PE", "_bibtex": "@misc{\nmurfet2021deep,\ntitle={Deep Learning is Singular, and That's Good},\nauthor={Daniel Murfet and Susan Wei and Mingming Gong and Hui Li and Jesse Gell-Redman and Thomas Quella},\nyear={2021},\nurl={https://openreview.net/forum?id=8EGmvcCVrmZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8EGmvcCVrmZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1361/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1361/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1361/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1361/Authors|ICLR.cc/2021/Conference/Paper1361/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1361/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860589, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1361/-/Official_Comment"}}}, {"id": "vksM4G2kO_A", "original": null, "number": 2, "cdate": 1605604197794, "ddate": null, "tcdate": 1605604197794, "tmdate": 1605604197794, "tddate": null, "forum": "8EGmvcCVrmZ", "replyto": "ioMbxZGoJrB", "invitation": "ICLR.cc/2021/Conference/Paper1361/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "We attempted to give a self-contained rapid introduction to singular learning theory in Section 3. This was necessarily terse given the page limit. We do agree that the discussion at the end of Section 3 should\u2019ve been further highlighted.\n\nReviewer 1 asks how the measure compares to other frameworks. We assume the question is, specifically, how the RLCT compares to other complexity measures in deep learning. As far as we know the RLCT is the only complexity measure in deep learning with a strong theoretical basis. However the RLCT is hard to estimate, so it would certainly be interesting to investigate other proposed complexity measures to see if they can offer a cheap method of approximating the RLCT.\n\nThere is no problem to move from building an approximate Bayesian predictive distribution for a regression task to a classification task. The experiments in Section 5 operate on the assumption that \u201cbeing a little bit Bayesian (in the last layer of the network)\u201d can already reap the benefits of being \u201cfully Bayesian.\u201d In particular, we hoped Section 5 would offer some preliminary empirical confirmation of the fact that Bayesian prediction is superior to MLE/MAP. Note however that the theory is definitive on this matter -- for singular models Bayesian prediction is superior to MLE/MAP -- and thus empirical evidence should be viewed as supplementary. \n\nIt would be interesting to see if singular learning theory can explain the phase transition from under to over-parametrised. Our current experiments were not designed with this in mind. In general, variation of the number of parameters within a family of related model is not a well-studied topic within singular learning theory, and this is an interesting open problem.\n\nRegarding the promise of the framework, and what the benefits the community could expect. While we agree that many people know the classical methods fail, it is widely under-appreciated how profound the failure is and how difficult and deep the new ideas required to deal with this failure are. In our view any fundamental mathematical theory of deep learning must eventually grapple with these issues in some form, and singular learning theory is at present the only theory to do so."}, "signatures": ["ICLR.cc/2021/Conference/Paper1361/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1361/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning is Singular, and That's Good", "authorids": ["~Daniel_Murfet1", "~Susan_Wei1", "~Mingming_Gong1", "huli2@student.unimelb.edu.au", "j.gell@unimelb.edu.au", "thomas.quella@unimelb.edu.au"], "authors": ["Daniel Murfet", "Susan Wei", "Mingming Gong", "Hui Li", "Jesse Gell-Redman", "Thomas Quella"], "keywords": ["deep learning theory", "effective degrees of freedom", "generalisation", "posterior predictive distribution", "real log canonical threshold", "singular learning theory"], "abstract": "In singular models, the optimal set of parameters forms an analytic set with singularities and classical statistical inference cannot be applied to such models. This is significant for deep learning as neural networks are singular and thus ``dividing\" by the determinant of the Hessian or employing the Laplace approximation are not appropriate. Despite its potential for addressing fundamental issues in deep learning, singular learning theory appears to have made little inroads into the developing canon of deep learning theory. Via a mix of theory and experiment, we present an invitation to singular learning theory as a vehicle for understanding deep learning and suggest important future work to make singular learning theory directly applicable to how deep learning is performed in practice. ", "one-sentence_summary": "An invitation to singular learning theory as a theory of deep learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "murfet|deep_learning_is_singular_and_thats_good", "supplementary_material": "/attachment/cdea96d33b68519a8fb04c435d1de90312a35fae.zip", "pdf": "/pdf/055b587f821423ecb63143818cafdc91a9a974af.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0L9nqwO3PE", "_bibtex": "@misc{\nmurfet2021deep,\ntitle={Deep Learning is Singular, and That's Good},\nauthor={Daniel Murfet and Susan Wei and Mingming Gong and Hui Li and Jesse Gell-Redman and Thomas Quella},\nyear={2021},\nurl={https://openreview.net/forum?id=8EGmvcCVrmZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8EGmvcCVrmZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1361/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1361/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1361/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1361/Authors|ICLR.cc/2021/Conference/Paper1361/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1361/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860589, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1361/-/Official_Comment"}}}, {"id": "XBy1AjYYhaA", "original": null, "number": 1, "cdate": 1603852545509, "ddate": null, "tcdate": 1603852545509, "tmdate": 1605024464534, "tddate": null, "forum": "8EGmvcCVrmZ", "replyto": "8EGmvcCVrmZ", "invitation": "ICLR.cc/2021/Conference/Paper1361/-/Official_Review", "content": {"title": "A review of singular learning theory", "review": "This paper is more like a review of singular learning theory and its implication on deep learning. The authors point out that deep neural networks are singular models and ways to characterize generalization error for regular models cannot produce satisfactory results in this setting. Then the authors introduce the singular learning theory, which has been developed for decades. Then, a series of topics for deep learning, such as flatness and generalization, are studied within the framework singular learning theory, with a combination of theoretical analysis and numerical experiments. The paper is clearly written and well organized. \n\npro:\nThe authors point out that the study of deep learning should be put into the framework of singular learning theory. They verified this point from different aspects, where it is shown that results drawn from singular learning theory is better than those drawn from regular learning theory.\n\ncon:\nIt seems most results in the paper are illustration or clarification of existing results.", "rating": "4: Ok but not good enough - rejection", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2021/Conference/Paper1361/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1361/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning is Singular, and That's Good", "authorids": ["~Daniel_Murfet1", "~Susan_Wei1", "~Mingming_Gong1", "huli2@student.unimelb.edu.au", "j.gell@unimelb.edu.au", "thomas.quella@unimelb.edu.au"], "authors": ["Daniel Murfet", "Susan Wei", "Mingming Gong", "Hui Li", "Jesse Gell-Redman", "Thomas Quella"], "keywords": ["deep learning theory", "effective degrees of freedom", "generalisation", "posterior predictive distribution", "real log canonical threshold", "singular learning theory"], "abstract": "In singular models, the optimal set of parameters forms an analytic set with singularities and classical statistical inference cannot be applied to such models. This is significant for deep learning as neural networks are singular and thus ``dividing\" by the determinant of the Hessian or employing the Laplace approximation are not appropriate. Despite its potential for addressing fundamental issues in deep learning, singular learning theory appears to have made little inroads into the developing canon of deep learning theory. Via a mix of theory and experiment, we present an invitation to singular learning theory as a vehicle for understanding deep learning and suggest important future work to make singular learning theory directly applicable to how deep learning is performed in practice. ", "one-sentence_summary": "An invitation to singular learning theory as a theory of deep learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "murfet|deep_learning_is_singular_and_thats_good", "supplementary_material": "/attachment/cdea96d33b68519a8fb04c435d1de90312a35fae.zip", "pdf": "/pdf/055b587f821423ecb63143818cafdc91a9a974af.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0L9nqwO3PE", "_bibtex": "@misc{\nmurfet2021deep,\ntitle={Deep Learning is Singular, and That's Good},\nauthor={Daniel Murfet and Susan Wei and Mingming Gong and Hui Li and Jesse Gell-Redman and Thomas Quella},\nyear={2021},\nurl={https://openreview.net/forum?id=8EGmvcCVrmZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "8EGmvcCVrmZ", "replyto": "8EGmvcCVrmZ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1361/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538120456, "tmdate": 1606915808158, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1361/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1361/-/Official_Review"}}}, {"id": "abyT4sGG61Q", "original": null, "number": 2, "cdate": 1603875637924, "ddate": null, "tcdate": 1603875637924, "tmdate": 1605024464477, "tddate": null, "forum": "8EGmvcCVrmZ", "replyto": "8EGmvcCVrmZ", "invitation": "ICLR.cc/2021/Conference/Paper1361/-/Official_Review", "content": {"title": "The paper studies the connection of Deep Learning to Singular Learning Theory but falls short in convincing why is this the right perspective.", "review": "The paper studies the connection of Deep Learning to Singular Learning Theory making the claim that the later could be a good foundation of the theory of Neural Networks as they are singular models. \n\n+++++++\n\nWhile I enjoyed the primer to singular learning theory, I found the paper's contribution marginal given the related work. The claim 'deep learning is singular' has already been claimed by Watanabe 07' in a much more general statement.  Moreover, each of the claims the authors made is unconvincing both from a theoretical and experimental perspectives. \nFor instance, the claim that RLCT governs the effective number of parameters. While the math looks sound to me, it has both has many strong assumptions on the model and lack of novelty given Watanbe 09'. To show that the calculation is meaningful, I would suggest showing the relationship to a real neural network and see how close the estimate to the real number of parameters (E.g,. in the random features (Rahimi & Recht 2008) model, the number of features is a good way to measure the effective number of parameters. Another way to estimate the real number of parameters is the location of the double descent peak (Belkin 18'). \n\nRegarding the empirical claims, the experiments are not convincing. In order to make a claim about deep learning, (say the bayes predictive error is superior to MAP or MLE) there should be either an extensive experimental demonstration of the claim (not two experiments) with proper ablation of when the claim fails (not to mention that the std of the experimental results puts all the claims to question). Same comments apply to the last section.\n\nTo conclude, my main comment is that while Deep Learning is singular (which is not a deep claim) to make the statement that the right way to study deep learning is by singularity theory, I would like to see either experiments or theory that give me insights about deep learning. This could be done by example by doing a theory on a toy model and showing that it holds for real deep learning applications (or at least for an array of synthetic distributions). I would like to qualify by saying that statistics is not my main field of study and I would be happy to receive clarifications if I misunderstood anything. ", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1361/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1361/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning is Singular, and That's Good", "authorids": ["~Daniel_Murfet1", "~Susan_Wei1", "~Mingming_Gong1", "huli2@student.unimelb.edu.au", "j.gell@unimelb.edu.au", "thomas.quella@unimelb.edu.au"], "authors": ["Daniel Murfet", "Susan Wei", "Mingming Gong", "Hui Li", "Jesse Gell-Redman", "Thomas Quella"], "keywords": ["deep learning theory", "effective degrees of freedom", "generalisation", "posterior predictive distribution", "real log canonical threshold", "singular learning theory"], "abstract": "In singular models, the optimal set of parameters forms an analytic set with singularities and classical statistical inference cannot be applied to such models. This is significant for deep learning as neural networks are singular and thus ``dividing\" by the determinant of the Hessian or employing the Laplace approximation are not appropriate. Despite its potential for addressing fundamental issues in deep learning, singular learning theory appears to have made little inroads into the developing canon of deep learning theory. Via a mix of theory and experiment, we present an invitation to singular learning theory as a vehicle for understanding deep learning and suggest important future work to make singular learning theory directly applicable to how deep learning is performed in practice. ", "one-sentence_summary": "An invitation to singular learning theory as a theory of deep learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "murfet|deep_learning_is_singular_and_thats_good", "supplementary_material": "/attachment/cdea96d33b68519a8fb04c435d1de90312a35fae.zip", "pdf": "/pdf/055b587f821423ecb63143818cafdc91a9a974af.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0L9nqwO3PE", "_bibtex": "@misc{\nmurfet2021deep,\ntitle={Deep Learning is Singular, and That's Good},\nauthor={Daniel Murfet and Susan Wei and Mingming Gong and Hui Li and Jesse Gell-Redman and Thomas Quella},\nyear={2021},\nurl={https://openreview.net/forum?id=8EGmvcCVrmZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "8EGmvcCVrmZ", "replyto": "8EGmvcCVrmZ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1361/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538120456, "tmdate": 1606915808158, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1361/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1361/-/Official_Review"}}}, {"id": "CgWmojHjqXf", "original": null, "number": 3, "cdate": 1603986936520, "ddate": null, "tcdate": 1603986936520, "tmdate": 1605024464413, "tddate": null, "forum": "8EGmvcCVrmZ", "replyto": "8EGmvcCVrmZ", "invitation": "ICLR.cc/2021/Conference/Paper1361/-/Official_Review", "content": {"title": "insufficient intellectual contributions", "review": "This paper studies the manifold of the weights in a neural network. The paper discusses the singular learning theory approach of Sumio Watanabe and argues for more exploration of this theory for understanding generalization performance of deep networks.\n\nMy opinion of this paper is lukewarm. There is a large amount of existing work on singular learning theory. I agree with the paper that the approach is a promising direction to understand generalization in deep learning. However, the intellectual contributions of this particular paper to the existing literature are difficult to ascertain and insufficient warrant publication.\n\nSome comments.\n1. I like the section on how flatness of the energy landscape connects with Real log canonical threshold (RLCT). The RLCT gives a much more refined treatment of different measures of local geometry of the energy landscape used in the literature (Hessian-based, local-entropy based, Bayes free energy etc.).\n2. The paper would benefit from Appendix A.1-4 to the main text. Some of these calculations have appeared elsewhere in the literature (see the work on K-FAC for the Fisher information) but the development in A.2-A.4 is interesting.\n\nSome questions that the authors could think about.\n1. The singular learning theory argues that the Bayes predictive distribution generalizes better MLE/MAP. How does this explain the fact that one sample from the posterior distribution also results in good generalization for deep networks?\n2. Singular Learning Theory is not new. While it is true that this approach has been overlooked in the recent work on deep learning, better experimental evidence is necessary in order to make a convincing case that it is a promising one. The results shown here are accurate computations using MCMC for small models. Can you also investigate whether one obtains non-vacuous generalization bounds for large deep networks using an estimate for the RLCT?\n3. Section 7: Is the true distribution of data really unrealizable? The reviewer is of the opinion that the fact that we can learn very good generative models for complex data indicates that it might be more viable to study the case when the true distribution is realizable for large-enough models.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1361/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1361/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning is Singular, and That's Good", "authorids": ["~Daniel_Murfet1", "~Susan_Wei1", "~Mingming_Gong1", "huli2@student.unimelb.edu.au", "j.gell@unimelb.edu.au", "thomas.quella@unimelb.edu.au"], "authors": ["Daniel Murfet", "Susan Wei", "Mingming Gong", "Hui Li", "Jesse Gell-Redman", "Thomas Quella"], "keywords": ["deep learning theory", "effective degrees of freedom", "generalisation", "posterior predictive distribution", "real log canonical threshold", "singular learning theory"], "abstract": "In singular models, the optimal set of parameters forms an analytic set with singularities and classical statistical inference cannot be applied to such models. This is significant for deep learning as neural networks are singular and thus ``dividing\" by the determinant of the Hessian or employing the Laplace approximation are not appropriate. Despite its potential for addressing fundamental issues in deep learning, singular learning theory appears to have made little inroads into the developing canon of deep learning theory. Via a mix of theory and experiment, we present an invitation to singular learning theory as a vehicle for understanding deep learning and suggest important future work to make singular learning theory directly applicable to how deep learning is performed in practice. ", "one-sentence_summary": "An invitation to singular learning theory as a theory of deep learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "murfet|deep_learning_is_singular_and_thats_good", "supplementary_material": "/attachment/cdea96d33b68519a8fb04c435d1de90312a35fae.zip", "pdf": "/pdf/055b587f821423ecb63143818cafdc91a9a974af.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0L9nqwO3PE", "_bibtex": "@misc{\nmurfet2021deep,\ntitle={Deep Learning is Singular, and That's Good},\nauthor={Daniel Murfet and Susan Wei and Mingming Gong and Hui Li and Jesse Gell-Redman and Thomas Quella},\nyear={2021},\nurl={https://openreview.net/forum?id=8EGmvcCVrmZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "8EGmvcCVrmZ", "replyto": "8EGmvcCVrmZ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1361/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538120456, "tmdate": 1606915808158, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1361/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1361/-/Official_Review"}}}, {"id": "ioMbxZGoJrB", "original": null, "number": 4, "cdate": 1603992145409, "ddate": null, "tcdate": 1603992145409, "tmdate": 1605024464340, "tddate": null, "forum": "8EGmvcCVrmZ", "replyto": "8EGmvcCVrmZ", "invitation": "ICLR.cc/2021/Conference/Paper1361/-/Official_Review", "content": {"title": "Useful probabilistic framework for singularity of NNs", "review": "The paper is a terse account of singularity of deep learning with a probabilistic view. Clearly written, gives an overview of the contributions and related work quickly and dives into the setup. The main byproduct of singularity is the inapplicability of classical methods. This is no news to many people in the field, yet I find the perspective provided in this work fresh and I think it has potential for further developments, although its current applicability is limited and it doesn\u2019t say something that was not already known. Here are further comments:\n\n- The paper would benefit a lot from clearly laying out the concepts and definitions. Especially section 3 would benefit a lot from such clarity and would help a wider audience to follow the work. \n- The end of section 3 contains the key idea and would benefit from further clarity.\n- How does the measure compare with the other frameworks? What does it imply for existing models (examples)?\n- What is the relevance of the tasks and experiments at the end of section 5? How can one move from regression to high dimensional classification tasks?\n- In light of the under/over-parametrization debate, can the framework account for such a phase transition, or can the experiments reflect this?\n- What are the promises of this framework? What benefits would people in the community expect if they study this?\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1361/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1361/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning is Singular, and That's Good", "authorids": ["~Daniel_Murfet1", "~Susan_Wei1", "~Mingming_Gong1", "huli2@student.unimelb.edu.au", "j.gell@unimelb.edu.au", "thomas.quella@unimelb.edu.au"], "authors": ["Daniel Murfet", "Susan Wei", "Mingming Gong", "Hui Li", "Jesse Gell-Redman", "Thomas Quella"], "keywords": ["deep learning theory", "effective degrees of freedom", "generalisation", "posterior predictive distribution", "real log canonical threshold", "singular learning theory"], "abstract": "In singular models, the optimal set of parameters forms an analytic set with singularities and classical statistical inference cannot be applied to such models. This is significant for deep learning as neural networks are singular and thus ``dividing\" by the determinant of the Hessian or employing the Laplace approximation are not appropriate. Despite its potential for addressing fundamental issues in deep learning, singular learning theory appears to have made little inroads into the developing canon of deep learning theory. Via a mix of theory and experiment, we present an invitation to singular learning theory as a vehicle for understanding deep learning and suggest important future work to make singular learning theory directly applicable to how deep learning is performed in practice. ", "one-sentence_summary": "An invitation to singular learning theory as a theory of deep learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "murfet|deep_learning_is_singular_and_thats_good", "supplementary_material": "/attachment/cdea96d33b68519a8fb04c435d1de90312a35fae.zip", "pdf": "/pdf/055b587f821423ecb63143818cafdc91a9a974af.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0L9nqwO3PE", "_bibtex": "@misc{\nmurfet2021deep,\ntitle={Deep Learning is Singular, and That's Good},\nauthor={Daniel Murfet and Susan Wei and Mingming Gong and Hui Li and Jesse Gell-Redman and Thomas Quella},\nyear={2021},\nurl={https://openreview.net/forum?id=8EGmvcCVrmZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "8EGmvcCVrmZ", "replyto": "8EGmvcCVrmZ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1361/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538120456, "tmdate": 1606915808158, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1361/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1361/-/Official_Review"}}}], "count": 10}