{"notes": [{"id": "H1gHELLK_V", "original": "Bklzt115PE", "number": 21, "cdate": 1553716780781, "ddate": null, "tcdate": 1553716780781, "tmdate": 1571693827865, "tddate": null, "forum": "H1gHELLK_V", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Blind_Submission", "content": {"title": "Learning to Defense by Learning to Attack", "authors": ["Zhehui Chen", "Haoming Jiang", "Yuyang Shi", "Bo Dai", "Tuo Zhao"], "authorids": ["zhchen@gatech.edu", "jianghm.ustc@gmail.com", "yyshi@gatech.edu", "bohr.dai@gmail.com", "tuo.zhao@isye.gatech.edu"], "keywords": ["Adversarial Training", "Learning to Learn/Optimize", "Nonconvex-Nonconcave Minmax Optimization"], "TL;DR": "Don't know how to optimize? Then just learn to optimize!", "abstract": "Adversarial training provides a principled approach for training robust neural networks. From an optimization perspective, the adversarial training is essentially solving a minmax robust optimization problem. The outer minimization is trying to learn a robust classifier, while the inner maximization is trying to generate adversarial samples. Unfortunately, such a minmax problem is very difficult to solve due to the lack of convex-concave structure. This work proposes a new adversarial training method based on a general learning-to-learn framework. Specifically, instead of applying the existing hand-design algorithms for the inner problem, we learn an optimizer, which is parametrized as a convolutional neural network. At the same time, a robust classifier is learned to defense the adversarial attack generated by the learned optimizer. From the perspective of generative learning, our proposed method can be viewed as learning a deep generative model for generating adversarial samples, which is adaptive to the robust classification. Our experiments demonstrate that our proposed method significantly outperforms existing adversarial training methods on CIFAR-10 and CIFAR-100 datasets.", "pdf": "/pdf/cbed39826dde09889c32d2c4a4cb200443b685e8.pdf", "paperhash": "chen|learning_to_defense_by_learning_to_attack"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Blind_Submission", "cdate": 1547567085825, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": [".*"]}, "writers": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1547567085825, "tmdate": 1555704438520, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}}, "tauthor": "OpenReview.net"}, {"id": "Sye8z12W5E", "original": null, "number": 2, "cdate": 1555312398102, "ddate": null, "tcdate": 1555312398102, "tmdate": 1556906138406, "tddate": null, "forum": "H1gHELLK_V", "replyto": "H1gHELLK_V", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper21/Official_Review", "content": {"title": "Very Interesting Idea", "review": "This paper proposes a very interesting idea, using the learning-to-learn framework to learn an attacker. I find this idea very novel in the literature and in retrospect, very natural. Furthermore, I believe using L2L framework to this adversarial setting is very promising as we can naturally generate many samples to fit L2L framework.\n\nThe experiments also look promising. I think this is a strong paper.", "rating": "5: Top 15% of accepted papers, strong accept", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper21/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper21/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Defense by Learning to Attack", "authors": ["Zhehui Chen", "Haoming Jiang", "Yuyang Shi", "Bo Dai", "Tuo Zhao"], "authorids": ["zhchen@gatech.edu", "jianghm.ustc@gmail.com", "yyshi@gatech.edu", "bohr.dai@gmail.com", "tuo.zhao@isye.gatech.edu"], "keywords": ["Adversarial Training", "Learning to Learn/Optimize", "Nonconvex-Nonconcave Minmax Optimization"], "TL;DR": "Don't know how to optimize? Then just learn to optimize!", "abstract": "Adversarial training provides a principled approach for training robust neural networks. From an optimization perspective, the adversarial training is essentially solving a minmax robust optimization problem. The outer minimization is trying to learn a robust classifier, while the inner maximization is trying to generate adversarial samples. Unfortunately, such a minmax problem is very difficult to solve due to the lack of convex-concave structure. This work proposes a new adversarial training method based on a general learning-to-learn framework. Specifically, instead of applying the existing hand-design algorithms for the inner problem, we learn an optimizer, which is parametrized as a convolutional neural network. At the same time, a robust classifier is learned to defense the adversarial attack generated by the learned optimizer. From the perspective of generative learning, our proposed method can be viewed as learning a deep generative model for generating adversarial samples, which is adaptive to the robust classification. Our experiments demonstrate that our proposed method significantly outperforms existing adversarial training methods on CIFAR-10 and CIFAR-100 datasets.", "pdf": "/pdf/cbed39826dde09889c32d2c4a4cb200443b685e8.pdf", "paperhash": "chen|learning_to_defense_by_learning_to_attack"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper21/Official_Review", "cdate": 1554234176957, "reply": {"forum": "H1gHELLK_V", "replyto": "H1gHELLK_V", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper21/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper21/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1554234176957, "tmdate": 1556906087647, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper21/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}, {"id": "Hyevqfi-5V", "original": null, "number": 1, "cdate": 1555309198538, "ddate": null, "tcdate": 1555309198538, "tmdate": 1556906138194, "tddate": null, "forum": "H1gHELLK_V", "replyto": "H1gHELLK_V", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper21/Official_Review", "content": {"title": "Good paper, though of unclear relevance", "review": "This paper proposes a way to train image classification models to be resistant to L-infinity perturbation attacks. The idea is to simultaneously learn the classification model and an adversary model that adds L-infinity-bounded perturbations to images, in order to maximally confuse the first model. This adversary model can use not only the image itself, but also gradient information from the classification model. It can even propose a perturbation, get gradient information on that perturbation, and then propose an updated perturbation, similarly to how projected gradient descent (PGD) can take multiple gradient steps to find a perturbation.\n\nThe main results are that training in this way improves adversarial accuracy compared to PGD, while improving training speed. On CIFAR-10 with epsilon=0.03, the proposed method gets 51.5% accuracy against a PGD adversary, whereas the PGD-trained model gets 40.7% accuracy. Madry et al. (2017) report better accuracy of 47%, but the proposed method still improves upon this. Moreover, the model-based adversary is faster than PGD, as shown by faster training times (more than 2x faster to get similar accuracy as PGD, and the best model is still about 50% faster).\n\nOverall, I believe the paper is above the acceptance threshold from a quality perspective, and likely in the top 50% of accepted papers. However, it may not be a good fit for the topic of this workshop. Technically you could argue that the adversary is synthesizing a perturbation to an image, so this is some sort of structured generation. Therefore I give an overall rating of 3, and defer to the workshop organizers regarding appropriateness.\n\nMinor note: In Algorithm 2, I think g(x_i, u_i; \\phi) should use the \\mathcal{A} notation used elsewhere.", "rating": "3: Marginally above acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper21/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper21/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Defense by Learning to Attack", "authors": ["Zhehui Chen", "Haoming Jiang", "Yuyang Shi", "Bo Dai", "Tuo Zhao"], "authorids": ["zhchen@gatech.edu", "jianghm.ustc@gmail.com", "yyshi@gatech.edu", "bohr.dai@gmail.com", "tuo.zhao@isye.gatech.edu"], "keywords": ["Adversarial Training", "Learning to Learn/Optimize", "Nonconvex-Nonconcave Minmax Optimization"], "TL;DR": "Don't know how to optimize? Then just learn to optimize!", "abstract": "Adversarial training provides a principled approach for training robust neural networks. From an optimization perspective, the adversarial training is essentially solving a minmax robust optimization problem. The outer minimization is trying to learn a robust classifier, while the inner maximization is trying to generate adversarial samples. Unfortunately, such a minmax problem is very difficult to solve due to the lack of convex-concave structure. This work proposes a new adversarial training method based on a general learning-to-learn framework. Specifically, instead of applying the existing hand-design algorithms for the inner problem, we learn an optimizer, which is parametrized as a convolutional neural network. At the same time, a robust classifier is learned to defense the adversarial attack generated by the learned optimizer. From the perspective of generative learning, our proposed method can be viewed as learning a deep generative model for generating adversarial samples, which is adaptive to the robust classification. Our experiments demonstrate that our proposed method significantly outperforms existing adversarial training methods on CIFAR-10 and CIFAR-100 datasets.", "pdf": "/pdf/cbed39826dde09889c32d2c4a4cb200443b685e8.pdf", "paperhash": "chen|learning_to_defense_by_learning_to_attack"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper21/Official_Review", "cdate": 1554234176957, "reply": {"forum": "H1gHELLK_V", "replyto": "H1gHELLK_V", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper21/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper21/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1554234176957, "tmdate": 1556906087647, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper21/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}, {"id": "H1xPWEOP9E", "original": null, "number": 1, "cdate": 1555690495199, "ddate": null, "tcdate": 1555690495199, "tmdate": 1556906137973, "tddate": null, "forum": "H1gHELLK_V", "replyto": "H1gHELLK_V", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper21/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Defense by Learning to Attack", "authors": ["Zhehui Chen", "Haoming Jiang", "Yuyang Shi", "Bo Dai", "Tuo Zhao"], "authorids": ["zhchen@gatech.edu", "jianghm.ustc@gmail.com", "yyshi@gatech.edu", "bohr.dai@gmail.com", "tuo.zhao@isye.gatech.edu"], "keywords": ["Adversarial Training", "Learning to Learn/Optimize", "Nonconvex-Nonconcave Minmax Optimization"], "TL;DR": "Don't know how to optimize? Then just learn to optimize!", "abstract": "Adversarial training provides a principled approach for training robust neural networks. From an optimization perspective, the adversarial training is essentially solving a minmax robust optimization problem. The outer minimization is trying to learn a robust classifier, while the inner maximization is trying to generate adversarial samples. Unfortunately, such a minmax problem is very difficult to solve due to the lack of convex-concave structure. This work proposes a new adversarial training method based on a general learning-to-learn framework. Specifically, instead of applying the existing hand-design algorithms for the inner problem, we learn an optimizer, which is parametrized as a convolutional neural network. At the same time, a robust classifier is learned to defense the adversarial attack generated by the learned optimizer. From the perspective of generative learning, our proposed method can be viewed as learning a deep generative model for generating adversarial samples, which is adaptive to the robust classification. Our experiments demonstrate that our proposed method significantly outperforms existing adversarial training methods on CIFAR-10 and CIFAR-100 datasets.", "pdf": "/pdf/cbed39826dde09889c32d2c4a4cb200443b685e8.pdf", "paperhash": "chen|learning_to_defense_by_learning_to_attack"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper21/Decision", "cdate": 1554814607642, "reply": {"forum": "H1gHELLK_V", "replyto": "H1gHELLK_V", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554814607642, "tmdate": 1556906098019, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}], "count": 4}