{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392928680000, "tcdate": 1392928680000, "number": 9, "id": "EnLfESm5kXnRD", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "kSHxSr1TPt8XB", "replyto": "kSHxSr1TPt8XB", "signatures": ["Thomas Paine"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We would like to thank the reviews for their comments.\r\n\r\nTo Anonymous 4f82:\r\nThank you for the comments. All your points are good ones. Exploring the effect of model size, and minibatch size vs performance is important. We will look into this for future work. We also agree that second order methods could be a great help here.\r\n\r\nTo Anonymous 6693:\r\nWe agree that our work builds directly on recent developments in high performance neural network training.\r\n\r\nWe would like to emphasize our contribution is exploring the benefits of combining these approaches, and making the results available to the community. To date no group has published results on GPUs, and distributed computing with neural networks of this scale. And we think overall this is a very promising direction.\r\n\r\nThank you."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training", "decision": "submitted, no decision", "abstract": "The ability to train large-scale neural networks has resulted in state-of-the-art performance in many areas of computer vision. These results have largely come from computational break throughs of two forms: model parallelism, e.g. GPU accelerated training, which has seen quick adoption in computer vision circles, and data parallelism, e.g. A-SGD, whose large scale has been used mostly in industry. We report early experiments with a system that makes use of both model parallelism and data parallelism, we call GPU A-SGD. We show using GPU A-SGD it is possible to speed up training of large convolutional neural networks useful for computer vision. We believe GPU A-SGD will make it possible to train larger networks on larger training sets in a reasonable amount of time.", "pdf": "https://arxiv.org/abs/1312.6186", "paperhash": "jin|gpu_asynchronous_stochastic_gradient_descent_to_speed_up_neural_network_training", "keywords": [], "conflicts": [], "authors": ["Hailin Jin", "Thomas Huang", "Zhe Lin", "Jianchao Yang", "Thomas Paine"], "authorids": ["hljin@adobe.com", "huang@ifp.uiuc.edu", "zlin@adobe.com", "jiayang@adobe.com", "tom.le.paine@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392065340000, "tcdate": 1392065340000, "number": 11, "id": "CC39DxlVuyfI0", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "kSHxSr1TPt8XB", "replyto": "kSHxSr1TPt8XB", "signatures": ["anonymous reviewer 6693"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training", "review": "Multi-computer GPU training of large networks is an important current topic for representation learning in industry-scale convnets. This paper describes ongoing efforts to combine model parallelism and data parallelism to reduce training time on the ILSVRC 2012 data set.\r\n\r\nPro:\r\n\r\n- they achieve several-fold reductions in runtime over Khrizhevsky's landmark implementation\r\n\r\nCon:\r\n\r\n- I'm not sure that there is significant novelty in their approach, relative to dist-Belief and existing work on asynchronous-SGD."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training", "decision": "submitted, no decision", "abstract": "The ability to train large-scale neural networks has resulted in state-of-the-art performance in many areas of computer vision. These results have largely come from computational break throughs of two forms: model parallelism, e.g. GPU accelerated training, which has seen quick adoption in computer vision circles, and data parallelism, e.g. A-SGD, whose large scale has been used mostly in industry. We report early experiments with a system that makes use of both model parallelism and data parallelism, we call GPU A-SGD. We show using GPU A-SGD it is possible to speed up training of large convolutional neural networks useful for computer vision. We believe GPU A-SGD will make it possible to train larger networks on larger training sets in a reasonable amount of time.", "pdf": "https://arxiv.org/abs/1312.6186", "paperhash": "jin|gpu_asynchronous_stochastic_gradient_descent_to_speed_up_neural_network_training", "keywords": [], "conflicts": [], "authors": ["Hailin Jin", "Thomas Huang", "Zhe Lin", "Jianchao Yang", "Thomas Paine"], "authorids": ["hljin@adobe.com", "huang@ifp.uiuc.edu", "zlin@adobe.com", "jiayang@adobe.com", "tom.le.paine@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391821200000, "tcdate": 1391821200000, "number": 10, "id": "-xjY-GMQtwuQN", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "kSHxSr1TPt8XB", "replyto": "kSHxSr1TPt8XB", "signatures": ["anonymous reviewer 4f82"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training", "review": "Summary\r\n------------\r\n\r\nThe paper explores running A-SGD as an approach for speeding up learning.\r\nOverall I think these are very interesting and informative results. Specifically for a workshop paper I believe the paper contains enough novelty and empirical exploration.\r\n\r\nComments:\r\n--------------\r\n\r\nIt would be interesting to try to quantify how much the size of the model influences these results. In particular I'm wondering of how the performance drops with the size of the gradients that need to be send over the network. \r\n\r\nAnother interesting plot will be to look at the size of the minibatch and how that influence convergence.\r\n\r\nI hypothesis that distributed algorithms where the parallelism is made over the data (rather than model), like it is done here at the node level, will benefit a lot more from complicated optimization techniques rather that SGD (even in its asynchronous version). It feels to me that with large models there is a high price to pay for sending the gradients over the network (case an point, n_sync is usually set to something higher than 1). We want to use an algorithm for which each step is itself expensive (and hence we have to send fewer gradients over the network) but that needs much less steps to converge. You can make each step mSGD arbitrarily expensive by increasing the minibatch size, though SGD is fairly inefficient at utilizing these large minibatches.\r\nI believe that distributing computation along data for deep models makes a lot more sense with algorithms such as second order methods or variants of natural  gradient."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training", "decision": "submitted, no decision", "abstract": "The ability to train large-scale neural networks has resulted in state-of-the-art performance in many areas of computer vision. These results have largely come from computational break throughs of two forms: model parallelism, e.g. GPU accelerated training, which has seen quick adoption in computer vision circles, and data parallelism, e.g. A-SGD, whose large scale has been used mostly in industry. We report early experiments with a system that makes use of both model parallelism and data parallelism, we call GPU A-SGD. We show using GPU A-SGD it is possible to speed up training of large convolutional neural networks useful for computer vision. We believe GPU A-SGD will make it possible to train larger networks on larger training sets in a reasonable amount of time.", "pdf": "https://arxiv.org/abs/1312.6186", "paperhash": "jin|gpu_asynchronous_stochastic_gradient_descent_to_speed_up_neural_network_training", "keywords": [], "conflicts": [], "authors": ["Hailin Jin", "Thomas Huang", "Zhe Lin", "Jianchao Yang", "Thomas Paine"], "authorids": ["hljin@adobe.com", "huang@ifp.uiuc.edu", "zlin@adobe.com", "jiayang@adobe.com", "tom.le.paine@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391657880000, "tcdate": 1391657880000, "number": 1, "id": "80DMXtygJV8Tm", "invitation": "ICLR.cc/2014/-/submission/workshop/reply", "forum": "kSHxSr1TPt8XB", "replyto": "OOTVKwQ9I7HkZ", "signatures": ["Thomas Paine"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Hi Liangliang,\r\nThanks for reading. And yes, during submission, the fields auto-populated in the wrong order. Prof Huang is the last author. I am the first. Sorry for the confusion.\r\n\r\nAll the figures plot the error on training minibatches of 128 images. The plots show the error for minibatches on one client. Plots are comparable across clients.\r\n\r\nDue to time constraints we didn't change learning rates on these later experiments. But instead focused on initial training speed increases. \r\n\r\nWe also measured on a validation set, and found that for these settings we see similar gains in validation set performance."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training", "decision": "submitted, no decision", "abstract": "The ability to train large-scale neural networks has resulted in state-of-the-art performance in many areas of computer vision. These results have largely come from computational break throughs of two forms: model parallelism, e.g. GPU accelerated training, which has seen quick adoption in computer vision circles, and data parallelism, e.g. A-SGD, whose large scale has been used mostly in industry. We report early experiments with a system that makes use of both model parallelism and data parallelism, we call GPU A-SGD. We show using GPU A-SGD it is possible to speed up training of large convolutional neural networks useful for computer vision. We believe GPU A-SGD will make it possible to train larger networks on larger training sets in a reasonable amount of time.", "pdf": "https://arxiv.org/abs/1312.6186", "paperhash": "jin|gpu_asynchronous_stochastic_gradient_descent_to_speed_up_neural_network_training", "keywords": [], "conflicts": [], "authors": ["Hailin Jin", "Thomas Huang", "Zhe Lin", "Jianchao Yang", "Thomas Paine"], "authorids": ["hljin@adobe.com", "huang@ifp.uiuc.edu", "zlin@adobe.com", "jiayang@adobe.com", "tom.le.paine@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391646480000, "tcdate": 1391646480000, "number": 5, "id": "OOTVKwQ9I7HkZ", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "kSHxSr1TPt8XB", "replyto": "kSHxSr1TPt8XB", "signatures": ["Liangliang Cao"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Interesting work. And it is also amusing to see the authorlist on this page. There may be a typo but from my understanding of the authors I believe the first author (Prof. Huang) did all the GPU programming and reported to the last author (Thomas Paine).\r\n\r\nOne thing confuses me is how did you measure the training error in Figure 2-4. Are these numbers from the whole training set (1.2M) or a batch? Did you change learning rate? Or measure on the validation set?\r\n\r\nAnother confuse which is totally my fault: at the beginning I thought A-SGD stands for Average SGD!"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training", "decision": "submitted, no decision", "abstract": "The ability to train large-scale neural networks has resulted in state-of-the-art performance in many areas of computer vision. These results have largely come from computational break throughs of two forms: model parallelism, e.g. GPU accelerated training, which has seen quick adoption in computer vision circles, and data parallelism, e.g. A-SGD, whose large scale has been used mostly in industry. We report early experiments with a system that makes use of both model parallelism and data parallelism, we call GPU A-SGD. We show using GPU A-SGD it is possible to speed up training of large convolutional neural networks useful for computer vision. We believe GPU A-SGD will make it possible to train larger networks on larger training sets in a reasonable amount of time.", "pdf": "https://arxiv.org/abs/1312.6186", "paperhash": "jin|gpu_asynchronous_stochastic_gradient_descent_to_speed_up_neural_network_training", "keywords": [], "conflicts": [], "authors": ["Hailin Jin", "Thomas Huang", "Zhe Lin", "Jianchao Yang", "Thomas Paine"], "authorids": ["hljin@adobe.com", "huang@ifp.uiuc.edu", "zlin@adobe.com", "jiayang@adobe.com", "tom.le.paine@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391228280000, "tcdate": 1391228280000, "number": 1, "id": "JxHIWtr0U5xjb", "invitation": "ICLR.cc/2014/-/submission/workshop/reply", "forum": "kSHxSr1TPt8XB", "replyto": "m4KEFoJGxcmDU", "signatures": ["Thomas Paine"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Hi Marc,\r\nThanks for reading the paper.\r\n\r\nYes, I think the plots you suggest make sense. We thought our minibatch measure was sensible, but we want our plots to be as clear and useful as possible so we will change them for the final version of the paper.\r\n\r\na) At the time of publication we didn't try more frequent updates, but we have been trying those recently.\r\n\r\nb) On Bluewaters, every server has 1 GPU node. So we weren't able to leverage same board communication, but that would be great."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training", "decision": "submitted, no decision", "abstract": "The ability to train large-scale neural networks has resulted in state-of-the-art performance in many areas of computer vision. These results have largely come from computational break throughs of two forms: model parallelism, e.g. GPU accelerated training, which has seen quick adoption in computer vision circles, and data parallelism, e.g. A-SGD, whose large scale has been used mostly in industry. We report early experiments with a system that makes use of both model parallelism and data parallelism, we call GPU A-SGD. We show using GPU A-SGD it is possible to speed up training of large convolutional neural networks useful for computer vision. We believe GPU A-SGD will make it possible to train larger networks on larger training sets in a reasonable amount of time.", "pdf": "https://arxiv.org/abs/1312.6186", "paperhash": "jin|gpu_asynchronous_stochastic_gradient_descent_to_speed_up_neural_network_training", "keywords": [], "conflicts": [], "authors": ["Hailin Jin", "Thomas Huang", "Zhe Lin", "Jianchao Yang", "Thomas Paine"], "authorids": ["hljin@adobe.com", "huang@ifp.uiuc.edu", "zlin@adobe.com", "jiayang@adobe.com", "tom.le.paine@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390867200000, "tcdate": 1390867200000, "number": 7, "id": "BUh4cSvQWDBQi", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "kSHxSr1TPt8XB", "replyto": "kSHxSr1TPt8XB", "signatures": ["Marc'Aurelio Ranzato"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "In general, I think it would make more sense to report test and training errors (y-axis) versus time (x-axis).  This is what we are interested in when we try to speed up convergence, not how many weight updates or samples we process. Since all your experiments use the same kind of GPU, the comparison is fair.\r\n\r\nQuestions: \r\na) have you tried to synchronize even more frequently (n_sync=1/10/50)?\r\nb) is every node on a different server? If not, do you leverage the fact that communication can be less costly when boards are on the same server?\r\n\r\nThank you."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training", "decision": "submitted, no decision", "abstract": "The ability to train large-scale neural networks has resulted in state-of-the-art performance in many areas of computer vision. These results have largely come from computational break throughs of two forms: model parallelism, e.g. GPU accelerated training, which has seen quick adoption in computer vision circles, and data parallelism, e.g. A-SGD, whose large scale has been used mostly in industry. We report early experiments with a system that makes use of both model parallelism and data parallelism, we call GPU A-SGD. We show using GPU A-SGD it is possible to speed up training of large convolutional neural networks useful for computer vision. We believe GPU A-SGD will make it possible to train larger networks on larger training sets in a reasonable amount of time.", "pdf": "https://arxiv.org/abs/1312.6186", "paperhash": "jin|gpu_asynchronous_stochastic_gradient_descent_to_speed_up_neural_network_training", "keywords": [], "conflicts": [], "authors": ["Hailin Jin", "Thomas Huang", "Zhe Lin", "Jianchao Yang", "Thomas Paine"], "authorids": ["hljin@adobe.com", "huang@ifp.uiuc.edu", "zlin@adobe.com", "jiayang@adobe.com", "tom.le.paine@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390867200000, "tcdate": 1390867200000, "number": 6, "id": "cBv9AZMeK_O3x", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "kSHxSr1TPt8XB", "replyto": "kSHxSr1TPt8XB", "signatures": ["Marc'Aurelio Ranzato"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "In general, I think it would make more sense to report test and training errors (y-axis) versus time (x-axis).  This is what we are interested in when we try to speed up convergence, not how many weight updates or samples we process. Since all your experiments use the same kind of GPU, the comparison is fair.\r\n\r\nQuestions: \r\na) have you tried to synchronize even more frequently (n_sync=1/10/50)?\r\nb) is every node on a different server? If not, do you leverage the fact that communication can be less costly when boards are on the same server?\r\n\r\nThank you."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training", "decision": "submitted, no decision", "abstract": "The ability to train large-scale neural networks has resulted in state-of-the-art performance in many areas of computer vision. These results have largely come from computational break throughs of two forms: model parallelism, e.g. GPU accelerated training, which has seen quick adoption in computer vision circles, and data parallelism, e.g. A-SGD, whose large scale has been used mostly in industry. We report early experiments with a system that makes use of both model parallelism and data parallelism, we call GPU A-SGD. We show using GPU A-SGD it is possible to speed up training of large convolutional neural networks useful for computer vision. We believe GPU A-SGD will make it possible to train larger networks on larger training sets in a reasonable amount of time.", "pdf": "https://arxiv.org/abs/1312.6186", "paperhash": "jin|gpu_asynchronous_stochastic_gradient_descent_to_speed_up_neural_network_training", "keywords": [], "conflicts": [], "authors": ["Hailin Jin", "Thomas Huang", "Zhe Lin", "Jianchao Yang", "Thomas Paine"], "authorids": ["hljin@adobe.com", "huang@ifp.uiuc.edu", "zlin@adobe.com", "jiayang@adobe.com", "tom.le.paine@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390867200000, "tcdate": 1390867200000, "number": 8, "id": "m4KEFoJGxcmDU", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "kSHxSr1TPt8XB", "replyto": "kSHxSr1TPt8XB", "signatures": ["Marc'Aurelio Ranzato"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "In general, I think it would make more sense to report test and training errors (y-axis) versus time (x-axis).  This is what we are interested in when we try to speed up convergence, not how many weight updates or samples we process. Since all your experiments use the same kind of GPU, the comparison is fair.\r\n\r\nQuestions: \r\na) have you tried to synchronize even more frequently (n_sync=1/10/50)?\r\nb) is every node on a different server? If not, do you leverage the fact that communication can be less costly when boards are on the same server?\r\n\r\nThank you."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training", "decision": "submitted, no decision", "abstract": "The ability to train large-scale neural networks has resulted in state-of-the-art performance in many areas of computer vision. These results have largely come from computational break throughs of two forms: model parallelism, e.g. GPU accelerated training, which has seen quick adoption in computer vision circles, and data parallelism, e.g. A-SGD, whose large scale has been used mostly in industry. We report early experiments with a system that makes use of both model parallelism and data parallelism, we call GPU A-SGD. We show using GPU A-SGD it is possible to speed up training of large convolutional neural networks useful for computer vision. We believe GPU A-SGD will make it possible to train larger networks on larger training sets in a reasonable amount of time.", "pdf": "https://arxiv.org/abs/1312.6186", "paperhash": "jin|gpu_asynchronous_stochastic_gradient_descent_to_speed_up_neural_network_training", "keywords": [], "conflicts": [], "authors": ["Hailin Jin", "Thomas Huang", "Zhe Lin", "Jianchao Yang", "Thomas Paine"], "authorids": ["hljin@adobe.com", "huang@ifp.uiuc.edu", "zlin@adobe.com", "jiayang@adobe.com", "tom.le.paine@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390287000000, "tcdate": 1390287000000, "number": 2, "id": "kk4_Fauz_DkzE", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "kSHxSr1TPt8XB", "replyto": "kSHxSr1TPt8XB", "signatures": ["Thomas Paine"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Hello reviewers,\r\nWe would like to bring your attention to a similar paper submitted to this ICLR workshop track:\r\n\r\nTitle: Multi-GPU Training of ConvNets\r\nLink: http://openreview.net/document/bbc93764-4f15-4ba5-b092-86dc80b727c7#bbc93764-4f15-4ba5-b092-86dc80b727c7\r\n\r\nBoth papers explore using many GPUs for training convnets in using an ASGD framework. \r\n\r\nIn theirs, they try using 2 GPUs on one machine for model parallelization (similar to Alex Krizhevsky's NIPS 2012 paper), as well as 2 and 4 nodes for data parallelization (ASGD).\r\n\r\nIn ours a single GPU is used for model parallelization, but many nodes are used for data parallelization (ASGD). The ASGD methods are similar and our method is compatible with the model parallelization they use.\r\n\r\nOurs work has additional experiments that explore how to tune ASGD to get the best performance with GPUs, and how this scales to as many as 32 GPUs.\r\n\r\nWe bring this up because one of their reviewers has recommend their paper for the Conference track, though they submitted to the workshop track. Since the papers have a lot of overlap we think it would be best to compare them on the same footing.\r\n\r\nBest,\r\nTom"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training", "decision": "submitted, no decision", "abstract": "The ability to train large-scale neural networks has resulted in state-of-the-art performance in many areas of computer vision. These results have largely come from computational break throughs of two forms: model parallelism, e.g. GPU accelerated training, which has seen quick adoption in computer vision circles, and data parallelism, e.g. A-SGD, whose large scale has been used mostly in industry. We report early experiments with a system that makes use of both model parallelism and data parallelism, we call GPU A-SGD. We show using GPU A-SGD it is possible to speed up training of large convolutional neural networks useful for computer vision. We believe GPU A-SGD will make it possible to train larger networks on larger training sets in a reasonable amount of time.", "pdf": "https://arxiv.org/abs/1312.6186", "paperhash": "jin|gpu_asynchronous_stochastic_gradient_descent_to_speed_up_neural_network_training", "keywords": [], "conflicts": [], "authors": ["Hailin Jin", "Thomas Huang", "Zhe Lin", "Jianchao Yang", "Thomas Paine"], "authorids": ["hljin@adobe.com", "huang@ifp.uiuc.edu", "zlin@adobe.com", "jiayang@adobe.com", "tom.le.paine@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390286940000, "tcdate": 1390286940000, "number": 3, "id": "R1DX12tB5P1bg", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "kSHxSr1TPt8XB", "replyto": "kSHxSr1TPt8XB", "signatures": ["Thomas Paine"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Hello reviewers,\r\nWe would like to bring your attention to a similar paper submitted to this ICLR workshop track:\r\n\r\nTitle: Multi-GPU Training of ConvNets\r\nLink: http://openreview.net/document/bbc93764-4f15-4ba5-b092-86dc80b727c7#bbc93764-4f15-4ba5-b092-86dc80b727c7\r\n\r\nBoth papers explore using many GPUs for training convnets in using an ASGD framework. \r\n\r\nIn theirs, they try using 2 GPUs on one machine for model parallelization (similar to Alex Krizhevsky's NIPS 2012 paper), as well as 2 and 4 nodes for data parallelization (ASGD).\r\n\r\nIn ours a single GPU is used for model parallelization, but many nodes are used for data parallelization (ASGD). The ASGD methods are similar and our method is compatible with the model parallelization they use.\r\n\r\nOurs work has additional experiments that explore how to tune ASGD to get the best performance with GPUs, and how this scales to as many as 32 GPUs.\r\n\r\nWe bring this up because one of their reviewers has recommend their paper for the Conference track, though they submitted to the workshop track. Since the papers have a lot of overlap we think it would be best to compare them on the same footing.\r\n\r\nBest,\r\nTom"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training", "decision": "submitted, no decision", "abstract": "The ability to train large-scale neural networks has resulted in state-of-the-art performance in many areas of computer vision. These results have largely come from computational break throughs of two forms: model parallelism, e.g. GPU accelerated training, which has seen quick adoption in computer vision circles, and data parallelism, e.g. A-SGD, whose large scale has been used mostly in industry. We report early experiments with a system that makes use of both model parallelism and data parallelism, we call GPU A-SGD. We show using GPU A-SGD it is possible to speed up training of large convolutional neural networks useful for computer vision. We believe GPU A-SGD will make it possible to train larger networks on larger training sets in a reasonable amount of time.", "pdf": "https://arxiv.org/abs/1312.6186", "paperhash": "jin|gpu_asynchronous_stochastic_gradient_descent_to_speed_up_neural_network_training", "keywords": [], "conflicts": [], "authors": ["Hailin Jin", "Thomas Huang", "Zhe Lin", "Jianchao Yang", "Thomas Paine"], "authorids": ["hljin@adobe.com", "huang@ifp.uiuc.edu", "zlin@adobe.com", "jiayang@adobe.com", "tom.le.paine@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390283940000, "tcdate": 1390283940000, "number": 4, "id": "vQUSuhGp0Mvja", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "kSHxSr1TPt8XB", "replyto": "kSHxSr1TPt8XB", "signatures": ["Thomas Paine"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Hello Daniel,\r\nYes. We did not state this explicitly, but in our plot, we are plotting the training error for one client in our ASGD system. And on average the overall ASGD system sees N times as many data points per minibatch.\r\n\r\nWe plotted our error vs minibatches instead of time because time is very dependent on the GPU used to perform training, e.g. using Titan cards instead of a Tesla K20Xs can significantly shorten training time."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training", "decision": "submitted, no decision", "abstract": "The ability to train large-scale neural networks has resulted in state-of-the-art performance in many areas of computer vision. These results have largely come from computational break throughs of two forms: model parallelism, e.g. GPU accelerated training, which has seen quick adoption in computer vision circles, and data parallelism, e.g. A-SGD, whose large scale has been used mostly in industry. We report early experiments with a system that makes use of both model parallelism and data parallelism, we call GPU A-SGD. We show using GPU A-SGD it is possible to speed up training of large convolutional neural networks useful for computer vision. We believe GPU A-SGD will make it possible to train larger networks on larger training sets in a reasonable amount of time.", "pdf": "https://arxiv.org/abs/1312.6186", "paperhash": "jin|gpu_asynchronous_stochastic_gradient_descent_to_speed_up_neural_network_training", "keywords": [], "conflicts": [], "authors": ["Hailin Jin", "Thomas Huang", "Zhe Lin", "Jianchao Yang", "Thomas Paine"], "authorids": ["hljin@adobe.com", "huang@ifp.uiuc.edu", "zlin@adobe.com", "jiayang@adobe.com", "tom.le.paine@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390037040000, "tcdate": 1390037040000, "number": 1, "id": "JJbRoZKlW6fQt", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "kSHxSr1TPt8XB", "replyto": "kSHxSr1TPt8XB", "signatures": ["Daniel Povey"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "It would be helpful if you clarify the meaning of the x-axis 'minibatches' in your plots.  It's not clear whether, in experiments with N GPUs, you are processing N times as many data points per minibatch.  In earlier graphs, I assumed no but in later graphs it looked like the other way around."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training", "decision": "submitted, no decision", "abstract": "The ability to train large-scale neural networks has resulted in state-of-the-art performance in many areas of computer vision. These results have largely come from computational break throughs of two forms: model parallelism, e.g. GPU accelerated training, which has seen quick adoption in computer vision circles, and data parallelism, e.g. A-SGD, whose large scale has been used mostly in industry. We report early experiments with a system that makes use of both model parallelism and data parallelism, we call GPU A-SGD. We show using GPU A-SGD it is possible to speed up training of large convolutional neural networks useful for computer vision. We believe GPU A-SGD will make it possible to train larger networks on larger training sets in a reasonable amount of time.", "pdf": "https://arxiv.org/abs/1312.6186", "paperhash": "jin|gpu_asynchronous_stochastic_gradient_descent_to_speed_up_neural_network_training", "keywords": [], "conflicts": [], "authors": ["Hailin Jin", "Thomas Huang", "Zhe Lin", "Jianchao Yang", "Thomas Paine"], "authorids": ["hljin@adobe.com", "huang@ifp.uiuc.edu", "zlin@adobe.com", "jiayang@adobe.com", "tom.le.paine@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387930800000, "tcdate": 1387930800000, "number": 15, "id": "kSHxSr1TPt8XB", "invitation": "ICLR.cc/2014/workshop/-/submission", "forum": "kSHxSr1TPt8XB", "signatures": ["hljin@adobe.com"], "readers": ["everyone"], "content": {"title": "GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training", "decision": "submitted, no decision", "abstract": "The ability to train large-scale neural networks has resulted in state-of-the-art performance in many areas of computer vision. These results have largely come from computational break throughs of two forms: model parallelism, e.g. GPU accelerated training, which has seen quick adoption in computer vision circles, and data parallelism, e.g. A-SGD, whose large scale has been used mostly in industry. We report early experiments with a system that makes use of both model parallelism and data parallelism, we call GPU A-SGD. We show using GPU A-SGD it is possible to speed up training of large convolutional neural networks useful for computer vision. We believe GPU A-SGD will make it possible to train larger networks on larger training sets in a reasonable amount of time.", "pdf": "https://arxiv.org/abs/1312.6186", "paperhash": "jin|gpu_asynchronous_stochastic_gradient_descent_to_speed_up_neural_network_training", "keywords": [], "conflicts": [], "authors": ["Hailin Jin", "Thomas Huang", "Zhe Lin", "Jianchao Yang", "Thomas Paine"], "authorids": ["hljin@adobe.com", "huang@ifp.uiuc.edu", "zlin@adobe.com", "jiayang@adobe.com", "tom.le.paine@gmail.com"]}, "writers": [], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357014, "id": "ICLR.cc/2014/workshop/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357014}}}], "count": 14}