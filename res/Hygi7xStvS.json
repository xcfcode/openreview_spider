{"notes": [{"id": "Hygi7xStvS", "original": "BJgDbSxFwS", "number": 2224, "cdate": 1569439779310, "ddate": null, "tcdate": 1569439779310, "tmdate": 1577168233502, "tddate": null, "forum": "Hygi7xStvS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["gizacard@gmail.com", "ajoulin@fb.com", "egrave@fb.com"], "title": "Lossless Data Compression with Transformer", "authors": ["Gautier Izacard", "Armand Joulin", "Edouard Grave"], "pdf": "/pdf/83555d5592e7501d04ab23681385b72a81bad943.pdf", "TL;DR": "Application of transformer networks to lossless data compression", "abstract": "Transformers have replaced long-short term memory and other recurrent neural networks variants in sequence modeling. It achieves state-of-the-art performance on a wide range of tasks related to natural language processing, including language modeling, machine translation, and sentence representation. Lossless compression is another problem that can benefit from better sequence models. It is closely related to the problem of online learning of language models. But, despite this ressemblance, it is an area where purely neural network based methods have not yet reached the compression ratio of state-of-the-art algorithms. In this paper, we propose a Transformer based lossless compression method that match the best compression ratio for text. Our approach is purely based on neural networks and does not rely on hand-crafted features as other lossless compression algorithms. We also provide a thorough study of the impact of the different components of the Transformer and its training on the compression ratio.", "keywords": ["data compression", "transformer"], "paperhash": "izacard|lossless_data_compression_with_transformer", "original_pdf": "/attachment/fbf51b759779842489eb99f9ffe136f8e27df5d2.pdf", "_bibtex": "@misc{\nizacard2020lossless,\ntitle={Lossless Data Compression with Transformer},\nauthor={Gautier Izacard and Armand Joulin and Edouard Grave},\nyear={2020},\nurl={https://openreview.net/forum?id=Hygi7xStvS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "H7U8NobZmb", "original": null, "number": 1, "cdate": 1576798743669, "ddate": null, "tcdate": 1576798743669, "tmdate": 1576800892546, "tddate": null, "forum": "Hygi7xStvS", "replyto": "Hygi7xStvS", "invitation": "ICLR.cc/2020/Conference/Paper2224/-/Decision", "content": {"decision": "Reject", "comment": "The paper proposes to use transformers to do lossless data compression. The idea is simple and straightforward (with adding n-gram inputs). The initial submission considered one dataset, a new dataset was added in the rebuttal. Still, there is no runtime in the experiments (and Transformers can take a lot of time to train). Since this is more an experimental paper, this is crucial (and the improvements reports are very small and it is difficult to judge if there are significant).\nOverall, there was a positive discussion between the authors and the reviewers. The reviewers commented that concerns have been addressed, but did not change the evaluation which is  unanimous reject.  ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gizacard@gmail.com", "ajoulin@fb.com", "egrave@fb.com"], "title": "Lossless Data Compression with Transformer", "authors": ["Gautier Izacard", "Armand Joulin", "Edouard Grave"], "pdf": "/pdf/83555d5592e7501d04ab23681385b72a81bad943.pdf", "TL;DR": "Application of transformer networks to lossless data compression", "abstract": "Transformers have replaced long-short term memory and other recurrent neural networks variants in sequence modeling. It achieves state-of-the-art performance on a wide range of tasks related to natural language processing, including language modeling, machine translation, and sentence representation. Lossless compression is another problem that can benefit from better sequence models. It is closely related to the problem of online learning of language models. But, despite this ressemblance, it is an area where purely neural network based methods have not yet reached the compression ratio of state-of-the-art algorithms. In this paper, we propose a Transformer based lossless compression method that match the best compression ratio for text. Our approach is purely based on neural networks and does not rely on hand-crafted features as other lossless compression algorithms. We also provide a thorough study of the impact of the different components of the Transformer and its training on the compression ratio.", "keywords": ["data compression", "transformer"], "paperhash": "izacard|lossless_data_compression_with_transformer", "original_pdf": "/attachment/fbf51b759779842489eb99f9ffe136f8e27df5d2.pdf", "_bibtex": "@misc{\nizacard2020lossless,\ntitle={Lossless Data Compression with Transformer},\nauthor={Gautier Izacard and Armand Joulin and Edouard Grave},\nyear={2020},\nurl={https://openreview.net/forum?id=Hygi7xStvS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Hygi7xStvS", "replyto": "Hygi7xStvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795703150, "tmdate": 1576800250446, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2224/-/Decision"}}}, {"id": "rkxDdksnjB", "original": null, "number": 3, "cdate": 1573855087252, "ddate": null, "tcdate": 1573855087252, "tmdate": 1573855087252, "tddate": null, "forum": "Hygi7xStvS", "replyto": "r1eFrDP19S", "invitation": "ICLR.cc/2020/Conference/Paper2224/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "We thank the reviewer for their feedback.\n\nIn this paper, we show that a method purely based on neural networks, without hand designed features, can obtain SoTA results compression results on benchmarks such as enwik8. Existing work showed a significant gap between methods purely based on neural networks, compared to methods such as PAQ8 or cmix (see https://bellard.org/nncp/nncp.pdf). Also note that for each application, there is a tradeoff between compression ratio and compression/decompression speed. Our focus in this paper is to obtain the best possible compression rate, at the expense of compression speed.\n\nRegarding SoTA results for compression, cmix and PAQ8 variants are the methods obtaining the best compression rates, according to https://cs.fit.edu/~mmahoney/compression/text.html,  http://mattmahoney.net/dc/silesia.html or http://qlic.altervista.org/LPCB.html. Could the reviewer indicates methods outperforming these approaches that we might have missed? Note that results reported on enwik8 in the traditional language modeling setting (i.e. training on 90% of data, validating and testing on the rest) are not comparable to the compression setting we study.\n\nWe will add a discussion of \"Practical Full Resolution Learned Lossless Image Compression\" [1] as well as the paper mentioned by reviewer 3 in the related work. The method proposed for image compression in [1] combines arithmetic coding with a neural network. As opposed to our work, the approach is designed to enable practical compression and decompression speed with compression ratio comparable with standard methods (while we focus on compression rate only). Another difference with our work is the evaluation setting: in [1] it is assumed that both the encoder and the decoder have access to the pre-trained network for free. As PAQ8 and cmix, we do not use a pre-trained network, it has to be included in the archive in order to be accessed by the decompressor. While it is true that sending a network to the decoder can be amortized over the decompression of large amount of data, the size of the L3C network archive is 35MB and not negligible compared to the size of the compressed archive of enwik8, around 15MB. It should also be noted that PAQ8 and CMIX achieve better compression ratio than other compression algorithms on a dataset composed of large images, at the expense of compression and decompression speed (e.g. see http://qlic.altervista.org/LPCB.html).\n\nBERT fundamentally differs from our setting in several aspects. First, BERT is not trained as a language model to predict the next character given the preceding characters, but as a denoising auto-encoder. As such, BERT is not a generative model of sequence, and it is not straightforward to apply it to data compression. Moreover, standard BERT models are several 100s of MB in size, which would need to be included in the archive to allow the decompression. As such, it does not make using BERT practical for lossless data compression.\n\nWe have not yet implemented an end-to-end framework for compression and decompression. The decoder and encoder can get the same numbers provided that they have access to the same random number generator and the same seed. Finally, we will release our code with the paper to allow reproducibility.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2224/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2224/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gizacard@gmail.com", "ajoulin@fb.com", "egrave@fb.com"], "title": "Lossless Data Compression with Transformer", "authors": ["Gautier Izacard", "Armand Joulin", "Edouard Grave"], "pdf": "/pdf/83555d5592e7501d04ab23681385b72a81bad943.pdf", "TL;DR": "Application of transformer networks to lossless data compression", "abstract": "Transformers have replaced long-short term memory and other recurrent neural networks variants in sequence modeling. It achieves state-of-the-art performance on a wide range of tasks related to natural language processing, including language modeling, machine translation, and sentence representation. Lossless compression is another problem that can benefit from better sequence models. It is closely related to the problem of online learning of language models. But, despite this ressemblance, it is an area where purely neural network based methods have not yet reached the compression ratio of state-of-the-art algorithms. In this paper, we propose a Transformer based lossless compression method that match the best compression ratio for text. Our approach is purely based on neural networks and does not rely on hand-crafted features as other lossless compression algorithms. We also provide a thorough study of the impact of the different components of the Transformer and its training on the compression ratio.", "keywords": ["data compression", "transformer"], "paperhash": "izacard|lossless_data_compression_with_transformer", "original_pdf": "/attachment/fbf51b759779842489eb99f9ffe136f8e27df5d2.pdf", "_bibtex": "@misc{\nizacard2020lossless,\ntitle={Lossless Data Compression with Transformer},\nauthor={Gautier Izacard and Armand Joulin and Edouard Grave},\nyear={2020},\nurl={https://openreview.net/forum?id=Hygi7xStvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hygi7xStvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2224/Authors", "ICLR.cc/2020/Conference/Paper2224/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2224/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2224/Reviewers", "ICLR.cc/2020/Conference/Paper2224/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2224/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2224/Authors|ICLR.cc/2020/Conference/Paper2224/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144517, "tmdate": 1576860553839, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2224/Authors", "ICLR.cc/2020/Conference/Paper2224/Reviewers", "ICLR.cc/2020/Conference/Paper2224/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2224/-/Official_Comment"}}}, {"id": "S1lcZ1ohsH", "original": null, "number": 2, "cdate": 1573854977645, "ddate": null, "tcdate": 1573854977645, "tmdate": 1573854977645, "tddate": null, "forum": "Hygi7xStvS", "replyto": "BkeZpB56FH", "invitation": "ICLR.cc/2020/Conference/Paper2224/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "We thank the reviewer for their feedback.\n\n1. We will add experiments on the silesia benchmark (http://mattmahoney.net/dc/silesia.html). Overall, our method compress the whole corpus in 33.4MB, compared to 33.3MB for cmix (we report bpc for individual files below).\n\n         \t| paq8 | cmix |\tours\ndickens  | 1.58 | 1.49 | 1.59\nmozilla\t| 1.61 | 1.49 | 1.41\nmr    \t| 1.68 | 1.62 | 1.48\nnci   \t| 0.23 | 0.20 | 0.20\nooffice\t| 1.87 | 1.74 | 2.25\nosdb  \t| 1.65 | 1.60 | 1.58\nreymont\t| 0.98 | 0.93 | 0.94\nsamba \t| 1.02 | 0.96 | 1.14\nsao   \t| 4.16 | 4.16 | 4.18\nwebster\t| 1.00 | 0.90 | 0.84\nx-ray \t| 3.41 | 3.37 | 3.33\nxml   \t| 0.41 | 0.37 | 0.54\n\nSimilarly to cmix and PAQ8, our method don\u2019t pretrain the language model on a dataset different from the data to compress. The neural network is randomly initialized at the beginning of the compression and decompression phase with the same seed and is trained during both phases. The only constraint is that a given character has first to be compressed before being used to update the model. We have considered initializing the neural network with a pre-trained model. However in order to decompress the archive, the decoder needs to have access to the pretrained model used by the encoder. Thus, the size of the pre-trained model has to be added to the size of the archive. As a result, we didn\u2019t manage to improve performances by using a pre-trained model.\n\n2. There is an inherent tradeoff between compression ratio and compression/decompression speed. In this paper we focus on the compression ratio, and at the expense of speed we match the performance of CMIX on enwik8. We will add runtime in the paper.\n\n3. Indeed, the model is trained using the softmax cross entropy. Without revisit, the weights of the network is updated every 256 characters. The gradient is obtained by backpropagating the error of the prediction associated with the 256 characters that have just been compressed. Thus during compression and decompression the network is trained for one epoch. With revisits, the number of times a given character is used to train the Transformer is increased. Enwik8 is composed of 100MB of wikipedia data in XML format.\n\n4. a) A revisit is a partial pass on data that have already been compressed. Revisits are performed during compression and decompression at fixed intervals to further train the network. All weight updates performed during compression have to be performed identically during decompression in order to losslessly decompressed data. Thus increasing the number of revisits makes compression and decompression slower. In particular since it is necessary to compute a forward pass to compress the data, the cost of backpropagating the error of the first prediction is amortized.\nb) Since we made the assumption (also made in PAQ8 and CMIX) that pretrained models used for compression have to be included in the archive in order to be accessed by the decoder, revisits is just the way to use data several times to learn the parameters of the model. \nc) During a revisit the learning rate is fixed, but it is linearly decreased over the compression phase. If the frequency of revisit F is too low, a higher revisit frequency could improve the compression ratio, especially at the beginning of the compression. We have observed that at some point increasing the frequency of revisit can be detrimental to the compression ratio. This can be explained by the fact that the network tends to forget the current context. Increasing the number of characters considered at each revisits improve the compression ratio but is detrimental to the compression/decompression speed. \n\nWe will address the minor comments in the paper, and will add the missing references to the related work.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2224/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2224/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gizacard@gmail.com", "ajoulin@fb.com", "egrave@fb.com"], "title": "Lossless Data Compression with Transformer", "authors": ["Gautier Izacard", "Armand Joulin", "Edouard Grave"], "pdf": "/pdf/83555d5592e7501d04ab23681385b72a81bad943.pdf", "TL;DR": "Application of transformer networks to lossless data compression", "abstract": "Transformers have replaced long-short term memory and other recurrent neural networks variants in sequence modeling. It achieves state-of-the-art performance on a wide range of tasks related to natural language processing, including language modeling, machine translation, and sentence representation. Lossless compression is another problem that can benefit from better sequence models. It is closely related to the problem of online learning of language models. But, despite this ressemblance, it is an area where purely neural network based methods have not yet reached the compression ratio of state-of-the-art algorithms. In this paper, we propose a Transformer based lossless compression method that match the best compression ratio for text. Our approach is purely based on neural networks and does not rely on hand-crafted features as other lossless compression algorithms. We also provide a thorough study of the impact of the different components of the Transformer and its training on the compression ratio.", "keywords": ["data compression", "transformer"], "paperhash": "izacard|lossless_data_compression_with_transformer", "original_pdf": "/attachment/fbf51b759779842489eb99f9ffe136f8e27df5d2.pdf", "_bibtex": "@misc{\nizacard2020lossless,\ntitle={Lossless Data Compression with Transformer},\nauthor={Gautier Izacard and Armand Joulin and Edouard Grave},\nyear={2020},\nurl={https://openreview.net/forum?id=Hygi7xStvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hygi7xStvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2224/Authors", "ICLR.cc/2020/Conference/Paper2224/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2224/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2224/Reviewers", "ICLR.cc/2020/Conference/Paper2224/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2224/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2224/Authors|ICLR.cc/2020/Conference/Paper2224/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144517, "tmdate": 1576860553839, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2224/Authors", "ICLR.cc/2020/Conference/Paper2224/Reviewers", "ICLR.cc/2020/Conference/Paper2224/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2224/-/Official_Comment"}}}, {"id": "BJxPqa9noS", "original": null, "number": 1, "cdate": 1573854606877, "ddate": null, "tcdate": 1573854606877, "tmdate": 1573854635239, "tddate": null, "forum": "Hygi7xStvS", "replyto": "HklOX_jl5r", "invitation": "ICLR.cc/2020/Conference/Paper2224/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "We thank the reviewer for their feedback.\n\nWe will update the paper to address the minor comments. Moreover, we evaluated our method on the additional Silesia benchmark (http://mattmahoney.net/dc/silesia.html), which includes files of different types (such as text, UNIX or Windows executables, databases, pdf etc). We report compression rates for our method, as well as PAQ8L and cmix v8 below, and will add it to the paper. Overall, our method compress the whole corpus in 33.4MB, compared to 33.3MB for cmix. We report bpc for individual files below.\n\n         \t| paq8 | cmix |\tours\ndickens  | 1.58 | 1.49 | 1.59\nmozilla\t| 1.61 | 1.49 | 1.41\nmr    \t| 1.68 | 1.62 | 1.48\nnci   \t| 0.23 | 0.20 | 0.20\nooffice\t| 1.87 | 1.74 | 2.25\nosdb  \t| 1.65 | 1.60 | 1.58\nreymont\t| 0.98 | 0.93 | 0.94\nsamba \t| 1.02 | 0.96 | 1.14\nsao   \t| 4.16 | 4.16 | 4.18\nwebster\t| 1.00 | 0.90 | 0.84\nx-ray \t| 3.41 | 3.37 | 3.33\nxml   \t| 0.41 | 0.37 | 0.54"}, "signatures": ["ICLR.cc/2020/Conference/Paper2224/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2224/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gizacard@gmail.com", "ajoulin@fb.com", "egrave@fb.com"], "title": "Lossless Data Compression with Transformer", "authors": ["Gautier Izacard", "Armand Joulin", "Edouard Grave"], "pdf": "/pdf/83555d5592e7501d04ab23681385b72a81bad943.pdf", "TL;DR": "Application of transformer networks to lossless data compression", "abstract": "Transformers have replaced long-short term memory and other recurrent neural networks variants in sequence modeling. It achieves state-of-the-art performance on a wide range of tasks related to natural language processing, including language modeling, machine translation, and sentence representation. Lossless compression is another problem that can benefit from better sequence models. It is closely related to the problem of online learning of language models. But, despite this ressemblance, it is an area where purely neural network based methods have not yet reached the compression ratio of state-of-the-art algorithms. In this paper, we propose a Transformer based lossless compression method that match the best compression ratio for text. Our approach is purely based on neural networks and does not rely on hand-crafted features as other lossless compression algorithms. We also provide a thorough study of the impact of the different components of the Transformer and its training on the compression ratio.", "keywords": ["data compression", "transformer"], "paperhash": "izacard|lossless_data_compression_with_transformer", "original_pdf": "/attachment/fbf51b759779842489eb99f9ffe136f8e27df5d2.pdf", "_bibtex": "@misc{\nizacard2020lossless,\ntitle={Lossless Data Compression with Transformer},\nauthor={Gautier Izacard and Armand Joulin and Edouard Grave},\nyear={2020},\nurl={https://openreview.net/forum?id=Hygi7xStvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hygi7xStvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2224/Authors", "ICLR.cc/2020/Conference/Paper2224/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2224/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2224/Reviewers", "ICLR.cc/2020/Conference/Paper2224/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2224/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2224/Authors|ICLR.cc/2020/Conference/Paper2224/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144517, "tmdate": 1576860553839, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2224/Authors", "ICLR.cc/2020/Conference/Paper2224/Reviewers", "ICLR.cc/2020/Conference/Paper2224/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2224/-/Official_Comment"}}}, {"id": "BkeZpB56FH", "original": null, "number": 1, "cdate": 1571820985377, "ddate": null, "tcdate": 1571820985377, "tmdate": 1572972366834, "tddate": null, "forum": "Hygi7xStvS", "replyto": "Hygi7xStvS", "invitation": "ICLR.cc/2020/Conference/Paper2224/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary: \nThe paper investigates using the transformer architecture for neural network-based lossless compression of text. The resulting model, obtained through a thorough investigation of the architecture hyper-parameters are on par with standard SOTA compression. The paper is well-written. In particular the authors have done a great job reviewing existing compression literature and positioning their method within the space of prior work.\n\nRecommendation: Weak Reject\nWhile the paper considers an interesting application of the Transformer architecture, and is well-written, it is of limited novelty. Specifically, the bulk of the paper is concerned with describing experimental results of a thorough (but standard) hyper-parameter search - considering things like Transformer context size, learning rate (schedule), number of layers and key, value, query dimensionality; and does not offer any new architectural modifications / insights.\n\nFurthermore, only a single dataset - enwik8 - is considered in the experimental validation and little attention is given to the description of the dataset split and any distribution differences between splits. Taken together, the existing experimental setup potentially creates an unfair advantage for the neural network-based methods - while the standard methods can be expected to perform similarly across a wide range of datasets / texts, the neural-network based methods have been trained and tested on very similar data and could be expected to perform well on these data, but not in case of a distributional shift (e.g. compressing legal texts instead of Wikipedia). The paper does not answer the question of whether or not this is true.\n\nFurthermore, similar to autoregressive models, transformers are known to be slow at inference time. I expect this to lead to very slow decoding. Therefore, methods in table 1 should be compared in compression/decompression time to give a better overview of the practical impact of this work. \n\nTaken together, in its current form the paper may be better suited for a workshop publication rather than a full conference paper.\n\nMajor comments:\n1. For reasons mentioned above, the paper should include additional experimental evaluation. In particular, it should consider the effect of training the model on one dataset, but evaluating it on another dataset; and discuss how differences in performance (if any) compare to standard methods.\n2. Compression/decompression times of the proposed method should be compared against the other compression methods in table 1. I expect the proposed transformer to be slow at decompressing.\n3. The paper does not contain the loss that the transformer model was used to optimize. I assume that it is the softmax cross entropy, but this is worth mentioning explicitly. It would also be worthwhile to explain the training procedure - for how many epochs was the model trained (see also next question), what was the dataset size? \n4. Description of the \u201ctraining with revisits\u201d is not very clear. My understanding is that it resembles a pass through the data, where some of it is considered again at specific intervals. My first assessment is that this should not be necessary - the data should already be considered multiple times during the training process.\na) The authors should provide a more detailed description of the training-with-revisits procedure, contrasting it specifically with a procedure where revisits are not done (i.e. normal training).\nb) If the goal of the revisits training is to observe some training examples more than once, then it would be very interesting if simply training for a longer time (several epochs == passes through the data) has a similar effect.\nc) Is there any motivation for the choice of the revisits hyper-parameters F and M? Was a different batch size used during the revisits training? Is the learning rate evolved during the revisits training phase or is it still decayed?\n\nMinor comments:\n1. There is some prior work on using Neural Networks for lossless image compression (e.g. [1], [2]. [3] that achieves SOTA compression ratios compared to standard methods. It may be interesting for the readers to mention these results. In particular the authors\u2019 statement that \u201c[...] purely neural network based models are still far from state of the art [...]\u201d may give the wrong impression to the readers.\n2. The authors mention that they \u201c[...] propose several improvements to its (the Transformer) architecture and training to accelerate and stabilize [...] training\u201d. In my view, the experiments described in the paper resemble a hyper-parameter search more than architectural improvements. The authors may want to clarify in the text which specific improvements they refer to.\n3. Page 1, last paragraph: \u201c[...] of all the important component [...]\u201d -> \u201c[...] of all the important components [...]\u201d\n4. Page 3: \u201c[...] attention span size across all layers as it suggested [...]\u201d -> \u201c[...] attention span size across all layers as was suggested [...]\u201d\n5. Page 3: Missing references.\n6. Page 3: Use of small n and capital N when talking about n-grams. Should be made consistent.\n7. Page 8 (Conclusion): \u201cwihtout\u201d -> \u201cwithout\u201d\n\n\n[1] F. H. Kingma, P. Abbeel, and J. Ho. Bit-Swap: recursive bits-back coding for lossless compression with hierarchical latent variables. In International Conference on Machine Learning (ICML), 2019.\n[2] Emiel Hoogeboom, Jorn W. T. Peters, Rianne van den Berg, and Max Welling. Integer Discrete Flows and Lossless Compression. arXiv e-prints, 2019.\n[3] Jonathan Ho, Evan Lohn, and Pieter Abbeel. Compression with Flows via Local Bits-Back Coding. arXiv e-prints, 2019.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2224/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2224/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gizacard@gmail.com", "ajoulin@fb.com", "egrave@fb.com"], "title": "Lossless Data Compression with Transformer", "authors": ["Gautier Izacard", "Armand Joulin", "Edouard Grave"], "pdf": "/pdf/83555d5592e7501d04ab23681385b72a81bad943.pdf", "TL;DR": "Application of transformer networks to lossless data compression", "abstract": "Transformers have replaced long-short term memory and other recurrent neural networks variants in sequence modeling. It achieves state-of-the-art performance on a wide range of tasks related to natural language processing, including language modeling, machine translation, and sentence representation. Lossless compression is another problem that can benefit from better sequence models. It is closely related to the problem of online learning of language models. But, despite this ressemblance, it is an area where purely neural network based methods have not yet reached the compression ratio of state-of-the-art algorithms. In this paper, we propose a Transformer based lossless compression method that match the best compression ratio for text. Our approach is purely based on neural networks and does not rely on hand-crafted features as other lossless compression algorithms. We also provide a thorough study of the impact of the different components of the Transformer and its training on the compression ratio.", "keywords": ["data compression", "transformer"], "paperhash": "izacard|lossless_data_compression_with_transformer", "original_pdf": "/attachment/fbf51b759779842489eb99f9ffe136f8e27df5d2.pdf", "_bibtex": "@misc{\nizacard2020lossless,\ntitle={Lossless Data Compression with Transformer},\nauthor={Gautier Izacard and Armand Joulin and Edouard Grave},\nyear={2020},\nurl={https://openreview.net/forum?id=Hygi7xStvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hygi7xStvS", "replyto": "Hygi7xStvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2224/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2224/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575476497287, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2224/Reviewers"], "noninvitees": [], "tcdate": 1570237725923, "tmdate": 1575476497302, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2224/-/Official_Review"}}}, {"id": "r1eFrDP19S", "original": null, "number": 2, "cdate": 1571940160657, "ddate": null, "tcdate": 1571940160657, "tmdate": 1572972366787, "tddate": null, "forum": "Hygi7xStvS", "replyto": "Hygi7xStvS", "invitation": "ICLR.cc/2020/Conference/Paper2224/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper provides a method for lossless compression of text. It's heavily inspired by the language modelling methods that have been developed for the purposes of predicting the next character/word in a sentence, and it uses this idea as its backbone. The only difference is that the results are presented in the compression setting.\n\nI think we should reject this paper due to the following reasons:\n- I don't see enough of a difference between this and previous work\n- the results are nowhere near SoTA for compression, despite the method being sold to this community\n- there are other papers that do lossless neural compression that could have been used to make a comparison rather than making no comparison at all. For example, \"Practical Full Resolution Learned Lossless Image Compression\" (CVPR 2019) provides a framework for image rather than text, but that could be adapted to this field without any major changes (predict convolutionally characters, rather than RGB values).\n- there's no comparison even with BERT (how well it do to predict the next character vs. this)...\n- no runtime numbers\n- no reproducibility discussion (i.e., how can I guarantee that my decoder can get exactly the same numbers as my encoder so that I can decompress on a different machine)\n- no discussion about whether files were created/decompressed (this is ABSOLUTELY CRUCIAL for compression papers to discuss)\n\nOverall, I am not excited about this paper, and unless the authors put a lot more into it, there's just not enough novelty to justify a publication at ICLR."}, "signatures": ["ICLR.cc/2020/Conference/Paper2224/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2224/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gizacard@gmail.com", "ajoulin@fb.com", "egrave@fb.com"], "title": "Lossless Data Compression with Transformer", "authors": ["Gautier Izacard", "Armand Joulin", "Edouard Grave"], "pdf": "/pdf/83555d5592e7501d04ab23681385b72a81bad943.pdf", "TL;DR": "Application of transformer networks to lossless data compression", "abstract": "Transformers have replaced long-short term memory and other recurrent neural networks variants in sequence modeling. It achieves state-of-the-art performance on a wide range of tasks related to natural language processing, including language modeling, machine translation, and sentence representation. Lossless compression is another problem that can benefit from better sequence models. It is closely related to the problem of online learning of language models. But, despite this ressemblance, it is an area where purely neural network based methods have not yet reached the compression ratio of state-of-the-art algorithms. In this paper, we propose a Transformer based lossless compression method that match the best compression ratio for text. Our approach is purely based on neural networks and does not rely on hand-crafted features as other lossless compression algorithms. We also provide a thorough study of the impact of the different components of the Transformer and its training on the compression ratio.", "keywords": ["data compression", "transformer"], "paperhash": "izacard|lossless_data_compression_with_transformer", "original_pdf": "/attachment/fbf51b759779842489eb99f9ffe136f8e27df5d2.pdf", "_bibtex": "@misc{\nizacard2020lossless,\ntitle={Lossless Data Compression with Transformer},\nauthor={Gautier Izacard and Armand Joulin and Edouard Grave},\nyear={2020},\nurl={https://openreview.net/forum?id=Hygi7xStvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hygi7xStvS", "replyto": "Hygi7xStvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2224/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2224/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575476497287, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2224/Reviewers"], "noninvitees": [], "tcdate": 1570237725923, "tmdate": 1575476497302, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2224/-/Official_Review"}}}, {"id": "HklOX_jl5r", "original": null, "number": 3, "cdate": 1572022304276, "ddate": null, "tcdate": 1572022304276, "tmdate": 1572972366729, "tddate": null, "forum": "Hygi7xStvS", "replyto": "Hygi7xStvS", "invitation": "ICLR.cc/2020/Conference/Paper2224/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\n\nThis paper explores the effectiveness of the Transformer architecture to the lossless data compression problem.\nIt also proposes a method to periodically revisit tokens that were already compressed for adopting the task setting of data compression, which is essentially online learning of sequence models. \n \nThe authors conduct their experiments on the enwik8 benchmark.\nThey show that the Transformer architecture obtains state-of-the-art results.\n \nThis paper is basically easy to follow, but several typos and statements that should be improved.\nThe problem setting to tackle is interesting.\nHowever, applying a deep neural network approach to data compression problem has already been discussed in several previous studies.\nTherefore, the novelty of this paper is somewhat limited.\n \n \nMy main concern of this paper is that the proposed method was only evaluated on a single benchmark data.\nI believe that it is a bit weak to support the effectiveness of the proposed method.\nThe authors should evaluate their method on several benchmark datasets that have different aspects, such as settings with easy and hard to compress. \n \n \nMinor comment:\nIn Section 4.2, there is a missing citation.\n... we do not use Adaptive Inputs (Baevski & Auli, 2018; ?) ...\nPlease check and fix it.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2224/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2224/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gizacard@gmail.com", "ajoulin@fb.com", "egrave@fb.com"], "title": "Lossless Data Compression with Transformer", "authors": ["Gautier Izacard", "Armand Joulin", "Edouard Grave"], "pdf": "/pdf/83555d5592e7501d04ab23681385b72a81bad943.pdf", "TL;DR": "Application of transformer networks to lossless data compression", "abstract": "Transformers have replaced long-short term memory and other recurrent neural networks variants in sequence modeling. It achieves state-of-the-art performance on a wide range of tasks related to natural language processing, including language modeling, machine translation, and sentence representation. Lossless compression is another problem that can benefit from better sequence models. It is closely related to the problem of online learning of language models. But, despite this ressemblance, it is an area where purely neural network based methods have not yet reached the compression ratio of state-of-the-art algorithms. In this paper, we propose a Transformer based lossless compression method that match the best compression ratio for text. Our approach is purely based on neural networks and does not rely on hand-crafted features as other lossless compression algorithms. We also provide a thorough study of the impact of the different components of the Transformer and its training on the compression ratio.", "keywords": ["data compression", "transformer"], "paperhash": "izacard|lossless_data_compression_with_transformer", "original_pdf": "/attachment/fbf51b759779842489eb99f9ffe136f8e27df5d2.pdf", "_bibtex": "@misc{\nizacard2020lossless,\ntitle={Lossless Data Compression with Transformer},\nauthor={Gautier Izacard and Armand Joulin and Edouard Grave},\nyear={2020},\nurl={https://openreview.net/forum?id=Hygi7xStvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hygi7xStvS", "replyto": "Hygi7xStvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2224/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2224/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575476497287, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2224/Reviewers"], "noninvitees": [], "tcdate": 1570237725923, "tmdate": 1575476497302, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2224/-/Official_Review"}}}], "count": 8}