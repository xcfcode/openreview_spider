{"notes": [{"id": "SkMuPjRcKQ", "original": "H1l26CvqFX", "number": 280, "cdate": 1538087776464, "ddate": null, "tcdate": 1538087776464, "tmdate": 1550845620334, "tddate": null, "forum": "SkMuPjRcKQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers", "abstract": "Probabilistic Neural Networks deal with various sources of stochasticity: input noise, dropout, stochastic neurons, parameter uncertainties modeled as random variables, etc.\nIn this paper we revisit a feed-forward propagation approach that allows one to estimate for each neuron its mean and variance w.r.t. all mentioned sources of stochasticity. In contrast, standard NNs propagate only point estimates, discarding the uncertainty.\nMethods propagating also the variance have been proposed by several authors in different context. The view presented here attempts to clarify the assumptions and derivation behind such methods, relate them to classical NNs and broaden their scope of applicability.\nThe main technical contributions are new approximations for the distributions of argmax and max-related transforms, which allow for fully analytic uncertainty propagation in networks with softmax and max-pooling layers as well as leaky ReLU activations.\nWe evaluate the accuracy of the approximation and suggest a simple calibration. Applying the method to networks with dropout allows for faster training and gives improved test likelihoods without the need of sampling.", "keywords": ["probabilistic neural network", "uncertainty", "dropout", "bayesian", "softmax", "argmax", "logsumexp"], "authorids": ["shekhovtsov@gmail.com", "bflach@inf.tu-dresden.de"], "authors": ["Alexander Shekhovtsov", "Boris Flach"], "TL;DR": "Approximating mean and variance of the NN output over noisy input / dropout / uncertain parameters. Analytic approximations for argmax, softmax and max layers.", "pdf": "/pdf/4ef42fa3995a010e559ae67213b0387f45015a99.pdf", "paperhash": "shekhovtsov|feedforward_propagation_in_probabilistic_neural_networks_with_categorical_and_max_layers", "_bibtex": "@inproceedings{\nshekhovtsov2018feedforward,\ntitle={Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers},\nauthor={Alexander Shekhovtsov and Boris Flach},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkMuPjRcKQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "H1eS3h2WxE", "original": null, "number": 1, "cdate": 1544830124783, "ddate": null, "tcdate": 1544830124783, "tmdate": 1545354530798, "tddate": null, "forum": "SkMuPjRcKQ", "replyto": "SkMuPjRcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper280/Meta_Review", "content": {"metareview": "Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers' comments into consideration to improve your submission for the camera ready.\n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Paper decision"}, "signatures": ["ICLR.cc/2019/Conference/Paper280/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper280/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers", "abstract": "Probabilistic Neural Networks deal with various sources of stochasticity: input noise, dropout, stochastic neurons, parameter uncertainties modeled as random variables, etc.\nIn this paper we revisit a feed-forward propagation approach that allows one to estimate for each neuron its mean and variance w.r.t. all mentioned sources of stochasticity. In contrast, standard NNs propagate only point estimates, discarding the uncertainty.\nMethods propagating also the variance have been proposed by several authors in different context. The view presented here attempts to clarify the assumptions and derivation behind such methods, relate them to classical NNs and broaden their scope of applicability.\nThe main technical contributions are new approximations for the distributions of argmax and max-related transforms, which allow for fully analytic uncertainty propagation in networks with softmax and max-pooling layers as well as leaky ReLU activations.\nWe evaluate the accuracy of the approximation and suggest a simple calibration. Applying the method to networks with dropout allows for faster training and gives improved test likelihoods without the need of sampling.", "keywords": ["probabilistic neural network", "uncertainty", "dropout", "bayesian", "softmax", "argmax", "logsumexp"], "authorids": ["shekhovtsov@gmail.com", "bflach@inf.tu-dresden.de"], "authors": ["Alexander Shekhovtsov", "Boris Flach"], "TL;DR": "Approximating mean and variance of the NN output over noisy input / dropout / uncertain parameters. Analytic approximations for argmax, softmax and max layers.", "pdf": "/pdf/4ef42fa3995a010e559ae67213b0387f45015a99.pdf", "paperhash": "shekhovtsov|feedforward_propagation_in_probabilistic_neural_networks_with_categorical_and_max_layers", "_bibtex": "@inproceedings{\nshekhovtsov2018feedforward,\ntitle={Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers},\nauthor={Alexander Shekhovtsov and Boris Flach},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkMuPjRcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper280/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353272148, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkMuPjRcKQ", "replyto": "SkMuPjRcKQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper280/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper280/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper280/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353272148}}}, {"id": "rJlwqywr1E", "original": null, "number": 10, "cdate": 1544019855157, "ddate": null, "tcdate": 1544019855157, "tmdate": 1544019855157, "tddate": null, "forum": "SkMuPjRcKQ", "replyto": "rJxcjNogTX", "invitation": "ICLR.cc/2019/Conference/-/Paper280/Official_Comment", "content": {"title": "Thanks for the response", "comment": "Thanks for your response. It addressed some of my concerns. However, I still have concerns on the lack of rigor when mentioning the term posterior distribution and the notation. The explanation/clarification provided does not convince me unfortunately. That said, I am still happy if the authors could be more careful when using the term (e.g., for p(X^k | x_0)) in the revised version.\n\nI am not sure that the statement \u2018the propagation method however cannot be expected to work very well in convolutional networks since outputs will be strongly spatially correlated\u2019 is correct. Convolutional layers are still linear layers, and propagation methods like NPN should be fairly easy to handle them (with some work on naturally extending the FC linear-layer propagation to its convolutional version)."}, "signatures": ["ICLR.cc/2019/Conference/Paper280/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper280/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper280/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers", "abstract": "Probabilistic Neural Networks deal with various sources of stochasticity: input noise, dropout, stochastic neurons, parameter uncertainties modeled as random variables, etc.\nIn this paper we revisit a feed-forward propagation approach that allows one to estimate for each neuron its mean and variance w.r.t. all mentioned sources of stochasticity. In contrast, standard NNs propagate only point estimates, discarding the uncertainty.\nMethods propagating also the variance have been proposed by several authors in different context. The view presented here attempts to clarify the assumptions and derivation behind such methods, relate them to classical NNs and broaden their scope of applicability.\nThe main technical contributions are new approximations for the distributions of argmax and max-related transforms, which allow for fully analytic uncertainty propagation in networks with softmax and max-pooling layers as well as leaky ReLU activations.\nWe evaluate the accuracy of the approximation and suggest a simple calibration. Applying the method to networks with dropout allows for faster training and gives improved test likelihoods without the need of sampling.", "keywords": ["probabilistic neural network", "uncertainty", "dropout", "bayesian", "softmax", "argmax", "logsumexp"], "authorids": ["shekhovtsov@gmail.com", "bflach@inf.tu-dresden.de"], "authors": ["Alexander Shekhovtsov", "Boris Flach"], "TL;DR": "Approximating mean and variance of the NN output over noisy input / dropout / uncertain parameters. Analytic approximations for argmax, softmax and max layers.", "pdf": "/pdf/4ef42fa3995a010e559ae67213b0387f45015a99.pdf", "paperhash": "shekhovtsov|feedforward_propagation_in_probabilistic_neural_networks_with_categorical_and_max_layers", "_bibtex": "@inproceedings{\nshekhovtsov2018feedforward,\ntitle={Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers},\nauthor={Alexander Shekhovtsov and Boris Flach},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkMuPjRcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper280/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614859, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkMuPjRcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper280/Authors", "ICLR.cc/2019/Conference/Paper280/Reviewers", "ICLR.cc/2019/Conference/Paper280/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper280/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper280/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper280/Authors|ICLR.cc/2019/Conference/Paper280/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper280/Reviewers", "ICLR.cc/2019/Conference/Paper280/Authors", "ICLR.cc/2019/Conference/Paper280/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614859}}}, {"id": "rJgQkXZ5Cm", "original": null, "number": 8, "cdate": 1543275227497, "ddate": null, "tcdate": 1543275227497, "tmdate": 1543275227497, "tddate": null, "forum": "SkMuPjRcKQ", "replyto": "rkla8-4tRm", "invitation": "ICLR.cc/2019/Conference/-/Paper280/Official_Comment", "content": {"title": "Yes, we agree", "comment": "So far, we did the following:\nThe variance calibration can be performed on the training set instead of the validation set. In the calibration of AP2 we correct the ratio of the noise due to dropout in the last layer. Essentially dropout samples are needed with any relevant data. Training data works just as fine here. We get same calibration ratios (two significant digits) and same test likelihoods as reported.\nFor a fair comparison with AP1 method approximating standard dropout we estimate a constant variance to be added in the last layer. It is estimated as the average variance of samples due to dropout. We would not expect such calibration to be better than MC with many samples, which it tries to approximate. It does indeed improve test likelihoods of AP1 to the level of about MC-10 likelihood.\nThese changes will be incorporated.\n\nCalibration of the scale parameter on the validation set is a totally different matter. All the considered methods may and would benefit from such cross-validation. Note also that results reported in the literature (eg. SOTA results we quote in Table 3) would be likely improved this way as well. We therefore prefer to leave this question out of scope of this contribution. Preliminary, cross-validation improves all methods in our experiments, making the test likelihoods not so clearly distinct. This also indicates that we should apply scale regularization during training to learn such models properly.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper280/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper280/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper280/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers", "abstract": "Probabilistic Neural Networks deal with various sources of stochasticity: input noise, dropout, stochastic neurons, parameter uncertainties modeled as random variables, etc.\nIn this paper we revisit a feed-forward propagation approach that allows one to estimate for each neuron its mean and variance w.r.t. all mentioned sources of stochasticity. In contrast, standard NNs propagate only point estimates, discarding the uncertainty.\nMethods propagating also the variance have been proposed by several authors in different context. The view presented here attempts to clarify the assumptions and derivation behind such methods, relate them to classical NNs and broaden their scope of applicability.\nThe main technical contributions are new approximations for the distributions of argmax and max-related transforms, which allow for fully analytic uncertainty propagation in networks with softmax and max-pooling layers as well as leaky ReLU activations.\nWe evaluate the accuracy of the approximation and suggest a simple calibration. Applying the method to networks with dropout allows for faster training and gives improved test likelihoods without the need of sampling.", "keywords": ["probabilistic neural network", "uncertainty", "dropout", "bayesian", "softmax", "argmax", "logsumexp"], "authorids": ["shekhovtsov@gmail.com", "bflach@inf.tu-dresden.de"], "authors": ["Alexander Shekhovtsov", "Boris Flach"], "TL;DR": "Approximating mean and variance of the NN output over noisy input / dropout / uncertain parameters. Analytic approximations for argmax, softmax and max layers.", "pdf": "/pdf/4ef42fa3995a010e559ae67213b0387f45015a99.pdf", "paperhash": "shekhovtsov|feedforward_propagation_in_probabilistic_neural_networks_with_categorical_and_max_layers", "_bibtex": "@inproceedings{\nshekhovtsov2018feedforward,\ntitle={Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers},\nauthor={Alexander Shekhovtsov and Boris Flach},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkMuPjRcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper280/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614859, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkMuPjRcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper280/Authors", "ICLR.cc/2019/Conference/Paper280/Reviewers", "ICLR.cc/2019/Conference/Paper280/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper280/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper280/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper280/Authors|ICLR.cc/2019/Conference/Paper280/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper280/Reviewers", "ICLR.cc/2019/Conference/Paper280/Authors", "ICLR.cc/2019/Conference/Paper280/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614859}}}, {"id": "rkla8-4tRm", "original": null, "number": 6, "cdate": 1543221588585, "ddate": null, "tcdate": 1543221588585, "tmdate": 1543221588585, "tddate": null, "forum": "SkMuPjRcKQ", "replyto": "r1xl0Njl6Q", "invitation": "ICLR.cc/2019/Conference/-/Paper280/Official_Comment", "content": {"title": "yes", "comment": "Yes I think estimating the final scaling of logits (by maximizing validation log likelihood) would provide a fair comparison. If your method would then still outperform it would be more convincing to me."}, "signatures": ["ICLR.cc/2019/Conference/Paper280/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper280/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper280/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers", "abstract": "Probabilistic Neural Networks deal with various sources of stochasticity: input noise, dropout, stochastic neurons, parameter uncertainties modeled as random variables, etc.\nIn this paper we revisit a feed-forward propagation approach that allows one to estimate for each neuron its mean and variance w.r.t. all mentioned sources of stochasticity. In contrast, standard NNs propagate only point estimates, discarding the uncertainty.\nMethods propagating also the variance have been proposed by several authors in different context. The view presented here attempts to clarify the assumptions and derivation behind such methods, relate them to classical NNs and broaden their scope of applicability.\nThe main technical contributions are new approximations for the distributions of argmax and max-related transforms, which allow for fully analytic uncertainty propagation in networks with softmax and max-pooling layers as well as leaky ReLU activations.\nWe evaluate the accuracy of the approximation and suggest a simple calibration. Applying the method to networks with dropout allows for faster training and gives improved test likelihoods without the need of sampling.", "keywords": ["probabilistic neural network", "uncertainty", "dropout", "bayesian", "softmax", "argmax", "logsumexp"], "authorids": ["shekhovtsov@gmail.com", "bflach@inf.tu-dresden.de"], "authors": ["Alexander Shekhovtsov", "Boris Flach"], "TL;DR": "Approximating mean and variance of the NN output over noisy input / dropout / uncertain parameters. Analytic approximations for argmax, softmax and max layers.", "pdf": "/pdf/4ef42fa3995a010e559ae67213b0387f45015a99.pdf", "paperhash": "shekhovtsov|feedforward_propagation_in_probabilistic_neural_networks_with_categorical_and_max_layers", "_bibtex": "@inproceedings{\nshekhovtsov2018feedforward,\ntitle={Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers},\nauthor={Alexander Shekhovtsov and Boris Flach},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkMuPjRcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper280/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614859, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkMuPjRcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper280/Authors", "ICLR.cc/2019/Conference/Paper280/Reviewers", "ICLR.cc/2019/Conference/Paper280/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper280/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper280/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper280/Authors|ICLR.cc/2019/Conference/Paper280/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper280/Reviewers", "ICLR.cc/2019/Conference/Paper280/Authors", "ICLR.cc/2019/Conference/Paper280/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614859}}}, {"id": "HyxIiujrCX", "original": null, "number": 5, "cdate": 1542989982493, "ddate": null, "tcdate": 1542989982493, "tmdate": 1542989982493, "tddate": null, "forum": "SkMuPjRcKQ", "replyto": "SkMuPjRcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper280/Official_Comment", "content": {"title": "Revision", "comment": "We thank all reviewers for their comments. We revisited the submission accordingly as follows.\n\nA comprehensive evaluation of how well the proposed approximations models the simulated expectations as well as its derivatives in individual layers (LReLU, Softmax, Argmax) is included now in the Appendix B.\n\nAs was pointed out by Reviewer 1, the linear time approximation for softmax was ad-hoc. This has showed up in the evaluation, esp. for larger noises. We have revisited this approximation (see Linear Time Approximation in 4.1). It is still linear complexity and has a good accuracy (evaluated in Appendix B). This did not affect other results in the paper because the learned models have small variance on the output (a consequence of maximum likelihood estimation) for which the ad-hoc approximation was accurate enough. \n\nThe evaluation shows that the approximation for softmax is reasonable in a big range in the sense that it models a function that is very similar to the expected value of softmax and has similar gradients.  All proposed approximations are comparable to and consistently more accurate for small noise range than MC-100. When a higher accuracy is required (in some other potential applications) an MC estimate correcting our approximations can be used (i.e. using analytic approximation as a baseline for variance reduction).\n\nWe removed the example with ReLU and updated the presentation of the maximum of two variables and Leaky ReLU to cite the preceding work and highlight that we propose a simplification, which is more practical for use in end-to-end training (sec. 4.3, evaluated in in Appendix B).\n\nWe made the necessary clarifications (confusion about posterior, non-random weights, notation of the noise variance in the experiments, etc.).\n\nMore advanced calibration using the validation set (discussed with Reviewer 3) of AP1 and calibration of AP1 / AP2 during learning is left for future work."}, "signatures": ["ICLR.cc/2019/Conference/Paper280/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper280/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper280/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers", "abstract": "Probabilistic Neural Networks deal with various sources of stochasticity: input noise, dropout, stochastic neurons, parameter uncertainties modeled as random variables, etc.\nIn this paper we revisit a feed-forward propagation approach that allows one to estimate for each neuron its mean and variance w.r.t. all mentioned sources of stochasticity. In contrast, standard NNs propagate only point estimates, discarding the uncertainty.\nMethods propagating also the variance have been proposed by several authors in different context. The view presented here attempts to clarify the assumptions and derivation behind such methods, relate them to classical NNs and broaden their scope of applicability.\nThe main technical contributions are new approximations for the distributions of argmax and max-related transforms, which allow for fully analytic uncertainty propagation in networks with softmax and max-pooling layers as well as leaky ReLU activations.\nWe evaluate the accuracy of the approximation and suggest a simple calibration. Applying the method to networks with dropout allows for faster training and gives improved test likelihoods without the need of sampling.", "keywords": ["probabilistic neural network", "uncertainty", "dropout", "bayesian", "softmax", "argmax", "logsumexp"], "authorids": ["shekhovtsov@gmail.com", "bflach@inf.tu-dresden.de"], "authors": ["Alexander Shekhovtsov", "Boris Flach"], "TL;DR": "Approximating mean and variance of the NN output over noisy input / dropout / uncertain parameters. Analytic approximations for argmax, softmax and max layers.", "pdf": "/pdf/4ef42fa3995a010e559ae67213b0387f45015a99.pdf", "paperhash": "shekhovtsov|feedforward_propagation_in_probabilistic_neural_networks_with_categorical_and_max_layers", "_bibtex": "@inproceedings{\nshekhovtsov2018feedforward,\ntitle={Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers},\nauthor={Alexander Shekhovtsov and Boris Flach},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkMuPjRcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper280/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614859, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkMuPjRcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper280/Authors", "ICLR.cc/2019/Conference/Paper280/Reviewers", "ICLR.cc/2019/Conference/Paper280/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper280/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper280/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper280/Authors|ICLR.cc/2019/Conference/Paper280/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper280/Reviewers", "ICLR.cc/2019/Conference/Paper280/Authors", "ICLR.cc/2019/Conference/Paper280/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614859}}}, {"id": "r1xl0Njl6Q", "original": null, "number": 3, "cdate": 1541612743857, "ddate": null, "tcdate": 1541612743857, "tmdate": 1541612743857, "tddate": null, "forum": "SkMuPjRcKQ", "replyto": "HyeBAMSynQ", "invitation": "ICLR.cc/2019/Conference/-/Paper280/Official_Comment", "content": {"title": "rebuttal and clarification requests", "comment": "\u201cScaling weights in standard test-time dropout\u201d:\nThe method AP1 is consistent with the mentioned standard scaling (by the probability of the activation being not dropped). More precisely, we follow the common implementation of dropout (described in Srivastava et al., 2014.) where the multiplicative noise variable Z attains value $1/p$ with probability $p$ and $0$ with probability 1-p at training time. Since the expectation of this variable is 1, there is no scaling of weights needed at test time.\n\n\u201cCalibration of AP1\u201d\nThis might be a valid idea, however it is not completely clear what is ment. Note that with AP2 we are scaling only the variances of the logits not logits themselves. In AP1 variances of logits are not available. The scaling of logits is a free degree of freedom of the last linear layer. Is the proposition then to estimate the final rescaling to maximize the likelihood of the validation set?\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper280/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper280/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper280/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers", "abstract": "Probabilistic Neural Networks deal with various sources of stochasticity: input noise, dropout, stochastic neurons, parameter uncertainties modeled as random variables, etc.\nIn this paper we revisit a feed-forward propagation approach that allows one to estimate for each neuron its mean and variance w.r.t. all mentioned sources of stochasticity. In contrast, standard NNs propagate only point estimates, discarding the uncertainty.\nMethods propagating also the variance have been proposed by several authors in different context. The view presented here attempts to clarify the assumptions and derivation behind such methods, relate them to classical NNs and broaden their scope of applicability.\nThe main technical contributions are new approximations for the distributions of argmax and max-related transforms, which allow for fully analytic uncertainty propagation in networks with softmax and max-pooling layers as well as leaky ReLU activations.\nWe evaluate the accuracy of the approximation and suggest a simple calibration. Applying the method to networks with dropout allows for faster training and gives improved test likelihoods without the need of sampling.", "keywords": ["probabilistic neural network", "uncertainty", "dropout", "bayesian", "softmax", "argmax", "logsumexp"], "authorids": ["shekhovtsov@gmail.com", "bflach@inf.tu-dresden.de"], "authors": ["Alexander Shekhovtsov", "Boris Flach"], "TL;DR": "Approximating mean and variance of the NN output over noisy input / dropout / uncertain parameters. Analytic approximations for argmax, softmax and max layers.", "pdf": "/pdf/4ef42fa3995a010e559ae67213b0387f45015a99.pdf", "paperhash": "shekhovtsov|feedforward_propagation_in_probabilistic_neural_networks_with_categorical_and_max_layers", "_bibtex": "@inproceedings{\nshekhovtsov2018feedforward,\ntitle={Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers},\nauthor={Alexander Shekhovtsov and Boris Flach},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkMuPjRcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper280/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614859, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkMuPjRcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper280/Authors", "ICLR.cc/2019/Conference/Paper280/Reviewers", "ICLR.cc/2019/Conference/Paper280/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper280/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper280/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper280/Authors|ICLR.cc/2019/Conference/Paper280/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper280/Reviewers", "ICLR.cc/2019/Conference/Paper280/Authors", "ICLR.cc/2019/Conference/Paper280/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614859}}}, {"id": "rJxcjNogTX", "original": null, "number": 2, "cdate": 1541612705596, "ddate": null, "tcdate": 1541612705596, "tmdate": 1541612705596, "tddate": null, "forum": "SkMuPjRcKQ", "replyto": "HklGBGu8h7", "invitation": "ICLR.cc/2019/Conference/-/Paper280/Official_Comment", "content": {"title": "rebuttal and clarification requests", "comment": "\u201cEq. (22)-(25) is exactly the same as Gast and Roth, 2018\u201d\nExpressions for the mean are indeed the same and well known. Expressions for the variance are different.\nOur main observation here is that that the variance of the output is proportional to the variance of the input and the proportionality factor is a function of one variable $a$ that can be well approximated, guaranteeing non-negativity and correct asymptotes for large and small inputs. In contrast expression (13b) in [Gast and Roth] requires computing (10b) twice that involves a difference of expressions depending on normal cdf, does not readily simplify and may result in negative values. They mention of adding 1e-4 variance to all activations for numerical reason, a quite large value that suggests that the problem is not void. We do not need to add any such constant, furthermore, it could negatively impact the accuracy when all activations are small due to the scaling. This is particularly important in approximating dataset statistics (Fig B.1).\n\n\u201cPosterior distribution. What is the corresponding prior?\u201d\nThe term posterior is used to denote the distribution of interest when conditioned on some observations, following graphical models (c.f. maximum a posteriori solution in MRF / CRF). (Technically, the a priori distribution of the outputs without any observations is also existing but in our case is not relevant).\nThe weights are indeed not treated as random variables in this work. Were they random, we would still speak of the posterior distribution of the outputs given the inputs and of the posterior distribution of the weights given both inputs and outputs in the context of Bayesian learning.\n\n\u201cW is not treated as random variables\u201d\nThis will be made clear. However, this is only for reasons of simplicity. W can be made random, in which case the variance for the linear layer needs to use the expression of the variance of product of independent random variables (weights and layer inputs). The propagation method however cannot be expected to work very well in convolutional networks since outputs will be strongly spatially correlated. This restriction applies to all related methods: Wang et al. (2016), Hern\u00b4andez-Lobato & Adams, 2015), etc. which do not do experiments with conv networks.\n\n\u201cEmpirical results on how accurate the approximation in Eq. (10) is\u201d\nWe interpret: \u201chow accurate is the step to forget the shape of the Gumbel distribution and only take into account its variance?\u201d. This is not exactly what happens, it is not a standalone approximation step. The approximating family for U with U_k = (X_y+Gamma_y) - (X_k+Gamma_k) is chosen to be multivariate logistic. This is the exact distribution in case X were deterministic (in case of two variables Gamma_2 - Gamma_1 is logistic). For random X, we lose in approximating the real distribution of U (which is hoped to be bell-shaped due to summing multiple terms) by a bell shaped multivariate logistic distribution. The step around (10) cannot be evaluated separately.\n\n\u201cinvestigate more about the accuracy of the approximation (13)-(15)\u201d\nThis we will gladly do. We will generate distributions for X and evaluate the quality of both approximations w.r.t. MC sampling.\n\n\u201c\\sigma^* needs to be computed using multi-pass MC estimates\u201d\nOnly once after training is done. I.e. not adding to the cost of training iterations nor at the test time. Future work will address calibration during training.\n\n\u201cThe noise level (std of 10^-4 and 0.01) seems quite small in Table 1\u201d\nThe numbers $10^-4$ and $0.01$ are the variances. The standard deviations are thus 0.01 and 0.1, respectively. The input range is in the interval [0,1], so the noise level is not small. The accuracy can be compared to the results of \n\nAdel Bibi, Modar Alfadly, Bernard  Ghanem, \"Analytic Expressions for Probabilistic Moments of PL-DNN with Gaussian Input\", (CVPR 2018) [Oral]\nIn Table 1, they evaluate the accuracy for LeNet of the statistics of logits with input noise variance=1 and signal range [0,255], which corresponds to std of 0.0039 in our scale, smaller than we evaluate. The ratio of variances 0.4-0.6 appears worse than both tested cases in our Table 1.\n\n\u201cRelation to natural-parameter networks (NPN) Wang et al. (2016),\u201d\nNPNs are a similar propagation method to ours. They are more general in allowing approximation by a member of exponential family in each layer. In practice, for networks with real-valued weights, the only reasonable model for the outputs of a linear layer is the Normal distribution (an exponential family member with mean and variance as sufficient statistics and unrestricted domain). They have simple propagation rules only with exponential nonlinearities. Their results may benefit from our numerically stable approximation for ReLU and the new approximation for softmax. Random weights is a difference that is not essential for the method as discussed above."}, "signatures": ["ICLR.cc/2019/Conference/Paper280/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper280/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper280/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers", "abstract": "Probabilistic Neural Networks deal with various sources of stochasticity: input noise, dropout, stochastic neurons, parameter uncertainties modeled as random variables, etc.\nIn this paper we revisit a feed-forward propagation approach that allows one to estimate for each neuron its mean and variance w.r.t. all mentioned sources of stochasticity. In contrast, standard NNs propagate only point estimates, discarding the uncertainty.\nMethods propagating also the variance have been proposed by several authors in different context. The view presented here attempts to clarify the assumptions and derivation behind such methods, relate them to classical NNs and broaden their scope of applicability.\nThe main technical contributions are new approximations for the distributions of argmax and max-related transforms, which allow for fully analytic uncertainty propagation in networks with softmax and max-pooling layers as well as leaky ReLU activations.\nWe evaluate the accuracy of the approximation and suggest a simple calibration. Applying the method to networks with dropout allows for faster training and gives improved test likelihoods without the need of sampling.", "keywords": ["probabilistic neural network", "uncertainty", "dropout", "bayesian", "softmax", "argmax", "logsumexp"], "authorids": ["shekhovtsov@gmail.com", "bflach@inf.tu-dresden.de"], "authors": ["Alexander Shekhovtsov", "Boris Flach"], "TL;DR": "Approximating mean and variance of the NN output over noisy input / dropout / uncertain parameters. Analytic approximations for argmax, softmax and max layers.", "pdf": "/pdf/4ef42fa3995a010e559ae67213b0387f45015a99.pdf", "paperhash": "shekhovtsov|feedforward_propagation_in_probabilistic_neural_networks_with_categorical_and_max_layers", "_bibtex": "@inproceedings{\nshekhovtsov2018feedforward,\ntitle={Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers},\nauthor={Alexander Shekhovtsov and Boris Flach},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkMuPjRcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper280/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614859, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkMuPjRcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper280/Authors", "ICLR.cc/2019/Conference/Paper280/Reviewers", "ICLR.cc/2019/Conference/Paper280/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper280/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper280/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper280/Authors|ICLR.cc/2019/Conference/Paper280/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper280/Reviewers", "ICLR.cc/2019/Conference/Paper280/Authors", "ICLR.cc/2019/Conference/Paper280/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614859}}}, {"id": "Hyls8EixaQ", "original": null, "number": 1, "cdate": 1541612627364, "ddate": null, "tcdate": 1541612627364, "tmdate": 1541612627364, "tddate": null, "forum": "SkMuPjRcKQ", "replyto": "r1gp_wDw3m", "invitation": "ICLR.cc/2019/Conference/-/Paper280/Official_Comment", "content": {"title": "rebuttal and clarification requests", "comment": "\u201cContribution 1: new insights about the method?\u201d\nWe mean: 1) a clear self-contained derivation, 2) the latent variable view of sigmoid that later extends to softmax, 3) the connection to standard NNs, 4) possibility to choose the approximating family at each layer in order to simplify the propagation. We believe this is useful and may help understanding by non-experts. Note that the frequently cited ADF is an incremental method for parameter estimation. Only example 3 (ReLU) is not directly used in the subsequent constructions. Do you definitely recommend to shorten this part? The numerical evaluation of accuracy has not been reported before with such methods. \n\n\u201cContribution 2: an approximation for argmax operations that avoids resorting to the normal cdf function that has numerical stability issues\u201d\nThere is likely to be a misunderstanding. The challenge of argmax is that it is a multivariate nonlinear function. There were no previously proposed analytic approximations using the normal cdf or not. Furthermore, evaluating the multivariate normal cdf is a hard computational problem.\n\nTo support the utility of the results, let us mention one more paper we discovered that achieved improvements in speech recognition with the uncertainty propagation but explicitly mentions that the approximation for softmax was an unsolved problem and a significant limitation:\nAstudillo et al. (2014) \u201cACCOUNTING FOR THE RESIDUAL UNCERTAINTY OF MULTI-LAYER PERCEPTRON BASED FEATURES\u201d\n\n\u201cA direct comparison with the classical paper (Hern\u00b4andez-Lobato & Adams, 2015)\u201d\nThis work performs Bayesian learning, which we don\u2019t do. Our work is related to the paragraph \u201cIncorporating the likelihood factors\u201d. They describe the case of ReLU with a numerical fix. Do you mean specifically comparing this approximation alone against ours? We do not see any other direct comparison applicable. The softmax is not considered there (they consider regression problems with fully connected 1-4 layers).\n\n\u201cComparison with a robust implementation of the normal cdf/pdf\u201d\nAccording to our experiments, the bottleneck in the accuracy is currently due to the independence assumption. More accurate calculation of the variance as in (Hern\u00b4andez-Lobato & Adams, 2015) is possible, but is more computationally costly. Truncation in order to force non-negativity of variance is particularly undesirable. Because the accumulated scaling in a deep network can lead to all activations being either large or small, a valid asymptotic behaviour is required. This is particularly important for normalization (Fig. B1). We propose that in the context of NNs cheaper approximations with valid asymptotes are more practical. \n\n\u201cShould not S_{n-1} be defined as the softmax operation\u201d\nWe quote the definition by Malik and Abraham. There is indeed relation S_{n-1}(u)  = softmax(0, u_1, \u2026 u_{n-1}). Think of the case with two variables X_1, X_2: we have U_1 =X_1 - X_2  and S_{1}(u) = sigmoid(u)."}, "signatures": ["ICLR.cc/2019/Conference/Paper280/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper280/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper280/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers", "abstract": "Probabilistic Neural Networks deal with various sources of stochasticity: input noise, dropout, stochastic neurons, parameter uncertainties modeled as random variables, etc.\nIn this paper we revisit a feed-forward propagation approach that allows one to estimate for each neuron its mean and variance w.r.t. all mentioned sources of stochasticity. In contrast, standard NNs propagate only point estimates, discarding the uncertainty.\nMethods propagating also the variance have been proposed by several authors in different context. The view presented here attempts to clarify the assumptions and derivation behind such methods, relate them to classical NNs and broaden their scope of applicability.\nThe main technical contributions are new approximations for the distributions of argmax and max-related transforms, which allow for fully analytic uncertainty propagation in networks with softmax and max-pooling layers as well as leaky ReLU activations.\nWe evaluate the accuracy of the approximation and suggest a simple calibration. Applying the method to networks with dropout allows for faster training and gives improved test likelihoods without the need of sampling.", "keywords": ["probabilistic neural network", "uncertainty", "dropout", "bayesian", "softmax", "argmax", "logsumexp"], "authorids": ["shekhovtsov@gmail.com", "bflach@inf.tu-dresden.de"], "authors": ["Alexander Shekhovtsov", "Boris Flach"], "TL;DR": "Approximating mean and variance of the NN output over noisy input / dropout / uncertain parameters. Analytic approximations for argmax, softmax and max layers.", "pdf": "/pdf/4ef42fa3995a010e559ae67213b0387f45015a99.pdf", "paperhash": "shekhovtsov|feedforward_propagation_in_probabilistic_neural_networks_with_categorical_and_max_layers", "_bibtex": "@inproceedings{\nshekhovtsov2018feedforward,\ntitle={Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers},\nauthor={Alexander Shekhovtsov and Boris Flach},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkMuPjRcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper280/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614859, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkMuPjRcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper280/Authors", "ICLR.cc/2019/Conference/Paper280/Reviewers", "ICLR.cc/2019/Conference/Paper280/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper280/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper280/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper280/Authors|ICLR.cc/2019/Conference/Paper280/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper280/Reviewers", "ICLR.cc/2019/Conference/Paper280/Authors", "ICLR.cc/2019/Conference/Paper280/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614859}}}, {"id": "r1gp_wDw3m", "original": null, "number": 3, "cdate": 1541007220756, "ddate": null, "tcdate": 1541007220756, "tmdate": 1541534128479, "tddate": null, "forum": "SkMuPjRcKQ", "replyto": "SkMuPjRcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper280/Official_Review", "content": {"title": "Novel contribution to propagate uncertainty across argmax/max operations. Some experiments are missing to show the real benefit of the method in practical scenarios.", "review": "* Summary\n\nThe authors focus on the problem of uncertainty propagation DNN. The authors claim two main contributions: they revisit the assumptions of the feed forward method (proposed by several authors as an inference method for BNNs based on ADF/EP) and proposed a new approximation for argmax/max based functions that allows to propagated the first two moments analytically. \n\n* Comments:\n\nThe authors claim two main contributions: an analysis for the feed forward method (sections 2 and 3) previously proposed by several authors as an inference method for BNN based on ADF/EP, and a new method to propagate the uncertainty through argmax/max based operations (section 4).\n\nRegarding the first contribution, I was expecting some new insights about the method that I did not find. I would suggest to focus on the second contribution and refactor this section as a background section. I would make it shorter, focusing on the representation of probabilities as latent variables trough a function, which is the important bit to understand the real contribution of the paper described in section 4. I would also remove some examples that do not seem critical to understand the rest of the paper and just increase its length.\n\nThe second contribution is quite novel. The authors propose a new approximation of argmax/max operations. The firstly proposed an approximation for argmax operations, e.g. latent variable view of the softmax, that avoids resorting to the normal cdf function that has numerical stability issues. Secondly, they suggest an approximation for max based operations, e.g. leaky relu, that again, does not depend on the gaussian cdf. \n\nIn the experimental section, the authors test:\na)\tThe accuracy of the proposed method approximating the posterior of the neurons\nb)\tEnd-to-end training benefits\n\nIn a) they use MC to collect the ground truth statistics and compare the proposed method (AP2) with a classical NN (AP1). The analysis is nice but I miss a comparison with other state-of-the-art methods. In particular, the authors claim that the novelty of their method compared to other feed-forward methods is that they can propagate the uncertainty through argmax/max operations analytically. They do not compare with these other feed forwards methods to show the benefit of this.  This is shown in the end-to-end training experiments; however, I would like to see a direct comparison with the classical paper (Hern\u00b4andez-Lobato & Adams, 2015). Finally, one of the justifications of the approximations that they propose is to avoid the numerical issues of the standard cdf. Have the authors compared with this, e.g. eq 18a, 18b? Using a robust implementation of the normal cdf/pdf function and further truncating them to avoid negative variances?\n\ntypo: Shortly before eq. 12, Should not S_{n-1} be defined as the softmax operation?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper280/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers", "abstract": "Probabilistic Neural Networks deal with various sources of stochasticity: input noise, dropout, stochastic neurons, parameter uncertainties modeled as random variables, etc.\nIn this paper we revisit a feed-forward propagation approach that allows one to estimate for each neuron its mean and variance w.r.t. all mentioned sources of stochasticity. In contrast, standard NNs propagate only point estimates, discarding the uncertainty.\nMethods propagating also the variance have been proposed by several authors in different context. The view presented here attempts to clarify the assumptions and derivation behind such methods, relate them to classical NNs and broaden their scope of applicability.\nThe main technical contributions are new approximations for the distributions of argmax and max-related transforms, which allow for fully analytic uncertainty propagation in networks with softmax and max-pooling layers as well as leaky ReLU activations.\nWe evaluate the accuracy of the approximation and suggest a simple calibration. Applying the method to networks with dropout allows for faster training and gives improved test likelihoods without the need of sampling.", "keywords": ["probabilistic neural network", "uncertainty", "dropout", "bayesian", "softmax", "argmax", "logsumexp"], "authorids": ["shekhovtsov@gmail.com", "bflach@inf.tu-dresden.de"], "authors": ["Alexander Shekhovtsov", "Boris Flach"], "TL;DR": "Approximating mean and variance of the NN output over noisy input / dropout / uncertain parameters. Analytic approximations for argmax, softmax and max layers.", "pdf": "/pdf/4ef42fa3995a010e559ae67213b0387f45015a99.pdf", "paperhash": "shekhovtsov|feedforward_propagation_in_probabilistic_neural_networks_with_categorical_and_max_layers", "_bibtex": "@inproceedings{\nshekhovtsov2018feedforward,\ntitle={Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers},\nauthor={Alexander Shekhovtsov and Boris Flach},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkMuPjRcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper280/Official_Review", "cdate": 1542234497794, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkMuPjRcKQ", "replyto": "SkMuPjRcKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper280/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335689044, "tmdate": 1552335689044, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper280/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HklGBGu8h7", "original": null, "number": 2, "cdate": 1540944441567, "ddate": null, "tcdate": 1540944441567, "tmdate": 1541534128271, "tddate": null, "forum": "SkMuPjRcKQ", "replyto": "SkMuPjRcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper280/Official_Review", "content": {"title": "Review", "review": "This paper revisits the feed-forward propagation of mean and variance in neurons. In particular, it addresses the problem of propagating uncertainty through max-pooling layers and softmax. This is important since previous methods on probabilistic neural networks have not handled these challenges, hence preventing them from using max-pooling and softmax in a principled way.\n\nIn general, the authors did a good job approximating the mean and variance for the output of max-pooling and softmax. I have several concerns:\n\nThe authors claimed that they derived new approximation for leaky ReLU as well. It seems the approximation in Eq. (22)-(25) is exactly the same as Gast and Roth, 2018, both leveraging the results on obtaining the maximum of two Gaussian random variables.\n\nThe Bayesian formulation is not clear enough and seems a bit problematic in Sec. 2. For example, in Eq. (2), the authors mentioned p(X^k | x_0) as the posterior distribution. In this case, what is the corresponding prior? Besides, it should be made clear from the beginning that the network parameters W is not treated as random variables.\n\nIt is an interesting idea to incorporate the Gumbel distribution\u2019s variance into the approximation in Eq. (10). Do you have any empirical results on how accurate the approximation in Eq. (10) is?\n\nSimilarly, the approximation from Eq. (13) to Eq. (14)-(15) seems a bit ad-hoc. It is good to know that the approximation is exact in the case of two input variables. However, it would be more convincing if the authors could investigate more about the accuracy of the approximation (either empirically or theoretically) when there are more than two variables.\n\nThe organization of the paper could be improved. The notion of nonlinearity is not mentioned until Sec. 3. When reading Sec. 2, one would wonder where the nonlinear transformation happens. It would help to clarify a bit at the start of Sec. 2.\n\nIn terms of experiments, one important benefit of feed-forward propagation is that it avoid the multi-pass MC estimates. However, it seems the performance boost on NLL mainly comes from the calibration, where \\sigma^* needs to be computed using multi-pass MC estimates.\n\nThe noise level (std of 10^-4 and 0.01) seems quite small in Table 1. According to the results, it seems the error of \\sigma_2 increases a lot as the noise level goes from 10^-4 to 0.01, suggesting that the approximation does not work well when the input noise is large. How is the accuracy when the noise level further increases?\n\nUnlike the natural-parameter networks (NPN) in Wang et al. (2016), the proposed work assumes zero variance in the parameters W. It would be interesting to see whether the proposed methods could also improve NPN.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper280/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers", "abstract": "Probabilistic Neural Networks deal with various sources of stochasticity: input noise, dropout, stochastic neurons, parameter uncertainties modeled as random variables, etc.\nIn this paper we revisit a feed-forward propagation approach that allows one to estimate for each neuron its mean and variance w.r.t. all mentioned sources of stochasticity. In contrast, standard NNs propagate only point estimates, discarding the uncertainty.\nMethods propagating also the variance have been proposed by several authors in different context. The view presented here attempts to clarify the assumptions and derivation behind such methods, relate them to classical NNs and broaden their scope of applicability.\nThe main technical contributions are new approximations for the distributions of argmax and max-related transforms, which allow for fully analytic uncertainty propagation in networks with softmax and max-pooling layers as well as leaky ReLU activations.\nWe evaluate the accuracy of the approximation and suggest a simple calibration. Applying the method to networks with dropout allows for faster training and gives improved test likelihoods without the need of sampling.", "keywords": ["probabilistic neural network", "uncertainty", "dropout", "bayesian", "softmax", "argmax", "logsumexp"], "authorids": ["shekhovtsov@gmail.com", "bflach@inf.tu-dresden.de"], "authors": ["Alexander Shekhovtsov", "Boris Flach"], "TL;DR": "Approximating mean and variance of the NN output over noisy input / dropout / uncertain parameters. Analytic approximations for argmax, softmax and max layers.", "pdf": "/pdf/4ef42fa3995a010e559ae67213b0387f45015a99.pdf", "paperhash": "shekhovtsov|feedforward_propagation_in_probabilistic_neural_networks_with_categorical_and_max_layers", "_bibtex": "@inproceedings{\nshekhovtsov2018feedforward,\ntitle={Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers},\nauthor={Alexander Shekhovtsov and Boris Flach},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkMuPjRcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper280/Official_Review", "cdate": 1542234497794, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkMuPjRcKQ", "replyto": "SkMuPjRcKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper280/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335689044, "tmdate": 1552335689044, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper280/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HyeBAMSynQ", "original": null, "number": 1, "cdate": 1540473548922, "ddate": null, "tcdate": 1540473548922, "tmdate": 1541534128066, "tddate": null, "forum": "SkMuPjRcKQ", "replyto": "SkMuPjRcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper280/Official_Review", "content": {"title": "valid technical contribution. a little on the incremental side", "review": "The main contribution of the paper are methods for propagating approximate uncertainty in neural networks through max and argmax layers. The proposed methods are explained well. The paper is clearly written. The methods are validated in small scale experiments and seem to work well.\n\nThe proposed approach is not much more accurate than Monte Carlo dropout, but is more computationally efficient. The standard way of efficiently predicting at test time with a dropout-trained network is to simply scale the weights. Could the authors try calibration on networks of this type and compare against the proposed method with calibration? (i.e. scale the predicted logits of the standard test-time network to be on the same scale as the logits under your approach)", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper280/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers", "abstract": "Probabilistic Neural Networks deal with various sources of stochasticity: input noise, dropout, stochastic neurons, parameter uncertainties modeled as random variables, etc.\nIn this paper we revisit a feed-forward propagation approach that allows one to estimate for each neuron its mean and variance w.r.t. all mentioned sources of stochasticity. In contrast, standard NNs propagate only point estimates, discarding the uncertainty.\nMethods propagating also the variance have been proposed by several authors in different context. The view presented here attempts to clarify the assumptions and derivation behind such methods, relate them to classical NNs and broaden their scope of applicability.\nThe main technical contributions are new approximations for the distributions of argmax and max-related transforms, which allow for fully analytic uncertainty propagation in networks with softmax and max-pooling layers as well as leaky ReLU activations.\nWe evaluate the accuracy of the approximation and suggest a simple calibration. Applying the method to networks with dropout allows for faster training and gives improved test likelihoods without the need of sampling.", "keywords": ["probabilistic neural network", "uncertainty", "dropout", "bayesian", "softmax", "argmax", "logsumexp"], "authorids": ["shekhovtsov@gmail.com", "bflach@inf.tu-dresden.de"], "authors": ["Alexander Shekhovtsov", "Boris Flach"], "TL;DR": "Approximating mean and variance of the NN output over noisy input / dropout / uncertain parameters. Analytic approximations for argmax, softmax and max layers.", "pdf": "/pdf/4ef42fa3995a010e559ae67213b0387f45015a99.pdf", "paperhash": "shekhovtsov|feedforward_propagation_in_probabilistic_neural_networks_with_categorical_and_max_layers", "_bibtex": "@inproceedings{\nshekhovtsov2018feedforward,\ntitle={Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers},\nauthor={Alexander Shekhovtsov and Boris Flach},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkMuPjRcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper280/Official_Review", "cdate": 1542234497794, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkMuPjRcKQ", "replyto": "SkMuPjRcKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper280/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335689044, "tmdate": 1552335689044, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper280/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 12}