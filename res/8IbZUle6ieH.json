{"notes": [{"id": "8IbZUle6ieH", "original": "O0qkgbesbNC", "number": 1357, "cdate": 1601308151510, "ddate": null, "tcdate": 1601308151510, "tmdate": 1614985717446, "tddate": null, "forum": "8IbZUle6ieH", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Graph Neural Network Acceleration via Matrix Dimension Reduction", "authorids": ["~Shunhua_Jiang1", "~Yunze_Man2", "~Zhao_Song3", "~Danyang_Zhuo1"], "authors": ["Shunhua Jiang", "Yunze Man", "Zhao Song", "Danyang Zhuo"], "keywords": ["Graph Neural Networks", "Deep learning", "Optimization", "Kernel Method"], "abstract": "Graph Neural Networks (GNNs) have become the de facto method for machine learning on graph data (e.g., social networks, protein structures, code ASTs), but they require significant time and resource to train. One alternative method is Graph Neural Tangent Kernel (GNTK), a kernel method that corresponds to infinitely wide multi-layer GNNs. GNTK's parameters can be solved directly in a single step, avoiding time-consuming gradient descent. Today, GNTK is the state-of-the-art method to achieve high training speed without compromising accuracy. Unfortunately, solving for the kernel and searching for parameters can still take hours to days on real-world graphs. The current computation of GNTK has running time $O(N^4)$, where $N$ is the number of nodes in the graph. This prevents GNTK from scaling to datasets that contain large graphs. Theoretically, we present two techniques to speed up GNTK training while preserving the generalization error: (1) We use a novel matrix decoupling method to reduce matrix dimensions during the kernel solving. This allows us to reduce the dominated computation bottleneck term from $O(N^4)$ to $O(N^3)$. (2)  We apply sketching to further reduce the bottleneck term to $o(N^{\\omega})$, where $\\omega \\approx 2.373$ is the exponent of current matrix multiplication. Experimentally, we demonstrate that our approaches speed up kernel learning by up to $19\\times$ on real-world benchmark datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|graph_neural_network_acceleration_via_matrix_dimension_reduction", "supplementary_material": "/attachment/5295a94888896e20f0bdebdec089f64d35fc342e.zip", "pdf": "/pdf/e884ab4f5c59ba8567103b14a9b521426fa1bb68.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=H3fDDCTS4n", "_bibtex": "@misc{\njiang2021graph,\ntitle={Graph Neural Network Acceleration via Matrix Dimension Reduction},\nauthor={Shunhua Jiang and Yunze Man and Zhao Song and Danyang Zhuo},\nyear={2021},\nurl={https://openreview.net/forum?id=8IbZUle6ieH}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "rY85litDgyS", "original": null, "number": 1, "cdate": 1610040424979, "ddate": null, "tcdate": 1610040424979, "tmdate": 1610474024206, "tddate": null, "forum": "8IbZUle6ieH", "replyto": "8IbZUle6ieH", "invitation": "ICLR.cc/2021/Conference/Paper1357/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "While the author response clarified some concerns, it could not convince the reviewers that the current version of the paper should be accepted for publication at ICLR. \n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Neural Network Acceleration via Matrix Dimension Reduction", "authorids": ["~Shunhua_Jiang1", "~Yunze_Man2", "~Zhao_Song3", "~Danyang_Zhuo1"], "authors": ["Shunhua Jiang", "Yunze Man", "Zhao Song", "Danyang Zhuo"], "keywords": ["Graph Neural Networks", "Deep learning", "Optimization", "Kernel Method"], "abstract": "Graph Neural Networks (GNNs) have become the de facto method for machine learning on graph data (e.g., social networks, protein structures, code ASTs), but they require significant time and resource to train. One alternative method is Graph Neural Tangent Kernel (GNTK), a kernel method that corresponds to infinitely wide multi-layer GNNs. GNTK's parameters can be solved directly in a single step, avoiding time-consuming gradient descent. Today, GNTK is the state-of-the-art method to achieve high training speed without compromising accuracy. Unfortunately, solving for the kernel and searching for parameters can still take hours to days on real-world graphs. The current computation of GNTK has running time $O(N^4)$, where $N$ is the number of nodes in the graph. This prevents GNTK from scaling to datasets that contain large graphs. Theoretically, we present two techniques to speed up GNTK training while preserving the generalization error: (1) We use a novel matrix decoupling method to reduce matrix dimensions during the kernel solving. This allows us to reduce the dominated computation bottleneck term from $O(N^4)$ to $O(N^3)$. (2)  We apply sketching to further reduce the bottleneck term to $o(N^{\\omega})$, where $\\omega \\approx 2.373$ is the exponent of current matrix multiplication. Experimentally, we demonstrate that our approaches speed up kernel learning by up to $19\\times$ on real-world benchmark datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|graph_neural_network_acceleration_via_matrix_dimension_reduction", "supplementary_material": "/attachment/5295a94888896e20f0bdebdec089f64d35fc342e.zip", "pdf": "/pdf/e884ab4f5c59ba8567103b14a9b521426fa1bb68.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=H3fDDCTS4n", "_bibtex": "@misc{\njiang2021graph,\ntitle={Graph Neural Network Acceleration via Matrix Dimension Reduction},\nauthor={Shunhua Jiang and Yunze Man and Zhao Song and Danyang Zhuo},\nyear={2021},\nurl={https://openreview.net/forum?id=8IbZUle6ieH}\n}"}, "tags": [], "invitation": {"reply": {"forum": "8IbZUle6ieH", "replyto": "8IbZUle6ieH", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040424964, "tmdate": 1610474024190, "id": "ICLR.cc/2021/Conference/Paper1357/-/Decision"}}}, {"id": "GYhwJl3tXno", "original": null, "number": 3, "cdate": 1603869694495, "ddate": null, "tcdate": 1603869694495, "tmdate": 1606815175236, "tddate": null, "forum": "8IbZUle6ieH", "replyto": "8IbZUle6ieH", "invitation": "ICLR.cc/2021/Conference/Paper1357/-/Official_Review", "content": {"title": "Interesting but misses convincing evaluation and has questionable results. ", "review": "\n#### **Post Rebuttal**\nI'm afraid the authors failed to answer my main question regarding the results and the applicability of their proposed approximation.\n\nTherefore, I decide to keep my score.\n\n---\n#### **Contribution Claims** \nThe paper vouches for the use of GNTK as a strong method for graph learning combining good properties of kernels and GNNs. However, it raises the issue of time complexity for calculating the kernel scaling with $O(N^4)$, and addresses it by suggesting two techniques to gain speed up. The proposed techniques are based on matrix decoupling, which simply improves complexity, and matrix sketching which approximates the correct kernel (in probability).\nThe authors further present a theoretical result that similar generalization guarantees, as attributed to the exact GNTK, are achieved with their approximation.\n\n**Strengths** \n- The paper gains a major speed up in GNTK calculation with no harm to performance (*up to a clarification regarding the reported results. See first item in weaknesses section) \n- The method generalization properties are analyzed and shown to be preserved.\n- Although the paper is packed with lots of details, the flow of the paper allows the reader to pick upon the general ideas presented in the paper.\n\n**Weaknesses** \n- *Results* - I have concerns regarding the reported performance in Table 2. I find it quite uncanny that the numbers are *exactly* the same as achieved by exact GNTK, although the proposed method is an approximation which also utilizes a stochastic component. In case I misunderstood, I would kindly ask the authors to clarify this point. \n- *Discussion compared to GNTK* - A discussion on how the exact GNTK generalization bound compares to the approximate one is missing.\n- *Related work* - The paper does not explain and mention previous and relevant uses of matrix sketching (either in deep learning or kernel methods in general) which makes it hard for someone who is not familiar with the term to understand solely from the paper what it means. It is also unclear how innovative the use of it is. \n- *Experimental setting* - Regarding the evaluation on social datasets, it is mentioned that a degree feature was added as an input. Is it true for the other baselines as well? \n- *Missing convincing use-cases* - As the paper claims to gain speed up making GNTK scalable to larger datasets, I would have liked to see the performance both in time and accuracy of the proposed method. \n\n#### **Decision Recommendation**\nFor now, I suggest to reject the paper.  \nI think that it is interesting and valuable but feels that it needs to be solidified. \nUpon clarification of the questionable results I would consider changing my decision. \n\n#### **Other Comments**\n- Typos - the paper contains many typos. e.g, Theorem 4.1 \"...n graphs n graphs...\" repeating twice.  \n\n\n\n\n\n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper1357/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1357/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Neural Network Acceleration via Matrix Dimension Reduction", "authorids": ["~Shunhua_Jiang1", "~Yunze_Man2", "~Zhao_Song3", "~Danyang_Zhuo1"], "authors": ["Shunhua Jiang", "Yunze Man", "Zhao Song", "Danyang Zhuo"], "keywords": ["Graph Neural Networks", "Deep learning", "Optimization", "Kernel Method"], "abstract": "Graph Neural Networks (GNNs) have become the de facto method for machine learning on graph data (e.g., social networks, protein structures, code ASTs), but they require significant time and resource to train. One alternative method is Graph Neural Tangent Kernel (GNTK), a kernel method that corresponds to infinitely wide multi-layer GNNs. GNTK's parameters can be solved directly in a single step, avoiding time-consuming gradient descent. Today, GNTK is the state-of-the-art method to achieve high training speed without compromising accuracy. Unfortunately, solving for the kernel and searching for parameters can still take hours to days on real-world graphs. The current computation of GNTK has running time $O(N^4)$, where $N$ is the number of nodes in the graph. This prevents GNTK from scaling to datasets that contain large graphs. Theoretically, we present two techniques to speed up GNTK training while preserving the generalization error: (1) We use a novel matrix decoupling method to reduce matrix dimensions during the kernel solving. This allows us to reduce the dominated computation bottleneck term from $O(N^4)$ to $O(N^3)$. (2)  We apply sketching to further reduce the bottleneck term to $o(N^{\\omega})$, where $\\omega \\approx 2.373$ is the exponent of current matrix multiplication. Experimentally, we demonstrate that our approaches speed up kernel learning by up to $19\\times$ on real-world benchmark datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|graph_neural_network_acceleration_via_matrix_dimension_reduction", "supplementary_material": "/attachment/5295a94888896e20f0bdebdec089f64d35fc342e.zip", "pdf": "/pdf/e884ab4f5c59ba8567103b14a9b521426fa1bb68.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=H3fDDCTS4n", "_bibtex": "@misc{\njiang2021graph,\ntitle={Graph Neural Network Acceleration via Matrix Dimension Reduction},\nauthor={Shunhua Jiang and Yunze Man and Zhao Song and Danyang Zhuo},\nyear={2021},\nurl={https://openreview.net/forum?id=8IbZUle6ieH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "8IbZUle6ieH", "replyto": "8IbZUle6ieH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1357/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538120563, "tmdate": 1606915779480, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1357/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1357/-/Official_Review"}}}, {"id": "stUQ4WUMBPD", "original": null, "number": 2, "cdate": 1603837441072, "ddate": null, "tcdate": 1603837441072, "tmdate": 1606771151270, "tddate": null, "forum": "8IbZUle6ieH", "replyto": "8IbZUle6ieH", "invitation": "ICLR.cc/2021/Conference/Paper1357/-/Official_Review", "content": {"title": "A good paper on accelerating graph neural tangent kernel", "review": "In this paper, the authors propose two techniques to speed up the training of the graph neural tangent kernel, by matrix decoupling and sketching. Experiments are convincing.\n\nWe find the title of the paper inappropriate, because the paper only consider the  graph neural tangent kernel.\n\nIn general, the paper is difficult to read because many important parts are only available in the appendices.\n\nThere are some spelling and grammatical errors that can be easily identified and corrected, such as \"The descriptions in this section is\".\n\n----------------\nFollowing the authors' response,  we have updated our rating accordingly considering the following facts: the authors didn't make any change to the paper within the rebuttal, while they had the possibility in response to several questions. They didn't address even our simplest concerns, about the title being inappropriate. They still want to keep the title too general, while the paper considers only graph neural tangent kernel. Even if the latter is equivalent to infinitely wide multi-layer GNN, it is still a very special case. Moreover, many of the major issues raised by the other reviews were not addressed.", "rating": "5: Marginally below acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2021/Conference/Paper1357/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1357/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Neural Network Acceleration via Matrix Dimension Reduction", "authorids": ["~Shunhua_Jiang1", "~Yunze_Man2", "~Zhao_Song3", "~Danyang_Zhuo1"], "authors": ["Shunhua Jiang", "Yunze Man", "Zhao Song", "Danyang Zhuo"], "keywords": ["Graph Neural Networks", "Deep learning", "Optimization", "Kernel Method"], "abstract": "Graph Neural Networks (GNNs) have become the de facto method for machine learning on graph data (e.g., social networks, protein structures, code ASTs), but they require significant time and resource to train. One alternative method is Graph Neural Tangent Kernel (GNTK), a kernel method that corresponds to infinitely wide multi-layer GNNs. GNTK's parameters can be solved directly in a single step, avoiding time-consuming gradient descent. Today, GNTK is the state-of-the-art method to achieve high training speed without compromising accuracy. Unfortunately, solving for the kernel and searching for parameters can still take hours to days on real-world graphs. The current computation of GNTK has running time $O(N^4)$, where $N$ is the number of nodes in the graph. This prevents GNTK from scaling to datasets that contain large graphs. Theoretically, we present two techniques to speed up GNTK training while preserving the generalization error: (1) We use a novel matrix decoupling method to reduce matrix dimensions during the kernel solving. This allows us to reduce the dominated computation bottleneck term from $O(N^4)$ to $O(N^3)$. (2)  We apply sketching to further reduce the bottleneck term to $o(N^{\\omega})$, where $\\omega \\approx 2.373$ is the exponent of current matrix multiplication. Experimentally, we demonstrate that our approaches speed up kernel learning by up to $19\\times$ on real-world benchmark datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|graph_neural_network_acceleration_via_matrix_dimension_reduction", "supplementary_material": "/attachment/5295a94888896e20f0bdebdec089f64d35fc342e.zip", "pdf": "/pdf/e884ab4f5c59ba8567103b14a9b521426fa1bb68.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=H3fDDCTS4n", "_bibtex": "@misc{\njiang2021graph,\ntitle={Graph Neural Network Acceleration via Matrix Dimension Reduction},\nauthor={Shunhua Jiang and Yunze Man and Zhao Song and Danyang Zhuo},\nyear={2021},\nurl={https://openreview.net/forum?id=8IbZUle6ieH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "8IbZUle6ieH", "replyto": "8IbZUle6ieH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1357/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538120563, "tmdate": 1606915779480, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1357/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1357/-/Official_Review"}}}, {"id": "xipkqnn64lQ", "original": null, "number": 10, "cdate": 1606254570403, "ddate": null, "tcdate": 1606254570403, "tmdate": 1606254570403, "tddate": null, "forum": "8IbZUle6ieH", "replyto": "M5kRupPjq7o", "invitation": "ICLR.cc/2021/Conference/Paper1357/-/Official_Comment", "content": {"title": "Response to Follow-up Comments", "comment": "Thanks for reading our response.  Our paper studies the theoretical aspects of our matrix decoupling method and our sketching method. However, we only provide empirical evaluations for our matrix decoupling method on real-world datasets. The speedup is up to 19x. On those real-world datasets (< 500 nodes per graph), after our matrix decoupling method is applied, matrix multiplication is no longer the bottleneck for GNTK training. Because we do not know any public graph datasets that have larger graphs, we didn't evaluate our sketching method empirically in terms of the performance-accuracy trade-off. We believe, however, as we are applying GNN to larger and larger graphs in the future for larger social networks or more complex protein structures, our sketching method will be useful, and this paper provides the theoretical understanding of the generalization error bound for the sketching method. The reviewer is correct that the implication on end-to-end performance accuracy trade-off on real-world large graphs for our sketching method is still an open question."}, "signatures": ["ICLR.cc/2021/Conference/Paper1357/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1357/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Neural Network Acceleration via Matrix Dimension Reduction", "authorids": ["~Shunhua_Jiang1", "~Yunze_Man2", "~Zhao_Song3", "~Danyang_Zhuo1"], "authors": ["Shunhua Jiang", "Yunze Man", "Zhao Song", "Danyang Zhuo"], "keywords": ["Graph Neural Networks", "Deep learning", "Optimization", "Kernel Method"], "abstract": "Graph Neural Networks (GNNs) have become the de facto method for machine learning on graph data (e.g., social networks, protein structures, code ASTs), but they require significant time and resource to train. One alternative method is Graph Neural Tangent Kernel (GNTK), a kernel method that corresponds to infinitely wide multi-layer GNNs. GNTK's parameters can be solved directly in a single step, avoiding time-consuming gradient descent. Today, GNTK is the state-of-the-art method to achieve high training speed without compromising accuracy. Unfortunately, solving for the kernel and searching for parameters can still take hours to days on real-world graphs. The current computation of GNTK has running time $O(N^4)$, where $N$ is the number of nodes in the graph. This prevents GNTK from scaling to datasets that contain large graphs. Theoretically, we present two techniques to speed up GNTK training while preserving the generalization error: (1) We use a novel matrix decoupling method to reduce matrix dimensions during the kernel solving. This allows us to reduce the dominated computation bottleneck term from $O(N^4)$ to $O(N^3)$. (2)  We apply sketching to further reduce the bottleneck term to $o(N^{\\omega})$, where $\\omega \\approx 2.373$ is the exponent of current matrix multiplication. Experimentally, we demonstrate that our approaches speed up kernel learning by up to $19\\times$ on real-world benchmark datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|graph_neural_network_acceleration_via_matrix_dimension_reduction", "supplementary_material": "/attachment/5295a94888896e20f0bdebdec089f64d35fc342e.zip", "pdf": "/pdf/e884ab4f5c59ba8567103b14a9b521426fa1bb68.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=H3fDDCTS4n", "_bibtex": "@misc{\njiang2021graph,\ntitle={Graph Neural Network Acceleration via Matrix Dimension Reduction},\nauthor={Shunhua Jiang and Yunze Man and Zhao Song and Danyang Zhuo},\nyear={2021},\nurl={https://openreview.net/forum?id=8IbZUle6ieH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8IbZUle6ieH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1357/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1357/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1357/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1357/Authors|ICLR.cc/2021/Conference/Paper1357/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1357/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860642, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1357/-/Official_Comment"}}}, {"id": "M5kRupPjq7o", "original": null, "number": 9, "cdate": 1606251381882, "ddate": null, "tcdate": 1606251381882, "tmdate": 1606251381882, "tddate": null, "forum": "8IbZUle6ieH", "replyto": "dfeYFqzgS5s", "invitation": "ICLR.cc/2021/Conference/Paper1357/-/Official_Comment", "content": {"title": "Some more questions", "comment": "I thank the authors for their answers. I do have some more comments / questions:\n\n- **Reported performance** - I now understand my confusion about the reported results, I thought the method used was also incorporated with the matrix sketching, which has is an approximation and not an equivalence. \nHowever, in that case, I think it is very good the authors showed the speed up gained by matrix decomposition, but what happens to performance when sketching is used?\n\n- **Experiments showing the sketching error** - Thank you for adding that. Yet, same question, how does it effect the performance of the computed kernel?"}, "signatures": ["ICLR.cc/2021/Conference/Paper1357/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1357/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Neural Network Acceleration via Matrix Dimension Reduction", "authorids": ["~Shunhua_Jiang1", "~Yunze_Man2", "~Zhao_Song3", "~Danyang_Zhuo1"], "authors": ["Shunhua Jiang", "Yunze Man", "Zhao Song", "Danyang Zhuo"], "keywords": ["Graph Neural Networks", "Deep learning", "Optimization", "Kernel Method"], "abstract": "Graph Neural Networks (GNNs) have become the de facto method for machine learning on graph data (e.g., social networks, protein structures, code ASTs), but they require significant time and resource to train. One alternative method is Graph Neural Tangent Kernel (GNTK), a kernel method that corresponds to infinitely wide multi-layer GNNs. GNTK's parameters can be solved directly in a single step, avoiding time-consuming gradient descent. Today, GNTK is the state-of-the-art method to achieve high training speed without compromising accuracy. Unfortunately, solving for the kernel and searching for parameters can still take hours to days on real-world graphs. The current computation of GNTK has running time $O(N^4)$, where $N$ is the number of nodes in the graph. This prevents GNTK from scaling to datasets that contain large graphs. Theoretically, we present two techniques to speed up GNTK training while preserving the generalization error: (1) We use a novel matrix decoupling method to reduce matrix dimensions during the kernel solving. This allows us to reduce the dominated computation bottleneck term from $O(N^4)$ to $O(N^3)$. (2)  We apply sketching to further reduce the bottleneck term to $o(N^{\\omega})$, where $\\omega \\approx 2.373$ is the exponent of current matrix multiplication. Experimentally, we demonstrate that our approaches speed up kernel learning by up to $19\\times$ on real-world benchmark datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|graph_neural_network_acceleration_via_matrix_dimension_reduction", "supplementary_material": "/attachment/5295a94888896e20f0bdebdec089f64d35fc342e.zip", "pdf": "/pdf/e884ab4f5c59ba8567103b14a9b521426fa1bb68.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=H3fDDCTS4n", "_bibtex": "@misc{\njiang2021graph,\ntitle={Graph Neural Network Acceleration via Matrix Dimension Reduction},\nauthor={Shunhua Jiang and Yunze Man and Zhao Song and Danyang Zhuo},\nyear={2021},\nurl={https://openreview.net/forum?id=8IbZUle6ieH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8IbZUle6ieH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1357/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1357/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1357/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1357/Authors|ICLR.cc/2021/Conference/Paper1357/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1357/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860642, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1357/-/Official_Comment"}}}, {"id": "17X8_y1KmRu", "original": null, "number": 4, "cdate": 1606172783695, "ddate": null, "tcdate": 1606172783695, "tmdate": 1606173378863, "tddate": null, "forum": "8IbZUle6ieH", "replyto": "GYhwJl3tXno", "invitation": "ICLR.cc/2021/Conference/Paper1357/-/Official_Comment", "content": {"title": "Response to AnonReviewer4 (Part 2/2) ", "comment": "- As the paper claims to gain speed up making GNTK scalable to larger datasets, I would have liked to see the performance both in time and accuracy of the proposed method.\n\nAnswer: The largest open graph datasets we can find are the ones presented in the existing benchmark graph classification datasets [8][9]. These datasets only contain graphs of average no more than 500 nodes. However, we envision that the sizes of graphs will grow in the future as graph neural networks are used for larger social networks and more complex protein structures.\n\nWe demonstrate a 19x speedup on these real-world datasets using our matrix decoupling method without loss of accuracy. Unfortunately, the size of these datasets (< 500 nodes per graph) is not large enough for us to study the end-to-end performance-accuracy tradeoff for our sketch method. With a small number of nodes per graph, the overall running time is not dominated by matrix multiplication after our matrix decoupling method is applied, so the acceleration is not significant. Our paper studies the theoretical aspects of both our matrix decoupling method and sketching method extensively but only provides empirical results for matrix decoupling method.  Although we have proved error bound for our matrix sketching method, the implication on end-to-end performance and accuracy on real large graphs is still an open and interesting question.\n\nTo this end, we provide some evidence to show how matrix sketching affects accuracy and time : Following Lemma 5.4, we validate the running time and error difference between matrix multiplication with and without the sketching method. Specifically, we randomly generate $[n, n]$ matrix $A$, $G$ and $H$. And matrix multiplication without sketching is calculated by $G^T A H$. For the sketching method, we randomly generate two AMS matrices $R$ and $S$ with size $[\\gamma n, n]$ where $\\gamma$ is the sketching ratio. And matrix multiplication with sketching is calculated by $G^T R^T R A S^T S H$. For error, we run experiments under different sketching rates from $0.1$ to $0.5$. Experiments show that our sketching error is always lower than the theoretical bound, and using sketching results in a shorter running time. We also observe that when sketching rate gets higher, the error decreases and in the meantime running time increases because the dimension of the matrix is larger, and we lose less information. This validates our Lemma 5.4, showing that our matrix sketching method has a strictly bounded error. The figure of sketching error and running time comparison are added in section F of our supplementary file.  \n\n**References**:\n\n[1] Jason D. Lee, Ruoqi Shen, Zhao Song, Mengdi Wang, Zheng Yu. Generalized Leverage Score Sampling for Neural Networks. NeurIPS 2020.\n\n[2] Haim Avron, Michael Kapralov, Cameron Musco, Christopher Musco, Ameya Velingker, Amir Zandieh. Random Fourier features for kernel ridge regression : approximation bounds and statistical guarantees. ICML 2017\n\n[3] Haim Avron, Michael Kapralov, Cameron Musco, Christopher Musco, Ameya Velingker, Amir Zandieh. A universal sampling method for reconstructing signals with simple Fourier transform. STOC 2019\n\n[4] Josh Alman, Timothy Chu, Aaron Schild, Zhao Song. Algorithms and Hardness for Linear Algebra on Geometric Graphs. FOCS 2020\n\n[5] Jan van den Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training (Overparametrized) Neural Networks in Near-Linear Time. ITCS 2021.\n\n[6] Du, Simon S., Kangcheng Hou, Russ R. Salakhutdinov, Barnabas Poczos, Ruosong Wang, and Keyulu Xu. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. NeurIPS 2019.\n\n[7] Xu, Keyulu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks?. ICLR 2019.\n\n[8] Yanardag, Pinar, and S. V. N. Vishwanathan. Deep graph kernels. ACM SIGKDD 2015.\n\n[9] Hu, Weihua, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687 (2020).\n\n[10] Kenneth L. Clarkson, David P. Woodruff. Low-rank PSD approximation in input-sparsity time. SODA 2017.\n\n[11] Jelani Nelson, Huy L. Nguy\u00ean. OSNAP: Faster numerical linear algebra algorithms via sparser subspace embeddings. FOCS 2013.\n\n[12] Yin Tat Lee, Zhao Song, Qiuyi Zhang. Solving Empirical Risk Minimization in the Current Matrix Multiplication Time. COLT 2019.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1357/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1357/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Neural Network Acceleration via Matrix Dimension Reduction", "authorids": ["~Shunhua_Jiang1", "~Yunze_Man2", "~Zhao_Song3", "~Danyang_Zhuo1"], "authors": ["Shunhua Jiang", "Yunze Man", "Zhao Song", "Danyang Zhuo"], "keywords": ["Graph Neural Networks", "Deep learning", "Optimization", "Kernel Method"], "abstract": "Graph Neural Networks (GNNs) have become the de facto method for machine learning on graph data (e.g., social networks, protein structures, code ASTs), but they require significant time and resource to train. One alternative method is Graph Neural Tangent Kernel (GNTK), a kernel method that corresponds to infinitely wide multi-layer GNNs. GNTK's parameters can be solved directly in a single step, avoiding time-consuming gradient descent. Today, GNTK is the state-of-the-art method to achieve high training speed without compromising accuracy. Unfortunately, solving for the kernel and searching for parameters can still take hours to days on real-world graphs. The current computation of GNTK has running time $O(N^4)$, where $N$ is the number of nodes in the graph. This prevents GNTK from scaling to datasets that contain large graphs. Theoretically, we present two techniques to speed up GNTK training while preserving the generalization error: (1) We use a novel matrix decoupling method to reduce matrix dimensions during the kernel solving. This allows us to reduce the dominated computation bottleneck term from $O(N^4)$ to $O(N^3)$. (2)  We apply sketching to further reduce the bottleneck term to $o(N^{\\omega})$, where $\\omega \\approx 2.373$ is the exponent of current matrix multiplication. Experimentally, we demonstrate that our approaches speed up kernel learning by up to $19\\times$ on real-world benchmark datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|graph_neural_network_acceleration_via_matrix_dimension_reduction", "supplementary_material": "/attachment/5295a94888896e20f0bdebdec089f64d35fc342e.zip", "pdf": "/pdf/e884ab4f5c59ba8567103b14a9b521426fa1bb68.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=H3fDDCTS4n", "_bibtex": "@misc{\njiang2021graph,\ntitle={Graph Neural Network Acceleration via Matrix Dimension Reduction},\nauthor={Shunhua Jiang and Yunze Man and Zhao Song and Danyang Zhuo},\nyear={2021},\nurl={https://openreview.net/forum?id=8IbZUle6ieH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8IbZUle6ieH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1357/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1357/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1357/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1357/Authors|ICLR.cc/2021/Conference/Paper1357/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1357/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860642, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1357/-/Official_Comment"}}}, {"id": "2CeiAxPm8sg", "original": null, "number": 7, "cdate": 1606173094181, "ddate": null, "tcdate": 1606173094181, "tmdate": 1606173159080, "tddate": null, "forum": "8IbZUle6ieH", "replyto": "UC0VmMn2ARp", "invitation": "ICLR.cc/2021/Conference/Paper1357/-/Official_Comment", "content": {"title": "Response to AnonReviewer2 (Part 2/2)", "comment": "- It would be more informative if the authors compare using other sketching methods such as Gaussian matrices and sampling methods.\n\nAnswer: Our theoretical analysis also holds for other common dense sketching matrices, including Gaussian matrices and SRHT matrices. But we cannot use sparse embedding matrices or count-sketch matrices ([2]). This is because the error in the main sketching lemma (Lemma 5.4) becomes too large with these sparse matrices.\n\nLeverage score sampling [7][8] can also be used in many places to replace sketching matrices, e.g,, subspace embedding and approximate matrix product. For a matrix A, the leverage score of the $i$-th row $a_i^{\\top}$ of $A$ is defined as $\\tau_i = a_i^{\\top} (A^{\\top} A)^{-1} a_i$. Leverage score sampling then samples each row of A with probability $\\tau_i$. Note leverage score sampling is not oblivious since it depends on the matrix. Also, it is not known how to do it on both sides of a matrix. Thus at this point we do not know if leverage score sampling can be used in our application of GNTK.\n\n[1] Kenneth L. Clarkson, David P. Woodruff. Low-rank PSD approximation in input-sparsity time. SODA 2017.\n\n[2] Jelani Nelson, Huy L. Nguy\u00ean. OSNAP: Faster numerical linear algebra algorithms via sparser subspace embeddings. FOCS 2013.\n\n[3] Yin Tat Lee, Zhao Song, Qiuyi Zhang. Solving Empirical Risk Minimization in the Current Matrix Multiplication Time. COLT 2019.\n\n[4] Jan van den Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training (Overparametrized) Neural Networks in Near-Linear Time. ITCS 2021.\n\n[5] Yanardag, Pinar, and S. V. N. Vishwanathan. Deep graph kernels. ACM SIGKDD 2015.\n\n[6] Hu, Weihua, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687 (2020).\n\n[7]  Daniel A. Spielman, Nikhil Srivastava. Graph sparsification by effective resistances. SIAM Journal on Computing 40, no. 6 (2011): 1913-1926.\n\n[8] Joshua Batson, Daniel A. Spielman, and Nikhil Srivastava. Twice-ramanujan sparsifiers. SIAM Journal on Computing 41, no. 6 (2012): 1704-1721.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1357/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1357/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Neural Network Acceleration via Matrix Dimension Reduction", "authorids": ["~Shunhua_Jiang1", "~Yunze_Man2", "~Zhao_Song3", "~Danyang_Zhuo1"], "authors": ["Shunhua Jiang", "Yunze Man", "Zhao Song", "Danyang Zhuo"], "keywords": ["Graph Neural Networks", "Deep learning", "Optimization", "Kernel Method"], "abstract": "Graph Neural Networks (GNNs) have become the de facto method for machine learning on graph data (e.g., social networks, protein structures, code ASTs), but they require significant time and resource to train. One alternative method is Graph Neural Tangent Kernel (GNTK), a kernel method that corresponds to infinitely wide multi-layer GNNs. GNTK's parameters can be solved directly in a single step, avoiding time-consuming gradient descent. Today, GNTK is the state-of-the-art method to achieve high training speed without compromising accuracy. Unfortunately, solving for the kernel and searching for parameters can still take hours to days on real-world graphs. The current computation of GNTK has running time $O(N^4)$, where $N$ is the number of nodes in the graph. This prevents GNTK from scaling to datasets that contain large graphs. Theoretically, we present two techniques to speed up GNTK training while preserving the generalization error: (1) We use a novel matrix decoupling method to reduce matrix dimensions during the kernel solving. This allows us to reduce the dominated computation bottleneck term from $O(N^4)$ to $O(N^3)$. (2)  We apply sketching to further reduce the bottleneck term to $o(N^{\\omega})$, where $\\omega \\approx 2.373$ is the exponent of current matrix multiplication. Experimentally, we demonstrate that our approaches speed up kernel learning by up to $19\\times$ on real-world benchmark datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|graph_neural_network_acceleration_via_matrix_dimension_reduction", "supplementary_material": "/attachment/5295a94888896e20f0bdebdec089f64d35fc342e.zip", "pdf": "/pdf/e884ab4f5c59ba8567103b14a9b521426fa1bb68.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=H3fDDCTS4n", "_bibtex": "@misc{\njiang2021graph,\ntitle={Graph Neural Network Acceleration via Matrix Dimension Reduction},\nauthor={Shunhua Jiang and Yunze Man and Zhao Song and Danyang Zhuo},\nyear={2021},\nurl={https://openreview.net/forum?id=8IbZUle6ieH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8IbZUle6ieH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1357/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1357/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1357/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1357/Authors|ICLR.cc/2021/Conference/Paper1357/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1357/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860642, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1357/-/Official_Comment"}}}, {"id": "DWXiV9hE1Q", "original": null, "number": 6, "cdate": 1606173047604, "ddate": null, "tcdate": 1606173047604, "tmdate": 1606173047604, "tddate": null, "forum": "8IbZUle6ieH", "replyto": "UC0VmMn2ARp", "invitation": "ICLR.cc/2021/Conference/Paper1357/-/Official_Comment", "content": {"title": "Response to AnonReviewer2 (Part 1/2)", "comment": "Thanks for the detailed feedback. We address the issues below.\n\n- My main issue with the paper is its lack of novelty.\n\nAnswer: This paper proposed two techniques to speed up GNTK computation: matrix decoupling, and sketching in GNTK, and we believe they are novel and will be useful for future research.\n\nWe especially want to stress the novelty of our design of combining the sketching methods with GNTK by comparing it with previous research. In the standard \u201csketch and solve\u201d paradigm, sketching methods are used for numerical linear algebra problems, e.g., linear regression, low rank approximation (see [1] and [2]). Recently sketching methods are also used for iterative methods (including the iterative optimization algorithm for deep learning), and they can be used with or without precomputation. In [3] the algorithm precomputes the product of multiple sketching matrices with a fixed matrix, and uses a different copy in each iteration. In [4] there is no precomputation, and the algorithm sketches a matrix on the fly.\n\nWe remark that these previous results all use sketching methods in the following way: 1. They always add sketching matrices on one side of a matrix to accelerate the computation of the matrix. 2. They use sketching methods to preserve subspace embedding and approximate matrix product.\n\nHowever, in our paper, we add sketching matrices on both sides of a matrix, and we add sketching matrices when computing each entry of the matrix. This is a totally new scenario and requires totally new ideas.\n\nWe also want to remark that the previous papers that use sketching in deep learning are usually not practical, e.g., [4] accelerates second order methods that are only good in theory but not in practice. Our sketching technique not only has theoretical improvement but we believe it will have experimental improvement when the datasets are large enough.\n\n- More details about the experiments would be helpful. Can the authors give details about varying dimensions of the sketch matrices and its effect on speed and accuracy?\n\nAnswer: In the supplementary section F, we provide more implementation and experiment details. And we also provide some evidence to study how matrix sketching affects accuracy and time.\n\nSince currently the graph datasets are not large enough to demonstrate the effectiveness of our sketching method, we only consider a toy problem that corresponds to our sketching error lemma (Lemma 5.4): Following Lemma 5.4, we validate the running time and error difference between matrix multiplication with and without the sketching method. Specifically, we randomly generate [n, n] matrix $A$, $G$ and $H$. And matrix multiplication without sketching is calculated by $G^T A H$. For the sketching method, we randomly generate two AMS matrices $R$ and $S$ with size $[\\gamma n, n]$ where $\\gamma$ is the sketching ratio. And matrix multiplication with sketching is calculated by $G^T R^T R A S^T S H$. For error, we run experiments under different sketching rates from $0.1$ to $0.5$. Experiments show that our sketching error is always lower than the theoretical bound, and using sketching results in a shorter running time. We also observe that when sketching rate gets higher, the error decreases and in the meantime running time increases because the dimension of the matrix is larger, and we lose less information. This validates our Lemma 5.4, showing that our matrix sketching method has a strictly bounded error. The figure of sketching error and running time comparison are added in section F of our supplementary file. \n\nExisting benchmark graph classification datasets [5][6] only provides graphs with average node number no more than 500. With these datasets, we show a 19x speedup with our matrix decoupling method, however, the limited size (< 500 nodes per graph) of the datasets do not allow us to study the end-to-end accuracy performance trade-off.  Our paper studies the theoretical aspects of both our matrix decoupling method and sketching method extensively but only provides empirical results for matrix decoupling method.  Although we have proved error bound for our matrix sketching method, the implication on end-to-end performance and accuracy on real large graphs is still an open question."}, "signatures": ["ICLR.cc/2021/Conference/Paper1357/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1357/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Neural Network Acceleration via Matrix Dimension Reduction", "authorids": ["~Shunhua_Jiang1", "~Yunze_Man2", "~Zhao_Song3", "~Danyang_Zhuo1"], "authors": ["Shunhua Jiang", "Yunze Man", "Zhao Song", "Danyang Zhuo"], "keywords": ["Graph Neural Networks", "Deep learning", "Optimization", "Kernel Method"], "abstract": "Graph Neural Networks (GNNs) have become the de facto method for machine learning on graph data (e.g., social networks, protein structures, code ASTs), but they require significant time and resource to train. One alternative method is Graph Neural Tangent Kernel (GNTK), a kernel method that corresponds to infinitely wide multi-layer GNNs. GNTK's parameters can be solved directly in a single step, avoiding time-consuming gradient descent. Today, GNTK is the state-of-the-art method to achieve high training speed without compromising accuracy. Unfortunately, solving for the kernel and searching for parameters can still take hours to days on real-world graphs. The current computation of GNTK has running time $O(N^4)$, where $N$ is the number of nodes in the graph. This prevents GNTK from scaling to datasets that contain large graphs. Theoretically, we present two techniques to speed up GNTK training while preserving the generalization error: (1) We use a novel matrix decoupling method to reduce matrix dimensions during the kernel solving. This allows us to reduce the dominated computation bottleneck term from $O(N^4)$ to $O(N^3)$. (2)  We apply sketching to further reduce the bottleneck term to $o(N^{\\omega})$, where $\\omega \\approx 2.373$ is the exponent of current matrix multiplication. Experimentally, we demonstrate that our approaches speed up kernel learning by up to $19\\times$ on real-world benchmark datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|graph_neural_network_acceleration_via_matrix_dimension_reduction", "supplementary_material": "/attachment/5295a94888896e20f0bdebdec089f64d35fc342e.zip", "pdf": "/pdf/e884ab4f5c59ba8567103b14a9b521426fa1bb68.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=H3fDDCTS4n", "_bibtex": "@misc{\njiang2021graph,\ntitle={Graph Neural Network Acceleration via Matrix Dimension Reduction},\nauthor={Shunhua Jiang and Yunze Man and Zhao Song and Danyang Zhuo},\nyear={2021},\nurl={https://openreview.net/forum?id=8IbZUle6ieH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8IbZUle6ieH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1357/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1357/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1357/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1357/Authors|ICLR.cc/2021/Conference/Paper1357/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1357/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860642, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1357/-/Official_Comment"}}}, {"id": "BMm7X7-Uj1", "original": null, "number": 5, "cdate": 1606172905395, "ddate": null, "tcdate": 1606172905395, "tmdate": 1606172905395, "tddate": null, "forum": "8IbZUle6ieH", "replyto": "stUQ4WUMBPD", "invitation": "ICLR.cc/2021/Conference/Paper1357/-/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "We thank the reviewer for the positive feedback. \n\n- We find the title of the paper inappropriate, because the paper only consider the graph neural tangent kernel.\n\nAnswer: As explained in Section 2 background and related work, GNTK is equivalent to infinitely wide multi-layer GNNs and GNTK currently is the one of the state-of-the-art methods to train GNNs in terms of achieving accuracy. That\u2019s why we use GNN in the title. We are open to more discussion for different options for the title.\n\n- In general, the paper is difficult to read because many important parts are only available in the appendices.\n\nAnswer: We put all the main results and a proof sketch (with a list of crucial lemmas for the proof) in the first ten pages, for example, our main generalization bound is presented in Theorem 5.1, and the key lemma for proving this theorem is Lemma 5.2, which then uses the sketching bound Lemma 5.4. Unfortunately, due to the lack of space, we have to delay the full proof to the appendices. We will further improve the writing to make it clearer when revising the paper.\n\n- There are some spelling and grammatical errors that can be easily identified and corrected, such as \"The descriptions in this section is\".\n\nAnswer: We will fix the typos and grammar errors when revising the paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1357/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1357/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Neural Network Acceleration via Matrix Dimension Reduction", "authorids": ["~Shunhua_Jiang1", "~Yunze_Man2", "~Zhao_Song3", "~Danyang_Zhuo1"], "authors": ["Shunhua Jiang", "Yunze Man", "Zhao Song", "Danyang Zhuo"], "keywords": ["Graph Neural Networks", "Deep learning", "Optimization", "Kernel Method"], "abstract": "Graph Neural Networks (GNNs) have become the de facto method for machine learning on graph data (e.g., social networks, protein structures, code ASTs), but they require significant time and resource to train. One alternative method is Graph Neural Tangent Kernel (GNTK), a kernel method that corresponds to infinitely wide multi-layer GNNs. GNTK's parameters can be solved directly in a single step, avoiding time-consuming gradient descent. Today, GNTK is the state-of-the-art method to achieve high training speed without compromising accuracy. Unfortunately, solving for the kernel and searching for parameters can still take hours to days on real-world graphs. The current computation of GNTK has running time $O(N^4)$, where $N$ is the number of nodes in the graph. This prevents GNTK from scaling to datasets that contain large graphs. Theoretically, we present two techniques to speed up GNTK training while preserving the generalization error: (1) We use a novel matrix decoupling method to reduce matrix dimensions during the kernel solving. This allows us to reduce the dominated computation bottleneck term from $O(N^4)$ to $O(N^3)$. (2)  We apply sketching to further reduce the bottleneck term to $o(N^{\\omega})$, where $\\omega \\approx 2.373$ is the exponent of current matrix multiplication. Experimentally, we demonstrate that our approaches speed up kernel learning by up to $19\\times$ on real-world benchmark datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|graph_neural_network_acceleration_via_matrix_dimension_reduction", "supplementary_material": "/attachment/5295a94888896e20f0bdebdec089f64d35fc342e.zip", "pdf": "/pdf/e884ab4f5c59ba8567103b14a9b521426fa1bb68.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=H3fDDCTS4n", "_bibtex": "@misc{\njiang2021graph,\ntitle={Graph Neural Network Acceleration via Matrix Dimension Reduction},\nauthor={Shunhua Jiang and Yunze Man and Zhao Song and Danyang Zhuo},\nyear={2021},\nurl={https://openreview.net/forum?id=8IbZUle6ieH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8IbZUle6ieH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1357/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1357/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1357/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1357/Authors|ICLR.cc/2021/Conference/Paper1357/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1357/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860642, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1357/-/Official_Comment"}}}, {"id": "dfeYFqzgS5s", "original": null, "number": 3, "cdate": 1606172744008, "ddate": null, "tcdate": 1606172744008, "tmdate": 1606172744008, "tddate": null, "forum": "8IbZUle6ieH", "replyto": "GYhwJl3tXno", "invitation": "ICLR.cc/2021/Conference/Paper1357/-/Official_Comment", "content": {"title": "Response to AnonReviewer4 (Part 1/2)", "comment": "We thank the reviewer for the constructive comments. We would like to clarify the concerns raised by the reviewer.\n\n- Concerns regarding the reported performance in Table 2: I find it quite uncanny that the numbers are exactly the same as achieved by exact GNTK.\n\nAnswer: As stated in the caption, Table 2 shows the results of our matrix decoupling method. In Section 4.2 of the paper, we show that our proposed matrix decoupling method is equivalent to the original Kronecker product while significantly reducing the computation time. Thus, the accuracy should not be affected at all with our matrix decoupling method. The results shown in Table 2 are expected.\n\n\n- Discussion compared to GNTK - A discussion on how the exact GNTK generalization bound compares to the approximate one is missing.\n\nAnswer: Compared to the exact GNTK generalization bound, our approximate one uses different assumptions on the data and labels. More specifically, we assume the labels to be the form of finite summation (see part 1 of Assumption C.6), while in the exact case they allowed the labels to be the form of infinite summation. Also, we add two more reasonable assumptions about the feature vectors and sketching sizes (see part 2 and 3 of Assumption C.6).\nWe will add more discussion about this comparison in the revised version.\n\n\n- The paper does not explain and mention previous and relevant uses of matrix sketching (either in deep learning or kernel methods in general).\n\nAnswer: Sketching method has been extensively applied in kernel methods, e.g. [1][2][3][4]. It has also been used in theoretical deep learning, e.g. [5]. We compare our design of combining the sketching methods with GNTK with previous research.\n\nIn the standard \u201csketch and solve\u201d paradigm, sketching methods are used for numerical linear algebra problems, e.g., linear regression, low rank approximation (see [10] and [11]). Recently sketching methods are also used for iterative methods (including the iterative optimization algorithm for deep learning), and they can be used with or without precomputation. In [12] the algorithm precomputes the product of multiple sketching matrices with a fixed matrix, and uses a different copy in each iteration. In [5] there is no precomputation, and the algorithm sketches a matrix on the fly.\n\nWe remark that these previous results all use sketching methods in the following way: 1. They always add sketching matrices on one side of a matrix to accelerate the computation of the matrix. 2. They use sketching methods to preserve subspace embedding and approximate matrix product.\n\nHowever, in our paper, we add sketching matrices on both sides of a matrix, and we add sketching matrices when computing each entry of the matrix. This is a totally new scenario and requires totally new ideas.\nWe also want to remark that the previous papers that use sketching in deep learning are usually not practical, e.g., [5] accelerates second order methods that are only good in theory but not in practice. Our sketching technique not only has theoretical improvement but we believe it will have experimental improvement when the graphs in the datasets are large enough.\n\n- Experimental setting - Regarding the evaluation on social datasets, it is mentioned that a degree feature was added as an input. Is it true for the other baselines as well?\n\nAnswer: As stated in \u201cDataset\u201d part, the nodes of bioinformatics dataset obtain categorical features initially, and we use that feature as the input h to the network. In comparison, nodes in the social network dataset do not have any pre-defined feature, so we treat the degree of each node as its feature h and input to the network. This is standard practice, also done in [6][7].\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1357/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1357/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Neural Network Acceleration via Matrix Dimension Reduction", "authorids": ["~Shunhua_Jiang1", "~Yunze_Man2", "~Zhao_Song3", "~Danyang_Zhuo1"], "authors": ["Shunhua Jiang", "Yunze Man", "Zhao Song", "Danyang Zhuo"], "keywords": ["Graph Neural Networks", "Deep learning", "Optimization", "Kernel Method"], "abstract": "Graph Neural Networks (GNNs) have become the de facto method for machine learning on graph data (e.g., social networks, protein structures, code ASTs), but they require significant time and resource to train. One alternative method is Graph Neural Tangent Kernel (GNTK), a kernel method that corresponds to infinitely wide multi-layer GNNs. GNTK's parameters can be solved directly in a single step, avoiding time-consuming gradient descent. Today, GNTK is the state-of-the-art method to achieve high training speed without compromising accuracy. Unfortunately, solving for the kernel and searching for parameters can still take hours to days on real-world graphs. The current computation of GNTK has running time $O(N^4)$, where $N$ is the number of nodes in the graph. This prevents GNTK from scaling to datasets that contain large graphs. Theoretically, we present two techniques to speed up GNTK training while preserving the generalization error: (1) We use a novel matrix decoupling method to reduce matrix dimensions during the kernel solving. This allows us to reduce the dominated computation bottleneck term from $O(N^4)$ to $O(N^3)$. (2)  We apply sketching to further reduce the bottleneck term to $o(N^{\\omega})$, where $\\omega \\approx 2.373$ is the exponent of current matrix multiplication. Experimentally, we demonstrate that our approaches speed up kernel learning by up to $19\\times$ on real-world benchmark datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|graph_neural_network_acceleration_via_matrix_dimension_reduction", "supplementary_material": "/attachment/5295a94888896e20f0bdebdec089f64d35fc342e.zip", "pdf": "/pdf/e884ab4f5c59ba8567103b14a9b521426fa1bb68.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=H3fDDCTS4n", "_bibtex": "@misc{\njiang2021graph,\ntitle={Graph Neural Network Acceleration via Matrix Dimension Reduction},\nauthor={Shunhua Jiang and Yunze Man and Zhao Song and Danyang Zhuo},\nyear={2021},\nurl={https://openreview.net/forum?id=8IbZUle6ieH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8IbZUle6ieH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1357/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1357/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1357/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1357/Authors|ICLR.cc/2021/Conference/Paper1357/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1357/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860642, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1357/-/Official_Comment"}}}, {"id": "UC0VmMn2ARp", "original": null, "number": 1, "cdate": 1603808972789, "ddate": null, "tcdate": 1603808972789, "tmdate": 1605024465531, "tddate": null, "forum": "8IbZUle6ieH", "replyto": "8IbZUle6ieH", "invitation": "ICLR.cc/2021/Conference/Paper1357/-/Official_Review", "content": {"title": "Good idea but lacks novelty", "review": "This paper proposes to apply Neural Tangent Kernel to GNN to improve training. This is a good initiative to apply state of the art methods to GNN training which would interest researchers and practitioners in the community. However, the novelty introduced in the paper low since the authors directly apply NTK to GNN.\n\nPros:\na.) A good point of the paper is the compressive experiments and theoretical guarantees which make the paper complete and useful to the community. The paper is also well written.\n\nb.) Good performances have been achieved from experiments with many benchmark datasets\n\n\nCons:\na.) My main issue with the paper is its lack of novelty. It is a good idea but the authors do not provide any new ideas for either NTK and GNN. The paper shows significant performance improvement with the proposed method, however, it is somewhat understandable. \n\nb.) More details about the experiments would be helpful. Can the authors give details about varying dimensions of the sketch matrices and its effect on speed and accuracy?\n\nc.) This paper focuses on using sketch matrices based on Alon et al.. It would be more informative if the authors compare using other sketching methods such as Gaussian matrices and sampling methods.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1357/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1357/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Neural Network Acceleration via Matrix Dimension Reduction", "authorids": ["~Shunhua_Jiang1", "~Yunze_Man2", "~Zhao_Song3", "~Danyang_Zhuo1"], "authors": ["Shunhua Jiang", "Yunze Man", "Zhao Song", "Danyang Zhuo"], "keywords": ["Graph Neural Networks", "Deep learning", "Optimization", "Kernel Method"], "abstract": "Graph Neural Networks (GNNs) have become the de facto method for machine learning on graph data (e.g., social networks, protein structures, code ASTs), but they require significant time and resource to train. One alternative method is Graph Neural Tangent Kernel (GNTK), a kernel method that corresponds to infinitely wide multi-layer GNNs. GNTK's parameters can be solved directly in a single step, avoiding time-consuming gradient descent. Today, GNTK is the state-of-the-art method to achieve high training speed without compromising accuracy. Unfortunately, solving for the kernel and searching for parameters can still take hours to days on real-world graphs. The current computation of GNTK has running time $O(N^4)$, where $N$ is the number of nodes in the graph. This prevents GNTK from scaling to datasets that contain large graphs. Theoretically, we present two techniques to speed up GNTK training while preserving the generalization error: (1) We use a novel matrix decoupling method to reduce matrix dimensions during the kernel solving. This allows us to reduce the dominated computation bottleneck term from $O(N^4)$ to $O(N^3)$. (2)  We apply sketching to further reduce the bottleneck term to $o(N^{\\omega})$, where $\\omega \\approx 2.373$ is the exponent of current matrix multiplication. Experimentally, we demonstrate that our approaches speed up kernel learning by up to $19\\times$ on real-world benchmark datasets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|graph_neural_network_acceleration_via_matrix_dimension_reduction", "supplementary_material": "/attachment/5295a94888896e20f0bdebdec089f64d35fc342e.zip", "pdf": "/pdf/e884ab4f5c59ba8567103b14a9b521426fa1bb68.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=H3fDDCTS4n", "_bibtex": "@misc{\njiang2021graph,\ntitle={Graph Neural Network Acceleration via Matrix Dimension Reduction},\nauthor={Shunhua Jiang and Yunze Man and Zhao Song and Danyang Zhuo},\nyear={2021},\nurl={https://openreview.net/forum?id=8IbZUle6ieH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "8IbZUle6ieH", "replyto": "8IbZUle6ieH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1357/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538120563, "tmdate": 1606915779480, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1357/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1357/-/Official_Review"}}}], "count": 12}