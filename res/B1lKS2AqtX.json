{"notes": [{"id": "B1lKS2AqtX", "original": "SJlLk02qFQ", "number": 1564, "cdate": 1538088001362, "ddate": null, "tcdate": 1538088001362, "tmdate": 1553131840140, "tddate": null, "forum": "B1lKS2AqtX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Eidetic 3D LSTM: A Model for Video Prediction and Beyond", "abstract": "Spatiotemporal predictive learning, though long considered to be a promising self-supervised feature learning method, seldom shows its effectiveness beyond future video prediction. The reason is that it is difficult to learn good representations for both short-term frame dependency and long-term high-level relations. We present a new model, Eidetic 3D LSTM (E3D-LSTM), that integrates 3D convolutions into RNNs. The encapsulated 3D-Conv makes local perceptrons of RNNs motion-aware and enables the memory cell to store better short-term features. For long-term relations, we make the present memory state interact with its historical records via a gate-controlled self-attention module. We describe this memory transition mechanism eidetic as it is able to effectively recall the stored memories across multiple time stamps even after long periods of disturbance. We first evaluate the E3D-LSTM network on widely-used future video prediction datasets and achieve the state-of-the-art performance. Then we show that the E3D-LSTM network also performs well on the early activity recognition to infer what is happening or what will happen after observing only limited frames of video. This task aligns well with video prediction in modeling action intentions and tendency.", "keywords": [], "authorids": ["yunbowang1989@gmail.com", "lujiang@google.com", "mhyang@ucmerced.edu", "lijiali@google.com", "mingsheng@tsinghua.edu.cn", "feifeili@cs.stanford.edu"], "authors": ["Yunbo Wang", "Lu Jiang", "Ming-Hsuan Yang", "Li-Jia Li", "Mingsheng Long", "Li Fei-Fei"], "pdf": "/pdf/bd20c943b4cf216371e5f710b113c2292ff0e7bb.pdf", "paperhash": "wang|eidetic_3d_lstm_a_model_for_video_prediction_and_beyond", "_bibtex": "@inproceedings{\nwang2018eidetic,\ntitle={Eidetic 3D {LSTM}: A Model for Video Prediction and Beyond},\nauthor={Yunbo Wang and Lu Jiang and Ming-Hsuan Yang and Li-Jia Li and Mingsheng Long and Li Fei-Fei},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lKS2AqtX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HygdBreglN", "original": null, "number": 1, "cdate": 1544713536472, "ddate": null, "tcdate": 1544713536472, "tmdate": 1545354507926, "tddate": null, "forum": "B1lKS2AqtX", "replyto": "B1lKS2AqtX", "invitation": "ICLR.cc/2019/Conference/-/Paper1564/Meta_Review", "content": {"metareview": "Strengths: Strong results on future frame video prediction using a 3D convolutional network. Use of future video prediction to jointly learn auxiliary tasks shown to to increase performance. Good ablation study.\n\nWeaknesses: Comparisons with older action recognition methods. Some concerns about novelty, the main contribution is the E3D-LSTM architecture, which R1 characterized as an LSTM with an extra gate and attention mechanism. \n\nContention: Authors point to novelty in 3D convolutions inside the RNN.\n\nConsensus: All reviewers give a final score of 7- well done experiments helped address concerns around novelty. Easy to recommend acceptance given the agreement.\n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Well executed exploration of a 3D CNN LSTM method"}, "signatures": ["ICLR.cc/2019/Conference/Paper1564/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1564/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Eidetic 3D LSTM: A Model for Video Prediction and Beyond", "abstract": "Spatiotemporal predictive learning, though long considered to be a promising self-supervised feature learning method, seldom shows its effectiveness beyond future video prediction. The reason is that it is difficult to learn good representations for both short-term frame dependency and long-term high-level relations. We present a new model, Eidetic 3D LSTM (E3D-LSTM), that integrates 3D convolutions into RNNs. The encapsulated 3D-Conv makes local perceptrons of RNNs motion-aware and enables the memory cell to store better short-term features. For long-term relations, we make the present memory state interact with its historical records via a gate-controlled self-attention module. We describe this memory transition mechanism eidetic as it is able to effectively recall the stored memories across multiple time stamps even after long periods of disturbance. We first evaluate the E3D-LSTM network on widely-used future video prediction datasets and achieve the state-of-the-art performance. Then we show that the E3D-LSTM network also performs well on the early activity recognition to infer what is happening or what will happen after observing only limited frames of video. This task aligns well with video prediction in modeling action intentions and tendency.", "keywords": [], "authorids": ["yunbowang1989@gmail.com", "lujiang@google.com", "mhyang@ucmerced.edu", "lijiali@google.com", "mingsheng@tsinghua.edu.cn", "feifeili@cs.stanford.edu"], "authors": ["Yunbo Wang", "Lu Jiang", "Ming-Hsuan Yang", "Li-Jia Li", "Mingsheng Long", "Li Fei-Fei"], "pdf": "/pdf/bd20c943b4cf216371e5f710b113c2292ff0e7bb.pdf", "paperhash": "wang|eidetic_3d_lstm_a_model_for_video_prediction_and_beyond", "_bibtex": "@inproceedings{\nwang2018eidetic,\ntitle={Eidetic 3D {LSTM}: A Model for Video Prediction and Beyond},\nauthor={Yunbo Wang and Lu Jiang and Ming-Hsuan Yang and Li-Jia Li and Mingsheng Long and Li Fei-Fei},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lKS2AqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1564/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352792069, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1lKS2AqtX", "replyto": "B1lKS2AqtX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1564/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1564/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1564/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352792069}}}, {"id": "HJeJHrJc27", "original": null, "number": 3, "cdate": 1541170487358, "ddate": null, "tcdate": 1541170487358, "tmdate": 1543582705513, "tddate": null, "forum": "B1lKS2AqtX", "replyto": "B1lKS2AqtX", "invitation": "ICLR.cc/2019/Conference/-/Paper1564/Official_Review", "content": {"title": "The authors propose a futurue video prediction model based on recurrent 3D-CNNs and propose a novel memory mechanism (Eidetic Memory) to capture long term relationships inside the recurrent layer itself. They obtain surpass the state of the art on two commonly used, (relatively) simple benchmark video prediction datasets. They further apply their model to early action recognition, performing an ablation study to evaluate the strengths of each model building block.", "review": "AFTER REBUTTAL:\n\nThis is an overall good work, and I do think proves its point. The results on the TaxiBJ dataset (not TatxtBJ, please correct the name in the paper) are compelling, and the concerns regarding some of the text explainations have been corrected.\n\n-----\n\nThe proposed model uses a 3D-CNN with a new kind of 3D-conv. recurrent layer named E3D-LSTM, an extension of 3D-RCNN layers where the recall mechanism is extended by using an attentional mechanism, allowing it to update the recurrent state not only based on the previous state, but on a mixture of previous states from all previous time steps.\n\nPros:\nThe new approach displays outstanding results for future video prediction. Firstly, it obtains better results in short term predictions thanks to the 3D-Convolutional topology. Secondly, the recall mechanism is shown to be more stable over time: The prediction accuracy is sustained over longer preiods of time (longer prediction sequences) with a much smaller degradation. Regarding early action recognition, the use of future video prediction as a jointly learned auxiliary task is shown to significantly increase the prediction accuracy. The ablation study is compelling.\n\nCons:\nThe model does not compare against other methods regarding early action recognition. Since this is a novel field of study in computer vision, and not too much work exists on the subject, it is understandable. Also, it is not the main focus of the work.\n\nIn the introduction, the authors state that they account for uncertainty by better modelling the temporal sequence. Please, remove or rephrase this part. Uncertainty in video prediction is not due to the lack of modelling ability, but due to the inherent uncertainty of the task. In real world scenarios (eg. the KTH dataset used here) there is a continuous space of possible futures. In the case of variational models, this is captured as a distribution from which to sample. Adversarial models collapse this space into a single future in order to create more realistic-looking predictions. I don't believe your approach should necessarily model that space (after all, the novelty is on better modelling the sequence itself, not the possible futures, and the model can be easily extended to do so, either through GANs or VAEs), but it is important to not mislead the reader.\n\nIt would have been interesting to analyse the work on more complex settings, such as UCF101. While KTH is already a real-world dataset, its variability is very limited: A small set of backgrounds and actions, performed by a small group of individuals.\n\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1564/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Eidetic 3D LSTM: A Model for Video Prediction and Beyond", "abstract": "Spatiotemporal predictive learning, though long considered to be a promising self-supervised feature learning method, seldom shows its effectiveness beyond future video prediction. The reason is that it is difficult to learn good representations for both short-term frame dependency and long-term high-level relations. We present a new model, Eidetic 3D LSTM (E3D-LSTM), that integrates 3D convolutions into RNNs. The encapsulated 3D-Conv makes local perceptrons of RNNs motion-aware and enables the memory cell to store better short-term features. For long-term relations, we make the present memory state interact with its historical records via a gate-controlled self-attention module. We describe this memory transition mechanism eidetic as it is able to effectively recall the stored memories across multiple time stamps even after long periods of disturbance. We first evaluate the E3D-LSTM network on widely-used future video prediction datasets and achieve the state-of-the-art performance. Then we show that the E3D-LSTM network also performs well on the early activity recognition to infer what is happening or what will happen after observing only limited frames of video. This task aligns well with video prediction in modeling action intentions and tendency.", "keywords": [], "authorids": ["yunbowang1989@gmail.com", "lujiang@google.com", "mhyang@ucmerced.edu", "lijiali@google.com", "mingsheng@tsinghua.edu.cn", "feifeili@cs.stanford.edu"], "authors": ["Yunbo Wang", "Lu Jiang", "Ming-Hsuan Yang", "Li-Jia Li", "Mingsheng Long", "Li Fei-Fei"], "pdf": "/pdf/bd20c943b4cf216371e5f710b113c2292ff0e7bb.pdf", "paperhash": "wang|eidetic_3d_lstm_a_model_for_video_prediction_and_beyond", "_bibtex": "@inproceedings{\nwang2018eidetic,\ntitle={Eidetic 3D {LSTM}: A Model for Video Prediction and Beyond},\nauthor={Yunbo Wang and Lu Jiang and Ming-Hsuan Yang and Li-Jia Li and Mingsheng Long and Li Fei-Fei},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lKS2AqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1564/Official_Review", "cdate": 1542234202765, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1lKS2AqtX", "replyto": "B1lKS2AqtX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1564/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335973446, "tmdate": 1552335973446, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1564/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJgyWXsuh7", "original": null, "number": 2, "cdate": 1541087991009, "ddate": null, "tcdate": 1541087991009, "tmdate": 1543571580616, "tddate": null, "forum": "B1lKS2AqtX", "replyto": "B1lKS2AqtX", "invitation": "ICLR.cc/2019/Conference/-/Paper1564/Official_Review", "content": {"title": "well-written, well-experimented paper with limited novelty", "review": "The paper proposes a spatiotemporal modeling of videos based on two currently available spatiotemporal modeling paradigms: RNNs and 3D convolutions. The main idea of this paper is to get the best world of both in a unified way. The method first encodes a sequence of frames using 3D-conv to capture short-term motion patterns, passes it to a specific type of LSTM (E3D-LSTM) which accepts spatiotemporal feature maps as input. E3D-LSTM captures long-term dependencies using an attention mechanism. Finally, there are 3D-conv based decoders which receive the output of E3D-LSTM and generate future frames. The message of the paper, I believe, is that 3D-conv and RNNs can be integrated to perform short and long predictions. They show in the experiments how the model can remember far past for reasoning and prediction.\nThe nice point of the method is that it is heavily investigated through experiments. It's evaluated on two datasets, with ablation studies on both. Moreover, the paper is well-written and clear. technically, the paper seems correct.\nHowever, my only big concern is about the limited novelty of the method. E3D-LSTM is the core of the novelty, which is basically an LSTM with extra gate, and attention mechanism.  \n\nother comments:\n- As the method by essence is a spatiotemporal learning model, why the method is not evaluated on full-length videos of the something-something dataset for classical action classification task, in order to compare it with the full architecture of I3D, or S3D?\n\n- While the paper discusses self-supervised learning, I would suggest showing its benefit on online action recognition task. One without frame-prediction loss and one with. \n\n- the something-something dataset has 174 classes, how was the process of selecting 41 classes out of it?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1564/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Eidetic 3D LSTM: A Model for Video Prediction and Beyond", "abstract": "Spatiotemporal predictive learning, though long considered to be a promising self-supervised feature learning method, seldom shows its effectiveness beyond future video prediction. The reason is that it is difficult to learn good representations for both short-term frame dependency and long-term high-level relations. We present a new model, Eidetic 3D LSTM (E3D-LSTM), that integrates 3D convolutions into RNNs. The encapsulated 3D-Conv makes local perceptrons of RNNs motion-aware and enables the memory cell to store better short-term features. For long-term relations, we make the present memory state interact with its historical records via a gate-controlled self-attention module. We describe this memory transition mechanism eidetic as it is able to effectively recall the stored memories across multiple time stamps even after long periods of disturbance. We first evaluate the E3D-LSTM network on widely-used future video prediction datasets and achieve the state-of-the-art performance. Then we show that the E3D-LSTM network also performs well on the early activity recognition to infer what is happening or what will happen after observing only limited frames of video. This task aligns well with video prediction in modeling action intentions and tendency.", "keywords": [], "authorids": ["yunbowang1989@gmail.com", "lujiang@google.com", "mhyang@ucmerced.edu", "lijiali@google.com", "mingsheng@tsinghua.edu.cn", "feifeili@cs.stanford.edu"], "authors": ["Yunbo Wang", "Lu Jiang", "Ming-Hsuan Yang", "Li-Jia Li", "Mingsheng Long", "Li Fei-Fei"], "pdf": "/pdf/bd20c943b4cf216371e5f710b113c2292ff0e7bb.pdf", "paperhash": "wang|eidetic_3d_lstm_a_model_for_video_prediction_and_beyond", "_bibtex": "@inproceedings{\nwang2018eidetic,\ntitle={Eidetic 3D {LSTM}: A Model for Video Prediction and Beyond},\nauthor={Yunbo Wang and Lu Jiang and Ming-Hsuan Yang and Li-Jia Li and Mingsheng Long and Li Fei-Fei},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lKS2AqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1564/Official_Review", "cdate": 1542234202765, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1lKS2AqtX", "replyto": "B1lKS2AqtX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1564/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335973446, "tmdate": 1552335973446, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1564/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJl_P_YA0m", "original": null, "number": 8, "cdate": 1543571552469, "ddate": null, "tcdate": 1543571552469, "tmdate": 1543571552469, "tddate": null, "forum": "B1lKS2AqtX", "replyto": "ryeBsAat0Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1564/Official_Comment", "content": {"title": "comment to authors", "comment": "Q3:\nAs R1 said, there are works integrating 2d convolution and RNNs, like \"VideoLSTM convolves, attends and flows for action recognition\". still, novelty is not convincing.\n\nQ4: A typical video classification model which can see full-length videos may make decisions mainly depending on the scene information.\nI understand this paper aims to predict the future. however, \"Zhou et. al, Temporal Relational Reasoning in Videos\" show that for recognizing actions in something-something dataset, scene clues are not enough and modeling temporal dependencies are important. so a classical classification problem on this dataset makes sense. \n\nThough novelty is still not fully convincing, the paper can shed insights into the topic."}, "signatures": ["ICLR.cc/2019/Conference/Paper1564/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1564/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1564/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Eidetic 3D LSTM: A Model for Video Prediction and Beyond", "abstract": "Spatiotemporal predictive learning, though long considered to be a promising self-supervised feature learning method, seldom shows its effectiveness beyond future video prediction. The reason is that it is difficult to learn good representations for both short-term frame dependency and long-term high-level relations. We present a new model, Eidetic 3D LSTM (E3D-LSTM), that integrates 3D convolutions into RNNs. The encapsulated 3D-Conv makes local perceptrons of RNNs motion-aware and enables the memory cell to store better short-term features. For long-term relations, we make the present memory state interact with its historical records via a gate-controlled self-attention module. We describe this memory transition mechanism eidetic as it is able to effectively recall the stored memories across multiple time stamps even after long periods of disturbance. We first evaluate the E3D-LSTM network on widely-used future video prediction datasets and achieve the state-of-the-art performance. Then we show that the E3D-LSTM network also performs well on the early activity recognition to infer what is happening or what will happen after observing only limited frames of video. This task aligns well with video prediction in modeling action intentions and tendency.", "keywords": [], "authorids": ["yunbowang1989@gmail.com", "lujiang@google.com", "mhyang@ucmerced.edu", "lijiali@google.com", "mingsheng@tsinghua.edu.cn", "feifeili@cs.stanford.edu"], "authors": ["Yunbo Wang", "Lu Jiang", "Ming-Hsuan Yang", "Li-Jia Li", "Mingsheng Long", "Li Fei-Fei"], "pdf": "/pdf/bd20c943b4cf216371e5f710b113c2292ff0e7bb.pdf", "paperhash": "wang|eidetic_3d_lstm_a_model_for_video_prediction_and_beyond", "_bibtex": "@inproceedings{\nwang2018eidetic,\ntitle={Eidetic 3D {LSTM}: A Model for Video Prediction and Beyond},\nauthor={Yunbo Wang and Lu Jiang and Ming-Hsuan Yang and Li-Jia Li and Mingsheng Long and Li Fei-Fei},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lKS2AqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1564/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610041, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1lKS2AqtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1564/Authors", "ICLR.cc/2019/Conference/Paper1564/Reviewers", "ICLR.cc/2019/Conference/Paper1564/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1564/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1564/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1564/Authors|ICLR.cc/2019/Conference/Paper1564/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1564/Reviewers", "ICLR.cc/2019/Conference/Paper1564/Authors", "ICLR.cc/2019/Conference/Paper1564/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610041}}}, {"id": "S1gVVTLVhX", "original": null, "number": 1, "cdate": 1540807980214, "ddate": null, "tcdate": 1540807980214, "tmdate": 1543395395692, "tddate": null, "forum": "B1lKS2AqtX", "replyto": "B1lKS2AqtX", "invitation": "ICLR.cc/2019/Conference/-/Paper1564/Official_Review", "content": {"title": "Nice experiments although it lacks a bit of novelty", "review": "# 1. Summary\nThis paper presents a model for future video prediction, which integrates 3D convolutions into RNNs. The internal operations of the RNN are modified by adding historical records controlled via a gate-controlled self-attention module. The authors show that the model is effective also for other tasks such as early activity recognition.\n\nStrengths:\n* Nice extensive experimentation on video prediction and early activity recognition tasks and comparison with recent papers\n* Each choice in the model definition are motivated, although some clarity is still missing (see below)\n\nWeaknesses:\n* Novelty: the proposed model is a small extension of a previous work (Wang et al., 2017) \n\n\n# 2. Clarity and Motivation\nIn general, the paper is clear and general motivation makes sense, however some points need to be improved with further discussion and motivation:\n\nA) Page 2 \u201cUnlike the conventional memory transition function, it learns the size of temporal interactions. For longer sequences, this allows attending to distant states containing salient information\u201d: This is not obvious. Can the authors add more details and motivate these two sentences? How is long-term relations are learned given Eq. 1? \nB) Page 5 \u201cThese two terms are respectively designed for short-term and long-term video modeling\u201d: How do you make sure that Recall(.) does not focus on the short-term modeling instead? Not clear why this should model long-term relations.\nC) Page 5 and Eq 1: motivation why layer norm is required when defining C_t^k is not clear\nD) What if the Recall is instead modeled as attention? The idea is to consider only C_{1:t-1}^k (not consider R_t) and have an attentional model that learn what to recall based only on C. Also, why does Recall need to depend on R_t?\nE) Page 5 \u201cto minimize the l1 + l2 loss over every pixel in the frame\u201d: this sentence is not clear. How does it relate to Eq. 2?\n\n\n# 3. Novelty\nNovelty is the major concern of this paper. Although the introduced new concepts and ideas are interesting, the work seems to be an extension of ST-LSTM and PredRNN where Eq 1 is slightly modified by introducing Recall. \nIn addition the existing relation between the proposed model and ST-LSTM is not clearly state. Page 2, first paragraph: here the authors should state that model is and extension of ST-LSTM and highlight what are the difference and advantage of the new model.\n\n\n# 4. Significance of the work\nThis paper deals with an interesting and challenging topic (video prediction) as well as it shows some results on the early activity recognition task. These are definitively nice problem which are far to be solved. From the application perspective this work is significant, however from the methodological perspective it lacks a bit of significance because of the novelty issues highlighted above.\n\n\n# 5. Experimentation\nThe experiments are robust with nice comparisons with recent methods and ablation study motivating the different components of the model (Table 1 and 2). Some suggested improvements:\n\nA) Page 7 \u201cSeq 1 and Seq 2 are completely irrelevant, and ahead of them, another sub-sequence called prior context is given as the input, which is exactly the same as Seq 2\u201d: The COPY task is a bit unclear and need to be better explained. Why are Seq. 1 and 2 irrelevant? I would suggest to rephrase this part.\nB) Sec. 4.2, \u201cDataset and setup\u201d: which architecture has been used here?\nC) Sec. 4.3, \u201cHyper-parameters and Baselines\u201c: the something-something dataset is more realising that the other two \u201ctoy\u201d dataset. Why did the authors choose to train a 2 layers 3D-CNN encoders, instead of using existing pretrained 3D CNNs? I would suspect that the results can improve quite a bit.\n\n\n# 6. Others\n* The term \u201cself-supervised auxiliary learning\u201d is introduced in the abstract, but at this point it\u2019s meaning is not clear. I\u2019d suggest to either remove it or explain its meaning.\n* Figure 1(a): inconsistent notation with 2b. Also add citation (Wang et al., 2017) since it ie the same model of that paper\n\n-------\n# Post-discussion\nI increased my rating: even if novelty is not high, the results support the incremental ideas proposed by the authors.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1564/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Eidetic 3D LSTM: A Model for Video Prediction and Beyond", "abstract": "Spatiotemporal predictive learning, though long considered to be a promising self-supervised feature learning method, seldom shows its effectiveness beyond future video prediction. The reason is that it is difficult to learn good representations for both short-term frame dependency and long-term high-level relations. We present a new model, Eidetic 3D LSTM (E3D-LSTM), that integrates 3D convolutions into RNNs. The encapsulated 3D-Conv makes local perceptrons of RNNs motion-aware and enables the memory cell to store better short-term features. For long-term relations, we make the present memory state interact with its historical records via a gate-controlled self-attention module. We describe this memory transition mechanism eidetic as it is able to effectively recall the stored memories across multiple time stamps even after long periods of disturbance. We first evaluate the E3D-LSTM network on widely-used future video prediction datasets and achieve the state-of-the-art performance. Then we show that the E3D-LSTM network also performs well on the early activity recognition to infer what is happening or what will happen after observing only limited frames of video. This task aligns well with video prediction in modeling action intentions and tendency.", "keywords": [], "authorids": ["yunbowang1989@gmail.com", "lujiang@google.com", "mhyang@ucmerced.edu", "lijiali@google.com", "mingsheng@tsinghua.edu.cn", "feifeili@cs.stanford.edu"], "authors": ["Yunbo Wang", "Lu Jiang", "Ming-Hsuan Yang", "Li-Jia Li", "Mingsheng Long", "Li Fei-Fei"], "pdf": "/pdf/bd20c943b4cf216371e5f710b113c2292ff0e7bb.pdf", "paperhash": "wang|eidetic_3d_lstm_a_model_for_video_prediction_and_beyond", "_bibtex": "@inproceedings{\nwang2018eidetic,\ntitle={Eidetic 3D {LSTM}: A Model for Video Prediction and Beyond},\nauthor={Yunbo Wang and Lu Jiang and Ming-Hsuan Yang and Li-Jia Li and Mingsheng Long and Li Fei-Fei},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lKS2AqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1564/Official_Review", "cdate": 1542234202765, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1lKS2AqtX", "replyto": "B1lKS2AqtX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1564/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335973446, "tmdate": 1552335973446, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1564/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SyxImDCiA7", "original": null, "number": 6, "cdate": 1543395102490, "ddate": null, "tcdate": 1543395102490, "tmdate": 1543395102490, "tddate": null, "forum": "B1lKS2AqtX", "replyto": "ByVd51AFRm", "invitation": "ICLR.cc/2019/Conference/-/Paper1564/Official_Comment", "content": {"title": "Comment to authors", "comment": "Q7 (novelty)\n1) It may be the first work using 3d convolutions in RNNs, however there is already a previous work using 2d convolution in RNNs: \"Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting, NIPS 2015.\". \n\nQ8 E) Please add this info to the paper.\n\nQ9 C) It would have been interesting to see an experiment with one of these pre-trained models, because the used 2-layer network might be not be able to learn good features for the task. \n\nOverall novelty is still not fully convincing, however the results support the incremental ideas proposed by the authors."}, "signatures": ["ICLR.cc/2019/Conference/Paper1564/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1564/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1564/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Eidetic 3D LSTM: A Model for Video Prediction and Beyond", "abstract": "Spatiotemporal predictive learning, though long considered to be a promising self-supervised feature learning method, seldom shows its effectiveness beyond future video prediction. The reason is that it is difficult to learn good representations for both short-term frame dependency and long-term high-level relations. We present a new model, Eidetic 3D LSTM (E3D-LSTM), that integrates 3D convolutions into RNNs. The encapsulated 3D-Conv makes local perceptrons of RNNs motion-aware and enables the memory cell to store better short-term features. For long-term relations, we make the present memory state interact with its historical records via a gate-controlled self-attention module. We describe this memory transition mechanism eidetic as it is able to effectively recall the stored memories across multiple time stamps even after long periods of disturbance. We first evaluate the E3D-LSTM network on widely-used future video prediction datasets and achieve the state-of-the-art performance. Then we show that the E3D-LSTM network also performs well on the early activity recognition to infer what is happening or what will happen after observing only limited frames of video. This task aligns well with video prediction in modeling action intentions and tendency.", "keywords": [], "authorids": ["yunbowang1989@gmail.com", "lujiang@google.com", "mhyang@ucmerced.edu", "lijiali@google.com", "mingsheng@tsinghua.edu.cn", "feifeili@cs.stanford.edu"], "authors": ["Yunbo Wang", "Lu Jiang", "Ming-Hsuan Yang", "Li-Jia Li", "Mingsheng Long", "Li Fei-Fei"], "pdf": "/pdf/bd20c943b4cf216371e5f710b113c2292ff0e7bb.pdf", "paperhash": "wang|eidetic_3d_lstm_a_model_for_video_prediction_and_beyond", "_bibtex": "@inproceedings{\nwang2018eidetic,\ntitle={Eidetic 3D {LSTM}: A Model for Video Prediction and Beyond},\nauthor={Yunbo Wang and Lu Jiang and Ming-Hsuan Yang and Li-Jia Li and Mingsheng Long and Li Fei-Fei},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lKS2AqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1564/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610041, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1lKS2AqtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1564/Authors", "ICLR.cc/2019/Conference/Paper1564/Reviewers", "ICLR.cc/2019/Conference/Paper1564/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1564/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1564/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1564/Authors|ICLR.cc/2019/Conference/Paper1564/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1564/Reviewers", "ICLR.cc/2019/Conference/Paper1564/Authors", "ICLR.cc/2019/Conference/Paper1564/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610041}}}, {"id": "ryeBsAat0Q", "original": null, "number": 3, "cdate": 1543261853179, "ddate": null, "tcdate": 1543261853179, "tmdate": 1543280557264, "tddate": null, "forum": "B1lKS2AqtX", "replyto": "BJgyWXsuh7", "invitation": "ICLR.cc/2019/Conference/-/Paper1564/Official_Comment", "content": {"title": "Responses to AnonReviewer1", "comment": "\nQ3: Concern about the novelty: my only big concern is about the limited novelty of the method. E3D-LSTM is the core of the novelty, which is basically an LSTM with extra gate, and attention mechanism.\n\nThe concern on limited novelty is mainly due to the seeming similarity to the prior work [Wang et al., 2017]. Below we clarify the differences to the prior work:\n\n1. Our paper is one of, if not the first, work to systematically explore 3D convolutions **inside** the RNN. More importantly, it is the first to show a carefully designed method achieves the state-of-the-art results on several public benchmarks. The improvements are otherwise not shown for any known combination of 3D convolutions and RNN. \n\n2. Our technical difference to the existing work includes: \na. we study where to apply the 3D info. For example, combine 2D or 3D inputs (see Figure 1), inflate the LSTM cell to 3D (see Figure 2b), or separate the 3D convolutions in the input and LSTM cell (see Table 4). \nb. we propose how to effectively embed the 3D convolution inside the LSTM (i.e. we introduce a new recall gate in Equation 1 for the 3D-memory transition inside the LSTM).\n\nAmong the recent advances in deep learning, many great models appear to be similar to prior work (e.g. ResNet and Highway Network, ConvLSTM and LSTM, C3D/I3D CNN and 2D CNN). However, it is not true as the devil is in the important details. Similarly, we build upon prior work, make only necessary, yet important, model designs, and validate the necessities with ablation studies to demonstrate their merits.  Our designs are driven by a clear motivation, innovative thinkings, and validated by extensive experiments (as agreed by all reviewers). \nWe hope this can resolve the concern on novelty.\n\nQ4: As the method by essence is a spatiotemporal learning model, why the method is not evaluated on full-length videos of the something-something dataset?\n\nThe main reason is that predicting on the full-length video may not align well with our topic. A typical video classification model which can see full-length videos may make decisions mainly depending on the scene information. As shown in Fig. 5, suppose the tasks is to predict a category \u201cPoking a stack of [Something], so the stack collapses\u201d. The problem would be very simple as long as the model sees the last frame which shows the outcome of the action. \n\nIn contrast, the early activity recognition task makes the model have no other choices but to depend on an inference of the action intentions when making decisions. It aligns well with the video prediction task, in which the sequential tendency and causality are important.\n\nWe notice that it would be more accurate to claim our model as a spatiotemporal predictive learning model, rather than a broad \u201cspatiotemporal learning model\u201d. We have revised that in the paper.\n\nQ5: Show the benefit on online action recognition task. \n\nAs suggested, we have added online early activity recognition by making the classifier only depend on a concatenation of the recurrent outputs regarding the last 5 timestamps. As such, the historical recurrent states are only kept for 5 timestamps and then discarded. In particular, we apply a sliding window of limited length to the inputs of the Recall gate, using $C_{t-5:t-1}$ instead of $C_{1:t-1}$ in Eq. 1. Experimental results are shown in Table 7. Despite the slightly decreased accuracy, applying the sliding window on the Recall gate improves the scalability of E3D-LSTM.\n\nQ6: How was the process of selecting 41 classes out of the something-something dataset?\n\nIn the original paper [Goyal et al. 2017] of the Something-Something dataset, the  41 classes (in Table 7) are listed as a standard and official dataset setting. This split contains 56k video clips for training and 7.5k for validation and is large enough and meanwhile computational convenient to compare a variety of baseline methods. We have clarified this point in the paper (Page 9)."}, "signatures": ["ICLR.cc/2019/Conference/Paper1564/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1564/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1564/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Eidetic 3D LSTM: A Model for Video Prediction and Beyond", "abstract": "Spatiotemporal predictive learning, though long considered to be a promising self-supervised feature learning method, seldom shows its effectiveness beyond future video prediction. The reason is that it is difficult to learn good representations for both short-term frame dependency and long-term high-level relations. We present a new model, Eidetic 3D LSTM (E3D-LSTM), that integrates 3D convolutions into RNNs. The encapsulated 3D-Conv makes local perceptrons of RNNs motion-aware and enables the memory cell to store better short-term features. For long-term relations, we make the present memory state interact with its historical records via a gate-controlled self-attention module. We describe this memory transition mechanism eidetic as it is able to effectively recall the stored memories across multiple time stamps even after long periods of disturbance. We first evaluate the E3D-LSTM network on widely-used future video prediction datasets and achieve the state-of-the-art performance. Then we show that the E3D-LSTM network also performs well on the early activity recognition to infer what is happening or what will happen after observing only limited frames of video. This task aligns well with video prediction in modeling action intentions and tendency.", "keywords": [], "authorids": ["yunbowang1989@gmail.com", "lujiang@google.com", "mhyang@ucmerced.edu", "lijiali@google.com", "mingsheng@tsinghua.edu.cn", "feifeili@cs.stanford.edu"], "authors": ["Yunbo Wang", "Lu Jiang", "Ming-Hsuan Yang", "Li-Jia Li", "Mingsheng Long", "Li Fei-Fei"], "pdf": "/pdf/bd20c943b4cf216371e5f710b113c2292ff0e7bb.pdf", "paperhash": "wang|eidetic_3d_lstm_a_model_for_video_prediction_and_beyond", "_bibtex": "@inproceedings{\nwang2018eidetic,\ntitle={Eidetic 3D {LSTM}: A Model for Video Prediction and Beyond},\nauthor={Yunbo Wang and Lu Jiang and Ming-Hsuan Yang and Li-Jia Li and Mingsheng Long and Li Fei-Fei},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lKS2AqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1564/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610041, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1lKS2AqtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1564/Authors", "ICLR.cc/2019/Conference/Paper1564/Reviewers", "ICLR.cc/2019/Conference/Paper1564/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1564/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1564/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1564/Authors|ICLR.cc/2019/Conference/Paper1564/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1564/Reviewers", "ICLR.cc/2019/Conference/Paper1564/Authors", "ICLR.cc/2019/Conference/Paper1564/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610041}}}, {"id": "ByVd51AFRm", "original": null, "number": 4, "cdate": 1543262096399, "ddate": null, "tcdate": 1543262096399, "tmdate": 1543262581673, "tddate": null, "forum": "B1lKS2AqtX", "replyto": "S1gVVTLVhX", "invitation": "ICLR.cc/2019/Conference/-/Paper1564/Official_Comment", "content": {"title": "Responses to AnonReviewer2", "comment": "\nQ7 Novelty: the proposed model is a small extension of a previous work [Wang et al., 2017]\n>> Please see the answer to Q3 above (the novelty of the paper)\n\nQ8 Clarity and Motivation:\n\nA) Page 2...How long-term relations are learned given Eq. 1? \n>> Different from standard LSTMs, we are motivated by modeling the long-term relations across frames.  The long-term relations are learned by the RECALL function in Eq. 1, whose inputs are the historical memory states C_{t-\\tau:t-1}^k (in particular, we use C_{1:t-1}^k for most experiments in this work). The RECALL function queries useful information from C_{t-\\tau:t-1}^k using R_t. We have clarified this point in the paper (Page 4).\n\nB) Page 5...Not clear why Recall(.) should model long-term relations.\n>> The RECALL function enables an adaptive learning of short-term and long-term modeling. More specifically, in Eq. 1, C_{t-1}^k is added to C_t^k via a short-cut connection controlled by the forget gate. Intuitively, it conveys short-term information, thus allowing the RECALL function to focus on long-term relations. Empirically, the COPY task verifies that our model could make use of information from the distant memory states when future predictions are severely dependent on the distant past.\n\nC) Eq 1: why layer norm is required when defining C_t^k is not clear.\n>> We use the layer normalization technique to mitigate the covariant shift and stabilize the training process, as it has been commonly used in RNNs. We have made it clear in the paper. \n\nD) What if the Recall is instead modeled as attention?\n>> Making the RECALL function solely based on memory states C will make the relations between C_{t-1}^k and itself (or the relations between very short-term memory states) dominate the result of RECALL(.). Thus, we encode X_t and H_{t-1}^k into R_t, and use it as the query of the attentive RECALL function. \n\nE) Page 5 \u201cto minimize the l1 + l2 loss over every pixel in the frame\u201d is not clear.\n>> We use different objective functions for different tasks:\n1. Video prediction: L1 + L2 loss.\n2. Early activity recognition: Eq. 3 in the revised paper.\n\nQ9 Experiments:\n\nA) Page 7...Why are Seq. 1 and Seq. 2 irrelevant?\n>> We have rephrased this part in Page 6. Basically, the COPY task is to evaluate whether our model could recall useful information from the distant memory states. A well-performed predictive model should make precise predictions regarding Seq 2, as it has seen all frames of this sequence before. But this task is difficult for previous LSTM networks. Because the Seq 1 is totally irrelevant, making predictions of Seq 1 will erase its memory of Seq 2.\n\nB) Sec. 4.2, \u201cDataset and setup\u201d: which architecture has been used here?\n>> We have made it clear that the architecture for KTH is exactly the same as that for the Moving MNIST.\n\nC) Sec. 4.3...the something-something dataset is more realising than the other two \u201ctoy\u201d dataset. Why did the authors choose to train a 2 layers 3D-CNN encoders, instead of using existing pretrained 3D CNNs?\n>> In this paper, our goal is to explore a generic method that can infer the action tendency and intentions from sequential video frames. We show that in a fair setting (the same training set and similar #learnable parameters), the improvements of our work come from a better model to capture and predict low-level video data trends, along with a better understanding of high-level actions.\nAlthough using the 3D-CNN model pre-trained on video datasets may improve the results, it also makes fair comparisons among all methods very tricky. First, suppose a model improves the results; it is less clear whether it is because the model learns a better representation on the pretrained data, or it is actually better in modeling the target dataset. Second, due to the domain difference, it is hard to select which pretrained models (e.g. Sports1M or Kinetics) to use on which dataset, and the pretrained model works on one dataset (e.g. something-something) may not work well on another dataset(e.g. KTH). These issues can result in a lengthy and unclear experimental section.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1564/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1564/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1564/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Eidetic 3D LSTM: A Model for Video Prediction and Beyond", "abstract": "Spatiotemporal predictive learning, though long considered to be a promising self-supervised feature learning method, seldom shows its effectiveness beyond future video prediction. The reason is that it is difficult to learn good representations for both short-term frame dependency and long-term high-level relations. We present a new model, Eidetic 3D LSTM (E3D-LSTM), that integrates 3D convolutions into RNNs. The encapsulated 3D-Conv makes local perceptrons of RNNs motion-aware and enables the memory cell to store better short-term features. For long-term relations, we make the present memory state interact with its historical records via a gate-controlled self-attention module. We describe this memory transition mechanism eidetic as it is able to effectively recall the stored memories across multiple time stamps even after long periods of disturbance. We first evaluate the E3D-LSTM network on widely-used future video prediction datasets and achieve the state-of-the-art performance. Then we show that the E3D-LSTM network also performs well on the early activity recognition to infer what is happening or what will happen after observing only limited frames of video. This task aligns well with video prediction in modeling action intentions and tendency.", "keywords": [], "authorids": ["yunbowang1989@gmail.com", "lujiang@google.com", "mhyang@ucmerced.edu", "lijiali@google.com", "mingsheng@tsinghua.edu.cn", "feifeili@cs.stanford.edu"], "authors": ["Yunbo Wang", "Lu Jiang", "Ming-Hsuan Yang", "Li-Jia Li", "Mingsheng Long", "Li Fei-Fei"], "pdf": "/pdf/bd20c943b4cf216371e5f710b113c2292ff0e7bb.pdf", "paperhash": "wang|eidetic_3d_lstm_a_model_for_video_prediction_and_beyond", "_bibtex": "@inproceedings{\nwang2018eidetic,\ntitle={Eidetic 3D {LSTM}: A Model for Video Prediction and Beyond},\nauthor={Yunbo Wang and Lu Jiang and Ming-Hsuan Yang and Li-Jia Li and Mingsheng Long and Li Fei-Fei},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lKS2AqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1564/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610041, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1lKS2AqtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1564/Authors", "ICLR.cc/2019/Conference/Paper1564/Reviewers", "ICLR.cc/2019/Conference/Paper1564/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1564/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1564/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1564/Authors|ICLR.cc/2019/Conference/Paper1564/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1564/Reviewers", "ICLR.cc/2019/Conference/Paper1564/Authors", "ICLR.cc/2019/Conference/Paper1564/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610041}}}, {"id": "HyxkqaatRm", "original": null, "number": 2, "cdate": 1543261575293, "ddate": null, "tcdate": 1543261575293, "tmdate": 1543261924021, "tddate": null, "forum": "B1lKS2AqtX", "replyto": "HJeJHrJc27", "invitation": "ICLR.cc/2019/Conference/-/Paper1564/Official_Comment", "content": {"title": "Responses to AnonReviewer3.", "comment": "\nQ1: In the introduction, the authors state that they account for uncertainty by better modeling the temporal sequence... \n\nWe have rephrased this expression for clarity in the revised paper (Page 2). Below is a copy: Future prediction errors of an imperfect model can be categorized by two factors: a) the \u201csystematic errors\u201d caused by a lack of modeling ability to the deterministic variations; b) the stochastic, inherent uncertainty of the future. We aim to minimize the first factor in this work.\n\nQ2: Analyze the work in more complex settings.\n\nWe have experimented with the Something-Something dataset for video prediction, but the generated frames are not satisfying even when integrated with adversarial training and variational methods. The results are not surprising as the number of training samples is too limited to capture the diverse scenes of real-world videos (due to the illumination, occlusion, camera motion, to name a few). This makes future prediction considerably difficult for all existing methods, including ours. Exploring very complex datasets will be an interesting future research direction for this task. \n\nHowever, as R3 suggested, we further evaluate our method on a real-world dataset for traffic flow prediction, i.e., TaxtBJ. In this dataset, traffic flows (in consecutive heat maps) are collected from the chaotic real-world environment. Predicting urban traffic conditions is a complex setting, as the heat maps are very noisy and we do not have any corresponding, underlying, additional information. Implementation details and empirical results can be found in Appendix B. We train the networks to predict 4 frames (the next 2 hours) from 4 observations and report MSE at every timestamp. As shown, our method achieves the state-of-the-art result in Table 8 and generates the most accurate predictions in Fig. 6.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1564/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1564/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1564/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Eidetic 3D LSTM: A Model for Video Prediction and Beyond", "abstract": "Spatiotemporal predictive learning, though long considered to be a promising self-supervised feature learning method, seldom shows its effectiveness beyond future video prediction. The reason is that it is difficult to learn good representations for both short-term frame dependency and long-term high-level relations. We present a new model, Eidetic 3D LSTM (E3D-LSTM), that integrates 3D convolutions into RNNs. The encapsulated 3D-Conv makes local perceptrons of RNNs motion-aware and enables the memory cell to store better short-term features. For long-term relations, we make the present memory state interact with its historical records via a gate-controlled self-attention module. We describe this memory transition mechanism eidetic as it is able to effectively recall the stored memories across multiple time stamps even after long periods of disturbance. We first evaluate the E3D-LSTM network on widely-used future video prediction datasets and achieve the state-of-the-art performance. Then we show that the E3D-LSTM network also performs well on the early activity recognition to infer what is happening or what will happen after observing only limited frames of video. This task aligns well with video prediction in modeling action intentions and tendency.", "keywords": [], "authorids": ["yunbowang1989@gmail.com", "lujiang@google.com", "mhyang@ucmerced.edu", "lijiali@google.com", "mingsheng@tsinghua.edu.cn", "feifeili@cs.stanford.edu"], "authors": ["Yunbo Wang", "Lu Jiang", "Ming-Hsuan Yang", "Li-Jia Li", "Mingsheng Long", "Li Fei-Fei"], "pdf": "/pdf/bd20c943b4cf216371e5f710b113c2292ff0e7bb.pdf", "paperhash": "wang|eidetic_3d_lstm_a_model_for_video_prediction_and_beyond", "_bibtex": "@inproceedings{\nwang2018eidetic,\ntitle={Eidetic 3D {LSTM}: A Model for Video Prediction and Beyond},\nauthor={Yunbo Wang and Lu Jiang and Ming-Hsuan Yang and Li-Jia Li and Mingsheng Long and Li Fei-Fei},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lKS2AqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1564/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610041, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1lKS2AqtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1564/Authors", "ICLR.cc/2019/Conference/Paper1564/Reviewers", "ICLR.cc/2019/Conference/Paper1564/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1564/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1564/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1564/Authors|ICLR.cc/2019/Conference/Paper1564/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1564/Reviewers", "ICLR.cc/2019/Conference/Paper1564/Authors", "ICLR.cc/2019/Conference/Paper1564/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610041}}}, {"id": "r1e-K3aF0Q", "original": null, "number": 1, "cdate": 1543261304794, "ddate": null, "tcdate": 1543261304794, "tmdate": 1543261910207, "tddate": null, "forum": "B1lKS2AqtX", "replyto": "B1lKS2AqtX", "invitation": "ICLR.cc/2019/Conference/-/Paper1564/Official_Comment", "content": {"title": "We thank reviewers for the valuable comments. ", "comment": "We thank reviewers for the valuable comments. Based on the reviews, we make the following changes (we mark these changes in blue in the revised paper):\n\n1. As suggested by R1, we enable our method to perform the online recognition tasks and compare our online model with and without the frame-prediction loss in Table 7.\n\n2. As suggested by R3, we add an additional real-world dataset on traffic flow prediction and evaluate our method under this complex setting. The results are presented in Appendix B.\n\n3. We rephrase/clarify all of the points raised by the reviewers.\n\nWe will address all questions in the individual replies.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1564/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1564/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1564/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Eidetic 3D LSTM: A Model for Video Prediction and Beyond", "abstract": "Spatiotemporal predictive learning, though long considered to be a promising self-supervised feature learning method, seldom shows its effectiveness beyond future video prediction. The reason is that it is difficult to learn good representations for both short-term frame dependency and long-term high-level relations. We present a new model, Eidetic 3D LSTM (E3D-LSTM), that integrates 3D convolutions into RNNs. The encapsulated 3D-Conv makes local perceptrons of RNNs motion-aware and enables the memory cell to store better short-term features. For long-term relations, we make the present memory state interact with its historical records via a gate-controlled self-attention module. We describe this memory transition mechanism eidetic as it is able to effectively recall the stored memories across multiple time stamps even after long periods of disturbance. We first evaluate the E3D-LSTM network on widely-used future video prediction datasets and achieve the state-of-the-art performance. Then we show that the E3D-LSTM network also performs well on the early activity recognition to infer what is happening or what will happen after observing only limited frames of video. This task aligns well with video prediction in modeling action intentions and tendency.", "keywords": [], "authorids": ["yunbowang1989@gmail.com", "lujiang@google.com", "mhyang@ucmerced.edu", "lijiali@google.com", "mingsheng@tsinghua.edu.cn", "feifeili@cs.stanford.edu"], "authors": ["Yunbo Wang", "Lu Jiang", "Ming-Hsuan Yang", "Li-Jia Li", "Mingsheng Long", "Li Fei-Fei"], "pdf": "/pdf/bd20c943b4cf216371e5f710b113c2292ff0e7bb.pdf", "paperhash": "wang|eidetic_3d_lstm_a_model_for_video_prediction_and_beyond", "_bibtex": "@inproceedings{\nwang2018eidetic,\ntitle={Eidetic 3D {LSTM}: A Model for Video Prediction and Beyond},\nauthor={Yunbo Wang and Lu Jiang and Ming-Hsuan Yang and Li-Jia Li and Mingsheng Long and Li Fei-Fei},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lKS2AqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1564/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610041, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1lKS2AqtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1564/Authors", "ICLR.cc/2019/Conference/Paper1564/Reviewers", "ICLR.cc/2019/Conference/Paper1564/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1564/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1564/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1564/Authors|ICLR.cc/2019/Conference/Paper1564/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1564/Reviewers", "ICLR.cc/2019/Conference/Paper1564/Authors", "ICLR.cc/2019/Conference/Paper1564/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610041}}}], "count": 11}