{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1363217640000, "tcdate": 1363217640000, "number": 1, "id": "fftnhM9InbLMv", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "-4IA4WgNAy4Wx", "replyto": "CC5h3a1ESBCav", "signatures": ["Guillaume Alain, Yoshua Bengio"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "> It would be good to compare these plots with other regularizers and show that getting log(p) for contractive one is somehow advantageous.\r\n\r\nWe have worked on the denoising/contracting auto-encoders with squared error because we were able to prove our results with them, but we believe that other regularized auto-encoders (even those with discrete inputs) also estimate something related to the score, i.e., the direction in input space in which probability increases the most. The intuition behind that statement can be obtained by studying figure 2: the estimation of this direction arises out of the conflict between reconstructing training examples well and making the auto-encoder as constant (regularized) as possible.\r\n\r\nOther regularizers (e.g. cross-entropy) as well as the challenging case of discrete data are in the back of our minds and we would very much like to extend mathematical results to these settings as well.\r\nWe have added a brief discussion in the conclusion about how we believe these results could be extended to models with discrete inputs, following the tracks of ratio matching (Hyvarinen 2007).\r\n\r\nWe have also added (in new sec. 3.2.3) a brief discussion of how these new results (on r(x)-x estimating the score) contradict previous interpretations of the reconstruction error of auto-encoders (Ranzato & Hinton NIPS 2007) as being akin to an energy function. Indeed whereas both interpretations agree on having a low reconstruction error at training examples, the score interpretation suggests (and we see it experimentally) other (median) regions that are local maxima of density, where the reconstruction error is also low.\r\n\r\n> it would be good to know something not in the limit of penalty going to zero\r\n\r\nWe agree. We did a few artificial data experiments. In fact, we ran the experiment shown in section 3.2.2 using values of lambda ranging from 10^-6 to 10^2 to observe the behavior of the optimal solutions when the penalty factor varies smoothly. The optimal solution degrades progressively into something comparable to what is shown in Figure 2. It becomes a series of increasing plateaus matching the density peaks. Regions of lesser density are used to 'catch up' with the fact that the reconstruction function r(x) should be relatively close to x.\r\n\r\n\r\n> Figure 4. - 'Top plots are for one model and bottom plots for another' - what are the two models? It would be good to specify this in the figure, e.g. denosing autoencoders with different initial conditions and parameter settings.\r\n\r\nWe have addressed this concern that many of the reviewers had. The whole section 3.2.3 has been edited and we decided to remove two of the plots which may have introduced confusion. Reviewers seem to focus on the difference between the two models and wanted to know why the outcomes were different. They were only different because of the non-convexity of the problem and the dependance on initial conditions (along with the random noise used for training). At the end of the day, the point is that the vector field points in the direction of the energy gradient, and that is illustrated nicely by the two plots left (far and close distance). \r\n\r\n> Section 3.2.5 is important and should be written a little more clearly.\r\n\r\nWe have reworked that section (now identified as 3.2.6), to emphasize the main point: whereas Vincent 2011 showed that denoising auto-encoders with a particular form estimated the score, our results extend this to a very large family of estimators (including the non-parametric case). The section also shows how to interpret Vincent's results so as to show that any auto-encoder whose reconstruction function is a derivative of an energy function can be shown to estimate a score. Instead, the rest of our paper shows that we achieve an estimator of the score even without that strong constraint on the form of the auto-encoder.\r\n\r\n> I would suggest deriving (13) in the appendix directly from (11) without having the reader recall or read about Euler-Lagrange equations\r\n\r\nWe must admit to not having understood the hints that you have given us. If indeed there was such a way to, as you say, spare the reader the headaches of Euler-Lagrange, we agree that it would be an interesting approach. \r\n\r\n> You don't actually derive formulas the second moments in the appendix like you do for the first moment, you mean they can similarly be derived?\r\n\r\nYes, an asymptotic expansion can be derived in a similar way for the second moment. That derivation is 2 to 3 times longer and is not very useful in the context of this paper.\r\n\r\nPlease also have a look at a new short section (now identified as 3.2.5) that we just added in."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "What Regularized Auto-Encoders Learn from the Data Generating Distribution", "decision": "conferenceOral-iclr2013-conference", "abstract": "What do auto-encoders learn about the underlying data generating distribution? Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of data. This paper clarifies some of these previous intuitive observations by showing that minimizing a particular form of regularized reconstruction error yields a reconstruction function that locally characterizes the shape of the data generating density. We show that the auto-encoder captures the score (derivative of the log-density with respect to the input), along with the second derivative of the density and the local mean associated with the unknown data-generating density. This is the second result linking denoising auto-encoders and score matching, but in way that is different from previous work, and can be applied to the case when the auto-encoder reconstruction function does not necessarily correspond to the derivative of an energy function. The theorems provided here are completely generic and do not depend on the parametrization of the auto-encoder: they show what the auto-encoder would tend to if given enough capacity and examples. These results are for a contractive training criterion we show to be similar to the denoising auto-encoder training criterion with small corruption noise, but with contraction applied on the whole reconstruction function rather than just encoder. Similarly to score matching, one can consider the proposed training criterion as a convenient alternative to maximum likelihood, i.e., one not involving a partition function.", "pdf": "https://arxiv.org/abs/1211.4246", "paperhash": "alain|what_regularized_autoencoders_learn_from_the_data_generating_distribution", "keywords": [], "conflicts": [], "authors": ["Guillaume Alain", "Yoshua Bengio"], "authorids": ["guillaume.alain.umontreal@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363217640000, "tcdate": 1363217640000, "number": 1, "id": "EEBiEfDQjdwft", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "-4IA4WgNAy4Wx", "replyto": "1WIBWMxZeG4UP", "signatures": ["Guillaume Alain, Yoshua Bengio"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "> I think this is quite an important result. even though limited to this specific type of model\r\n\r\nAs argued in a previous response (to reviewer 4222), we believe that at least at a qualitative level the same is true in general of regularized auto-encoders. We copy here the response: \r\n'We have worked on the denoising/contracting auto-encoders with squared error because we were able to prove our results with them, but we believe that other regularized auto-encoders (even those with discrete inputs) also estimate something related to the score, i.e., the direction in input space in which probability increases the most. The intuition behind that statement can be obtained by studying figure 2: the estimation of this direction arises out of the conflict between reconstructing training examples well and making the auto-encoder as constant (regularized) as possible.'\r\nWe have added a brief discussion in the conclusion about how we believe these results could be extended to models with discrete inputs, following the tracks of ratio matching (Hyvarinen 2007).\r\n\r\nWe have also added (in new sec. 3.2.3) a brief discussion of how these new results (on r(x)-x estimating the score) contradict previous interpretations of the reconstruction error of auto-encoders (Ranzato & Hinton NIPS 2007) as being akin to an energy function. Indeed whereas both interpretations agree on having a low reconstruction error at training examples, the score interpretation suggests (and we see it experimentally) other (median) regions that are local maxima of density, where the reconstruction error is also low.\r\n\r\n> I find the experiment shown in Figure 4 somewhat confusing.\r\n\r\nWe have addressed this concern that many of the reviewers had. The whole section 3.2.3 has been edited and we decided to remove two of the plots which may have introduced confusion. Reviewers seem to focus on the difference between the two models and wanted to know why the outcomes were different. They were only different because of the non-convexity of the problem and the dependance on initial conditions (along with the random noise used for training). At the end of the day, the point is that the vector field points in the direction of the energy gradient, and that is illustrated nicely by the two plots left (far and close distance). \r\n\r\n\r\n> Section 3.2.4. I am not clear what is the importance of this section. It seems to state the relationship between the score and reconstruction derivative.\r\n\r\nAre you referring to section 3.3 ? If you are indeed referring to section 3.2.4, the idea there is that it is possible to start the investigation from a trained DAE where the noise level for the training is unknown to us (but it is known by the person who trained the DAE). In that case, we would be in a situation where we the best that could be done was to recover the energy function gradient up to a scaling constant.\r\n\r\n\r\n> Is it possible to link these results and theory to other forms of auto-encoders, such as sparse auto-encoders or with different type of non-linear activation functions? It would be very useful to have similar analysis for more general types of auto-encoders too.\r\n\r\nSee our first response above.\r\n\r\nPlease also have a look at a new short section (now identified as 3.2.5) that we just added in."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "What Regularized Auto-Encoders Learn from the Data Generating Distribution", "decision": "conferenceOral-iclr2013-conference", "abstract": "What do auto-encoders learn about the underlying data generating distribution? Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of data. This paper clarifies some of these previous intuitive observations by showing that minimizing a particular form of regularized reconstruction error yields a reconstruction function that locally characterizes the shape of the data generating density. We show that the auto-encoder captures the score (derivative of the log-density with respect to the input), along with the second derivative of the density and the local mean associated with the unknown data-generating density. This is the second result linking denoising auto-encoders and score matching, but in way that is different from previous work, and can be applied to the case when the auto-encoder reconstruction function does not necessarily correspond to the derivative of an energy function. The theorems provided here are completely generic and do not depend on the parametrization of the auto-encoder: they show what the auto-encoder would tend to if given enough capacity and examples. These results are for a contractive training criterion we show to be similar to the denoising auto-encoder training criterion with small corruption noise, but with contraction applied on the whole reconstruction function rather than just encoder. Similarly to score matching, one can consider the proposed training criterion as a convenient alternative to maximum likelihood, i.e., one not involving a partition function.", "pdf": "https://arxiv.org/abs/1211.4246", "paperhash": "alain|what_regularized_autoencoders_learn_from_the_data_generating_distribution", "keywords": [], "conflicts": [], "authors": ["Guillaume Alain", "Yoshua Bengio"], "authorids": ["guillaume.alain.umontreal@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363216980000, "tcdate": 1363216980000, "number": 1, "id": "mmLgxpNpu1xGP", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "-4IA4WgNAy4Wx", "replyto": "kGNDPAwn1jGUc", "signatures": ["Guillaume Alain, Yoshua Bengio"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "> It's interesting that in the classical CAE, there is an implicit contractive effect on g() via the side effect of tying the weights whereas in the form of the DAE presented, g() is explicitly made contractive via r(). Have you investigated the effective difference?\r\n\r\nNot really, no. The results that we have for general autoencoders r does not even assume that r is decomposable into two meaningful steps (encode, decode). However, in our experiments we found better results (due to optimization issues) with untied weights (and a contractive or denoising penalty on the whole of r(.)=decoder(encoder(.)) rather than just the encoder).\r\n\r\nWe have also added (in new sec. 3.2.3) a brief discussion of how these new results (on r(x)-x estimating the score) contradict previous interpretations of the reconstruction error of auto-encoders (Ranzato & Hinton NIPS 2007) as being akin to an energy function. Indeed whereas both interpretations agree on having a low reconstruction error at training examples, the score interpretation suggests (and we see it experimentally) other (median) regions that are local maxima of density, where the reconstruction error is also low.\r\n\r\n\r\n> Although in the caption, you mention the difference between upper/lower and left/right subplots in Fig 4, I would prefer those (model 1/model 2) to be labeled directly on the subplots, it would just make for easier parsing.\r\n\r\nThe section with Figure 4 has been edited and we are now showing only two plots.\r\n\r\nWe have made all the suggested changes regarding typos and form. Please also have a look at a new short section (now identified as 3.2.5) that we just added in."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "What Regularized Auto-Encoders Learn from the Data Generating Distribution", "decision": "conferenceOral-iclr2013-conference", "abstract": "What do auto-encoders learn about the underlying data generating distribution? Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of data. This paper clarifies some of these previous intuitive observations by showing that minimizing a particular form of regularized reconstruction error yields a reconstruction function that locally characterizes the shape of the data generating density. We show that the auto-encoder captures the score (derivative of the log-density with respect to the input), along with the second derivative of the density and the local mean associated with the unknown data-generating density. This is the second result linking denoising auto-encoders and score matching, but in way that is different from previous work, and can be applied to the case when the auto-encoder reconstruction function does not necessarily correspond to the derivative of an energy function. The theorems provided here are completely generic and do not depend on the parametrization of the auto-encoder: they show what the auto-encoder would tend to if given enough capacity and examples. These results are for a contractive training criterion we show to be similar to the denoising auto-encoder training criterion with small corruption noise, but with contraction applied on the whole reconstruction function rather than just encoder. Similarly to score matching, one can consider the proposed training criterion as a convenient alternative to maximum likelihood, i.e., one not involving a partition function.", "pdf": "https://arxiv.org/abs/1211.4246", "paperhash": "alain|what_regularized_autoencoders_learn_from_the_data_generating_distribution", "keywords": [], "conflicts": [], "authors": ["Guillaume Alain", "Yoshua Bengio"], "authorids": ["guillaume.alain.umontreal@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362368160000, "tcdate": 1362368160000, "number": 2, "id": "1WIBWMxZeG4UP", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "-4IA4WgNAy4Wx", "replyto": "-4IA4WgNAy4Wx", "signatures": ["anonymous reviewer 7ffb"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of What Regularized Auto-Encoders Learn from the Data Generating Distribution", "review": "The paper presents a method to analyse how and what the auto-encoder models that use reconstruction error together with a regularisation cost, are learning with respect to the underlying data distribution. The paper focuses on contractive auto-encoder models and also reformulates denoising auto-encoder as a form of contractive auto-encoder where the contraction is achieved through regularisation of the derivative of reconstruction error wrt to the input data. The rest of the paper presents a theoretical analysis of this form of auto-encoders and also provides couple of toy examples showing empirical support.\r\n\r\nThe paper is easy to read and the theoretical analysis is nicely split between the main paper and appendices. The details in the main paper are sufficient for the reader to understand the concept that is presented in the paper.\r\n\r\nThe theory and empirical data show that one can recover the true data distribution if using contractive auto-encoders of the given type. I think this is quite an important result. even though limited to this specific type of model, quantitative analysis of generative capabilities of auto-encoders have been limited.\r\n\r\nI find the experiment shown in Figure 4 somewhat confusing. The text suggests that the only difference between the two models is their initial conditions and optimisation hyper parameters. Is the main reason due to initial conditions or hyper parameters? Which hyper parameters? Is the difference in initial condition just a different random seed or different type of initialisation of the network? I think this requires more in depth explanation. Is it normal to expect such different solutions depending on initial conditions?\r\n\r\nSection 3.2.4. I am not clear what is the importance of this section. It seems to state the relationship between the score and reconstruction derivative.\r\n\r\nIs it possible to link these results and theory to other forms of auto-encoders, such as sparse auto-encoders or with different type of non-linear activation functions? It would be very useful to have similar analysis for more general types of auto-encoders too."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "What Regularized Auto-Encoders Learn from the Data Generating Distribution", "decision": "conferenceOral-iclr2013-conference", "abstract": "What do auto-encoders learn about the underlying data generating distribution? Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of data. This paper clarifies some of these previous intuitive observations by showing that minimizing a particular form of regularized reconstruction error yields a reconstruction function that locally characterizes the shape of the data generating density. We show that the auto-encoder captures the score (derivative of the log-density with respect to the input), along with the second derivative of the density and the local mean associated with the unknown data-generating density. This is the second result linking denoising auto-encoders and score matching, but in way that is different from previous work, and can be applied to the case when the auto-encoder reconstruction function does not necessarily correspond to the derivative of an energy function. The theorems provided here are completely generic and do not depend on the parametrization of the auto-encoder: they show what the auto-encoder would tend to if given enough capacity and examples. These results are for a contractive training criterion we show to be similar to the denoising auto-encoder training criterion with small corruption noise, but with contraction applied on the whole reconstruction function rather than just encoder. Similarly to score matching, one can consider the proposed training criterion as a convenient alternative to maximum likelihood, i.e., one not involving a partition function.", "pdf": "https://arxiv.org/abs/1211.4246", "paperhash": "alain|what_regularized_autoencoders_learn_from_the_data_generating_distribution", "keywords": [], "conflicts": [], "authors": ["Guillaume Alain", "Yoshua Bengio"], "authorids": ["guillaume.alain.umontreal@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362321540000, "tcdate": 1362321540000, "number": 3, "id": "CC5h3a1ESBCav", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "-4IA4WgNAy4Wx", "replyto": "-4IA4WgNAy4Wx", "signatures": ["anonymous reviewer 4222"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of What Regularized Auto-Encoders Learn from the Data Generating Distribution", "review": "This paper shows that we can relate the solution of specific autoencoder to the data generating distribution. Specifically solving for general reconstruction function with regularizer that is the L2 penalty of reconstruction contraction relates the reconstruction function derivative of the data probability log likelihood. This is in the limit of small regularization. The paper also shows that in the limit of small penalty this autoencoder is equivalent to denoising autoencoder with small noise.\r\n\r\nSection 3.2.3: You get similar attractive behavior using almost any autoencoder with limited capacity. The point of your work is that with the specific form of regularization - square norm of contraction of r - the r(x)-x relates to derivative of log probability (proof seem to require it - it would be interesting to know what can be said about other regularizers). It would be good to compare these plots with other regularizers and show that getting log(p) for contractive one is somehow advantageous. Otherwise this section doesn't support this paper in any way.\r\n\r\nAs authors point out, it would be good to know something not in the limit of penalty going to zero. At least have some numerical experiments, for example in 1d or 2d. \r\n\r\nFigure 4. - 'Top plots are for one model and bottom plots for another' - what are the two models? It would be good to specify this in the figure, e.g. denosing autoencoders with different initial conditions and parameter settings.\r\n\r\nSection 3.2.5 is important and should be written a little more clearly. \r\n\r\nI would suggest deriving (13) in the appendix directly from (11) without having the reader recall or read about Euler-Lagrange equations, and it might actually turn out to be simpler. Differentiating the first term with r(x) gives r(x)-x. For the second term one moves the derivative to the other size using integration by parts (and droping the boundary term) and then just applying it to the product p(x)dr/dx resulting in (13).\r\n\r\nMinor - twice you say in the appending that the proof is in the appendinx (e.g. after statement of theorem 1)\r\n\r\nThe second last sentence in the abstract is uncomfortable to read.\r\n\r\nThis is probably not important, but can we assume that r given by (11) actually has a taylor expansion in lambda? (probably, but in the spirit of prooving things).\r\n\r\nYou don't actually derive formulas the second moments in the appendix like you do for the first moment, you mean they can similarly be derived?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "What Regularized Auto-Encoders Learn from the Data Generating Distribution", "decision": "conferenceOral-iclr2013-conference", "abstract": "What do auto-encoders learn about the underlying data generating distribution? Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of data. This paper clarifies some of these previous intuitive observations by showing that minimizing a particular form of regularized reconstruction error yields a reconstruction function that locally characterizes the shape of the data generating density. We show that the auto-encoder captures the score (derivative of the log-density with respect to the input), along with the second derivative of the density and the local mean associated with the unknown data-generating density. This is the second result linking denoising auto-encoders and score matching, but in way that is different from previous work, and can be applied to the case when the auto-encoder reconstruction function does not necessarily correspond to the derivative of an energy function. The theorems provided here are completely generic and do not depend on the parametrization of the auto-encoder: they show what the auto-encoder would tend to if given enough capacity and examples. These results are for a contractive training criterion we show to be similar to the denoising auto-encoder training criterion with small corruption noise, but with contraction applied on the whole reconstruction function rather than just encoder. Similarly to score matching, one can consider the proposed training criterion as a convenient alternative to maximum likelihood, i.e., one not involving a partition function.", "pdf": "https://arxiv.org/abs/1211.4246", "paperhash": "alain|what_regularized_autoencoders_learn_from_the_data_generating_distribution", "keywords": [], "conflicts": [], "authors": ["Guillaume Alain", "Yoshua Bengio"], "authorids": ["guillaume.alain.umontreal@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362214560000, "tcdate": 1362214560000, "number": 1, "id": "kGNDPAwn1jGUc", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "-4IA4WgNAy4Wx", "replyto": "-4IA4WgNAy4Wx", "signatures": ["anonymous reviewer f62a"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of What Regularized Auto-Encoders Learn from the Data Generating Distribution", "review": "Many unsupervised representation-learning algorithms are based on minimizing reconstruction error. This paper aims at addressing the important questions around what these training criteria actually learn about the input density. \r\n\r\nThe paper makes two main contributions: it first makes a link between denoising autoencoders (DAE) and contractive autoencoders (CAE), showing that the DAE with very small Gaussian corruption and squared error is actually a particular kind of CAE (Theorem 1). Then, in the context of the contractive training criteria, it answers the question 'what does an auto-encoder learn about the data-generating distribution': it estimates both the first and second derivatives of the log-data generating density (Theorem 2) as well as other various local properties of this log-density. An important aspect of this work is that, compared to previous work that linked DAEs to score matching, the results in this paper do not require the reconstruction function of the AE to correspond to the score function of a density, making these results more general.\r\n\r\nPositive aspects of the paper:\r\n  * A pretty theoretical paper (for representation learning) but well presented in that most of the heavy math is in the appendix and the main text nicely presents the key results\r\n  * Following the theorems, I like the way in which the various assumptions (perfect world scenario) are gradually pulled away to show what can still be learned about the data-generating distribution; in particular, the simple numerical example (which could be easily re-implemented) is a nice way to connect the abstractness of the result to something concrete\r\n\r\nNegative aspects of the paper:\r\n  * Since the results heavily rely on derivatives with respect to the data, they only apply to continous data (extensions to discrete data are mentioned as future work)\r\n\r\nComments, Questions\r\n--------\r\n\r\nIt's interesting that in the classical CAE, there is an implicit contractive effect on g() via the side effect of tying the weights whereas in the form of the DAE presented, g() is explicitly made contractive via r(). Have you investigated the effective difference?\r\n\r\nMinor comments, typos, etc\r\n--------------------------\r\nFig 2 - green is not really green, it's more like turquoise\r\n      - 'high-capcity' -> 'high-capacity'\r\n      - the figure makes reference to lambda but at this point in the paper, lambda is yet to be defined\r\n\r\nobjective function for L_DAE (top of p4) - last term o() coming from the Taylor expansion is explicitly discussed in appendix (and perhaps obvious here) but is not explicitly defined in the main text\r\n\r\nRight before 3.2.4 'high dimensional <data> (such as images)' \r\n\r\nAlthough in the caption, you mention the difference between upper/lower and left/right subplots in Fig 4, I would prefer those (model 1/model 2) to be labeled directly on the subplots, it would just make for easier parsing."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "What Regularized Auto-Encoders Learn from the Data Generating Distribution", "decision": "conferenceOral-iclr2013-conference", "abstract": "What do auto-encoders learn about the underlying data generating distribution? Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of data. This paper clarifies some of these previous intuitive observations by showing that minimizing a particular form of regularized reconstruction error yields a reconstruction function that locally characterizes the shape of the data generating density. We show that the auto-encoder captures the score (derivative of the log-density with respect to the input), along with the second derivative of the density and the local mean associated with the unknown data-generating density. This is the second result linking denoising auto-encoders and score matching, but in way that is different from previous work, and can be applied to the case when the auto-encoder reconstruction function does not necessarily correspond to the derivative of an energy function. The theorems provided here are completely generic and do not depend on the parametrization of the auto-encoder: they show what the auto-encoder would tend to if given enough capacity and examples. These results are for a contractive training criterion we show to be similar to the denoising auto-encoder training criterion with small corruption noise, but with contraction applied on the whole reconstruction function rather than just encoder. Similarly to score matching, one can consider the proposed training criterion as a convenient alternative to maximum likelihood, i.e., one not involving a partition function.", "pdf": "https://arxiv.org/abs/1211.4246", "paperhash": "alain|what_regularized_autoencoders_learn_from_the_data_generating_distribution", "keywords": [], "conflicts": [], "authors": ["Guillaume Alain", "Yoshua Bengio"], "authorids": ["guillaume.alain.umontreal@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358406900000, "tcdate": 1358406900000, "number": 6, "id": "-4IA4WgNAy4Wx", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "-4IA4WgNAy4Wx", "signatures": ["guillaume.alain.umontreal@gmail.com"], "readers": ["everyone"], "content": {"title": "What Regularized Auto-Encoders Learn from the Data Generating Distribution", "decision": "conferenceOral-iclr2013-conference", "abstract": "What do auto-encoders learn about the underlying data generating distribution? Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of data. This paper clarifies some of these previous intuitive observations by showing that minimizing a particular form of regularized reconstruction error yields a reconstruction function that locally characterizes the shape of the data generating density. We show that the auto-encoder captures the score (derivative of the log-density with respect to the input), along with the second derivative of the density and the local mean associated with the unknown data-generating density. This is the second result linking denoising auto-encoders and score matching, but in way that is different from previous work, and can be applied to the case when the auto-encoder reconstruction function does not necessarily correspond to the derivative of an energy function. The theorems provided here are completely generic and do not depend on the parametrization of the auto-encoder: they show what the auto-encoder would tend to if given enough capacity and examples. These results are for a contractive training criterion we show to be similar to the denoising auto-encoder training criterion with small corruption noise, but with contraction applied on the whole reconstruction function rather than just encoder. Similarly to score matching, one can consider the proposed training criterion as a convenient alternative to maximum likelihood, i.e., one not involving a partition function.", "pdf": "https://arxiv.org/abs/1211.4246", "paperhash": "alain|what_regularized_autoencoders_learn_from_the_data_generating_distribution", "keywords": [], "conflicts": [], "authors": ["Guillaume Alain", "Yoshua Bengio"], "authorids": ["guillaume.alain.umontreal@gmail.com", "yoshua.bengio@gmail.com"]}, "writers": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 7}