{"notes": [{"tddate": null, "ddate": null, "tmdate": 1520286968176, "tcdate": 1520286968176, "number": 1, "cdate": 1520286968176, "id": "H1x-pNoOf", "invitation": "ICLR.cc/2017/workshop/-/paper129/public/comment", "forum": "r1rqJyHKg", "replyto": "r1rqJyHKg", "signatures": ["~Xingyu_Liu1"], "readers": ["everyone"], "writers": ["~Xingyu_Liu1"], "content": {"title": "New Version", "comment": "The new version of our paper was accepted into ICLR 2018. It can be found at https://openreview.net/forum?id=HJzgZ3JCW. We also open-source our code at https://github.com/xingyul/Sparse-Winograd-CNN. The arXiv version is at https://arxiv.org/abs/1802.06367."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Sparse-Winograd Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (CNNs) are compute intensive which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd\u2019s minimal filtering algorithm (Lavin (2015)) and network pruning (Han et al. (2015)) reduce the operation count. Unfortunately, these two methods cannot be combined\u2014because applying theWinograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we prune the weights in the \u201dWinograd domain\u201d (after the transform) to exploit static weight sparsity. Second, we move the ReLU operation into the \u201dWinograd domain\u201d to improve the sparsity of the transformed activations. On CIFAR-10, our method reduces the number of multiplications in the VGG-nagadomi model by 10.2x with no loss of accuracy.", "pdf": "/pdf/58e46cd9549f34179948bd78f602d5fc481f6e58.pdf", "TL;DR": "Prune and ReLU in Winograd domain for efficient convolutional neural network", "paperhash": "liu|efficient_sparsewinograd_convolutional_neural_networks", "keywords": ["Deep learning"], "conflicts": ["stanford.edu", "nvidia.com"], "authors": ["Xingyu Liu", "Song Han", "Huizi Mao", "William J. Dally"], "authorids": ["xyl@stanford.edu", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487363981823, "tcdate": 1487363981823, "id": "ICLR.cc/2017/workshop/-/paper129/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper129/reviewers"], "reply": {"forum": "r1rqJyHKg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487363981823}}}, {"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028617160, "tcdate": 1490028617160, "number": 1, "id": "HyZUuKTjg", "invitation": "ICLR.cc/2017/workshop/-/paper129/acceptance", "forum": "r1rqJyHKg", "replyto": "r1rqJyHKg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Sparse-Winograd Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (CNNs) are compute intensive which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd\u2019s minimal filtering algorithm (Lavin (2015)) and network pruning (Han et al. (2015)) reduce the operation count. Unfortunately, these two methods cannot be combined\u2014because applying theWinograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we prune the weights in the \u201dWinograd domain\u201d (after the transform) to exploit static weight sparsity. Second, we move the ReLU operation into the \u201dWinograd domain\u201d to improve the sparsity of the transformed activations. On CIFAR-10, our method reduces the number of multiplications in the VGG-nagadomi model by 10.2x with no loss of accuracy.", "pdf": "/pdf/58e46cd9549f34179948bd78f602d5fc481f6e58.pdf", "TL;DR": "Prune and ReLU in Winograd domain for efficient convolutional neural network", "paperhash": "liu|efficient_sparsewinograd_convolutional_neural_networks", "keywords": ["Deep learning"], "conflicts": ["stanford.edu", "nvidia.com"], "authors": ["Xingyu Liu", "Song Han", "Huizi Mao", "William J. Dally"], "authorids": ["xyl@stanford.edu", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028617708, "id": "ICLR.cc/2017/workshop/-/paper129/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "r1rqJyHKg", "replyto": "r1rqJyHKg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028617708}}}, {"tddate": null, "tmdate": 1489172314244, "tcdate": 1489172314244, "number": 2, "id": "BkMwvOejg", "invitation": "ICLR.cc/2017/workshop/-/paper129/official/review", "forum": "r1rqJyHKg", "replyto": "r1rqJyHKg", "signatures": ["ICLR.cc/2017/workshop/paper129/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper129/AnonReviewer1"], "content": {"title": "Nice trick to reduce the number of multiplications required to evaluate a CNN", "rating": "7: Good paper, accept", "review": "The paper presents two tricks to reduce the number of multiplications needed to compute convolutions with small kernels. First, the trainable filters are stored directly in the Winograd domain, rather than stored in feature domain and then transformed. This directly impacts the number of multiplications that need to be performed. Second, the  Winograd transformation and ReLU are swapped for network activations bringing even more sparsity into the Winograd domain. Oddly enough this change doesn't affect the performance of the networks, even though they no longer perform convolutions followed by ReLUs. The two tricks reduce the number of multiplications 2.2 times over a pruned network, while losing little accuracy.\n\nThe paper is a nice starting point for investigation of possivle speedups and efficient implementation of pruned networks. The empirical evaluation is slightly underwhelming, but sufficient for a workshop paper. I would like to see more evidence that the ReLU trick works (it drastically changes the semantics of the network). I would also want to see baselines such as smaller pruned networks. \n\nNit: I found Figure 1 hard to read and would prefer to have it accompanied by the equations for the computations performed.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Sparse-Winograd Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (CNNs) are compute intensive which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd\u2019s minimal filtering algorithm (Lavin (2015)) and network pruning (Han et al. (2015)) reduce the operation count. Unfortunately, these two methods cannot be combined\u2014because applying theWinograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we prune the weights in the \u201dWinograd domain\u201d (after the transform) to exploit static weight sparsity. Second, we move the ReLU operation into the \u201dWinograd domain\u201d to improve the sparsity of the transformed activations. On CIFAR-10, our method reduces the number of multiplications in the VGG-nagadomi model by 10.2x with no loss of accuracy.", "pdf": "/pdf/58e46cd9549f34179948bd78f602d5fc481f6e58.pdf", "TL;DR": "Prune and ReLU in Winograd domain for efficient convolutional neural network", "paperhash": "liu|efficient_sparsewinograd_convolutional_neural_networks", "keywords": ["Deep learning"], "conflicts": ["stanford.edu", "nvidia.com"], "authors": ["Xingyu Liu", "Song Han", "Huizi Mao", "William J. Dally"], "authorids": ["xyl@stanford.edu", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489172314964, "id": "ICLR.cc/2017/workshop/-/paper129/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper129/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper129/AnonReviewer2", "ICLR.cc/2017/workshop/paper129/AnonReviewer1"], "reply": {"forum": "r1rqJyHKg", "replyto": "r1rqJyHKg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper129/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper129/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489172314964}}}, {"tddate": null, "tmdate": 1489040518875, "tcdate": 1489040518875, "number": 1, "id": "B1k94_A5l", "invitation": "ICLR.cc/2017/workshop/-/paper129/official/review", "forum": "r1rqJyHKg", "replyto": "r1rqJyHKg", "signatures": ["ICLR.cc/2017/workshop/paper129/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper129/AnonReviewer2"], "content": {"title": "simple yet quite useful approach for reducing the number of multiplications required in a CNN.", "rating": "8: Top 50% of accepted papers, clear accept", "review": " Introducing the advantages of weight pruning and sparsity, in decreasing number of multiplications, into the Winograd domain results in a larger decrease in number  of multiplications required per forward pass. \nThe authors propose training the kernel in transform domain and also pruning the weights in the same thereby enabling Winograd transform to be used with sparse weights. \nThe Relu is now applied after Winograd transform for not loosing the advantage of operating on sparse activations.\n\nExperimental validation supports the authors claim of decrease in number of multiplications by using proposed approach.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Sparse-Winograd Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (CNNs) are compute intensive which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd\u2019s minimal filtering algorithm (Lavin (2015)) and network pruning (Han et al. (2015)) reduce the operation count. Unfortunately, these two methods cannot be combined\u2014because applying theWinograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we prune the weights in the \u201dWinograd domain\u201d (after the transform) to exploit static weight sparsity. Second, we move the ReLU operation into the \u201dWinograd domain\u201d to improve the sparsity of the transformed activations. On CIFAR-10, our method reduces the number of multiplications in the VGG-nagadomi model by 10.2x with no loss of accuracy.", "pdf": "/pdf/58e46cd9549f34179948bd78f602d5fc481f6e58.pdf", "TL;DR": "Prune and ReLU in Winograd domain for efficient convolutional neural network", "paperhash": "liu|efficient_sparsewinograd_convolutional_neural_networks", "keywords": ["Deep learning"], "conflicts": ["stanford.edu", "nvidia.com"], "authors": ["Xingyu Liu", "Song Han", "Huizi Mao", "William J. Dally"], "authorids": ["xyl@stanford.edu", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489172314964, "id": "ICLR.cc/2017/workshop/-/paper129/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper129/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper129/AnonReviewer2", "ICLR.cc/2017/workshop/paper129/AnonReviewer1"], "reply": {"forum": "r1rqJyHKg", "replyto": "r1rqJyHKg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper129/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper129/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489172314964}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487535571232, "tcdate": 1487363981282, "number": 129, "id": "r1rqJyHKg", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "r1rqJyHKg", "signatures": ["~Xingyu_Liu1"], "readers": ["everyone"], "content": {"title": "Efficient Sparse-Winograd Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (CNNs) are compute intensive which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd\u2019s minimal filtering algorithm (Lavin (2015)) and network pruning (Han et al. (2015)) reduce the operation count. Unfortunately, these two methods cannot be combined\u2014because applying theWinograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we prune the weights in the \u201dWinograd domain\u201d (after the transform) to exploit static weight sparsity. Second, we move the ReLU operation into the \u201dWinograd domain\u201d to improve the sparsity of the transformed activations. On CIFAR-10, our method reduces the number of multiplications in the VGG-nagadomi model by 10.2x with no loss of accuracy.", "pdf": "/pdf/58e46cd9549f34179948bd78f602d5fc481f6e58.pdf", "TL;DR": "Prune and ReLU in Winograd domain for efficient convolutional neural network", "paperhash": "liu|efficient_sparsewinograd_convolutional_neural_networks", "keywords": ["Deep learning"], "conflicts": ["stanford.edu", "nvidia.com"], "authors": ["Xingyu Liu", "Song Han", "Huizi Mao", "William J. Dally"], "authorids": ["xyl@stanford.edu", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 5}