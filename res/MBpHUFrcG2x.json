{"notes": [{"id": "MBpHUFrcG2x", "original": "tFwrE3P8WD", "number": 2275, "cdate": 1601308250743, "ddate": null, "tcdate": 1601308250743, "tmdate": 1614098253672, "tddate": null, "forum": "MBpHUFrcG2x", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Projected Latent Markov Chain Monte Carlo: Conditional Sampling of Normalizing Flows", "authorids": ["~Chris_Cannella1", "~Mohammadreza_Soltani1", "~Vahid_Tarokh1"], "authors": ["Chris Cannella", "Mohammadreza Soltani", "Vahid Tarokh"], "keywords": ["Conditional Sampling", "Normalizing Flows", "Markov Chain Monte Carlo", "Missing Data Inference"], "abstract": "We introduce Projected Latent Markov Chain Monte Carlo (PL-MCMC), a technique for sampling from the exact conditional distributions learned by normalizing flows. As a conditional sampling method, PL-MCMC enables Monte Carlo Expectation Maximization (MC-EM) training of normalizing flows from incomplete data. Through experimental tests applying normalizing flows to missing data tasks for a variety of data sets, we demonstrate the efficacy of PL-MCMC for conditional sampling from normalizing flows.", "one-sentence_summary": "We introduce and demonstrate a novel MCMC technique for sampling from the exact conditional distributions known by normalizing flows.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cannella|projected_latent_markov_chain_monte_carlo_conditional_sampling_of_normalizing_flows", "supplementary_material": "/attachment/7142f0503843ba6a0b7b8b11d57c8ef8da7e7880.zip", "pdf": "/pdf/32946e80b74b4bb7d6f25d74cb773ac68b9b4a36.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncannella2021projected,\ntitle={Projected Latent Markov Chain Monte Carlo: Conditional Sampling of Normalizing Flows},\nauthor={Chris Cannella and Mohammadreza Soltani and Vahid Tarokh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MBpHUFrcG2x}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "cV5i-S_q21q", "original": null, "number": 1, "cdate": 1610040505630, "ddate": null, "tcdate": 1610040505630, "tmdate": 1610474112848, "tddate": null, "forum": "MBpHUFrcG2x", "replyto": "MBpHUFrcG2x", "invitation": "ICLR.cc/2021/Conference/Paper2275/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This work combines normalizing flows with conditional sampling. While there are connections to other works, the paper seems novel and applicable, and has nice experimental results. The authors did a good job clarifying the reviewers questions, and have addressed their major concerns. We appreciate the additional analyses added to the paper."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Projected Latent Markov Chain Monte Carlo: Conditional Sampling of Normalizing Flows", "authorids": ["~Chris_Cannella1", "~Mohammadreza_Soltani1", "~Vahid_Tarokh1"], "authors": ["Chris Cannella", "Mohammadreza Soltani", "Vahid Tarokh"], "keywords": ["Conditional Sampling", "Normalizing Flows", "Markov Chain Monte Carlo", "Missing Data Inference"], "abstract": "We introduce Projected Latent Markov Chain Monte Carlo (PL-MCMC), a technique for sampling from the exact conditional distributions learned by normalizing flows. As a conditional sampling method, PL-MCMC enables Monte Carlo Expectation Maximization (MC-EM) training of normalizing flows from incomplete data. Through experimental tests applying normalizing flows to missing data tasks for a variety of data sets, we demonstrate the efficacy of PL-MCMC for conditional sampling from normalizing flows.", "one-sentence_summary": "We introduce and demonstrate a novel MCMC technique for sampling from the exact conditional distributions known by normalizing flows.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cannella|projected_latent_markov_chain_monte_carlo_conditional_sampling_of_normalizing_flows", "supplementary_material": "/attachment/7142f0503843ba6a0b7b8b11d57c8ef8da7e7880.zip", "pdf": "/pdf/32946e80b74b4bb7d6f25d74cb773ac68b9b4a36.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncannella2021projected,\ntitle={Projected Latent Markov Chain Monte Carlo: Conditional Sampling of Normalizing Flows},\nauthor={Chris Cannella and Mohammadreza Soltani and Vahid Tarokh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MBpHUFrcG2x}\n}"}, "tags": [], "invitation": {"reply": {"forum": "MBpHUFrcG2x", "replyto": "MBpHUFrcG2x", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040505615, "tmdate": 1610474112832, "id": "ICLR.cc/2021/Conference/Paper2275/-/Decision"}}}, {"id": "7ka9z27ytVg", "original": null, "number": 3, "cdate": 1603961919267, "ddate": null, "tcdate": 1603961919267, "tmdate": 1607325057351, "tddate": null, "forum": "MBpHUFrcG2x", "replyto": "MBpHUFrcG2x", "invitation": "ICLR.cc/2021/Conference/Paper2275/-/Official_Review", "content": {"title": "A slight variation on standard stochastic EM algorithms", "review": "Summary\n\nThis paper proposes a stochastic expectation--maximisation (EM) algorithm. The main idea is that the target distribution is specified as a deterministic mapping, a.k.a. a normalising flow, from some simple \"base\" distribution.\n\n\nStrengths\n\nThe algorithm appears to be formally correct (in the sense that it is a standard stochastic EM algorithm). The method is demonstrated on a large number of examples.\n\nWeaknesses\n\nThe proposed algorithm is just a standard Metropolis--Hastings (MH) update interspersed with a stochastic EM update for the parameters of the target distribution. This does not seem novel; such algorithms have been around for decades.\n\nThe authors should explain why they need for extending the space to include $y_O$ instead of just mapping $\\xi$ to $y_M$.\n\n\nMinor comments\n\n- There are a number of typos in the bibliography mostly related to inconsistent use of capital letters in article titles and journal/conference names.\n\n- What does the semi-colon in $p_{f,\\theta}(x_M; x_O)$ mean? Why not use a comma if this is meant to be a joint density?\n\n- In Section 3, it would be helpful to write the (extended) target distribution of the Metropolis--Hastings algorithm down explicitly and formally.\n\n- Within LaTeX's maths mode, you cannot just write operators $min$ and $Uniform$ (LaTeX treats this, e.g., as multiplying $m$ by $i$ by $n$ which leads to the wrong spacing).", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2275/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2275/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Projected Latent Markov Chain Monte Carlo: Conditional Sampling of Normalizing Flows", "authorids": ["~Chris_Cannella1", "~Mohammadreza_Soltani1", "~Vahid_Tarokh1"], "authors": ["Chris Cannella", "Mohammadreza Soltani", "Vahid Tarokh"], "keywords": ["Conditional Sampling", "Normalizing Flows", "Markov Chain Monte Carlo", "Missing Data Inference"], "abstract": "We introduce Projected Latent Markov Chain Monte Carlo (PL-MCMC), a technique for sampling from the exact conditional distributions learned by normalizing flows. As a conditional sampling method, PL-MCMC enables Monte Carlo Expectation Maximization (MC-EM) training of normalizing flows from incomplete data. Through experimental tests applying normalizing flows to missing data tasks for a variety of data sets, we demonstrate the efficacy of PL-MCMC for conditional sampling from normalizing flows.", "one-sentence_summary": "We introduce and demonstrate a novel MCMC technique for sampling from the exact conditional distributions known by normalizing flows.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cannella|projected_latent_markov_chain_monte_carlo_conditional_sampling_of_normalizing_flows", "supplementary_material": "/attachment/7142f0503843ba6a0b7b8b11d57c8ef8da7e7880.zip", "pdf": "/pdf/32946e80b74b4bb7d6f25d74cb773ac68b9b4a36.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncannella2021projected,\ntitle={Projected Latent Markov Chain Monte Carlo: Conditional Sampling of Normalizing Flows},\nauthor={Chris Cannella and Mohammadreza Soltani and Vahid Tarokh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MBpHUFrcG2x}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "MBpHUFrcG2x", "replyto": "MBpHUFrcG2x", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2275/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538100011, "tmdate": 1606915801656, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2275/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2275/-/Official_Review"}}}, {"id": "oWQ1rg9bfsJ", "original": null, "number": 2, "cdate": 1603894946743, "ddate": null, "tcdate": 1603894946743, "tmdate": 1606335597386, "tddate": null, "forum": "MBpHUFrcG2x", "replyto": "MBpHUFrcG2x", "invitation": "ICLR.cc/2021/Conference/Paper2275/-/Official_Review", "content": {"title": "Good paper on missing data completion with normalizing flows, some questions", "review": "I enjoyed reading this paper and the idea of combining normalizing flow density models with conditional sampling seems natural, useful and has interesting potential applications to missing data problems.\n\nThings that were not clear to me are the following:\n\n1. I don't understand what this \"pre-trained\" model being referred to is. Is this a generative model for the entire data set? Do you mean that you get the model $p_{f, \\theta}$ from somewhere already and not learn it yourself?\n\n2. The choice of $q$ is one of the more interesting parts of the proposed algorithm that deserves to be studied in greater detail! If we don't use $q$, wouldn't the algorithm still work? You should try a version of your algorithm without assuming any $q$ on $y_O$, then checking how much does it affect performance.\n\n3. I am not convinced that it is fair to set \"q to be an independent normal distribution centered on the conditioning values $x_{O}$\". Clearly, with sufficiently low variance for $q$ it is possible to force the chain to sample values of $y_{O}$ that are very close to $x_{O}$. Since the dependence between $y_{O}$ and $y_{M}$ is generally strong, this will have you perform good reconstruction of $x_{M}$. If you do use a $q$ you should try variances other than the (1e-03)^2 which you have used as mentioned in the Appendix.\n\n4. Is it expensive to compute the probability density for the Metropolis correction? If you don't do a Metropolis correction, do the conditional samples still look reasonably good?\n\n5. Somewhat related to 2). In section 6.3 figure 7 the acceptance rate for PL-MCMC looks really low and is continuing to drop. You should probably run the chain for even longer until the acceptance rate plateaus completely and see what the samples at this point look like. Or perhaps you should decrease your proposal scaling.\n\nI am willing to upgrade my review if particularly point 2,3 is addressed.\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2275/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2275/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Projected Latent Markov Chain Monte Carlo: Conditional Sampling of Normalizing Flows", "authorids": ["~Chris_Cannella1", "~Mohammadreza_Soltani1", "~Vahid_Tarokh1"], "authors": ["Chris Cannella", "Mohammadreza Soltani", "Vahid Tarokh"], "keywords": ["Conditional Sampling", "Normalizing Flows", "Markov Chain Monte Carlo", "Missing Data Inference"], "abstract": "We introduce Projected Latent Markov Chain Monte Carlo (PL-MCMC), a technique for sampling from the exact conditional distributions learned by normalizing flows. As a conditional sampling method, PL-MCMC enables Monte Carlo Expectation Maximization (MC-EM) training of normalizing flows from incomplete data. Through experimental tests applying normalizing flows to missing data tasks for a variety of data sets, we demonstrate the efficacy of PL-MCMC for conditional sampling from normalizing flows.", "one-sentence_summary": "We introduce and demonstrate a novel MCMC technique for sampling from the exact conditional distributions known by normalizing flows.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cannella|projected_latent_markov_chain_monte_carlo_conditional_sampling_of_normalizing_flows", "supplementary_material": "/attachment/7142f0503843ba6a0b7b8b11d57c8ef8da7e7880.zip", "pdf": "/pdf/32946e80b74b4bb7d6f25d74cb773ac68b9b4a36.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncannella2021projected,\ntitle={Projected Latent Markov Chain Monte Carlo: Conditional Sampling of Normalizing Flows},\nauthor={Chris Cannella and Mohammadreza Soltani and Vahid Tarokh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MBpHUFrcG2x}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "MBpHUFrcG2x", "replyto": "MBpHUFrcG2x", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2275/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538100011, "tmdate": 1606915801656, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2275/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2275/-/Official_Review"}}}, {"id": "RUUf0B8wHPP", "original": null, "number": 2, "cdate": 1605439106447, "ddate": null, "tcdate": 1605439106447, "tmdate": 1605444072652, "tddate": null, "forum": "MBpHUFrcG2x", "replyto": "7ka9z27ytVg", "invitation": "ICLR.cc/2021/Conference/Paper2275/-/Official_Comment", "content": {"title": "The contribution of the work is on conditional sampling, not EM training.", "comment": "Thank you for your time and consideration.\n\nTo address a misunderstanding, the purpose of our work (as indicated throughout) is to introduce a method for efficiently sampling from the conditional distributions known by normalizing flows.  The described EM training procedure is not the focus of the work.  Instead, the EM training procedure is described (see the beginning of Section 4) as an application of our proposed conditional sampling methodology.  We explain (see the first paragraph of Section 6) that the purpose of our experiments regarding the EM training of normalizing flows from missing data is to gauge whether our proposed conditional sampling methodology effectively samples from its intended distributions in practice.  In our discussion of the results of our EM training experiments (see the final paragraphs of Sections 5.1 and 5.2), we conclude that these experiments demonstrate that our proposed methodology is at least sufficiently effective in sampling from the conditional distributions of normalizing flows to enable the EM training of normalizing flows from missing data to a standard comparable to that established by existing techniques for training other generative models from incomplete data. \n\nTo address particular comments:\n * **Why extend the Markov Chain to include the working variable $\\\\mathbf{y}_{O}$?**  \nThis extension of the Markov Chain's state space is fundamental to the operation of our proposed PL-MCMC conditional sampling technique.  Including $\\\\mathbf{y}_{O}$ allows us to define a Markov Chain within the latent space of the normalizing flow that is asymptotically guaranteed (see Section 3.3) to sample from the desired conditional distributions.  If the state space is not expanded, asymptotically exact conditional sampling requires solving intractable integrals relating to conditional likelihoods given the observed data.  Expanding the state space lets us leverage the latent representation already provided by the normalizing flow while avoiding otherwise intractable computations.\n* **What is the meaning of the semi-colon within $p_{f, \\\\theta}$ $(\\\\mathbf\\{x\\}_\\{M\\}; \\\\mathbf\\{x\\}_\\{O\\})$?** \n\n By $p_{f, \\\\theta}$ $(\\\\mathbf\\{x\\}_\\{M\\}; \\\\mathbf\\{x\\}_\\{O\\})$, we are referring to the normalizing flow's modeled joint density between the missing and observed portions of the modeled data.  We find that using the traditional comma notation for the joint density heavily implies a fixed ordering for the density function's arguments that conflicts with the subsets of missing and observed variables.  We use the semi-colon notation to represent the somewhat unusual operation of separating observed from missing variables.  While the notation may not be particularly common, we believe it remains unobtrusive and concisely represents the underlying mathematical concept.\n* **Description of PL-MCMC's target distribution:**  \n\n A complete description of the target distribution was already provided within Section 3.1, \"The Projected Latent Target Distribution\".\n* **Comments regarding typesetting:**  \n\n We have addressed these typographic problems within the newly revised version of the work."}, "signatures": ["ICLR.cc/2021/Conference/Paper2275/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2275/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Projected Latent Markov Chain Monte Carlo: Conditional Sampling of Normalizing Flows", "authorids": ["~Chris_Cannella1", "~Mohammadreza_Soltani1", "~Vahid_Tarokh1"], "authors": ["Chris Cannella", "Mohammadreza Soltani", "Vahid Tarokh"], "keywords": ["Conditional Sampling", "Normalizing Flows", "Markov Chain Monte Carlo", "Missing Data Inference"], "abstract": "We introduce Projected Latent Markov Chain Monte Carlo (PL-MCMC), a technique for sampling from the exact conditional distributions learned by normalizing flows. As a conditional sampling method, PL-MCMC enables Monte Carlo Expectation Maximization (MC-EM) training of normalizing flows from incomplete data. Through experimental tests applying normalizing flows to missing data tasks for a variety of data sets, we demonstrate the efficacy of PL-MCMC for conditional sampling from normalizing flows.", "one-sentence_summary": "We introduce and demonstrate a novel MCMC technique for sampling from the exact conditional distributions known by normalizing flows.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cannella|projected_latent_markov_chain_monte_carlo_conditional_sampling_of_normalizing_flows", "supplementary_material": "/attachment/7142f0503843ba6a0b7b8b11d57c8ef8da7e7880.zip", "pdf": "/pdf/32946e80b74b4bb7d6f25d74cb773ac68b9b4a36.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncannella2021projected,\ntitle={Projected Latent Markov Chain Monte Carlo: Conditional Sampling of Normalizing Flows},\nauthor={Chris Cannella and Mohammadreza Soltani and Vahid Tarokh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MBpHUFrcG2x}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MBpHUFrcG2x", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2275/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2275/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2275/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2275/Authors|ICLR.cc/2021/Conference/Paper2275/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2275/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850263, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2275/-/Official_Comment"}}}, {"id": "lzJsN4cuvQ", "original": null, "number": 5, "cdate": 1605443584122, "ddate": null, "tcdate": 1605443584122, "tmdate": 1605443705797, "tddate": null, "forum": "MBpHUFrcG2x", "replyto": "y_YRoMR2zpN", "invitation": "ICLR.cc/2021/Conference/Paper2275/-/Official_Comment", "content": {"title": "Excellent suggestion regarding hyperparameter analysis.  One potential clarification.", "comment": "Thank you for your time and consideration.  \n\nTo address a potential misunderstanding, our proposed technique only requires the normalizing flow to satisfy very mild positivity and smoothness constraints to ensure asymptotically exact conditional sampling.  The described MC-EM training procedure is not required for the conditional sampling from normalizing flows.  We included the MC-EM training procedure as a notable example application of the PL-MCMC technique.  The experiments involving the MC-EM training of normalizing flows are intended to offer some indication as to how effectively PL-MCMC samples from its intended distributions.  These experiments are not intended to advocate for our particular implementation of the MC-EM training procedure.\n\nTo address individual comments:\n\n* **How do the choices of auxiliary distribution, $q$, and transition proposal density, $g$, affect conditional sampling performance?**  \n  In the revised draft of our work, we have expanded Section 5.3 and Appendix E.3 to better explore the effect choices of $q$ and $g$ have on conditional sampling performance with PL-MCMC. \n    \n* **Does the implemented MC-EM training procedure maximize complete data likelihood?**\n\n  We assume the concern is whether the MC-EM training procedure maximizes complete data likelihood, in the sense of maximizing the likelihood of the complete ground truth data if training on an intentionally corrupted data set.  In this case, we point out that we do not believe the MC-EM training procedure should be used to train a normalizing flow from a complete data training set by intentionally removing values from the training data.  If a complete data training set is available, the normalizing flow should be trained to maximize the complete data likelihood directly.  For conditional sampling, PL-MCMC does not require the flow to have been trained in any particular manner to ensure asymptotically exact sampling from the flow's described distributions.  \n    \n    If this concern were instead whether our implemented MC-EM procedure maximizes observed data likelihood (the intended objective of expectation-maximization), we would agree that this may not be guaranteed.  We would point out that we made no claim of such a guarantee and that we do not intend to advocate for our particular implementation of the MC-EM procedure.  We simply use the comparative success of our MC-EM training to determine whether PL-MCMC can accurately sample from its intended conditional distributions.\n     \n* **How do you choose the perturbation scales for transition proposals?**\n\n  Because this work is intended to introduce and prove a novel technique, we only sought to demonstrate that there exist some choices of these transition proposals that enable efficient conditional sampling.  The scales presented within the work were therefore determined by some manual trial and error across some reasonable values.  We leave the development of a principled method for selecting these transition proposal scales to future work.\n     \n* **Can the natural scale of the target distribution differ across dimensions and affect approval rate?**\n\n  The fixed proposal density Metropolis-Hastings implementation PL-MCMC can absolutely be affected by the target distribution scaling unevenly across different dimensions.  This issue arises from the Metropolis-Hastings implementation, and not from PL-MCMC itself.  We leave the development and proof of more advanced and adaptive implementations of PL-MCMC to future work.\n     \n* **How does approval rate influence MC-EM training volatility?**\n\n  In our view, the approval rate reflects how well matched the proposal density is to the target distribution.  Markov Chains can have very low mixing rates (hence low volatility) while maintaining high acceptance rates.  We believe that volatility during MC-EM training (assuming the conditional sampling remains reasonably satisfactory) is derived from the properties of the normalizing flow's conditional distributions.  If the flow's conditional distribution admits many likely completions for missing values, then the training could be more volatile.\n     \n* **Why does the acceptance rate fall in Figure 7?**\n\n This drop is the result of our using a constant scale transition distribution.  Performance in this regard could be improved by the future development of adaptive implementations of PL-MCMC.\n     \n* **Why do you use an approximation for transition probability when including a resampling kernel?**\n\n  With our first implementation utilizing the exact transition densities, we ran into numerical issues when calculating the ratio of transition densities.  The approximation nicely avoided these numerical issues while introducing a negligible error for the transition distributions considered within our experiments."}, "signatures": ["ICLR.cc/2021/Conference/Paper2275/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2275/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Projected Latent Markov Chain Monte Carlo: Conditional Sampling of Normalizing Flows", "authorids": ["~Chris_Cannella1", "~Mohammadreza_Soltani1", "~Vahid_Tarokh1"], "authors": ["Chris Cannella", "Mohammadreza Soltani", "Vahid Tarokh"], "keywords": ["Conditional Sampling", "Normalizing Flows", "Markov Chain Monte Carlo", "Missing Data Inference"], "abstract": "We introduce Projected Latent Markov Chain Monte Carlo (PL-MCMC), a technique for sampling from the exact conditional distributions learned by normalizing flows. As a conditional sampling method, PL-MCMC enables Monte Carlo Expectation Maximization (MC-EM) training of normalizing flows from incomplete data. Through experimental tests applying normalizing flows to missing data tasks for a variety of data sets, we demonstrate the efficacy of PL-MCMC for conditional sampling from normalizing flows.", "one-sentence_summary": "We introduce and demonstrate a novel MCMC technique for sampling from the exact conditional distributions known by normalizing flows.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cannella|projected_latent_markov_chain_monte_carlo_conditional_sampling_of_normalizing_flows", "supplementary_material": "/attachment/7142f0503843ba6a0b7b8b11d57c8ef8da7e7880.zip", "pdf": "/pdf/32946e80b74b4bb7d6f25d74cb773ac68b9b4a36.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncannella2021projected,\ntitle={Projected Latent Markov Chain Monte Carlo: Conditional Sampling of Normalizing Flows},\nauthor={Chris Cannella and Mohammadreza Soltani and Vahid Tarokh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MBpHUFrcG2x}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MBpHUFrcG2x", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2275/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2275/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2275/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2275/Authors|ICLR.cc/2021/Conference/Paper2275/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2275/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850263, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2275/-/Official_Comment"}}}, {"id": "mB9pCKG5_kG", "original": null, "number": 4, "cdate": 1605441918612, "ddate": null, "tcdate": 1605441918612, "tmdate": 1605442091626, "tddate": null, "forum": "MBpHUFrcG2x", "replyto": "oWQ1rg9bfsJ", "invitation": "ICLR.cc/2021/Conference/Paper2275/-/Official_Comment", "content": {"title": "Great questions and suggestions.", "comment": "Thank you for your time and consideration.  \n\nFor each question in order:\n\n1. **What is meant by \"pre-trained\" model?**  \n  By \"pre-trained\" model, we mean a publicly available model that we have not trained ourselves.  One of the motivations for PL-MCMC is to enable efficient conditional sampling from all of the impressive normalizing flows that have recently been introduced.  It is not unreasonable to suppose that certain architectures or training methodologies could yield a normalizing flow that is better suited to conditional sampling using PL-MCMC.  We hope that our use of these \"pre-trained\" models provides a relatively faithful indication of the performance that can be expected of PL-MCMC to perform conditional inference with the normalizing flows that are already trained and in use today.\n\n2. **Can the auxiliary density, $q$, be omitted?**\n\n  We believe not using $q$ is equivalent to replacing $q$ with an improper uniform distribution.  Our proof of the implementation's convergence in Section 3.3 requires $q$ to be a proper and positive density.  Because convergence is guaranteed for all proper choices of $q$ in a limiting sequence towards the improper uniform distribution (e.g. $q = \\mathcal{N}(\\mathbf{x}_{O}; \\sigma I)$ as $\\sigma \\rightarrow \\infty$), we suspect that $q$ can be omitted without losing asymptotic convergence.  We have included this choice of $q$ in our revised expansion of Section 5.3.  We find that omitting $q$ still leads to improved sampling performance,due to PL-MCMC's use of the flow's latent space to produce effective transition proposals, but find that \"stronger\" choices of $q$ can provide even better sampling performance.\n3. **Is it fair to make the auxiliary distribution, $q$, strongly centered on the observed data?**\n\n  Selecting $q$ to be a multivariate normal distribution centered on the observed data values offers no additional information beyond what is already available within the context of conditional sampling given the observed values.  If analytic conditional sampling from normalizing flows were available to us, we would not consider it's use of the observed data values unfair and we simply call the process \"conditional sampling from the model described by the normalizing flow\".  Without a tractable analytical method, we are forced to use a Markov Chain technique, wherein the auxiliary distribution, $q$, encourages the Markov Chain to favor exploring latent states that rebuild the observed data.  \n    \n    Our current intuition regarding $q$ is that it represents some prior belief regarding the coupling between observed and missing data within the model described by the normalizing flow.  If, as is often the case, this prior belief is correct and the normalizing flow does exhibit tight coupling between missing and observed data, then choosing $q$ to be strongly centered on observed data will benefit conditional sampling performance with PL-MCMC.  If it happens that the prior belief associated with \"strong\" choice of $q$ is incorrect, the results are not disastrous, as asymptotic convergence is still guaranteed, but sampling performance could be worse than would be obtained with a \"softer\" choice of $q$.  When PL-MCMC exhibits good performance in the task of reconstructing missing data, its performance derives entirely from the computational effort originally expended in training an accurate normalizing flow with a convenient-to-explore latent space.\n    \n4. **What is the computational cost of PL-MCMC?**\n\n  For normalizing flows, the cost of evaluating joint likelihood is dominated by the cost of performing the transformation from the flow's modeled data space to its latent space.  For the popular normalizing flow architectures that we are aware of, this transformation is as costly as its reverse transformation.  PL-MCMC requires one transformation from  the latent space to the modeled data space and one transformation from the modeled data space to the latent space.  We estimate that a Metropolis-Hastings implementation of PL-MCMC is about twice as costly as any Metropolis-Hastings technique operating within the modeled data space of the flow.  The potential sampling performance increase provided by PL-MCMC can far outweigh this doubling of computational cost per Metropolis-Hastings proposal.  \n\n  We would strongly advise against dropping the correction term, as this removes the guarantee of asymptotic convergence while only giving a modest doubling of speed.\n    \n5. **On the fall of acceptance rate within Figure 7:**\n\n  This drop is  the result of our using a constant scale transition distribution.  Performance in this regard could be improved by the future development of adaptive implementations of PL-MCMC.  We investigate the effect of reducing proposal scale within our expanded analysis regarding the effect of $q$ and $g$ on conditional sampling performance in Section 5.3."}, "signatures": ["ICLR.cc/2021/Conference/Paper2275/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2275/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Projected Latent Markov Chain Monte Carlo: Conditional Sampling of Normalizing Flows", "authorids": ["~Chris_Cannella1", "~Mohammadreza_Soltani1", "~Vahid_Tarokh1"], "authors": ["Chris Cannella", "Mohammadreza Soltani", "Vahid Tarokh"], "keywords": ["Conditional Sampling", "Normalizing Flows", "Markov Chain Monte Carlo", "Missing Data Inference"], "abstract": "We introduce Projected Latent Markov Chain Monte Carlo (PL-MCMC), a technique for sampling from the exact conditional distributions learned by normalizing flows. As a conditional sampling method, PL-MCMC enables Monte Carlo Expectation Maximization (MC-EM) training of normalizing flows from incomplete data. Through experimental tests applying normalizing flows to missing data tasks for a variety of data sets, we demonstrate the efficacy of PL-MCMC for conditional sampling from normalizing flows.", "one-sentence_summary": "We introduce and demonstrate a novel MCMC technique for sampling from the exact conditional distributions known by normalizing flows.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cannella|projected_latent_markov_chain_monte_carlo_conditional_sampling_of_normalizing_flows", "supplementary_material": "/attachment/7142f0503843ba6a0b7b8b11d57c8ef8da7e7880.zip", "pdf": "/pdf/32946e80b74b4bb7d6f25d74cb773ac68b9b4a36.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncannella2021projected,\ntitle={Projected Latent Markov Chain Monte Carlo: Conditional Sampling of Normalizing Flows},\nauthor={Chris Cannella and Mohammadreza Soltani and Vahid Tarokh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MBpHUFrcG2x}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MBpHUFrcG2x", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2275/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2275/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2275/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2275/Authors|ICLR.cc/2021/Conference/Paper2275/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2275/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850263, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2275/-/Official_Comment"}}}, {"id": "y_YRoMR2zpN", "original": null, "number": 1, "cdate": 1603709896830, "ddate": null, "tcdate": 1603709896830, "tmdate": 1605024248677, "tddate": null, "forum": "MBpHUFrcG2x", "replyto": "MBpHUFrcG2x", "invitation": "ICLR.cc/2021/Conference/Paper2275/-/Official_Review", "content": {"title": "Official Review 4", "review": "Summary:\nThe submission introduces an approach for conditional sampling by running a Random walk Metropolis algorithm on the latent space as a pullback of normalizing flows. The normalizing flows are learnt via a MC-EM approach for incomplete training data. The method is illustrated on numerous experiments sampling missing parts for image and UCI datasets. \n\nPositives:\nThe combination of normalizing flows and MCMC sampling for conditional sampling is new as far as I am aware and is an interesting approach. Qualitative experimental results on different data sets seem promising with quantitative results indicating that it performs competitively (better reconstruction RMSE compared to MisGAN, closely matching the performance of MIWAE). More advanced MCMC samplers (such as gradient-based ones) could be used in the proposed framework, potentially yielding better experimental results.\n\nNegatives:\nThe approach relies on hyperparameters such as the choice of the auxiliary data density q and the MCMC proposal density g, and I feel that choices for these should be analyzed in greater detail in the paper. For example, the choice of both q and g affect the exploration of the target and so do they affect the sampling quality in practice for a reasonable number MCMC steps? Likewise, it is well documented that adaptive MCMC methods can converge to a different target and I am not yet convinced that the MC-EM approach approximately maximizes the complete training data likelihood without some restrictions on the MCMC adaptation strategy.\n\nRecommendation:\nI am recommending a weak accept, since the approach is interesting and is supported by many experiments, however, the hyperparameter choices for the adaptation should be better analyzed.\n\nComments:\nHow do you choose the perturbation scales for the random walk proposals?\nCould it be that the target has different scales across different dimensions, so that the MCMC kernel explores only some dimensions well and does this affect the learning process?\nDo the acceptance rates have a large influence on the MC-EM training? My intuition would be that for high acceptance rates, the learning process would be more volatile (the training samples X\u2019_train would have many same entries and some very different ones)?\nWhy do the acceptance rate go to zero in Figure 7?\nWhy do you make the approximation of the Metropolis-Hastings ratio in the experiments? Is the proposal not just a simple 2-mixture density which should not incur a much higher computational cost?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2275/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2275/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Projected Latent Markov Chain Monte Carlo: Conditional Sampling of Normalizing Flows", "authorids": ["~Chris_Cannella1", "~Mohammadreza_Soltani1", "~Vahid_Tarokh1"], "authors": ["Chris Cannella", "Mohammadreza Soltani", "Vahid Tarokh"], "keywords": ["Conditional Sampling", "Normalizing Flows", "Markov Chain Monte Carlo", "Missing Data Inference"], "abstract": "We introduce Projected Latent Markov Chain Monte Carlo (PL-MCMC), a technique for sampling from the exact conditional distributions learned by normalizing flows. As a conditional sampling method, PL-MCMC enables Monte Carlo Expectation Maximization (MC-EM) training of normalizing flows from incomplete data. Through experimental tests applying normalizing flows to missing data tasks for a variety of data sets, we demonstrate the efficacy of PL-MCMC for conditional sampling from normalizing flows.", "one-sentence_summary": "We introduce and demonstrate a novel MCMC technique for sampling from the exact conditional distributions known by normalizing flows.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cannella|projected_latent_markov_chain_monte_carlo_conditional_sampling_of_normalizing_flows", "supplementary_material": "/attachment/7142f0503843ba6a0b7b8b11d57c8ef8da7e7880.zip", "pdf": "/pdf/32946e80b74b4bb7d6f25d74cb773ac68b9b4a36.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncannella2021projected,\ntitle={Projected Latent Markov Chain Monte Carlo: Conditional Sampling of Normalizing Flows},\nauthor={Chris Cannella and Mohammadreza Soltani and Vahid Tarokh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MBpHUFrcG2x}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "MBpHUFrcG2x", "replyto": "MBpHUFrcG2x", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2275/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538100011, "tmdate": 1606915801656, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2275/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2275/-/Official_Review"}}}], "count": 8}