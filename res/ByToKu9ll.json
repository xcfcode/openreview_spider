{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396595983, "tcdate": 1486396595983, "number": 1, "id": "r1hnhf8_e", "invitation": "ICLR.cc/2017/conference/-/paper460/acceptance", "forum": "ByToKu9ll", "replyto": "ByToKu9ll", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The paper investigates several retraining approached based upon adversarial data. While the experimental evaluation looks reasonable, the actual contribution of this paper is quite small. The approaches being evaluated, for the most part, are already proposed in the literature, with the one exception being the \"improved autoencoder stacked with classifier\" (IAEC), which is really just a minor modification to the existing AEC approach with an additional regularization term. The results are fairly thorough, and seem to suggest that the IAEC method performs best in some cases, but this is definitely not a novel enough contribution to warrant publication at ICLR.\n \n Pros:\n + Nice empirical evaluation of several adversarial retraining methods\n \n Cons:\n - Extremely minor algorithmic advances\n - Not clear what is the significant contribution of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models", "abstract": "Due to deep cascades of nonlinear units, deep neural networks (DNNs) can automatically learn non-local generalization priors from data and have achieved high performance in various applications.\nHowever, such properties have also opened a door for adversaries to generate the so-called adversarial examples to fool DNNs. Specifically, adversaries can inject small perturbations to the input data and therefore decrease the performance of deep neural networks significantly.\nEven worse, these adversarial examples have the transferability to attack a black-box model based on finite queries without knowledge of the target model. \nTherefore, we aim to empirically compare different defensive strategies against various adversary models and analyze the cross-model efficiency for these robust learners. We conclude that the adversarial retraining framework also has the transferability, which can defend adversarial examples without requiring prior knowledge of the adversary models.\nWe compare the general adversarial retraining framework with the state-of-the-art robust deep neural networks, such as distillation, autoencoder stacked with classifier (AEC), and our improved version, IAEC, to evaluate their robustness as well as the vulnerability in terms of the distortion required to mislead the learner.\nOur experimental results show that the adversarial retraining framework can defend most of the adversarial examples notably and consistently without adding additional\nvulnerabilities or performance penalty to the original model.", "pdf": "/pdf/5ffc265e88c70d978a04bdfa570cb41fc25e89e8.pdf", "TL;DR": "robust adversarial retraining", "paperhash": "chen|evaluation_of_defensive_methods_for_dnns_against_multiple_adversarial_evasion_models", "keywords": ["Deep learning"], "authors": ["Xinyun Chen", "Bo Li", "Yevgeniy Vorobeychik"], "conflicts": ["umich.edu", "vanderbilt.edu", "sjtu.edu.cn"], "authorids": ["jungyhuk@gmail.com", "bbbli@umich.edu", "yevgeniy.vorobeychik@vanderbilt.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396596480, "id": "ICLR.cc/2017/conference/-/paper460/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "ByToKu9ll", "replyto": "ByToKu9ll", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396596480}}}, {"tddate": null, "tmdate": 1484545723932, "tcdate": 1484545723932, "number": 8, "id": "By46A0FIx", "invitation": "ICLR.cc/2017/conference/-/paper460/public/comment", "forum": "ByToKu9ll", "replyto": "ByToKu9ll", "signatures": ["~Xinyun_Chen1"], "readers": ["everyone"], "writers": ["~Xinyun_Chen1"], "content": {"title": "Paper revision", "comment": "We thank all reviewers and readers for the helpful comments! We have updated the paper with the following changes:\n\n1) Provide more details about the experimental settings in section 4.1.\n2) Add a section \"Related Work\" (section 2) and move the detailed discussion of related works in Introduction to this section, so that the Introduction focuses more on our contributions.\n3) Update the Introduction section to highlight the differences between our work and previous works, and emphasize our contributions. Also, we provide more explanation of the term \"cross-model\" in Introduction section (at the top of page 2) to avoid confusion.\n4) Add the discussion of following related works: Huang et al. (https://arxiv.org/abs/1511.03034), Kurakin et al. (https://arxiv.org/abs/1611.01236), and adversarial training proposed in Goodfellow et al. (https://arxiv.org/abs/1412.6572).\n\nNew comments are welcome!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models", "abstract": "Due to deep cascades of nonlinear units, deep neural networks (DNNs) can automatically learn non-local generalization priors from data and have achieved high performance in various applications.\nHowever, such properties have also opened a door for adversaries to generate the so-called adversarial examples to fool DNNs. Specifically, adversaries can inject small perturbations to the input data and therefore decrease the performance of deep neural networks significantly.\nEven worse, these adversarial examples have the transferability to attack a black-box model based on finite queries without knowledge of the target model. \nTherefore, we aim to empirically compare different defensive strategies against various adversary models and analyze the cross-model efficiency for these robust learners. We conclude that the adversarial retraining framework also has the transferability, which can defend adversarial examples without requiring prior knowledge of the adversary models.\nWe compare the general adversarial retraining framework with the state-of-the-art robust deep neural networks, such as distillation, autoencoder stacked with classifier (AEC), and our improved version, IAEC, to evaluate their robustness as well as the vulnerability in terms of the distortion required to mislead the learner.\nOur experimental results show that the adversarial retraining framework can defend most of the adversarial examples notably and consistently without adding additional\nvulnerabilities or performance penalty to the original model.", "pdf": "/pdf/5ffc265e88c70d978a04bdfa570cb41fc25e89e8.pdf", "TL;DR": "robust adversarial retraining", "paperhash": "chen|evaluation_of_defensive_methods_for_dnns_against_multiple_adversarial_evasion_models", "keywords": ["Deep learning"], "authors": ["Xinyun Chen", "Bo Li", "Yevgeniy Vorobeychik"], "conflicts": ["umich.edu", "vanderbilt.edu", "sjtu.edu.cn"], "authorids": ["jungyhuk@gmail.com", "bbbli@umich.edu", "yevgeniy.vorobeychik@vanderbilt.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287567963, "id": "ICLR.cc/2017/conference/-/paper460/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByToKu9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper460/reviewers", "ICLR.cc/2017/conference/paper460/areachairs"], "cdate": 1485287567963}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1483770301760, "tcdate": 1478293926613, "number": 460, "id": "ByToKu9ll", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "ByToKu9ll", "signatures": ["~Bo_Li2"], "readers": ["everyone"], "content": {"title": "Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models", "abstract": "Due to deep cascades of nonlinear units, deep neural networks (DNNs) can automatically learn non-local generalization priors from data and have achieved high performance in various applications.\nHowever, such properties have also opened a door for adversaries to generate the so-called adversarial examples to fool DNNs. Specifically, adversaries can inject small perturbations to the input data and therefore decrease the performance of deep neural networks significantly.\nEven worse, these adversarial examples have the transferability to attack a black-box model based on finite queries without knowledge of the target model. \nTherefore, we aim to empirically compare different defensive strategies against various adversary models and analyze the cross-model efficiency for these robust learners. We conclude that the adversarial retraining framework also has the transferability, which can defend adversarial examples without requiring prior knowledge of the adversary models.\nWe compare the general adversarial retraining framework with the state-of-the-art robust deep neural networks, such as distillation, autoencoder stacked with classifier (AEC), and our improved version, IAEC, to evaluate their robustness as well as the vulnerability in terms of the distortion required to mislead the learner.\nOur experimental results show that the adversarial retraining framework can defend most of the adversarial examples notably and consistently without adding additional\nvulnerabilities or performance penalty to the original model.", "pdf": "/pdf/5ffc265e88c70d978a04bdfa570cb41fc25e89e8.pdf", "TL;DR": "robust adversarial retraining", "paperhash": "chen|evaluation_of_defensive_methods_for_dnns_against_multiple_adversarial_evasion_models", "keywords": ["Deep learning"], "authors": ["Xinyun Chen", "Bo Li", "Yevgeniy Vorobeychik"], "conflicts": ["umich.edu", "vanderbilt.edu", "sjtu.edu.cn"], "authorids": ["jungyhuk@gmail.com", "bbbli@umich.edu", "yevgeniy.vorobeychik@vanderbilt.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1483027039804, "tcdate": 1483027039804, "number": 7, "id": "rkdDG2fSx", "invitation": "ICLR.cc/2017/conference/-/paper460/public/comment", "forum": "ByToKu9ll", "replyto": "SJMQWxG4g", "signatures": ["~Xinyun_Chen1"], "readers": ["everyone"], "writers": ["~Xinyun_Chen1"], "content": {"title": "Response and revision plan", "comment": "Thank you for the review and suggestions!\n\nAs for attack and defense methods evaluated in our paper, we are not intended to cover all proposed methods in our experiments; instead, we select several representative methods. We will add more discussion about different proposed methods and provide more comparisons in our revision. In particular, as for the references you mentioned, Miyato et al. ([1]) is already mentioned in paragraph 2 of the Introduction in our paper, and we will add the discussion of Huang et al. ([2]).\n\nWe will provide more experimental details for the results. Meanwhile, we will polish the paper to provide further analysis of experimental results and highlight our contributions.\n\nThe improved version of AEC actually performs much better than AEC (see Table 1). Our goal of proposing IAEC is to provide the strongest version of this strategy for later comparisons with other defense methods.\n\nWe will address the minor comment in our revision.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models", "abstract": "Due to deep cascades of nonlinear units, deep neural networks (DNNs) can automatically learn non-local generalization priors from data and have achieved high performance in various applications.\nHowever, such properties have also opened a door for adversaries to generate the so-called adversarial examples to fool DNNs. Specifically, adversaries can inject small perturbations to the input data and therefore decrease the performance of deep neural networks significantly.\nEven worse, these adversarial examples have the transferability to attack a black-box model based on finite queries without knowledge of the target model. \nTherefore, we aim to empirically compare different defensive strategies against various adversary models and analyze the cross-model efficiency for these robust learners. We conclude that the adversarial retraining framework also has the transferability, which can defend adversarial examples without requiring prior knowledge of the adversary models.\nWe compare the general adversarial retraining framework with the state-of-the-art robust deep neural networks, such as distillation, autoencoder stacked with classifier (AEC), and our improved version, IAEC, to evaluate their robustness as well as the vulnerability in terms of the distortion required to mislead the learner.\nOur experimental results show that the adversarial retraining framework can defend most of the adversarial examples notably and consistently without adding additional\nvulnerabilities or performance penalty to the original model.", "pdf": "/pdf/5ffc265e88c70d978a04bdfa570cb41fc25e89e8.pdf", "TL;DR": "robust adversarial retraining", "paperhash": "chen|evaluation_of_defensive_methods_for_dnns_against_multiple_adversarial_evasion_models", "keywords": ["Deep learning"], "authors": ["Xinyun Chen", "Bo Li", "Yevgeniy Vorobeychik"], "conflicts": ["umich.edu", "vanderbilt.edu", "sjtu.edu.cn"], "authorids": ["jungyhuk@gmail.com", "bbbli@umich.edu", "yevgeniy.vorobeychik@vanderbilt.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287567963, "id": "ICLR.cc/2017/conference/-/paper460/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByToKu9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper460/reviewers", "ICLR.cc/2017/conference/paper460/areachairs"], "cdate": 1485287567963}}}, {"tddate": null, "tmdate": 1483026956113, "tcdate": 1483026956113, "number": 6, "id": "Hk4Mz3MHe", "invitation": "ICLR.cc/2017/conference/-/paper460/public/comment", "forum": "ByToKu9ll", "replyto": "HJGeihxEl", "signatures": ["~Xinyun_Chen1"], "readers": ["everyone"], "writers": ["~Xinyun_Chen1"], "content": {"title": "Response and revision plan", "comment": "Thank you for your review and suggestions!\n\nThe \u201cAdversarial retraining framework\u201d has been proposed but no work has been done to evaluate its efficiency in general or provide insights about its robustness and weaknesses. Therefore, to fill up this gap, we propose this comparison work to offer a general reference for how and when to use adversarial retraining framework and what to expect.\nOur contributions here are trying to evaluate the retraining framework on adversarial examples generated using different attack methods. The evaluation of cross-model efficiency (Section 3.2) and additional attacks (Section 3.3) are also our contributions. We will revise our paper to make our points clearer.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models", "abstract": "Due to deep cascades of nonlinear units, deep neural networks (DNNs) can automatically learn non-local generalization priors from data and have achieved high performance in various applications.\nHowever, such properties have also opened a door for adversaries to generate the so-called adversarial examples to fool DNNs. Specifically, adversaries can inject small perturbations to the input data and therefore decrease the performance of deep neural networks significantly.\nEven worse, these adversarial examples have the transferability to attack a black-box model based on finite queries without knowledge of the target model. \nTherefore, we aim to empirically compare different defensive strategies against various adversary models and analyze the cross-model efficiency for these robust learners. We conclude that the adversarial retraining framework also has the transferability, which can defend adversarial examples without requiring prior knowledge of the adversary models.\nWe compare the general adversarial retraining framework with the state-of-the-art robust deep neural networks, such as distillation, autoencoder stacked with classifier (AEC), and our improved version, IAEC, to evaluate their robustness as well as the vulnerability in terms of the distortion required to mislead the learner.\nOur experimental results show that the adversarial retraining framework can defend most of the adversarial examples notably and consistently without adding additional\nvulnerabilities or performance penalty to the original model.", "pdf": "/pdf/5ffc265e88c70d978a04bdfa570cb41fc25e89e8.pdf", "TL;DR": "robust adversarial retraining", "paperhash": "chen|evaluation_of_defensive_methods_for_dnns_against_multiple_adversarial_evasion_models", "keywords": ["Deep learning"], "authors": ["Xinyun Chen", "Bo Li", "Yevgeniy Vorobeychik"], "conflicts": ["umich.edu", "vanderbilt.edu", "sjtu.edu.cn"], "authorids": ["jungyhuk@gmail.com", "bbbli@umich.edu", "yevgeniy.vorobeychik@vanderbilt.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287567963, "id": "ICLR.cc/2017/conference/-/paper460/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByToKu9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper460/reviewers", "ICLR.cc/2017/conference/paper460/areachairs"], "cdate": 1485287567963}}}, {"tddate": null, "tmdate": 1483026821782, "tcdate": 1483026821782, "number": 5, "id": "BJCYZnfBx", "invitation": "ICLR.cc/2017/conference/-/paper460/public/comment", "forum": "ByToKu9ll", "replyto": "Byu9da2Ql", "signatures": ["~Xinyun_Chen1"], "readers": ["everyone"], "writers": ["~Xinyun_Chen1"], "content": {"title": "Response and revision plan", "comment": "Thank you for your review and suggestions!\n\nWe will add more details about the experimental settings. \nThe \u201ccross-model\u201d here means the first point as you mentioned, i.e., add instances crafted using one adversarial example generation algorithm and then evaluate adversarial images derived by another.\nActually, we aim to evaluate the ability of defender here to check if the defender can defend against various adversarial models if he applies a different algorithm to generate the retraining instances and improve the classifier. This provides information about how retraining framework can generalize across different adversarial models.\nHowever, the second point mentioned by the reviewer is from the attacker\u2019s perspective and hope to check if the attacker can generalize their attacks against different defensive models. This is one interesting point but not the scope of this paper here, and in fact such work on transferability of attacks have been discussed in Introduction as reference [Papernot et al., 2016a;b].\n\nWe will also add the discussion of work [1]. We didn\u2019t discuss work [2] when we wrote the paper, since it is another submission for this conference at the same time. Also, work [2] aims to solve the scalability issue of adversarial training, which is not the main topic of our paper. We aim to test the efficacy of different adversarial example generation algorithms here, but we are happy to discuss this work in our revision.\n\nThe \u201crecall\u201d here means the true positive rate. Since we focus on the binary classification problem and only allow the malicious (positive) instances to evade the classifier as benign (negative) ones, we want to check how many of these modified instances are correctly classified after retraining. Meanwhile, we have reported the classification error for all the experimental comparisons as suggested, the classification errors corresponding to experimental settings of Figure 1 and Figure 2 are reported in Table 1 and Table 2 respectively.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models", "abstract": "Due to deep cascades of nonlinear units, deep neural networks (DNNs) can automatically learn non-local generalization priors from data and have achieved high performance in various applications.\nHowever, such properties have also opened a door for adversaries to generate the so-called adversarial examples to fool DNNs. Specifically, adversaries can inject small perturbations to the input data and therefore decrease the performance of deep neural networks significantly.\nEven worse, these adversarial examples have the transferability to attack a black-box model based on finite queries without knowledge of the target model. \nTherefore, we aim to empirically compare different defensive strategies against various adversary models and analyze the cross-model efficiency for these robust learners. We conclude that the adversarial retraining framework also has the transferability, which can defend adversarial examples without requiring prior knowledge of the adversary models.\nWe compare the general adversarial retraining framework with the state-of-the-art robust deep neural networks, such as distillation, autoencoder stacked with classifier (AEC), and our improved version, IAEC, to evaluate their robustness as well as the vulnerability in terms of the distortion required to mislead the learner.\nOur experimental results show that the adversarial retraining framework can defend most of the adversarial examples notably and consistently without adding additional\nvulnerabilities or performance penalty to the original model.", "pdf": "/pdf/5ffc265e88c70d978a04bdfa570cb41fc25e89e8.pdf", "TL;DR": "robust adversarial retraining", "paperhash": "chen|evaluation_of_defensive_methods_for_dnns_against_multiple_adversarial_evasion_models", "keywords": ["Deep learning"], "authors": ["Xinyun Chen", "Bo Li", "Yevgeniy Vorobeychik"], "conflicts": ["umich.edu", "vanderbilt.edu", "sjtu.edu.cn"], "authorids": ["jungyhuk@gmail.com", "bbbli@umich.edu", "yevgeniy.vorobeychik@vanderbilt.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287567963, "id": "ICLR.cc/2017/conference/-/paper460/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByToKu9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper460/reviewers", "ICLR.cc/2017/conference/paper460/areachairs"], "cdate": 1485287567963}}}, {"tddate": null, "tmdate": 1481929015313, "tcdate": 1481928985610, "number": 3, "id": "SJMQWxG4g", "invitation": "ICLR.cc/2017/conference/-/paper460/official/review", "forum": "ByToKu9ll", "replyto": "ByToKu9ll", "signatures": ["ICLR.cc/2017/conference/paper460/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper460/AnonReviewer1"], "content": {"title": "Systematic experimental setting, but lack of clarity and originality", "rating": "5: Marginally below acceptance threshold", "review": "This paper performs a series of experiments to systematically evaluate the robustness of several defense methods, including RAD, AEC and its improved version etc.. It provides interesting observations. Overall, RAD and distillation have the best performances, but none of the methods can really resist the 'additional' attack from cg or adam. Since it is an experimental paper, my main concern is about its clarity. See the comments below for details.\n\nPros:\n1. This paper provides a good comparison of the performances for the selected methods.\n2. Section 3.3 (the 'additional' attack) is a interesting investigation. Although the final result about the defense methods is negative, its results are still inspiring. \n3. Overall, this paper provides interesting and inspiring experimental results about the selected methods.\n\nCons:\n1. There are several other methods in the literature that are missing from the paper. For example the defense methods and the attack methods in the papers [1,2]. \n2. Although a long list of experimental results are provided in the paper, many details are skipped. For example, details of the experiments that generate the results in Table 5. \n3. Without further explanations and analyses about the experimental results, the contribution of the paper seems limited. \n4. This paper proposed an improved version of the AEC algorithm. But its experimental results seems not promising. \n\nMinor comments:\nPage 3: Equation (3) is also non-convex. So the non-convexity of Equation (2) should not be the motivation of Equation (3).\n\n[1] https://arxiv.org/abs/1507.00677\n[2] https://arxiv.org/abs/1511.03034", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models", "abstract": "Due to deep cascades of nonlinear units, deep neural networks (DNNs) can automatically learn non-local generalization priors from data and have achieved high performance in various applications.\nHowever, such properties have also opened a door for adversaries to generate the so-called adversarial examples to fool DNNs. Specifically, adversaries can inject small perturbations to the input data and therefore decrease the performance of deep neural networks significantly.\nEven worse, these adversarial examples have the transferability to attack a black-box model based on finite queries without knowledge of the target model. \nTherefore, we aim to empirically compare different defensive strategies against various adversary models and analyze the cross-model efficiency for these robust learners. We conclude that the adversarial retraining framework also has the transferability, which can defend adversarial examples without requiring prior knowledge of the adversary models.\nWe compare the general adversarial retraining framework with the state-of-the-art robust deep neural networks, such as distillation, autoencoder stacked with classifier (AEC), and our improved version, IAEC, to evaluate their robustness as well as the vulnerability in terms of the distortion required to mislead the learner.\nOur experimental results show that the adversarial retraining framework can defend most of the adversarial examples notably and consistently without adding additional\nvulnerabilities or performance penalty to the original model.", "pdf": "/pdf/5ffc265e88c70d978a04bdfa570cb41fc25e89e8.pdf", "TL;DR": "robust adversarial retraining", "paperhash": "chen|evaluation_of_defensive_methods_for_dnns_against_multiple_adversarial_evasion_models", "keywords": ["Deep learning"], "authors": ["Xinyun Chen", "Bo Li", "Yevgeniy Vorobeychik"], "conflicts": ["umich.edu", "vanderbilt.edu", "sjtu.edu.cn"], "authorids": ["jungyhuk@gmail.com", "bbbli@umich.edu", "yevgeniy.vorobeychik@vanderbilt.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512578169, "id": "ICLR.cc/2017/conference/-/paper460/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper460/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper460/AnonReviewer2", "ICLR.cc/2017/conference/paper460/AnonReviewer3", "ICLR.cc/2017/conference/paper460/AnonReviewer1"], "reply": {"forum": "ByToKu9ll", "replyto": "ByToKu9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper460/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper460/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512578169}}}, {"tddate": null, "tmdate": 1481849601948, "tcdate": 1481849577983, "number": 2, "id": "HJGeihxEl", "invitation": "ICLR.cc/2017/conference/-/paper460/official/review", "forum": "ByToKu9ll", "replyto": "ByToKu9ll", "signatures": ["ICLR.cc/2017/conference/paper460/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper460/AnonReviewer3"], "content": {"title": "Interesting comparisons, not very original.", "rating": "5: Marginally below acceptance threshold", "review": "The paper compares several defense mechanisms against adversarial attacks: retraining, two kinds of autoencoders and distillation with the conclusion that the retraining methodology proposed by Li et al. works best of those approaches.\n\nThe paper documents a series of experiments on making models robust against adversarial examples. The methods proposed here are not all too original, RAD was proposed by Li et al, distillation was proposed in Goodfellow et al's \"Explaining and harnessing adversarial examples\", stacked autoencoders were proposed by Szegedy et al's \"Intriguing Properties of Neural Networks\". The most original part of the paper is the improved version of autoencoders proposed in this paper.\n\nThe paper establishes experimental evidence that the RAD framework provides the best defense mechanism against adversarial attacks which makes the introduction of the improved autoencoder mechanism less appealing.\n\nAlthough the paper establishes interesting measurement points and therefore it has the potential for being cited as a reference, its relative lack of originality decreases its significance.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models", "abstract": "Due to deep cascades of nonlinear units, deep neural networks (DNNs) can automatically learn non-local generalization priors from data and have achieved high performance in various applications.\nHowever, such properties have also opened a door for adversaries to generate the so-called adversarial examples to fool DNNs. Specifically, adversaries can inject small perturbations to the input data and therefore decrease the performance of deep neural networks significantly.\nEven worse, these adversarial examples have the transferability to attack a black-box model based on finite queries without knowledge of the target model. \nTherefore, we aim to empirically compare different defensive strategies against various adversary models and analyze the cross-model efficiency for these robust learners. We conclude that the adversarial retraining framework also has the transferability, which can defend adversarial examples without requiring prior knowledge of the adversary models.\nWe compare the general adversarial retraining framework with the state-of-the-art robust deep neural networks, such as distillation, autoencoder stacked with classifier (AEC), and our improved version, IAEC, to evaluate their robustness as well as the vulnerability in terms of the distortion required to mislead the learner.\nOur experimental results show that the adversarial retraining framework can defend most of the adversarial examples notably and consistently without adding additional\nvulnerabilities or performance penalty to the original model.", "pdf": "/pdf/5ffc265e88c70d978a04bdfa570cb41fc25e89e8.pdf", "TL;DR": "robust adversarial retraining", "paperhash": "chen|evaluation_of_defensive_methods_for_dnns_against_multiple_adversarial_evasion_models", "keywords": ["Deep learning"], "authors": ["Xinyun Chen", "Bo Li", "Yevgeniy Vorobeychik"], "conflicts": ["umich.edu", "vanderbilt.edu", "sjtu.edu.cn"], "authorids": ["jungyhuk@gmail.com", "bbbli@umich.edu", "yevgeniy.vorobeychik@vanderbilt.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512578169, "id": "ICLR.cc/2017/conference/-/paper460/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper460/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper460/AnonReviewer2", "ICLR.cc/2017/conference/paper460/AnonReviewer3", "ICLR.cc/2017/conference/paper460/AnonReviewer1"], "reply": {"forum": "ByToKu9ll", "replyto": "ByToKu9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper460/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper460/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512578169}}}, {"tddate": null, "tmdate": 1481590927901, "tcdate": 1481590927891, "number": 1, "id": "Byu9da2Ql", "invitation": "ICLR.cc/2017/conference/-/paper460/official/review", "forum": "ByToKu9ll", "replyto": "ByToKu9ll", "signatures": ["ICLR.cc/2017/conference/paper460/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper460/AnonReviewer2"], "content": {"title": "Official review of submission", "rating": "4: Ok but not good enough - rejection", "review": "I reviewed the manuscript as of December 6th.\n\nThe authors perform a systematic investigation of various retraining methods for making a classification network robust to adversarial examples. The authors achieve lower error rates using their RAD and IAEC methods perform better then previously introduced distillation methods for retraining networks to be robust to adversarial examples. This method suggests a promising direction for building a defense for adversarial examples.\n\nMajor Comments:\nI find the paper to not be lacking in exposition and clarity. The paper has a laundry list of related results (page 2) but no clear message. I *think* one larger point is the superior performance of their retraining techniques but it is not clear how well these techniques perform compared to other retraining techniques, nor are the details of the retraining techniques clear. The paper requires more discussion and a clear exposition about the methods the authors introduced (i.e. RAD, IAEC). What follow are some more detailed comments along this theme of improving the exposition and clarity:\n\n- The authors should provide more details about how they constructed the auto-encoder in the IAEC method (diagram?). The same needs to be said for the RAD method. The authors point to a previous workshop submission (https://arxiv.org/abs/1604.02606) but the authors need more discussion about what this method entails and how it compares to other retraining methods (e.g. [1,2]) since this is the first peer-review of this work.\n\n- Section 3.2 indicates that the authors are concerned with 'cross-model' efficiency but it is not clear from the text what this means. Are the author exploring the phenomenon of retraining off one algorithm and then evaluating adversarial images derived on another? Or, are the authors examining how examples derived from one instance of a trained model may fool or trick a second instance of a model? The latter point is quite important because this points towards examples and retraining procedures that can generalize across the class of all models.\n\n- How do RAD compare with basic retraining methods described in [1, 2]? Since the main contribution of this paper seems to be evaluating the efficacy of RAD, AEC and IAEC, I would suggest that the authors provide more discussion and exposition.\n\n- Why are the authors measuring 'recall' (https://en.wikipedia.org/wiki/Precision_and_recall) in Section 3.1? What does recall mean in this context? I would expect the authors to measure something more like the error rate of the classifier after employing the retraining procedure. This needs to be clarified in the manuscript.\n\n[1] https://arxiv.org/abs/1412.6572\n[2] https://arxiv.org/abs/1611.01236", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models", "abstract": "Due to deep cascades of nonlinear units, deep neural networks (DNNs) can automatically learn non-local generalization priors from data and have achieved high performance in various applications.\nHowever, such properties have also opened a door for adversaries to generate the so-called adversarial examples to fool DNNs. Specifically, adversaries can inject small perturbations to the input data and therefore decrease the performance of deep neural networks significantly.\nEven worse, these adversarial examples have the transferability to attack a black-box model based on finite queries without knowledge of the target model. \nTherefore, we aim to empirically compare different defensive strategies against various adversary models and analyze the cross-model efficiency for these robust learners. We conclude that the adversarial retraining framework also has the transferability, which can defend adversarial examples without requiring prior knowledge of the adversary models.\nWe compare the general adversarial retraining framework with the state-of-the-art robust deep neural networks, such as distillation, autoencoder stacked with classifier (AEC), and our improved version, IAEC, to evaluate their robustness as well as the vulnerability in terms of the distortion required to mislead the learner.\nOur experimental results show that the adversarial retraining framework can defend most of the adversarial examples notably and consistently without adding additional\nvulnerabilities or performance penalty to the original model.", "pdf": "/pdf/5ffc265e88c70d978a04bdfa570cb41fc25e89e8.pdf", "TL;DR": "robust adversarial retraining", "paperhash": "chen|evaluation_of_defensive_methods_for_dnns_against_multiple_adversarial_evasion_models", "keywords": ["Deep learning"], "authors": ["Xinyun Chen", "Bo Li", "Yevgeniy Vorobeychik"], "conflicts": ["umich.edu", "vanderbilt.edu", "sjtu.edu.cn"], "authorids": ["jungyhuk@gmail.com", "bbbli@umich.edu", "yevgeniy.vorobeychik@vanderbilt.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512578169, "id": "ICLR.cc/2017/conference/-/paper460/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper460/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper460/AnonReviewer2", "ICLR.cc/2017/conference/paper460/AnonReviewer3", "ICLR.cc/2017/conference/paper460/AnonReviewer1"], "reply": {"forum": "ByToKu9ll", "replyto": "ByToKu9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper460/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper460/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512578169}}}, {"tddate": null, "tmdate": 1481236551155, "tcdate": 1481236551148, "number": 4, "id": "SkyIxPvQg", "invitation": "ICLR.cc/2017/conference/-/paper460/public/comment", "forum": "ByToKu9ll", "replyto": "SyugANVQe", "signatures": ["~Bo_Li2"], "readers": ["everyone"], "writers": ["~Bo_Li2"], "content": {"title": "Re:Answer to \"About the experiments of cg and adam\"", "comment": "Thank you for your question!\nIn Table 5, the perturbations for LeNet-5 is the noise needed in the training of the original robust network. The perturbations for different defensive algorithms from row 2-7 are the perturbations of cg and adam in the additional attack.\nYes, all pixel values for all the experiments are in the range of [-0.5, 0.5]."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models", "abstract": "Due to deep cascades of nonlinear units, deep neural networks (DNNs) can automatically learn non-local generalization priors from data and have achieved high performance in various applications.\nHowever, such properties have also opened a door for adversaries to generate the so-called adversarial examples to fool DNNs. Specifically, adversaries can inject small perturbations to the input data and therefore decrease the performance of deep neural networks significantly.\nEven worse, these adversarial examples have the transferability to attack a black-box model based on finite queries without knowledge of the target model. \nTherefore, we aim to empirically compare different defensive strategies against various adversary models and analyze the cross-model efficiency for these robust learners. We conclude that the adversarial retraining framework also has the transferability, which can defend adversarial examples without requiring prior knowledge of the adversary models.\nWe compare the general adversarial retraining framework with the state-of-the-art robust deep neural networks, such as distillation, autoencoder stacked with classifier (AEC), and our improved version, IAEC, to evaluate their robustness as well as the vulnerability in terms of the distortion required to mislead the learner.\nOur experimental results show that the adversarial retraining framework can defend most of the adversarial examples notably and consistently without adding additional\nvulnerabilities or performance penalty to the original model.", "pdf": "/pdf/5ffc265e88c70d978a04bdfa570cb41fc25e89e8.pdf", "TL;DR": "robust adversarial retraining", "paperhash": "chen|evaluation_of_defensive_methods_for_dnns_against_multiple_adversarial_evasion_models", "keywords": ["Deep learning"], "authors": ["Xinyun Chen", "Bo Li", "Yevgeniy Vorobeychik"], "conflicts": ["umich.edu", "vanderbilt.edu", "sjtu.edu.cn"], "authorids": ["jungyhuk@gmail.com", "bbbli@umich.edu", "yevgeniy.vorobeychik@vanderbilt.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287567963, "id": "ICLR.cc/2017/conference/-/paper460/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByToKu9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper460/reviewers", "ICLR.cc/2017/conference/paper460/areachairs"], "cdate": 1485287567963}}}, {"tddate": null, "tmdate": 1481031152010, "tcdate": 1481031152005, "number": 1, "id": "SyugANVQe", "invitation": "ICLR.cc/2017/conference/-/paper460/official/comment", "forum": "ByToKu9ll", "replyto": "HyK2gkWQg", "signatures": ["ICLR.cc/2017/conference/paper460/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper460/AnonReviewer1"], "content": {"title": "Re: Answer to \"About the experiments of cg and adam\"", "comment": "In Table 3, how is the size of the perturbation for cg and adam in the additional attack, compared to the size in the training of the robustified network? Sorry for the previous vague question.\n\nAlso, all pixel values for all the experiments are in the range of [-0.5, 0.5]?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models", "abstract": "Due to deep cascades of nonlinear units, deep neural networks (DNNs) can automatically learn non-local generalization priors from data and have achieved high performance in various applications.\nHowever, such properties have also opened a door for adversaries to generate the so-called adversarial examples to fool DNNs. Specifically, adversaries can inject small perturbations to the input data and therefore decrease the performance of deep neural networks significantly.\nEven worse, these adversarial examples have the transferability to attack a black-box model based on finite queries without knowledge of the target model. \nTherefore, we aim to empirically compare different defensive strategies against various adversary models and analyze the cross-model efficiency for these robust learners. We conclude that the adversarial retraining framework also has the transferability, which can defend adversarial examples without requiring prior knowledge of the adversary models.\nWe compare the general adversarial retraining framework with the state-of-the-art robust deep neural networks, such as distillation, autoencoder stacked with classifier (AEC), and our improved version, IAEC, to evaluate their robustness as well as the vulnerability in terms of the distortion required to mislead the learner.\nOur experimental results show that the adversarial retraining framework can defend most of the adversarial examples notably and consistently without adding additional\nvulnerabilities or performance penalty to the original model.", "pdf": "/pdf/5ffc265e88c70d978a04bdfa570cb41fc25e89e8.pdf", "TL;DR": "robust adversarial retraining", "paperhash": "chen|evaluation_of_defensive_methods_for_dnns_against_multiple_adversarial_evasion_models", "keywords": ["Deep learning"], "authors": ["Xinyun Chen", "Bo Li", "Yevgeniy Vorobeychik"], "conflicts": ["umich.edu", "vanderbilt.edu", "sjtu.edu.cn"], "authorids": ["jungyhuk@gmail.com", "bbbli@umich.edu", "yevgeniy.vorobeychik@vanderbilt.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287567838, "id": "ICLR.cc/2017/conference/-/paper460/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "ByToKu9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper460/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper460/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper460/reviewers", "ICLR.cc/2017/conference/paper460/areachairs"], "cdate": 1485287567838}}}, {"tddate": null, "tmdate": 1480815424292, "tcdate": 1480815424286, "number": 3, "id": "SkurmeZQg", "invitation": "ICLR.cc/2017/conference/-/paper460/public/comment", "forum": "ByToKu9ll", "replyto": "rkabn6kXg", "signatures": ["~Bo_Li2"], "readers": ["everyone"], "writers": ["~Bo_Li2"], "content": {"title": "Answer to \"The definition of distortion\"", "comment": "Thank you for your question!\nn is the the number of pixels within one image. We are trying to calculating the distortion per pixel per channel. So inside the root squared sign we calculate the sum of distortions for all the pixels and then divide it by n to get the average distortion."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models", "abstract": "Due to deep cascades of nonlinear units, deep neural networks (DNNs) can automatically learn non-local generalization priors from data and have achieved high performance in various applications.\nHowever, such properties have also opened a door for adversaries to generate the so-called adversarial examples to fool DNNs. Specifically, adversaries can inject small perturbations to the input data and therefore decrease the performance of deep neural networks significantly.\nEven worse, these adversarial examples have the transferability to attack a black-box model based on finite queries without knowledge of the target model. \nTherefore, we aim to empirically compare different defensive strategies against various adversary models and analyze the cross-model efficiency for these robust learners. We conclude that the adversarial retraining framework also has the transferability, which can defend adversarial examples without requiring prior knowledge of the adversary models.\nWe compare the general adversarial retraining framework with the state-of-the-art robust deep neural networks, such as distillation, autoencoder stacked with classifier (AEC), and our improved version, IAEC, to evaluate their robustness as well as the vulnerability in terms of the distortion required to mislead the learner.\nOur experimental results show that the adversarial retraining framework can defend most of the adversarial examples notably and consistently without adding additional\nvulnerabilities or performance penalty to the original model.", "pdf": "/pdf/5ffc265e88c70d978a04bdfa570cb41fc25e89e8.pdf", "TL;DR": "robust adversarial retraining", "paperhash": "chen|evaluation_of_defensive_methods_for_dnns_against_multiple_adversarial_evasion_models", "keywords": ["Deep learning"], "authors": ["Xinyun Chen", "Bo Li", "Yevgeniy Vorobeychik"], "conflicts": ["umich.edu", "vanderbilt.edu", "sjtu.edu.cn"], "authorids": ["jungyhuk@gmail.com", "bbbli@umich.edu", "yevgeniy.vorobeychik@vanderbilt.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287567963, "id": "ICLR.cc/2017/conference/-/paper460/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByToKu9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper460/reviewers", "ICLR.cc/2017/conference/paper460/areachairs"], "cdate": 1485287567963}}}, {"tddate": null, "tmdate": 1480810672997, "tcdate": 1480810672991, "number": 2, "id": "HyK2gkWQg", "invitation": "ICLR.cc/2017/conference/-/paper460/public/comment", "forum": "ByToKu9ll", "replyto": "Sy6Qx_1Ql", "signatures": ["~Bo_Li2"], "readers": ["everyone"], "writers": ["~Bo_Li2"], "content": {"title": "Answer to \"About the experiments of cg and adam\"", "comment": "Thank you for your question. \nPrecisely, we have performed the analysis you are interested in. In Table 5 and 6 we show how much perturbation is needed for cg and adam to evade different defensive methods (rows) on MNIST and CIFAR-10, respectively. Here we normalize the values of pixels to [-0.5,0.5]. The magnitude of noise added for cg and adam are actually slightly smaller than fgs_0.5 (distortion=0.013 for MNIST, 0.0075 for CIFAR-10) but still succeed to attack as shown in Table 3 and 4. \nThe generated adversarial examples are not imperceptible after being attacked by cg and adam in this case, which is shown in Figure 3, 4 (d) & (e) for MNIST and CIFAR-10."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models", "abstract": "Due to deep cascades of nonlinear units, deep neural networks (DNNs) can automatically learn non-local generalization priors from data and have achieved high performance in various applications.\nHowever, such properties have also opened a door for adversaries to generate the so-called adversarial examples to fool DNNs. Specifically, adversaries can inject small perturbations to the input data and therefore decrease the performance of deep neural networks significantly.\nEven worse, these adversarial examples have the transferability to attack a black-box model based on finite queries without knowledge of the target model. \nTherefore, we aim to empirically compare different defensive strategies against various adversary models and analyze the cross-model efficiency for these robust learners. We conclude that the adversarial retraining framework also has the transferability, which can defend adversarial examples without requiring prior knowledge of the adversary models.\nWe compare the general adversarial retraining framework with the state-of-the-art robust deep neural networks, such as distillation, autoencoder stacked with classifier (AEC), and our improved version, IAEC, to evaluate their robustness as well as the vulnerability in terms of the distortion required to mislead the learner.\nOur experimental results show that the adversarial retraining framework can defend most of the adversarial examples notably and consistently without adding additional\nvulnerabilities or performance penalty to the original model.", "pdf": "/pdf/5ffc265e88c70d978a04bdfa570cb41fc25e89e8.pdf", "TL;DR": "robust adversarial retraining", "paperhash": "chen|evaluation_of_defensive_methods_for_dnns_against_multiple_adversarial_evasion_models", "keywords": ["Deep learning"], "authors": ["Xinyun Chen", "Bo Li", "Yevgeniy Vorobeychik"], "conflicts": ["umich.edu", "vanderbilt.edu", "sjtu.edu.cn"], "authorids": ["jungyhuk@gmail.com", "bbbli@umich.edu", "yevgeniy.vorobeychik@vanderbilt.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287567963, "id": "ICLR.cc/2017/conference/-/paper460/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByToKu9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper460/reviewers", "ICLR.cc/2017/conference/paper460/areachairs"], "cdate": 1485287567963}}}, {"tddate": null, "tmdate": 1480739844908, "tcdate": 1480739844902, "number": 3, "id": "rkabn6kXg", "invitation": "ICLR.cc/2017/conference/-/paper460/pre-review/question", "forum": "ByToKu9ll", "replyto": "ByToKu9ll", "signatures": ["ICLR.cc/2017/conference/paper460/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper460/AnonReviewer3"], "content": {"title": "The definition of distortion", "question": "What is n in 1/n \\sqrt(\\sum ...) at the end of the introduction of Section 3? \nWhy is n outside the square root rather then inside?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models", "abstract": "Due to deep cascades of nonlinear units, deep neural networks (DNNs) can automatically learn non-local generalization priors from data and have achieved high performance in various applications.\nHowever, such properties have also opened a door for adversaries to generate the so-called adversarial examples to fool DNNs. Specifically, adversaries can inject small perturbations to the input data and therefore decrease the performance of deep neural networks significantly.\nEven worse, these adversarial examples have the transferability to attack a black-box model based on finite queries without knowledge of the target model. \nTherefore, we aim to empirically compare different defensive strategies against various adversary models and analyze the cross-model efficiency for these robust learners. We conclude that the adversarial retraining framework also has the transferability, which can defend adversarial examples without requiring prior knowledge of the adversary models.\nWe compare the general adversarial retraining framework with the state-of-the-art robust deep neural networks, such as distillation, autoencoder stacked with classifier (AEC), and our improved version, IAEC, to evaluate their robustness as well as the vulnerability in terms of the distortion required to mislead the learner.\nOur experimental results show that the adversarial retraining framework can defend most of the adversarial examples notably and consistently without adding additional\nvulnerabilities or performance penalty to the original model.", "pdf": "/pdf/5ffc265e88c70d978a04bdfa570cb41fc25e89e8.pdf", "TL;DR": "robust adversarial retraining", "paperhash": "chen|evaluation_of_defensive_methods_for_dnns_against_multiple_adversarial_evasion_models", "keywords": ["Deep learning"], "authors": ["Xinyun Chen", "Bo Li", "Yevgeniy Vorobeychik"], "conflicts": ["umich.edu", "vanderbilt.edu", "sjtu.edu.cn"], "authorids": ["jungyhuk@gmail.com", "bbbli@umich.edu", "yevgeniy.vorobeychik@vanderbilt.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959269587, "id": "ICLR.cc/2017/conference/-/paper460/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper460/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper460/AnonReviewer2", "ICLR.cc/2017/conference/paper460/AnonReviewer1", "ICLR.cc/2017/conference/paper460/AnonReviewer3"], "reply": {"forum": "ByToKu9ll", "replyto": "ByToKu9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper460/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper460/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959269587}}}, {"tddate": null, "tmdate": 1480716324628, "tcdate": 1480716324624, "number": 2, "id": "Sy6Qx_1Ql", "invitation": "ICLR.cc/2017/conference/-/paper460/pre-review/question", "forum": "ByToKu9ll", "replyto": "ByToKu9ll", "signatures": ["ICLR.cc/2017/conference/paper460/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper460/AnonReviewer1"], "content": {"title": "About the experiments of cg and adam", "question": "Hi,\n\nI am curious if you have checked the perturbation magnitude of the attacker using cg or adam. For example, in the result of table 3 and table 4, while fgs fails, does it take cg and adam a huge perturbation to change the prediction label? Are the generated adversarial examples still imperceptible to human?\n\n\nBest,"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models", "abstract": "Due to deep cascades of nonlinear units, deep neural networks (DNNs) can automatically learn non-local generalization priors from data and have achieved high performance in various applications.\nHowever, such properties have also opened a door for adversaries to generate the so-called adversarial examples to fool DNNs. Specifically, adversaries can inject small perturbations to the input data and therefore decrease the performance of deep neural networks significantly.\nEven worse, these adversarial examples have the transferability to attack a black-box model based on finite queries without knowledge of the target model. \nTherefore, we aim to empirically compare different defensive strategies against various adversary models and analyze the cross-model efficiency for these robust learners. We conclude that the adversarial retraining framework also has the transferability, which can defend adversarial examples without requiring prior knowledge of the adversary models.\nWe compare the general adversarial retraining framework with the state-of-the-art robust deep neural networks, such as distillation, autoencoder stacked with classifier (AEC), and our improved version, IAEC, to evaluate their robustness as well as the vulnerability in terms of the distortion required to mislead the learner.\nOur experimental results show that the adversarial retraining framework can defend most of the adversarial examples notably and consistently without adding additional\nvulnerabilities or performance penalty to the original model.", "pdf": "/pdf/5ffc265e88c70d978a04bdfa570cb41fc25e89e8.pdf", "TL;DR": "robust adversarial retraining", "paperhash": "chen|evaluation_of_defensive_methods_for_dnns_against_multiple_adversarial_evasion_models", "keywords": ["Deep learning"], "authors": ["Xinyun Chen", "Bo Li", "Yevgeniy Vorobeychik"], "conflicts": ["umich.edu", "vanderbilt.edu", "sjtu.edu.cn"], "authorids": ["jungyhuk@gmail.com", "bbbli@umich.edu", "yevgeniy.vorobeychik@vanderbilt.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959269587, "id": "ICLR.cc/2017/conference/-/paper460/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper460/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper460/AnonReviewer2", "ICLR.cc/2017/conference/paper460/AnonReviewer1", "ICLR.cc/2017/conference/paper460/AnonReviewer3"], "reply": {"forum": "ByToKu9ll", "replyto": "ByToKu9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper460/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper460/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959269587}}}, {"tddate": null, "tmdate": 1480584344590, "tcdate": 1480584344580, "number": 1, "id": "r1WohPpGe", "invitation": "ICLR.cc/2017/conference/-/paper460/public/comment", "forum": "ByToKu9ll", "replyto": "rJJ-XanGl", "signatures": ["~Bo_Li2"], "readers": ["everyone"], "writers": ["~Bo_Li2"], "content": {"title": "Answer to \"Explored how each defense responds to transferability\"", "comment": "Thank you for your question. Indeed, we have performed precisely the analysis you suggest; the results are shown in Tables 1 & 2 for the MNIST and CIFAR-10 datasets, respectively. In these tables, each column corresponds to a model used to generate adversarial examples, and each row represents a specific method for defending against attacks.  Thus, we evaluate all defensive approaches against all attack methods, with each cell in the table reporting the performance of the \u201cdefense\u201d algorithm. The \"no adversary\" column in these tables shows the original test data without being contaminated for comparison."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models", "abstract": "Due to deep cascades of nonlinear units, deep neural networks (DNNs) can automatically learn non-local generalization priors from data and have achieved high performance in various applications.\nHowever, such properties have also opened a door for adversaries to generate the so-called adversarial examples to fool DNNs. Specifically, adversaries can inject small perturbations to the input data and therefore decrease the performance of deep neural networks significantly.\nEven worse, these adversarial examples have the transferability to attack a black-box model based on finite queries without knowledge of the target model. \nTherefore, we aim to empirically compare different defensive strategies against various adversary models and analyze the cross-model efficiency for these robust learners. We conclude that the adversarial retraining framework also has the transferability, which can defend adversarial examples without requiring prior knowledge of the adversary models.\nWe compare the general adversarial retraining framework with the state-of-the-art robust deep neural networks, such as distillation, autoencoder stacked with classifier (AEC), and our improved version, IAEC, to evaluate their robustness as well as the vulnerability in terms of the distortion required to mislead the learner.\nOur experimental results show that the adversarial retraining framework can defend most of the adversarial examples notably and consistently without adding additional\nvulnerabilities or performance penalty to the original model.", "pdf": "/pdf/5ffc265e88c70d978a04bdfa570cb41fc25e89e8.pdf", "TL;DR": "robust adversarial retraining", "paperhash": "chen|evaluation_of_defensive_methods_for_dnns_against_multiple_adversarial_evasion_models", "keywords": ["Deep learning"], "authors": ["Xinyun Chen", "Bo Li", "Yevgeniy Vorobeychik"], "conflicts": ["umich.edu", "vanderbilt.edu", "sjtu.edu.cn"], "authorids": ["jungyhuk@gmail.com", "bbbli@umich.edu", "yevgeniy.vorobeychik@vanderbilt.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287567963, "id": "ICLR.cc/2017/conference/-/paper460/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByToKu9ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper460/reviewers", "ICLR.cc/2017/conference/paper460/areachairs"], "cdate": 1485287567963}}}, {"tddate": null, "tmdate": 1480540919190, "tcdate": 1480540919183, "number": 1, "id": "rJJ-XanGl", "invitation": "ICLR.cc/2017/conference/-/paper460/pre-review/question", "forum": "ByToKu9ll", "replyto": "ByToKu9ll", "signatures": ["ICLR.cc/2017/conference/paper460/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper460/AnonReviewer2"], "content": {"title": "Explored how each defense responds to transferability ", "question": "Adversarial examples for one network may be adversarial examples for different networks (trained with different initial conditions or even different network architectures). Have you examined how each defense method performs when an adversarial example is calculated on a different model then where it is evaluated on?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models", "abstract": "Due to deep cascades of nonlinear units, deep neural networks (DNNs) can automatically learn non-local generalization priors from data and have achieved high performance in various applications.\nHowever, such properties have also opened a door for adversaries to generate the so-called adversarial examples to fool DNNs. Specifically, adversaries can inject small perturbations to the input data and therefore decrease the performance of deep neural networks significantly.\nEven worse, these adversarial examples have the transferability to attack a black-box model based on finite queries without knowledge of the target model. \nTherefore, we aim to empirically compare different defensive strategies against various adversary models and analyze the cross-model efficiency for these robust learners. We conclude that the adversarial retraining framework also has the transferability, which can defend adversarial examples without requiring prior knowledge of the adversary models.\nWe compare the general adversarial retraining framework with the state-of-the-art robust deep neural networks, such as distillation, autoencoder stacked with classifier (AEC), and our improved version, IAEC, to evaluate their robustness as well as the vulnerability in terms of the distortion required to mislead the learner.\nOur experimental results show that the adversarial retraining framework can defend most of the adversarial examples notably and consistently without adding additional\nvulnerabilities or performance penalty to the original model.", "pdf": "/pdf/5ffc265e88c70d978a04bdfa570cb41fc25e89e8.pdf", "TL;DR": "robust adversarial retraining", "paperhash": "chen|evaluation_of_defensive_methods_for_dnns_against_multiple_adversarial_evasion_models", "keywords": ["Deep learning"], "authors": ["Xinyun Chen", "Bo Li", "Yevgeniy Vorobeychik"], "conflicts": ["umich.edu", "vanderbilt.edu", "sjtu.edu.cn"], "authorids": ["jungyhuk@gmail.com", "bbbli@umich.edu", "yevgeniy.vorobeychik@vanderbilt.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959269587, "id": "ICLR.cc/2017/conference/-/paper460/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper460/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper460/AnonReviewer2", "ICLR.cc/2017/conference/paper460/AnonReviewer1", "ICLR.cc/2017/conference/paper460/AnonReviewer3"], "reply": {"forum": "ByToKu9ll", "replyto": "ByToKu9ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper460/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper460/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959269587}}}], "count": 17}