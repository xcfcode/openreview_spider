{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396543191, "tcdate": 1486396543191, "number": 1, "id": "Hkvt2M8dx", "invitation": "ICLR.cc/2017/conference/-/paper367/acceptance", "forum": "BJ_MGwqlg", "replyto": "BJ_MGwqlg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The reviewers feel that this is a well written paper on floating and fixed point representations for inference with several state of the art deep learning architectures. At the same time, in order for results to be more convincing, they recommend using 16-bit floats as a more proper baseline for comparison, and to analyze tradeoffs in overall workload speedup, i.e broader system-level issues surrounding the implementation of custom floating point units."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Numerical Representations for Deep Neural Networks", "abstract": "With ever-increasing computational demand for deep learning, it is critical to investigate the implications of the numeric representation and precision of DNN model weights and activations on computational efficiency. In this work, we explore unconventional narrow-precision floating-point representations as it relates to inference accuracy and efficiency to steer the improved design of future DNN platforms.  We show that inference using these custom numeric representations on production-grade DNNs, including GoogLeNet and VGG, achieves an average speedup of 7.6x with less than 1% degradation in inference accuracy relative to a state-of-the-art baseline platform representing the most sophisticated hardware using single-precision floating point.  To facilitate the use of such customized precision, we also present a novel technique that drastically reduces the time required to derive the optimal precision configuration.", "pdf": "/pdf/a1a6a7b3d41ce16869ae64277f5d10c95d1df604.pdf", "TL;DR": "We find that the optimal numerical representation for large-scale DNNs is very different than the small-scale ones that are used in current DNN hardware research.", "paperhash": "hill|rethinking_numerical_representations_for_deep_neural_networks", "keywords": ["Deep learning"], "conflicts": ["umich.edu"], "authors": ["Parker Hill", "Babak Zamirai", "Shengshuo Lu", "Yu-Wei Chao", "Michael Laurenzano", "Mehrzad Samadi", "Marios Papaefthymiou", "Scott Mahlke", "Thomas Wenisch", "Jia Deng", "Lingjia Tang", "Jason Mars"], "authorids": ["parkerhh@umich.edu", "zamirai@umich.edu", "luss@umich.edu", "ywchao@umich.edu", "mlaurenz@umich.edu", "mehrzads@umich.edu", "marios@umich.edu", "mahlke@umich.edu", "twenisch@umich.edu", "jiadeng@umich.edu", "lingjia@umich.edu", "profmars@umich.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396543718, "id": "ICLR.cc/2017/conference/-/paper367/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BJ_MGwqlg", "replyto": "BJ_MGwqlg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396543718}}}, {"tddate": null, "tmdate": 1484351397222, "tcdate": 1484351397222, "number": 5, "id": "r16jP1DLx", "invitation": "ICLR.cc/2017/conference/-/paper367/public/comment", "forum": "BJ_MGwqlg", "replyto": "SyusKkUNx", "signatures": ["~Parker_Hill1"], "readers": ["everyone"], "writers": ["~Parker_Hill1"], "content": {"title": "Response to AnonReviewer2", "comment": "We thank you for your valuable time and comments.\n\n===Search method clarifications\nOur search model only requires a small subset (evaluation uses 10) of the validation inputs to predict accuracy by leveraging all of the activations in the last layer, rather than observing only the top-1 (or top-5) results. Last layer activations require fewer inputs compared to top-1 accuracy, since each DNN input produces many scalar activations rather than a single binary correctness value. For example, 10 inputs using top-1 accuracy provides 10 binary values (i.e. each DNN output being either correct or incorrect), while the activations for 10 ImageNet inputs provides 10,000 scalar values (i.e. 10 DNN inputs * 1000 scalar output activations per ImageNet input).\n\n===Batch normalization\nBatch normalization primarily impacts the training process of DNNs rather than inference, so we expect to arrive at very similar conclusions with or without it. Batch normalization during inference applies a fixed linear transformation to each activation, which requires few hardware resources and does not change the shape of the activation distribution.\n\n===Importance of inference performance\nReducing the computational requirements of DNN inference is relevant to the machine learning community. A number of papers on this topic have been published at venues such as ICLR, ICML, and NIPS[1,2,3,4].\n\n[1] Compressing Neural Networks with the Hashing Trick. ICML 2015. Chen, at al.\n[2] Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation. NIPS 2014. Denton, et al.\n[3] Learning both Weights and Connections for Efficient Neural Networks. NIPS 2015. Han, et al.\n[4] Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding. ICLR 2016. Han, et al."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Numerical Representations for Deep Neural Networks", "abstract": "With ever-increasing computational demand for deep learning, it is critical to investigate the implications of the numeric representation and precision of DNN model weights and activations on computational efficiency. In this work, we explore unconventional narrow-precision floating-point representations as it relates to inference accuracy and efficiency to steer the improved design of future DNN platforms.  We show that inference using these custom numeric representations on production-grade DNNs, including GoogLeNet and VGG, achieves an average speedup of 7.6x with less than 1% degradation in inference accuracy relative to a state-of-the-art baseline platform representing the most sophisticated hardware using single-precision floating point.  To facilitate the use of such customized precision, we also present a novel technique that drastically reduces the time required to derive the optimal precision configuration.", "pdf": "/pdf/a1a6a7b3d41ce16869ae64277f5d10c95d1df604.pdf", "TL;DR": "We find that the optimal numerical representation for large-scale DNNs is very different than the small-scale ones that are used in current DNN hardware research.", "paperhash": "hill|rethinking_numerical_representations_for_deep_neural_networks", "keywords": ["Deep learning"], "conflicts": ["umich.edu"], "authors": ["Parker Hill", "Babak Zamirai", "Shengshuo Lu", "Yu-Wei Chao", "Michael Laurenzano", "Mehrzad Samadi", "Marios Papaefthymiou", "Scott Mahlke", "Thomas Wenisch", "Jia Deng", "Lingjia Tang", "Jason Mars"], "authorids": ["parkerhh@umich.edu", "zamirai@umich.edu", "luss@umich.edu", "ywchao@umich.edu", "mlaurenz@umich.edu", "mehrzads@umich.edu", "marios@umich.edu", "mahlke@umich.edu", "twenisch@umich.edu", "jiadeng@umich.edu", "lingjia@umich.edu", "profmars@umich.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287605641, "id": "ICLR.cc/2017/conference/-/paper367/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ_MGwqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper367/reviewers", "ICLR.cc/2017/conference/paper367/areachairs"], "cdate": 1485287605641}}}, {"tddate": null, "tmdate": 1484351341844, "tcdate": 1484351341844, "number": 4, "id": "S18_vJwUx", "invitation": "ICLR.cc/2017/conference/-/paper367/public/comment", "forum": "BJ_MGwqlg", "replyto": "SkO-TExSl", "signatures": ["~Parker_Hill1"], "readers": ["everyone"], "writers": ["~Parker_Hill1"], "content": {"title": "Response to AnonReviewer5", "comment": "We thank you for your valuable time and comments.\n\n===Denormal floating-point units\nOur evaluated floating-point ALUs do not operate on denormal floating-point representations. Aside from minor differences in empirical results that depend on denormal floating-point arithmetic support, we expect that our conclusions hold. A different design that operates on denormal floating-point representations will, ideally, allow one less bit in the floating-point exponent for the same accuracy, but will be disadvantaged by increased hardware area, circuit delay, and power.\n\n===Frequency scaling of customized-precision units\nWe scale the ALU frequency with the inverse of the hardware circuit delay, so each customized-precision design can have a different frequency.\n\n===Frequency of compute compared to storage\nOur DRAM memory and compute clocks (frequencies) are different, which is common across almost all hardware designs (e.g., modern CPU/GPU designs). DRAM memory fabrication is primarily optimized for higher transistor density (i.e. memory capacity) rather than higher frequency, which is opposite of computational units.\n\n===Validity of MAC results\nThe MAC operations capture the majority of the DNN performance and power breakdown [1]. Other units will scale with the customized-precision design as well, so we expect that a full-system implementation would show benefits very similar to our results. For example, using 84% [1] as the power used by compute, we find our reported 3.4x savings in energy would become 3.15x (time = 0.84/3.4 + 0.16/(32-bit/14-bit from linear scaling) = 0.31 => speedup = 1/0.31 = 3.15x).\n\n===Compute throughput as DNN bottleneck\nThe DRAM memory bandwidth requirements for DNNs is much lower than its computational requirements [2]. This is due to matrix multiplication, the central DNN computational kernel, requiring roughly N^2 memory operations (i.e. from loop tiling [3]) compared to N^3 arithmetic operations.\n\n===Customized-precision memory access\nDRAM memory access can be left unchanged when using customized-precision compute units. Similar to how GPUs do 32-bit computation efficiently when using a 128- to 512-bit memory bus, a single memory access is distributed to multiple compute units. For example, a 128-bit bus provides data to nine 14-bit compute units.\n\n[1] ShiDianNao: Shifting Vision Processing Closer to the Sensor. ISCA 2015. Zidong Du, et al.\n[2] DjiNN and Tonic: DNN as a Service and Its Implications for Future Warehouse Scale Computers. ISCA 2015. Hauswald, et al.\n[3] Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks. FPGA 2015. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Numerical Representations for Deep Neural Networks", "abstract": "With ever-increasing computational demand for deep learning, it is critical to investigate the implications of the numeric representation and precision of DNN model weights and activations on computational efficiency. In this work, we explore unconventional narrow-precision floating-point representations as it relates to inference accuracy and efficiency to steer the improved design of future DNN platforms.  We show that inference using these custom numeric representations on production-grade DNNs, including GoogLeNet and VGG, achieves an average speedup of 7.6x with less than 1% degradation in inference accuracy relative to a state-of-the-art baseline platform representing the most sophisticated hardware using single-precision floating point.  To facilitate the use of such customized precision, we also present a novel technique that drastically reduces the time required to derive the optimal precision configuration.", "pdf": "/pdf/a1a6a7b3d41ce16869ae64277f5d10c95d1df604.pdf", "TL;DR": "We find that the optimal numerical representation for large-scale DNNs is very different than the small-scale ones that are used in current DNN hardware research.", "paperhash": "hill|rethinking_numerical_representations_for_deep_neural_networks", "keywords": ["Deep learning"], "conflicts": ["umich.edu"], "authors": ["Parker Hill", "Babak Zamirai", "Shengshuo Lu", "Yu-Wei Chao", "Michael Laurenzano", "Mehrzad Samadi", "Marios Papaefthymiou", "Scott Mahlke", "Thomas Wenisch", "Jia Deng", "Lingjia Tang", "Jason Mars"], "authorids": ["parkerhh@umich.edu", "zamirai@umich.edu", "luss@umich.edu", "ywchao@umich.edu", "mlaurenz@umich.edu", "mehrzads@umich.edu", "marios@umich.edu", "mahlke@umich.edu", "twenisch@umich.edu", "jiadeng@umich.edu", "lingjia@umich.edu", "profmars@umich.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287605641, "id": "ICLR.cc/2017/conference/-/paper367/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ_MGwqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper367/reviewers", "ICLR.cc/2017/conference/paper367/areachairs"], "cdate": 1485287605641}}}, {"tddate": null, "tmdate": 1484351281399, "tcdate": 1484351281399, "number": 3, "id": "H1FVw1w8e", "invitation": "ICLR.cc/2017/conference/-/paper367/public/comment", "forum": "BJ_MGwqlg", "replyto": "SJIZu9_Sg", "signatures": ["~Parker_Hill1"], "readers": ["everyone"], "writers": ["~Parker_Hill1"], "content": {"title": "Response to AnonReviewer3", "comment": "We thank you for your valuable time and comments.\n\n===Performance and power methodology\nWe evaluate our results based on the multiply-accumulate (MAC) operations, the key building block of any hardware implementation of a DNN. The MAC operations capture the majority of the DNN performance and power breakdown [1]. Other units will scale with the customized-precision design as well, so we expect that a full-system implementation would show benefits very similar to our results. For example, using 84% [1] as the power used by compute, we find our reported 3.4x savings in energy would become 3.15x (time = 0.84/3.4 + 0.16/(32-bit/14-bit from linear scaling) = 0.31 => speedup = 1/0.31 = 3.15x). Similarly, full-system performance is dictated by MAC performance.\n\n===ASIC design tool details\nOur comparisons are made using designs synthesized with Synopsys Design Compiler and are further verified using Synopsys PrimeTime and SPICE simulations. The specific cell library, provided by the manufacturer, which produces area, power, and timing information, cannot be disclosed. These cell libraries are used for timing verification for chip tape out, hence, they must (and do) accurately model real hardware.\n\n===Importance of search\nOur search method\u2019s 170x speedup mitigates the time-intensive process of emulating customized precision operations, allowing researchers to iteratively adjust DNN topology for optimized hardware efficiency.\n\n===Suggestions for future work and clarifications\nThank you for the suggestions. We will integrate this feedback into future versions of our work.\n\n[1] ShiDianNao: Shifting Vision Processing Closer to the Sensor. ISCA 2015. Zidong Du, et al."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Numerical Representations for Deep Neural Networks", "abstract": "With ever-increasing computational demand for deep learning, it is critical to investigate the implications of the numeric representation and precision of DNN model weights and activations on computational efficiency. In this work, we explore unconventional narrow-precision floating-point representations as it relates to inference accuracy and efficiency to steer the improved design of future DNN platforms.  We show that inference using these custom numeric representations on production-grade DNNs, including GoogLeNet and VGG, achieves an average speedup of 7.6x with less than 1% degradation in inference accuracy relative to a state-of-the-art baseline platform representing the most sophisticated hardware using single-precision floating point.  To facilitate the use of such customized precision, we also present a novel technique that drastically reduces the time required to derive the optimal precision configuration.", "pdf": "/pdf/a1a6a7b3d41ce16869ae64277f5d10c95d1df604.pdf", "TL;DR": "We find that the optimal numerical representation for large-scale DNNs is very different than the small-scale ones that are used in current DNN hardware research.", "paperhash": "hill|rethinking_numerical_representations_for_deep_neural_networks", "keywords": ["Deep learning"], "conflicts": ["umich.edu"], "authors": ["Parker Hill", "Babak Zamirai", "Shengshuo Lu", "Yu-Wei Chao", "Michael Laurenzano", "Mehrzad Samadi", "Marios Papaefthymiou", "Scott Mahlke", "Thomas Wenisch", "Jia Deng", "Lingjia Tang", "Jason Mars"], "authorids": ["parkerhh@umich.edu", "zamirai@umich.edu", "luss@umich.edu", "ywchao@umich.edu", "mlaurenz@umich.edu", "mehrzads@umich.edu", "marios@umich.edu", "mahlke@umich.edu", "twenisch@umich.edu", "jiadeng@umich.edu", "lingjia@umich.edu", "profmars@umich.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287605641, "id": "ICLR.cc/2017/conference/-/paper367/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ_MGwqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper367/reviewers", "ICLR.cc/2017/conference/paper367/areachairs"], "cdate": 1485287605641}}}, {"tddate": null, "tmdate": 1483413501867, "tcdate": 1483413501867, "number": 3, "id": "SJIZu9_Sg", "invitation": "ICLR.cc/2017/conference/-/paper367/official/review", "forum": "BJ_MGwqlg", "replyto": "BJ_MGwqlg", "signatures": ["ICLR.cc/2017/conference/paper367/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper367/AnonReviewer3"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "The paper studies the impact of using customized number representations on accuracy, speed, and energy consumption of neural network inference. Several standard computer vision architectures including VGG and GoogleNet are considered for the experiments, and it is concluded that floating point representations are preferred over fixed point representations, and floating point numbers with about 14 bits are sufficient for the considered architectures resulting in a small loss in accuracy.\n\nThe paper provides a nice overview of floating and fixed point representations and focuses on an important aspect of deep learning that is not well studied. There are several aspects of the paper that could be improved, but overall, I am leaned toward weak accept assuming that the authors address the issues below.\n\n1- The paper is not clear that it is only focusing on neural network inference. Please include the word \"inference\" in the title / abstract to clarify this point and mention that the findings of the paper do not necessarily apply to neural network training as training dynamics could be different.\n\n2- The paper does not discuss the possibility of adopting quantization tricks during training, which may result in the use of fewer bits at inference.\n\n3- The paper is not clear whether in computing the running time and power consumption, it includes all of the modules or only multiply-accumulate units? Also, how accurate are these numbers given different possible designs and the potential difference between simulation and production? Please elaborate on the details of simulation in the paper.\n\n4- The whole discussion about \"efficient customized precision search\" seem unimportant to me. When such important hardware considerations are concerned, even spending 20x simulation time is not that important. The exhaustive search process could be easily parallelized and one may rather spend more time at simulation at the cost of finding the exact best configuration rather than an approximation. That said, weak configurations could be easily filtered after evaluating just a few examples.\n\n5- Nvidia's Pascal GP100 GPU supports FP16. This should be discussed in the paper and relevant Nvidia papers / documents should be cited.\n\nMore comments:\n\n- Parts of the paper discussing \"efficient customized precision search\" are not clear to me.\n\n- As future work, the impact of number representations on batch normalization and recurrent neural networks could be studied.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Numerical Representations for Deep Neural Networks", "abstract": "With ever-increasing computational demand for deep learning, it is critical to investigate the implications of the numeric representation and precision of DNN model weights and activations on computational efficiency. In this work, we explore unconventional narrow-precision floating-point representations as it relates to inference accuracy and efficiency to steer the improved design of future DNN platforms.  We show that inference using these custom numeric representations on production-grade DNNs, including GoogLeNet and VGG, achieves an average speedup of 7.6x with less than 1% degradation in inference accuracy relative to a state-of-the-art baseline platform representing the most sophisticated hardware using single-precision floating point.  To facilitate the use of such customized precision, we also present a novel technique that drastically reduces the time required to derive the optimal precision configuration.", "pdf": "/pdf/a1a6a7b3d41ce16869ae64277f5d10c95d1df604.pdf", "TL;DR": "We find that the optimal numerical representation for large-scale DNNs is very different than the small-scale ones that are used in current DNN hardware research.", "paperhash": "hill|rethinking_numerical_representations_for_deep_neural_networks", "keywords": ["Deep learning"], "conflicts": ["umich.edu"], "authors": ["Parker Hill", "Babak Zamirai", "Shengshuo Lu", "Yu-Wei Chao", "Michael Laurenzano", "Mehrzad Samadi", "Marios Papaefthymiou", "Scott Mahlke", "Thomas Wenisch", "Jia Deng", "Lingjia Tang", "Jason Mars"], "authorids": ["parkerhh@umich.edu", "zamirai@umich.edu", "luss@umich.edu", "ywchao@umich.edu", "mlaurenz@umich.edu", "mehrzads@umich.edu", "marios@umich.edu", "mahlke@umich.edu", "twenisch@umich.edu", "jiadeng@umich.edu", "lingjia@umich.edu", "profmars@umich.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483413502549, "id": "ICLR.cc/2017/conference/-/paper367/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper367/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper367/AnonReviewer2", "ICLR.cc/2017/conference/paper367/AnonReviewer5", "ICLR.cc/2017/conference/paper367/AnonReviewer3"], "reply": {"forum": "BJ_MGwqlg", "replyto": "BJ_MGwqlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper367/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper367/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483413502549}}}, {"tddate": null, "tmdate": 1483338343352, "tcdate": 1482865920526, "number": 2, "id": "SkO-TExSl", "invitation": "ICLR.cc/2017/conference/-/paper367/official/review", "forum": "BJ_MGwqlg", "replyto": "BJ_MGwqlg", "signatures": ["ICLR.cc/2017/conference/paper367/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper367/AnonReviewer5"], "content": {"title": "Ignores broader system-level issues, needs to use 16-bit floats as baseline", "rating": "5: Marginally below acceptance threshold", "review": "This paper explores the performance-area-energy-model accuracy tradeoff encountered in designing custom number representations for deep learning inference. Common image-based benchmarks: VGG, Googlenet etc are used to demonstrate that fewer than1 6 bits in a custom floating point representation can lead to improvement in runtime performance and energy efficiency with only a small loss in model accuracy.\n\nQuestions:\n\n1. Does the custom floating point number representation take into account support for de-normal numbers? \n2. Is the custom floating point unit clocked at the same frequency as the baseline 32-bit floating point unit? If not, what are the different frequencies used and how would this impact the overall system design in terms of feeding the data to the floating point units from the memory\n\nComments:\n\n1. I would recommend using the IEEE half-precision floating point (1bit sign, 5bit exponent, and 10bit mantissa) as a baseline for comparison. At this point, it is well known in both the ML and the HW communities that 32-bit floats are an overkill for DNN inference and major HW vendors already include support for IEEE half-precision floats. \n2. In my opinion, the claim that switching to custom floating point  lead to a YY.ZZ x savings in energy is misleading. It might be true that the floating-point unit itself might consume less energy due to smaller bit-width of the operands, however a large fraction of the total energy is spent in data movement to/from the memories. As a result, reducing the floating point unit\u2019s energy consumption by a certain factor will not translate to the same reduction in the total energy. A reader not familiar with such nuances (for example a typical member of the ML community), may be mislead by such claims. \n3. On a similar note as comment 2, the authors should explicitly mention that the claimed speedup is that of the floating point unit only, and it will not translate to the overall workload speedup. Although the speedup of the compute unit is roughly quadratic in the bit-width, the bandwidth requirements scale linearly with bit-width. As a result, it is possible that these custom floating point units may be starved on memory bandwidth, in which case the claims of speedup and energy savings need to be revisited.\n4. The authors should also comment on the complexities and overheads introduced in data accesses, designing the various system buses/ data paths when the number representation is not byte-aligned. Moving to a custom 14-bit number representation (for example) can improve the performance and energy-efficiency of the floating point unit, but these gains can be partially eroded due to the additional overhead in supporting non-byte aligned memory accesses.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Numerical Representations for Deep Neural Networks", "abstract": "With ever-increasing computational demand for deep learning, it is critical to investigate the implications of the numeric representation and precision of DNN model weights and activations on computational efficiency. In this work, we explore unconventional narrow-precision floating-point representations as it relates to inference accuracy and efficiency to steer the improved design of future DNN platforms.  We show that inference using these custom numeric representations on production-grade DNNs, including GoogLeNet and VGG, achieves an average speedup of 7.6x with less than 1% degradation in inference accuracy relative to a state-of-the-art baseline platform representing the most sophisticated hardware using single-precision floating point.  To facilitate the use of such customized precision, we also present a novel technique that drastically reduces the time required to derive the optimal precision configuration.", "pdf": "/pdf/a1a6a7b3d41ce16869ae64277f5d10c95d1df604.pdf", "TL;DR": "We find that the optimal numerical representation for large-scale DNNs is very different than the small-scale ones that are used in current DNN hardware research.", "paperhash": "hill|rethinking_numerical_representations_for_deep_neural_networks", "keywords": ["Deep learning"], "conflicts": ["umich.edu"], "authors": ["Parker Hill", "Babak Zamirai", "Shengshuo Lu", "Yu-Wei Chao", "Michael Laurenzano", "Mehrzad Samadi", "Marios Papaefthymiou", "Scott Mahlke", "Thomas Wenisch", "Jia Deng", "Lingjia Tang", "Jason Mars"], "authorids": ["parkerhh@umich.edu", "zamirai@umich.edu", "luss@umich.edu", "ywchao@umich.edu", "mlaurenz@umich.edu", "mehrzads@umich.edu", "marios@umich.edu", "mahlke@umich.edu", "twenisch@umich.edu", "jiadeng@umich.edu", "lingjia@umich.edu", "profmars@umich.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483413502549, "id": "ICLR.cc/2017/conference/-/paper367/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper367/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper367/AnonReviewer2", "ICLR.cc/2017/conference/paper367/AnonReviewer5", "ICLR.cc/2017/conference/paper367/AnonReviewer3"], "reply": {"forum": "BJ_MGwqlg", "replyto": "BJ_MGwqlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper367/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper367/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483413502549}}}, {"tddate": null, "tmdate": 1482189216518, "tcdate": 1482189216518, "number": 1, "id": "SyusKkUNx", "invitation": "ICLR.cc/2017/conference/-/paper367/official/review", "forum": "BJ_MGwqlg", "replyto": "BJ_MGwqlg", "signatures": ["ICLR.cc/2017/conference/paper367/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper367/AnonReviewer2"], "content": {"title": "Can be improved", "rating": "5: Marginally below acceptance threshold", "review": "The paper provides a first study of customized precision hardware for large convolutional networks, namely alexnet, vgg and googlenet. It shows that it is possible to achieve larger speed-ups using floating-point precision (up to 7x) when using fewer bits, and better than using fixed-point representations. \n\nThe paper also explores predicting custom floating-point precision parameters directly from the neural network activations, avoiding exhaustive search, but i could not follow this part. Only the activations of the last layer are evaluated, but on what data ? On all the validation set ? Why would this be faster than computing the classification accuracy ?\n\nThe results should be useful for hardware manufacturers, but with a catch. All popular convolutional networks now use batch normalization, while none of the evaluated ones do. It may well be that the conclusions of this study will be completely different on batch normalization networks, and fixed-point representations are best there, but that remains to be seen. It seems like something worth exploring.\n\nOverall there is not a great deal of novelty other than being a useful study on numerical precision trade-offs at neural network test time. Training time is also something of interest. There are a lot more researchers trying to train new networks fast than trying to evaluate old ones fast. \n\nI am also no expert in digital logic design, but my educated guess is that this paper is marginally below the acceptance threshold.", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Numerical Representations for Deep Neural Networks", "abstract": "With ever-increasing computational demand for deep learning, it is critical to investigate the implications of the numeric representation and precision of DNN model weights and activations on computational efficiency. In this work, we explore unconventional narrow-precision floating-point representations as it relates to inference accuracy and efficiency to steer the improved design of future DNN platforms.  We show that inference using these custom numeric representations on production-grade DNNs, including GoogLeNet and VGG, achieves an average speedup of 7.6x with less than 1% degradation in inference accuracy relative to a state-of-the-art baseline platform representing the most sophisticated hardware using single-precision floating point.  To facilitate the use of such customized precision, we also present a novel technique that drastically reduces the time required to derive the optimal precision configuration.", "pdf": "/pdf/a1a6a7b3d41ce16869ae64277f5d10c95d1df604.pdf", "TL;DR": "We find that the optimal numerical representation for large-scale DNNs is very different than the small-scale ones that are used in current DNN hardware research.", "paperhash": "hill|rethinking_numerical_representations_for_deep_neural_networks", "keywords": ["Deep learning"], "conflicts": ["umich.edu"], "authors": ["Parker Hill", "Babak Zamirai", "Shengshuo Lu", "Yu-Wei Chao", "Michael Laurenzano", "Mehrzad Samadi", "Marios Papaefthymiou", "Scott Mahlke", "Thomas Wenisch", "Jia Deng", "Lingjia Tang", "Jason Mars"], "authorids": ["parkerhh@umich.edu", "zamirai@umich.edu", "luss@umich.edu", "ywchao@umich.edu", "mlaurenz@umich.edu", "mehrzads@umich.edu", "marios@umich.edu", "mahlke@umich.edu", "twenisch@umich.edu", "jiadeng@umich.edu", "lingjia@umich.edu", "profmars@umich.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483413502549, "id": "ICLR.cc/2017/conference/-/paper367/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper367/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper367/AnonReviewer2", "ICLR.cc/2017/conference/paper367/AnonReviewer5", "ICLR.cc/2017/conference/paper367/AnonReviewer3"], "reply": {"forum": "BJ_MGwqlg", "replyto": "BJ_MGwqlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper367/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper367/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483413502549}}}, {"tddate": null, "tmdate": 1481166653989, "tcdate": 1481166653984, "number": 2, "id": "ryUr1IU7g", "invitation": "ICLR.cc/2017/conference/-/paper367/public/comment", "forum": "BJ_MGwqlg", "replyto": "HkQJ0t17x", "signatures": ["~Parker_Hill1"], "readers": ["everyone"], "writers": ["~Parker_Hill1"], "content": {"title": "Elaboration on prior work comparison", "comment": "===Comparison with binary DNNs===\nCustomized-precision DNNs provide transparent, flexible benefits to existing DNN models by removing inconsequential bits. XNOR-Net[1] and similar works[2,3] operate on binary computational primitives that require changes to the DNN architecture and are inflexible (i.e. either on or off). These problems are manifested when applied to large DNNs. For example, the state-of-the-art XNOR-Net technique applied to GoogleNet does not converge (XNOR-Net[1], table 2), while our customized-precision approach yields over 5x speedup with less than 1% loss in accuracy.\n\n\n===Comparison with DNN compression===\nOur customized-precision DNNs use less precision for computation and storage, while previous works that evaluate compressed DNNs[4,5,6] use less precision for storage *without* optimizing the compute units. Computation precision tuning allows quadratic improvement in throughput due to smaller hardware designs (more parallelism) and shorter circuit delay (higher frequency). Smaller numeric representations in storage are limited to a linear increase in throughput, since storage and bandwidth scale linearly with the size of the data.\n\n\n[1] XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks. ECCV'16. Rastegari, et al.\n[2] BinaryConnect: Training Deep Neural Networks with Binary Weights during Propagations. NIPS'15. Courbariaux, et al.\n[3] Training Binary Multilayer Neural Networks for Image Classification using Expectation Backpropagation.  arXiv'15. Cheng, et al.\n[4] Compressing Deep Convolutional Networks using Vector Quantization. arXiv'14. Gong, et al.\n[5] EIE: Efficient Inference Engine on Compressed Deep Neural Network. ISCA'16. Han, et al.\n[6] Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding. ICLR'16. Han, et al."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Numerical Representations for Deep Neural Networks", "abstract": "With ever-increasing computational demand for deep learning, it is critical to investigate the implications of the numeric representation and precision of DNN model weights and activations on computational efficiency. In this work, we explore unconventional narrow-precision floating-point representations as it relates to inference accuracy and efficiency to steer the improved design of future DNN platforms.  We show that inference using these custom numeric representations on production-grade DNNs, including GoogLeNet and VGG, achieves an average speedup of 7.6x with less than 1% degradation in inference accuracy relative to a state-of-the-art baseline platform representing the most sophisticated hardware using single-precision floating point.  To facilitate the use of such customized precision, we also present a novel technique that drastically reduces the time required to derive the optimal precision configuration.", "pdf": "/pdf/a1a6a7b3d41ce16869ae64277f5d10c95d1df604.pdf", "TL;DR": "We find that the optimal numerical representation for large-scale DNNs is very different than the small-scale ones that are used in current DNN hardware research.", "paperhash": "hill|rethinking_numerical_representations_for_deep_neural_networks", "keywords": ["Deep learning"], "conflicts": ["umich.edu"], "authors": ["Parker Hill", "Babak Zamirai", "Shengshuo Lu", "Yu-Wei Chao", "Michael Laurenzano", "Mehrzad Samadi", "Marios Papaefthymiou", "Scott Mahlke", "Thomas Wenisch", "Jia Deng", "Lingjia Tang", "Jason Mars"], "authorids": ["parkerhh@umich.edu", "zamirai@umich.edu", "luss@umich.edu", "ywchao@umich.edu", "mlaurenz@umich.edu", "mehrzads@umich.edu", "marios@umich.edu", "mahlke@umich.edu", "twenisch@umich.edu", "jiadeng@umich.edu", "lingjia@umich.edu", "profmars@umich.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287605641, "id": "ICLR.cc/2017/conference/-/paper367/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ_MGwqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper367/reviewers", "ICLR.cc/2017/conference/paper367/areachairs"], "cdate": 1485287605641}}}, {"tddate": null, "tmdate": 1481166405627, "tcdate": 1481166405622, "number": 1, "id": "HJRrAH8Xe", "invitation": "ICLR.cc/2017/conference/-/paper367/public/comment", "forum": "BJ_MGwqlg", "replyto": "SkaankJXx", "signatures": ["~Parker_Hill1"], "readers": ["everyone"], "writers": ["~Parker_Hill1"], "content": {"title": "Customized-precision training", "comment": "We briefly explored training DNNs with customized precision and found that training requires more precision than inference, since training makes minute adjustments to DNN weights until they converge to their final values. A complete sweep of the customized-precision design space for inference already requires a large amount of time (i.e. the product of several DNNs, hundreds of customized-precision design points, millions of input images, and orders-of-magnitude slowdown from hardware emulation). The same study for training would require further order-of-magnitude increase in experiment runtime, since training is an iterative process."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Numerical Representations for Deep Neural Networks", "abstract": "With ever-increasing computational demand for deep learning, it is critical to investigate the implications of the numeric representation and precision of DNN model weights and activations on computational efficiency. In this work, we explore unconventional narrow-precision floating-point representations as it relates to inference accuracy and efficiency to steer the improved design of future DNN platforms.  We show that inference using these custom numeric representations on production-grade DNNs, including GoogLeNet and VGG, achieves an average speedup of 7.6x with less than 1% degradation in inference accuracy relative to a state-of-the-art baseline platform representing the most sophisticated hardware using single-precision floating point.  To facilitate the use of such customized precision, we also present a novel technique that drastically reduces the time required to derive the optimal precision configuration.", "pdf": "/pdf/a1a6a7b3d41ce16869ae64277f5d10c95d1df604.pdf", "TL;DR": "We find that the optimal numerical representation for large-scale DNNs is very different than the small-scale ones that are used in current DNN hardware research.", "paperhash": "hill|rethinking_numerical_representations_for_deep_neural_networks", "keywords": ["Deep learning"], "conflicts": ["umich.edu"], "authors": ["Parker Hill", "Babak Zamirai", "Shengshuo Lu", "Yu-Wei Chao", "Michael Laurenzano", "Mehrzad Samadi", "Marios Papaefthymiou", "Scott Mahlke", "Thomas Wenisch", "Jia Deng", "Lingjia Tang", "Jason Mars"], "authorids": ["parkerhh@umich.edu", "zamirai@umich.edu", "luss@umich.edu", "ywchao@umich.edu", "mlaurenz@umich.edu", "mehrzads@umich.edu", "marios@umich.edu", "mahlke@umich.edu", "twenisch@umich.edu", "jiadeng@umich.edu", "lingjia@umich.edu", "profmars@umich.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287605641, "id": "ICLR.cc/2017/conference/-/paper367/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ_MGwqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper367/reviewers", "ICLR.cc/2017/conference/paper367/areachairs"], "cdate": 1485287605641}}}, {"tddate": null, "tmdate": 1480723930899, "tcdate": 1480723930895, "number": 2, "id": "HkQJ0t17x", "invitation": "ICLR.cc/2017/conference/-/paper367/pre-review/question", "forum": "BJ_MGwqlg", "replyto": "BJ_MGwqlg", "signatures": ["ICLR.cc/2017/conference/paper367/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper367/AnonReviewer1"], "content": {"title": "Relationship with binarization and quantization techniques", "question": "Could you please elaborate on the link with other architecture such as XNOR-NET by Rastegari and other related works (binarizing or quantizing, such as Gong et al.), aiming at compressing networks weights or activations?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Numerical Representations for Deep Neural Networks", "abstract": "With ever-increasing computational demand for deep learning, it is critical to investigate the implications of the numeric representation and precision of DNN model weights and activations on computational efficiency. In this work, we explore unconventional narrow-precision floating-point representations as it relates to inference accuracy and efficiency to steer the improved design of future DNN platforms.  We show that inference using these custom numeric representations on production-grade DNNs, including GoogLeNet and VGG, achieves an average speedup of 7.6x with less than 1% degradation in inference accuracy relative to a state-of-the-art baseline platform representing the most sophisticated hardware using single-precision floating point.  To facilitate the use of such customized precision, we also present a novel technique that drastically reduces the time required to derive the optimal precision configuration.", "pdf": "/pdf/a1a6a7b3d41ce16869ae64277f5d10c95d1df604.pdf", "TL;DR": "We find that the optimal numerical representation for large-scale DNNs is very different than the small-scale ones that are used in current DNN hardware research.", "paperhash": "hill|rethinking_numerical_representations_for_deep_neural_networks", "keywords": ["Deep learning"], "conflicts": ["umich.edu"], "authors": ["Parker Hill", "Babak Zamirai", "Shengshuo Lu", "Yu-Wei Chao", "Michael Laurenzano", "Mehrzad Samadi", "Marios Papaefthymiou", "Scott Mahlke", "Thomas Wenisch", "Jia Deng", "Lingjia Tang", "Jason Mars"], "authorids": ["parkerhh@umich.edu", "zamirai@umich.edu", "luss@umich.edu", "ywchao@umich.edu", "mlaurenz@umich.edu", "mehrzads@umich.edu", "marios@umich.edu", "mahlke@umich.edu", "twenisch@umich.edu", "jiadeng@umich.edu", "lingjia@umich.edu", "profmars@umich.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959319803, "id": "ICLR.cc/2017/conference/-/paper367/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper367/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper367/AnonReviewer2", "ICLR.cc/2017/conference/paper367/AnonReviewer1"], "reply": {"forum": "BJ_MGwqlg", "replyto": "BJ_MGwqlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper367/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper367/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959319803}}}, {"tddate": null, "tmdate": 1480682693219, "tcdate": 1480682693214, "number": 1, "id": "SkaankJXx", "invitation": "ICLR.cc/2017/conference/-/paper367/pre-review/question", "forum": "BJ_MGwqlg", "replyto": "BJ_MGwqlg", "signatures": ["ICLR.cc/2017/conference/paper367/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper367/AnonReviewer2"], "content": {"title": "training ?", "question": "Did you try training these networks using these customized precisions too ? It would be a bonus if the same hardware could be used for training."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rethinking Numerical Representations for Deep Neural Networks", "abstract": "With ever-increasing computational demand for deep learning, it is critical to investigate the implications of the numeric representation and precision of DNN model weights and activations on computational efficiency. In this work, we explore unconventional narrow-precision floating-point representations as it relates to inference accuracy and efficiency to steer the improved design of future DNN platforms.  We show that inference using these custom numeric representations on production-grade DNNs, including GoogLeNet and VGG, achieves an average speedup of 7.6x with less than 1% degradation in inference accuracy relative to a state-of-the-art baseline platform representing the most sophisticated hardware using single-precision floating point.  To facilitate the use of such customized precision, we also present a novel technique that drastically reduces the time required to derive the optimal precision configuration.", "pdf": "/pdf/a1a6a7b3d41ce16869ae64277f5d10c95d1df604.pdf", "TL;DR": "We find that the optimal numerical representation for large-scale DNNs is very different than the small-scale ones that are used in current DNN hardware research.", "paperhash": "hill|rethinking_numerical_representations_for_deep_neural_networks", "keywords": ["Deep learning"], "conflicts": ["umich.edu"], "authors": ["Parker Hill", "Babak Zamirai", "Shengshuo Lu", "Yu-Wei Chao", "Michael Laurenzano", "Mehrzad Samadi", "Marios Papaefthymiou", "Scott Mahlke", "Thomas Wenisch", "Jia Deng", "Lingjia Tang", "Jason Mars"], "authorids": ["parkerhh@umich.edu", "zamirai@umich.edu", "luss@umich.edu", "ywchao@umich.edu", "mlaurenz@umich.edu", "mehrzads@umich.edu", "marios@umich.edu", "mahlke@umich.edu", "twenisch@umich.edu", "jiadeng@umich.edu", "lingjia@umich.edu", "profmars@umich.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959319803, "id": "ICLR.cc/2017/conference/-/paper367/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper367/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper367/AnonReviewer2", "ICLR.cc/2017/conference/paper367/AnonReviewer1"], "reply": {"forum": "BJ_MGwqlg", "replyto": "BJ_MGwqlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper367/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper367/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959319803}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478287888072, "tcdate": 1478287888062, "number": 367, "id": "BJ_MGwqlg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BJ_MGwqlg", "signatures": ["~Parker_Hill1"], "readers": ["everyone"], "content": {"title": "Rethinking Numerical Representations for Deep Neural Networks", "abstract": "With ever-increasing computational demand for deep learning, it is critical to investigate the implications of the numeric representation and precision of DNN model weights and activations on computational efficiency. In this work, we explore unconventional narrow-precision floating-point representations as it relates to inference accuracy and efficiency to steer the improved design of future DNN platforms.  We show that inference using these custom numeric representations on production-grade DNNs, including GoogLeNet and VGG, achieves an average speedup of 7.6x with less than 1% degradation in inference accuracy relative to a state-of-the-art baseline platform representing the most sophisticated hardware using single-precision floating point.  To facilitate the use of such customized precision, we also present a novel technique that drastically reduces the time required to derive the optimal precision configuration.", "pdf": "/pdf/a1a6a7b3d41ce16869ae64277f5d10c95d1df604.pdf", "TL;DR": "We find that the optimal numerical representation for large-scale DNNs is very different than the small-scale ones that are used in current DNN hardware research.", "paperhash": "hill|rethinking_numerical_representations_for_deep_neural_networks", "keywords": ["Deep learning"], "conflicts": ["umich.edu"], "authors": ["Parker Hill", "Babak Zamirai", "Shengshuo Lu", "Yu-Wei Chao", "Michael Laurenzano", "Mehrzad Samadi", "Marios Papaefthymiou", "Scott Mahlke", "Thomas Wenisch", "Jia Deng", "Lingjia Tang", "Jason Mars"], "authorids": ["parkerhh@umich.edu", "zamirai@umich.edu", "luss@umich.edu", "ywchao@umich.edu", "mlaurenz@umich.edu", "mehrzads@umich.edu", "marios@umich.edu", "mahlke@umich.edu", "twenisch@umich.edu", "jiadeng@umich.edu", "lingjia@umich.edu", "profmars@umich.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 12}