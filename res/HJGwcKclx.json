{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1492426048653, "tcdate": 1478298201939, "number": 516, "id": "HJGwcKclx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HJGwcKclx", "signatures": ["~Karen_Ullrich1"], "readers": ["everyone"], "content": {"title": "Soft Weight-Sharing for Neural Network Compression", "abstract": "The success of deep learning in numerous application domains created the desire to run and train them on mobile devices. This however, conflicts with their computationally, memory and energy intense nature, leading to a growing interest in compression.\nRecent work by Han et al. (2016) propose a pipeline that involves retraining, pruning and quantization of neural network weights, obtaining state-of-the-art compression rates.\nIn this paper, we show that competitive compression rates can be achieved by using a version of \"soft weight-sharing\" (Nowlan & Hinton, 1991). Our method achieves both quantization and pruning in one simple (re-)training procedure. \nThis point of view also exposes the relation between compression and the minimum description length (MDL) principle. ", "pdf": "/pdf/30991bcc3803c08ad381efb5299066b7e9558ba2.pdf", "TL;DR": "We use soft weight-sharing to compress neural network weights.", "paperhash": "ullrich|soft_weightsharing_for_neural_network_compression", "conflicts": ["uva.nl"], "keywords": ["Deep learning", "Optimization"], "authors": ["Karen Ullrich", "Edward Meeds", "Max Welling"], "authorids": ["karen.ullrich@uva.nl", "tmeeds@gmail.com", "welling.max@gmail.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1487171126450, "tcdate": 1487171126450, "number": 6, "id": "B10VCJztl", "invitation": "ICLR.cc/2017/conference/-/paper516/public/comment", "forum": "HJGwcKclx", "replyto": "HJGwcKclx", "signatures": ["~Karen_Ullrich1"], "readers": ["everyone"], "writers": ["~Karen_Ullrich1"], "content": {"title": "UPDATE: code now available", "comment": "Along the paper we publish a little tutorial. It contains the basic functionalities. \n\nhttps://github.com/KarenUllrich/Tutorial-SoftWeightSharingForNNCompression/blob/master/tutorial.ipynb\n\nKind regards,\nKaren"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Soft Weight-Sharing for Neural Network Compression", "abstract": "The success of deep learning in numerous application domains created the desire to run and train them on mobile devices. This however, conflicts with their computationally, memory and energy intense nature, leading to a growing interest in compression.\nRecent work by Han et al. (2016) propose a pipeline that involves retraining, pruning and quantization of neural network weights, obtaining state-of-the-art compression rates.\nIn this paper, we show that competitive compression rates can be achieved by using a version of \"soft weight-sharing\" (Nowlan & Hinton, 1991). Our method achieves both quantization and pruning in one simple (re-)training procedure. \nThis point of view also exposes the relation between compression and the minimum description length (MDL) principle. ", "pdf": "/pdf/30991bcc3803c08ad381efb5299066b7e9558ba2.pdf", "TL;DR": "We use soft weight-sharing to compress neural network weights.", "paperhash": "ullrich|soft_weightsharing_for_neural_network_compression", "conflicts": ["uva.nl"], "keywords": ["Deep learning", "Optimization"], "authors": ["Karen Ullrich", "Edward Meeds", "Max Welling"], "authorids": ["karen.ullrich@uva.nl", "tmeeds@gmail.com", "welling.max@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287541888, "id": "ICLR.cc/2017/conference/-/paper516/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJGwcKclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper516/reviewers", "ICLR.cc/2017/conference/paper516/areachairs"], "cdate": 1485287541888}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396650672, "tcdate": 1486396650672, "number": 1, "id": "BJQeTz8_x", "invitation": "ICLR.cc/2017/conference/-/paper516/acceptance", "forum": "HJGwcKclx", "replyto": "HJGwcKclx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This paper provides a principled and practical formulation for weight-sharing and quantization, using a simple mixture of Guassians on the weights, and stochastic variational inference. The main idea and results are presented clearly, along with illustrative side-experiments showing the properties of this method in practice. Also, the method is illustrated on non-toy problems.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Soft Weight-Sharing for Neural Network Compression", "abstract": "The success of deep learning in numerous application domains created the desire to run and train them on mobile devices. This however, conflicts with their computationally, memory and energy intense nature, leading to a growing interest in compression.\nRecent work by Han et al. (2016) propose a pipeline that involves retraining, pruning and quantization of neural network weights, obtaining state-of-the-art compression rates.\nIn this paper, we show that competitive compression rates can be achieved by using a version of \"soft weight-sharing\" (Nowlan & Hinton, 1991). Our method achieves both quantization and pruning in one simple (re-)training procedure. \nThis point of view also exposes the relation between compression and the minimum description length (MDL) principle. ", "pdf": "/pdf/30991bcc3803c08ad381efb5299066b7e9558ba2.pdf", "TL;DR": "We use soft weight-sharing to compress neural network weights.", "paperhash": "ullrich|soft_weightsharing_for_neural_network_compression", "conflicts": ["uva.nl"], "keywords": ["Deep learning", "Optimization"], "authors": ["Karen Ullrich", "Edward Meeds", "Max Welling"], "authorids": ["karen.ullrich@uva.nl", "tmeeds@gmail.com", "welling.max@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396651257, "id": "ICLR.cc/2017/conference/-/paper516/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HJGwcKclx", "replyto": "HJGwcKclx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396651257}}}, {"tddate": null, "tmdate": 1484870220806, "tcdate": 1484870220806, "number": 5, "id": "SJH8z0RUl", "invitation": "ICLR.cc/2017/conference/-/paper516/public/comment", "forum": "HJGwcKclx", "replyto": "HJGwcKclx", "signatures": ["~Karen_Ullrich1"], "readers": ["everyone"], "writers": ["~Karen_Ullrich1"], "content": {"title": "Final update and comments", "comment": "Dear reviewers,\n\n\nWe would like to announce our final update, main changes include:\n\n(A) A Pseudo-Algorithm for more clarity of the process\n\n(B) Clarified description of the method and experiment section in particular.\n\n(C) Results for wide ResNets on CIFAR-10.\n\n(D) Proposal for prior update given extremely many parameters such as VGG in Appendix C.\n\n\n\nWe also would like to make some final comments on the procedure.\n\nWe do believe that that our method is principled and will achieve high compression rates even when the format of storing changes, because we optimize the lower bound directly. One could imagine a scenario where we store noisy weights as proposed by Hinton et al. (1992).\n\nHowever, there is to say that the process involves a lot of hyper parameters that are hard to tune.\nInteresting, also, that there seems to be a regime where the empirical prior helps improving upon pretrained results (often very significantly). For us that encourages using empirical priors for neural network training in general such as training from scratch, training in a teacher student setting or training networks with little data."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Soft Weight-Sharing for Neural Network Compression", "abstract": "The success of deep learning in numerous application domains created the desire to run and train them on mobile devices. This however, conflicts with their computationally, memory and energy intense nature, leading to a growing interest in compression.\nRecent work by Han et al. (2016) propose a pipeline that involves retraining, pruning and quantization of neural network weights, obtaining state-of-the-art compression rates.\nIn this paper, we show that competitive compression rates can be achieved by using a version of \"soft weight-sharing\" (Nowlan & Hinton, 1991). Our method achieves both quantization and pruning in one simple (re-)training procedure. \nThis point of view also exposes the relation between compression and the minimum description length (MDL) principle. ", "pdf": "/pdf/30991bcc3803c08ad381efb5299066b7e9558ba2.pdf", "TL;DR": "We use soft weight-sharing to compress neural network weights.", "paperhash": "ullrich|soft_weightsharing_for_neural_network_compression", "conflicts": ["uva.nl"], "keywords": ["Deep learning", "Optimization"], "authors": ["Karen Ullrich", "Edward Meeds", "Max Welling"], "authorids": ["karen.ullrich@uva.nl", "tmeeds@gmail.com", "welling.max@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287541888, "id": "ICLR.cc/2017/conference/-/paper516/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJGwcKclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper516/reviewers", "ICLR.cc/2017/conference/paper516/areachairs"], "cdate": 1485287541888}}}, {"tddate": null, "tmdate": 1482607707281, "tcdate": 1482607707281, "number": 4, "id": "Hy7Pnr3Eg", "invitation": "ICLR.cc/2017/conference/-/paper516/public/comment", "forum": "HJGwcKclx", "replyto": "HJGwcKclx", "signatures": ["~Karen_Ullrich1"], "readers": ["everyone"], "writers": ["~Karen_Ullrich1"], "content": {"title": "Comment on reviews", "comment": "Thank you very much for the carefully crafted reviews. We agree with most points you are making. We would like to comment on them and give an outlook of future efforts.\n \nVGG: We ran our algorithm that was successful for MNIST on ImageNet using the pretrained VGG network. However, due to some implementation inefficiencies we were only able to run it for a few epochs. Therefore, we have strong reasons to believe the network has not converged the way that LeNet had. In order to quickly verify that our method works on natural images we ran it on CIFAR10/100 and the results look in line with our LeNet experiments. Our computational bottleneck seems to stem from the updating of the prior parameters. In the future, we will be updating prior parameters with influence of less weights.  \n\nClarity: The methods and experiments section will be improved and extended to enhance its clarity. In particular, we will add an explicit algorithm. Furthermore, we will put our code up online.\n\nSpearmint: We agree with AnonReviewer1 that there is no theoretical justification for a linear relationship of accuracy and compression rate. We can derive some theoretical results under the assumption that the amount of pruned weights and the accuracy have a known relationship and considering only the storage format proposed by Han et al. (2016).\nWe will improve that in the next upcoming version.\n\nIn response to: \"In Figure 2 I am noticing two things: On the left, there is a large number of points with improved accuracy which is not the case for LeNet5-Caffe. Is there any intuition for why that's the case?\"\nNowlan and Hinton (1992) did originally propose the method to improve generalization. They offered it as an alternative to convolutional weight-sharing. I think for this fully connected architecture that is exactly what is happening. The network has more room for generalization because there has been no other form of regularization.\n\nWe hope to be able to report more soon ... next year :).\n\nTill than happy Christmas and New Year."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Soft Weight-Sharing for Neural Network Compression", "abstract": "The success of deep learning in numerous application domains created the desire to run and train them on mobile devices. This however, conflicts with their computationally, memory and energy intense nature, leading to a growing interest in compression.\nRecent work by Han et al. (2016) propose a pipeline that involves retraining, pruning and quantization of neural network weights, obtaining state-of-the-art compression rates.\nIn this paper, we show that competitive compression rates can be achieved by using a version of \"soft weight-sharing\" (Nowlan & Hinton, 1991). Our method achieves both quantization and pruning in one simple (re-)training procedure. \nThis point of view also exposes the relation between compression and the minimum description length (MDL) principle. ", "pdf": "/pdf/30991bcc3803c08ad381efb5299066b7e9558ba2.pdf", "TL;DR": "We use soft weight-sharing to compress neural network weights.", "paperhash": "ullrich|soft_weightsharing_for_neural_network_compression", "conflicts": ["uva.nl"], "keywords": ["Deep learning", "Optimization"], "authors": ["Karen Ullrich", "Edward Meeds", "Max Welling"], "authorids": ["karen.ullrich@uva.nl", "tmeeds@gmail.com", "welling.max@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287541888, "id": "ICLR.cc/2017/conference/-/paper516/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJGwcKclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper516/reviewers", "ICLR.cc/2017/conference/paper516/areachairs"], "cdate": 1485287541888}}}, {"tddate": null, "tmdate": 1481920696170, "tcdate": 1481920696170, "number": 3, "id": "Skl6x0-El", "invitation": "ICLR.cc/2017/conference/-/paper516/official/review", "forum": "HJGwcKclx", "replyto": "HJGwcKclx", "signatures": ["ICLR.cc/2017/conference/paper516/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper516/AnonReviewer1"], "content": {"title": "Empirical Bayesian learning applied for neural net parameter compression", "rating": "7: Good paper, accept", "review": "This paper proposes to use an empirical Bayesian approach to learn the parameters of a neural network, and their priors.\nA mixture model prior over the weights leads to a clustering effect in the weight posterior distributions (which are approximated with delta peaks). \nThis clustering effect can exploited for parameter quantisation and compression of the network parameters.\nThe authors show that this leads to compression rates and predictive accuracy comparable to related approaches. \n\nEarlier work [Han et al. 2015] is based on a three-stage process of pruning small magnitude weights, clustering the remaining ones, and updating the cluster centres to optimise performance. The current work provides a more principled approach that does not have such an ad-hoc multi-stage structure, but a single iterative optimisation process.\n\nA first experiment, described in section 6.1 shows that an empirical Bayes\u2019 approach, without the use of hyper priors, already leads to a pronounced clustering effect and to setting many weights to zero. \nIn particular a compression rate of 64.2 is obtained on the LeNet300-100 model.\nIn section 6.1 the text refers to figure C, I suppose this should be figure 1.\n\nSection 6.2 describes an experiment where hyper-priors are used, and the parameters of these distributions, as well as other hyper-parameters such as the learning rates, are being optimised using Spearmint (Snoek et al., 2012). Figure 2 shows the performance of the  different points in the hyper-parameter space that have been evaluated (each trained network gives an accuracy-compressionrate point in the graph). The text claims that best results lie on a line, this seems a little opportunistic interpretation given the limited data. Moreover, it would be useful to add a small discussion on whether such a linear relationship would be expected or not. Currently the results of this experiment lack interpretation.\n\nSection 6.3 describes results obtained for both CNN models and compares results to the recent results of (Han et al., 2015) and (Guo et al., 2016).\nComparable results are obtained in terms of compression rate and accuracy. \nThe authors state that their current algorithm is too slow to be useful for larger models such as VGG-19, but they do briefly report some results obtained for this model (but do not compare to related work). It would be useful here to explain what slows the training down with respect to standard training without the weight clustering approach, and how the proposed algorithm scales in terms of the relevant quantities of the data and the model.\n\nThe contribution of this paper is mostly experimental, leveraging fairly standard ideas from empirical Bayesian learning to introduce weight clustering effects in CNN training.\nThis being said, it is an interesting result that such a relatively straightforward approach leads to results that are on par with state-of-the-art, but more ad-hoc, network compression techniques.\nThe paper could be improved by clearly describing the algorithm used for training, and how it scales to large networks and datasets.\nAnother point that would deserve further discussion is how the hyper-parameter search is performed ( not using test data I assume), and how the compared methods dealt with the search over hyper-parameters to determine the accuracy-compression tradeoff. Ideally, I think, methods should be evaluated across different points on this trade-off.\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Soft Weight-Sharing for Neural Network Compression", "abstract": "The success of deep learning in numerous application domains created the desire to run and train them on mobile devices. This however, conflicts with their computationally, memory and energy intense nature, leading to a growing interest in compression.\nRecent work by Han et al. (2016) propose a pipeline that involves retraining, pruning and quantization of neural network weights, obtaining state-of-the-art compression rates.\nIn this paper, we show that competitive compression rates can be achieved by using a version of \"soft weight-sharing\" (Nowlan & Hinton, 1991). Our method achieves both quantization and pruning in one simple (re-)training procedure. \nThis point of view also exposes the relation between compression and the minimum description length (MDL) principle. ", "pdf": "/pdf/30991bcc3803c08ad381efb5299066b7e9558ba2.pdf", "TL;DR": "We use soft weight-sharing to compress neural network weights.", "paperhash": "ullrich|soft_weightsharing_for_neural_network_compression", "conflicts": ["uva.nl"], "keywords": ["Deep learning", "Optimization"], "authors": ["Karen Ullrich", "Edward Meeds", "Max Welling"], "authorids": ["karen.ullrich@uva.nl", "tmeeds@gmail.com", "welling.max@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512556953, "id": "ICLR.cc/2017/conference/-/paper516/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper516/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper516/AnonReviewer4", "ICLR.cc/2017/conference/paper516/AnonReviewer3", "ICLR.cc/2017/conference/paper516/AnonReviewer1"], "reply": {"forum": "HJGwcKclx", "replyto": "HJGwcKclx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper516/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper516/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512556953}}}, {"tddate": null, "tmdate": 1481906697248, "tcdate": 1481906697248, "number": 2, "id": "Hy-z55WNg", "invitation": "ICLR.cc/2017/conference/-/paper516/official/review", "forum": "HJGwcKclx", "replyto": "HJGwcKclx", "signatures": ["ICLR.cc/2017/conference/paper516/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper516/AnonReviewer3"], "content": {"title": "Nice idea, some minor issues", "rating": "7: Good paper, accept", "review": "The authors propose a method to compress neural networks by retraining them while putting a mixture of Gaussians prior on the weights with learned means and variances which then can be used to compress the neural network by first setting all weights to the mean of their infered mixture component (resulting in a possible loss of precision) and storing the network in a format which saves only the fixture index and exploits the sparseness of the weights that was enforced in training.\n\nQuality:\nOf course it is a serious drawback that the method doesn't seem to work on VGG which would render the method unusable for production (as it is right now, maybe this can be improved). I guess AlexNet takes too long to process, too, otherwise this might be a very valuable addition.\nIn Figure 2 I am noticing two things: On the left, there is a large number of points with improved accuracy which is not the case for LeNet5-Caffe. Is there any intuition for why that's the case? Additionally regarding the spearmint optimization: Do they authors have found any clues about which hyperparameter settings worked well? This might be helpful for other people trying to apply this method.\nI really like Figure 7 in it's latest version.\n\nClarity:\nEspecially section 2 on MDL is written very well and gives a nice theoretic introduction. Sections 4, 5 and 6 are very short but seem to contain most relevant information. It might be helpful to have at least some more details about the used models in the paper (maybe the number of layers and the number of parameters).\nIn 6.1 the authors claim \"Even though most variances seem to be reasonable small there are some that are large\". From figure 1 this is very hard to assess, especially as the vertical histogram essentially shows only the zero component. It might be helpful to have either a log histogram or separate histograms for each componenent. What are the large points in Figure 2 as opposed to the smaller ones? They seem to have a very good compression/accuracy loss ratio, is that it?\nSome other points are listed below\n\noriginality: While there has been some work on compressing neural networks by using a reduced number of bits to store the parameters and exploiting sparsity structure, I like the idea to directly learn the quantization by means of a gaussian mixture prior in retraining which seems to be more principled than other approaches\n\nsignificance: The method achievs state-of-the-art performance on the two shown examples on MNIST, however these networks are far from the deep networks used in state-of-the-art models. This obviously is a drawback for the practical usability of the methods and therefor it's significance. If the method could be made to work on more state-of-the-art networks like VGG or ResNet, I would consider this a contribution of high significance.\n\nMinor issues:\n\npage 1: There seems to be a space in front of the first author's name\npage 3: \"in this scenario, pi_0 may be fixed...\". Missing backslash in TeX?\npage 6: 6.2: two wrong blanks in \"the number of components_, \\tau_.\"\npage 6, 6.3: \"in experiences with VGG\": In experiments?\npage 12: \"Figure C\": Figure 7?\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Soft Weight-Sharing for Neural Network Compression", "abstract": "The success of deep learning in numerous application domains created the desire to run and train them on mobile devices. This however, conflicts with their computationally, memory and energy intense nature, leading to a growing interest in compression.\nRecent work by Han et al. (2016) propose a pipeline that involves retraining, pruning and quantization of neural network weights, obtaining state-of-the-art compression rates.\nIn this paper, we show that competitive compression rates can be achieved by using a version of \"soft weight-sharing\" (Nowlan & Hinton, 1991). Our method achieves both quantization and pruning in one simple (re-)training procedure. \nThis point of view also exposes the relation between compression and the minimum description length (MDL) principle. ", "pdf": "/pdf/30991bcc3803c08ad381efb5299066b7e9558ba2.pdf", "TL;DR": "We use soft weight-sharing to compress neural network weights.", "paperhash": "ullrich|soft_weightsharing_for_neural_network_compression", "conflicts": ["uva.nl"], "keywords": ["Deep learning", "Optimization"], "authors": ["Karen Ullrich", "Edward Meeds", "Max Welling"], "authorids": ["karen.ullrich@uva.nl", "tmeeds@gmail.com", "welling.max@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512556953, "id": "ICLR.cc/2017/conference/-/paper516/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper516/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper516/AnonReviewer4", "ICLR.cc/2017/conference/paper516/AnonReviewer3", "ICLR.cc/2017/conference/paper516/AnonReviewer1"], "reply": {"forum": "HJGwcKclx", "replyto": "HJGwcKclx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper516/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper516/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512556953}}}, {"tddate": null, "tmdate": 1481904562272, "tcdate": 1481904562272, "number": 1, "id": "SJc3b9bNl", "invitation": "ICLR.cc/2017/conference/-/paper516/official/review", "forum": "HJGwcKclx", "replyto": "HJGwcKclx", "signatures": ["ICLR.cc/2017/conference/paper516/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper516/AnonReviewer4"], "content": {"title": "nice tie-in to classic NN literature through lens of modern engineering needs", "rating": "7: Good paper, accept", "review": "This paper revives a classic idea involving regularization for purposes of compression for modern CNN models on resource constrained devices. Model compression is hot and we're in the midst of lots of people rediscovering old ideas in this area so it is nice to have a paper that explicitly draws upon classic approaches from the early 90s to obtain competitive results on standard benchmarks.\n\nThere's not too much to say here: this study is an instance of a simple idea applied effectively to an important problem, written up in an illuminating manner with appropriate references to classic approaches. The addition of the filter visualizations enhances the contribution.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Soft Weight-Sharing for Neural Network Compression", "abstract": "The success of deep learning in numerous application domains created the desire to run and train them on mobile devices. This however, conflicts with their computationally, memory and energy intense nature, leading to a growing interest in compression.\nRecent work by Han et al. (2016) propose a pipeline that involves retraining, pruning and quantization of neural network weights, obtaining state-of-the-art compression rates.\nIn this paper, we show that competitive compression rates can be achieved by using a version of \"soft weight-sharing\" (Nowlan & Hinton, 1991). Our method achieves both quantization and pruning in one simple (re-)training procedure. \nThis point of view also exposes the relation between compression and the minimum description length (MDL) principle. ", "pdf": "/pdf/30991bcc3803c08ad381efb5299066b7e9558ba2.pdf", "TL;DR": "We use soft weight-sharing to compress neural network weights.", "paperhash": "ullrich|soft_weightsharing_for_neural_network_compression", "conflicts": ["uva.nl"], "keywords": ["Deep learning", "Optimization"], "authors": ["Karen Ullrich", "Edward Meeds", "Max Welling"], "authorids": ["karen.ullrich@uva.nl", "tmeeds@gmail.com", "welling.max@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512556953, "id": "ICLR.cc/2017/conference/-/paper516/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper516/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper516/AnonReviewer4", "ICLR.cc/2017/conference/paper516/AnonReviewer3", "ICLR.cc/2017/conference/paper516/AnonReviewer1"], "reply": {"forum": "HJGwcKclx", "replyto": "HJGwcKclx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper516/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper516/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512556953}}}, {"tddate": null, "tmdate": 1481903485543, "tcdate": 1481903485543, "number": 3, "id": "SyStTYW4g", "invitation": "ICLR.cc/2017/conference/-/paper516/public/comment", "forum": "HJGwcKclx", "replyto": "H1lFChRmg", "signatures": ["~Karen_Ullrich1"], "readers": ["everyone"], "writers": ["~Karen_Ullrich1"], "content": {"title": "Updated figures", "comment": "We updated the figures for more clearness."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Soft Weight-Sharing for Neural Network Compression", "abstract": "The success of deep learning in numerous application domains created the desire to run and train them on mobile devices. This however, conflicts with their computationally, memory and energy intense nature, leading to a growing interest in compression.\nRecent work by Han et al. (2016) propose a pipeline that involves retraining, pruning and quantization of neural network weights, obtaining state-of-the-art compression rates.\nIn this paper, we show that competitive compression rates can be achieved by using a version of \"soft weight-sharing\" (Nowlan & Hinton, 1991). Our method achieves both quantization and pruning in one simple (re-)training procedure. \nThis point of view also exposes the relation between compression and the minimum description length (MDL) principle. ", "pdf": "/pdf/30991bcc3803c08ad381efb5299066b7e9558ba2.pdf", "TL;DR": "We use soft weight-sharing to compress neural network weights.", "paperhash": "ullrich|soft_weightsharing_for_neural_network_compression", "conflicts": ["uva.nl"], "keywords": ["Deep learning", "Optimization"], "authors": ["Karen Ullrich", "Edward Meeds", "Max Welling"], "authorids": ["karen.ullrich@uva.nl", "tmeeds@gmail.com", "welling.max@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287541888, "id": "ICLR.cc/2017/conference/-/paper516/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJGwcKclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper516/reviewers", "ICLR.cc/2017/conference/paper516/areachairs"], "cdate": 1485287541888}}}, {"tddate": null, "tmdate": 1481719416083, "tcdate": 1481719416077, "number": 1, "id": "H1lFChRmg", "invitation": "ICLR.cc/2017/conference/-/paper516/official/comment", "forum": "HJGwcKclx", "replyto": "BkjuOHp7l", "signatures": ["ICLR.cc/2017/conference/paper516/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper516/AnonReviewer3"], "content": {"title": "Question regarding Figure 7", "comment": "Thanks for Figure 7! I have a question about it: how are the filters displayed? If they are convolutional, they should be four dimensional (or for each output channel three dimensional). But since the displayed filters are two dimensional, I am missing one dimension (which could be flattened or only one slice of this dimension could be shown). Could you clarify this?\n\nAdditionally, the second sentence in appendix C (\"For some of the feature maps from layer 2 seem to be redundant hence the almost empty columns.\") seems to have some syntactical error. Maybe you meant \"Some of the...\"? With respect to my original question, this sentence again would be easier to understand if you indicate which direction in the figure shows the different feature maps."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Soft Weight-Sharing for Neural Network Compression", "abstract": "The success of deep learning in numerous application domains created the desire to run and train them on mobile devices. This however, conflicts with their computationally, memory and energy intense nature, leading to a growing interest in compression.\nRecent work by Han et al. (2016) propose a pipeline that involves retraining, pruning and quantization of neural network weights, obtaining state-of-the-art compression rates.\nIn this paper, we show that competitive compression rates can be achieved by using a version of \"soft weight-sharing\" (Nowlan & Hinton, 1991). Our method achieves both quantization and pruning in one simple (re-)training procedure. \nThis point of view also exposes the relation between compression and the minimum description length (MDL) principle. ", "pdf": "/pdf/30991bcc3803c08ad381efb5299066b7e9558ba2.pdf", "TL;DR": "We use soft weight-sharing to compress neural network weights.", "paperhash": "ullrich|soft_weightsharing_for_neural_network_compression", "conflicts": ["uva.nl"], "keywords": ["Deep learning", "Optimization"], "authors": ["Karen Ullrich", "Edward Meeds", "Max Welling"], "authorids": ["karen.ullrich@uva.nl", "tmeeds@gmail.com", "welling.max@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287541758, "id": "ICLR.cc/2017/conference/-/paper516/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "HJGwcKclx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper516/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper516/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper516/reviewers", "ICLR.cc/2017/conference/paper516/areachairs"], "cdate": 1485287541758}}}, {"tddate": null, "tmdate": 1481623666643, "tcdate": 1481623666636, "number": 2, "id": "BkjuOHp7l", "invitation": "ICLR.cc/2017/conference/-/paper516/public/comment", "forum": "HJGwcKclx", "replyto": "HJGwcKclx", "signatures": ["~Karen_Ullrich1"], "readers": ["everyone"], "writers": ["~Karen_Ullrich1"], "content": {"title": "Paper Update", "comment": "We present an updated version of the paper:\n\n(1)  We clarified the section that connects MDL,variational learning and compression and the methods section.\n\n(2) We added the results from a spearmint experiment. \n\n(3) \"We achieve state-of-the-art compression rates in both examples. However, for large networks such as VGG with 138 million parameters the algorithm as is, is too slow to get usable results. In experiences with VGG we were able to prune 93% of the weights without loss of accuracy, however, the quantization step resulted in significant loss of accuracy. We think this is due to the network not having convergened.\"\n\n(4) Visualizations of the filters of the second convolutional layer of LeNet-5-Caffe."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Soft Weight-Sharing for Neural Network Compression", "abstract": "The success of deep learning in numerous application domains created the desire to run and train them on mobile devices. This however, conflicts with their computationally, memory and energy intense nature, leading to a growing interest in compression.\nRecent work by Han et al. (2016) propose a pipeline that involves retraining, pruning and quantization of neural network weights, obtaining state-of-the-art compression rates.\nIn this paper, we show that competitive compression rates can be achieved by using a version of \"soft weight-sharing\" (Nowlan & Hinton, 1991). Our method achieves both quantization and pruning in one simple (re-)training procedure. \nThis point of view also exposes the relation between compression and the minimum description length (MDL) principle. ", "pdf": "/pdf/30991bcc3803c08ad381efb5299066b7e9558ba2.pdf", "TL;DR": "We use soft weight-sharing to compress neural network weights.", "paperhash": "ullrich|soft_weightsharing_for_neural_network_compression", "conflicts": ["uva.nl"], "keywords": ["Deep learning", "Optimization"], "authors": ["Karen Ullrich", "Edward Meeds", "Max Welling"], "authorids": ["karen.ullrich@uva.nl", "tmeeds@gmail.com", "welling.max@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287541888, "id": "ICLR.cc/2017/conference/-/paper516/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJGwcKclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper516/reviewers", "ICLR.cc/2017/conference/paper516/areachairs"], "cdate": 1485287541888}}}, {"tddate": null, "tmdate": 1481253332651, "tcdate": 1481253332642, "number": 2, "id": "ryaA-ivmx", "invitation": "ICLR.cc/2017/conference/-/paper516/pre-review/question", "forum": "HJGwcKclx", "replyto": "HJGwcKclx", "signatures": ["ICLR.cc/2017/conference/paper516/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper516/AnonReviewer4"], "content": {"title": "filter visualizations", "question": "I am also interesting in seeing (or getting a description) of any filter visualizations you might be able to compute. This could give us additional ideas for variants of weight sharing that are influenced by old ideas involving filterbanks and sparsity."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Soft Weight-Sharing for Neural Network Compression", "abstract": "The success of deep learning in numerous application domains created the desire to run and train them on mobile devices. This however, conflicts with their computationally, memory and energy intense nature, leading to a growing interest in compression.\nRecent work by Han et al. (2016) propose a pipeline that involves retraining, pruning and quantization of neural network weights, obtaining state-of-the-art compression rates.\nIn this paper, we show that competitive compression rates can be achieved by using a version of \"soft weight-sharing\" (Nowlan & Hinton, 1991). Our method achieves both quantization and pruning in one simple (re-)training procedure. \nThis point of view also exposes the relation between compression and the minimum description length (MDL) principle. ", "pdf": "/pdf/30991bcc3803c08ad381efb5299066b7e9558ba2.pdf", "TL;DR": "We use soft weight-sharing to compress neural network weights.", "paperhash": "ullrich|soft_weightsharing_for_neural_network_compression", "conflicts": ["uva.nl"], "keywords": ["Deep learning", "Optimization"], "authors": ["Karen Ullrich", "Edward Meeds", "Max Welling"], "authorids": ["karen.ullrich@uva.nl", "tmeeds@gmail.com", "welling.max@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481253333300, "id": "ICLR.cc/2017/conference/-/paper516/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper516/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper516/AnonReviewer3", "ICLR.cc/2017/conference/paper516/AnonReviewer4"], "reply": {"forum": "HJGwcKclx", "replyto": "HJGwcKclx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper516/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper516/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481253333300}}}, {"tddate": null, "tmdate": 1480595831939, "tcdate": 1480595831934, "number": 1, "id": "SketKqpzl", "invitation": "ICLR.cc/2017/conference/-/paper516/public/comment", "forum": "HJGwcKclx", "replyto": "HyAgtssfe", "signatures": ["~Karen_Ullrich1"], "readers": ["everyone"], "writers": ["~Karen_Ullrich1"], "content": {"title": "Answer to AnonReviewer3:", "comment": "\n\n(A) Translation to convolutional layers: Indeed similar results can be derived for convolutional layers. The pruning rate seems to be related to the total number of parameters in a layer rather than the position of the layer within the graph or its type.\n\n(B) Location of pruned weights:  We are planning to include filter visualisations, pre and post (re-) training. Although many filters can be removed, this is due to the enormous amount of weights being pruned. We emphasis that our approach does not explicitly regularise structure; however, in general the proposed framework does allow for such generalization as mentioned in future work.\n\n(C) VGG results: Unfortunately training a network of that size is relatively computationally expensive. Our preliminary results show pruned weights 93% (SOTA) without loss in accuracy.  We obviously intend to put the final results in the paper as soon as possible.\n\nThank you for your comments and patience."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Soft Weight-Sharing for Neural Network Compression", "abstract": "The success of deep learning in numerous application domains created the desire to run and train them on mobile devices. This however, conflicts with their computationally, memory and energy intense nature, leading to a growing interest in compression.\nRecent work by Han et al. (2016) propose a pipeline that involves retraining, pruning and quantization of neural network weights, obtaining state-of-the-art compression rates.\nIn this paper, we show that competitive compression rates can be achieved by using a version of \"soft weight-sharing\" (Nowlan & Hinton, 1991). Our method achieves both quantization and pruning in one simple (re-)training procedure. \nThis point of view also exposes the relation between compression and the minimum description length (MDL) principle. ", "pdf": "/pdf/30991bcc3803c08ad381efb5299066b7e9558ba2.pdf", "TL;DR": "We use soft weight-sharing to compress neural network weights.", "paperhash": "ullrich|soft_weightsharing_for_neural_network_compression", "conflicts": ["uva.nl"], "keywords": ["Deep learning", "Optimization"], "authors": ["Karen Ullrich", "Edward Meeds", "Max Welling"], "authorids": ["karen.ullrich@uva.nl", "tmeeds@gmail.com", "welling.max@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287541888, "id": "ICLR.cc/2017/conference/-/paper516/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJGwcKclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper516/reviewers", "ICLR.cc/2017/conference/paper516/areachairs"], "cdate": 1485287541888}}}, {"tddate": null, "tmdate": 1480468726031, "tcdate": 1480468726025, "number": 1, "id": "HyAgtssfe", "invitation": "ICLR.cc/2017/conference/-/paper516/pre-review/question", "forum": "HJGwcKclx", "replyto": "HJGwcKclx", "signatures": ["ICLR.cc/2017/conference/paper516/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper516/AnonReviewer3"], "content": {"title": "Question regarding the zero weights after compression", "question": "In 4.5 you write that \"96% of the first layer (235K parameter), 90% of the second (30K) and only 18% of the\nfinal layer (10K) are being pruned.\". Especially for the first two layers it would be very interesting to know how the zeros are distributed. E.g. it could be whole filters set to zero which would suggest that not as many convolutional filters are needed. Or the filters themselves could be much sparser in space or in the input channels. Maybe the convolutions are even reduced to 1x1 convolutions which would basically be a \"pixelwise\" nonlinearity.\n\nAlso it would be interesting to know whether these results (most zeros in early layers) translate to the VGG network's compression."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Soft Weight-Sharing for Neural Network Compression", "abstract": "The success of deep learning in numerous application domains created the desire to run and train them on mobile devices. This however, conflicts with their computationally, memory and energy intense nature, leading to a growing interest in compression.\nRecent work by Han et al. (2016) propose a pipeline that involves retraining, pruning and quantization of neural network weights, obtaining state-of-the-art compression rates.\nIn this paper, we show that competitive compression rates can be achieved by using a version of \"soft weight-sharing\" (Nowlan & Hinton, 1991). Our method achieves both quantization and pruning in one simple (re-)training procedure. \nThis point of view also exposes the relation between compression and the minimum description length (MDL) principle. ", "pdf": "/pdf/30991bcc3803c08ad381efb5299066b7e9558ba2.pdf", "TL;DR": "We use soft weight-sharing to compress neural network weights.", "paperhash": "ullrich|soft_weightsharing_for_neural_network_compression", "conflicts": ["uva.nl"], "keywords": ["Deep learning", "Optimization"], "authors": ["Karen Ullrich", "Edward Meeds", "Max Welling"], "authorids": ["karen.ullrich@uva.nl", "tmeeds@gmail.com", "welling.max@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481253333300, "id": "ICLR.cc/2017/conference/-/paper516/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper516/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper516/AnonReviewer3", "ICLR.cc/2017/conference/paper516/AnonReviewer4"], "reply": {"forum": "HJGwcKclx", "replyto": "HJGwcKclx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper516/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper516/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481253333300}}}], "count": 14}