{"notes": [{"id": "7R7fAoUygoa", "original": "pyvwxs27VA6", "number": 1938, "cdate": 1601308213573, "ddate": null, "tcdate": 1601308213573, "tmdate": 1615958890154, "tddate": null, "forum": "7R7fAoUygoa", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Optimal Regularization can Mitigate Double Descent", "authorids": ["~Preetum_Nakkiran1", "pvenkat@g.harvard.edu", "~Sham_M._Kakade1", "~Tengyu_Ma1"], "authors": ["Preetum Nakkiran", "Prayaag Venkat", "Sham M. Kakade", "Tengyu Ma"], "keywords": ["double descent", "generalization", "regularization", "regression", "monotonicity"], "abstract": "Recent empirical and theoretical studies have shown that many learning algorithms -- from linear regression to neural networks -- can have test performance that is non-monotonic in quantities such the sample size and model size. This striking phenomenon, often referred to as \"double descent\", has raised questions of if we need to re-think our current understanding of generalization. In this work, we study whether the double-descent phenomenon can be avoided by using optimal regularization. Theoretically, we prove that for certain linear regression models with isotropic data distribution, optimally-tuned $\\ell_2$ regularization achieves monotonic test performance as we grow either the sample size or the model size.\nWe also demonstrate empirically that optimally-tuned $\\ell_2$ regularization can mitigate double descent for more general models, including neural networks.\nOur results suggest that it may also be informative to study the test risk scalings of various algorithms in the context of appropriately tuned regularization.", "one-sentence_summary": "Optimal regularization can provably avoid double-descent in certain settings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nakkiran|optimal_regularization_can_mitigate_double_descent", "pdf": "/pdf/3424b7750a87532de0707e6ced4dd6f62ee9ca29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnakkiran2021optimal,\ntitle={Optimal Regularization can Mitigate Double Descent},\nauthor={Preetum Nakkiran and Prayaag Venkat and Sham M. Kakade and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7R7fAoUygoa}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Oyuvlu7u2O0", "original": null, "number": 1, "cdate": 1610040525010, "ddate": null, "tcdate": 1610040525010, "tmdate": 1610474134112, "tddate": null, "forum": "7R7fAoUygoa", "replyto": "7R7fAoUygoa", "invitation": "ICLR.cc/2021/Conference/Paper1938/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "Quality: the paper takes an important question and analyzes it well from a theoretical angle; it also provides empirical evidence to back up its main message in more complex models. The proofs are non-trivial. The paper adds value in improving our understanding of the double descent phenomenon by providing a clear picture of the non-asymptotic regime.\n\nClarity: The motivation of studying the double-descent phenomenon with optimal regularization is well-explained in the introduction. Connections and comparisons with existing related works are discussed clearly. The paper is clearly written, and exposes the results in a clear and accessible fashion. \n\nOriginality: The presented theoretical results on the linear regression model are non-asymptotic, which is new and different from existing works. \n\nSignificance: The proof techniques seem to heavily depend on the specific choice of the loss function and the regularizer, that is, the mean squared loss and the ridge penalty. It is not clear if the techniques can generalize to other settings, which affects its significance.\n\nMain Pros:\n- the paper takes an important question and analyzes it well from a theoretical angle. The proofs are non-trivial; the paper adds value in improving our understanding of the double descent phenomenon by providing a clear picture of the non-asymptotic regime.\n\nMain Cons:\n- Generality of the results. The paper mainly focuses on a simplified linear regression model, where the response variable is linearly generated using some ground-truth parameters \\beta^*. \n- The experiments need to be more extensive and better-explained, especially for the CIFAR-100 experiments. It is important to discuss this difference clearly at the beginning. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Regularization can Mitigate Double Descent", "authorids": ["~Preetum_Nakkiran1", "pvenkat@g.harvard.edu", "~Sham_M._Kakade1", "~Tengyu_Ma1"], "authors": ["Preetum Nakkiran", "Prayaag Venkat", "Sham M. Kakade", "Tengyu Ma"], "keywords": ["double descent", "generalization", "regularization", "regression", "monotonicity"], "abstract": "Recent empirical and theoretical studies have shown that many learning algorithms -- from linear regression to neural networks -- can have test performance that is non-monotonic in quantities such the sample size and model size. This striking phenomenon, often referred to as \"double descent\", has raised questions of if we need to re-think our current understanding of generalization. In this work, we study whether the double-descent phenomenon can be avoided by using optimal regularization. Theoretically, we prove that for certain linear regression models with isotropic data distribution, optimally-tuned $\\ell_2$ regularization achieves monotonic test performance as we grow either the sample size or the model size.\nWe also demonstrate empirically that optimally-tuned $\\ell_2$ regularization can mitigate double descent for more general models, including neural networks.\nOur results suggest that it may also be informative to study the test risk scalings of various algorithms in the context of appropriately tuned regularization.", "one-sentence_summary": "Optimal regularization can provably avoid double-descent in certain settings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nakkiran|optimal_regularization_can_mitigate_double_descent", "pdf": "/pdf/3424b7750a87532de0707e6ced4dd6f62ee9ca29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnakkiran2021optimal,\ntitle={Optimal Regularization can Mitigate Double Descent},\nauthor={Preetum Nakkiran and Prayaag Venkat and Sham M. Kakade and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7R7fAoUygoa}\n}"}, "tags": [], "invitation": {"reply": {"forum": "7R7fAoUygoa", "replyto": "7R7fAoUygoa", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040524997, "tmdate": 1610474134096, "id": "ICLR.cc/2021/Conference/Paper1938/-/Decision"}}}, {"id": "9lknDoIrEpm", "original": null, "number": 4, "cdate": 1604268508146, "ddate": null, "tcdate": 1604268508146, "tmdate": 1606780576274, "tddate": null, "forum": "7R7fAoUygoa", "replyto": "7R7fAoUygoa", "invitation": "ICLR.cc/2021/Conference/Paper1938/-/Official_Review", "content": {"title": "Nice paper addressing the role of regularization in mitigating double descent", "review": "he paper studies the surprising phenomenon of \u201cdouble descent\u201d in machine learning models which has recently come into light through many prior works. The phenomenon is used to describe the behavior of test performance of an estimator as the model parameters (complexity) or the number of samples are increased. It is observed that in many cases, the test error first decreases then increases attaining a peak and then starts to decrease again as the number of model parameters (or samples) are increased. This pervades many different models including neural networks, decision trees and linear regression.\nPrior works have noted that this occurs primarily for unregularized or under-regularized models and that leads to the motivating question for this work: can optimal regularization remove double-descent? The paper studies this question from a theoretical and empirical perspective. \nTheoretical: For the setting of linear regression the authors show that an optimal amount of l2 regularizer added to the objective completely removes the double descent phenomenon with respect to both number of samples and number of model parameters. \nEmpirical: For random feature classifiers and convolutional neural networks, the authors empirically show that an optimal amount of l2 regularization removes the appearance of double descent curves.\n\nIn terms of the theoretical analysis provided in this paper, a highlight is that the results are non-asymptotic and the Theorems identify precise values of the different parameters when the double descent phenomenon disappears. \nThe theoretical results have the caveat that they apply only when the input data is Gaussian. If the data is not Gaussian, l2 regularization might not be able to remove double descent as the authors demonstrate via the means of a counterexample. Moreover, if the covariates are not isotropic, then the authors conjecture but cant prove that optimal l2 regularization suffices.\n\nIn my view, the paper takes an important question and analyzes it well from a theoretical angle and also provides empirical evidence to back up its main message in more complex models. The proofs are non-trivial and I think the paper adds value in improving our understanding of the double descent phenomenon by providing a clear picture of the non-asymptotic regime.\n\n\nQuestions:\n1. If I recall prior works correctly, many of them point out that double descent continues to hold even under different regularization schemes both theoretically and empirically. What changed here or what were the prior works missing? I am asking this because based on prior work your result sends a conflicting message that we can get rid of double descent and also not sacrifice performance at the same time.\n2. Curious to hear what happens if we look at deeper neural networks? Could the optimal regularization amount be so high that it starts to hurt performance?\n3. Equation (9) in the Appendix would benefit from more explanation. In particular, the calculation showing how we arrive at the singular values of the matrix (X^TX + lambdaI)^-1X^T.\n\nMinor typos:\n1. Abstract: \u201cquantities such as the \u2026\u201d\n2. Extra bracket in definition of lambda_n^opt on page 4\n\n\n-------------\n\nThank you to the authors for their response. It has helped clear some questions I had in mind. I am keeping my rating.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1938/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1938/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Regularization can Mitigate Double Descent", "authorids": ["~Preetum_Nakkiran1", "pvenkat@g.harvard.edu", "~Sham_M._Kakade1", "~Tengyu_Ma1"], "authors": ["Preetum Nakkiran", "Prayaag Venkat", "Sham M. Kakade", "Tengyu Ma"], "keywords": ["double descent", "generalization", "regularization", "regression", "monotonicity"], "abstract": "Recent empirical and theoretical studies have shown that many learning algorithms -- from linear regression to neural networks -- can have test performance that is non-monotonic in quantities such the sample size and model size. This striking phenomenon, often referred to as \"double descent\", has raised questions of if we need to re-think our current understanding of generalization. In this work, we study whether the double-descent phenomenon can be avoided by using optimal regularization. Theoretically, we prove that for certain linear regression models with isotropic data distribution, optimally-tuned $\\ell_2$ regularization achieves monotonic test performance as we grow either the sample size or the model size.\nWe also demonstrate empirically that optimally-tuned $\\ell_2$ regularization can mitigate double descent for more general models, including neural networks.\nOur results suggest that it may also be informative to study the test risk scalings of various algorithms in the context of appropriately tuned regularization.", "one-sentence_summary": "Optimal regularization can provably avoid double-descent in certain settings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nakkiran|optimal_regularization_can_mitigate_double_descent", "pdf": "/pdf/3424b7750a87532de0707e6ced4dd6f62ee9ca29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnakkiran2021optimal,\ntitle={Optimal Regularization can Mitigate Double Descent},\nauthor={Preetum Nakkiran and Prayaag Venkat and Sham M. Kakade and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7R7fAoUygoa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "7R7fAoUygoa", "replyto": "7R7fAoUygoa", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1938/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538107456, "tmdate": 1606915807121, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1938/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1938/-/Official_Review"}}}, {"id": "0uLR7_q331c", "original": null, "number": 3, "cdate": 1603976890338, "ddate": null, "tcdate": 1603976890338, "tmdate": 1606771671075, "tddate": null, "forum": "7R7fAoUygoa", "replyto": "7R7fAoUygoa", "invitation": "ICLR.cc/2021/Conference/Paper1938/-/Official_Review", "content": {"title": "Fundamental research question, insightful theoretical and experimental results", "review": "This paper studies the double descent phenomena -- for increasing sample/model size -- in linear ridge regression. When the model is well-specified and the features are drawn from an isotropic Gaussian distribution, for the optimal (or larger than optimal) ridge regularization parameter, the authors show that there will be no double descent with respect to the sample size, i.e. the test error monotonically decreases as a function of the sample size. Furthermore, under a (randomly) projected feature space model, they showed that the test performance is also monotone with respect to projection size. The authors verify some of the the theoretical findings in practice, and show that the same insights carry over for more complicated models.\n\n++:\nThe paper studies a fundamental question in theoretical machine learning.\nThe claims are supported both by clean theoretical results as well as empirical evaluations.\n\n--:\nThe proof techniques seem to heavily depend on the specific choice of the loss function and the regularizer, that is, the mean squared loss and the ridge penalty. It is not clear if the techniques can generalize to other settings.\n\nQuestion / Minor Comment:\nThe paper puts forward the idea that double descent is an artifact of underregularization. To be more precise, the main takeaway from the paper is that optimal \\ell_2-regularization can mitigate double descent, provably in certain linear ridge regression problems; and in practice, in certain deep learning problems. A natural question is if authors have observed similar phenomena under different regularization techniques, including other norm-based penalties, or the more exotic ones from the deep learning literature.\nI suggest that in the experiments section, you clearly state how you obtain the optimal regularization parameter in each of the subsections.\nWhy does the experiments section only cover the \u201csample monotonicity\u201d part of the theoretical results (Theorems 1 and 2)? I think it helps if you verify Theorem 3 as well.\nCan you comment on the requirement d <= p in Theorem 3? It\u2019s true that d > p will not give a subspace of the p-dimensional ambient space, but one can perhaps extend the setting to d > p requiring that P^T P = I_p. \n\nOverall, I enjoyed reading the paper and recommend it for acceptance in ICLR. The paper is well-written for the most part. I found the theoretical results insightful, and well-supported by experiments.\n\n\n\n#####################################\nI have considered the rebuttal as well as other comments in my final recommendation.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1938/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1938/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Regularization can Mitigate Double Descent", "authorids": ["~Preetum_Nakkiran1", "pvenkat@g.harvard.edu", "~Sham_M._Kakade1", "~Tengyu_Ma1"], "authors": ["Preetum Nakkiran", "Prayaag Venkat", "Sham M. Kakade", "Tengyu Ma"], "keywords": ["double descent", "generalization", "regularization", "regression", "monotonicity"], "abstract": "Recent empirical and theoretical studies have shown that many learning algorithms -- from linear regression to neural networks -- can have test performance that is non-monotonic in quantities such the sample size and model size. This striking phenomenon, often referred to as \"double descent\", has raised questions of if we need to re-think our current understanding of generalization. In this work, we study whether the double-descent phenomenon can be avoided by using optimal regularization. Theoretically, we prove that for certain linear regression models with isotropic data distribution, optimally-tuned $\\ell_2$ regularization achieves monotonic test performance as we grow either the sample size or the model size.\nWe also demonstrate empirically that optimally-tuned $\\ell_2$ regularization can mitigate double descent for more general models, including neural networks.\nOur results suggest that it may also be informative to study the test risk scalings of various algorithms in the context of appropriately tuned regularization.", "one-sentence_summary": "Optimal regularization can provably avoid double-descent in certain settings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nakkiran|optimal_regularization_can_mitigate_double_descent", "pdf": "/pdf/3424b7750a87532de0707e6ced4dd6f62ee9ca29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnakkiran2021optimal,\ntitle={Optimal Regularization can Mitigate Double Descent},\nauthor={Preetum Nakkiran and Prayaag Venkat and Sham M. Kakade and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7R7fAoUygoa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "7R7fAoUygoa", "replyto": "7R7fAoUygoa", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1938/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538107456, "tmdate": 1606915807121, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1938/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1938/-/Official_Review"}}}, {"id": "wW5XLve9llz", "original": null, "number": 1, "cdate": 1603880681693, "ddate": null, "tcdate": 1603880681693, "tmdate": 1606766708231, "tddate": null, "forum": "7R7fAoUygoa", "replyto": "7R7fAoUygoa", "invitation": "ICLR.cc/2021/Conference/Paper1938/-/Official_Review", "content": {"title": "An interesting paper with neat results but potentially limited impact", "review": "The authors consider the problem of double descent in regularized linear regression estimators, and show that, with optimal regularization, such problems can be totally mitigated when the covariates are isotropic gaussian. Additionally, the authors show that there exist cases where ridge regression is provably non-monotonic in its average risk. These theoretical results are complemented with some empirical results on ridge regression and small scale neural networks.\n\nThe phenomenon of double descent has gathered substantial interest in the past couple of years, and this paper presents an interesting contribution towards cementing our understanding in the context of double descent in linear models. Although the obtained results are perhaps not overly surprising given the strong connection between the ridge estimator and linear regression with isotropic gaussian variables and homoskedastic gaussian noise, the finite-sample proofs characterize the phenomenon in a concise manner. The paper is clearly written, and exposes the results in a clear and accessible fashion. However, the impact of this paper is limited by the fact that the general phenomenon of regularization avoiding double descent is well-known (if not necessarily in such an explicit setup, at least from considerations in the Bayes-risk or minimax-risk framework, and penalized regression in general), and that interest in double descent mostly stems from the existence of this behavior in unregularized settings.\n\nOther notes: there are some typos (especially in the appendix):\nP.16 regularzier -> regularizer\nP. 17 homeostatic -> homoskedastic\n\n=====================\n\nUpdate after author response: I thank the authors for their responses. I broadly agree with the points brought up by the other reviewers, and despite some weaknesses brought up by various reviewers, this paper is a good contribution to the community. I have brought my score from 6 to 7.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1938/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1938/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Regularization can Mitigate Double Descent", "authorids": ["~Preetum_Nakkiran1", "pvenkat@g.harvard.edu", "~Sham_M._Kakade1", "~Tengyu_Ma1"], "authors": ["Preetum Nakkiran", "Prayaag Venkat", "Sham M. Kakade", "Tengyu Ma"], "keywords": ["double descent", "generalization", "regularization", "regression", "monotonicity"], "abstract": "Recent empirical and theoretical studies have shown that many learning algorithms -- from linear regression to neural networks -- can have test performance that is non-monotonic in quantities such the sample size and model size. This striking phenomenon, often referred to as \"double descent\", has raised questions of if we need to re-think our current understanding of generalization. In this work, we study whether the double-descent phenomenon can be avoided by using optimal regularization. Theoretically, we prove that for certain linear regression models with isotropic data distribution, optimally-tuned $\\ell_2$ regularization achieves monotonic test performance as we grow either the sample size or the model size.\nWe also demonstrate empirically that optimally-tuned $\\ell_2$ regularization can mitigate double descent for more general models, including neural networks.\nOur results suggest that it may also be informative to study the test risk scalings of various algorithms in the context of appropriately tuned regularization.", "one-sentence_summary": "Optimal regularization can provably avoid double-descent in certain settings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nakkiran|optimal_regularization_can_mitigate_double_descent", "pdf": "/pdf/3424b7750a87532de0707e6ced4dd6f62ee9ca29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnakkiran2021optimal,\ntitle={Optimal Regularization can Mitigate Double Descent},\nauthor={Preetum Nakkiran and Prayaag Venkat and Sham M. Kakade and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7R7fAoUygoa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "7R7fAoUygoa", "replyto": "7R7fAoUygoa", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1938/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538107456, "tmdate": 1606915807121, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1938/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1938/-/Official_Review"}}}, {"id": "Dn3-o2c5hpa", "original": null, "number": 2, "cdate": 1603897152700, "ddate": null, "tcdate": 1603897152700, "tmdate": 1606753090459, "tddate": null, "forum": "7R7fAoUygoa", "replyto": "7R7fAoUygoa", "invitation": "ICLR.cc/2021/Conference/Paper1938/-/Official_Review", "content": {"title": "Marginally above acceptance threshold", "review": "This paper studies the double-descent phenomenon from a theoretical perspective. It proves that for certain linear regression tasks, optimally-tuned L-2 regularized model is able to achieve monotonic test performance as the sample size or the model size increases. Empirical results on the effect of L-2 regularization on the double-descent phenomenon are provided for more general models and tasks.\n\n\nPros:\n\n+ The motivation of studying the double-descent phenomenon with optimal regularization is well-explained in the introduction. Connections and comparisons with existing related works are discussed clearly.\n\n+ The presented theoretical results on the linear regression model are non-asymptotic, which is new and different from existing works. The theoretical results are discussed with enough details such that I can easily follow the Lemmas and Theorems.\n\n\nCons:\n\n- My main concern is the generality of the results. The paper mainly focuses on a simplified linear regression model, where the response variable is linearly generated using some ground-truth parameters \\beta^*. One question is whether the results apply to the agnostic settings (the relationship between x and y is unknown)? Moreover, going beyond regression tasks, can we draw the same conclusion for classification tasks? \n\n- The experiments need to be more extensive and better-explained, especially for the CIFAR-100 experiments. This is an image classification task, which is different from previously introduced regression setting. It is important to discuss this difference clearly at the beginning. In addition, the author uses a 5-layer CNN architecture, whereas the state-of-the-art CIFAR-100 results adopts a much larger architecture. Varying architecture size and even considering more image benchmarks would strengthen the experiments.\n\n\nMinor Comments:\n\n1. For Figure 3, it seems that for certain model size, the optimally-regularized test error is much lower than any other one. How do you compute the optimally regularized curve in Figure 3?\n\n===== Post-Discussion Update =====\n\nI thank the authors' efforts for responding my questions. Overall, I find the results presented in the paper interesting and worth publishing. It would be nice to extend the results to more general settings.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1938/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1938/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Regularization can Mitigate Double Descent", "authorids": ["~Preetum_Nakkiran1", "pvenkat@g.harvard.edu", "~Sham_M._Kakade1", "~Tengyu_Ma1"], "authors": ["Preetum Nakkiran", "Prayaag Venkat", "Sham M. Kakade", "Tengyu Ma"], "keywords": ["double descent", "generalization", "regularization", "regression", "monotonicity"], "abstract": "Recent empirical and theoretical studies have shown that many learning algorithms -- from linear regression to neural networks -- can have test performance that is non-monotonic in quantities such the sample size and model size. This striking phenomenon, often referred to as \"double descent\", has raised questions of if we need to re-think our current understanding of generalization. In this work, we study whether the double-descent phenomenon can be avoided by using optimal regularization. Theoretically, we prove that for certain linear regression models with isotropic data distribution, optimally-tuned $\\ell_2$ regularization achieves monotonic test performance as we grow either the sample size or the model size.\nWe also demonstrate empirically that optimally-tuned $\\ell_2$ regularization can mitigate double descent for more general models, including neural networks.\nOur results suggest that it may also be informative to study the test risk scalings of various algorithms in the context of appropriately tuned regularization.", "one-sentence_summary": "Optimal regularization can provably avoid double-descent in certain settings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nakkiran|optimal_regularization_can_mitigate_double_descent", "pdf": "/pdf/3424b7750a87532de0707e6ced4dd6f62ee9ca29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnakkiran2021optimal,\ntitle={Optimal Regularization can Mitigate Double Descent},\nauthor={Preetum Nakkiran and Prayaag Venkat and Sham M. Kakade and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7R7fAoUygoa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "7R7fAoUygoa", "replyto": "7R7fAoUygoa", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1938/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538107456, "tmdate": 1606915807121, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1938/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1938/-/Official_Review"}}}, {"id": "cI5weEF_tmB", "original": null, "number": 7, "cdate": 1605833038498, "ddate": null, "tcdate": 1605833038498, "tmdate": 1605833038498, "tddate": null, "forum": "7R7fAoUygoa", "replyto": "wW5XLve9llz", "invitation": "ICLR.cc/2021/Conference/Paper1938/-/Official_Comment", "content": {"title": "Response to R1", "comment": "Thanks for your useful feedback, and for recognizing the value in our contributions towards understanding double descent.\n\nRegarding your concern on the \"regularization avoiding double-descent\" being well-known: We emphasize that this is subtle, since it is not true as universally as you may expect. For example, Section 4 shows a simple example of a linear regression setting where regularization does *not* suffice for monotonicity. Moreover, note that the \"instance-specific\" notion of monotonicity that we desire is significantly stronger than minimax-style monotonicity (which is worst-case over problem settings). In particular, there exist minimax-optimal estimators which are *not* monotonic in our instance-specific sense.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1938/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1938/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Regularization can Mitigate Double Descent", "authorids": ["~Preetum_Nakkiran1", "pvenkat@g.harvard.edu", "~Sham_M._Kakade1", "~Tengyu_Ma1"], "authors": ["Preetum Nakkiran", "Prayaag Venkat", "Sham M. Kakade", "Tengyu Ma"], "keywords": ["double descent", "generalization", "regularization", "regression", "monotonicity"], "abstract": "Recent empirical and theoretical studies have shown that many learning algorithms -- from linear regression to neural networks -- can have test performance that is non-monotonic in quantities such the sample size and model size. This striking phenomenon, often referred to as \"double descent\", has raised questions of if we need to re-think our current understanding of generalization. In this work, we study whether the double-descent phenomenon can be avoided by using optimal regularization. Theoretically, we prove that for certain linear regression models with isotropic data distribution, optimally-tuned $\\ell_2$ regularization achieves monotonic test performance as we grow either the sample size or the model size.\nWe also demonstrate empirically that optimally-tuned $\\ell_2$ regularization can mitigate double descent for more general models, including neural networks.\nOur results suggest that it may also be informative to study the test risk scalings of various algorithms in the context of appropriately tuned regularization.", "one-sentence_summary": "Optimal regularization can provably avoid double-descent in certain settings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nakkiran|optimal_regularization_can_mitigate_double_descent", "pdf": "/pdf/3424b7750a87532de0707e6ced4dd6f62ee9ca29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnakkiran2021optimal,\ntitle={Optimal Regularization can Mitigate Double Descent},\nauthor={Preetum Nakkiran and Prayaag Venkat and Sham M. Kakade and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7R7fAoUygoa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7R7fAoUygoa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1938/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1938/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1938/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1938/Authors|ICLR.cc/2021/Conference/Paper1938/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1938/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854070, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1938/-/Official_Comment"}}}, {"id": "ycIX1N10mVX", "original": null, "number": 6, "cdate": 1605833021713, "ddate": null, "tcdate": 1605833021713, "tmdate": 1605833021713, "tddate": null, "forum": "7R7fAoUygoa", "replyto": "Dn3-o2c5hpa", "invitation": "ICLR.cc/2021/Conference/Paper1938/-/Official_Comment", "content": {"title": "Response to R3", "comment": "Thank you for your valuable feedback, and for appreciating the motivation of our work and the technical contribution of non-asymptotic results.\n\nWe address your concerns and questions below:\n- Regarding generality, and \"whether the results apply to the agnostic settings\": We agree that it would be great to extend these results past the linear models considered in this work. One concern with even seemingly-minor extensions (such as the agnostic case) is that we do not fully understand even well-specified linear regression without the isotropic condition, and we have reason to believe that this is highly nontrivial. Specifically, proving monotonicity for non-isotropic covariates is still open (Conjecture 1 in the Appendix), and we have reason to believe that proving this requires highly nontrivial statements in random matrix theory (Conjecture 2). The difficulty in these settings arises because we desire **non-asymptotic** results, which would be much stronger than their asymptotic counterparts. Some versions of these questions are addressed in asymptotic settings in recent works of [Mei-Montanari https://arxiv.org/abs/1908.05355] and [d'Ascoli et al. https://arxiv.org/abs/2003.01054]. \n\n- \"can we draw the same conclusion for classification tasks?\": We did not theoretically analyze classification settings, but we did test monotonicity for experimental classification settings in Section 5. It may be possible to analyze optimal regularization in **asymptotic** classification settings, building on recent works such as [https://arxiv.org/abs/1911.01544] and [https://arxiv.org/abs/2001.11572]. However, these techniques do not directly apply in the non-asymptotic settings considered here.\n\n- Thank you for the feedback on the experimental section; we will improve the clarity of the presentation.\n\n- \"5-layer CNN\": Note that while this 5-layer CNN is not a ResNet, it is in fact capable of reaching close to state-of-the-art accuracies on CIFAR-10/100, and is likely to have similar behavior as ResNets. In particular, this network was used in \"Deep Double Descent\" [https://arxiv.org/abs/1912.02292] where it was observed to have similar behavior as ResNet18s. This same network has also been used in works [https://arxiv.org/abs/2003.02237] and [https://arxiv.org/abs/2010.08508]. Thus, we believe that conclusions reached using this 5-layer CNN will carry over to larger and deeper networks as well.\n\n- \"Figure 3\": We apologize for the unclear figure labeling here. In Figure 3, the optimal regularization value (pink highlight) is always selected as one of the 6 values of regularization in the legend. The pink highlight overlaps the other lines in the figure, which unfortunately obscures them (we will fix this in the revision). For example, for model widths < 25, the optimal regularization is lambda=0.005, and then it reduces to lambda=0.003, etc.\n\nWe hope we have addressed your questions; please let us know if there are any additional concerns.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1938/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1938/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Regularization can Mitigate Double Descent", "authorids": ["~Preetum_Nakkiran1", "pvenkat@g.harvard.edu", "~Sham_M._Kakade1", "~Tengyu_Ma1"], "authors": ["Preetum Nakkiran", "Prayaag Venkat", "Sham M. Kakade", "Tengyu Ma"], "keywords": ["double descent", "generalization", "regularization", "regression", "monotonicity"], "abstract": "Recent empirical and theoretical studies have shown that many learning algorithms -- from linear regression to neural networks -- can have test performance that is non-monotonic in quantities such the sample size and model size. This striking phenomenon, often referred to as \"double descent\", has raised questions of if we need to re-think our current understanding of generalization. In this work, we study whether the double-descent phenomenon can be avoided by using optimal regularization. Theoretically, we prove that for certain linear regression models with isotropic data distribution, optimally-tuned $\\ell_2$ regularization achieves monotonic test performance as we grow either the sample size or the model size.\nWe also demonstrate empirically that optimally-tuned $\\ell_2$ regularization can mitigate double descent for more general models, including neural networks.\nOur results suggest that it may also be informative to study the test risk scalings of various algorithms in the context of appropriately tuned regularization.", "one-sentence_summary": "Optimal regularization can provably avoid double-descent in certain settings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nakkiran|optimal_regularization_can_mitigate_double_descent", "pdf": "/pdf/3424b7750a87532de0707e6ced4dd6f62ee9ca29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnakkiran2021optimal,\ntitle={Optimal Regularization can Mitigate Double Descent},\nauthor={Preetum Nakkiran and Prayaag Venkat and Sham M. Kakade and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7R7fAoUygoa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7R7fAoUygoa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1938/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1938/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1938/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1938/Authors|ICLR.cc/2021/Conference/Paper1938/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1938/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854070, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1938/-/Official_Comment"}}}, {"id": "xjpZpc2nWtV", "original": null, "number": 5, "cdate": 1605832836132, "ddate": null, "tcdate": 1605832836132, "tmdate": 1605832836132, "tddate": null, "forum": "7R7fAoUygoa", "replyto": "0uLR7_q331c", "invitation": "ICLR.cc/2021/Conference/Paper1938/-/Official_Comment", "content": {"title": "Response to R4", "comment": "Thank you for your feedback, and for your helpful suggestions on the presentation. We will incorporate your suggestions in the revision.\n\nTo answer your questions:\n- \"Do [we] observe similar phenomena under different regularization techniques\": We agree it is a good question to ask if other kinds of regularization / capacity-control can also eliminate double-descent in the same way. We expect that this will indeed be the case empirically (for many standard regularizers like dropout, etc), though we have not extensively tested this. Understanding the situation theoretically in these settings may also be much more difficult. However, we do have some evidence from prior works that other regularizers can also induce monotonicity. For example, if we consider early-stopping as a form of regularization, then Figure 1 of \"Deep Double Descent\" [https://arxiv.org/abs/1912.02292] shows that optimal early-stopping can empirically eliminate the double-descent in some settings (though this is not the focus of that work).\n\n- \"Why does the experiments section only cover the \u201csample monotonicity\u201d part of the theoretical results?\": Note that Section 5.2 includes experiments on model-size monotonicity, to test nonlinear versions of Theorem 3 in practice. (This includes experiments on random-feature models and CNNs).\n\n- \"the requirement d <= p in Theorem 3\": You are correct that the theorem could be mathematically extended to d > p using similar techniques. However, this regime is quite different, in that it does not capture the \"right\" notion of increasing model size. Specifically, in the \"d <= p\" regime, increasing the \"model size\" by increasing 'd' makes the model more powerful, and achieves smaller test risk. However, once d > p, then all subsequent model sizes (d+1, d+2,...) are essentially the same model, and will have the same test risk as the d=p model. This is because for d > p, we can consider the model restricted to the p-dimensional image of P, and the d-dimensional model is equivalent to learning a p-dimensional model within this image. Intuitively, increasing 'd' once d > p is no longer giving the model more information about the covariates, but it's just changing the representation of the space.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1938/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1938/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Regularization can Mitigate Double Descent", "authorids": ["~Preetum_Nakkiran1", "pvenkat@g.harvard.edu", "~Sham_M._Kakade1", "~Tengyu_Ma1"], "authors": ["Preetum Nakkiran", "Prayaag Venkat", "Sham M. Kakade", "Tengyu Ma"], "keywords": ["double descent", "generalization", "regularization", "regression", "monotonicity"], "abstract": "Recent empirical and theoretical studies have shown that many learning algorithms -- from linear regression to neural networks -- can have test performance that is non-monotonic in quantities such the sample size and model size. This striking phenomenon, often referred to as \"double descent\", has raised questions of if we need to re-think our current understanding of generalization. In this work, we study whether the double-descent phenomenon can be avoided by using optimal regularization. Theoretically, we prove that for certain linear regression models with isotropic data distribution, optimally-tuned $\\ell_2$ regularization achieves monotonic test performance as we grow either the sample size or the model size.\nWe also demonstrate empirically that optimally-tuned $\\ell_2$ regularization can mitigate double descent for more general models, including neural networks.\nOur results suggest that it may also be informative to study the test risk scalings of various algorithms in the context of appropriately tuned regularization.", "one-sentence_summary": "Optimal regularization can provably avoid double-descent in certain settings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nakkiran|optimal_regularization_can_mitigate_double_descent", "pdf": "/pdf/3424b7750a87532de0707e6ced4dd6f62ee9ca29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnakkiran2021optimal,\ntitle={Optimal Regularization can Mitigate Double Descent},\nauthor={Preetum Nakkiran and Prayaag Venkat and Sham M. Kakade and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7R7fAoUygoa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7R7fAoUygoa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1938/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1938/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1938/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1938/Authors|ICLR.cc/2021/Conference/Paper1938/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1938/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854070, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1938/-/Official_Comment"}}}, {"id": "lGMdQLIbhm8", "original": null, "number": 4, "cdate": 1605832774012, "ddate": null, "tcdate": 1605832774012, "tmdate": 1605832774012, "tddate": null, "forum": "7R7fAoUygoa", "replyto": "9lknDoIrEpm", "invitation": "ICLR.cc/2021/Conference/Paper1938/-/Official_Comment", "content": {"title": "Response to R2", "comment": "Thank you for your valuable feedback, and for recognizing the theoretical and empirical contributions in our work.\n\nTo answer your questions:\n1. \"What changed here or what were the prior works missing?\": Many of the prior works observed double-descent in settings where the model size or samples was increased, but all other hyperparameters (eg regularization strength, and train time) were held fixed. In contrast, the message of our work is that if regularization is tuned optimally **as a function of** model size -- which is more aligned with the situation in practice -- then double descent can be mitigated. At a high level, the takeaway from prior works on double descent was that \"if you are not careful, you could encounter pathological behaviors near the critical regime\". While the message of our work is \"if you **are** careful (i.e. you optimally regularize), then you can avoid these pathologies.\"\n\n2. \"Could the optimal regularization amount be so high that it starts to hurt performance?\". Great question. On the one hand, in theory we always include \\lambda=0 as a possibile regularizer, so \"optimal regularization\" is always at least as good as no regularization in terms of performance. However, you are right that as we look at networks with larger capacity, the optimal regularization value seems to tend to 0 in practice. This is evident in our Figure 3, and has also been observed in prior works (e.g. https://arxiv.org/abs/1805.10939).\n\n3. \"Equation (9) in the Appendix\": Thanks for noticing this, we will make it more clear in the revision. Briefly, the $\\gamma_i$s are eigenvalues of the matrix $X$ itself, and Equation (9) follows by using the SVD decomposition of $X = U \\textrm{diag}(\\gamma_i) V^T$ and the fact that U and V are orthonormal.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1938/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1938/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Regularization can Mitigate Double Descent", "authorids": ["~Preetum_Nakkiran1", "pvenkat@g.harvard.edu", "~Sham_M._Kakade1", "~Tengyu_Ma1"], "authors": ["Preetum Nakkiran", "Prayaag Venkat", "Sham M. Kakade", "Tengyu Ma"], "keywords": ["double descent", "generalization", "regularization", "regression", "monotonicity"], "abstract": "Recent empirical and theoretical studies have shown that many learning algorithms -- from linear regression to neural networks -- can have test performance that is non-monotonic in quantities such the sample size and model size. This striking phenomenon, often referred to as \"double descent\", has raised questions of if we need to re-think our current understanding of generalization. In this work, we study whether the double-descent phenomenon can be avoided by using optimal regularization. Theoretically, we prove that for certain linear regression models with isotropic data distribution, optimally-tuned $\\ell_2$ regularization achieves monotonic test performance as we grow either the sample size or the model size.\nWe also demonstrate empirically that optimally-tuned $\\ell_2$ regularization can mitigate double descent for more general models, including neural networks.\nOur results suggest that it may also be informative to study the test risk scalings of various algorithms in the context of appropriately tuned regularization.", "one-sentence_summary": "Optimal regularization can provably avoid double-descent in certain settings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nakkiran|optimal_regularization_can_mitigate_double_descent", "pdf": "/pdf/3424b7750a87532de0707e6ced4dd6f62ee9ca29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnakkiran2021optimal,\ntitle={Optimal Regularization can Mitigate Double Descent},\nauthor={Preetum Nakkiran and Prayaag Venkat and Sham M. Kakade and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7R7fAoUygoa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7R7fAoUygoa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1938/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1938/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1938/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1938/Authors|ICLR.cc/2021/Conference/Paper1938/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1938/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854070, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1938/-/Official_Comment"}}}, {"id": "p5C66sJzsQY", "original": null, "number": 2, "cdate": 1605595979961, "ddate": null, "tcdate": 1605595979961, "tmdate": 1605596594911, "tddate": null, "forum": "7R7fAoUygoa", "replyto": "04PlfCC7pi5", "invitation": "ICLR.cc/2021/Conference/Paper1938/-/Public_Comment", "content": {"title": "Reply to authors", "comment": "Thanks for the reply. To clarify a few things,\n\n+ I agree that asymptotic risk characterizations on ridge regression (and double descent-related results) are properly cited. \nWhat I intend to convey is that the *risk monotonicity* of optimally-tuned ridge regression is a known result in the proportional limit (under the same isotropic features that this submission assumes), and I do not see this fact being acknowledged in the related works (even though [Sheng and Dobriban 2019] is cited).\nSo while the statement that the current submission provides the first non-asymptotic characterization of such monotonicity is correct (to my knowledge), I think it should be highlighted that similar observations exist and have been rigorously proved in the asymptotic limit. \n\n+ Similarly, I brought up [Krogh and Hertz 1992] not because it demonstrates sample-wise monotonicity (which I believe is the reason you're referring to the earlier works?), but rather that it indicates weight decay can *suppress* the double descent peak, which is very relevant to your message.\n\n+ Regarding the \"triple descent\" risk curve, since this discussion is included in the \"Universal vs Asymptotic\" paragraph, my impression is you're suggesting that the presence of this trend is a benefit of the non-asymptotic setup, and would not be observed in the asymptotic analysis (which is *not true*); if this is not the case, sorry for the misinterpretation.   \nThe reason that I mentioned [Dobriban and Wager 2018] is that under the general data covariance that they considered, it is very straightforward to construct multiple peaks (for instance see remark after Corollary 3 in [Wu and Xu 2020]), although the shape of the risk curve is not the concern of the original paper."}, "signatures": ["~Denny_Wu2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Denny_Wu2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Regularization can Mitigate Double Descent", "authorids": ["~Preetum_Nakkiran1", "pvenkat@g.harvard.edu", "~Sham_M._Kakade1", "~Tengyu_Ma1"], "authors": ["Preetum Nakkiran", "Prayaag Venkat", "Sham M. Kakade", "Tengyu Ma"], "keywords": ["double descent", "generalization", "regularization", "regression", "monotonicity"], "abstract": "Recent empirical and theoretical studies have shown that many learning algorithms -- from linear regression to neural networks -- can have test performance that is non-monotonic in quantities such the sample size and model size. This striking phenomenon, often referred to as \"double descent\", has raised questions of if we need to re-think our current understanding of generalization. In this work, we study whether the double-descent phenomenon can be avoided by using optimal regularization. Theoretically, we prove that for certain linear regression models with isotropic data distribution, optimally-tuned $\\ell_2$ regularization achieves monotonic test performance as we grow either the sample size or the model size.\nWe also demonstrate empirically that optimally-tuned $\\ell_2$ regularization can mitigate double descent for more general models, including neural networks.\nOur results suggest that it may also be informative to study the test risk scalings of various algorithms in the context of appropriately tuned regularization.", "one-sentence_summary": "Optimal regularization can provably avoid double-descent in certain settings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nakkiran|optimal_regularization_can_mitigate_double_descent", "pdf": "/pdf/3424b7750a87532de0707e6ced4dd6f62ee9ca29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnakkiran2021optimal,\ntitle={Optimal Regularization can Mitigate Double Descent},\nauthor={Preetum Nakkiran and Prayaag Venkat and Sham M. Kakade and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7R7fAoUygoa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7R7fAoUygoa", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1938/Authors", "ICLR.cc/2021/Conference/Paper1938/Reviewers", "ICLR.cc/2021/Conference/Paper1938/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024967407, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1938/-/Public_Comment"}}}, {"id": "uOWLuto4noB", "original": null, "number": 1, "cdate": 1605481324902, "ddate": null, "tcdate": 1605481324902, "tmdate": 1605592707902, "tddate": null, "forum": "7R7fAoUygoa", "replyto": "7R7fAoUygoa", "invitation": "ICLR.cc/2021/Conference/Paper1938/-/Public_Comment", "content": {"title": "A few comments on prior works", "comment": "Thanks for the interesting work. A few comments:\n\n+ The observation that ridge regularization suppresses the double descent peak is not new \u2014 it at least dates back to 1992 (and should therefore be cited):  \nKrogh, A. and Hertz, J.A., 1992. A simple weight decay can improve generalization. Advances in neural information processing systems.\n\n+ The discussion of \u201ctriple descent\u201d is rather misleading \u2014 such trend is not a unique feature of the non-asymptotic analysis; instead it can be easily obtained (in fact even more peaks) in the asymptotic setting of [Dobriban and Wager 2018] or [Hastie et al. 2019],  by manipulating the data covariance. This point needs to be clarified.\n\n+ For isotropic data, the monotonicity of the optimally (ridge-) regularized prediction risk is a relatively well-known conclusion in the proportional limit (e.g. Proposition 6 in [Dobriban and Sheng 2019], but earlier reference could exist).   \nI therefore think that the current submission should be more explicit in describing (part of) its theoretical contribution as proving a non-asymptotic version of existing results on risk monotonicity.\n\n+ Regarding general covariates, we recently showed that in the proportional limit, such monotonicity holds for the random-effect model studied in [Dobriban and Wager 2018]. But for more general orientation of the true parameters this result may be challenging to establish \u2014 note that in this case the optimal regularization strength $\\lambda$ can be negative.  \nWu, D. and Xu, J., 2020. On the optimal weighted $\\ell_2$ regularization in overparameterized linear regression. Advances in neural information processing systems."}, "signatures": ["~Denny_Wu2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Denny_Wu2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Regularization can Mitigate Double Descent", "authorids": ["~Preetum_Nakkiran1", "pvenkat@g.harvard.edu", "~Sham_M._Kakade1", "~Tengyu_Ma1"], "authors": ["Preetum Nakkiran", "Prayaag Venkat", "Sham M. Kakade", "Tengyu Ma"], "keywords": ["double descent", "generalization", "regularization", "regression", "monotonicity"], "abstract": "Recent empirical and theoretical studies have shown that many learning algorithms -- from linear regression to neural networks -- can have test performance that is non-monotonic in quantities such the sample size and model size. This striking phenomenon, often referred to as \"double descent\", has raised questions of if we need to re-think our current understanding of generalization. In this work, we study whether the double-descent phenomenon can be avoided by using optimal regularization. Theoretically, we prove that for certain linear regression models with isotropic data distribution, optimally-tuned $\\ell_2$ regularization achieves monotonic test performance as we grow either the sample size or the model size.\nWe also demonstrate empirically that optimally-tuned $\\ell_2$ regularization can mitigate double descent for more general models, including neural networks.\nOur results suggest that it may also be informative to study the test risk scalings of various algorithms in the context of appropriately tuned regularization.", "one-sentence_summary": "Optimal regularization can provably avoid double-descent in certain settings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nakkiran|optimal_regularization_can_mitigate_double_descent", "pdf": "/pdf/3424b7750a87532de0707e6ced4dd6f62ee9ca29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnakkiran2021optimal,\ntitle={Optimal Regularization can Mitigate Double Descent},\nauthor={Preetum Nakkiran and Prayaag Venkat and Sham M. Kakade and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7R7fAoUygoa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7R7fAoUygoa", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1938/Authors", "ICLR.cc/2021/Conference/Paper1938/Reviewers", "ICLR.cc/2021/Conference/Paper1938/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024967407, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1938/-/Public_Comment"}}}, {"id": "04PlfCC7pi5", "original": null, "number": 3, "cdate": 1605591142461, "ddate": null, "tcdate": 1605591142461, "tmdate": 1605591142461, "tddate": null, "forum": "7R7fAoUygoa", "replyto": "uOWLuto4noB", "invitation": "ICLR.cc/2021/Conference/Paper1938/-/Official_Comment", "content": {"title": "Response to Denny Wu", "comment": "Thank you for your interest. We would like to clarify a few points. Our work makes very clear that our contribution is non-asymptotic; such a finite sample, non-asymptotic analysis is important to accurately characterize real phenomena (also see further related work below). Furthermore, we have cited all the references you mention, aside from [Krogh, A. and Hertz, J.A., 1992] and your concurrent work [Wu, D. and Xu, J., 2020], which we will add. \nIn addition, we believe a number of your remarks are misleading for the following reasons:\n\n- We did include references as early as [Trunk 1979] as part of our list of works which have observed and studied this phenomena in some form; we are not claiming this is new. We will also add the reference to the work [Krogh, A. and Hertz, J.A., 1992] to this list.\n\n- We do not believe the discussion of \"triple descent\" is misleading, nor do we say that it is a consequence of non-asymptotic analysis. For works you referenced, we do not see a discussion (or experiments) exhibiting triple or multiple descent. If we are mistaken, can you please explicitly provide a reference in these papers? We would be happy to clarify this point because we do seek to credit this study in an accurate manner. The only prior work we are aware of that discusses this is [Liang et al. 2020], which we cite in context. Also, the work of [Chen et al] on multiple descent does not cite prior to [Liang et al. 2020] on this phenomena.\n\n- We clearly acknowledge the asymptotic related works in our paper, including [Dobriban and Sheng 2019] and [Dobriban and Wager 2018]. We are also very explicit about the non-asymptotic aspect of our contribution.\n\nFinally, note that several works which appeared subsequent to our work also discuss triple-descent, and acknowledge our contribution in isolating this phenomena, including:\n\nMultiple descent: Design your own generalization curve. Lin Chen, Yifei Min, Mikhail Belkin, Amin Karbasi. 2020\n\nTriple descent and the two kinds of overfitting: Where & why do they appear? St\u00e9phane d'Ascoli, Levent Sagun, Giulio Biroli. 2020.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1938/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1938/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Regularization can Mitigate Double Descent", "authorids": ["~Preetum_Nakkiran1", "pvenkat@g.harvard.edu", "~Sham_M._Kakade1", "~Tengyu_Ma1"], "authors": ["Preetum Nakkiran", "Prayaag Venkat", "Sham M. Kakade", "Tengyu Ma"], "keywords": ["double descent", "generalization", "regularization", "regression", "monotonicity"], "abstract": "Recent empirical and theoretical studies have shown that many learning algorithms -- from linear regression to neural networks -- can have test performance that is non-monotonic in quantities such the sample size and model size. This striking phenomenon, often referred to as \"double descent\", has raised questions of if we need to re-think our current understanding of generalization. In this work, we study whether the double-descent phenomenon can be avoided by using optimal regularization. Theoretically, we prove that for certain linear regression models with isotropic data distribution, optimally-tuned $\\ell_2$ regularization achieves monotonic test performance as we grow either the sample size or the model size.\nWe also demonstrate empirically that optimally-tuned $\\ell_2$ regularization can mitigate double descent for more general models, including neural networks.\nOur results suggest that it may also be informative to study the test risk scalings of various algorithms in the context of appropriately tuned regularization.", "one-sentence_summary": "Optimal regularization can provably avoid double-descent in certain settings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nakkiran|optimal_regularization_can_mitigate_double_descent", "pdf": "/pdf/3424b7750a87532de0707e6ced4dd6f62ee9ca29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnakkiran2021optimal,\ntitle={Optimal Regularization can Mitigate Double Descent},\nauthor={Preetum Nakkiran and Prayaag Venkat and Sham M. Kakade and Tengyu Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7R7fAoUygoa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7R7fAoUygoa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1938/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1938/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1938/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1938/Authors|ICLR.cc/2021/Conference/Paper1938/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1938/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854070, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1938/-/Official_Comment"}}}], "count": 13}