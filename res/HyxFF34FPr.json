{"notes": [{"id": "HyxFF34FPr", "original": "HkxTyNRh8r", "number": 86, "cdate": 1569438848906, "ddate": null, "tcdate": 1569438848906, "tmdate": 1577168294234, "tddate": null, "forum": "HyxFF34FPr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["taokongcn@gmail.com", "fcsun@tsinghua.edu.cn", "hpliu@tsinghua.edu.cn", "jiangyuning@bytedance.com", "lileilab@bytedance.com", "jshi@seas.upenn.edu"], "title": "FoveaBox: Beyound Anchor-based Object Detection", "authors": ["Tao Kong", "Fuchun Sun", "Huaping Liu", "Yuning Jiang", "Lei Li", "Jianbo Shi"], "pdf": "/pdf/c729b3a75ff60601acc3e68bd461d7d017e01970.pdf", "abstract": "We present FoveaBox, an accurate, flexible, and completely anchor-free framework for object detection. While almost all state-of-the-art object detectors utilize predefined anchors to enumerate possible locations, scales and aspect ratios for the search of the objects, their performance and generalization ability are also limited to the design of anchors. Instead, FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility, and (b) producing category-agnostic bounding box for each position that potentially contains an object. The scales of target boxes are naturally associated with feature pyramid representations. We demonstrate its effectiveness on standard benchmarks and report extensive experimental analysis.  Without bells and whistles, FoveaBox achieves state-of-the-art single model performance on the standard COCO detection benchmark. More importantly, FoveaBox avoids all computation and hyper-parameters related to anchor boxes, which are often sensitive to the final detection performance.  We believe the simple and effective approach will serve as a solid baseline and help ease future research for object detection. ", "code": "https://drive.google.com/file/d/1Iwe3Vfbunv5NaLFFn0i_fsfNT-XP0Zmx/view?usp=sharing", "keywords": [], "paperhash": "kong|foveabox_beyound_anchorbased_object_detection", "original_pdf": "/attachment/c729b3a75ff60601acc3e68bd461d7d017e01970.pdf", "_bibtex": "@misc{\nkong2020foveabox,\ntitle={FoveaBox: Beyound Anchor-based Object Detection},\nauthor={Tao Kong and Fuchun Sun and Huaping Liu and Yuning Jiang and Lei Li and Jianbo Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxFF34FPr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "34fTue2NkT", "original": null, "number": 1, "cdate": 1576798687101, "ddate": null, "tcdate": 1576798687101, "tmdate": 1576800947999, "tddate": null, "forum": "HyxFF34FPr", "replyto": "HyxFF34FPr", "invitation": "ICLR.cc/2020/Conference/Paper86/-/Decision", "content": {"decision": "Reject", "comment": "The paper proposes a method for object detection by predicting category-specific object probability and category-agnostic bounding box coordinates for each position that's likely to contain an object. The proposed idea is interesting and the experimental results show improvement over RetinaNet and other baselines. However, in terms of weakness, (1) conceptually speaking it's unclear whether the proposed method is a big departure from the existing frameworks; and (2) although the authors are claiming SOTA performance, the proposed method seems to be worse than other existing/recent work. Some example references are listed below (more available here: https://paperswithcode.com/paper/foveabox-beyond-anchor-based-object-detector). \n\n[1] Scale-Aware Trident Networks for Object Detection\nhttps://arxiv.org/abs/1901.01892\n\n[2] GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond\nhttps://arxiv.org/abs/1904.11492\n\n[3] CBNet: A Novel Composite Backbone Network Architecture for Object Detection\nhttps://arxiv.org/abs/1909.03625\n\n[4] EfficientDet: Scalable and Efficient Object Detection\nhttps://arxiv.org/abs/1911.09070\n\nReferences [3] and [4] are concurrent works so shouldn't be a ground of rejection per se, but the performance gap is quite large. Compared to [1] and [2] which have been on arxiv for a while (+5 months) the performance of the proposed method is still inferior. Despite considering that object detection is a very competitive field, the conceptual/technical novelty and overall practical significance seem limited for ICLR. For a future submission, I would suggest that a revision of this paper being reviewed in a computer vision conference, rather than ML conference.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["taokongcn@gmail.com", "fcsun@tsinghua.edu.cn", "hpliu@tsinghua.edu.cn", "jiangyuning@bytedance.com", "lileilab@bytedance.com", "jshi@seas.upenn.edu"], "title": "FoveaBox: Beyound Anchor-based Object Detection", "authors": ["Tao Kong", "Fuchun Sun", "Huaping Liu", "Yuning Jiang", "Lei Li", "Jianbo Shi"], "pdf": "/pdf/c729b3a75ff60601acc3e68bd461d7d017e01970.pdf", "abstract": "We present FoveaBox, an accurate, flexible, and completely anchor-free framework for object detection. While almost all state-of-the-art object detectors utilize predefined anchors to enumerate possible locations, scales and aspect ratios for the search of the objects, their performance and generalization ability are also limited to the design of anchors. Instead, FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility, and (b) producing category-agnostic bounding box for each position that potentially contains an object. The scales of target boxes are naturally associated with feature pyramid representations. We demonstrate its effectiveness on standard benchmarks and report extensive experimental analysis.  Without bells and whistles, FoveaBox achieves state-of-the-art single model performance on the standard COCO detection benchmark. More importantly, FoveaBox avoids all computation and hyper-parameters related to anchor boxes, which are often sensitive to the final detection performance.  We believe the simple and effective approach will serve as a solid baseline and help ease future research for object detection. ", "code": "https://drive.google.com/file/d/1Iwe3Vfbunv5NaLFFn0i_fsfNT-XP0Zmx/view?usp=sharing", "keywords": [], "paperhash": "kong|foveabox_beyound_anchorbased_object_detection", "original_pdf": "/attachment/c729b3a75ff60601acc3e68bd461d7d017e01970.pdf", "_bibtex": "@misc{\nkong2020foveabox,\ntitle={FoveaBox: Beyound Anchor-based Object Detection},\nauthor={Tao Kong and Fuchun Sun and Huaping Liu and Yuning Jiang and Lei Li and Jianbo Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxFF34FPr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HyxFF34FPr", "replyto": "HyxFF34FPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795730447, "tmdate": 1576800283245, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper86/-/Decision"}}}, {"id": "H1e8TZVM9S", "original": null, "number": 2, "cdate": 1572123070119, "ddate": null, "tcdate": 1572123070119, "tmdate": 1574251926702, "tddate": null, "forum": "HyxFF34FPr", "replyto": "HyxFF34FPr", "invitation": "ICLR.cc/2020/Conference/Paper86/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "This paper introduces an anchor-free object detection framework that aims at simultaneously predicting the object position and the corresponding boundary. To achieve this, the proposed FoveaBox detector predicts category-sensitive semantic maps for the object existing possibility, and  produces category-agnostic bounding box for each position that is likely to contain an object. The scales of target boxes are associated with feature pyramid representations. Experiments are performed on MS COCO detection benchmark.\n\nPros:\nThe proposed approach is simple and is shown to avoid most computation and hyper-parameters related to anchor boxes. The paper is well written and easy to follow. \nCons:\nThe main issue with the paper is the main idea is similar to [1,2, 3, 4, 5]. For instance, CenterNet also represents each object instance by its features at the center point and achieves similar detection performance compared to the proposed detector. Further, no speed comparison with these approaches is provided in the paper. Without a fair speed comparison and with similar detection performance, it is difficult to fully assess the merits of the proposed approach. Though inference speed comparison is reported with RetineNet. However, a proper and detailed comparison with [1, 2, 3, 4, 5] is missing. \n\n1: Xingyi Zhou, Dequan Wang, Philipp Kr\u00e4henb\u00fchl: Objects as Points. CoRR abs/1904.07850 (2019).\n2: Xingyi Zhou, Jiacheng Zhuo, Philipp Kr\u00e4henb\u00fchl: Bottom-Up Object Detection by Grouping Extreme and Center Points. CVPR 2019.\n3: Zhi Tian, Chunhua Shen, Hao Chen, Tong He: FCOS: Fully Convolutional One-Stage Object Detection. CoRR abs/1904.01355 (2019).\n4: Hei Law and Jia Deng: Cornernet: Detecting objects as paired keypoints. ECCV 2018.\n5: Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qingming Huang, Qi Tian: CenterNet: Keypoint Triplets for Object Detection. CoRR abs/1904.08189 (2019).\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper86/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper86/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["taokongcn@gmail.com", "fcsun@tsinghua.edu.cn", "hpliu@tsinghua.edu.cn", "jiangyuning@bytedance.com", "lileilab@bytedance.com", "jshi@seas.upenn.edu"], "title": "FoveaBox: Beyound Anchor-based Object Detection", "authors": ["Tao Kong", "Fuchun Sun", "Huaping Liu", "Yuning Jiang", "Lei Li", "Jianbo Shi"], "pdf": "/pdf/c729b3a75ff60601acc3e68bd461d7d017e01970.pdf", "abstract": "We present FoveaBox, an accurate, flexible, and completely anchor-free framework for object detection. While almost all state-of-the-art object detectors utilize predefined anchors to enumerate possible locations, scales and aspect ratios for the search of the objects, their performance and generalization ability are also limited to the design of anchors. Instead, FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility, and (b) producing category-agnostic bounding box for each position that potentially contains an object. The scales of target boxes are naturally associated with feature pyramid representations. We demonstrate its effectiveness on standard benchmarks and report extensive experimental analysis.  Without bells and whistles, FoveaBox achieves state-of-the-art single model performance on the standard COCO detection benchmark. More importantly, FoveaBox avoids all computation and hyper-parameters related to anchor boxes, which are often sensitive to the final detection performance.  We believe the simple and effective approach will serve as a solid baseline and help ease future research for object detection. ", "code": "https://drive.google.com/file/d/1Iwe3Vfbunv5NaLFFn0i_fsfNT-XP0Zmx/view?usp=sharing", "keywords": [], "paperhash": "kong|foveabox_beyound_anchorbased_object_detection", "original_pdf": "/attachment/c729b3a75ff60601acc3e68bd461d7d017e01970.pdf", "_bibtex": "@misc{\nkong2020foveabox,\ntitle={FoveaBox: Beyound Anchor-based Object Detection},\nauthor={Tao Kong and Fuchun Sun and Huaping Liu and Yuning Jiang and Lei Li and Jianbo Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxFF34FPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyxFF34FPr", "replyto": "HyxFF34FPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper86/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper86/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576086993787, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper86/Reviewers"], "noninvitees": [], "tcdate": 1570237757283, "tmdate": 1576086993805, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper86/-/Official_Review"}}}, {"id": "HyxX4-1OiB", "original": null, "number": 5, "cdate": 1573544234800, "ddate": null, "tcdate": 1573544234800, "tmdate": 1573544234800, "tddate": null, "forum": "HyxFF34FPr", "replyto": "H1e8TZVM9S", "invitation": "ICLR.cc/2020/Conference/Paper86/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "Thank you for the detailed review. The main concerns of the reviewer are addressed separately below.\n\n--The main idea is similar to [1, 2, 3, 4, 5].\nThe listed works can be grouped into two categories: bottom-up methods [2, 4, 5] and top-down methods [1, 3]. \n(a) Bottom-up methods [2, 4, 5]. CornerNet and ExtremeNet are recently proposed one-stage object detectors. The key idea is to detect pairs of corner keypoints and groups them to form the final detected boxes. CornerNet requires much more complicated post-processing to group the pairs of corners belonging to the same instance. An extra associative embedding is learned for the purpose of grouping. The method in [5] is based on CornerNet, and tries to detect each object as a triplet, rather than a pair of keypoints. The authors also designed cascade corner pooling and center pooling to further enrich the corner information. ExtremeNet requires a combinatorial grouping stage after keypoint detection, which significantly slows down the algorithm. Our FoveaBox, on the other hand, simply predicts the center area per object and boundaries without the need for grouping or post-processing.\n(b) Top-down methods [1, 3]. Here we note that [1, 3] and FoveaBox are contemporary and independent works. In [1], the authors try to model an object as a single point and its bounding box. The detector uses keypoint estimation to find center points and regresses to boundaries. The idea of our FoveaBox is most similar to FCOS [3]. In FCOS, the authors propose to utilize the additional \u201ccenterness\u201d branch to suppress low-quality predicted boxes.  In contrast, the occurrence area proposed in our work could effectively suppress most false positive predictions. We also thank Zhi Tian for pointing out the relation between FCOS and FoveaBox.\n\n--Speed comparison \nPer the reviewer's suggestion, we evaluate the inference time of these methods. The experiments are performed on a single Nvidia V100 GPU  by averaging 10 runs. The speed field could remotely reflect the actual runtime of a model due to the difference in implementations. \n\nMethod                                          |    backbone           |     AP       |    Speed\nCenterNet (Zhou et al., 2019)    |    Hourglass-104   |     42.1     |    119 ms\nCenterNet (Duan et al., 2019)   |    Hourglass-104   |     44.9     |    329 ms\nExtremNet (Zhou et al., 2019)   |    Hourglass-104   |     40.1     |    301 ms\nCornerNet (Law et al., 2018)     |    Hourglass-104    |    40.5     |    291 ms\nRetinaNet                                     |    ResNeXt-101       |    40.8     |    106 ms\nFCOS                                             |    ResNeXt-101       |    42.1     |     98 ms\nFoveaBox                                     |    ResNeXt-101       |    42.3     |     95 ms\nFoveaBox-align                           |    ResNeXt-101       |    43.9     |     138 ms\n\nWe will include these discussions and results in the revised version. Finally, thanks and we are looking forward to your further valuable response.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper86/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper86/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["taokongcn@gmail.com", "fcsun@tsinghua.edu.cn", "hpliu@tsinghua.edu.cn", "jiangyuning@bytedance.com", "lileilab@bytedance.com", "jshi@seas.upenn.edu"], "title": "FoveaBox: Beyound Anchor-based Object Detection", "authors": ["Tao Kong", "Fuchun Sun", "Huaping Liu", "Yuning Jiang", "Lei Li", "Jianbo Shi"], "pdf": "/pdf/c729b3a75ff60601acc3e68bd461d7d017e01970.pdf", "abstract": "We present FoveaBox, an accurate, flexible, and completely anchor-free framework for object detection. While almost all state-of-the-art object detectors utilize predefined anchors to enumerate possible locations, scales and aspect ratios for the search of the objects, their performance and generalization ability are also limited to the design of anchors. Instead, FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility, and (b) producing category-agnostic bounding box for each position that potentially contains an object. The scales of target boxes are naturally associated with feature pyramid representations. We demonstrate its effectiveness on standard benchmarks and report extensive experimental analysis.  Without bells and whistles, FoveaBox achieves state-of-the-art single model performance on the standard COCO detection benchmark. More importantly, FoveaBox avoids all computation and hyper-parameters related to anchor boxes, which are often sensitive to the final detection performance.  We believe the simple and effective approach will serve as a solid baseline and help ease future research for object detection. ", "code": "https://drive.google.com/file/d/1Iwe3Vfbunv5NaLFFn0i_fsfNT-XP0Zmx/view?usp=sharing", "keywords": [], "paperhash": "kong|foveabox_beyound_anchorbased_object_detection", "original_pdf": "/attachment/c729b3a75ff60601acc3e68bd461d7d017e01970.pdf", "_bibtex": "@misc{\nkong2020foveabox,\ntitle={FoveaBox: Beyound Anchor-based Object Detection},\nauthor={Tao Kong and Fuchun Sun and Huaping Liu and Yuning Jiang and Lei Li and Jianbo Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxFF34FPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxFF34FPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper86/Authors", "ICLR.cc/2020/Conference/Paper86/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper86/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper86/Reviewers", "ICLR.cc/2020/Conference/Paper86/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper86/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper86/Authors|ICLR.cc/2020/Conference/Paper86/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176604, "tmdate": 1576860529455, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper86/Authors", "ICLR.cc/2020/Conference/Paper86/Reviewers", "ICLR.cc/2020/Conference/Paper86/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper86/-/Official_Comment"}}}, {"id": "rJx937q8iH", "original": null, "number": 4, "cdate": 1573458865770, "ddate": null, "tcdate": 1573458865770, "tmdate": 1573458865770, "tddate": null, "forum": "HyxFF34FPr", "replyto": "rJlOCkwbqH", "invitation": "ICLR.cc/2020/Conference/Paper86/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "Thank you for the detailed review. We appreciate your comments on the contributions of our work and your valuable suggestions to improve our paper. \n\n--About the paper writing and overview figure.\nThanks for pointing out this, and we will further revise the paper, especially the overview figure, to make it more clear and self-standing.\n\n--Ablation on another dataset.\nPer the reviewer's suggestion, we conducted additional experiments on Pascal VOC object detection dataset (http://host.robots.ox.ac.uk/pascal/VOC/index.html).  This dataset covers 20 object categories, and the performance is measured by mean average precision (mAP) at IoU=0.5. All variants are trained on VOC2007 trainval and VOC2012 trainval, and tested on VOC2007 test dataset. Based on ResNet-50, we can compare the performances of FoveaBox and RetinaNet.  \n\nMethod      |    backbone      |    mAP@0.5    |    speed    |    hyper-params\nRetinaNet  |    ResNet-50     |        75.5           |    74 ms    |    9 anchors, IoU-pos=0.5, IoU-neg=0.4\nRetinaNet  |    ResNet-50     |        75.1           |    69 ms    |    6 anchors, IoU-pos=0.5, IoU-neg=0.4\nRetinaNet  |    ResNet-50     |        74.3           |    60 ms    |    3 anchors, IoU-pos=0.5, IoU-neg=0.4\nFoveaBox  |    ResNet-50     |        76.6           |    61 ms    |    $\\eta$=2.0\nFoveaBox  |    ResNet-50     |        76.0           |    61 ms    |    $\\eta$=1.5\nFoveaBox  |    ResNet-50     |        76.4           |    61 ms    |    $\\eta$=2.5\n\nFrom this experiment, we can see that $\\eta$ is robust across different datasets. FoveaBox also outperforms RetinaNet in Pascal VOC dataset (+1.1 mAP).\n \n\n--The scale is still discretized\nFollowing FPN, we detect different sizes of objects on different levels of feature maps. Unlike anchor-based detectors, which assign anchor boxes with different sizes to different feature levels, we directly limit the predicting range for  GT boxes. The multi-level prediction can help predicting objects of different scales.\n\n--Multiple bounding boxes around the same pixels?\nWe believe your concern is the ambiguous samples due to the overlapping in GT boxes. Here we calculate the ratios of ambiguous samples to all positive samples on COCO train 2017 split. With FPN, only 3.2% positive samples are ambiguous samples. Note that it does not imply that there are 3.2% samples FoveaBox cannot work, since these locations are associated with GT boxes with minimal area.  \nWe also count the detected boxes produced by the ambiguous locations, and only 1.5% detected boxes come from these locations. As shown in our experiments, these minimal samples do not make FoveaBox inferior to anchor-based methods.\n\n--Inference computation cost.\nPer the reviewer's suggestion, we evaluate the inference time of these methods. The experiments are performed on a single Nvidia V100 GPU  by averaging 10 runs. \n\nMethod                                          |    backbone           |    AP       |    Speed\nCenterNet (Zhou et al., 2019)    |    Hourglass-104   |    42.1     |    119 ms\nExtremNet (Zhou et al., 2019)   |    Hourglass-104   |    40.1     |    301 ms\nCornerNet (Law et al., 2018)      |    Hourglass-104   |    40.5     |    291 ms\nRetinaNet                                      |    ResNeXt-101      |    40.8     |    106 ms\nFCOS                                              |    ResNeXt-101       |    42.1     |     98 ms\nFoveaBox                                      |    ResNeXt-101       |    42.3     |     95 ms\n\nSince most of the inference time are spent in the ResNet backbone, the acceleration by our method is not so big compared to RetinaNet. We will include these experimental results in the revised version.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper86/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper86/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["taokongcn@gmail.com", "fcsun@tsinghua.edu.cn", "hpliu@tsinghua.edu.cn", "jiangyuning@bytedance.com", "lileilab@bytedance.com", "jshi@seas.upenn.edu"], "title": "FoveaBox: Beyound Anchor-based Object Detection", "authors": ["Tao Kong", "Fuchun Sun", "Huaping Liu", "Yuning Jiang", "Lei Li", "Jianbo Shi"], "pdf": "/pdf/c729b3a75ff60601acc3e68bd461d7d017e01970.pdf", "abstract": "We present FoveaBox, an accurate, flexible, and completely anchor-free framework for object detection. While almost all state-of-the-art object detectors utilize predefined anchors to enumerate possible locations, scales and aspect ratios for the search of the objects, their performance and generalization ability are also limited to the design of anchors. Instead, FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility, and (b) producing category-agnostic bounding box for each position that potentially contains an object. The scales of target boxes are naturally associated with feature pyramid representations. We demonstrate its effectiveness on standard benchmarks and report extensive experimental analysis.  Without bells and whistles, FoveaBox achieves state-of-the-art single model performance on the standard COCO detection benchmark. More importantly, FoveaBox avoids all computation and hyper-parameters related to anchor boxes, which are often sensitive to the final detection performance.  We believe the simple and effective approach will serve as a solid baseline and help ease future research for object detection. ", "code": "https://drive.google.com/file/d/1Iwe3Vfbunv5NaLFFn0i_fsfNT-XP0Zmx/view?usp=sharing", "keywords": [], "paperhash": "kong|foveabox_beyound_anchorbased_object_detection", "original_pdf": "/attachment/c729b3a75ff60601acc3e68bd461d7d017e01970.pdf", "_bibtex": "@misc{\nkong2020foveabox,\ntitle={FoveaBox: Beyound Anchor-based Object Detection},\nauthor={Tao Kong and Fuchun Sun and Huaping Liu and Yuning Jiang and Lei Li and Jianbo Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxFF34FPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxFF34FPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper86/Authors", "ICLR.cc/2020/Conference/Paper86/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper86/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper86/Reviewers", "ICLR.cc/2020/Conference/Paper86/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper86/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper86/Authors|ICLR.cc/2020/Conference/Paper86/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176604, "tmdate": 1576860529455, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper86/Authors", "ICLR.cc/2020/Conference/Paper86/Reviewers", "ICLR.cc/2020/Conference/Paper86/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper86/-/Official_Comment"}}}, {"id": "SyxULIiziH", "original": null, "number": 3, "cdate": 1573201485553, "ddate": null, "tcdate": 1573201485553, "tmdate": 1573201485553, "tddate": null, "forum": "HyxFF34FPr", "replyto": "rygtjxmbiH", "invitation": "ICLR.cc/2020/Conference/Paper86/-/Official_Comment", "content": {"title": "Thanks for your reply", "comment": "We thank Zhi Tian for pointing out the relation between FCOS and FoveaBox."}, "signatures": ["ICLR.cc/2020/Conference/Paper86/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper86/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["taokongcn@gmail.com", "fcsun@tsinghua.edu.cn", "hpliu@tsinghua.edu.cn", "jiangyuning@bytedance.com", "lileilab@bytedance.com", "jshi@seas.upenn.edu"], "title": "FoveaBox: Beyound Anchor-based Object Detection", "authors": ["Tao Kong", "Fuchun Sun", "Huaping Liu", "Yuning Jiang", "Lei Li", "Jianbo Shi"], "pdf": "/pdf/c729b3a75ff60601acc3e68bd461d7d017e01970.pdf", "abstract": "We present FoveaBox, an accurate, flexible, and completely anchor-free framework for object detection. While almost all state-of-the-art object detectors utilize predefined anchors to enumerate possible locations, scales and aspect ratios for the search of the objects, their performance and generalization ability are also limited to the design of anchors. Instead, FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility, and (b) producing category-agnostic bounding box for each position that potentially contains an object. The scales of target boxes are naturally associated with feature pyramid representations. We demonstrate its effectiveness on standard benchmarks and report extensive experimental analysis.  Without bells and whistles, FoveaBox achieves state-of-the-art single model performance on the standard COCO detection benchmark. More importantly, FoveaBox avoids all computation and hyper-parameters related to anchor boxes, which are often sensitive to the final detection performance.  We believe the simple and effective approach will serve as a solid baseline and help ease future research for object detection. ", "code": "https://drive.google.com/file/d/1Iwe3Vfbunv5NaLFFn0i_fsfNT-XP0Zmx/view?usp=sharing", "keywords": [], "paperhash": "kong|foveabox_beyound_anchorbased_object_detection", "original_pdf": "/attachment/c729b3a75ff60601acc3e68bd461d7d017e01970.pdf", "_bibtex": "@misc{\nkong2020foveabox,\ntitle={FoveaBox: Beyound Anchor-based Object Detection},\nauthor={Tao Kong and Fuchun Sun and Huaping Liu and Yuning Jiang and Lei Li and Jianbo Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxFF34FPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxFF34FPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper86/Authors", "ICLR.cc/2020/Conference/Paper86/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper86/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper86/Reviewers", "ICLR.cc/2020/Conference/Paper86/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper86/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper86/Authors|ICLR.cc/2020/Conference/Paper86/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176604, "tmdate": 1576860529455, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper86/Authors", "ICLR.cc/2020/Conference/Paper86/Reviewers", "ICLR.cc/2020/Conference/Paper86/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper86/-/Official_Comment"}}}, {"id": "SklvOrjfjS", "original": null, "number": 2, "cdate": 1573201262937, "ddate": null, "tcdate": 1573201262937, "tmdate": 1573201262937, "tddate": null, "forum": "HyxFF34FPr", "replyto": "rJlAJhYL9r", "invitation": "ICLR.cc/2020/Conference/Paper86/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "We thank the reviewer for the helpful feedback and suggestions. The main concerns of the reviewer are addressed separately below.\n\n--Comparison with \"single\" anchor\nFoveaBox views locations as training samples instead of anchors, like FCNs for semantic segmentation. It directly predicts the object existing possibility and boundary without any anchors. Previous works usually utilize \"discrete\" anchors to enumerate possible scales/aspect ratios on each output position and further refine the anchors. We believe that our scheme is simpler and more flexible. As shown in Table 1(a), FoveaBox gives 4.9 AP gains compared with single-anchor based RetinaNet.\n\n--On single-scale feature map\nPer the reviewer's concern, we conducted the ablation on single feature map of ResNet-50 (C4). In this ablation, we compare the region proposal performance with RPN (both on C4/FPN of ResNet-50). Here we can see that FoveaBox-C4 gets comparable performance with RPN-C4 (51.0 v.s. 51.6), and FoveaBox-C4 is much better than RPN-C4 with single anchor (+17.1). Another interesting observation is that FoveaBox-C4 gets comparable performance with RPN-FPN with single anchor (51.0 v.s. 50.1).  \n\nMethod                       |     backbone   |  AR@1000\nRPN                             |     R-50-C4       |  51.6\nRPN-single-anchor   |     R-50-C4       |  33.9\nFoveaBox                   |     R-50-C4       |  51.0\nRPN                             |     R-50-FPN    |  56.6\nRPN-single-anchor   |     R-50-FPN    |  50.1\nFoveaBox                   |     R-50-FPN    |  61.5\n\n\n--Comparison with DeepMask\n(a) DeepMask is designed for object proposals while FoveaBox is developed for object detection.\n(b) DeepMask relies on predefined patches on each output spatial position to define positive/negative training samples, and the predefined patches are similar to anchors. On the other hand, FoveaBox generates positive/negative training samples directly from GT boxes. \n(c) At inference phase, the patch is used in DeepMask to generate object mask and box. In FoveaBox, the object box is directly learnt by the box branch. \n(d) As for performance, the box proposal recall of FoveaBox is much higher than that in DeepMask (44.6AR@1000).\n\n--Comparison with TensorMask and CenterNet\nHere we first note that TensorMask, CenterNet and FoveaBox are concurrent works. TensorMask tries to investigate the paradigm of dense instance segmentation. We agree that CenterNet (Zhou et al., 2019) and our FoveaBox share similar ideas to detect objects, however as Zhi (one author of FCOS) said, concurrent works should not weaken the originality of our work.\n\nFinally, the ablations and discussions will be updated to the revised paper. Thanks and we are looking forward to your further valuable response."}, "signatures": ["ICLR.cc/2020/Conference/Paper86/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper86/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["taokongcn@gmail.com", "fcsun@tsinghua.edu.cn", "hpliu@tsinghua.edu.cn", "jiangyuning@bytedance.com", "lileilab@bytedance.com", "jshi@seas.upenn.edu"], "title": "FoveaBox: Beyound Anchor-based Object Detection", "authors": ["Tao Kong", "Fuchun Sun", "Huaping Liu", "Yuning Jiang", "Lei Li", "Jianbo Shi"], "pdf": "/pdf/c729b3a75ff60601acc3e68bd461d7d017e01970.pdf", "abstract": "We present FoveaBox, an accurate, flexible, and completely anchor-free framework for object detection. While almost all state-of-the-art object detectors utilize predefined anchors to enumerate possible locations, scales and aspect ratios for the search of the objects, their performance and generalization ability are also limited to the design of anchors. Instead, FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility, and (b) producing category-agnostic bounding box for each position that potentially contains an object. The scales of target boxes are naturally associated with feature pyramid representations. We demonstrate its effectiveness on standard benchmarks and report extensive experimental analysis.  Without bells and whistles, FoveaBox achieves state-of-the-art single model performance on the standard COCO detection benchmark. More importantly, FoveaBox avoids all computation and hyper-parameters related to anchor boxes, which are often sensitive to the final detection performance.  We believe the simple and effective approach will serve as a solid baseline and help ease future research for object detection. ", "code": "https://drive.google.com/file/d/1Iwe3Vfbunv5NaLFFn0i_fsfNT-XP0Zmx/view?usp=sharing", "keywords": [], "paperhash": "kong|foveabox_beyound_anchorbased_object_detection", "original_pdf": "/attachment/c729b3a75ff60601acc3e68bd461d7d017e01970.pdf", "_bibtex": "@misc{\nkong2020foveabox,\ntitle={FoveaBox: Beyound Anchor-based Object Detection},\nauthor={Tao Kong and Fuchun Sun and Huaping Liu and Yuning Jiang and Lei Li and Jianbo Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxFF34FPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxFF34FPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper86/Authors", "ICLR.cc/2020/Conference/Paper86/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper86/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper86/Reviewers", "ICLR.cc/2020/Conference/Paper86/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper86/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper86/Authors|ICLR.cc/2020/Conference/Paper86/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176604, "tmdate": 1576860529455, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper86/Authors", "ICLR.cc/2020/Conference/Paper86/Reviewers", "ICLR.cc/2020/Conference/Paper86/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper86/-/Official_Comment"}}}, {"id": "rygtjxmbiH", "original": null, "number": 2, "cdate": 1573101729466, "ddate": null, "tcdate": 1573101729466, "tmdate": 1573101729466, "tddate": null, "forum": "HyxFF34FPr", "replyto": "H1e8TZVM9S", "invitation": "ICLR.cc/2020/Conference/Paper86/-/Public_Comment", "content": {"title": "Relation between Foveabox and FCOS", "comment": "I am one of the authors of FCOS (Tian et al., 2019). Just for your information, Foveabox and FCOS are contemporary and independent works, so FCOS should not weaken the originality of this work. Thank you."}, "signatures": ["~Zhi_Tian2"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Zhi_Tian2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["taokongcn@gmail.com", "fcsun@tsinghua.edu.cn", "hpliu@tsinghua.edu.cn", "jiangyuning@bytedance.com", "lileilab@bytedance.com", "jshi@seas.upenn.edu"], "title": "FoveaBox: Beyound Anchor-based Object Detection", "authors": ["Tao Kong", "Fuchun Sun", "Huaping Liu", "Yuning Jiang", "Lei Li", "Jianbo Shi"], "pdf": "/pdf/c729b3a75ff60601acc3e68bd461d7d017e01970.pdf", "abstract": "We present FoveaBox, an accurate, flexible, and completely anchor-free framework for object detection. While almost all state-of-the-art object detectors utilize predefined anchors to enumerate possible locations, scales and aspect ratios for the search of the objects, their performance and generalization ability are also limited to the design of anchors. Instead, FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility, and (b) producing category-agnostic bounding box for each position that potentially contains an object. The scales of target boxes are naturally associated with feature pyramid representations. We demonstrate its effectiveness on standard benchmarks and report extensive experimental analysis.  Without bells and whistles, FoveaBox achieves state-of-the-art single model performance on the standard COCO detection benchmark. More importantly, FoveaBox avoids all computation and hyper-parameters related to anchor boxes, which are often sensitive to the final detection performance.  We believe the simple and effective approach will serve as a solid baseline and help ease future research for object detection. ", "code": "https://drive.google.com/file/d/1Iwe3Vfbunv5NaLFFn0i_fsfNT-XP0Zmx/view?usp=sharing", "keywords": [], "paperhash": "kong|foveabox_beyound_anchorbased_object_detection", "original_pdf": "/attachment/c729b3a75ff60601acc3e68bd461d7d017e01970.pdf", "_bibtex": "@misc{\nkong2020foveabox,\ntitle={FoveaBox: Beyound Anchor-based Object Detection},\nauthor={Tao Kong and Fuchun Sun and Huaping Liu and Yuning Jiang and Lei Li and Jianbo Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxFF34FPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxFF34FPr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504214148, "tmdate": 1576860563218, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper86/Authors", "ICLR.cc/2020/Conference/Paper86/Reviewers", "ICLR.cc/2020/Conference/Paper86/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper86/-/Public_Comment"}}}, {"id": "rJlOCkwbqH", "original": null, "number": 1, "cdate": 1572069327958, "ddate": null, "tcdate": 1572069327958, "tmdate": 1572972640411, "tddate": null, "forum": "HyxFF34FPr", "replyto": "HyxFF34FPr", "invitation": "ICLR.cc/2020/Conference/Paper86/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "## Overview\nThe paper tackles object detection without predefined anchors (sliding windows). \nThe paper is well motivated and existing detection methods often rely on anchors, which may limits their potential. So this paper is solving an interesting problem and seems novel.\nThe paper provides experiments to support the proposed architecture empirically.\n\n## Summary of the contribution:\n1. The paper proposed an object detection approach called FoveaBox that does not rely on anchors (sliding widows). \n2. The paper shows that FoveaBox outperforms some existing object detection methods.\n3. The paper shows that FoveaBox can also be used for object proposals by changing the classification target to class-agnostic head.\n\n## My feedback:\nI think the paper is well motivated and has value to the object detection community. So I am leaning positive to accept this paper. The major reasons are:\n1. The paper studies an interesting architecture which does not rely on sliding windows and shows its effectiveness. The problem seems not very well studied in the existing object detection literature.\n2. The paper provides sufficient ablation study to analyze and understand the proposed methods.\nHowever, I believe the paper could be significantly improved especially in the writing. So my position is not strong.\n\n## Improvements\n1. The writing can be significantly improved. The paper reads a bit confusing and unclear especially at the technical part. The paper could benefit from a clear overview figure about the proposed approach. Figure 4 seems to be doing that illustration but giving only the tensor shape seems quite confusing. I also listed some questions below about the actual algorithm.\n2. The paper could be improved with experiments and ablation on another dataset. Current ablation shows that \\eta should be 2.0 for best performance but without a second dataset it is hard to say this value is general. So one may have to tune this parameters in different datasets.\n3. The scale is still discretized. So that is essentially anchors in the scale space. I wonder how could the approach applies to scale to?\n\n## Questions\n1. How would the proposed approach address the issue when there are multiple bounding boxes around the same pixels? It seems the current approach is predicting a box per pixel?\n2. What is the inference computation cost? And do you have a comparison with existing methods on the compute cost?"}, "signatures": ["ICLR.cc/2020/Conference/Paper86/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper86/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["taokongcn@gmail.com", "fcsun@tsinghua.edu.cn", "hpliu@tsinghua.edu.cn", "jiangyuning@bytedance.com", "lileilab@bytedance.com", "jshi@seas.upenn.edu"], "title": "FoveaBox: Beyound Anchor-based Object Detection", "authors": ["Tao Kong", "Fuchun Sun", "Huaping Liu", "Yuning Jiang", "Lei Li", "Jianbo Shi"], "pdf": "/pdf/c729b3a75ff60601acc3e68bd461d7d017e01970.pdf", "abstract": "We present FoveaBox, an accurate, flexible, and completely anchor-free framework for object detection. While almost all state-of-the-art object detectors utilize predefined anchors to enumerate possible locations, scales and aspect ratios for the search of the objects, their performance and generalization ability are also limited to the design of anchors. Instead, FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility, and (b) producing category-agnostic bounding box for each position that potentially contains an object. The scales of target boxes are naturally associated with feature pyramid representations. We demonstrate its effectiveness on standard benchmarks and report extensive experimental analysis.  Without bells and whistles, FoveaBox achieves state-of-the-art single model performance on the standard COCO detection benchmark. More importantly, FoveaBox avoids all computation and hyper-parameters related to anchor boxes, which are often sensitive to the final detection performance.  We believe the simple and effective approach will serve as a solid baseline and help ease future research for object detection. ", "code": "https://drive.google.com/file/d/1Iwe3Vfbunv5NaLFFn0i_fsfNT-XP0Zmx/view?usp=sharing", "keywords": [], "paperhash": "kong|foveabox_beyound_anchorbased_object_detection", "original_pdf": "/attachment/c729b3a75ff60601acc3e68bd461d7d017e01970.pdf", "_bibtex": "@misc{\nkong2020foveabox,\ntitle={FoveaBox: Beyound Anchor-based Object Detection},\nauthor={Tao Kong and Fuchun Sun and Huaping Liu and Yuning Jiang and Lei Li and Jianbo Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxFF34FPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyxFF34FPr", "replyto": "HyxFF34FPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper86/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper86/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576086993787, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper86/Reviewers"], "noninvitees": [], "tcdate": 1570237757283, "tmdate": 1576086993805, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper86/-/Official_Review"}}}, {"id": "rJlAJhYL9r", "original": null, "number": 3, "cdate": 1572408294392, "ddate": null, "tcdate": 1572408294392, "tmdate": 1572972640320, "tddate": null, "forum": "HyxFF34FPr", "replyto": "HyxFF34FPr", "invitation": "ICLR.cc/2020/Conference/Paper86/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper introduces foveabox, a method that performs \"keypoint\" like object detection -- instead of \"anchor\" based detection (to be discussed later). The idea is simple: predict class labels for pixels that fall within (a reduced version) the GT boxes of the instance; and predict bounding box offsets for those positive pixels. The idea is built on top of the FPN backbone, where a set of feature maps (each representing a specific scale) are used to detect object boxes in multiple scales.  The method is mainly compared against RetinaNet (which is \"anchor\" driven), and also compared against other more recent methods (ExtremeNet, CenterNet, FCOS) etc. \n\n+ The paper is quite well written and structured, the illustrations are also clear;\n+ I have also read its previous version, and the new version has added a significant amount of work improving it -- e.g. added feature alignment and group norm;\n+ I haven't fully checked the results section of other concurrent papers for full comparison, but the current results are among the state-of-the-art for one-stage detectors.\n\n- I don't think the paper breaks away from the notion of \"anchors\". The current approach is in fact implicitly defining a *single* anchor for each feature map, and changing the assignment rule from IoU based to distance/scale based. I firmly believe that such modification can lead to concrete improvements -- for example recent work has shown that changing assignment rule can improve AP even with fewer anchors (TensorMask); but to say that the paper breaks away from the usage of anchors, it is a bit far for me. I think I will be way more convinced if the experiments are done on a single-scale feature map (anchor also works with single-scale but I am not sure without defining different anchors it can work just on C4 for example).\n- Also because such assignment rule has been there before (e.g. DeepMask has used x, y, scale for assignment), and with other recent works (CenterNet, Objects as Keypoints), the contribution of this work is fairly limited\n\nTherefore, I vote for reject."}, "signatures": ["ICLR.cc/2020/Conference/Paper86/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper86/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["taokongcn@gmail.com", "fcsun@tsinghua.edu.cn", "hpliu@tsinghua.edu.cn", "jiangyuning@bytedance.com", "lileilab@bytedance.com", "jshi@seas.upenn.edu"], "title": "FoveaBox: Beyound Anchor-based Object Detection", "authors": ["Tao Kong", "Fuchun Sun", "Huaping Liu", "Yuning Jiang", "Lei Li", "Jianbo Shi"], "pdf": "/pdf/c729b3a75ff60601acc3e68bd461d7d017e01970.pdf", "abstract": "We present FoveaBox, an accurate, flexible, and completely anchor-free framework for object detection. While almost all state-of-the-art object detectors utilize predefined anchors to enumerate possible locations, scales and aspect ratios for the search of the objects, their performance and generalization ability are also limited to the design of anchors. Instead, FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility, and (b) producing category-agnostic bounding box for each position that potentially contains an object. The scales of target boxes are naturally associated with feature pyramid representations. We demonstrate its effectiveness on standard benchmarks and report extensive experimental analysis.  Without bells and whistles, FoveaBox achieves state-of-the-art single model performance on the standard COCO detection benchmark. More importantly, FoveaBox avoids all computation and hyper-parameters related to anchor boxes, which are often sensitive to the final detection performance.  We believe the simple and effective approach will serve as a solid baseline and help ease future research for object detection. ", "code": "https://drive.google.com/file/d/1Iwe3Vfbunv5NaLFFn0i_fsfNT-XP0Zmx/view?usp=sharing", "keywords": [], "paperhash": "kong|foveabox_beyound_anchorbased_object_detection", "original_pdf": "/attachment/c729b3a75ff60601acc3e68bd461d7d017e01970.pdf", "_bibtex": "@misc{\nkong2020foveabox,\ntitle={FoveaBox: Beyound Anchor-based Object Detection},\nauthor={Tao Kong and Fuchun Sun and Huaping Liu and Yuning Jiang and Lei Li and Jianbo Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxFF34FPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyxFF34FPr", "replyto": "HyxFF34FPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper86/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper86/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576086993787, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper86/Reviewers"], "noninvitees": [], "tcdate": 1570237757283, "tmdate": 1576086993805, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper86/-/Official_Review"}}}, {"id": "ryx_zZm2_B", "original": null, "number": 1, "cdate": 1570677008338, "ddate": null, "tcdate": 1570677008338, "tmdate": 1570677008338, "tddate": null, "forum": "HyxFF34FPr", "replyto": "SJgLDgzndr", "invitation": "ICLR.cc/2020/Conference/Paper86/-/Official_Comment", "content": {"comment": "There are several versions of these methods in arXiv, the latest versions of these methods may get better performance based on better training/testing strategies. \n(a) CenterNet(Duan et al.,2019), we have checked the paper and the results are mistakenly taken from another CenterNet (Zhou et al., 2019) (https://arxiv.org/pdf/1904.07850.pdf). Thanks and we will fix the issue in the next version. Here we note that CenterNet are using original and  flipped image for inference.\n(b) FCOS (Tian et al., 2019), the original FCOS gets 42.1% AP (https://arxiv.org/pdf/1904.01355v3.pdf). The latest FCOS could get better performance (+0.6) with centerness-regression and center-sampling. \n(c) RPDet (Yang et al., 2019), we believe ResNet-DCN version of RPDet is not equivalent with FoveaBox-align, because FoveaBox-align only add one layer of deformable conv. On the other hand, ResNet-DCN version of RPDet uses DCN backbone and DCN head. FoveaBox-ResNet101-align (42.1%) is the equivalence of RPDet-ResNet101 (41.0%).  We believe our FoveaBox could get better performance with DCN backbone.  We will fix the mistake \"ResNeXt-101---->ResNet-101\".\n\nFinally, thank you for your valuable comments and suggestions.", "title": "Thanks for your interest in our paper"}, "signatures": ["ICLR.cc/2020/Conference/Paper86/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper86/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["taokongcn@gmail.com", "fcsun@tsinghua.edu.cn", "hpliu@tsinghua.edu.cn", "jiangyuning@bytedance.com", "lileilab@bytedance.com", "jshi@seas.upenn.edu"], "title": "FoveaBox: Beyound Anchor-based Object Detection", "authors": ["Tao Kong", "Fuchun Sun", "Huaping Liu", "Yuning Jiang", "Lei Li", "Jianbo Shi"], "pdf": "/pdf/c729b3a75ff60601acc3e68bd461d7d017e01970.pdf", "abstract": "We present FoveaBox, an accurate, flexible, and completely anchor-free framework for object detection. While almost all state-of-the-art object detectors utilize predefined anchors to enumerate possible locations, scales and aspect ratios for the search of the objects, their performance and generalization ability are also limited to the design of anchors. Instead, FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility, and (b) producing category-agnostic bounding box for each position that potentially contains an object. The scales of target boxes are naturally associated with feature pyramid representations. We demonstrate its effectiveness on standard benchmarks and report extensive experimental analysis.  Without bells and whistles, FoveaBox achieves state-of-the-art single model performance on the standard COCO detection benchmark. More importantly, FoveaBox avoids all computation and hyper-parameters related to anchor boxes, which are often sensitive to the final detection performance.  We believe the simple and effective approach will serve as a solid baseline and help ease future research for object detection. ", "code": "https://drive.google.com/file/d/1Iwe3Vfbunv5NaLFFn0i_fsfNT-XP0Zmx/view?usp=sharing", "keywords": [], "paperhash": "kong|foveabox_beyound_anchorbased_object_detection", "original_pdf": "/attachment/c729b3a75ff60601acc3e68bd461d7d017e01970.pdf", "_bibtex": "@misc{\nkong2020foveabox,\ntitle={FoveaBox: Beyound Anchor-based Object Detection},\nauthor={Tao Kong and Fuchun Sun and Huaping Liu and Yuning Jiang and Lei Li and Jianbo Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxFF34FPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxFF34FPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper86/Authors", "ICLR.cc/2020/Conference/Paper86/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper86/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper86/Reviewers", "ICLR.cc/2020/Conference/Paper86/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper86/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper86/Authors|ICLR.cc/2020/Conference/Paper86/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176604, "tmdate": 1576860529455, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper86/Authors", "ICLR.cc/2020/Conference/Paper86/Reviewers", "ICLR.cc/2020/Conference/Paper86/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper86/-/Official_Comment"}}}, {"id": "SJgLDgzndr", "original": null, "number": 1, "cdate": 1570672734095, "ddate": null, "tcdate": 1570672734095, "tmdate": 1570672734095, "tddate": null, "forum": "HyxFF34FPr", "replyto": "HyxFF34FPr", "invitation": "ICLR.cc/2020/Conference/Paper86/-/Public_Comment", "content": {"comment": "Hi,\n\nThanks for sharing your work with our community. The idea of scale assignment is inspiring.\n\nI have some questions about Table 4. There seem to be some errors in several entries of Table 4. For example, the CenterNet (Duan et al., 2019) with Hourglass-104 achieves a single-model single scale AP of 44.9% instead of 42.1% as reported in this paper. The FCOS (Tian et al., 2019) achieves 42.7% AP and 43.2% AP with ResNeXt-101-32x8d and ResNeXt-101-64x4d respectively, both higher than the reported AP (42.1%) in this manuscript. And for RPDet (Yang et al., 2019), there is no result based on ResNeXt-101. The original paper presents a ResNet-101-DCN version of RPDet, which is the equivalence of the FoveaBox-align with ResNet-101. And RPDet using ResNet-101-DCN also outperforms FoveaBox-align using ResNet-101 (42.8% vs 42.1%).\n\nGiven the above facts, it may be too aggressive to claim that all instantiations of the proposed method outperform baseline variants of previous state-of-the-art models.", "title": "Questions about Table 4"}, "signatures": ["~Lei_Wu5"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Lei_Wu5", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["taokongcn@gmail.com", "fcsun@tsinghua.edu.cn", "hpliu@tsinghua.edu.cn", "jiangyuning@bytedance.com", "lileilab@bytedance.com", "jshi@seas.upenn.edu"], "title": "FoveaBox: Beyound Anchor-based Object Detection", "authors": ["Tao Kong", "Fuchun Sun", "Huaping Liu", "Yuning Jiang", "Lei Li", "Jianbo Shi"], "pdf": "/pdf/c729b3a75ff60601acc3e68bd461d7d017e01970.pdf", "abstract": "We present FoveaBox, an accurate, flexible, and completely anchor-free framework for object detection. While almost all state-of-the-art object detectors utilize predefined anchors to enumerate possible locations, scales and aspect ratios for the search of the objects, their performance and generalization ability are also limited to the design of anchors. Instead, FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility, and (b) producing category-agnostic bounding box for each position that potentially contains an object. The scales of target boxes are naturally associated with feature pyramid representations. We demonstrate its effectiveness on standard benchmarks and report extensive experimental analysis.  Without bells and whistles, FoveaBox achieves state-of-the-art single model performance on the standard COCO detection benchmark. More importantly, FoveaBox avoids all computation and hyper-parameters related to anchor boxes, which are often sensitive to the final detection performance.  We believe the simple and effective approach will serve as a solid baseline and help ease future research for object detection. ", "code": "https://drive.google.com/file/d/1Iwe3Vfbunv5NaLFFn0i_fsfNT-XP0Zmx/view?usp=sharing", "keywords": [], "paperhash": "kong|foveabox_beyound_anchorbased_object_detection", "original_pdf": "/attachment/c729b3a75ff60601acc3e68bd461d7d017e01970.pdf", "_bibtex": "@misc{\nkong2020foveabox,\ntitle={FoveaBox: Beyound Anchor-based Object Detection},\nauthor={Tao Kong and Fuchun Sun and Huaping Liu and Yuning Jiang and Lei Li and Jianbo Shi},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxFF34FPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxFF34FPr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504214148, "tmdate": 1576860563218, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper86/Authors", "ICLR.cc/2020/Conference/Paper86/Reviewers", "ICLR.cc/2020/Conference/Paper86/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper86/-/Public_Comment"}}}], "count": 12}