{"notes": [{"id": "HklE01BYDB", "original": "rklE-vJFwr", "number": 2020, "cdate": 1569439691574, "ddate": null, "tcdate": 1569439691574, "tmdate": 1577168293957, "tddate": null, "forum": "HklE01BYDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["denisyarats@cs.nyu.edu", "amyzhang@fb.com", "ik1078@nyu.edu", "brandon.amos.cs@gmail.com", "jpineau@fb.com", "robfergus@fb.com"], "title": "Improving Sample Efficiency in Model-Free Reinforcement Learning from Images", "authors": ["Denis Yarats", "Amy Zhang", "Ilya Kostrikov", "Brandon Amos", "Joelle Pineau", "Rob Fergus"], "pdf": "/pdf/26bfa35b5ec3aeacf128b36cd8d5347f4cabcd26.pdf", "TL;DR": "We design a simple and efficient model-free off-policy method for image-based reinforcement learning that matches the state-of-the-art model-based methods in sample efficiency", "abstract": "Training an agent to solve control tasks directly from high-dimensional images with model-free reinforcement learning (RL) has proven difficult. The agent needs to learn  a latent representation together with a control policy to perform the task. Fitting a high-capacity encoder using a scarce reward signal is not only extremely sample inefficient, but also prone to suboptimal convergence. Two ways to improve sample efficiency are to learn a good feature representation and use off-policy algorithms. We dissect various approaches of learning good latent features, and conclude that the image reconstruction loss is the essential ingredient that enables efficient and stable representation learning in image-based RL. Following these findings, we devise an off-policy actor-critic algorithm with an auxiliary decoder that trains end-to-end and matches state-of-the-art performance across both model-free and model-based algorithms on many challenging control tasks. We release our code to encourage future research on image-based RL.", "code": "https://drive.google.com/file/d/1slqgCj3f8br5K6KiHUKHJBnjAhYnw3M-/view", "keywords": ["reinforcement learning", "model-free", "off-policy", "image-based reinforcement learning", "continuous control"], "paperhash": "yarats|improving_sample_efficiency_in_modelfree_reinforcement_learning_from_images", "original_pdf": "/attachment/26bfa35b5ec3aeacf128b36cd8d5347f4cabcd26.pdf", "_bibtex": "@misc{\nyarats2020improving,\ntitle={Improving Sample Efficiency in Model-Free Reinforcement Learning from Images},\nauthor={Denis Yarats and Amy Zhang and Ilya Kostrikov and Brandon Amos and Joelle Pineau and Rob Fergus},\nyear={2020},\nurl={https://openreview.net/forum?id=HklE01BYDB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "XM93tEDQra", "original": null, "number": 1, "cdate": 1576798738455, "ddate": null, "tcdate": 1576798738455, "tmdate": 1576800897917, "tddate": null, "forum": "HklE01BYDB", "replyto": "HklE01BYDB", "invitation": "ICLR.cc/2020/Conference/Paper2020/-/Decision", "content": {"decision": "Reject", "comment": "The paper investigates how sample efficiency of image based model-free RL can be improved  by including an image reconstruction loss as an auxiliary task and applies it to soft actor-critic. The method is demonstrated to yield a substantial improvement compared to SAC learned directly from pixels, and comparable performance to other prior works, such as SLAC and PlaNet, but with a simpler learning setup. The reviewers generally appreciate the clarity of presentation and good experimental evaluation. However, all reviewers raise concerns regarding limited novelty, as auxiliary losses for RL have been studied before, and the contribution is mainly in the design choices of the implementation. In this view, and given that the results are on a par with SOTA, the contribution of this paper seems too incremental for publishing in this venue, and I\u2019m recommending rejection. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["denisyarats@cs.nyu.edu", "amyzhang@fb.com", "ik1078@nyu.edu", "brandon.amos.cs@gmail.com", "jpineau@fb.com", "robfergus@fb.com"], "title": "Improving Sample Efficiency in Model-Free Reinforcement Learning from Images", "authors": ["Denis Yarats", "Amy Zhang", "Ilya Kostrikov", "Brandon Amos", "Joelle Pineau", "Rob Fergus"], "pdf": "/pdf/26bfa35b5ec3aeacf128b36cd8d5347f4cabcd26.pdf", "TL;DR": "We design a simple and efficient model-free off-policy method for image-based reinforcement learning that matches the state-of-the-art model-based methods in sample efficiency", "abstract": "Training an agent to solve control tasks directly from high-dimensional images with model-free reinforcement learning (RL) has proven difficult. The agent needs to learn  a latent representation together with a control policy to perform the task. Fitting a high-capacity encoder using a scarce reward signal is not only extremely sample inefficient, but also prone to suboptimal convergence. Two ways to improve sample efficiency are to learn a good feature representation and use off-policy algorithms. We dissect various approaches of learning good latent features, and conclude that the image reconstruction loss is the essential ingredient that enables efficient and stable representation learning in image-based RL. Following these findings, we devise an off-policy actor-critic algorithm with an auxiliary decoder that trains end-to-end and matches state-of-the-art performance across both model-free and model-based algorithms on many challenging control tasks. We release our code to encourage future research on image-based RL.", "code": "https://drive.google.com/file/d/1slqgCj3f8br5K6KiHUKHJBnjAhYnw3M-/view", "keywords": ["reinforcement learning", "model-free", "off-policy", "image-based reinforcement learning", "continuous control"], "paperhash": "yarats|improving_sample_efficiency_in_modelfree_reinforcement_learning_from_images", "original_pdf": "/attachment/26bfa35b5ec3aeacf128b36cd8d5347f4cabcd26.pdf", "_bibtex": "@misc{\nyarats2020improving,\ntitle={Improving Sample Efficiency in Model-Free Reinforcement Learning from Images},\nauthor={Denis Yarats and Amy Zhang and Ilya Kostrikov and Brandon Amos and Joelle Pineau and Rob Fergus},\nyear={2020},\nurl={https://openreview.net/forum?id=HklE01BYDB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HklE01BYDB", "replyto": "HklE01BYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795725755, "tmdate": 1576800277721, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2020/-/Decision"}}}, {"id": "r1ly8pPKuH", "original": null, "number": 1, "cdate": 1570499910666, "ddate": null, "tcdate": 1570499910666, "tmdate": 1574203311261, "tddate": null, "forum": "HklE01BYDB", "replyto": "HklE01BYDB", "invitation": "ICLR.cc/2020/Conference/Paper2020/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "The paper aims to tackle the problem of improving sample efficiency of model-free, off-policy reinforcement learning in an image-based environment. They do so by taking SAC and adding a deterministic autoencoder, trained end-to-end with the actor and critic, with the actor and critic trained on top of the learned latent space z. They call this SAC-AE. Experiments in the DeepMind control suite demonstrate that the result models train much faster than SAC directly on the pixels, in some cases reaching close to the performance of SAC on raw state. Ablation studies demonstrate their approach is most stable with deterministic autoencoders proposed by (Ghosh et al, 2019), rather than the beta-VAE autoencoder proposed in (Nair et al, 2018), end-to-end learning of the autoencoder gives improved performance, and the encoder transfers to some similar tasks.\n\nI thought the paper was written well, and its experiments were done quite carefully, but it was lacking on the novelty front. At a high level, the paper has many similarities with the UNREAL paper (Jaderberg et al, 2017), which is acknowledged in the related work. This paper says it differs from UNREAL because they use an off-policy algorithm, and that UNREAL's auxiliary tasks are based off real-world inductive priors.\n\nI don't see the off-policy distinction as very relevant, because in the end, both UNREAL and SAC-AE are actor-critic algorithms (using A3C and SAC respectively). The way that SAC is used in the paper always collects data in a near on-policy manner, and UNREAL includes experience replay from a replay buffer, which introduces some off-policy nature to UNREAL as well. Therefore this doesn't feel like a strong argument.\n\nFurthermore, although some of the auxiliary tasks in UNREAL are based off human intuition for what makes sense in those environments, they also include task-agnostic auxiliary tasks: reward prediction and pixel-level control. These do not depend on real-world inductive priors, and are shown to improve performance.\n\nOverall, this doesn't feel like a strong enough contribution for ICLR.\n\nMore specific comments:\n* Section 6.1 examines the representation power of the encoder by reconstructing proprioceptive state from the encoder. I am not sure the comparison between SAC+AE and SAC is particularly meaningful here. The predictors are learned on top of the encoder output, and in SAC+AE we would expect task information to be encoded in the learned z. But in baseline SAC, there is no reason to expect this to be true - task information is more likely to be distributed across the entire network architecture. The case for SAC+AE seems much stronger from the reward curves, rather than these plots.\n* The paper argues that their approach is stable and sample-efficient, but when looking at the reward curves, it looked about as stable as SAC. Figure 3 (where they do not train the VAE end-to-end in the red curve) has a similar story. This makes me believe that any claims of added stability are more thanks to SAC, rather than proposed methods.\n\nEdit: I would like to clarify that the rating system only provides a 3 for Weak Reject and 6 for Weak Accept. On a 1-10 scale I would rate this as a 5, I feel it is closer to Weak Accept than Weak Reject.\n\nEdit 2: I've read the other author's comments. I'm not particularly convinced by the case for novelty, but I didn't realize that UNREAL's replay buffer was only 2k transitions instead of 1 million transitions. On reflection, I believe the main contribution here is showing that deterministic autoencoders are more reliable than stochastic ones for the RL setting, and this isn't the biggest contribution, but it's enough to make me update to weak accept.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2020/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2020/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["denisyarats@cs.nyu.edu", "amyzhang@fb.com", "ik1078@nyu.edu", "brandon.amos.cs@gmail.com", "jpineau@fb.com", "robfergus@fb.com"], "title": "Improving Sample Efficiency in Model-Free Reinforcement Learning from Images", "authors": ["Denis Yarats", "Amy Zhang", "Ilya Kostrikov", "Brandon Amos", "Joelle Pineau", "Rob Fergus"], "pdf": "/pdf/26bfa35b5ec3aeacf128b36cd8d5347f4cabcd26.pdf", "TL;DR": "We design a simple and efficient model-free off-policy method for image-based reinforcement learning that matches the state-of-the-art model-based methods in sample efficiency", "abstract": "Training an agent to solve control tasks directly from high-dimensional images with model-free reinforcement learning (RL) has proven difficult. The agent needs to learn  a latent representation together with a control policy to perform the task. Fitting a high-capacity encoder using a scarce reward signal is not only extremely sample inefficient, but also prone to suboptimal convergence. Two ways to improve sample efficiency are to learn a good feature representation and use off-policy algorithms. We dissect various approaches of learning good latent features, and conclude that the image reconstruction loss is the essential ingredient that enables efficient and stable representation learning in image-based RL. Following these findings, we devise an off-policy actor-critic algorithm with an auxiliary decoder that trains end-to-end and matches state-of-the-art performance across both model-free and model-based algorithms on many challenging control tasks. We release our code to encourage future research on image-based RL.", "code": "https://drive.google.com/file/d/1slqgCj3f8br5K6KiHUKHJBnjAhYnw3M-/view", "keywords": ["reinforcement learning", "model-free", "off-policy", "image-based reinforcement learning", "continuous control"], "paperhash": "yarats|improving_sample_efficiency_in_modelfree_reinforcement_learning_from_images", "original_pdf": "/attachment/26bfa35b5ec3aeacf128b36cd8d5347f4cabcd26.pdf", "_bibtex": "@misc{\nyarats2020improving,\ntitle={Improving Sample Efficiency in Model-Free Reinforcement Learning from Images},\nauthor={Denis Yarats and Amy Zhang and Ilya Kostrikov and Brandon Amos and Joelle Pineau and Rob Fergus},\nyear={2020},\nurl={https://openreview.net/forum?id=HklE01BYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HklE01BYDB", "replyto": "HklE01BYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2020/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2020/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576594740000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2020/Reviewers"], "noninvitees": [], "tcdate": 1570237728921, "tmdate": 1574722994101, "super": "ICLR.cc/2020/Conference/-/Official_Review", "final": [], "signatures": ["~Super_User1"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2020/-/Official_Review"}}}, {"id": "H1l3eXJ8tB", "original": null, "number": 3, "cdate": 1571316468174, "ddate": null, "tcdate": 1571316468174, "tmdate": 1573310351633, "tddate": null, "forum": "HklE01BYDB", "replyto": "HklE01BYDB", "invitation": "ICLR.cc/2020/Conference/Paper2020/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "Summary\n\nThis paper proposes an approach to make the model-free state-of-the-art soft actor-critic (SAC) algorithm for proprioceptive state spaces sample-efficient in higher-dimensional visual state spaces. To this end, an encoder-decoder structure to minimize image reconstruction loss is added to SAC's learning objectives. Importantly, the encoder is shared between the encoder-decoder architecture, the critic and the policy. Furthermore, Q-critic updates backpropagate through the encoder such that encoder weights need to trade off image reconstruction and critic learning. The approach is evaluated on six tasks from the DeepMind control suite and compared against proprioceptive SAC, pixel-based SAC, D4PG as well as to the model-based baselines PlaNet and SLAC. The proposed method seems to achieve results competitive with the model-based baselines and significantly improves over raw pixel-based SAC. Further ablation studies are presented to investigate the information capacity of the learned latent representation and generalization to unseen tasks.\n\nQuality\n\nSince this is a paper with a strong practical focus, the quality needs to be judged based on the experiments. The quality of those are good in terms of the number of environments, baselines, benchmarks and seeds. I also liked the ablation studies to investigate latent representations and generalization to new tasks.\n\nClarity\n\nThe paper is very clearly written and easy to follow.\n\nOriginality\n\nUnfortunately, the originality is very low. Combining reinforcement learning with auxiliary objectives is not novel and has been studied in the Atari domain (discrete actions) as noted by the authors, see Jaderberg et al., ICLR, 2017 and Shelhamer et al., arXiv, 2017. The conceptual idea of using a reconstruction loss for images as auxiliary objective is not novel either and has been presented in earlier work already, see Shelhamer et al. The idea of sharing parameters between RL and auxiliary components is also not novel, see Jaderberg et al. One citation that is conceptually very similar to the authors' work is missing: 'Felix Leibfried and Peter Vrancx, Model-based regularization for deep reinforcement learning with transcoder networks. In NeurIPS Deep Reinforcement Learning Workshop, 2018'. The former work combines Q-value learning with auxiliary losses for learning an environment model end to end (with a reconstruction loss for the next state) in the domain of Atari.\n\nSignificance\n\nThe significance is minor to low. The fact that the authors investigate auxiliary losses in continuous-action domains has minor significance. But all in all, the paper might be better suited for a workshop rather than the main track of ICLR.\n\nMinor Details\n\nOn page 3, first equation (not numbered), there is an average over s_{t+1} missing because of the reward definition used by the authors?\n\nUpdate\n\nI read the other reviews and the authors' response. I still feel that the novelty of the work is very limited and the authors' response to lacking novelty does not convince me. However, in light of the strong experimental analysis, I feel in hindsight that a score of 1 from my side was too harsh. I therefore increase my score to 3, but I do still believe that the paper is better suited as a workshop contribution.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2020/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2020/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["denisyarats@cs.nyu.edu", "amyzhang@fb.com", "ik1078@nyu.edu", "brandon.amos.cs@gmail.com", "jpineau@fb.com", "robfergus@fb.com"], "title": "Improving Sample Efficiency in Model-Free Reinforcement Learning from Images", "authors": ["Denis Yarats", "Amy Zhang", "Ilya Kostrikov", "Brandon Amos", "Joelle Pineau", "Rob Fergus"], "pdf": "/pdf/26bfa35b5ec3aeacf128b36cd8d5347f4cabcd26.pdf", "TL;DR": "We design a simple and efficient model-free off-policy method for image-based reinforcement learning that matches the state-of-the-art model-based methods in sample efficiency", "abstract": "Training an agent to solve control tasks directly from high-dimensional images with model-free reinforcement learning (RL) has proven difficult. The agent needs to learn  a latent representation together with a control policy to perform the task. Fitting a high-capacity encoder using a scarce reward signal is not only extremely sample inefficient, but also prone to suboptimal convergence. Two ways to improve sample efficiency are to learn a good feature representation and use off-policy algorithms. We dissect various approaches of learning good latent features, and conclude that the image reconstruction loss is the essential ingredient that enables efficient and stable representation learning in image-based RL. Following these findings, we devise an off-policy actor-critic algorithm with an auxiliary decoder that trains end-to-end and matches state-of-the-art performance across both model-free and model-based algorithms on many challenging control tasks. We release our code to encourage future research on image-based RL.", "code": "https://drive.google.com/file/d/1slqgCj3f8br5K6KiHUKHJBnjAhYnw3M-/view", "keywords": ["reinforcement learning", "model-free", "off-policy", "image-based reinforcement learning", "continuous control"], "paperhash": "yarats|improving_sample_efficiency_in_modelfree_reinforcement_learning_from_images", "original_pdf": "/attachment/26bfa35b5ec3aeacf128b36cd8d5347f4cabcd26.pdf", "_bibtex": "@misc{\nyarats2020improving,\ntitle={Improving Sample Efficiency in Model-Free Reinforcement Learning from Images},\nauthor={Denis Yarats and Amy Zhang and Ilya Kostrikov and Brandon Amos and Joelle Pineau and Rob Fergus},\nyear={2020},\nurl={https://openreview.net/forum?id=HklE01BYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HklE01BYDB", "replyto": "HklE01BYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2020/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2020/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576594740000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2020/Reviewers"], "noninvitees": [], "tcdate": 1570237728921, "tmdate": 1574722994101, "super": "ICLR.cc/2020/Conference/-/Official_Review", "final": [], "signatures": ["~Super_User1"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2020/-/Official_Review"}}}, {"id": "SJlvWnSmjS", "original": null, "number": 1, "cdate": 1573243902835, "ddate": null, "tcdate": 1573243902835, "tmdate": 1573251053418, "tddate": null, "forum": "HklE01BYDB", "replyto": "HklE01BYDB", "invitation": "ICLR.cc/2020/Conference/Paper2020/-/Official_Comment", "content": {"title": "Review response -- thanks for the feedback! [Part 1 of 2]", "comment": "We thank the reviewers for their comments. In particular, we were gratified by R2\u2019s positive observations: \u201cThe approach is fairly simple and appears to be effective for a suite of challenging tasks\u201d; \u201cthis work could have a significant impact on the community\u201d and \u201cThe experiments are also thorough and well thought out\u201d. We also appreciate that the reviewers find our paper to be well written and easy to follow.\n \n\nR1, R3: Lack of novelty. \nWe respectfully disagree with R1 and R3\u2019s concerns. As we discuss in the paper, auxiliary objectives, including reconstruction loss, have certainly been used before in RL. However, the empirical performance of previous approaches is dramatically worse than our approach in Mujoco settings. This discrepancy is noted by R2, who correctly recognizes this as \u201ca case where details matter\u201d.\n \nWe are thus concerned that R1 and R3 may not fully appreciate this aspect of our paper. This concern is bolstered by R1\u2019s view that our approach is that same as that of [Shelhamer et al.\u201917], when in fact this work vividly illustrates how differing \u201cdetails\u201d lead to very different experimental outcomes. \n\nLike our paper, [Shelhamer et al.\u201917] explored an auxiliary reconstruction loss (amongst others). But their training setup differs from ours in a variety of ways which turn out to be crucial to performance. In section 4.2, the authors note that \u201cReconstruction by VAE is mostly harmful\u201d and that \u201cThe VAE even diverges for several environments\u201d when trained stage-wise (section 4.3). Possibly because of this, the VAE is absent from their end-to-end training experiments (section 4.5). Thus due to an incorrect training protocol the authors (erroneously) conclude that an input reconstruction auxiliary loss is not effective. By contrast, our paper devises an *effective* training protocol for an input reconstruction auxiliary loss and shows that it is key to obtaining SOTA-comparable performance.\n \n\nR3: Relation to UNREAL [Jaderberg et al.\u201917] -- on vs off-policy. \nWhile UNREAL uses a replay buffer to train the critic and auxiliary models in off-policy fashion, the actor itself is trained on-policy via policy gradients (first paragraph in section 3.4). \n\nThe quasi off-policy nature of training in UNREAL is further demonstrated by the manner in which the replay buffer is used. The replay buffer in UNREAL \u201cstores the most recent 2k observations, actions, and rewards taken by the base agent\u201d (appendix B), a small fraction of the 25M transitions experienced in training. By contrast, in our true off-policy setting, the replay buffer stores 1M transitions (which usually is the entirety of all training transitions) and samples are drawn *uniformly* from *all* accumulated experiences. Our approach is able to achieve superior sample efficiency, despite the out-of-distribution nature of the replay buffer samples, thanks to the auxiliary reconstruction loss.   \n\n\nR3: Auxiliary tasks in UNREAL [Jaderberg et al.\u201917].\nAs with [Shelhamer et al.\u201917], UNREAL explores reconstruction loss as an auxiliary task for A3C, but find that it performs worse than A3C alone (see Fig. 5(left) in [Jaderberg et al.\u201917]). In contrast, with our approach we obtain dramatic performance gains by adding a generic input reconstruction loss.\n\nOf the other auxiliary tasks considered in UNREAL, Pixel Control is the most effective. However, in maximizing changes in local patches, it imposes strong inductive biases that assume that dramatically changing pixel values and textures are correlated with good exploration and reward. The other generic auxiliary tasks (reward prediction and value prediction) only provide marginal improvements (see Fig. 3(top left) in [Jaderberg et al.\u201917]) and may require specific reward structure (e.g. dense reward). \n\n\nR3: Stability of training is due to SAC.\nWe disagree. Augmenting SAC with a VAE (very similar conceptually to the RAE used in our approach) makes it highly unstable and sensitive to the choice of \\beta (see https://drive.google.com/open?id=1qYeiPXYl0iEmJYImZxDNhgjtNrDWtlfL ). Thus seems implausible that SAC is the main source of stability in our method. Furthermore,  [Shelhamer et al.\u201917] combine VAE\u2019s with several RL algorithms and also find that it makes them unstable. \n\n\nR1, R3: Importance of simplicity.\nWe respectfully feel that this aspect has been overlooked by R1 and R3. Our method matches the performance of current SOTA methods, while being far simpler (both from an implementation and conceptual standpoint). For many reasons, this should make our approach preferable, not least of which is reproducibility. Indeed, our approach is straightforward to reimplement, unlike many RL algorithms.  We also support our submission with a compact and easy to understand PyTorch implementation.\n \n \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2020/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2020/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["denisyarats@cs.nyu.edu", "amyzhang@fb.com", "ik1078@nyu.edu", "brandon.amos.cs@gmail.com", "jpineau@fb.com", "robfergus@fb.com"], "title": "Improving Sample Efficiency in Model-Free Reinforcement Learning from Images", "authors": ["Denis Yarats", "Amy Zhang", "Ilya Kostrikov", "Brandon Amos", "Joelle Pineau", "Rob Fergus"], "pdf": "/pdf/26bfa35b5ec3aeacf128b36cd8d5347f4cabcd26.pdf", "TL;DR": "We design a simple and efficient model-free off-policy method for image-based reinforcement learning that matches the state-of-the-art model-based methods in sample efficiency", "abstract": "Training an agent to solve control tasks directly from high-dimensional images with model-free reinforcement learning (RL) has proven difficult. The agent needs to learn  a latent representation together with a control policy to perform the task. Fitting a high-capacity encoder using a scarce reward signal is not only extremely sample inefficient, but also prone to suboptimal convergence. Two ways to improve sample efficiency are to learn a good feature representation and use off-policy algorithms. We dissect various approaches of learning good latent features, and conclude that the image reconstruction loss is the essential ingredient that enables efficient and stable representation learning in image-based RL. Following these findings, we devise an off-policy actor-critic algorithm with an auxiliary decoder that trains end-to-end and matches state-of-the-art performance across both model-free and model-based algorithms on many challenging control tasks. We release our code to encourage future research on image-based RL.", "code": "https://drive.google.com/file/d/1slqgCj3f8br5K6KiHUKHJBnjAhYnw3M-/view", "keywords": ["reinforcement learning", "model-free", "off-policy", "image-based reinforcement learning", "continuous control"], "paperhash": "yarats|improving_sample_efficiency_in_modelfree_reinforcement_learning_from_images", "original_pdf": "/attachment/26bfa35b5ec3aeacf128b36cd8d5347f4cabcd26.pdf", "_bibtex": "@misc{\nyarats2020improving,\ntitle={Improving Sample Efficiency in Model-Free Reinforcement Learning from Images},\nauthor={Denis Yarats and Amy Zhang and Ilya Kostrikov and Brandon Amos and Joelle Pineau and Rob Fergus},\nyear={2020},\nurl={https://openreview.net/forum?id=HklE01BYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklE01BYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2020/Authors", "ICLR.cc/2020/Conference/Paper2020/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2020/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2020/Reviewers", "ICLR.cc/2020/Conference/Paper2020/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2020/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2020/Authors|ICLR.cc/2020/Conference/Paper2020/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147485, "tmdate": 1576860529573, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2020/Authors", "ICLR.cc/2020/Conference/Paper2020/Reviewers", "ICLR.cc/2020/Conference/Paper2020/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2020/-/Official_Comment"}}}, {"id": "S1lOShrmsB", "original": null, "number": 2, "cdate": 1573243967519, "ddate": null, "tcdate": 1573243967519, "tmdate": 1573250064996, "tddate": null, "forum": "HklE01BYDB", "replyto": "HklE01BYDB", "invitation": "ICLR.cc/2020/Conference/Paper2020/-/Official_Comment", "content": {"title": "Review response -- thanks for the feedback! [Part 2 of 2]", "comment": "Minor comments:\n \nR1: Achieve results competitive with the model-based baselines. \nPlease note that these \u201cbaselines\u201d are the current state-of-art-methods.\n \n\nR1: Typo in the equation 1.\nThank you for spotting this typo, we will update the paper.\n\n\nR1: Missing reference.\nWe will update our paper with the missing reference, thank you for pointing this out.\n\n\nR2: SLAC is not being model-based. \nWe concur with the reviewer that SLAC does not use transitions sampled from the model for training and perhaps should be labeled as a model-free method. Our primary motivation was to showcase that SLAC trains a complicated latent model for dynamics that enjoys a lot of auxiliary supervision. In contrast, our method achieve competitive performance while being significantly simpler. We will update the wording in the paper to make the classification clearer.\n\n\nR3: Representation power. \nWe would like to point out that the encoder attributes for 90% of weights of our agent. The Q-function and policy networks are just small 3 layers MLPs on top of the encoder. Thus, we believe that the encoder should encapsulate some meaningful representations of internal states and our experiment is an adequate way to measure the amount of captured information.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2020/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2020/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["denisyarats@cs.nyu.edu", "amyzhang@fb.com", "ik1078@nyu.edu", "brandon.amos.cs@gmail.com", "jpineau@fb.com", "robfergus@fb.com"], "title": "Improving Sample Efficiency in Model-Free Reinforcement Learning from Images", "authors": ["Denis Yarats", "Amy Zhang", "Ilya Kostrikov", "Brandon Amos", "Joelle Pineau", "Rob Fergus"], "pdf": "/pdf/26bfa35b5ec3aeacf128b36cd8d5347f4cabcd26.pdf", "TL;DR": "We design a simple and efficient model-free off-policy method for image-based reinforcement learning that matches the state-of-the-art model-based methods in sample efficiency", "abstract": "Training an agent to solve control tasks directly from high-dimensional images with model-free reinforcement learning (RL) has proven difficult. The agent needs to learn  a latent representation together with a control policy to perform the task. Fitting a high-capacity encoder using a scarce reward signal is not only extremely sample inefficient, but also prone to suboptimal convergence. Two ways to improve sample efficiency are to learn a good feature representation and use off-policy algorithms. We dissect various approaches of learning good latent features, and conclude that the image reconstruction loss is the essential ingredient that enables efficient and stable representation learning in image-based RL. Following these findings, we devise an off-policy actor-critic algorithm with an auxiliary decoder that trains end-to-end and matches state-of-the-art performance across both model-free and model-based algorithms on many challenging control tasks. We release our code to encourage future research on image-based RL.", "code": "https://drive.google.com/file/d/1slqgCj3f8br5K6KiHUKHJBnjAhYnw3M-/view", "keywords": ["reinforcement learning", "model-free", "off-policy", "image-based reinforcement learning", "continuous control"], "paperhash": "yarats|improving_sample_efficiency_in_modelfree_reinforcement_learning_from_images", "original_pdf": "/attachment/26bfa35b5ec3aeacf128b36cd8d5347f4cabcd26.pdf", "_bibtex": "@misc{\nyarats2020improving,\ntitle={Improving Sample Efficiency in Model-Free Reinforcement Learning from Images},\nauthor={Denis Yarats and Amy Zhang and Ilya Kostrikov and Brandon Amos and Joelle Pineau and Rob Fergus},\nyear={2020},\nurl={https://openreview.net/forum?id=HklE01BYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklE01BYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2020/Authors", "ICLR.cc/2020/Conference/Paper2020/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2020/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2020/Reviewers", "ICLR.cc/2020/Conference/Paper2020/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2020/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2020/Authors|ICLR.cc/2020/Conference/Paper2020/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147485, "tmdate": 1576860529573, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2020/Authors", "ICLR.cc/2020/Conference/Paper2020/Reviewers", "ICLR.cc/2020/Conference/Paper2020/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2020/-/Official_Comment"}}}, {"id": "HkeFDVFtur", "original": null, "number": 2, "cdate": 1570505825047, "ddate": null, "tcdate": 1570505825047, "tmdate": 1572972393589, "tddate": null, "forum": "HklE01BYDB", "replyto": "HklE01BYDB", "invitation": "ICLR.cc/2020/Conference/Paper2020/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This work presents a simple method for model-free RL from image observations. The key component of the method is the addition of an autoencoder that is trained jointly with the policy and value function, in contrast to previous methods which separate feature learning from policy learning. Another important modification is the use of a deterministic regularized autoencoder instead of a stochastic variational autoencoder. The method is evaluated a variety of control tasks, and shows strong performance when compared to a number of state-of-the-art model-based and model-free methods for RL with image observations.\n\nThe paper is well written and provides a very clear description of the method. The approach is fairly simple and appears to be effective for a suite of challenging tasks. RL from images remains a very challenging problem, and the approach outlined in this work could have a significant impact on the community. The experiments are also thorough and well thought out, and the release of the source code is much appreciated. While the overall novelty is a bit limited, this could be a case where details matter, and insights provided by this work can be valuable for the community. For these reasons, I would like to recommend acceptance.\n\nThere is mention of SLAC as a model-based algorithm. This is not entirely accurate. SLAC does learn a dynamics model as means of acquiring a latent state-representation, but this model is not used to train the policy or for planning at runtime. The policy in SLAC is trained in a model-free manner.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2020/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2020/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["denisyarats@cs.nyu.edu", "amyzhang@fb.com", "ik1078@nyu.edu", "brandon.amos.cs@gmail.com", "jpineau@fb.com", "robfergus@fb.com"], "title": "Improving Sample Efficiency in Model-Free Reinforcement Learning from Images", "authors": ["Denis Yarats", "Amy Zhang", "Ilya Kostrikov", "Brandon Amos", "Joelle Pineau", "Rob Fergus"], "pdf": "/pdf/26bfa35b5ec3aeacf128b36cd8d5347f4cabcd26.pdf", "TL;DR": "We design a simple and efficient model-free off-policy method for image-based reinforcement learning that matches the state-of-the-art model-based methods in sample efficiency", "abstract": "Training an agent to solve control tasks directly from high-dimensional images with model-free reinforcement learning (RL) has proven difficult. The agent needs to learn  a latent representation together with a control policy to perform the task. Fitting a high-capacity encoder using a scarce reward signal is not only extremely sample inefficient, but also prone to suboptimal convergence. Two ways to improve sample efficiency are to learn a good feature representation and use off-policy algorithms. We dissect various approaches of learning good latent features, and conclude that the image reconstruction loss is the essential ingredient that enables efficient and stable representation learning in image-based RL. Following these findings, we devise an off-policy actor-critic algorithm with an auxiliary decoder that trains end-to-end and matches state-of-the-art performance across both model-free and model-based algorithms on many challenging control tasks. We release our code to encourage future research on image-based RL.", "code": "https://drive.google.com/file/d/1slqgCj3f8br5K6KiHUKHJBnjAhYnw3M-/view", "keywords": ["reinforcement learning", "model-free", "off-policy", "image-based reinforcement learning", "continuous control"], "paperhash": "yarats|improving_sample_efficiency_in_modelfree_reinforcement_learning_from_images", "original_pdf": "/attachment/26bfa35b5ec3aeacf128b36cd8d5347f4cabcd26.pdf", "_bibtex": "@misc{\nyarats2020improving,\ntitle={Improving Sample Efficiency in Model-Free Reinforcement Learning from Images},\nauthor={Denis Yarats and Amy Zhang and Ilya Kostrikov and Brandon Amos and Joelle Pineau and Rob Fergus},\nyear={2020},\nurl={https://openreview.net/forum?id=HklE01BYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HklE01BYDB", "replyto": "HklE01BYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2020/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2020/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576594740000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2020/Reviewers"], "noninvitees": [], "tcdate": 1570237728921, "tmdate": 1574722994101, "super": "ICLR.cc/2020/Conference/-/Official_Review", "final": [], "signatures": ["~Super_User1"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2020/-/Official_Review"}}}], "count": 7}