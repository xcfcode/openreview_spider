{"notes": [{"id": "fhcMwjavKEZ", "original": "YTjX6RGcwhg", "number": 70, "cdate": 1601308016947, "ddate": null, "tcdate": 1601308016947, "tmdate": 1614985678391, "tddate": null, "forum": "fhcMwjavKEZ", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "A Simple and General Graph Neural Network with Stochastic Message Passing", "authorids": ["~Ziwei_Zhang1", "nch16@mails.tsinghua.edu.cn", "~Peng_Cui1", "nevinzhang@tencent.com", "cuiwei@songshuai.com", "~Wenwu_Zhu1"], "authors": ["Ziwei Zhang", "Chenhao Niu", "Peng Cui", "Bo Zhang", "Wei Cui", "Wenwu Zhu"], "keywords": ["Graph Neural Network", "Node Proximity", "Permutation-equivariant"], "abstract": "Graph neural networks (GNNs) are emerging machine learning models on graphs. One key property behind the expressiveness of existing GNNs is that the learned node representations are permutation-equivariant. Though being a desirable property for certain tasks, however, permutation-equivariance prevents GNNs from being proximity-aware, i.e., preserving the walk-based proximities between pairs of nodes, which is another critical property for graph analytical tasks. On the other hand, some variants of GNNs are proposed to preserve node proximities, but they fail to maintain permutation-equivariance. How to empower GNNs to be proximity aware while maintaining permutation-equivariance remains an open problem. In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to maintain both proximity-awareness and permutation-equivariance properties. Specifically, we augment the existing GNNs with stochastic node representations learned to preserve node proximities. Though seemingly simple, we prove that such a mechanism can enable GNNs to preserve node proximities in theory while maintaining permutation-equivariance with certain parametrization. Extensive experimental results demonstrate the effectiveness and efficiency of SMP for tasks including node classification and link prediction.", "one-sentence_summary": "A simple GNN that maintains both proximity-awareness and permutation-equivariance properties using Stochastic Message Passing", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|a_simple_and_general_graph_neural_network_with_stochastic_message_passing", "pdf": "/pdf/cd009ff9657ac7470f15efc26a9ffacff8cfd79c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8TzrbYcOP", "_bibtex": "@misc{\nzhang2021a,\ntitle={A Simple and General Graph Neural Network with Stochastic Message Passing},\nauthor={Ziwei Zhang and Chenhao Niu and Peng Cui and Bo Zhang and Wei Cui and Wenwu Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=fhcMwjavKEZ}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 22, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "ttCtX4RK-cA", "original": null, "number": 1, "cdate": 1610040476172, "ddate": null, "tcdate": 1610040476172, "tmdate": 1610474080722, "tddate": null, "forum": "fhcMwjavKEZ", "replyto": "fhcMwjavKEZ", "invitation": "ICLR.cc/2021/Conference/Paper70/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This work proposes a modification of a GNN architecture by feeding random node features to bootstrap the message propagation. This enables the discriminability of automorphic node pairs with a lightweight, simple change. Experiments are reported showing improvements over baselines. \nReviewers had mixed impressions of this work. On one hand, they found the proposed model principled and with strong empirical performance. On the other hand, they perceived a general lack of novelty and a somewhat misleading theoretical analysis. After careful review, the AC ultimately believes that this work does require an extra iteration that further solidifies the contributions and aligns the theoretical analysis with the empirical performance. In particular, the use of random initialization is folklore in the GNN literature, especially with regards to spectral methods (e.g. power iterations are typically initialized using a random vector, and these constitute the simplest forms of linear GNNs). The authors are encouraged to address these comparisons with further detail, as well as the excellent feedback given by the reviewers. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple and General Graph Neural Network with Stochastic Message Passing", "authorids": ["~Ziwei_Zhang1", "nch16@mails.tsinghua.edu.cn", "~Peng_Cui1", "nevinzhang@tencent.com", "cuiwei@songshuai.com", "~Wenwu_Zhu1"], "authors": ["Ziwei Zhang", "Chenhao Niu", "Peng Cui", "Bo Zhang", "Wei Cui", "Wenwu Zhu"], "keywords": ["Graph Neural Network", "Node Proximity", "Permutation-equivariant"], "abstract": "Graph neural networks (GNNs) are emerging machine learning models on graphs. One key property behind the expressiveness of existing GNNs is that the learned node representations are permutation-equivariant. Though being a desirable property for certain tasks, however, permutation-equivariance prevents GNNs from being proximity-aware, i.e., preserving the walk-based proximities between pairs of nodes, which is another critical property for graph analytical tasks. On the other hand, some variants of GNNs are proposed to preserve node proximities, but they fail to maintain permutation-equivariance. How to empower GNNs to be proximity aware while maintaining permutation-equivariance remains an open problem. In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to maintain both proximity-awareness and permutation-equivariance properties. Specifically, we augment the existing GNNs with stochastic node representations learned to preserve node proximities. Though seemingly simple, we prove that such a mechanism can enable GNNs to preserve node proximities in theory while maintaining permutation-equivariance with certain parametrization. Extensive experimental results demonstrate the effectiveness and efficiency of SMP for tasks including node classification and link prediction.", "one-sentence_summary": "A simple GNN that maintains both proximity-awareness and permutation-equivariance properties using Stochastic Message Passing", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|a_simple_and_general_graph_neural_network_with_stochastic_message_passing", "pdf": "/pdf/cd009ff9657ac7470f15efc26a9ffacff8cfd79c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8TzrbYcOP", "_bibtex": "@misc{\nzhang2021a,\ntitle={A Simple and General Graph Neural Network with Stochastic Message Passing},\nauthor={Ziwei Zhang and Chenhao Niu and Peng Cui and Bo Zhang and Wei Cui and Wenwu Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=fhcMwjavKEZ}\n}"}, "tags": [], "invitation": {"reply": {"forum": "fhcMwjavKEZ", "replyto": "fhcMwjavKEZ", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040476158, "tmdate": 1610474080707, "id": "ICLR.cc/2021/Conference/Paper70/-/Decision"}}}, {"id": "v0lnaBcGBKY", "original": null, "number": 19, "cdate": 1605933782609, "ddate": null, "tcdate": 1605933782609, "tmdate": 1605933782609, "tddate": null, "forum": "fhcMwjavKEZ", "replyto": "zw_RyPp0w4D", "invitation": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment", "content": {"title": "Responses", "comment": "We thank the reviewer for clarifying the question (there may be typos in the previous question).\n\nWe think not adopting graph structures in the decoder is the standard setting of using GNNs and node embeddings--otherwise, you can build a do-nothing GNN and name the combination of a real GNN and decoder as a new \"decoder\", which is essentially not in accordance with the encoder-decoder framework, i.e., encoding structral information in the encoder (i.e., GNNs) and decoding information from obtained node representations. Please see Figure 3 of survey [1] or Figure 4.2 of book [2] for the encoder-decoder setting.  Notice that even for network embedding methods such as node2vec or MF, the graph structure is not utilized after obtaining the node embedding vectors.  \n[1] Representation Learning on Graphs: Methods and Applications, Bulletin of the IEEE 2017.   \n[2] Deep Learning on Graphs, Cambridge University Press, 2020.  \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper70/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple and General Graph Neural Network with Stochastic Message Passing", "authorids": ["~Ziwei_Zhang1", "nch16@mails.tsinghua.edu.cn", "~Peng_Cui1", "nevinzhang@tencent.com", "cuiwei@songshuai.com", "~Wenwu_Zhu1"], "authors": ["Ziwei Zhang", "Chenhao Niu", "Peng Cui", "Bo Zhang", "Wei Cui", "Wenwu Zhu"], "keywords": ["Graph Neural Network", "Node Proximity", "Permutation-equivariant"], "abstract": "Graph neural networks (GNNs) are emerging machine learning models on graphs. One key property behind the expressiveness of existing GNNs is that the learned node representations are permutation-equivariant. Though being a desirable property for certain tasks, however, permutation-equivariance prevents GNNs from being proximity-aware, i.e., preserving the walk-based proximities between pairs of nodes, which is another critical property for graph analytical tasks. On the other hand, some variants of GNNs are proposed to preserve node proximities, but they fail to maintain permutation-equivariance. How to empower GNNs to be proximity aware while maintaining permutation-equivariance remains an open problem. In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to maintain both proximity-awareness and permutation-equivariance properties. Specifically, we augment the existing GNNs with stochastic node representations learned to preserve node proximities. Though seemingly simple, we prove that such a mechanism can enable GNNs to preserve node proximities in theory while maintaining permutation-equivariance with certain parametrization. Extensive experimental results demonstrate the effectiveness and efficiency of SMP for tasks including node classification and link prediction.", "one-sentence_summary": "A simple GNN that maintains both proximity-awareness and permutation-equivariance properties using Stochastic Message Passing", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|a_simple_and_general_graph_neural_network_with_stochastic_message_passing", "pdf": "/pdf/cd009ff9657ac7470f15efc26a9ffacff8cfd79c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8TzrbYcOP", "_bibtex": "@misc{\nzhang2021a,\ntitle={A Simple and General Graph Neural Network with Stochastic Message Passing},\nauthor={Ziwei Zhang and Chenhao Niu and Peng Cui and Bo Zhang and Wei Cui and Wenwu Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=fhcMwjavKEZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fhcMwjavKEZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper70/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper70/Authors|ICLR.cc/2021/Conference/Paper70/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874839, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment"}}}, {"id": "zw_RyPp0w4D", "original": null, "number": 18, "cdate": 1605931326742, "ddate": null, "tcdate": 1605931326742, "tmdate": 1605931326742, "tddate": null, "forum": "fhcMwjavKEZ", "replyto": "Hiyae8RLKe", "invitation": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment", "content": {"title": "Clarification", "comment": "Re: We have explained multiple times that we do not use the graph structure in the decoder.\n\nYes, I understand your method does not. My point is that in the transductive setting, the decoder **can** use the graph structure, and there is no need to perform message passing on the encoder side. The randomly-sampled Gaussian vectors can be directly used, and the expressive decoder can memorize the proximity between all the pairs of nodes in the graph. Is there any issue with this approach? (This approach is kind of similar to node2vec or conventional matrix factorization techniques.)"}, "signatures": ["ICLR.cc/2021/Conference/Paper70/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple and General Graph Neural Network with Stochastic Message Passing", "authorids": ["~Ziwei_Zhang1", "nch16@mails.tsinghua.edu.cn", "~Peng_Cui1", "nevinzhang@tencent.com", "cuiwei@songshuai.com", "~Wenwu_Zhu1"], "authors": ["Ziwei Zhang", "Chenhao Niu", "Peng Cui", "Bo Zhang", "Wei Cui", "Wenwu Zhu"], "keywords": ["Graph Neural Network", "Node Proximity", "Permutation-equivariant"], "abstract": "Graph neural networks (GNNs) are emerging machine learning models on graphs. One key property behind the expressiveness of existing GNNs is that the learned node representations are permutation-equivariant. Though being a desirable property for certain tasks, however, permutation-equivariance prevents GNNs from being proximity-aware, i.e., preserving the walk-based proximities between pairs of nodes, which is another critical property for graph analytical tasks. On the other hand, some variants of GNNs are proposed to preserve node proximities, but they fail to maintain permutation-equivariance. How to empower GNNs to be proximity aware while maintaining permutation-equivariance remains an open problem. In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to maintain both proximity-awareness and permutation-equivariance properties. Specifically, we augment the existing GNNs with stochastic node representations learned to preserve node proximities. Though seemingly simple, we prove that such a mechanism can enable GNNs to preserve node proximities in theory while maintaining permutation-equivariance with certain parametrization. Extensive experimental results demonstrate the effectiveness and efficiency of SMP for tasks including node classification and link prediction.", "one-sentence_summary": "A simple GNN that maintains both proximity-awareness and permutation-equivariance properties using Stochastic Message Passing", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|a_simple_and_general_graph_neural_network_with_stochastic_message_passing", "pdf": "/pdf/cd009ff9657ac7470f15efc26a9ffacff8cfd79c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8TzrbYcOP", "_bibtex": "@misc{\nzhang2021a,\ntitle={A Simple and General Graph Neural Network with Stochastic Message Passing},\nauthor={Ziwei Zhang and Chenhao Niu and Peng Cui and Bo Zhang and Wei Cui and Wenwu Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=fhcMwjavKEZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fhcMwjavKEZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper70/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper70/Authors|ICLR.cc/2021/Conference/Paper70/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874839, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment"}}}, {"id": "Hiyae8RLKe", "original": null, "number": 17, "cdate": 1605928475095, "ddate": null, "tcdate": 1605928475095, "tmdate": 1605928475095, "tddate": null, "forum": "fhcMwjavKEZ", "replyto": "6W8wmNQTzX5", "invitation": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment", "content": {"title": "Thanks for Your Comments and Improving the Score", "comment": "We thank the reviewer for summarizing the comments and improving the score. Here are our responses.  \n\nQ: Results on node classification  \nA: We greatly outperform P-GNN on node classification (since the reviewer seems to think this baseline is most relevant) and retain comparable results with other GNNs, thus making our proposed method a flexible solution to both proximity-aware and permutation-equivariant tasks.  \n  \nQ: The method is the baseline of P-GNN  \nA: As explained in previous comments, our method is **different** from simply using one-hot IDs (using one-hot IDs + using Gaussian to initialize + fixing the first-layer parameters is only similar to **the stochastic part** of our proposed method). Otherwise, we could not achieve better results than P-GNN.  \n\nQ: Theoretical claims hinge on unrealistic assumptions  \nA: The assumptions mostly inherit from previous work [1] (bijective functions, countable feature space). Besides, as explained in previous comments, we have a more practical version, i.e., the linear variant of SMP and Theorem 2. Experimental results also support that the linear variant is expressive enough in most cases.  \n[1] How Powerful are Graph Neural Networks, ICLR 2019  \n\nQ: Although the authors do use the graph structure in their proof of Theorem 3, in a transductive setting, it still seems that all we need are a set of unique node embeddings + the graph-specific expressive decoder (no need for graph-aware message passing). Do the authors have any reason why the decoder needs to use the graph structure (as a result, it can only predict L-hop node proximity)?  \nA: We have explained multiple times that we **do not** use the graph structure in the decoder, i.e., we only use the embeddings obtained by GNNs $H$, the stochastic matrix $E$, and the similarity function $\\mathcal{S}(\\cdot)$. Please let us know which step of the proof you feel we have used the graph structure. This also helps to answer your questions: since only L-hop proximities are encoded in $H$, we can only preserve them."}, "signatures": ["ICLR.cc/2021/Conference/Paper70/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple and General Graph Neural Network with Stochastic Message Passing", "authorids": ["~Ziwei_Zhang1", "nch16@mails.tsinghua.edu.cn", "~Peng_Cui1", "nevinzhang@tencent.com", "cuiwei@songshuai.com", "~Wenwu_Zhu1"], "authors": ["Ziwei Zhang", "Chenhao Niu", "Peng Cui", "Bo Zhang", "Wei Cui", "Wenwu Zhu"], "keywords": ["Graph Neural Network", "Node Proximity", "Permutation-equivariant"], "abstract": "Graph neural networks (GNNs) are emerging machine learning models on graphs. One key property behind the expressiveness of existing GNNs is that the learned node representations are permutation-equivariant. Though being a desirable property for certain tasks, however, permutation-equivariance prevents GNNs from being proximity-aware, i.e., preserving the walk-based proximities between pairs of nodes, which is another critical property for graph analytical tasks. On the other hand, some variants of GNNs are proposed to preserve node proximities, but they fail to maintain permutation-equivariance. How to empower GNNs to be proximity aware while maintaining permutation-equivariance remains an open problem. In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to maintain both proximity-awareness and permutation-equivariance properties. Specifically, we augment the existing GNNs with stochastic node representations learned to preserve node proximities. Though seemingly simple, we prove that such a mechanism can enable GNNs to preserve node proximities in theory while maintaining permutation-equivariance with certain parametrization. Extensive experimental results demonstrate the effectiveness and efficiency of SMP for tasks including node classification and link prediction.", "one-sentence_summary": "A simple GNN that maintains both proximity-awareness and permutation-equivariance properties using Stochastic Message Passing", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|a_simple_and_general_graph_neural_network_with_stochastic_message_passing", "pdf": "/pdf/cd009ff9657ac7470f15efc26a9ffacff8cfd79c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8TzrbYcOP", "_bibtex": "@misc{\nzhang2021a,\ntitle={A Simple and General Graph Neural Network with Stochastic Message Passing},\nauthor={Ziwei Zhang and Chenhao Niu and Peng Cui and Bo Zhang and Wei Cui and Wenwu Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=fhcMwjavKEZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fhcMwjavKEZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper70/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper70/Authors|ICLR.cc/2021/Conference/Paper70/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874839, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment"}}}, {"id": "6W8wmNQTzX5", "original": null, "number": 16, "cdate": 1605892810864, "ddate": null, "tcdate": 1605892810864, "tmdate": 1605896228848, "tddate": null, "forum": "fhcMwjavKEZ", "replyto": "ZtPbrIbyo9p", "invitation": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment", "content": {"title": "Opinions", "comment": "I thank the authors for addressing my questions. I summarized my opinion below so that our AC can make a final decision.\n\nPro:\n- Method is simple. I definitely favor a simple method over complex ones, and I agree with R2 (https://openreview.net/forum?id=fhcMwjavKEZ&noteId=C1nSRs79gKk).\n- The authors perform a serious effort in evaluating the idea. In practice, the method is working well on the link prediction datasets, but not on the node classification datasets. \n\nCon:\n- From the theory side, I did not learn any new things. All the claims are either already made or (in my opinion) minor extensions from the P-GNN work.\n- Although the method is simple, the method is the baseline of P-GNN (Section 6.2 of the paper). I am not sure if we are making progress by proposing the baseline method again, although I also have the intuition that the baseline method is likely to work better than the P-GNN architecture.\n- Theoretical claims hinge on unrealistic assumptions on the power of GNNs and the decoder, especially when the node embeddings are stochastic (my Q1). \n- Although the authors do use the graph structure in their proof of Theorem 3, in a transductive setting, it still seems that all we need are a set of unique node embeddings + the graph-specific expressive decoder (no need for graph-aware message passing). Do the authors have any reason why the decoder needs to use the graph structure (as a result, it can only predict L-hop node proximity)?\n\n===\nI updated my score from 1 to 3 to reflect the authors' effort in implementing and evaluating the simple method. Still, I am inclined not to accept this paper since I find the technical contribution to be minimal compared to the P-GNN paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper70/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple and General Graph Neural Network with Stochastic Message Passing", "authorids": ["~Ziwei_Zhang1", "nch16@mails.tsinghua.edu.cn", "~Peng_Cui1", "nevinzhang@tencent.com", "cuiwei@songshuai.com", "~Wenwu_Zhu1"], "authors": ["Ziwei Zhang", "Chenhao Niu", "Peng Cui", "Bo Zhang", "Wei Cui", "Wenwu Zhu"], "keywords": ["Graph Neural Network", "Node Proximity", "Permutation-equivariant"], "abstract": "Graph neural networks (GNNs) are emerging machine learning models on graphs. One key property behind the expressiveness of existing GNNs is that the learned node representations are permutation-equivariant. Though being a desirable property for certain tasks, however, permutation-equivariance prevents GNNs from being proximity-aware, i.e., preserving the walk-based proximities between pairs of nodes, which is another critical property for graph analytical tasks. On the other hand, some variants of GNNs are proposed to preserve node proximities, but they fail to maintain permutation-equivariance. How to empower GNNs to be proximity aware while maintaining permutation-equivariance remains an open problem. In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to maintain both proximity-awareness and permutation-equivariance properties. Specifically, we augment the existing GNNs with stochastic node representations learned to preserve node proximities. Though seemingly simple, we prove that such a mechanism can enable GNNs to preserve node proximities in theory while maintaining permutation-equivariance with certain parametrization. Extensive experimental results demonstrate the effectiveness and efficiency of SMP for tasks including node classification and link prediction.", "one-sentence_summary": "A simple GNN that maintains both proximity-awareness and permutation-equivariance properties using Stochastic Message Passing", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|a_simple_and_general_graph_neural_network_with_stochastic_message_passing", "pdf": "/pdf/cd009ff9657ac7470f15efc26a9ffacff8cfd79c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8TzrbYcOP", "_bibtex": "@misc{\nzhang2021a,\ntitle={A Simple and General Graph Neural Network with Stochastic Message Passing},\nauthor={Ziwei Zhang and Chenhao Niu and Peng Cui and Bo Zhang and Wei Cui and Wenwu Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=fhcMwjavKEZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fhcMwjavKEZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper70/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper70/Authors|ICLR.cc/2021/Conference/Paper70/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874839, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment"}}}, {"id": "y9a3ejyDknM", "original": null, "number": 1, "cdate": 1602813163184, "ddate": null, "tcdate": 1602813163184, "tmdate": 1605895911092, "tddate": null, "forum": "fhcMwjavKEZ", "replyto": "fhcMwjavKEZ", "invitation": "ICLR.cc/2021/Conference/Paper70/-/Official_Review", "content": {"title": "Methods are known, results are trivial.", "review": "While the paper is easy to follow, I found all the results in this paper trivial and already-known.\n\nPros:\n1. The paper is well-written and easy to follow.\n\nCons:\n1. The entire Section 3 is not novel. It is a mixture of the preliminaries of GNNs and the trivial results of Corollary 1 and Theorem 1. Moreover, Theorem 1 is not rigorous. What if walk-based proximity is just a constant function? What if a given graph does not contain any automorphism?\n2. \u201cPreserve walk-based similarity\u201d is not rigorously defined. It seems that it just means all nodes have different embeddings.\n3. The proposed model is not permutation-equivariant after adding Gaussian noise. It is trivial that the model becomes permutation-equivariant when the Gaussian noise is ignored (because the model just reduces to an ordinary GNN).\n4. The claim that SMP preserves walk-based proximity is trivial and just relies on the fact that randomly-sampled vectors from a Gaussian distribution are different from each other.\n5. The idea of using node identifiers (essentially equivalent to the Gaussian noise) to make GNNs position-aware is not new. In fact, this idea is clearly mentioned in the P-GNN paper already (Section 6.2 of [1] \u201cfor inductive tasks, augmenting node attributes with one-hot identifiers restricts a model\u2019s generalization ability\u201d).\n6. In Section 5.3, it is unclear why the OGB node classification datasets are not used.\n\n[1] https://arxiv.org/abs/1906.04817\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper70/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple and General Graph Neural Network with Stochastic Message Passing", "authorids": ["~Ziwei_Zhang1", "nch16@mails.tsinghua.edu.cn", "~Peng_Cui1", "nevinzhang@tencent.com", "cuiwei@songshuai.com", "~Wenwu_Zhu1"], "authors": ["Ziwei Zhang", "Chenhao Niu", "Peng Cui", "Bo Zhang", "Wei Cui", "Wenwu Zhu"], "keywords": ["Graph Neural Network", "Node Proximity", "Permutation-equivariant"], "abstract": "Graph neural networks (GNNs) are emerging machine learning models on graphs. One key property behind the expressiveness of existing GNNs is that the learned node representations are permutation-equivariant. Though being a desirable property for certain tasks, however, permutation-equivariance prevents GNNs from being proximity-aware, i.e., preserving the walk-based proximities between pairs of nodes, which is another critical property for graph analytical tasks. On the other hand, some variants of GNNs are proposed to preserve node proximities, but they fail to maintain permutation-equivariance. How to empower GNNs to be proximity aware while maintaining permutation-equivariance remains an open problem. In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to maintain both proximity-awareness and permutation-equivariance properties. Specifically, we augment the existing GNNs with stochastic node representations learned to preserve node proximities. Though seemingly simple, we prove that such a mechanism can enable GNNs to preserve node proximities in theory while maintaining permutation-equivariance with certain parametrization. Extensive experimental results demonstrate the effectiveness and efficiency of SMP for tasks including node classification and link prediction.", "one-sentence_summary": "A simple GNN that maintains both proximity-awareness and permutation-equivariance properties using Stochastic Message Passing", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|a_simple_and_general_graph_neural_network_with_stochastic_message_passing", "pdf": "/pdf/cd009ff9657ac7470f15efc26a9ffacff8cfd79c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8TzrbYcOP", "_bibtex": "@misc{\nzhang2021a,\ntitle={A Simple and General Graph Neural Network with Stochastic Message Passing},\nauthor={Ziwei Zhang and Chenhao Niu and Peng Cui and Bo Zhang and Wei Cui and Wenwu Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=fhcMwjavKEZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "fhcMwjavKEZ", "replyto": "fhcMwjavKEZ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper70/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538150888, "tmdate": 1606915793705, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper70/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper70/-/Official_Review"}}}, {"id": "C1nSRs79gKk", "original": null, "number": 15, "cdate": 1605877699513, "ddate": null, "tcdate": 1605877699513, "tmdate": 1605877699513, "tddate": null, "forum": "fhcMwjavKEZ", "replyto": "2t0Jez5QbeC", "invitation": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment", "content": {"title": "Thank you for the response.", "comment": "I thank the authors for the additional effort, polishing the work further. My evaluation remains the same.\n\nAfter reading the other reviews, discussions and revisions carefully, I'd like to explicitly mention that I do not share the opinion of Reviewer3 that this work contains only trivial results.\n\nThe method itself certainly is simple. It does not really matter if you present it as noise as input or as a non-trainable, randomly initialized layer following one-hot features, as both variants are similarly straight-forward and seem to be equivalent. I do not consider the fact that those views are interchangeable as argument for any position.\n\nIn general, I think the complexity of a presented method should not be a strong deciding factor. A lot of past high-impact work presents, analyzes and evaluates simple methods that can be easily adapted and are efficient. Both is the case in this work. As a researcher more focused on practical applications, I might even favor simple, effective approaches over complex ones that have strong theoretical properties but are inapplicable in practice. The approach presented in this paper has practical advantages over the previous work (P-GNNs).\n\nI agree that the approach is not entirely novel since it has been applied before as an optimization trick in some works. However, to my knowledge, there is no work which has this idea as a main contribution and makes an effort analyzing it's advantages, and where those advantages come from. This, in my opinion, is a non-trivial and potentially impactful contribution. Future GNN research can build upon this work, which previous work might not allow due to practical limitations."}, "signatures": ["ICLR.cc/2021/Conference/Paper70/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple and General Graph Neural Network with Stochastic Message Passing", "authorids": ["~Ziwei_Zhang1", "nch16@mails.tsinghua.edu.cn", "~Peng_Cui1", "nevinzhang@tencent.com", "cuiwei@songshuai.com", "~Wenwu_Zhu1"], "authors": ["Ziwei Zhang", "Chenhao Niu", "Peng Cui", "Bo Zhang", "Wei Cui", "Wenwu Zhu"], "keywords": ["Graph Neural Network", "Node Proximity", "Permutation-equivariant"], "abstract": "Graph neural networks (GNNs) are emerging machine learning models on graphs. One key property behind the expressiveness of existing GNNs is that the learned node representations are permutation-equivariant. Though being a desirable property for certain tasks, however, permutation-equivariance prevents GNNs from being proximity-aware, i.e., preserving the walk-based proximities between pairs of nodes, which is another critical property for graph analytical tasks. On the other hand, some variants of GNNs are proposed to preserve node proximities, but they fail to maintain permutation-equivariance. How to empower GNNs to be proximity aware while maintaining permutation-equivariance remains an open problem. In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to maintain both proximity-awareness and permutation-equivariance properties. Specifically, we augment the existing GNNs with stochastic node representations learned to preserve node proximities. Though seemingly simple, we prove that such a mechanism can enable GNNs to preserve node proximities in theory while maintaining permutation-equivariance with certain parametrization. Extensive experimental results demonstrate the effectiveness and efficiency of SMP for tasks including node classification and link prediction.", "one-sentence_summary": "A simple GNN that maintains both proximity-awareness and permutation-equivariance properties using Stochastic Message Passing", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|a_simple_and_general_graph_neural_network_with_stochastic_message_passing", "pdf": "/pdf/cd009ff9657ac7470f15efc26a9ffacff8cfd79c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8TzrbYcOP", "_bibtex": "@misc{\nzhang2021a,\ntitle={A Simple and General Graph Neural Network with Stochastic Message Passing},\nauthor={Ziwei Zhang and Chenhao Niu and Peng Cui and Bo Zhang and Wei Cui and Wenwu Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=fhcMwjavKEZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fhcMwjavKEZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper70/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper70/Authors|ICLR.cc/2021/Conference/Paper70/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874839, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment"}}}, {"id": "ZtPbrIbyo9p", "original": null, "number": 13, "cdate": 1605863204097, "ddate": null, "tcdate": 1605863204097, "tmdate": 1605863204097, "tddate": null, "forum": "fhcMwjavKEZ", "replyto": "UaDIXoi9MGe", "invitation": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment", "content": {"title": "All questions are addressed", "comment": "We thank the reviewer for the comments.  \n\nQ: Regarding Q1 and Unique Node Identifiers  \nA: We do not see what is the specific question of the reviewer since the previous Q1 was addressed by our slight revision of Definition 4 and we also explicitly stated that we do not use the graph structure in the decoder. We also gave the reference [1] showing that it is formally proved that unique node identifiers are not sufficient (but a necessary) condition for GNNs to be universal.  Thus, we consider this question already addressed.  \n[1] What graph neural networks cannot learn: depth vs width, ICLR 2020  \n\nQ: Differences with P-GNN  \nA: As explained in previous responses, we have compared with P-GNN both methodologically (lines 84-87) and empirically (we have adopted P-GNN as a baseline in all our experiments). Other reviewers also agree that our proposed method is \"a significant improvement over P-GNN\". So we strongly disagree with your comment that our results should be considered trivial compared to P-GNN.  \n\nIf the reviewer does not have more specific questions, we will consider all your questions have been addressed. The rest we can do is agreeing to differ.  \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper70/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple and General Graph Neural Network with Stochastic Message Passing", "authorids": ["~Ziwei_Zhang1", "nch16@mails.tsinghua.edu.cn", "~Peng_Cui1", "nevinzhang@tencent.com", "cuiwei@songshuai.com", "~Wenwu_Zhu1"], "authors": ["Ziwei Zhang", "Chenhao Niu", "Peng Cui", "Bo Zhang", "Wei Cui", "Wenwu Zhu"], "keywords": ["Graph Neural Network", "Node Proximity", "Permutation-equivariant"], "abstract": "Graph neural networks (GNNs) are emerging machine learning models on graphs. One key property behind the expressiveness of existing GNNs is that the learned node representations are permutation-equivariant. Though being a desirable property for certain tasks, however, permutation-equivariance prevents GNNs from being proximity-aware, i.e., preserving the walk-based proximities between pairs of nodes, which is another critical property for graph analytical tasks. On the other hand, some variants of GNNs are proposed to preserve node proximities, but they fail to maintain permutation-equivariance. How to empower GNNs to be proximity aware while maintaining permutation-equivariance remains an open problem. In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to maintain both proximity-awareness and permutation-equivariance properties. Specifically, we augment the existing GNNs with stochastic node representations learned to preserve node proximities. Though seemingly simple, we prove that such a mechanism can enable GNNs to preserve node proximities in theory while maintaining permutation-equivariance with certain parametrization. Extensive experimental results demonstrate the effectiveness and efficiency of SMP for tasks including node classification and link prediction.", "one-sentence_summary": "A simple GNN that maintains both proximity-awareness and permutation-equivariance properties using Stochastic Message Passing", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|a_simple_and_general_graph_neural_network_with_stochastic_message_passing", "pdf": "/pdf/cd009ff9657ac7470f15efc26a9ffacff8cfd79c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8TzrbYcOP", "_bibtex": "@misc{\nzhang2021a,\ntitle={A Simple and General Graph Neural Network with Stochastic Message Passing},\nauthor={Ziwei Zhang and Chenhao Niu and Peng Cui and Bo Zhang and Wei Cui and Wenwu Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=fhcMwjavKEZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fhcMwjavKEZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper70/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper70/Authors|ICLR.cc/2021/Conference/Paper70/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874839, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment"}}}, {"id": "UaDIXoi9MGe", "original": null, "number": 12, "cdate": 1605800336529, "ddate": null, "tcdate": 1605800336529, "tmdate": 1605800336529, "tddate": null, "forum": "fhcMwjavKEZ", "replyto": "oAHCGHfuUH1", "invitation": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment", "content": {"title": "Thank you for the response. Not convinced.", "comment": "I thank the authors for their response. Unfortunately, after a series of correspondence, I am now confident about my judgment that this paper should not be accepted. \n\n- Q1 is essentially not resolved.\n\n- The change of the claim of Definition 4 (claiming the existence of the universal graph decoder that takes the stochastic node embeddings as input) is rather major and strongly hinges on the unrealistic assumption of my Q1. Furthermore, for transductive scenarios, we are only given a single graph. Therefore, whether we have the universal decoder for any graphs does not really matter. All we need are a unique set of node embeddings + the graph-specific expressive decoder.\n\n- All the essential method (one-hot ID) and insight (limitation of GNNs to capture position information) are already in the P-GNN paper. The newly-derived results are all trivial to me."}, "signatures": ["ICLR.cc/2021/Conference/Paper70/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple and General Graph Neural Network with Stochastic Message Passing", "authorids": ["~Ziwei_Zhang1", "nch16@mails.tsinghua.edu.cn", "~Peng_Cui1", "nevinzhang@tencent.com", "cuiwei@songshuai.com", "~Wenwu_Zhu1"], "authors": ["Ziwei Zhang", "Chenhao Niu", "Peng Cui", "Bo Zhang", "Wei Cui", "Wenwu Zhu"], "keywords": ["Graph Neural Network", "Node Proximity", "Permutation-equivariant"], "abstract": "Graph neural networks (GNNs) are emerging machine learning models on graphs. One key property behind the expressiveness of existing GNNs is that the learned node representations are permutation-equivariant. Though being a desirable property for certain tasks, however, permutation-equivariance prevents GNNs from being proximity-aware, i.e., preserving the walk-based proximities between pairs of nodes, which is another critical property for graph analytical tasks. On the other hand, some variants of GNNs are proposed to preserve node proximities, but they fail to maintain permutation-equivariance. How to empower GNNs to be proximity aware while maintaining permutation-equivariance remains an open problem. In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to maintain both proximity-awareness and permutation-equivariance properties. Specifically, we augment the existing GNNs with stochastic node representations learned to preserve node proximities. Though seemingly simple, we prove that such a mechanism can enable GNNs to preserve node proximities in theory while maintaining permutation-equivariance with certain parametrization. Extensive experimental results demonstrate the effectiveness and efficiency of SMP for tasks including node classification and link prediction.", "one-sentence_summary": "A simple GNN that maintains both proximity-awareness and permutation-equivariance properties using Stochastic Message Passing", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|a_simple_and_general_graph_neural_network_with_stochastic_message_passing", "pdf": "/pdf/cd009ff9657ac7470f15efc26a9ffacff8cfd79c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8TzrbYcOP", "_bibtex": "@misc{\nzhang2021a,\ntitle={A Simple and General Graph Neural Network with Stochastic Message Passing},\nauthor={Ziwei Zhang and Chenhao Niu and Peng Cui and Bo Zhang and Wei Cui and Wenwu Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=fhcMwjavKEZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fhcMwjavKEZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper70/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper70/Authors|ICLR.cc/2021/Conference/Paper70/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874839, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment"}}}, {"id": "VggXIUOUB54", "original": null, "number": 7, "cdate": 1605677863656, "ddate": null, "tcdate": 1605677863656, "tmdate": 1605783132506, "tddate": null, "forum": "fhcMwjavKEZ", "replyto": "fhcMwjavKEZ", "invitation": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment", "content": {"title": "Summary of Changes", "comment": "We would like to thank all the reviewers again for their thoughtful comments on our paper. Based on their comments, we have made the following changes, which further improve the quality of our paper.  \n\n* We have moved Definition 4 (preserve walk-based proximity, lines 142-149 in the revised manuscript) and Table 5 (running time comparison, Page 9 in the revised manuscript) into the main paper from the appendix since they are important to our proposed method.  \n* We have improved related works (lines 88-101 in the revised manuscript) by revising the comparison with recent works and adding one related work (Fey et al., 2020).  \n* We have added one ablation study (Table 12, Appendix B.5 in the revised manuscript) to compare whether the stochastic matrix E is fixed or resampled during different training epochs.  \n* We have given an alternate proof of Theorem 1 (lines 526-550, Appendix A.1 in the revised manuscript) by maintaining one connected component in constructing the counter-examples.  \n* We have added one subsection (Appendix B.6 in the revised manuscript) to investigate how linear and non-linear variants of permutation-equivariant GNN perform for the link prediction task.  \n* We have slightly modified Definition 4 and Theorem 3 to prevent misunderstanding and corner cases (do not affect other parts of the paper).\n* We have clarified some expressions and fixed some typos.  \n\nWe would be happy to address any other questions or comments."}, "signatures": ["ICLR.cc/2021/Conference/Paper70/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple and General Graph Neural Network with Stochastic Message Passing", "authorids": ["~Ziwei_Zhang1", "nch16@mails.tsinghua.edu.cn", "~Peng_Cui1", "nevinzhang@tencent.com", "cuiwei@songshuai.com", "~Wenwu_Zhu1"], "authors": ["Ziwei Zhang", "Chenhao Niu", "Peng Cui", "Bo Zhang", "Wei Cui", "Wenwu Zhu"], "keywords": ["Graph Neural Network", "Node Proximity", "Permutation-equivariant"], "abstract": "Graph neural networks (GNNs) are emerging machine learning models on graphs. One key property behind the expressiveness of existing GNNs is that the learned node representations are permutation-equivariant. Though being a desirable property for certain tasks, however, permutation-equivariance prevents GNNs from being proximity-aware, i.e., preserving the walk-based proximities between pairs of nodes, which is another critical property for graph analytical tasks. On the other hand, some variants of GNNs are proposed to preserve node proximities, but they fail to maintain permutation-equivariance. How to empower GNNs to be proximity aware while maintaining permutation-equivariance remains an open problem. In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to maintain both proximity-awareness and permutation-equivariance properties. Specifically, we augment the existing GNNs with stochastic node representations learned to preserve node proximities. Though seemingly simple, we prove that such a mechanism can enable GNNs to preserve node proximities in theory while maintaining permutation-equivariance with certain parametrization. Extensive experimental results demonstrate the effectiveness and efficiency of SMP for tasks including node classification and link prediction.", "one-sentence_summary": "A simple GNN that maintains both proximity-awareness and permutation-equivariance properties using Stochastic Message Passing", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|a_simple_and_general_graph_neural_network_with_stochastic_message_passing", "pdf": "/pdf/cd009ff9657ac7470f15efc26a9ffacff8cfd79c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8TzrbYcOP", "_bibtex": "@misc{\nzhang2021a,\ntitle={A Simple and General Graph Neural Network with Stochastic Message Passing},\nauthor={Ziwei Zhang and Chenhao Niu and Peng Cui and Bo Zhang and Wei Cui and Wenwu Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=fhcMwjavKEZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fhcMwjavKEZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper70/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper70/Authors|ICLR.cc/2021/Conference/Paper70/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874839, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment"}}}, {"id": "oAHCGHfuUH1", "original": null, "number": 11, "cdate": 1605782703963, "ddate": null, "tcdate": 1605782703963, "tmdate": 1605782703963, "tddate": null, "forum": "fhcMwjavKEZ", "replyto": "54cUhOiKjs", "invitation": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment", "content": {"title": "Further Responses", "comment": "We thank the reviewer for further responses. Before addressing your detailed comments, we would like to emphasize again that our paper is the first study to maintain both proximity-awareness and permutation-equivariance, which are critical properties of GNNs. Besides the results already shown in the paper, we recently find that such a topic may be very helpful in drug repurposing. For example, the work of [1] by Barabasi et al. shows that GNNs and proximity algorithms lead to complementary discoveries of drug candidates for COVID-19 (see A1-A4, P1-P3 of figure 1 and 2(c) of [1]). Our work provides deep understandings for such a phenomenon since they exactly correspond to proximity-awareness and permutation-equivariance. This is only another example of the potential impact of our paper. Thus, we strongly recommend the reviewer to reconsider whether the results in our paper should all be considered trivial and deserve a score of 1.  \n[1] Network Medicine Framework for Identifying Drug Repurposing Opportunities for COVID-19, arXiv 2004.07229  \n\nQ: In my opinion, this is an extremely unrealistic assumption given that Gaussian distribution has support everywhere in real vector space. In practice, I do not think we can learn the decoder such that it preserves node proximity for uncountably-infinitely-many sets of Gaussian random vectors. Theorem 3 definitely needs to be modified to avoid such an assumption.  \nA: (1) Our proof for Theorem 3 is given in Appendix A.2. In the proof, assuming the message-passing functions in GNNs are bijective, embedding vectors with different stochastic Gaussian matrices essentially only affect the node ID decoding step (i.e., line 591). Thus, the same decoder should be able to handle different stochastic representations. Please let us know if you find the proof has any specific issue.    \n(2) We agree that though Theorem 3 provides important theoretical results, some of its assumptions may be too strong in practice. As explained multiple times in the paper (lines 59-60, 229-232, and the entire Section 5.4), we empirically find that the linear variant of SMP is expressive enough and may be of more practical usage (recall that it has a much simpler decoder, i.e., inner product, and also with theoretical results, i.e., Theorem 2).   \n(3) We agree that uncountability may bring theoretical issues (similar to concerns raised in [2]). Considering that we can only round numbers to machine precision in practice, we assume the Gaussian vectors are rounded to a countable subset of real numbers in Theorem 3. We have added this assumption (lines 226-227) in the revised paper.   \n[2] How Powerful Are Graph Neural Networks, ICLR 2019\n\nQ: This does not accord with Definition 4, where the authors claim that for each graph, there exists a decoder to approximate the node proximity. This means that the decoder can just be graph-specific; thus, all we need are a unique embedding for each node, and the rest will be taken care of by the graph-specific expressive decoder.  \nA: Thank you for providing this corner case. To make the theorem more rigorous and avoid further misunderstanding, we have slightly revised Definition 4 so that the decoder is explicitly stated as not graph-specific to prevent the trivial cases you mention (we have checked that the rest of our results are not affected by this slight modification). Please refer to the revised manuscript for details (Definition 4 and line 148). \n\nQ: The authors need to do an ablation study to compare with one-hot IDs.  \nA: As explained in the paper and our first response, we have reported such a comparison (using stochastic matrix vs. using one-hot IDs) in Table 8 in the appendix. The results show that our proposed method generally achieves better results. \n\nQ: I still think the method is already known and trivial.  \nA: We would be appreciated if the reviewer can list the exact reference if there exist other methods/analyses similar to our paper, or our studied problem (maintaining both proximity-awareness and permutation-equivariance in GNNs)."}, "signatures": ["ICLR.cc/2021/Conference/Paper70/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple and General Graph Neural Network with Stochastic Message Passing", "authorids": ["~Ziwei_Zhang1", "nch16@mails.tsinghua.edu.cn", "~Peng_Cui1", "nevinzhang@tencent.com", "cuiwei@songshuai.com", "~Wenwu_Zhu1"], "authors": ["Ziwei Zhang", "Chenhao Niu", "Peng Cui", "Bo Zhang", "Wei Cui", "Wenwu Zhu"], "keywords": ["Graph Neural Network", "Node Proximity", "Permutation-equivariant"], "abstract": "Graph neural networks (GNNs) are emerging machine learning models on graphs. One key property behind the expressiveness of existing GNNs is that the learned node representations are permutation-equivariant. Though being a desirable property for certain tasks, however, permutation-equivariance prevents GNNs from being proximity-aware, i.e., preserving the walk-based proximities between pairs of nodes, which is another critical property for graph analytical tasks. On the other hand, some variants of GNNs are proposed to preserve node proximities, but they fail to maintain permutation-equivariance. How to empower GNNs to be proximity aware while maintaining permutation-equivariance remains an open problem. In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to maintain both proximity-awareness and permutation-equivariance properties. Specifically, we augment the existing GNNs with stochastic node representations learned to preserve node proximities. Though seemingly simple, we prove that such a mechanism can enable GNNs to preserve node proximities in theory while maintaining permutation-equivariance with certain parametrization. Extensive experimental results demonstrate the effectiveness and efficiency of SMP for tasks including node classification and link prediction.", "one-sentence_summary": "A simple GNN that maintains both proximity-awareness and permutation-equivariance properties using Stochastic Message Passing", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|a_simple_and_general_graph_neural_network_with_stochastic_message_passing", "pdf": "/pdf/cd009ff9657ac7470f15efc26a9ffacff8cfd79c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8TzrbYcOP", "_bibtex": "@misc{\nzhang2021a,\ntitle={A Simple and General Graph Neural Network with Stochastic Message Passing},\nauthor={Ziwei Zhang and Chenhao Niu and Peng Cui and Bo Zhang and Wei Cui and Wenwu Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=fhcMwjavKEZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fhcMwjavKEZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper70/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper70/Authors|ICLR.cc/2021/Conference/Paper70/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874839, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment"}}}, {"id": "54cUhOiKjs", "original": null, "number": 10, "cdate": 1605727498848, "ddate": null, "tcdate": 1605727498848, "tmdate": 1605727765739, "tddate": null, "forum": "fhcMwjavKEZ", "replyto": "lkJqFdGlXUG", "invitation": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment", "content": {"title": "Thanks for clarification. Further questions.", "comment": "I thank the authors for clarification, especially on how SMP works. I still have further questions regarding the other answers.\n\nRe: The decoder is very expressive and also takes the stochastic matrix as inputs.\n\nIn my opinion, this is an extremely unrealistic assumption given that Gaussian distribution has support everywhere in real vector space. In practice, I do not think we can learn the decoder such that it preserves node proximity for uncountably-infinitely-many sets of Gaussian random vectors. Theorem 3 definitely needs to be modified to avoid such an assumption.\n\nRe: Note that in the decoding phase, we cannot use the actual proximity or the graph structure (otherwise, there is no point preserving the proximity, since we already know the proximity or we can recalculate the proximity)\n\nThis does not accord with Definition 4, where the authors claim that for each graph, there exists a decoder to approximate the node proximity. This means that the decoder can just be graph-specific; thus, all we need are a unique embedding for each node, and the rest will be taken care of by the graph-specific expressive decoder.\n\nRe: However, we do not find that such a method is used in any previous GNN, i.e., when referring to using one-hot IDs as inputs, they all assume the first layer is trainable.\n\nTo me, making the embedding non-trainable is quite trivial, and I cannot buy the argument. If the authors claim the main methodological novelty is not training the embedding layer, then the authors need to do an ablation study for that. However, I still think the method is already known and trivial.\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper70/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple and General Graph Neural Network with Stochastic Message Passing", "authorids": ["~Ziwei_Zhang1", "nch16@mails.tsinghua.edu.cn", "~Peng_Cui1", "nevinzhang@tencent.com", "cuiwei@songshuai.com", "~Wenwu_Zhu1"], "authors": ["Ziwei Zhang", "Chenhao Niu", "Peng Cui", "Bo Zhang", "Wei Cui", "Wenwu Zhu"], "keywords": ["Graph Neural Network", "Node Proximity", "Permutation-equivariant"], "abstract": "Graph neural networks (GNNs) are emerging machine learning models on graphs. One key property behind the expressiveness of existing GNNs is that the learned node representations are permutation-equivariant. Though being a desirable property for certain tasks, however, permutation-equivariance prevents GNNs from being proximity-aware, i.e., preserving the walk-based proximities between pairs of nodes, which is another critical property for graph analytical tasks. On the other hand, some variants of GNNs are proposed to preserve node proximities, but they fail to maintain permutation-equivariance. How to empower GNNs to be proximity aware while maintaining permutation-equivariance remains an open problem. In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to maintain both proximity-awareness and permutation-equivariance properties. Specifically, we augment the existing GNNs with stochastic node representations learned to preserve node proximities. Though seemingly simple, we prove that such a mechanism can enable GNNs to preserve node proximities in theory while maintaining permutation-equivariance with certain parametrization. Extensive experimental results demonstrate the effectiveness and efficiency of SMP for tasks including node classification and link prediction.", "one-sentence_summary": "A simple GNN that maintains both proximity-awareness and permutation-equivariance properties using Stochastic Message Passing", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|a_simple_and_general_graph_neural_network_with_stochastic_message_passing", "pdf": "/pdf/cd009ff9657ac7470f15efc26a9ffacff8cfd79c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8TzrbYcOP", "_bibtex": "@misc{\nzhang2021a,\ntitle={A Simple and General Graph Neural Network with Stochastic Message Passing},\nauthor={Ziwei Zhang and Chenhao Niu and Peng Cui and Bo Zhang and Wei Cui and Wenwu Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=fhcMwjavKEZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fhcMwjavKEZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper70/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper70/Authors|ICLR.cc/2021/Conference/Paper70/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874839, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment"}}}, {"id": "lkJqFdGlXUG", "original": null, "number": 9, "cdate": 1605696762028, "ddate": null, "tcdate": 1605696762028, "tmdate": 1605696762028, "tddate": null, "forum": "fhcMwjavKEZ", "replyto": "f4Q4Ga-_aY", "invitation": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment", "content": {"title": "Responses", "comment": "We thank the reviewer for additional feedbacks. Here are our responses.  \n\nQ: How does SMP work?  \nA: As we have explained in Section 4.1 (lines 174-181 in the revised version), we fix the Gaussian vector (i.e., (2) as you suggested) for transductive datasets and resample the vector (i.e., (1) as you suggested) for inductive datasets. An ablation study is also reported (Table 12 in appendix B.5) to verify this design.\n\nQ: Why Theorem 3 holds even if the node embeddings are stochastic?  \nA: There are 3 conditions for Theorem 3 to hold (shown in lines 575-579 in the revised version): the stochastic matrix contains unique signals for different nodes; the functions in GNNs are bijective; the decoder is very expressive and also takes the stochastic matrix as inputs. Under these conditions, since we only use the stochastic matrix as node identifiers, we can handle stochastic node embeddings (i.e., input different stochastic node embeddings into the the same decoder) without modifying the definition or the theorem.  \n\nQ: Is having unique embedding all you need to preserve proximities?  \nA: We kindly do not agree that having unique embeddings can approximate any function. Note that in the decoding phase, we cannot use the actual proximity or the graph structure (otherwise, there is no point preserving the proximity, since we already know the proximity or we can recalculate the proximity). More theoretically, [1] shows that having unique node identifiers being one necessary (but not sufficient) condition for GNNs to be universal approximators (specifically, [1] shows that the GNN also needs to be sufficiently expressive). Thus, the theorems cannot be trivially attained. We will further clarify our expressions.  \n[1] What graph neural networks cannot learn: depth vs width, ICLR 2020\n\nQ: Regarding one-hot IDs  \nA: We agree that using one-hot IDs while fixing the parameters in the first layer as random Gaussians is equivalent to the stochastic part of our proposed method (essentially, this is to replace E in our proposed method with I*E). However, we do not find that such a method is used in any previous GNN, i.e., when referring to using one-hot IDs as inputs, they all assume the first layer is trainable. Besides, our proposed SMP also has a permutation-equivariant GNN and an output function. Thus, we feel that the reviewer's comments provide an alternative understanding of SMP, but do not affect the novelty of our proposed method."}, "signatures": ["ICLR.cc/2021/Conference/Paper70/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple and General Graph Neural Network with Stochastic Message Passing", "authorids": ["~Ziwei_Zhang1", "nch16@mails.tsinghua.edu.cn", "~Peng_Cui1", "nevinzhang@tencent.com", "cuiwei@songshuai.com", "~Wenwu_Zhu1"], "authors": ["Ziwei Zhang", "Chenhao Niu", "Peng Cui", "Bo Zhang", "Wei Cui", "Wenwu Zhu"], "keywords": ["Graph Neural Network", "Node Proximity", "Permutation-equivariant"], "abstract": "Graph neural networks (GNNs) are emerging machine learning models on graphs. One key property behind the expressiveness of existing GNNs is that the learned node representations are permutation-equivariant. Though being a desirable property for certain tasks, however, permutation-equivariance prevents GNNs from being proximity-aware, i.e., preserving the walk-based proximities between pairs of nodes, which is another critical property for graph analytical tasks. On the other hand, some variants of GNNs are proposed to preserve node proximities, but they fail to maintain permutation-equivariance. How to empower GNNs to be proximity aware while maintaining permutation-equivariance remains an open problem. In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to maintain both proximity-awareness and permutation-equivariance properties. Specifically, we augment the existing GNNs with stochastic node representations learned to preserve node proximities. Though seemingly simple, we prove that such a mechanism can enable GNNs to preserve node proximities in theory while maintaining permutation-equivariance with certain parametrization. Extensive experimental results demonstrate the effectiveness and efficiency of SMP for tasks including node classification and link prediction.", "one-sentence_summary": "A simple GNN that maintains both proximity-awareness and permutation-equivariance properties using Stochastic Message Passing", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|a_simple_and_general_graph_neural_network_with_stochastic_message_passing", "pdf": "/pdf/cd009ff9657ac7470f15efc26a9ffacff8cfd79c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8TzrbYcOP", "_bibtex": "@misc{\nzhang2021a,\ntitle={A Simple and General Graph Neural Network with Stochastic Message Passing},\nauthor={Ziwei Zhang and Chenhao Niu and Peng Cui and Bo Zhang and Wei Cui and Wenwu Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=fhcMwjavKEZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fhcMwjavKEZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper70/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper70/Authors|ICLR.cc/2021/Conference/Paper70/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874839, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment"}}}, {"id": "f4Q4Ga-_aY", "original": null, "number": 8, "cdate": 1605682300167, "ddate": null, "tcdate": 1605682300167, "tmdate": 1605682366630, "tddate": null, "forum": "fhcMwjavKEZ", "replyto": "SeZIgAK6dhq", "invitation": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment", "content": {"title": "Thank you for your response. Need clarification.", "comment": "I thank the authors for the response. I need some clarification on how SMP works exactly. Does SMP (1) randomly sample the input node embeddings from Gaussian in every forward pass (hence, the computed node embeddings are different in every forward pass), or (2) sample the Gaussian vectors once at the beginning and subsequently use the same ones throughout the training and inference (hence, the computed node embeddings are the same for different forward passes)?\n\nIf (1) is the case, Theorem 3 should not hold since the computed node embeddings are stochastic, while Definition 4 is about the static property (no statement about \"with high probability\"). I think Definition 4 needs to be modified to take the stochastic nature of the SMP into account.\n\nIf (2) is the case, then all the results are trivial to me, since having fixed unique embeddings for each node + having the very expressive decoder should be able to approximate any functions over a pair of nodes (e.g., shortest path distance). Thus, Definition 4 is trivially attained.\n\nRe: \"Besides, using one-hot IDs will drastically increase the number of parameters since the input features of GNNs have a dimensionality of O(n).\"\n\nI disagree with this. If one-hot IDs are used and the embedding layer is not trained (so the parameters stay the randomly initialized vectors throughout the training), then this is equivalent to SMP of case (2).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper70/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple and General Graph Neural Network with Stochastic Message Passing", "authorids": ["~Ziwei_Zhang1", "nch16@mails.tsinghua.edu.cn", "~Peng_Cui1", "nevinzhang@tencent.com", "cuiwei@songshuai.com", "~Wenwu_Zhu1"], "authors": ["Ziwei Zhang", "Chenhao Niu", "Peng Cui", "Bo Zhang", "Wei Cui", "Wenwu Zhu"], "keywords": ["Graph Neural Network", "Node Proximity", "Permutation-equivariant"], "abstract": "Graph neural networks (GNNs) are emerging machine learning models on graphs. One key property behind the expressiveness of existing GNNs is that the learned node representations are permutation-equivariant. Though being a desirable property for certain tasks, however, permutation-equivariance prevents GNNs from being proximity-aware, i.e., preserving the walk-based proximities between pairs of nodes, which is another critical property for graph analytical tasks. On the other hand, some variants of GNNs are proposed to preserve node proximities, but they fail to maintain permutation-equivariance. How to empower GNNs to be proximity aware while maintaining permutation-equivariance remains an open problem. In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to maintain both proximity-awareness and permutation-equivariance properties. Specifically, we augment the existing GNNs with stochastic node representations learned to preserve node proximities. Though seemingly simple, we prove that such a mechanism can enable GNNs to preserve node proximities in theory while maintaining permutation-equivariance with certain parametrization. Extensive experimental results demonstrate the effectiveness and efficiency of SMP for tasks including node classification and link prediction.", "one-sentence_summary": "A simple GNN that maintains both proximity-awareness and permutation-equivariance properties using Stochastic Message Passing", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|a_simple_and_general_graph_neural_network_with_stochastic_message_passing", "pdf": "/pdf/cd009ff9657ac7470f15efc26a9ffacff8cfd79c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8TzrbYcOP", "_bibtex": "@misc{\nzhang2021a,\ntitle={A Simple and General Graph Neural Network with Stochastic Message Passing},\nauthor={Ziwei Zhang and Chenhao Niu and Peng Cui and Bo Zhang and Wei Cui and Wenwu Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=fhcMwjavKEZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fhcMwjavKEZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper70/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper70/Authors|ICLR.cc/2021/Conference/Paper70/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874839, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment"}}}, {"id": "1nYuZcVvOpL", "original": null, "number": 6, "cdate": 1605677471174, "ddate": null, "tcdate": 1605677471174, "tmdate": 1605677471174, "tddate": null, "forum": "fhcMwjavKEZ", "replyto": "whLxvQhrGVY", "invitation": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment", "content": {"title": "To Reviewer 4", "comment": "Thank you for your recognition and kind words! Here are our responses to your comments.   \n\nQ: If I had to nitpick, I'd say that part of Table 7 (currently in the Appendix) belongs to the main paper, as I was convinced about the superior runtime performance of SMP only after reading those numbers. If the avg running time for an SMP epoch was significantly larger than the one for GAT (for instance), I would have considered SMP yet another specialized model. Instead, given that the GPU consumption (both in terms of computation and memory) is similar to any other GNN model, I believe SMP could be adopted as a more flexible graph ML method (which would avoid having to choose a method given the target task, e.g., node classification vs. link prediction).  \nA: Thanks for the suggestion. We have moved the table to the main paper in the revised version. We also appreciate the reviewer\u2019s comments that the high-efficiency makes our proposed method a flexible approach to handle graph-based machine learning tasks.\n\nQ: One remark about L278: SMP cannot be considered anymore SotA for ogbl-ppa. The current SotA is more than 10 points above the performance achieved by SMP.  \nA: Thanks for the notice that SEAL recently achieves a new SOTA on PPA (which happens after our submission). We have clarified our expression in the revised version. Besides, since SEAL is a GNN variant specifically designed for link prediction, we are interested to see whether our proposed method can further improve SEAL.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper70/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple and General Graph Neural Network with Stochastic Message Passing", "authorids": ["~Ziwei_Zhang1", "nch16@mails.tsinghua.edu.cn", "~Peng_Cui1", "nevinzhang@tencent.com", "cuiwei@songshuai.com", "~Wenwu_Zhu1"], "authors": ["Ziwei Zhang", "Chenhao Niu", "Peng Cui", "Bo Zhang", "Wei Cui", "Wenwu Zhu"], "keywords": ["Graph Neural Network", "Node Proximity", "Permutation-equivariant"], "abstract": "Graph neural networks (GNNs) are emerging machine learning models on graphs. One key property behind the expressiveness of existing GNNs is that the learned node representations are permutation-equivariant. Though being a desirable property for certain tasks, however, permutation-equivariance prevents GNNs from being proximity-aware, i.e., preserving the walk-based proximities between pairs of nodes, which is another critical property for graph analytical tasks. On the other hand, some variants of GNNs are proposed to preserve node proximities, but they fail to maintain permutation-equivariance. How to empower GNNs to be proximity aware while maintaining permutation-equivariance remains an open problem. In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to maintain both proximity-awareness and permutation-equivariance properties. Specifically, we augment the existing GNNs with stochastic node representations learned to preserve node proximities. Though seemingly simple, we prove that such a mechanism can enable GNNs to preserve node proximities in theory while maintaining permutation-equivariance with certain parametrization. Extensive experimental results demonstrate the effectiveness and efficiency of SMP for tasks including node classification and link prediction.", "one-sentence_summary": "A simple GNN that maintains both proximity-awareness and permutation-equivariance properties using Stochastic Message Passing", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|a_simple_and_general_graph_neural_network_with_stochastic_message_passing", "pdf": "/pdf/cd009ff9657ac7470f15efc26a9ffacff8cfd79c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8TzrbYcOP", "_bibtex": "@misc{\nzhang2021a,\ntitle={A Simple and General Graph Neural Network with Stochastic Message Passing},\nauthor={Ziwei Zhang and Chenhao Niu and Peng Cui and Bo Zhang and Wei Cui and Wenwu Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=fhcMwjavKEZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fhcMwjavKEZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper70/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper70/Authors|ICLR.cc/2021/Conference/Paper70/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874839, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment"}}}, {"id": "xrgXOf-KBqE", "original": null, "number": 5, "cdate": 1605677421238, "ddate": null, "tcdate": 1605677421238, "tmdate": 1605677421238, "tddate": null, "forum": "fhcMwjavKEZ", "replyto": "kxYrBgPHjeW", "invitation": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment", "content": {"title": "To Reviewer 1", "comment": "Thanks for your recognition and comments. Here are our responses to your questions.  \n\nQ1: Multiple tasks have been conducted to show the performance of the proposed model, but it is unclear which dataset or which task requires to be proximity-aware. It would be great if there's some quantitative metric that shows the importance of proximity in a given task. For example, the number of automorphic node pairs (within k-hop) and the ratio of label (dis)agreement would be a possible metric in node classification tasks. This will characterize the differences between datasets and highlight the contribution of the proposed method. Additionally, showing some examples of automorphic node pairs and the performance on these nodes could demonstrate the difference between models.  \nA1: Thank you for this insightful suggestion. We agree that a metric to quantify to what degree a task is proximity-aware or permutation-equivariant will be helpful for further studying GNNs. However, we find that such a metric is difficult to design due to two reasons. Firstly, only comparing whether two nodes are strictly automorphic, as you have suggested, is important but may not be sufficient since if the local structures of two nodes are very similar (but not strictly automorphic), the results of GNNs will also be affected. Thus, we need to quantify the similarities between local structures of different nodes (rather than only comparing whether they are exactly automorphic). However, this is an interesting research question itself, e.g., commonly studied in graph kernels. Secondly, besides this metric related to permutation-equivariance, we need another metric to quantify whether the task can benefit from proximity-awareness, which is also non-trivial to develop. All things considered, we leave studying this interesting topic as important future works.  \n\nQ2: Resampling random matrix at each epoch is emphasized multiple times in the manuscript, but without any empirical experiments. Would it be beneficial to resample this random matrix at every epoch? Although in theory, it would be possible to learn GNN that preserves node proximity, if a given task doesn't need to model proximity-aware representations, random resampling may hinder the convergence of the proposed method.  \nA2: Thank you for this comment. As we explain in Section 4.1, we find that fixing the stochastic matrix can help the model to memorize the stochastic representation and distinguish different nodes, while resampling the matrix in each epoch will make our method more capable of handling nodes not seen during training. As a result, we fix the stochastic matrix on transductive datasets and resample the stochastic matrix on inductive datasets. Following your comments, we have added an ablation study in Table 12 in Appendix B.5 in the revised manuscript. The results verify that our design leads to better results in most cases, i.e., fixing the stochastic matrix on transductive datasets and resampling the stochastic matrix on inductive datasets. Please refer to the revised manuscript for details. \n\nQ3: There was a bug in the official release of the P-GNN paper, which has been recently fixed (please check the GitHub pull request history at https://github.com/JiaxuanYou/P-GNN/pull/12). I wonder which codebase the authors used for the experiments. / It would be also good if there's some comment on what makes P-GNN memory hunger. Is it because of improper implementation or because of some inherent limitations?  \nA3: Thank you for this notice. We indeed adopt the implementation by the authors. Since this bug is fixed after our submission, the results in the paper are based on the implementation before the fix. We have tried the fixed codes and find the results comparable or even slightly worse than the results reported in the paper. As for the memory issue, we think it is caused by the mechanism of P-GNN, i.e., explicit construction of the anchor nodes and performing message-passing on them.   \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper70/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple and General Graph Neural Network with Stochastic Message Passing", "authorids": ["~Ziwei_Zhang1", "nch16@mails.tsinghua.edu.cn", "~Peng_Cui1", "nevinzhang@tencent.com", "cuiwei@songshuai.com", "~Wenwu_Zhu1"], "authors": ["Ziwei Zhang", "Chenhao Niu", "Peng Cui", "Bo Zhang", "Wei Cui", "Wenwu Zhu"], "keywords": ["Graph Neural Network", "Node Proximity", "Permutation-equivariant"], "abstract": "Graph neural networks (GNNs) are emerging machine learning models on graphs. One key property behind the expressiveness of existing GNNs is that the learned node representations are permutation-equivariant. Though being a desirable property for certain tasks, however, permutation-equivariance prevents GNNs from being proximity-aware, i.e., preserving the walk-based proximities between pairs of nodes, which is another critical property for graph analytical tasks. On the other hand, some variants of GNNs are proposed to preserve node proximities, but they fail to maintain permutation-equivariance. How to empower GNNs to be proximity aware while maintaining permutation-equivariance remains an open problem. In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to maintain both proximity-awareness and permutation-equivariance properties. Specifically, we augment the existing GNNs with stochastic node representations learned to preserve node proximities. Though seemingly simple, we prove that such a mechanism can enable GNNs to preserve node proximities in theory while maintaining permutation-equivariance with certain parametrization. Extensive experimental results demonstrate the effectiveness and efficiency of SMP for tasks including node classification and link prediction.", "one-sentence_summary": "A simple GNN that maintains both proximity-awareness and permutation-equivariance properties using Stochastic Message Passing", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|a_simple_and_general_graph_neural_network_with_stochastic_message_passing", "pdf": "/pdf/cd009ff9657ac7470f15efc26a9ffacff8cfd79c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8TzrbYcOP", "_bibtex": "@misc{\nzhang2021a,\ntitle={A Simple and General Graph Neural Network with Stochastic Message Passing},\nauthor={Ziwei Zhang and Chenhao Niu and Peng Cui and Bo Zhang and Wei Cui and Wenwu Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=fhcMwjavKEZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fhcMwjavKEZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper70/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper70/Authors|ICLR.cc/2021/Conference/Paper70/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874839, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment"}}}, {"id": "2t0Jez5QbeC", "original": null, "number": 4, "cdate": 1605677332924, "ddate": null, "tcdate": 1605677332924, "tmdate": 1605677332924, "tddate": null, "forum": "fhcMwjavKEZ", "replyto": "5HVM0qdJs1V", "invitation": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment", "content": {"title": "To Reviewer 2", "comment": "Thanks for your kind words and detailed comments. Here are our responses to your comments.  \n\nQ1. The work of Sato et al. [1] seems to be closer to this work than the authors let know. I would welcome a more in-depth discussion than given in related work, even if the paper is only on arxiv.  \n[1] Sato et al., Random features strengthen graph neural networks. arXiv:2002.03155  \nA1. Thanks for your suggestion. We have revised the related works as follows: \u201dFor example, Sato et al. (Sato et al., 2020) novelly show that random numbers can enhance GNNs in tackling two important graph-based NP problems with a theoretical guarantee, namely the minimum dominating set and the maximum matching problem\u2026Our work differs in that we systematically study how to preserve permutation-equivariance and proximity-awareness simultaneously in a simple yet effective framework, which is a new topic different from these existing works. Besides, we theoretically prove that our proposed method can preserve walk-based proximities by using the random projection literature. We also demonstrate the effectiveness of our method on various large-scale benchmarks for both node- and edge-level tasks, while no similar results are reported in the literature.\u201d\n\nQ2. Definition 4 should go to the main text, as it is crucial for understanding.  \nA2: Thanks for the suggestion. We have moved the definition to the main paper in the revised version.\n\nQ3. The proof of Theorem 1 shows the result only for graphs with 2 connected components. I think the theorem also holds for connected graphs (with the right automorphisms), which is important. If it wouldn't, the result would not always be relevant for practice. I think the current way of proving the theorem lacks that insight and discussion. Isn't it possible to prove the theorem for all graphs with a certain automorphism, regardless of number of connected components?  \nA3. Thanks for this insightful and constructive comment. We indeed find that we can prove the theorem using one connected component under a mild assumption (i.e., the walk-based proximity is of finite length). We have provided this additional proof in Appendix A.1 in the revised version. The essential idea is to construct automorphism using three copies of the graph and three bridges to connect these copies. Please refer to the revised manuscript for details.\n\nQ4. Can the authors verify the suspicion that the non-linear variants overfit (line 270) by presenting the results on training data?  \nA4: Thank you for this insightful comment. We have provided your suggested experiments in Appendix B.6 in the revised version. The results show that non-linear variants of GNNs such as GAT indeed exhibit more serious overfitting, i.e., the margins between the training accuracies and the testing accuracies are usually larger than the linear variant SGC. Besides, non-linear variants are also difficult to train, i.e., the training accuracies are sometimes also worse than the linear variant SGC. We have clarified our expressions as follows: \u201cSome plausible reasons include that the additional model complexity brought by non-linear operators makes the models tend to overfit and also difficult to train.\u201d\n\nQ5. It might be of interest to the authors that the idea was already used in a practical graph matching method by [2] (page 5, last paragraph), although without any theoretical analysis. It is nice to have a formal framework that justifies the application of this trick.  \n[2] Fey et al., Deep Graph Matching Consensus, ICLR 2020  \nA5. Thanks for your notice on this interesting relevant literature. We have added it to the related works in the revised version.\n\nQ6. In general, GNNs to solve matching tasks are a very fitting application for the proposed method, which the authors do not consider. They often rely on comparing distance measures in both domains, thus need proximity awareness.  \nA6. Thanks for your constructive suggestion. We will consider trying our proposed method in graph matching tasks.\n\nQ7: Typos:  \nLine 87: \"computationally expansive\" -> expensive  \nLine 607: \"computationally expansive\" -> expensive  \nA7: Thanks for pointing them out. We have fixed them in the revised version.\n\nQ8. All in all, there is not much to complain about. The paper achieves what it sets out to do in providing an exhaustive theoretical and empirical analysis of a simple but effective idea. The method itself is straight-forward and also not entirely novel. However, the formal framework and analysis is a contribution that is of interest to the community. Due to the shown properties and the high efficiency of the approach, the paper can have significant impact on future GNN architectures in practice. I therefore recommend to accept the paper.  \nA8: Thank you for your recognition that our paper can significantly impact future GNNs! We intend to further explore this direction in the near future.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper70/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple and General Graph Neural Network with Stochastic Message Passing", "authorids": ["~Ziwei_Zhang1", "nch16@mails.tsinghua.edu.cn", "~Peng_Cui1", "nevinzhang@tencent.com", "cuiwei@songshuai.com", "~Wenwu_Zhu1"], "authors": ["Ziwei Zhang", "Chenhao Niu", "Peng Cui", "Bo Zhang", "Wei Cui", "Wenwu Zhu"], "keywords": ["Graph Neural Network", "Node Proximity", "Permutation-equivariant"], "abstract": "Graph neural networks (GNNs) are emerging machine learning models on graphs. One key property behind the expressiveness of existing GNNs is that the learned node representations are permutation-equivariant. Though being a desirable property for certain tasks, however, permutation-equivariance prevents GNNs from being proximity-aware, i.e., preserving the walk-based proximities between pairs of nodes, which is another critical property for graph analytical tasks. On the other hand, some variants of GNNs are proposed to preserve node proximities, but they fail to maintain permutation-equivariance. How to empower GNNs to be proximity aware while maintaining permutation-equivariance remains an open problem. In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to maintain both proximity-awareness and permutation-equivariance properties. Specifically, we augment the existing GNNs with stochastic node representations learned to preserve node proximities. Though seemingly simple, we prove that such a mechanism can enable GNNs to preserve node proximities in theory while maintaining permutation-equivariance with certain parametrization. Extensive experimental results demonstrate the effectiveness and efficiency of SMP for tasks including node classification and link prediction.", "one-sentence_summary": "A simple GNN that maintains both proximity-awareness and permutation-equivariance properties using Stochastic Message Passing", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|a_simple_and_general_graph_neural_network_with_stochastic_message_passing", "pdf": "/pdf/cd009ff9657ac7470f15efc26a9ffacff8cfd79c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8TzrbYcOP", "_bibtex": "@misc{\nzhang2021a,\ntitle={A Simple and General Graph Neural Network with Stochastic Message Passing},\nauthor={Ziwei Zhang and Chenhao Niu and Peng Cui and Bo Zhang and Wei Cui and Wenwu Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=fhcMwjavKEZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fhcMwjavKEZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper70/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper70/Authors|ICLR.cc/2021/Conference/Paper70/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874839, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment"}}}, {"id": "SeZIgAK6dhq", "original": null, "number": 3, "cdate": 1605677149695, "ddate": null, "tcdate": 1605677149695, "tmdate": 1605677261757, "tddate": null, "forum": "fhcMwjavKEZ", "replyto": "309j2Q8Yzew", "invitation": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment", "content": {"title": "To Reviewer 3 (continued)", "comment": "Q6. In Section 5.3, it is unclear why the OGB node classification datasets are not used.  \nA6: We do not adopt more OGB datasets because we have already adopted ten datasets commonly used in GNN. These datasets already cover a wide spectrum of domains, sizes, and with or without node features, which we believe is more than sufficient to demonstrate our proposed method."}, "signatures": ["ICLR.cc/2021/Conference/Paper70/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple and General Graph Neural Network with Stochastic Message Passing", "authorids": ["~Ziwei_Zhang1", "nch16@mails.tsinghua.edu.cn", "~Peng_Cui1", "nevinzhang@tencent.com", "cuiwei@songshuai.com", "~Wenwu_Zhu1"], "authors": ["Ziwei Zhang", "Chenhao Niu", "Peng Cui", "Bo Zhang", "Wei Cui", "Wenwu Zhu"], "keywords": ["Graph Neural Network", "Node Proximity", "Permutation-equivariant"], "abstract": "Graph neural networks (GNNs) are emerging machine learning models on graphs. One key property behind the expressiveness of existing GNNs is that the learned node representations are permutation-equivariant. Though being a desirable property for certain tasks, however, permutation-equivariance prevents GNNs from being proximity-aware, i.e., preserving the walk-based proximities between pairs of nodes, which is another critical property for graph analytical tasks. On the other hand, some variants of GNNs are proposed to preserve node proximities, but they fail to maintain permutation-equivariance. How to empower GNNs to be proximity aware while maintaining permutation-equivariance remains an open problem. In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to maintain both proximity-awareness and permutation-equivariance properties. Specifically, we augment the existing GNNs with stochastic node representations learned to preserve node proximities. Though seemingly simple, we prove that such a mechanism can enable GNNs to preserve node proximities in theory while maintaining permutation-equivariance with certain parametrization. Extensive experimental results demonstrate the effectiveness and efficiency of SMP for tasks including node classification and link prediction.", "one-sentence_summary": "A simple GNN that maintains both proximity-awareness and permutation-equivariance properties using Stochastic Message Passing", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|a_simple_and_general_graph_neural_network_with_stochastic_message_passing", "pdf": "/pdf/cd009ff9657ac7470f15efc26a9ffacff8cfd79c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8TzrbYcOP", "_bibtex": "@misc{\nzhang2021a,\ntitle={A Simple and General Graph Neural Network with Stochastic Message Passing},\nauthor={Ziwei Zhang and Chenhao Niu and Peng Cui and Bo Zhang and Wei Cui and Wenwu Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=fhcMwjavKEZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fhcMwjavKEZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper70/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper70/Authors|ICLR.cc/2021/Conference/Paper70/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874839, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment"}}}, {"id": "309j2Q8Yzew", "original": null, "number": 2, "cdate": 1605677073044, "ddate": null, "tcdate": 1605677073044, "tmdate": 1605677235912, "tddate": null, "forum": "fhcMwjavKEZ", "replyto": "y9a3ejyDknM", "invitation": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment", "content": {"title": "To Reviewer 3", "comment": "Thank you for your detailed comments. Here are our responses to your questions. \n\nQ1-1. The entire Section 3 is not novel. It is a mixture of the preliminaries of GNNs and the trivial results of Corollary 1 and Theorem 1.   \nA1-1. Section 3 provides the necessary preliminaries of GNNs and several key definitions, which is important to keep our paper self-contained. Besides, we give novel proof that the existing permutation-equivariant GNNs cannot preserve node proximities, which lays the foundations for further developing our proposed method.  \nQ1-2. Moreover, Theorem 1 is not rigorous. What if walk-based proximity is just a constant function?  \nA1-2. Thanks for the suggestion. We have clarified that we only consider non-trivial walk-based proximity in the revised version. Please refer to Theorem 1 of the revised manuscript for details.  \nQ1-3. What if a given graph does not contain any automorphism?  \nA1-3: As we show in Theorem 1, (non-trivial) automorphism presents an important limitation to the existing GNNs, i.e., preventing GNNs from being proximity-aware. Considering the importance of this limitation, we think it is critical to study this problem even not all graphs contain automorphism.  \n\nQ2. \u201cPreserve walk-based similarity\u201d is not rigorously defined. It seems that it just means all nodes have different embeddings.  \nA2. We kindly do not agree that preserving walk-based proximity is equivalent to having different node embeddings. The definition was formally provided in Definition 4 in Appendix A.1 due to the page limit and we have moved it into the main paper (Section 3) in the revised version. In a nutshell, a GNN is said to be able to preserve walk-based similarities if we can recover the similarity between two nodes from the corresponding two node embeddings. Thus, the embedding vectors need to encode sufficient information of the walk-based proximity rather than simply being unique node identifiers. Please refer to the revised manuscript for details.\n\nQ3. The proposed model is not permutation-equivariant after adding Gaussian noise. It is trivial that the model becomes permutation-equivariant when the Gaussian noise is ignored (because the model just reduces to an ordinary GNN).  \nA3. We kindly do not agree that these theorems should be considered trivial. Our Remark 1 and Corollary 2 indeed show that we can easily recover a permutation-equivariant GNN by ignoring the stochastics representations. Though not using sophisticated proof strategies, these theorems are indispensable to show that our proposed method is able to handle both permutation-equivariant and proximity-aware tasks. On the contrary, though P-GNN can preserve node proximities to a certain extent, it cannot reduce to a permutation-equivariant GNN and thus fail to handle tasks where permutation-equivariance is helpful. We also empirically validate the importance of this result in experiments, i.e., in the node classification task in Section 5.3, our proposed SMP achieves comparable results as GCNs but P-GNN performs poorly. As a result, we believe these theorems are important for our proposed method.\n\nQ4. The claim that SMP preserves walk-based proximity is trivial and just relies on the fact that randomly-sampled vectors from a Gaussian distribution are different from each other.  \nA4. We kindly do not agree that the theorem is trivial. If only i.i.d. Gaussian random vectors are adopted as the reviewer suggests, it is impossible to preserve walk-based proximities since these random vectors are independent of the proximity. In fact, we need to properly perform message-passing on those random vectors and carefully decode the node representations to preserve the proximity. We have given and proved the exact procedure for both the linear case (see Theorem 2) and the non-linear case (see Theorem 3).    \n\nQ5. The idea of using node identifiers (essentially equivalent to the Gaussian noise) to make GNNs position-aware is not new. In fact, this idea is clearly mentioned in the P-GNN paper already (Section 6.2 of [1] \u201cfor inductive tasks, augmenting node attributes with one-hot identifiers restricts a model\u2019s generalization ability\u201d).  \n[1] https://arxiv.org/abs/1906.04817  \nA5. We humbly do not agree that using Gaussian random vectors is equivalent to using one-hot identifiers. We have compared our proposed method using random vectors with using one-hot IDs as node identifiers in Table 8 in the appendix. The results show that our proposed method generally achieves better results. Besides, using one-hot IDs will drastically increase the number of parameters since the input features of GNNs have a dimensionality of O(n). Besides, we\u2019d like to point out that we have compared with P-GNN as a baseline in all the experiments.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper70/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple and General Graph Neural Network with Stochastic Message Passing", "authorids": ["~Ziwei_Zhang1", "nch16@mails.tsinghua.edu.cn", "~Peng_Cui1", "nevinzhang@tencent.com", "cuiwei@songshuai.com", "~Wenwu_Zhu1"], "authors": ["Ziwei Zhang", "Chenhao Niu", "Peng Cui", "Bo Zhang", "Wei Cui", "Wenwu Zhu"], "keywords": ["Graph Neural Network", "Node Proximity", "Permutation-equivariant"], "abstract": "Graph neural networks (GNNs) are emerging machine learning models on graphs. One key property behind the expressiveness of existing GNNs is that the learned node representations are permutation-equivariant. Though being a desirable property for certain tasks, however, permutation-equivariance prevents GNNs from being proximity-aware, i.e., preserving the walk-based proximities between pairs of nodes, which is another critical property for graph analytical tasks. On the other hand, some variants of GNNs are proposed to preserve node proximities, but they fail to maintain permutation-equivariance. How to empower GNNs to be proximity aware while maintaining permutation-equivariance remains an open problem. In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to maintain both proximity-awareness and permutation-equivariance properties. Specifically, we augment the existing GNNs with stochastic node representations learned to preserve node proximities. Though seemingly simple, we prove that such a mechanism can enable GNNs to preserve node proximities in theory while maintaining permutation-equivariance with certain parametrization. Extensive experimental results demonstrate the effectiveness and efficiency of SMP for tasks including node classification and link prediction.", "one-sentence_summary": "A simple GNN that maintains both proximity-awareness and permutation-equivariance properties using Stochastic Message Passing", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|a_simple_and_general_graph_neural_network_with_stochastic_message_passing", "pdf": "/pdf/cd009ff9657ac7470f15efc26a9ffacff8cfd79c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8TzrbYcOP", "_bibtex": "@misc{\nzhang2021a,\ntitle={A Simple and General Graph Neural Network with Stochastic Message Passing},\nauthor={Ziwei Zhang and Chenhao Niu and Peng Cui and Bo Zhang and Wei Cui and Wenwu Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=fhcMwjavKEZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fhcMwjavKEZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper70/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper70/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper70/Authors|ICLR.cc/2021/Conference/Paper70/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874839, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper70/-/Official_Comment"}}}, {"id": "kxYrBgPHjeW", "original": null, "number": 3, "cdate": 1603851956747, "ddate": null, "tcdate": 1603851956747, "tmdate": 1605024769235, "tddate": null, "forum": "fhcMwjavKEZ", "replyto": "fhcMwjavKEZ", "invitation": "ICLR.cc/2021/Conference/Paper70/-/Official_Review", "content": {"title": "Review...", "review": "This paper proposed a proximity-aware graph neural network while maintaining the permutation equivariance property. The proposed model, dubbed as stochastic message passing (SMP), arguments the existing GNNs with stochastic node representations. The author proved the proposed method can model proximity-aware representations based on random projection theory. The experimental results show that the SMP can be used for multiple graphs and tasks. \n\nAlthough the proposed technique is relatively simple, the theorem shows that why such stochastic representations are beneficial for proximity-aware tasks. The experimental results also suggest that this simple technique is quite effective in many tasks. There are a few things that I'd like to comment on or ask listed below:\n\n- Multiple tasks have been conducted to show the performance of the proposed model, but it is unclear which dataset or which task requires to be proximity-aware. It would be great if there's some quantitative metric that shows the importance of proximity in a given task. For example, the number of automorphic node pairs (within k-hop) and the ratio of label (dis)agreement would be a possible metric in node classification tasks. This will characterize the differences between datasets and highlight the contribution of the proposed method. Additionally, showing some examples of automorphic node pairs and the performance on these nodes could demonstrate the difference between models.\n\n- Resampling random matrix at each epoch is emphasized multiple times in the manuscript, but without any empirical experiments. Would it be beneficial to resample this random matrix at every epoch? Although in theory, it would be possible to learn GNN that preserves node proximity, if a given task doesn't need to model proximity-aware representations, random resampling may hinder the convergence of the proposed method.\n\n- There was a bug in the official release of the P-GNN paper, which has been recently fixed (please check the GitHub pull request history at https://github.com/JiaxuanYou/P-GNN/pull/12). I wonder which codebase the authors used for the experiments. / It would be also good if there's some comment on what makes P-GNN memory hunger. Is it because of improper implementation or because of some inherent limitations?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper70/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple and General Graph Neural Network with Stochastic Message Passing", "authorids": ["~Ziwei_Zhang1", "nch16@mails.tsinghua.edu.cn", "~Peng_Cui1", "nevinzhang@tencent.com", "cuiwei@songshuai.com", "~Wenwu_Zhu1"], "authors": ["Ziwei Zhang", "Chenhao Niu", "Peng Cui", "Bo Zhang", "Wei Cui", "Wenwu Zhu"], "keywords": ["Graph Neural Network", "Node Proximity", "Permutation-equivariant"], "abstract": "Graph neural networks (GNNs) are emerging machine learning models on graphs. One key property behind the expressiveness of existing GNNs is that the learned node representations are permutation-equivariant. Though being a desirable property for certain tasks, however, permutation-equivariance prevents GNNs from being proximity-aware, i.e., preserving the walk-based proximities between pairs of nodes, which is another critical property for graph analytical tasks. On the other hand, some variants of GNNs are proposed to preserve node proximities, but they fail to maintain permutation-equivariance. How to empower GNNs to be proximity aware while maintaining permutation-equivariance remains an open problem. In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to maintain both proximity-awareness and permutation-equivariance properties. Specifically, we augment the existing GNNs with stochastic node representations learned to preserve node proximities. Though seemingly simple, we prove that such a mechanism can enable GNNs to preserve node proximities in theory while maintaining permutation-equivariance with certain parametrization. Extensive experimental results demonstrate the effectiveness and efficiency of SMP for tasks including node classification and link prediction.", "one-sentence_summary": "A simple GNN that maintains both proximity-awareness and permutation-equivariance properties using Stochastic Message Passing", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|a_simple_and_general_graph_neural_network_with_stochastic_message_passing", "pdf": "/pdf/cd009ff9657ac7470f15efc26a9ffacff8cfd79c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8TzrbYcOP", "_bibtex": "@misc{\nzhang2021a,\ntitle={A Simple and General Graph Neural Network with Stochastic Message Passing},\nauthor={Ziwei Zhang and Chenhao Niu and Peng Cui and Bo Zhang and Wei Cui and Wenwu Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=fhcMwjavKEZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "fhcMwjavKEZ", "replyto": "fhcMwjavKEZ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper70/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538150888, "tmdate": 1606915793705, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper70/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper70/-/Official_Review"}}}, {"id": "5HVM0qdJs1V", "original": null, "number": 2, "cdate": 1603798992300, "ddate": null, "tcdate": 1603798992300, "tmdate": 1605024769162, "tddate": null, "forum": "fhcMwjavKEZ", "replyto": "fhcMwjavKEZ", "invitation": "ICLR.cc/2021/Conference/Paper70/-/Official_Review", "content": {"title": "A well-rounded analysis of a simple method.", "review": "The authors propose to add random node features to the input of message passing graph neural networks for them to become proximity-aware. The paper provides exhaustive theoretical and empirical analysis of this idea, highlighting the advantageous properties.\n\nStrengths:\n- The paper is well written.\n- The authors substantiate their claims with proofs and experiments.\n- The method and proofs seem to be mathematically sound.\n- Including the experiments in the appendix, the empirical analysis is very exhaustive and seems to be reproducible.\n- The method is simple but effective, which is good. It also does not lead to a significant computational overhead but fits nicely into the existing message passing framework with linear time complexity (in number of edges).\n- The paper provides a formal framework and analysis for a trick that has already been used successfully in practice.\n\nComments and Questions:\n- The work of Sato et al. [1] seems to be closer to this work than the authors let know. I would welcome a more in-depth discussion than given in related work, even if the paper is only on arxiv.\n- Definition 4 should go to the main text, as it is crucial for understanding.\n- The proof of Theorem 1 shows the result only for graphs with 2 connected components. I think the theorem also holds for connected graphs (with the right automorphisms), which is important. If it wouldn't, the result would not always be relevant for practice. I think the current way of proving the theorem lacks that insight and discussion. Isn't it possible to prove the theorem for all graphs with a certain automorphism, regardless of number of connected components?\n- Can the authors verify the suspicion that the non-linear variants overfit (line 270) by presenting the results on training data?\n- It might be of interest to the authors that the idea was already used in a practical graph matching method by [2] (page 5, last paragraph), although without any theoretical analysis. It is nice to have a formal framework that justifies the application of this trick.\n- In general, GNNs to solve matching tasks are a very fitting application for the proposed method, which the authors do not consider. They often rely on comparing distance measures in both domains, thus need proximity awareness. \n\nTypos:\n- Line 87: \"computationally expansive\" -> expensive\n- Line 607: \"computationally expansive\" -> expensive\n\nAll in all, there is not much to complain about. The paper achieves what it sets out to do in providing an exhaustive theoretical and empirical analysis of a simple but effective idea. The method itself is straight-forward and also not entirely novel. However, the formal framework and analysis is a contribution that is of interest to the community. Due to the shown properties and the high efficiency of the approach, the paper can have significant impact on future GNN architectures in practice. I therefore recommend to accept the paper.\n\n[1] Sato et al., Random features strengthen graph neural networks. arXiv:2002.03155 \n\n[2] Fey et al., Deep Graph Matching Consensus, ICLR 2020", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper70/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple and General Graph Neural Network with Stochastic Message Passing", "authorids": ["~Ziwei_Zhang1", "nch16@mails.tsinghua.edu.cn", "~Peng_Cui1", "nevinzhang@tencent.com", "cuiwei@songshuai.com", "~Wenwu_Zhu1"], "authors": ["Ziwei Zhang", "Chenhao Niu", "Peng Cui", "Bo Zhang", "Wei Cui", "Wenwu Zhu"], "keywords": ["Graph Neural Network", "Node Proximity", "Permutation-equivariant"], "abstract": "Graph neural networks (GNNs) are emerging machine learning models on graphs. One key property behind the expressiveness of existing GNNs is that the learned node representations are permutation-equivariant. Though being a desirable property for certain tasks, however, permutation-equivariance prevents GNNs from being proximity-aware, i.e., preserving the walk-based proximities between pairs of nodes, which is another critical property for graph analytical tasks. On the other hand, some variants of GNNs are proposed to preserve node proximities, but they fail to maintain permutation-equivariance. How to empower GNNs to be proximity aware while maintaining permutation-equivariance remains an open problem. In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to maintain both proximity-awareness and permutation-equivariance properties. Specifically, we augment the existing GNNs with stochastic node representations learned to preserve node proximities. Though seemingly simple, we prove that such a mechanism can enable GNNs to preserve node proximities in theory while maintaining permutation-equivariance with certain parametrization. Extensive experimental results demonstrate the effectiveness and efficiency of SMP for tasks including node classification and link prediction.", "one-sentence_summary": "A simple GNN that maintains both proximity-awareness and permutation-equivariance properties using Stochastic Message Passing", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|a_simple_and_general_graph_neural_network_with_stochastic_message_passing", "pdf": "/pdf/cd009ff9657ac7470f15efc26a9ffacff8cfd79c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8TzrbYcOP", "_bibtex": "@misc{\nzhang2021a,\ntitle={A Simple and General Graph Neural Network with Stochastic Message Passing},\nauthor={Ziwei Zhang and Chenhao Niu and Peng Cui and Bo Zhang and Wei Cui and Wenwu Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=fhcMwjavKEZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "fhcMwjavKEZ", "replyto": "fhcMwjavKEZ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper70/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538150888, "tmdate": 1606915793705, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper70/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper70/-/Official_Review"}}}, {"id": "whLxvQhrGVY", "original": null, "number": 4, "cdate": 1604207354037, "ddate": null, "tcdate": 1604207354037, "tmdate": 1605024769095, "tddate": null, "forum": "fhcMwjavKEZ", "replyto": "fhcMwjavKEZ", "invitation": "ICLR.cc/2021/Conference/Paper70/-/Official_Review", "content": {"title": "A significant improvement over P-GNN", "review": "Since the publication of P-GNN (You et al., 2019), it has become clear to the graph ML community that node positional information can be effectively leveraged for link prediction and pairwise node classification tasks.\nThis paper introduces SMP, a novel stochastic message passing approach that preserves both permutation-equivariance (common to GNN models) and node proximities. Extensive experimental results show that SMP not only achieves competitive performance on many common graph ML datasets, but also succeeds to combine together the expressiveness of a standard GNN with P-GNN  (without incurring the scalability problem of P-GNN).\n\nI thoroughly enjoyed reading this paper, both for the insights and the technical soundness. I have very few remarks about the paper, as I believe that i) SMP is an ingenious idea, ii) the experimental setting is adequate, iii) the quality of the writeup is high, iv) and the results appear to be reproducible.\n\nIf I had to nitpick, I'd say that part of Table 7 (currently in the Appendix) belongs to the main paper, as I was convinced about the superior runtime performance of SMP only after reading those numbers. If the avg running time for an SMP epoch was significantly larger than the one for GAT (for instance), I would have considered SMP yet another specialized model.\nInstead, given that the GPU consumption (both in terms of computation and memory) is similar to any other GNN model, I believe SMP could be adopted as a more flexible graph ML method (which would avoid having to choose a method given the target task, e.g., node classification vs. link prediction).\n\nOne remark about L278: SMP cannot be considered anymore SotA for ogbl-ppa. The current SotA is more than 10 points above the performance achieved by SMP.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper70/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper70/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Simple and General Graph Neural Network with Stochastic Message Passing", "authorids": ["~Ziwei_Zhang1", "nch16@mails.tsinghua.edu.cn", "~Peng_Cui1", "nevinzhang@tencent.com", "cuiwei@songshuai.com", "~Wenwu_Zhu1"], "authors": ["Ziwei Zhang", "Chenhao Niu", "Peng Cui", "Bo Zhang", "Wei Cui", "Wenwu Zhu"], "keywords": ["Graph Neural Network", "Node Proximity", "Permutation-equivariant"], "abstract": "Graph neural networks (GNNs) are emerging machine learning models on graphs. One key property behind the expressiveness of existing GNNs is that the learned node representations are permutation-equivariant. Though being a desirable property for certain tasks, however, permutation-equivariance prevents GNNs from being proximity-aware, i.e., preserving the walk-based proximities between pairs of nodes, which is another critical property for graph analytical tasks. On the other hand, some variants of GNNs are proposed to preserve node proximities, but they fail to maintain permutation-equivariance. How to empower GNNs to be proximity aware while maintaining permutation-equivariance remains an open problem. In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to maintain both proximity-awareness and permutation-equivariance properties. Specifically, we augment the existing GNNs with stochastic node representations learned to preserve node proximities. Though seemingly simple, we prove that such a mechanism can enable GNNs to preserve node proximities in theory while maintaining permutation-equivariance with certain parametrization. Extensive experimental results demonstrate the effectiveness and efficiency of SMP for tasks including node classification and link prediction.", "one-sentence_summary": "A simple GNN that maintains both proximity-awareness and permutation-equivariance properties using Stochastic Message Passing", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|a_simple_and_general_graph_neural_network_with_stochastic_message_passing", "pdf": "/pdf/cd009ff9657ac7470f15efc26a9ffacff8cfd79c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=8TzrbYcOP", "_bibtex": "@misc{\nzhang2021a,\ntitle={A Simple and General Graph Neural Network with Stochastic Message Passing},\nauthor={Ziwei Zhang and Chenhao Niu and Peng Cui and Bo Zhang and Wei Cui and Wenwu Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=fhcMwjavKEZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "fhcMwjavKEZ", "replyto": "fhcMwjavKEZ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper70/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538150888, "tmdate": 1606915793705, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper70/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper70/-/Official_Review"}}}], "count": 23}