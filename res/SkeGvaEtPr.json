{"notes": [{"id": "SkeGvaEtPr", "original": "HJgnys5PvS", "number": 589, "cdate": 1569439066515, "ddate": null, "tcdate": 1569439066515, "tmdate": 1577168283146, "tddate": null, "forum": "SkeGvaEtPr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Neural Markov Logic Networks", "authors": ["Giuseppe Marra", "Ond\u0159ej Ku\u017eelka"], "authorids": ["g.marra@unifi.it", "kuzelo1@gmail.com"], "keywords": ["Statistical Relational Learning", "Markov Logic Networks"], "TL;DR": " We introduce a statistical relational learning system that borrows ideas from Markov logic but learns an implicit representation of rules as a neural network.", "abstract": "We introduce Neural Markov Logic Networks (NMLNs), a statistical relational learning system that borrows ideas from Markov logic. Like Markov Logic Networks (MLNs), NMLNs are an exponential-family model for modelling distributions over possible worlds, but unlike MLNs, they do not rely on explicitly specified first-order logic rules. Instead, NMLNs learn an implicit representation of such rules as a neural network that acts as a potential function on fragments of the relational structure. Interestingly, any MLN can be represented as an NMLN. Similarly to recently proposed Neural theorem provers (NTPs) (Rocktaschel at al. 2017), NMLNs can exploit embeddings of constants but, unlike NTPs, NMLNs work well also in their absence. This is extremely important for predicting in settings other than the transductive one. We showcase the potential of NMLNs on knowledge-base completion tasks and on generation of molecular (graph) data.", "pdf": "/pdf/acc10593e90c41f40a61b1c64456c342e773fd62.pdf", "paperhash": "marra|neural_markov_logic_networks", "original_pdf": "/attachment/acc10593e90c41f40a61b1c64456c342e773fd62.pdf", "_bibtex": "@misc{\nmarra2020neural,\ntitle={Neural Markov Logic Networks},\nauthor={Giuseppe Marra and Ond{\\v{r}}ej Ku{\\v{z}}elka},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeGvaEtPr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "xxJgAvD97D", "original": null, "number": 1, "cdate": 1576798700598, "ddate": null, "tcdate": 1576798700598, "tmdate": 1576800935345, "tddate": null, "forum": "SkeGvaEtPr", "replyto": "SkeGvaEtPr", "invitation": "ICLR.cc/2020/Conference/Paper589/-/Decision", "content": {"decision": "Reject", "comment": "This paper on extending MLNs using NNs is borderline acceptable: one reviewer is strongly opposed, although I confess I don't really understand their response to the rebuttal or see what the issue with novelty is (a position shared by the other reviewers). I'm not sure how to weigh this review, but there is not a lot of signal in favour of rejection aside from the rating.\n\nThe remaining two reviews are in favour of acceptance, with their enthusiasm only bounded by the lack of scalability of the method, something they appreciate the authors are upfront about. My view is this paper brings something new to the table which will interest the community, but doesn't oversell the result.\n\nGiven the distribution of papers in my area, this one is just a little too borderline to accept, but this is primarily a reflection of the number of high-quality papers reviewed and the limited space of the conference. I have no doubt this paper will be successful at another conference, and it's a bit of a shame we were not in a position to accept it to this one.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Markov Logic Networks", "authors": ["Giuseppe Marra", "Ond\u0159ej Ku\u017eelka"], "authorids": ["g.marra@unifi.it", "kuzelo1@gmail.com"], "keywords": ["Statistical Relational Learning", "Markov Logic Networks"], "TL;DR": " We introduce a statistical relational learning system that borrows ideas from Markov logic but learns an implicit representation of rules as a neural network.", "abstract": "We introduce Neural Markov Logic Networks (NMLNs), a statistical relational learning system that borrows ideas from Markov logic. Like Markov Logic Networks (MLNs), NMLNs are an exponential-family model for modelling distributions over possible worlds, but unlike MLNs, they do not rely on explicitly specified first-order logic rules. Instead, NMLNs learn an implicit representation of such rules as a neural network that acts as a potential function on fragments of the relational structure. Interestingly, any MLN can be represented as an NMLN. Similarly to recently proposed Neural theorem provers (NTPs) (Rocktaschel at al. 2017), NMLNs can exploit embeddings of constants but, unlike NTPs, NMLNs work well also in their absence. This is extremely important for predicting in settings other than the transductive one. We showcase the potential of NMLNs on knowledge-base completion tasks and on generation of molecular (graph) data.", "pdf": "/pdf/acc10593e90c41f40a61b1c64456c342e773fd62.pdf", "paperhash": "marra|neural_markov_logic_networks", "original_pdf": "/attachment/acc10593e90c41f40a61b1c64456c342e773fd62.pdf", "_bibtex": "@misc{\nmarra2020neural,\ntitle={Neural Markov Logic Networks},\nauthor={Giuseppe Marra and Ond{\\v{r}}ej Ku{\\v{z}}elka},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeGvaEtPr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SkeGvaEtPr", "replyto": "SkeGvaEtPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795729175, "tmdate": 1576800281723, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper589/-/Decision"}}}, {"id": "BkllX-qycH", "original": null, "number": 2, "cdate": 1571950872453, "ddate": null, "tcdate": 1571950872453, "tmdate": 1574639149126, "tddate": null, "forum": "SkeGvaEtPr", "replyto": "SkeGvaEtPr", "invitation": "ICLR.cc/2020/Conference/Paper589/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "This paper presents Neural Markov Logic Networks  (NMLN), which is a generalization of Markov Logic Networks (MLN). Unlike MLN which relies on pre-specified first-order logic (FOL) rules, NMLN learns potential functions parameterized by neural networks on fragments of the graph. The potential function can possibly take into account the constants present using embeddings to better solve transductive problems\uff08otherwise the potential can only use relational structure). To make computation tractable, the size of local potential functions is constrained. Training of this MRF is performed by solving a min-max entropy problem: conditioned on an informative potential, the uncertainties shall be decreased. Experiments on a knowledge base completion task and a graph generation task show superior performance compared to baselines like neural theorem provers.\n\nPros:\n1. no need to specify FOL rules and can potentially discover subtle relations not evident to us.\n2. can be used for generation since the learned rules might be more fine-grained than what we can specify.\n3. it's interesting that on nations knowledge base completion problem even without constant embeddings it works fine, which shows the power of just using relational structure.\n\nCons:\n1. the computation complexity of the global potential function grows combinatorally with the clique size k and polynomially with graph size n, which is unrealistic to any larger graphs than the small molecules, if any higher order statistics matters (e.g. in molecules there are rings).\n\nQuestions:\n1. for training can we use MLE?\n\nOverall this is an interesting work. I think it is a natural generalization of Markov Logic Networks and works on two small problems. I am inclined to recommend this paper to the community.\n\n\n-----updates after reading rebuttal-----\nThanks for the clarification. I don't have further questions.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper589/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper589/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Markov Logic Networks", "authors": ["Giuseppe Marra", "Ond\u0159ej Ku\u017eelka"], "authorids": ["g.marra@unifi.it", "kuzelo1@gmail.com"], "keywords": ["Statistical Relational Learning", "Markov Logic Networks"], "TL;DR": " We introduce a statistical relational learning system that borrows ideas from Markov logic but learns an implicit representation of rules as a neural network.", "abstract": "We introduce Neural Markov Logic Networks (NMLNs), a statistical relational learning system that borrows ideas from Markov logic. Like Markov Logic Networks (MLNs), NMLNs are an exponential-family model for modelling distributions over possible worlds, but unlike MLNs, they do not rely on explicitly specified first-order logic rules. Instead, NMLNs learn an implicit representation of such rules as a neural network that acts as a potential function on fragments of the relational structure. Interestingly, any MLN can be represented as an NMLN. Similarly to recently proposed Neural theorem provers (NTPs) (Rocktaschel at al. 2017), NMLNs can exploit embeddings of constants but, unlike NTPs, NMLNs work well also in their absence. This is extremely important for predicting in settings other than the transductive one. We showcase the potential of NMLNs on knowledge-base completion tasks and on generation of molecular (graph) data.", "pdf": "/pdf/acc10593e90c41f40a61b1c64456c342e773fd62.pdf", "paperhash": "marra|neural_markov_logic_networks", "original_pdf": "/attachment/acc10593e90c41f40a61b1c64456c342e773fd62.pdf", "_bibtex": "@misc{\nmarra2020neural,\ntitle={Neural Markov Logic Networks},\nauthor={Giuseppe Marra and Ond{\\v{r}}ej Ku{\\v{z}}elka},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeGvaEtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkeGvaEtPr", "replyto": "SkeGvaEtPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper589/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper589/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575652133913, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper589/Reviewers"], "noninvitees": [], "tcdate": 1570237749959, "tmdate": 1575652133927, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper589/-/Official_Review"}}}, {"id": "S1xZDwXgsS", "original": null, "number": 4, "cdate": 1573037912740, "ddate": null, "tcdate": 1573037912740, "tmdate": 1573037912740, "tddate": null, "forum": "SkeGvaEtPr", "replyto": "rkxIiJNo5r", "invitation": "ICLR.cc/2020/Conference/Paper589/-/Official_Comment", "content": {"title": "Author response", "comment": "Thank you for the review!\n\n> I think their strong point is at the same time their weak point. \", they do not rely on \n> explicitly specified first-order logic rules.\" My question would be, why not use that \n> information if available as prior knowledge? \n\nIn case such information is available, it can be easily added to the NMLN as another potential - one such rule will correspond to one potential function. For instance, let us suppose that we want to add the rule alpha = sm(x) & friends(x,y) => smokes(y). We can add a potential function which counts the number of true groundings of this rule exactly (as in classical MLNs) to the list of potential functions Phi_i with a weight Beta_alpha. We then treat Beta_alpha exactly as the other Beta_i\u2019s and solve the optimization problem. \n\n> In the contribution it says \"(i) we introduce a new statistical relational model, which\n> overcomes actual limitations of both classical and recent related models such as \" I \n> would have really liked it to have been spelt out which limitations, very specifically and \n> concisely the paper overcomes in that section.\n\nRegarding the recent methods combining neural networks and relational learning, their main limitation compared to NMLNs is that they do not allow expressing joint probability distributions of complete relational structures. On the one hand, it allows them to scale to larger domains for certain problems (e.g. predicting one target predicate from other predicates) but they cannot work for more complex learning and reasoning where one needs to work with the joint probability distribution (one such example is the generation of molecules).\n\nRegarding the more classical methods, such as MLNs (which can model joint probability distributions), their main limitation is the combinatorial nature of the search needed to find the right sets of first-order-logic rules (they also do not allow embeddings which turn out to be useful in the transductive setting)."}, "signatures": ["ICLR.cc/2020/Conference/Paper589/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper589/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Markov Logic Networks", "authors": ["Giuseppe Marra", "Ond\u0159ej Ku\u017eelka"], "authorids": ["g.marra@unifi.it", "kuzelo1@gmail.com"], "keywords": ["Statistical Relational Learning", "Markov Logic Networks"], "TL;DR": " We introduce a statistical relational learning system that borrows ideas from Markov logic but learns an implicit representation of rules as a neural network.", "abstract": "We introduce Neural Markov Logic Networks (NMLNs), a statistical relational learning system that borrows ideas from Markov logic. Like Markov Logic Networks (MLNs), NMLNs are an exponential-family model for modelling distributions over possible worlds, but unlike MLNs, they do not rely on explicitly specified first-order logic rules. Instead, NMLNs learn an implicit representation of such rules as a neural network that acts as a potential function on fragments of the relational structure. Interestingly, any MLN can be represented as an NMLN. Similarly to recently proposed Neural theorem provers (NTPs) (Rocktaschel at al. 2017), NMLNs can exploit embeddings of constants but, unlike NTPs, NMLNs work well also in their absence. This is extremely important for predicting in settings other than the transductive one. We showcase the potential of NMLNs on knowledge-base completion tasks and on generation of molecular (graph) data.", "pdf": "/pdf/acc10593e90c41f40a61b1c64456c342e773fd62.pdf", "paperhash": "marra|neural_markov_logic_networks", "original_pdf": "/attachment/acc10593e90c41f40a61b1c64456c342e773fd62.pdf", "_bibtex": "@misc{\nmarra2020neural,\ntitle={Neural Markov Logic Networks},\nauthor={Giuseppe Marra and Ond{\\v{r}}ej Ku{\\v{z}}elka},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeGvaEtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkeGvaEtPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper589/Authors", "ICLR.cc/2020/Conference/Paper589/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper589/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper589/Reviewers", "ICLR.cc/2020/Conference/Paper589/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper589/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper589/Authors|ICLR.cc/2020/Conference/Paper589/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169193, "tmdate": 1576860534200, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper589/Authors", "ICLR.cc/2020/Conference/Paper589/Reviewers", "ICLR.cc/2020/Conference/Paper589/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper589/-/Official_Comment"}}}, {"id": "B1l4a8mesr", "original": null, "number": 3, "cdate": 1573037756029, "ddate": null, "tcdate": 1573037756029, "tmdate": 1573037756029, "tddate": null, "forum": "SkeGvaEtPr", "replyto": "BkllX-qycH", "invitation": "ICLR.cc/2020/Conference/Paper589/-/Official_Comment", "content": {"title": "Author response", "comment": "Thank you for the review!\n\n> Cons:\n> 1. the computation complexity of the global potential function grows combinatorally with \n> the clique size k and polynomially with graph size n, which is unrealistic to any larger \n> graphs than the small molecules, if any higher order statistics matters (e.g. in molecules \n> there are rings).\n\nWe completely agree and we explicitly admit in the paper as well that scalability is something that we have not solved yet. However, we believe that there are algorithmic ways to improve scalability of the framework for subclasses of NMLNs (first, however, we wanted to present the general framework and show that it can already do interesting things even if only on small relational structures so far). This way the community may also contribute to the development, if people find the framework that we describe here interesting.\n\nFor instance, one such subclass could be NMLNs with potentials that are constant for disconnected fragments. Since many real-world domains are sparse, this might turn out to allow us to scale to larger datasets. There are likely other restricted classes of NMLNs that will turn out to be more tractable.\n\n> Questions:\n> 1. for training can we use MLE?\n\nIndeed, solving the min-max entropy problem is shown in the paper to be equivalent to a maximum-likelihood problem. However, the importance of the min-max entropy view is that it (i) dictates the structure of the neural networks and (ii) allows for debugging. Ad (i) the neural networks must have a linear output layer with a learnable weight (this corresponds to the respective lagrange multiplier), otherwise it would not always be possible to guarantee that the NMLN will faithfully model the statistics (potential functions) encoded by the neural network. Ad (ii) We know that the expected value of the potential function encoded by the neural network should be (approximately) the same as the value of the potentials computed on the training instance. This could be used in future to aid the tuning of hyperparameters and for debugging.\n\n> Overall this is an interesting work. I think it is a natural generalization of Markov Logic \n> Networks and works on two small problems. \n\nThanks, one of the reasons we are excited about this framework is exactly because it is a natural extension of MLNs.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper589/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper589/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Markov Logic Networks", "authors": ["Giuseppe Marra", "Ond\u0159ej Ku\u017eelka"], "authorids": ["g.marra@unifi.it", "kuzelo1@gmail.com"], "keywords": ["Statistical Relational Learning", "Markov Logic Networks"], "TL;DR": " We introduce a statistical relational learning system that borrows ideas from Markov logic but learns an implicit representation of rules as a neural network.", "abstract": "We introduce Neural Markov Logic Networks (NMLNs), a statistical relational learning system that borrows ideas from Markov logic. Like Markov Logic Networks (MLNs), NMLNs are an exponential-family model for modelling distributions over possible worlds, but unlike MLNs, they do not rely on explicitly specified first-order logic rules. Instead, NMLNs learn an implicit representation of such rules as a neural network that acts as a potential function on fragments of the relational structure. Interestingly, any MLN can be represented as an NMLN. Similarly to recently proposed Neural theorem provers (NTPs) (Rocktaschel at al. 2017), NMLNs can exploit embeddings of constants but, unlike NTPs, NMLNs work well also in their absence. This is extremely important for predicting in settings other than the transductive one. We showcase the potential of NMLNs on knowledge-base completion tasks and on generation of molecular (graph) data.", "pdf": "/pdf/acc10593e90c41f40a61b1c64456c342e773fd62.pdf", "paperhash": "marra|neural_markov_logic_networks", "original_pdf": "/attachment/acc10593e90c41f40a61b1c64456c342e773fd62.pdf", "_bibtex": "@misc{\nmarra2020neural,\ntitle={Neural Markov Logic Networks},\nauthor={Giuseppe Marra and Ond{\\v{r}}ej Ku{\\v{z}}elka},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeGvaEtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkeGvaEtPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper589/Authors", "ICLR.cc/2020/Conference/Paper589/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper589/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper589/Reviewers", "ICLR.cc/2020/Conference/Paper589/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper589/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper589/Authors|ICLR.cc/2020/Conference/Paper589/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169193, "tmdate": 1576860534200, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper589/Authors", "ICLR.cc/2020/Conference/Paper589/Reviewers", "ICLR.cc/2020/Conference/Paper589/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper589/-/Official_Comment"}}}, {"id": "rkeBlLXljr", "original": null, "number": 2, "cdate": 1573037549268, "ddate": null, "tcdate": 1573037549268, "tmdate": 1573037549268, "tddate": null, "forum": "SkeGvaEtPr", "replyto": "BylW-TCTtS", "invitation": "ICLR.cc/2020/Conference/Paper589/-/Official_Comment", "content": {"title": "Author response", "comment": "Thank you for the review!\n\n> As defined by the paper, fragments are connected subsets derived from relational data. \n\nWe would like to clarify that fragments are not necessarily connected.\n\n> The authors derived sets of fragments with constants defined by values in the data and \n> anonymized fragment sets with integer assignments. With potential functions sampled \n> from the anonymized and the true value fragments. \n\nWe would like to clarify that we do not sample potential functions. We suspect this might be just a typo in the review but we wanted to clarify this to avoid confusion.\n\n> It is not exactly clear to me why the anonymization of fragments is necessary, but the \n> authors suggest this places a greater focus on the graph structure and minimizes the \n> model acting differently with different constants. \n\nIf the potential function can use information about constants then it cannot be used in the inductive setting where the constants in the test data are not the same as constants in the training data. However, it makes sense to use information about constants for the transductive setting (which we do using embeddings that we learn automatically in the NMLN in the transductive setting of knowledge graph completion).\n\n\n> At the same time, however, given the emerging contributions in the area, such as \n> relational neural networks (Poole et al.), although the technical development is sensible, \n> I don\u2019t find the contribution that technically novel, and seems somewhat straightforward. \n> Given the preliminary investigations, this is a reject from my side. \n\nWe respectfully disagree on this point.\n\nIndeed, we are aware of the works that use neural networks for relational data, such as RelNN (Kazemi and Poole, AAAI 18) or lifted relational neural networks (Sourek et al, JAIR 2018) and some newer ones as well. While similar from the high level perspective - they all use neural networks on relational data, there is a crucial difference here: to the best of our knowledge, none of these frameworks is capable of learning a joint distributions on relational structures. That is none of these methods combining neural networks and relational learning could be used in the molecule generation experiment. In particular, RelNN is \u201conly\u201d a neural-extension of relational logistic regression. We model the joint probability distributions of the relational structures which is a much more difficult task (also from the computational-complexity point of view).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper589/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper589/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Markov Logic Networks", "authors": ["Giuseppe Marra", "Ond\u0159ej Ku\u017eelka"], "authorids": ["g.marra@unifi.it", "kuzelo1@gmail.com"], "keywords": ["Statistical Relational Learning", "Markov Logic Networks"], "TL;DR": " We introduce a statistical relational learning system that borrows ideas from Markov logic but learns an implicit representation of rules as a neural network.", "abstract": "We introduce Neural Markov Logic Networks (NMLNs), a statistical relational learning system that borrows ideas from Markov logic. Like Markov Logic Networks (MLNs), NMLNs are an exponential-family model for modelling distributions over possible worlds, but unlike MLNs, they do not rely on explicitly specified first-order logic rules. Instead, NMLNs learn an implicit representation of such rules as a neural network that acts as a potential function on fragments of the relational structure. Interestingly, any MLN can be represented as an NMLN. Similarly to recently proposed Neural theorem provers (NTPs) (Rocktaschel at al. 2017), NMLNs can exploit embeddings of constants but, unlike NTPs, NMLNs work well also in their absence. This is extremely important for predicting in settings other than the transductive one. We showcase the potential of NMLNs on knowledge-base completion tasks and on generation of molecular (graph) data.", "pdf": "/pdf/acc10593e90c41f40a61b1c64456c342e773fd62.pdf", "paperhash": "marra|neural_markov_logic_networks", "original_pdf": "/attachment/acc10593e90c41f40a61b1c64456c342e773fd62.pdf", "_bibtex": "@misc{\nmarra2020neural,\ntitle={Neural Markov Logic Networks},\nauthor={Giuseppe Marra and Ond{\\v{r}}ej Ku{\\v{z}}elka},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeGvaEtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkeGvaEtPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper589/Authors", "ICLR.cc/2020/Conference/Paper589/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper589/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper589/Reviewers", "ICLR.cc/2020/Conference/Paper589/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper589/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper589/Authors|ICLR.cc/2020/Conference/Paper589/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169193, "tmdate": 1576860534200, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper589/Authors", "ICLR.cc/2020/Conference/Paper589/Reviewers", "ICLR.cc/2020/Conference/Paper589/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper589/-/Official_Comment"}}}, {"id": "BylW-TCTtS", "original": null, "number": 1, "cdate": 1571839225346, "ddate": null, "tcdate": 1571839225346, "tmdate": 1572972576533, "tddate": null, "forum": "SkeGvaEtPr", "replyto": "SkeGvaEtPr", "invitation": "ICLR.cc/2020/Conference/Paper589/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The following paper provides an extension to Markov Logic Networks(MLNs), by removing their dependency on pre-defined first-order logic rules.  This is handled via neural networks which are able to capture the statistical relations, so-called Neural Markov Logic Networks(NMLNs). As this is an implicit representation from the neural network, the rules act as potential functions on the MLN structure. As general MLN techniques are reliant on domain experts or exhaustive structure learning approaches, NMLNs are able to model more domains as provided in the work with knowledge-base completion tasks and generative modelling of molecules. \n\nThe primary contribution in this body of work is based on the observation that relational structure repeats regularities in the data, and where deriving the statistics of these regularities is what allows for improved accuracy in a model.  The proposed NMLN  is architecturally identical to MLNs with the difference being the addition of the potential function.\n\nAs defined by the paper, fragments are connected subsets derived from relational data. The authors derived sets of fragments with constants defined by values in the data and anonymized fragment sets with integer assignments. With potential functions sampled from the anonymized and the true value fragments. The objective is a search for a maximum-entropy distribution to model the data derived fragments. The neural network aspect comes in the form of the minimum-maximum entropy modelling with weights for given fragments being learned by minimising the entropy of the fragment potential function. Where the model also maximizes the log-likelihood related to the anonymized fragments. The intuition in this work is that by selecting the maximum entropy distribution while also minimizing it by selecting the most informative statistical information for it, we will derive an accurate probability distribution given the possible worlds.\nOverall, the paper performs a well enough job explaining the technical aspects. It does a thorough job explaining the algorithmic detail in the main body, and the appendix provides clear and implementable pseudocode equations. It is not exactly clear to me why the anonymization of fragments is necessary, but the authors suggest this places a greater focus on the graph structure and minimizes the model acting differently with different constants. The min-max entropy modelling also appears to be a novel approach in terms of statistical relational modelling. The results also demonstrate the success of NMLNs modelling on relational data and KB completion.\n\nRegarding the technical aspects, a few concerns are the claims that the domains for experimentation seem rather trivial since the smoking and nations dataset are common relational datasets, and the strength of this model is the ability to learn on other domains. This is possibly addressed with the molecule experiments, but more datasets would have helped in confirming the breadth of domains as claimed by the paper. It is also difficult to measure the success of their model with generative modelling as no baseline was present for the molecule experiments. I still view this paper as a positive contribution to MLN research, with a technique that is successful among the few experiments tested.\n\nAt the same time, however, given the emerging contributions in the area, such as relational neural networks (Poole et al.), although the technical development is sensible, I don\u2019t find the contribution that technically novel, and seems somewhat straightforward. Given the preliminary investigations, this is a reject from my side. "}, "signatures": ["ICLR.cc/2020/Conference/Paper589/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper589/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Markov Logic Networks", "authors": ["Giuseppe Marra", "Ond\u0159ej Ku\u017eelka"], "authorids": ["g.marra@unifi.it", "kuzelo1@gmail.com"], "keywords": ["Statistical Relational Learning", "Markov Logic Networks"], "TL;DR": " We introduce a statistical relational learning system that borrows ideas from Markov logic but learns an implicit representation of rules as a neural network.", "abstract": "We introduce Neural Markov Logic Networks (NMLNs), a statistical relational learning system that borrows ideas from Markov logic. Like Markov Logic Networks (MLNs), NMLNs are an exponential-family model for modelling distributions over possible worlds, but unlike MLNs, they do not rely on explicitly specified first-order logic rules. Instead, NMLNs learn an implicit representation of such rules as a neural network that acts as a potential function on fragments of the relational structure. Interestingly, any MLN can be represented as an NMLN. Similarly to recently proposed Neural theorem provers (NTPs) (Rocktaschel at al. 2017), NMLNs can exploit embeddings of constants but, unlike NTPs, NMLNs work well also in their absence. This is extremely important for predicting in settings other than the transductive one. We showcase the potential of NMLNs on knowledge-base completion tasks and on generation of molecular (graph) data.", "pdf": "/pdf/acc10593e90c41f40a61b1c64456c342e773fd62.pdf", "paperhash": "marra|neural_markov_logic_networks", "original_pdf": "/attachment/acc10593e90c41f40a61b1c64456c342e773fd62.pdf", "_bibtex": "@misc{\nmarra2020neural,\ntitle={Neural Markov Logic Networks},\nauthor={Giuseppe Marra and Ond{\\v{r}}ej Ku{\\v{z}}elka},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeGvaEtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkeGvaEtPr", "replyto": "SkeGvaEtPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper589/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper589/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575652133913, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper589/Reviewers"], "noninvitees": [], "tcdate": 1570237749959, "tmdate": 1575652133927, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper589/-/Official_Review"}}}, {"id": "rkxIiJNo5r", "original": null, "number": 3, "cdate": 1572712349756, "ddate": null, "tcdate": 1572712349756, "tmdate": 1572972576448, "tddate": null, "forum": "SkeGvaEtPr", "replyto": "SkeGvaEtPr", "invitation": "ICLR.cc/2020/Conference/Paper589/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "I think their strong point is at the same time their weak point. \", they do not rely on explicitly specified first-order logic rules.\" My question would be, why not use that information if available as prior knowledge? Perhaps I'd like to see a stronger motivation for the use of having to learn this part. I can see how its might be useful but would be very happy to see more motivation for this. I think I've seen Kotler learn the rules of sudoku but sudoku is such a specified problem that Im not convinced yet this is usueful to learn. Thats a different paper but I missed the motivation for that here too.\n\nI like the honesty of the authors for saying it doesn't scale to larger problems. Regardless, I think this paper is good to push the field in that direction. I particularly like the graph generation task. Graph generation, afaik, is not easy. \n\nWhat I invite the authors to do is to not be restricted by theoretically/principled motivated ways. I believe its better to find things that work well first and then to find a theory (the other way round). This is not enough to reject the paper for me because I do believe this is pushing the field forward in a good direction. If possible I'd suggest to relax the theory and then compare the two models if possible.\n\nIn the contribution it says \"(i) we introduce a new statistical relational model, which\novercomes actual limitations of both classical and recent related models such as \" I would have really liked it to have been spelt out which limitations, very specifically and concisely the paper overcomes in that section.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper589/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper589/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Markov Logic Networks", "authors": ["Giuseppe Marra", "Ond\u0159ej Ku\u017eelka"], "authorids": ["g.marra@unifi.it", "kuzelo1@gmail.com"], "keywords": ["Statistical Relational Learning", "Markov Logic Networks"], "TL;DR": " We introduce a statistical relational learning system that borrows ideas from Markov logic but learns an implicit representation of rules as a neural network.", "abstract": "We introduce Neural Markov Logic Networks (NMLNs), a statistical relational learning system that borrows ideas from Markov logic. Like Markov Logic Networks (MLNs), NMLNs are an exponential-family model for modelling distributions over possible worlds, but unlike MLNs, they do not rely on explicitly specified first-order logic rules. Instead, NMLNs learn an implicit representation of such rules as a neural network that acts as a potential function on fragments of the relational structure. Interestingly, any MLN can be represented as an NMLN. Similarly to recently proposed Neural theorem provers (NTPs) (Rocktaschel at al. 2017), NMLNs can exploit embeddings of constants but, unlike NTPs, NMLNs work well also in their absence. This is extremely important for predicting in settings other than the transductive one. We showcase the potential of NMLNs on knowledge-base completion tasks and on generation of molecular (graph) data.", "pdf": "/pdf/acc10593e90c41f40a61b1c64456c342e773fd62.pdf", "paperhash": "marra|neural_markov_logic_networks", "original_pdf": "/attachment/acc10593e90c41f40a61b1c64456c342e773fd62.pdf", "_bibtex": "@misc{\nmarra2020neural,\ntitle={Neural Markov Logic Networks},\nauthor={Giuseppe Marra and Ond{\\v{r}}ej Ku{\\v{z}}elka},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeGvaEtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkeGvaEtPr", "replyto": "SkeGvaEtPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper589/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper589/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575652133913, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper589/Reviewers"], "noninvitees": [], "tcdate": 1570237749959, "tmdate": 1575652133927, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper589/-/Official_Review"}}}], "count": 8}