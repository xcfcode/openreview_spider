{"notes": [{"id": "bnY0jm4l59", "original": "beDoc9hE4dc", "number": 2408, "cdate": 1601308265751, "ddate": null, "tcdate": 1601308265751, "tmdate": 1615979353335, "tddate": null, "forum": "bnY0jm4l59", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Memory Optimization for Deep Networks", "authorids": ["~Aashaka_Shah1", "~Chao-Yuan_Wu1", "jaya@cs.utexas.edu", "~Vijay_Chidambaram1", "~Philipp_Kraehenbuehl1"], "authors": ["Aashaka Shah", "Chao-Yuan Wu", "Jayashree Mohan", "Vijay Chidambaram", "Philipp Kraehenbuehl"], "keywords": ["memory optimized training", "memory efficient training", "checkpointing", "deep network training"], "abstract": "Deep learning is slowly, but steadily, hitting a memory bottleneck. While the tensor computation in top-of-the-line GPUs increased by $32\\times$ over the last five years, the total available memory only grew by $2.5\\times$. This prevents researchers from exploring larger architectures, as training large networks requires more memory for storing intermediate outputs. In this paper, we present MONeT, an automatic framework that minimizes both the memory footprint and computational overhead of deep networks. MONeT jointly optimizes the checkpointing schedule and the implementation of various operators. MONeT is able to outperform all prior hand-tuned operations as well as automated checkpointing. MONeT reduces the overall memory requirement by $3\\times$ for various PyTorch models, with a 9-16$\\%$ overhead in computation. For the same computation cost, MONeT requires 1.2-1.8$\\times$ less memory than current state-of-the-art automated checkpointing frameworks. Our code will be made publicly available upon acceptance.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|memory_optimization_for_deep_networks", "one-sentence_summary": "MONeT reduces the memory footprint of training while minimizing compute overhead by jointly optimizing checkpointing with operator optimizations.", "pdf": "/pdf/c229005882f26be8587687dbfc967c17b75d1bac.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshah2021memory,\ntitle={Memory Optimization for Deep Networks},\nauthor={Aashaka Shah and Chao-Yuan Wu and Jayashree Mohan and Vijay Chidambaram and Philipp Kraehenbuehl},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=bnY0jm4l59}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "LNrhZtJf4It", "original": null, "number": 1, "cdate": 1610040359642, "ddate": null, "tcdate": 1610040359642, "tmdate": 1610473949720, "tddate": null, "forum": "bnY0jm4l59", "replyto": "bnY0jm4l59", "invitation": "ICLR.cc/2021/Conference/Paper2408/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Spotlight)", "comment": "The reviewers all agree that Monet proposed in the paper which optimizes for both local and global memory saving in Deep learning models is theoretically sound and experimentally convincing.\nAccept!"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Memory Optimization for Deep Networks", "authorids": ["~Aashaka_Shah1", "~Chao-Yuan_Wu1", "jaya@cs.utexas.edu", "~Vijay_Chidambaram1", "~Philipp_Kraehenbuehl1"], "authors": ["Aashaka Shah", "Chao-Yuan Wu", "Jayashree Mohan", "Vijay Chidambaram", "Philipp Kraehenbuehl"], "keywords": ["memory optimized training", "memory efficient training", "checkpointing", "deep network training"], "abstract": "Deep learning is slowly, but steadily, hitting a memory bottleneck. While the tensor computation in top-of-the-line GPUs increased by $32\\times$ over the last five years, the total available memory only grew by $2.5\\times$. This prevents researchers from exploring larger architectures, as training large networks requires more memory for storing intermediate outputs. In this paper, we present MONeT, an automatic framework that minimizes both the memory footprint and computational overhead of deep networks. MONeT jointly optimizes the checkpointing schedule and the implementation of various operators. MONeT is able to outperform all prior hand-tuned operations as well as automated checkpointing. MONeT reduces the overall memory requirement by $3\\times$ for various PyTorch models, with a 9-16$\\%$ overhead in computation. For the same computation cost, MONeT requires 1.2-1.8$\\times$ less memory than current state-of-the-art automated checkpointing frameworks. Our code will be made publicly available upon acceptance.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|memory_optimization_for_deep_networks", "one-sentence_summary": "MONeT reduces the memory footprint of training while minimizing compute overhead by jointly optimizing checkpointing with operator optimizations.", "pdf": "/pdf/c229005882f26be8587687dbfc967c17b75d1bac.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshah2021memory,\ntitle={Memory Optimization for Deep Networks},\nauthor={Aashaka Shah and Chao-Yuan Wu and Jayashree Mohan and Vijay Chidambaram and Philipp Kraehenbuehl},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=bnY0jm4l59}\n}"}, "tags": [], "invitation": {"reply": {"forum": "bnY0jm4l59", "replyto": "bnY0jm4l59", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040359628, "tmdate": 1610473949702, "id": "ICLR.cc/2021/Conference/Paper2408/-/Decision"}}}, {"id": "wcvP6mRf6f1", "original": null, "number": 6, "cdate": 1606302268834, "ddate": null, "tcdate": 1606302268834, "tmdate": 1606302268834, "tddate": null, "forum": "bnY0jm4l59", "replyto": "zgoGYctB5q5", "invitation": "ICLR.cc/2021/Conference/Paper2408/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We thank the reviewer for the insightful questions and analysis.\nWe provide our responses below:\n\n### Solution times and scalability\n\n**[Solution times for the ILP formulation]**\nWe evaluate schedules obtained using solution times set to a maximum of 24 hours.\nWe have added Table 3 and Table 4 in Appendix H with detailed discussion of times to obtain close-to-optimal solutions for multiple models with multiple memory budgets.\nFor all the 5 architectures and 30 memory limits that we tried, MONeT reached a 5% close-to-optimal solution within a few hours or sometimes minutes.\nWe additionally include a comparison to Checkmate in the table; for larger models, MONeT's solver converged to a 5% close-to-optimal solution up to 27x faster, and to a 2% close-to-optimal solution up to 16x faster than Checkmate.\n\n---\n\n**[Why is Checkmate's solver time slower than MONeT?]**\nMONeT and Checkmate differ in their translation of the computation graph into constraints.\nMONeT-NoOp, which is MONeT with only checkpointing enabled and without using operator optimization, has on average 50% fewer constraints and 67% fewer variables than Checkmate.\nJointly-optimized MONeT has a slightly larger number of constraints and on average 40% fewer variables than Checkmate.\nMONeT\u2019s formulation is more efficient, and might be the reason that it reaches a good solution faster than Checkmate.\n\n---\n\n**[Scalability of the system for larger problems & ILP Statistics]**\nWe have added data about solver times and ILP statistics for Checkmate, MONeT-NoOp (joint operator-optimization disabled), and MONeT in Appendix H. Three of the models we investigate are relatively large, containing more than 150 operators.\n        MONeT converges to a 5% close-to-optimal solution for the problems within a few hours.\n\nWe would also like to point out that the 24-hours of solver time is a one-time effort - once a MONeT schedule has been solved, it can be used by all users for different purposes and can be extended to multiple batch sizes.\nThis cost is usually tiny compared with the costs or time required to develop a model for distribution.\n\n\n### Questions about the formulation\n\n**[Gradient computations of op k only depends on ops > k (Alg 2)]**\nThis is a good point.\n    We have added a small discussion about it in Appendix B where we talk about tightening the upper bound on local memory.\n    In practice, we notice that while this reduces the ILP problem size, the solver time remains similar as before.\n\n---\n\n\n**[How to get computational efficiency of ops? If fusion happens in compiler, then sum of individual ops may not realistically model the execution behavior.]**\nWe obtain the computation graph of the model by JIT tracing the model and run the graph through optimizations like constant propagation and dead code elimination.\n    The graph nodes obtained are low-level pytorch primitives (aten::relu_, aten::_convolution, etc), for which we implement custom forward and backward functions leveraging the PyTorch ATen library functions.\n    We get the computational efficiency by profiling these ops.\n\nPyTorch by default does not fuse the low-level primitives for training. We too do not benchmark or execute with operator fusion currently in MONeT, but it is a great idea for future work.\n\n---\n\n\n**[The formulation does not take into account compiler optimizations such as fusion, tiling etc.]**\nThat\u2019s correct.\n    We currently do not profile runtimes with compiler optimizations such as fusion and tiling enabled, or use them in our actual execution.\n    As mentioned above, we think it is a very good idea to consider compiler optimizations with MONeT.\n    \n\n### Questions about experiments\n**[What implementations of ops are used in the PyTorch implementation? Are these default ops or post-optimization similar to checkmate?]**\nWe use PyTorch\u2019s default ops.\n    We enable cuDNN benchmarking for PyTorch, which, much like the greedy post-optimization of convolutions in Checkmate, internally allows PyTorch to select the fastest convolution algorithm.\n\n---\n\n\n**[What if memory ratio = 1 and compare it with the PyTorch implementation?]**\nOn setting memory ratio = 1 and comparing it with the PyTorch implementation, we see a -3% to 7% overhead for MONeT.\n    Runtimes for ResNet-50 and MobileNet-V2 are very similar to PyTorch (0.5% and -0.3% overhead of MONeT over PyTorch respectively).\n    GoogleNet and UNet are slightly slower than PyTorch (by 7% and 4.3% respectively) owing to the presence of many MaxPooling layers, for which our implementation is slightly slower.\n    We observe that MONeT's VGG-16 selects faster convolution algorithms than PyTorch in a couple of cases, resulting in a slight net performance gain (overhead of -3%).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2408/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2408/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Memory Optimization for Deep Networks", "authorids": ["~Aashaka_Shah1", "~Chao-Yuan_Wu1", "jaya@cs.utexas.edu", "~Vijay_Chidambaram1", "~Philipp_Kraehenbuehl1"], "authors": ["Aashaka Shah", "Chao-Yuan Wu", "Jayashree Mohan", "Vijay Chidambaram", "Philipp Kraehenbuehl"], "keywords": ["memory optimized training", "memory efficient training", "checkpointing", "deep network training"], "abstract": "Deep learning is slowly, but steadily, hitting a memory bottleneck. While the tensor computation in top-of-the-line GPUs increased by $32\\times$ over the last five years, the total available memory only grew by $2.5\\times$. This prevents researchers from exploring larger architectures, as training large networks requires more memory for storing intermediate outputs. In this paper, we present MONeT, an automatic framework that minimizes both the memory footprint and computational overhead of deep networks. MONeT jointly optimizes the checkpointing schedule and the implementation of various operators. MONeT is able to outperform all prior hand-tuned operations as well as automated checkpointing. MONeT reduces the overall memory requirement by $3\\times$ for various PyTorch models, with a 9-16$\\%$ overhead in computation. For the same computation cost, MONeT requires 1.2-1.8$\\times$ less memory than current state-of-the-art automated checkpointing frameworks. Our code will be made publicly available upon acceptance.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|memory_optimization_for_deep_networks", "one-sentence_summary": "MONeT reduces the memory footprint of training while minimizing compute overhead by jointly optimizing checkpointing with operator optimizations.", "pdf": "/pdf/c229005882f26be8587687dbfc967c17b75d1bac.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshah2021memory,\ntitle={Memory Optimization for Deep Networks},\nauthor={Aashaka Shah and Chao-Yuan Wu and Jayashree Mohan and Vijay Chidambaram and Philipp Kraehenbuehl},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=bnY0jm4l59}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "bnY0jm4l59", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2408/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2408/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2408/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2408/Authors|ICLR.cc/2021/Conference/Paper2408/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2408/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848729, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2408/-/Official_Comment"}}}, {"id": "D67vDjJpfy", "original": null, "number": 5, "cdate": 1606301912262, "ddate": null, "tcdate": 1606301912262, "tmdate": 1606301912262, "tddate": null, "forum": "bnY0jm4l59", "replyto": "T6n8h4FcX1f", "invitation": "ICLR.cc/2021/Conference/Paper2408/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thank the reviewer for the detailed comments and insightful questions.\nWe provide our responses below:\n\n**[Exponential number of graphs for Checkmate]**\n>\"In Checkmate, changes in operator implementation induce a different computation graph\" <- While this is technically true, only the cost associated with that operator changes, yes? In this way, Checkmate could run multiple passes over the computed static graph with different operator costs, but this approach would require an expoential (in the number of operators with varying costs) number of evaluations.]\n\nYes, in Checkmate, changes in operator implementation will change the cost associated with the operator. It may also change computational dependencies, and thus decisions about whether another operator's memory should be freed or not.\nCheckmate would require an exponential number of graphs to address these requirements.\n\nThanks for bringing up this point. We also added a discussion on operator selection for Checkmate in Appendix F.\n\n### Implementation Details \n**[PyTorch version]**\nWe use PyTorch v1.5.1 for both MONeT and Checkmate.\n---\n**[Checkmate implementation]**\nWe use the Checkmate solver as-is to obtain Checkmate schedules.\nSince Checkmate does not provide an execution engine for PyTorch, we run the generated Checkmate schedules on our own execution framework.\nOur inference engine uses the same operator implementations for Checkmate and MONeT.\n\nWe have added these details to Section 5 and Appendix D in the paper.\n\n---\n**[Release Checkmate source with MONeT code?]**\nYes.\n---\n**[Solver runtime of MONeT vs. checkmate]**\nWe have added Table 3, 4 in Appendix H to provide solver times to obtain close-to-optimal solutions for MONeT and Checkmate on multiple models and memory budgets.\nWe observed that for larger models, MONeT's solver converged to a 5% close-to-optimal solution up to 27x faster, and to a 2% close-to-optimal solution up to 16x faster than Checkmate.\n\nNote that running a solver is a one-time cost for a model - once a MONeT schedule has been solved, it can be used by everyone to train the model for different purposes with different batch sizes.\nThe cost (typically seconds to hours) is tiny compared to the efforts and costs to develop a model for distribution in most cases.\n---\n**[Runtime of open-source solvers vs. commercial Gurobi solver]**\nWe observe that the Gurobi solver was much faster compared to open-source solvers (eg. Cbc).\nCompared to the Cbc solver, Gurobi was 244x-259x faster in solving VGG-16 and MobileNet-V2 schedules with memory ratio 0.67.\nWe think this might be due to better heuristics and pre-solving in Gurobi.\nGurobi's license is available for free for academic use, making it more accessible than any other commercial solver behind a paywall.\nNonetheless, we implement the MONeT solver using the CVXPY modeling language, which can seamlessly switch between Gurobi, Cbc, and other solvers if needed.\n\n### Answer to Clarification Questions\n**[Paragraph \"We measure the empirical performance ... difference of 1-2%.\"; is this to say 24 hours were included in execution time?]**\nWe have reworded the paragraph (5th paragraph of Section 5) in the paper to make it clearer.\nThe up to 24-hour solver time is not included in the execution time.\n---\n**[Does PyTorch exceed device memory for VGG-16? Does this cause negative overhead?]**\nPyTorch does not exceed the device-memory for the VGG-16 execution in the ablation study.\nMONeT achieves a slightly faster execution time because it stores fewer tensors, saving space for choosing faster but more workspace-memory-hungry convolution algorithms.\nThis effect is particularly apparent for VGG-16, which uses very large tensors in the earlier layers and its convolutional layers have extreme memory-speed tradeoffs.\nFor example, a convolutions algorithm may require a large workspace memory of 2.3GB-4.5GB, while being significantly faster (by 43ms-53ms, or 3-3.8% of PyTorch training time) compared to other algorithms.\n\nThe runtime gains due to convolution selection dominate over the runtime loss due to recomputations and intermediate-activated operator optimization, finally resulting in a net runtime improvement.\n\n### Comments Regarding Notation\nWe appreciate the reviewer's insightful feedback. \nWe have incorporated the suggestions in the paper.\nThe change includes an extended notation table in Appendix A that additionally describes variable y, the sets L, D, and S among other variables.\nThe variable r (the reviewer's understanding is correct) is also mentioned in the table.\nWe also refer readers to this table early in our paper for ease of reading.\n\n**[Standardized notation by the three categories]**\nGreat idea! We added explanations regarding our notation convention along with our notation table in Appendix A.\nIn particular, notations with only i in subscript/superscript relate to the forward pass, with only k relate to the backward pass, and with both i and k relate to the recomputation phase."}, "signatures": ["ICLR.cc/2021/Conference/Paper2408/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2408/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Memory Optimization for Deep Networks", "authorids": ["~Aashaka_Shah1", "~Chao-Yuan_Wu1", "jaya@cs.utexas.edu", "~Vijay_Chidambaram1", "~Philipp_Kraehenbuehl1"], "authors": ["Aashaka Shah", "Chao-Yuan Wu", "Jayashree Mohan", "Vijay Chidambaram", "Philipp Kraehenbuehl"], "keywords": ["memory optimized training", "memory efficient training", "checkpointing", "deep network training"], "abstract": "Deep learning is slowly, but steadily, hitting a memory bottleneck. While the tensor computation in top-of-the-line GPUs increased by $32\\times$ over the last five years, the total available memory only grew by $2.5\\times$. This prevents researchers from exploring larger architectures, as training large networks requires more memory for storing intermediate outputs. In this paper, we present MONeT, an automatic framework that minimizes both the memory footprint and computational overhead of deep networks. MONeT jointly optimizes the checkpointing schedule and the implementation of various operators. MONeT is able to outperform all prior hand-tuned operations as well as automated checkpointing. MONeT reduces the overall memory requirement by $3\\times$ for various PyTorch models, with a 9-16$\\%$ overhead in computation. For the same computation cost, MONeT requires 1.2-1.8$\\times$ less memory than current state-of-the-art automated checkpointing frameworks. Our code will be made publicly available upon acceptance.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|memory_optimization_for_deep_networks", "one-sentence_summary": "MONeT reduces the memory footprint of training while minimizing compute overhead by jointly optimizing checkpointing with operator optimizations.", "pdf": "/pdf/c229005882f26be8587687dbfc967c17b75d1bac.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshah2021memory,\ntitle={Memory Optimization for Deep Networks},\nauthor={Aashaka Shah and Chao-Yuan Wu and Jayashree Mohan and Vijay Chidambaram and Philipp Kraehenbuehl},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=bnY0jm4l59}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "bnY0jm4l59", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2408/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2408/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2408/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2408/Authors|ICLR.cc/2021/Conference/Paper2408/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2408/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848729, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2408/-/Official_Comment"}}}, {"id": "e6d-Sb-ykF", "original": null, "number": 4, "cdate": 1606300972020, "ddate": null, "tcdate": 1606300972020, "tmdate": 1606300972020, "tddate": null, "forum": "bnY0jm4l59", "replyto": "lehSWIkOa4d", "invitation": "ICLR.cc/2021/Conference/Paper2408/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "We thank the reviewer for the review and appreciate their encouragement towards this line of work.\n\nWe appreciate the great suggestion regarding 3D-UNet; we have added the results to Appendix J.\nAs a quick summary of the results: using MONeT, we could run a 3D-UNet model using 0.54x times the memory and with 8.86% compute overhead over PyTorch. We provide a detailed runtime-memory tradeoff curve for MONeT below (from Appendix J). At a 0.81 memory ratio, we incur almost no computational overhead, because MONeT, making use of operator optimizations, is able to bring down the recomputation cost to zero.\n\n(We used the 3D-UNet model present at https://github.com/wolny/pytorch-3dunet with a batch size of 22, which just fits on a 16 GB P100 GPU. Other training configuration is similar to \"3DUnet\\_confocal\\_boundary\" provided in the repository.)\n\n\n| Memory ratio | Runtime overhead |\n|-------------:|-----------------:|\n|         0.54 |            8.86% |\n|         0.58 |            5.14% |\n|         0.65 |            1.81% |\n|         0.72 |            0.78% |\n|         0.81 |            0.20% |\n|              |                  |"}, "signatures": ["ICLR.cc/2021/Conference/Paper2408/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2408/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Memory Optimization for Deep Networks", "authorids": ["~Aashaka_Shah1", "~Chao-Yuan_Wu1", "jaya@cs.utexas.edu", "~Vijay_Chidambaram1", "~Philipp_Kraehenbuehl1"], "authors": ["Aashaka Shah", "Chao-Yuan Wu", "Jayashree Mohan", "Vijay Chidambaram", "Philipp Kraehenbuehl"], "keywords": ["memory optimized training", "memory efficient training", "checkpointing", "deep network training"], "abstract": "Deep learning is slowly, but steadily, hitting a memory bottleneck. While the tensor computation in top-of-the-line GPUs increased by $32\\times$ over the last five years, the total available memory only grew by $2.5\\times$. This prevents researchers from exploring larger architectures, as training large networks requires more memory for storing intermediate outputs. In this paper, we present MONeT, an automatic framework that minimizes both the memory footprint and computational overhead of deep networks. MONeT jointly optimizes the checkpointing schedule and the implementation of various operators. MONeT is able to outperform all prior hand-tuned operations as well as automated checkpointing. MONeT reduces the overall memory requirement by $3\\times$ for various PyTorch models, with a 9-16$\\%$ overhead in computation. For the same computation cost, MONeT requires 1.2-1.8$\\times$ less memory than current state-of-the-art automated checkpointing frameworks. Our code will be made publicly available upon acceptance.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|memory_optimization_for_deep_networks", "one-sentence_summary": "MONeT reduces the memory footprint of training while minimizing compute overhead by jointly optimizing checkpointing with operator optimizations.", "pdf": "/pdf/c229005882f26be8587687dbfc967c17b75d1bac.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshah2021memory,\ntitle={Memory Optimization for Deep Networks},\nauthor={Aashaka Shah and Chao-Yuan Wu and Jayashree Mohan and Vijay Chidambaram and Philipp Kraehenbuehl},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=bnY0jm4l59}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "bnY0jm4l59", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2408/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2408/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2408/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2408/Authors|ICLR.cc/2021/Conference/Paper2408/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2408/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848729, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2408/-/Official_Comment"}}}, {"id": "Q-We0kSL7MK", "original": null, "number": 3, "cdate": 1606300821409, "ddate": null, "tcdate": 1606300821409, "tmdate": 1606300821409, "tddate": null, "forum": "bnY0jm4l59", "replyto": "wOb533kplA4", "invitation": "ICLR.cc/2021/Conference/Paper2408/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank the reviewer for the comments and feedback.\nWe provide our responses below:\n\n**[Comparison with schemes apart from Checkmate]**\nWe have added Appendix K which provides a comparison between MONeT and an operator-based memory-efficient scheme Gist [1].\nGist uses various techniques to encode stashed forward pass tensors into smaller tensors which require less memory.\nWe find that MONeT uses 1.4x-2.1x less memory than Gist for multiple architectures, while incurring lower compute overheads.\n\nWe evaluate Gist on PyTorch using MONeT\u2019s execution framework, since an official implementation of Gist is not available.\nWe get similar memory saving results for reimplemented-Gist as in the original paper for VGG-16, but see a higher computational overhead in our implementation of Gist.\nThis could be because of evaluations on different frameworks (PyTorch v/s CNTK) and different GPU models (Nvidia P100 v/s Nvidia Maxwell GTX Titan X).\nTo ensure a fair comparison, we just compare the maximum memory savings obtained by MONeT against our Gist implementation, and report the compute overhead for completeness.\n\nAppendix K provides further details of the implementation and comparison with Gist.\n\nWe are also very happy to take suggestions for other baselines to compare against.\n\n[1] Animesh Jain, Amar Phanishayee, Jason Mars, Lingjia Tang, and Gennady Pekhimenko. Gist: Efficient data encoding for deep neural network training. In ISCA, 2018.\n\n---\n\n**[Optimizations on other architectures]**\nGreat comment!\nWe have added additional results on a memory-intensive model, 3D-UNet, in Appendix J.\nWe observe a consistent memory reduction to 0.54x of PyTorch memory with an overhead of 8.86% in the high-memory regime.\nTogether with other results we present, we've observed consistent and good memory-overhead trade-offs on 6 models, with a wide range of memory budgets.\nWe are working on experimenting with even more models, including Transformers."}, "signatures": ["ICLR.cc/2021/Conference/Paper2408/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2408/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Memory Optimization for Deep Networks", "authorids": ["~Aashaka_Shah1", "~Chao-Yuan_Wu1", "jaya@cs.utexas.edu", "~Vijay_Chidambaram1", "~Philipp_Kraehenbuehl1"], "authors": ["Aashaka Shah", "Chao-Yuan Wu", "Jayashree Mohan", "Vijay Chidambaram", "Philipp Kraehenbuehl"], "keywords": ["memory optimized training", "memory efficient training", "checkpointing", "deep network training"], "abstract": "Deep learning is slowly, but steadily, hitting a memory bottleneck. While the tensor computation in top-of-the-line GPUs increased by $32\\times$ over the last five years, the total available memory only grew by $2.5\\times$. This prevents researchers from exploring larger architectures, as training large networks requires more memory for storing intermediate outputs. In this paper, we present MONeT, an automatic framework that minimizes both the memory footprint and computational overhead of deep networks. MONeT jointly optimizes the checkpointing schedule and the implementation of various operators. MONeT is able to outperform all prior hand-tuned operations as well as automated checkpointing. MONeT reduces the overall memory requirement by $3\\times$ for various PyTorch models, with a 9-16$\\%$ overhead in computation. For the same computation cost, MONeT requires 1.2-1.8$\\times$ less memory than current state-of-the-art automated checkpointing frameworks. Our code will be made publicly available upon acceptance.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|memory_optimization_for_deep_networks", "one-sentence_summary": "MONeT reduces the memory footprint of training while minimizing compute overhead by jointly optimizing checkpointing with operator optimizations.", "pdf": "/pdf/c229005882f26be8587687dbfc967c17b75d1bac.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshah2021memory,\ntitle={Memory Optimization for Deep Networks},\nauthor={Aashaka Shah and Chao-Yuan Wu and Jayashree Mohan and Vijay Chidambaram and Philipp Kraehenbuehl},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=bnY0jm4l59}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "bnY0jm4l59", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2408/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2408/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2408/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2408/Authors|ICLR.cc/2021/Conference/Paper2408/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2408/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848729, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2408/-/Official_Comment"}}}, {"id": "aiT1CG_txr-", "original": null, "number": 2, "cdate": 1606300712216, "ddate": null, "tcdate": 1606300712216, "tmdate": 1606300712216, "tddate": null, "forum": "bnY0jm4l59", "replyto": "bnY0jm4l59", "invitation": "ICLR.cc/2021/Conference/Paper2408/-/Official_Comment", "content": {"title": "Thanks for the reviews and summary of main changes", "comment": "We thank all reviewers for their time and valuable feedback.\nWe are glad that all reviewers recommend acceptance and find the results impressive and convincing (R1, R2, R4), the description clear and well-written (R1, R4), and like the integer-programming formulation (R1, R3).\n\nAccording to the comments, we have updated the paper with the following changes:\n\n- Clarified a paragraph talking about solver times in the paper (5th paragraph of Section 5)\n\n- Added more notations regarding the formulation in Appendix A\n\n- Added implementation details about Checkmate in Appendix D\n\n- Added Appendix H which provides a discussion about solver times and ILP statistics\n\n- Added Appendix J which provides the runtime-memory tradeoff results for 3D-UNet using MONeT\n\n- Added Appendix K which provides a comparison of MONeT with a data encoding scheme, Gist\n\nWe answer each reviewer's questions in an individual response for each review below."}, "signatures": ["ICLR.cc/2021/Conference/Paper2408/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2408/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Memory Optimization for Deep Networks", "authorids": ["~Aashaka_Shah1", "~Chao-Yuan_Wu1", "jaya@cs.utexas.edu", "~Vijay_Chidambaram1", "~Philipp_Kraehenbuehl1"], "authors": ["Aashaka Shah", "Chao-Yuan Wu", "Jayashree Mohan", "Vijay Chidambaram", "Philipp Kraehenbuehl"], "keywords": ["memory optimized training", "memory efficient training", "checkpointing", "deep network training"], "abstract": "Deep learning is slowly, but steadily, hitting a memory bottleneck. While the tensor computation in top-of-the-line GPUs increased by $32\\times$ over the last five years, the total available memory only grew by $2.5\\times$. This prevents researchers from exploring larger architectures, as training large networks requires more memory for storing intermediate outputs. In this paper, we present MONeT, an automatic framework that minimizes both the memory footprint and computational overhead of deep networks. MONeT jointly optimizes the checkpointing schedule and the implementation of various operators. MONeT is able to outperform all prior hand-tuned operations as well as automated checkpointing. MONeT reduces the overall memory requirement by $3\\times$ for various PyTorch models, with a 9-16$\\%$ overhead in computation. For the same computation cost, MONeT requires 1.2-1.8$\\times$ less memory than current state-of-the-art automated checkpointing frameworks. Our code will be made publicly available upon acceptance.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|memory_optimization_for_deep_networks", "one-sentence_summary": "MONeT reduces the memory footprint of training while minimizing compute overhead by jointly optimizing checkpointing with operator optimizations.", "pdf": "/pdf/c229005882f26be8587687dbfc967c17b75d1bac.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshah2021memory,\ntitle={Memory Optimization for Deep Networks},\nauthor={Aashaka Shah and Chao-Yuan Wu and Jayashree Mohan and Vijay Chidambaram and Philipp Kraehenbuehl},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=bnY0jm4l59}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "bnY0jm4l59", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2408/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2408/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2408/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2408/Authors|ICLR.cc/2021/Conference/Paper2408/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2408/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848729, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2408/-/Official_Comment"}}}, {"id": "zgoGYctB5q5", "original": null, "number": 1, "cdate": 1603900064268, "ddate": null, "tcdate": 1603900064268, "tmdate": 1605024218331, "tddate": null, "forum": "bnY0jm4l59", "replyto": "bnY0jm4l59", "invitation": "ICLR.cc/2021/Conference/Paper2408/-/Official_Review", "content": {"title": "Good paper; please answer the questions posed ", "review": "## Summary:\n\nThe paper provides a framework (MoNET) to perform automatic memory optimization targeting deep neural networks. Their technique jointly optimizes the checkpointing schedule and the choice of individual ops to find the implementation with the least possible runtime overhead under certain memory constraints. They formulate the problem as a ILP problem where the objective is to minimize runtime subject to strict memory constraints. They show that the solutions achieve 3x reduction in memory footprint compared to a pytorch implementation with minimal runtime overhead.\n\n## Strengths:\n* Making joint decisions on checkpointing and on the implementation of individual ops. This enables the authors to make both global and local decisions and to exploit the synergies in both.\n* Experiments on multiple NN models to show the impact of their technique.\n* Easy to read formulation of the ILP problem.\n\n## Weaknesses and Questions for authors:\n* Solution times and scalability\nThe authors do not explicitly mention the solution times for their ILP formulation. Also, it is unclear to me how the \u2018checkmate\u2019 framework can be slower compared to MoNET (as authors claim in page 7), since checkmate is only solving for the optimal checkpointing schedule.  I am also curious to know the scalability of the system for larger problems, specially since the authors are doing a joint optimization. Statistics about ILP problem sizes would be helpful.\n* Questions about the formulation\n  * Why do you need to compute $r_i^k$ for $i > k$? The gradient computations of op $k$ only depends on ops with numbers > $k$ (Algorithm 2)\n  * How do you get the computational efficiency of ops? If fusion of ops happens inside the compiler, then adding up the computational efficiency of individual ops may not realistically model the execution behavior.\n  * The formulation does not take into account possible compiler optimizations such as fusion, tiling etc. for both memory consumption and runtime calculations.\n* Questions about experiments\n  * It\u2019s unclear what implementations of ops are used in the PyTorch implementation? Are these default ops or did you do a post-optimization similar to what you did for checkmate?\n  * What if you set memory ratio = 1 and compare it with the pytorch implementation, what is the overhead?\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2408/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2408/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Memory Optimization for Deep Networks", "authorids": ["~Aashaka_Shah1", "~Chao-Yuan_Wu1", "jaya@cs.utexas.edu", "~Vijay_Chidambaram1", "~Philipp_Kraehenbuehl1"], "authors": ["Aashaka Shah", "Chao-Yuan Wu", "Jayashree Mohan", "Vijay Chidambaram", "Philipp Kraehenbuehl"], "keywords": ["memory optimized training", "memory efficient training", "checkpointing", "deep network training"], "abstract": "Deep learning is slowly, but steadily, hitting a memory bottleneck. While the tensor computation in top-of-the-line GPUs increased by $32\\times$ over the last five years, the total available memory only grew by $2.5\\times$. This prevents researchers from exploring larger architectures, as training large networks requires more memory for storing intermediate outputs. In this paper, we present MONeT, an automatic framework that minimizes both the memory footprint and computational overhead of deep networks. MONeT jointly optimizes the checkpointing schedule and the implementation of various operators. MONeT is able to outperform all prior hand-tuned operations as well as automated checkpointing. MONeT reduces the overall memory requirement by $3\\times$ for various PyTorch models, with a 9-16$\\%$ overhead in computation. For the same computation cost, MONeT requires 1.2-1.8$\\times$ less memory than current state-of-the-art automated checkpointing frameworks. Our code will be made publicly available upon acceptance.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|memory_optimization_for_deep_networks", "one-sentence_summary": "MONeT reduces the memory footprint of training while minimizing compute overhead by jointly optimizing checkpointing with operator optimizations.", "pdf": "/pdf/c229005882f26be8587687dbfc967c17b75d1bac.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshah2021memory,\ntitle={Memory Optimization for Deep Networks},\nauthor={Aashaka Shah and Chao-Yuan Wu and Jayashree Mohan and Vijay Chidambaram and Philipp Kraehenbuehl},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=bnY0jm4l59}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "bnY0jm4l59", "replyto": "bnY0jm4l59", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2408/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097041, "tmdate": 1606915761102, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2408/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2408/-/Official_Review"}}}, {"id": "T6n8h4FcX1f", "original": null, "number": 2, "cdate": 1603919696761, "ddate": null, "tcdate": 1603919696761, "tmdate": 1605024218265, "tddate": null, "forum": "bnY0jm4l59", "replyto": "bnY0jm4l59", "invitation": "ICLR.cc/2021/Conference/Paper2408/-/Official_Review", "content": {"title": "Intuitive approach and framework to push the state-of-the-art in memory-constrained deep learning", "review": "The authors present MONeT, an automatic approach to jointly optimize operator cost and checkpoint scheduling for deep learning on a fixed memory budget.  The paper thoroughly defines the problem, relevant previous work, and the MONeT framework.  Given a fixed GPU memory budget, MONeT solves an integer program in order to jointly minimize the computational overhead of checkpointing with various operator implementation.  This approach is intuitive, as previous approaches, such as the recently proposed CheckMate, only optimize the checkpoint schedule.  The derived integer program is also a nontrivial extension of previous work.  With MONeT implemented in PyTorch, a large number of empirical results are presented, which show the superiority of MONeT compared to CheckMate, and show memory savings (versus impressively slight overhead) compared to PyTorch.\n\nThe paper is well written, the description of computational cost and the derivation of the integer program were interesting, and results are very compelling (and easy to understand).  One area where the paper could be improved is on the notation used throughout Section 4 (see below for suggestions), which was difficult to follow due to the density of the section, the large number of different variables/variants of variables described, and some implicit definitions.  There are also a few small details and discussions which seem warranted, but, overall, I enjoyed reading this paper.\n\nSpecific comments:\n\n-\"n Checkmate, changes in operator implementation induce a different computation graph\" <- While this is\ntechnically true, only the cost associated with that operator changes,\nyes?  In this way, Checkmate could run multiple passes over the\ncomputed static graph with different operator costs, but this approach\nwould require an expoential (in the number of operators with varying\ncosts) number of evaluations.\n\n-\"We reimplement Checkmate in PyTorch\" <-This is non-trivial, so please include some re-implementation details.\n\n-Please include the version of PyTorch which was forked for the Monet  and Checkmate implementations in Section 5.\n\n-As CheckMate currently only supports tensorflow, it would be very helpful if the authors could also release the source for CheckMate in PyTorch when the source for MONeT is released.\n\n-Could you comment on the solver runtime needed to solve the integer\nprogram in monet, in contrast to the solver runtime for the MILP in\ncheckmate?\n\n-How do open-source solvers compare to the runtime for the commerical\nGurobi solver for the integer programs solved in monet?\n\n-\"We measure the empirical performance of the checkpointed schedules\nrunning on GPUs instead of just providing the solver values; this is\nimportant since Checkmate doesn\u2019t consider workspace cost and\noverestimates its savings... Hence,\nwe show the results with solver running for 1 day for both MON E T and\nCheckmate. In contrast, MON E T finds the execution plans efficiently,\nits 1-hr solution already close to the 1-day solution with a small\ndifference of 1-2%.\"  <- This paragraph of worded somewhat vaguely; is\nthis to say 24 hours were included in execution time?\n\n-For VGG-16 in the ablation study in Figure 4, PyTorch exceeds\ndevice-memory for this dataset, yes? Is this the reason why monet\nachieves negative overhead; i.e., faster execution time than PyTorch\nitself?\n\nComments/questions regarding notation:\n\n-What is variable r in the schedule (s,r)?  It *seems* like r is the\n indicator function for activations which require recomputation.  Is\n that correct?  If so, please (please) state this explicitly in the\n paper.  If not, please define r.\n\n-A supplementary table for variables would be helpful.  It took some\n time to find a definition for $y$ in Equation 2.  While I eventually\n found one in Algorithm 2 (please define this explicitly inline,\n preceding Equation 2), a table would have made this much easier,\n especially considering how dense Section 3 is.\n\n-By the time the reader gets to Section 4, the table of variables\n becomes mandatory (there are different values of L and S with various\n superscripts, subscripts, and hats, it is very difficult to recall\n which is which and to look back in the dense text for their\n definitions).\n\n-Also, if possible, please standardize notation by the three\n categories: \n(a) peak\n forward pass memory consumption\n(b) peak backward pass memory consumption\n(c) peak recomputation memory consumption\nso that a reader can ascertain what collection Ls, Ds, and Ss are\nbeing referred to.  I understand there is overlap between these three\ncategories, but there must be some organizational way to more\neasily refer to these variables without having to research for their\ndefinitions when reading later sections of the paper.\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2408/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2408/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Memory Optimization for Deep Networks", "authorids": ["~Aashaka_Shah1", "~Chao-Yuan_Wu1", "jaya@cs.utexas.edu", "~Vijay_Chidambaram1", "~Philipp_Kraehenbuehl1"], "authors": ["Aashaka Shah", "Chao-Yuan Wu", "Jayashree Mohan", "Vijay Chidambaram", "Philipp Kraehenbuehl"], "keywords": ["memory optimized training", "memory efficient training", "checkpointing", "deep network training"], "abstract": "Deep learning is slowly, but steadily, hitting a memory bottleneck. While the tensor computation in top-of-the-line GPUs increased by $32\\times$ over the last five years, the total available memory only grew by $2.5\\times$. This prevents researchers from exploring larger architectures, as training large networks requires more memory for storing intermediate outputs. In this paper, we present MONeT, an automatic framework that minimizes both the memory footprint and computational overhead of deep networks. MONeT jointly optimizes the checkpointing schedule and the implementation of various operators. MONeT is able to outperform all prior hand-tuned operations as well as automated checkpointing. MONeT reduces the overall memory requirement by $3\\times$ for various PyTorch models, with a 9-16$\\%$ overhead in computation. For the same computation cost, MONeT requires 1.2-1.8$\\times$ less memory than current state-of-the-art automated checkpointing frameworks. Our code will be made publicly available upon acceptance.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|memory_optimization_for_deep_networks", "one-sentence_summary": "MONeT reduces the memory footprint of training while minimizing compute overhead by jointly optimizing checkpointing with operator optimizations.", "pdf": "/pdf/c229005882f26be8587687dbfc967c17b75d1bac.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshah2021memory,\ntitle={Memory Optimization for Deep Networks},\nauthor={Aashaka Shah and Chao-Yuan Wu and Jayashree Mohan and Vijay Chidambaram and Philipp Kraehenbuehl},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=bnY0jm4l59}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "bnY0jm4l59", "replyto": "bnY0jm4l59", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2408/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097041, "tmdate": 1606915761102, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2408/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2408/-/Official_Review"}}}, {"id": "lehSWIkOa4d", "original": null, "number": 3, "cdate": 1603925872304, "ddate": null, "tcdate": 1603925872304, "tmdate": 1605024218194, "tddate": null, "forum": "bnY0jm4l59", "replyto": "bnY0jm4l59", "invitation": "ICLR.cc/2021/Conference/Paper2408/-/Official_Review", "content": {"title": "Valuable contribution for deep learning training", "review": "Training deep learning models is becoming increasingly challenging due to a memory bottleneck that limits the size of the feature maps that can be stored. The paper presents an automatic framework (MONET) that minimizes the memory footprint for deep networks. The novelty of MONET is that it jointly optimizes over: (a) global compute graph level techniques (such as checkpointing) and (b) local techniques (such as memory-efficient implementations of individual operators). While there are several existing works that focus separately on optimizing global techniques (e.g. the work on \u201cCheckmate\u201d) or local techniques, MONET is the first to jointly optimize over global and local techniques.\n\nThe memory constraints are carefully analyzed for the forward and backward passes, and expressed as a 0-1 integer program, which is then solved using the state-of-the-art solver Gurobi. The experimental evaluation confirms the theoretical gains provided by the solution of the optimization problem. In particular, the authors compare with a vanilla implementation in PyTorch, with the Checkmate-D (default Checkmate that uses global techniques), and with Checkmate-O (post-optimized to greedily run the fastest convolution algorithm). It is interesting to notice that MONET offers significant gains in all cases, even over Checkmate-O, underlying the need to perform the joint optimization in order to obtain the best schedule.\n\nI advocate for the acceptance of the paper. The memory bottleneck is an acute problem in deep learning, and the paper provides a practical solution that can alleviate this problem to some extent. I encourage this line of work and hope to see such schedule optimization become a standard option in deep learning frameworks. The paper is very well written, with a clear description of the approach and convincing experimental evaluation, while providing abundant references about related work.\n\nMinor comments, questions for the authors:\nThe memory requirements for some of the networks shown in the experimental evaluation are still small. While I understand that the GPU memory is limited to 16GB and there was a desire to compare against vanilla implementations, I think it would be interesting to show how models that require a lot of memory for training can benefit from MONET. For example, the UNet that was used in the paper seems to be the 2D version. The 3D UNet can require up to several hundred GB without optimizations (for varying batch sizes). Would it be possible to include even a simple estimate of the possible gains when using MONET? What would be the computational tradeoff to train it on the GPU used in the paper?\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2408/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2408/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Memory Optimization for Deep Networks", "authorids": ["~Aashaka_Shah1", "~Chao-Yuan_Wu1", "jaya@cs.utexas.edu", "~Vijay_Chidambaram1", "~Philipp_Kraehenbuehl1"], "authors": ["Aashaka Shah", "Chao-Yuan Wu", "Jayashree Mohan", "Vijay Chidambaram", "Philipp Kraehenbuehl"], "keywords": ["memory optimized training", "memory efficient training", "checkpointing", "deep network training"], "abstract": "Deep learning is slowly, but steadily, hitting a memory bottleneck. While the tensor computation in top-of-the-line GPUs increased by $32\\times$ over the last five years, the total available memory only grew by $2.5\\times$. This prevents researchers from exploring larger architectures, as training large networks requires more memory for storing intermediate outputs. In this paper, we present MONeT, an automatic framework that minimizes both the memory footprint and computational overhead of deep networks. MONeT jointly optimizes the checkpointing schedule and the implementation of various operators. MONeT is able to outperform all prior hand-tuned operations as well as automated checkpointing. MONeT reduces the overall memory requirement by $3\\times$ for various PyTorch models, with a 9-16$\\%$ overhead in computation. For the same computation cost, MONeT requires 1.2-1.8$\\times$ less memory than current state-of-the-art automated checkpointing frameworks. Our code will be made publicly available upon acceptance.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|memory_optimization_for_deep_networks", "one-sentence_summary": "MONeT reduces the memory footprint of training while minimizing compute overhead by jointly optimizing checkpointing with operator optimizations.", "pdf": "/pdf/c229005882f26be8587687dbfc967c17b75d1bac.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshah2021memory,\ntitle={Memory Optimization for Deep Networks},\nauthor={Aashaka Shah and Chao-Yuan Wu and Jayashree Mohan and Vijay Chidambaram and Philipp Kraehenbuehl},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=bnY0jm4l59}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "bnY0jm4l59", "replyto": "bnY0jm4l59", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2408/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097041, "tmdate": 1606915761102, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2408/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2408/-/Official_Review"}}}, {"id": "wOb533kplA4", "original": null, "number": 4, "cdate": 1604008759886, "ddate": null, "tcdate": 1604008759886, "tmdate": 1605024218128, "tddate": null, "forum": "bnY0jm4l59", "replyto": "bnY0jm4l59", "invitation": "ICLR.cc/2021/Conference/Paper2408/-/Official_Review", "content": {"title": "The paper proposes MONeT to jointly optimizing both local operators and the global compute graph. The authors express this joint optimization problem as a integer program. Results are impressive with 3x memory reduction at a small overhead or 1.2-1.8x reduction at the same computation.", "review": "Optimizing Neural Network Memory is broadly done through two channels - 1)  local operator level - like storing only signs of a ReLU function or bit quantization \n2) global graph level - like optimizing checkpointing schedule for a given compute graph. These channels are usually orthogonal/independent. \n\nIn this paper, the authors propose MONet which tries to find the best checkpointing schedule that can jointly optimize both the above channels. \nThe authors create an auxiliary graph to encapsulate operators and perform schedule optimization on the new graph rather than the usual graph in existing frameworks.\n\nTo obtain the new graph, they first perform a theoretical quantification of the peak memory consumption of the forward pass,backward pass and recomputation.\nThen, under a fixed memory budget M, we try to optimize the operators for computational efficiency.\n\nEach operator has multiple implementations that trade-off workspace memory 'c' and compute efficiency \\tau and we can exactly use only one implementation per operator. \nSo we end up with a linear combination of compute costs for all operations that looks like \n\n\\sum_i\\sum_l(\\tau_i^l \\delta_il) where \\delta is an indicator of which implementation to choose.\n\nWith 3 such double summations, the authors come up with a join objective function which constraints the overall memory sum to <=M \nand minimizes the compute cost objective above (which can be solved by a linear program).\n\nOn several popular architectures like ResNet-50, VGG-16 and UNet etc, MONeT outperforms Checkmate by abt 1.3-2x on memory and full-blown PyTorch by abt 2-3x.\n\nPros:\n1. Thoretically optimized\n2. Experiments on multiple CNN architectures\n\nCons:\n\n1. Effetcively no comparisons against any other schemes except Checkmate.\n2. No other architectures optimizations like Transformers or large scale FFNs.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2408/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2408/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Memory Optimization for Deep Networks", "authorids": ["~Aashaka_Shah1", "~Chao-Yuan_Wu1", "jaya@cs.utexas.edu", "~Vijay_Chidambaram1", "~Philipp_Kraehenbuehl1"], "authors": ["Aashaka Shah", "Chao-Yuan Wu", "Jayashree Mohan", "Vijay Chidambaram", "Philipp Kraehenbuehl"], "keywords": ["memory optimized training", "memory efficient training", "checkpointing", "deep network training"], "abstract": "Deep learning is slowly, but steadily, hitting a memory bottleneck. While the tensor computation in top-of-the-line GPUs increased by $32\\times$ over the last five years, the total available memory only grew by $2.5\\times$. This prevents researchers from exploring larger architectures, as training large networks requires more memory for storing intermediate outputs. In this paper, we present MONeT, an automatic framework that minimizes both the memory footprint and computational overhead of deep networks. MONeT jointly optimizes the checkpointing schedule and the implementation of various operators. MONeT is able to outperform all prior hand-tuned operations as well as automated checkpointing. MONeT reduces the overall memory requirement by $3\\times$ for various PyTorch models, with a 9-16$\\%$ overhead in computation. For the same computation cost, MONeT requires 1.2-1.8$\\times$ less memory than current state-of-the-art automated checkpointing frameworks. Our code will be made publicly available upon acceptance.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|memory_optimization_for_deep_networks", "one-sentence_summary": "MONeT reduces the memory footprint of training while minimizing compute overhead by jointly optimizing checkpointing with operator optimizations.", "pdf": "/pdf/c229005882f26be8587687dbfc967c17b75d1bac.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshah2021memory,\ntitle={Memory Optimization for Deep Networks},\nauthor={Aashaka Shah and Chao-Yuan Wu and Jayashree Mohan and Vijay Chidambaram and Philipp Kraehenbuehl},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=bnY0jm4l59}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "bnY0jm4l59", "replyto": "bnY0jm4l59", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2408/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097041, "tmdate": 1606915761102, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2408/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2408/-/Official_Review"}}}], "count": 11}