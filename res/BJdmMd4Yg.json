{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028588700, "tcdate": 1490028588700, "number": 1, "id": "ByBNOYase", "invitation": "ICLR.cc/2017/workshop/-/paper83/acceptance", "forum": "BJdmMd4Yg", "replyto": "BJdmMd4Yg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Who Said What: Modeling individual labelers improves classification", "abstract": "Data are often labeled by many different experts, with each expert labeling a small fraction of the data and each sample receiving multiple labels. When experts disagree, the standard approaches are to treat the majority opinion as the truth or to model the truth as a distribution, but these do not make any use of potentially valuable information about which expert produced which label. We propose modeling the experts individually and then learning averaging weights for combining them, possibly in sample-specific ways. This allows us to give more weight to more reliable experts and take advantage of the unique strengths of individual experts at classifying certain types of data. We show that our approach performs better than three competing methods in computer-aided diagnosis of diabetic retinopathy.", "pdf": "/pdf/c3310f113caf4d1e9a69ad921a1e3b3986a1bb90.pdf", "paperhash": "guan|who_said_what_modeling_individual_labelers_improves_classification", "conflicts": ["google.com"], "keywords": ["Computer vision", "Deep learning", "Supervised Learning"], "authors": ["Melody Y. Guan", "Varun Gulshan", "Andrew M. Dai", "Geoffrey E. Hinton"], "authorids": ["melodyguan@google.com", "varungulshan@google.com", "adai@google.com", "geoffhinton@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028589248, "id": "ICLR.cc/2017/workshop/-/paper83/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BJdmMd4Yg", "replyto": "BJdmMd4Yg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028589248}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489865225131, "tcdate": 1489865090557, "number": 3, "id": "HkotYbjil", "invitation": "ICLR.cc/2017/workshop/-/paper83/public/comment", "forum": "BJdmMd4Yg", "replyto": "S17AN1Vil", "signatures": ["~Melody_Yun_Jia_Guan1"], "readers": ["everyone"], "writers": ["~Melody_Yun_Jia_Guan1"], "content": {"title": "Response to AnonReviewer2", "comment": "Thank you very much for your time and review! We have tried our best to to incorporate your feedback and clarify things for you.\n\nWe do think it would be a good idea to add references; we have revised our paper to discuss prior literature in crowdsourcing which deals with the same problem space tackled by our paper. Refer to the the \"Estimating doctor reliability with EM\" paragraph of section 3.\n\nWe would also like to clarify that our work is distinct from the usual \"mixture of experts\" papers (MoE concept introduced by [1,2]). **Usual \"mixture of experts\" models are not about modeling individual experts, but rather training latent experts on the same data.** (In more detail: These latent experts are trained using a training set where each data point has a single label and there is also no information on the origin of the label. Our paper concerns datasets labelled by multiple *observed* experts where each data point has multiple overlapping labels from a subset of the experts. In this context we are combining experts in a way not explored before, learning from the identity of individual experts by modeling them (with each modeled expert trained on a restricted subset of the data), learning their specialties, and learning how to combine them.)\n\nThe Nature paper referenced is probably \"A solution to the single-question crowd wisdom problem\" [3]. We find this paper extremely interesting as well, but like the reviewer pointed out, it involves asking extra questions (what each expert thinks the popular opinion would be) and that is not feasible for existing large datasets which have already been labeled by several experts (often with huge expenses). Our goal was to develop a method that could be applied to existing labeled datasets, as is the case with the vast majority of real world datasets.\n \nTo help readers better understand the nets, we rewrote section 3. We also moved the paragraph on binary loss in section 4 to the appendix (Appendix J) in order to provide more space for section 3.  But due to the 3-page limit for workshop papers, there was only so much more we could add, so Appendix D we also added 3 additional paragraphs of detailed explanation of the net with references to parts of Figure 4. We also provided the loss inputs in tabular form (previously this information was only provided in text from). We hope that these changes are helpful, and if the reviewer has any specific points of confusion we would be very happy to address those in further comments!\n\nHopefully our response helps the reviewer better understand the context and content of the paper.  We believe our approach to be novel, simple and useful, and thank the reviewer for their helpful comments.\n\n[1] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. 1991. Adaptive mixtures of local experts. Neural Computing. 3, 1 (February 1991), 79-87\n\n[2] M. I. Jordan and R. A. Jacobs. 1994. Hierarchical mixtures of experts and the EM algorithm. Neural Computing. 6, 2 (March 1994), 181-214\n\n[3] D. Prelec, H. S. Seung, and J. McCoy. 2017. A solution to the single-question crowd wisdom problem. Nature. 541 (January 2017), 532\u2013535"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Who Said What: Modeling individual labelers improves classification", "abstract": "Data are often labeled by many different experts, with each expert labeling a small fraction of the data and each sample receiving multiple labels. When experts disagree, the standard approaches are to treat the majority opinion as the truth or to model the truth as a distribution, but these do not make any use of potentially valuable information about which expert produced which label. We propose modeling the experts individually and then learning averaging weights for combining them, possibly in sample-specific ways. This allows us to give more weight to more reliable experts and take advantage of the unique strengths of individual experts at classifying certain types of data. We show that our approach performs better than three competing methods in computer-aided diagnosis of diabetic retinopathy.", "pdf": "/pdf/c3310f113caf4d1e9a69ad921a1e3b3986a1bb90.pdf", "paperhash": "guan|who_said_what_modeling_individual_labelers_improves_classification", "conflicts": ["google.com"], "keywords": ["Computer vision", "Deep learning", "Supervised Learning"], "authors": ["Melody Y. Guan", "Varun Gulshan", "Andrew M. Dai", "Geoffrey E. Hinton"], "authorids": ["melodyguan@google.com", "varungulshan@google.com", "adai@google.com", "geoffhinton@google.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487335969001, "tcdate": 1487335969001, "id": "ICLR.cc/2017/workshop/-/paper83/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper83/reviewers"], "reply": {"forum": "BJdmMd4Yg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487335969001}}}, {"tddate": null, "replyto": null, "nonreaders": null, "ddate": null, "tmdate": 1489865119693, "tcdate": 1487335968422, "number": 83, "id": "BJdmMd4Yg", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "BJdmMd4Yg", "signatures": ["~Melody_Yun_Jia_Guan1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Who Said What: Modeling individual labelers improves classification", "abstract": "Data are often labeled by many different experts, with each expert labeling a small fraction of the data and each sample receiving multiple labels. When experts disagree, the standard approaches are to treat the majority opinion as the truth or to model the truth as a distribution, but these do not make any use of potentially valuable information about which expert produced which label. We propose modeling the experts individually and then learning averaging weights for combining them, possibly in sample-specific ways. This allows us to give more weight to more reliable experts and take advantage of the unique strengths of individual experts at classifying certain types of data. We show that our approach performs better than three competing methods in computer-aided diagnosis of diabetic retinopathy.", "pdf": "/pdf/c3310f113caf4d1e9a69ad921a1e3b3986a1bb90.pdf", "paperhash": "guan|who_said_what_modeling_individual_labelers_improves_classification", "conflicts": ["google.com"], "keywords": ["Computer vision", "Deep learning", "Supervised Learning"], "authors": ["Melody Y. Guan", "Varun Gulshan", "Andrew M. Dai", "Geoffrey E. Hinton"], "authorids": ["melodyguan@google.com", "varungulshan@google.com", "adai@google.com", "geoffhinton@google.com"]}, "writers": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "tmdate": 1489396939463, "tcdate": 1489396939463, "number": 2, "id": "S17AN1Vil", "invitation": "ICLR.cc/2017/workshop/-/paper83/official/review", "forum": "BJdmMd4Yg", "replyto": "BJdmMd4Yg", "signatures": ["ICLR.cc/2017/workshop/paper83/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper83/AnonReviewer2"], "content": {"title": "Empirical results on an unique dataset using mixing of experts", "rating": "5: Marginally below acceptance threshold", "review": "Despite the interesting results, the paper is a very empirical paper on mixing of experts. Mixing of experts is a very old issue. Authors should cite some references around mixing of experts that would help the reader to understand the problem at hand and to asses the contributions.\n\nHonestly i do not understand all the nets proposed (Figure 4, doesn't help me). In general section 3 should be improved. \n\nAround the idea of mixing of experts i remember a paper that was published in Nature (i think so) where the authors proposed a very interesting idea for mixing experts beyond the typical weight associated to the expert reliability. The authors propose to ask an additional question to the experts about what they think the other experts are going to answer. And use this additional question to detect where an expert is a good expert. For instance, when one expert is sure about his decision but at the same time he knows that the problem is hard he thinks that the other experts (or some group) are going to fail and then he is going to claim that his answer is A but others experts answer is going to be B. \n\nIn my opinion, these are the things that would be interesting to explore, beyond weighting opinion. Perhaps a NN could help to solve this problem without that additional question. Perhaps this paper is in that direction but sorry i couldn't understand it.\n\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Who Said What: Modeling individual labelers improves classification", "abstract": "Data are often labeled by many different experts, with each expert labeling a small fraction of the data and each sample receiving multiple labels. When experts disagree, the standard approaches are to treat the majority opinion as the truth or to model the truth as a distribution, but these do not make any use of potentially valuable information about which expert produced which label. We propose modeling the experts individually and then learning averaging weights for combining them, possibly in sample-specific ways. This allows us to give more weight to more reliable experts and take advantage of the unique strengths of individual experts at classifying certain types of data. We show that our approach performs better than three competing methods in computer-aided diagnosis of diabetic retinopathy.", "pdf": "/pdf/c3310f113caf4d1e9a69ad921a1e3b3986a1bb90.pdf", "paperhash": "guan|who_said_what_modeling_individual_labelers_improves_classification", "conflicts": ["google.com"], "keywords": ["Computer vision", "Deep learning", "Supervised Learning"], "authors": ["Melody Y. Guan", "Varun Gulshan", "Andrew M. Dai", "Geoffrey E. Hinton"], "authorids": ["melodyguan@google.com", "varungulshan@google.com", "adai@google.com", "geoffhinton@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489396940168, "id": "ICLR.cc/2017/workshop/-/paper83/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper83/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper83/AnonReviewer1", "ICLR.cc/2017/workshop/paper83/AnonReviewer2"], "reply": {"forum": "BJdmMd4Yg", "replyto": "BJdmMd4Yg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper83/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper83/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489396940168}}}, {"tddate": null, "tmdate": 1489225603147, "tcdate": 1489225603147, "number": 2, "id": "B1sFvSbie", "invitation": "ICLR.cc/2017/workshop/-/paper83/public/comment", "forum": "BJdmMd4Yg", "replyto": "rkhWNJ-jg", "signatures": ["~Melody_Yun_Jia_Guan1"], "readers": ["everyone"], "writers": ["~Melody_Yun_Jia_Guan1"], "content": {"title": "Response to AnonReviewer1", "comment": "Thanks so much to the reviewer for their time and comments! Responses to the three details you mentioned are below:\n\n1. \n\n\"how could this be detected, since they may have provided a disproportionate number of labels in the test set as well?\"\n\nThey do not provide a disproportionate number of labels in the test set because the doctors used in training/validation are disjoint from the doctors used in the test set.\n- \"3 retina specialists graded all images in the test dataset, and any disagreements were discussed until a consensus label was obtained\" (Appendix C)\n- \"we remove grades of doctors who graded test set images from training and validation sets to reduce the chance that the model is overfitting on certain experts.\" (Appendix E)\nFor additional clarity we updated the paper to include this second point from Appendix in page 2 paragraph 1 as well (see revision).\n\n\"Is there any rebalancing between doctors (as opposed to classes)?\"\n\nWe do not rebalance between doctors. In a sense this is an implementation choice (i.e. it is reasonable to try rebalancing the doctors) but we also felt that it was better to allow doctors who labelled more examples to have more say. This is because we can create better models for doctors with more data, which means that a) all else being equal, their models will have better predictions, and b) their models' reliabilities can be more confidently estimated so if a doctor is bad this will be reflected in its weight. Also note that the baseline of using the average labeler opinion favors more frequent labelers as well so this is not a phenomenon limited to our approach.\n\n2. \n\nPlease note that the test distribution is not assumed to be known (it would indeed be a questionable assumption)! Rather, \"Our assumed test class distribution for computing the log prior correction was the mean distribution of all known images (those of the training and validation sets)\" (page 8, paragraph 2). Also yes, all baselines in comparisons use this adjustment. \n\n3. \n\nWe have updated page 3 paragraph 1 to include the formula for additional clarity (see revision).\n\nThanks again! We hope that this resolves all your concerns!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Who Said What: Modeling individual labelers improves classification", "abstract": "Data are often labeled by many different experts, with each expert labeling a small fraction of the data and each sample receiving multiple labels. When experts disagree, the standard approaches are to treat the majority opinion as the truth or to model the truth as a distribution, but these do not make any use of potentially valuable information about which expert produced which label. We propose modeling the experts individually and then learning averaging weights for combining them, possibly in sample-specific ways. This allows us to give more weight to more reliable experts and take advantage of the unique strengths of individual experts at classifying certain types of data. We show that our approach performs better than three competing methods in computer-aided diagnosis of diabetic retinopathy.", "pdf": "/pdf/c3310f113caf4d1e9a69ad921a1e3b3986a1bb90.pdf", "paperhash": "guan|who_said_what_modeling_individual_labelers_improves_classification", "conflicts": ["google.com"], "keywords": ["Computer vision", "Deep learning", "Supervised Learning"], "authors": ["Melody Y. Guan", "Varun Gulshan", "Andrew M. Dai", "Geoffrey E. Hinton"], "authorids": ["melodyguan@google.com", "varungulshan@google.com", "adai@google.com", "geoffhinton@google.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487335969001, "tcdate": 1487335969001, "id": "ICLR.cc/2017/workshop/-/paper83/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper83/reviewers"], "reply": {"forum": "BJdmMd4Yg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487335969001}}}, {"tddate": null, "tmdate": 1489200131965, "tcdate": 1489200131965, "number": 1, "id": "rkhWNJ-jg", "invitation": "ICLR.cc/2017/workshop/-/paper83/official/review", "forum": "BJdmMd4Yg", "replyto": "BJdmMd4Yg", "signatures": ["ICLR.cc/2017/workshop/paper83/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper83/AnonReviewer1"], "content": {"title": "", "rating": "6: Marginally above acceptance threshold", "review": "This work aims to improve classification accuracy in cases where there is high disagreement among labelers, some of which may be due to systemic differences between labelers.  The general approach is simple and interesting, making separate predictions for each labeler individually, and averaging at test time.  Weights to make this a weighted average are also learned, and two additional conditionings for the model are explored.  A single dataset, to classify diabetic retinopathy, is explored.\n\nOverall I feel this is an interesting approach, though a few details could be better explained and justified, in my opinion:\n\n- If a single doctor does more labeling than any other doctor, the majority vote may tend to favor this labeler (they have more chances to be in the majority).  Would the learned weights then mostly just favor the most frequent doctor, and how could this be detected, since they may have provided a disproportionate number of labels in the test set as well?  Is there any rebalancing between doctors (as opposed to classes)?\n\n- The appendix mentions a step where the biases are adjusted to account for class frequencies in the test set.  IMO this is a slightly questionable step, assuming that the test distribution is known, but this indeed may be the case in many situations.  Also I'm supposing that all baselines in comparisons also used this adjustment -- is this the case?\n\n- I feel the summary of the loss theta_ll' could be a bit clearer:  What is the final loss exactly?\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Who Said What: Modeling individual labelers improves classification", "abstract": "Data are often labeled by many different experts, with each expert labeling a small fraction of the data and each sample receiving multiple labels. When experts disagree, the standard approaches are to treat the majority opinion as the truth or to model the truth as a distribution, but these do not make any use of potentially valuable information about which expert produced which label. We propose modeling the experts individually and then learning averaging weights for combining them, possibly in sample-specific ways. This allows us to give more weight to more reliable experts and take advantage of the unique strengths of individual experts at classifying certain types of data. We show that our approach performs better than three competing methods in computer-aided diagnosis of diabetic retinopathy.", "pdf": "/pdf/c3310f113caf4d1e9a69ad921a1e3b3986a1bb90.pdf", "paperhash": "guan|who_said_what_modeling_individual_labelers_improves_classification", "conflicts": ["google.com"], "keywords": ["Computer vision", "Deep learning", "Supervised Learning"], "authors": ["Melody Y. Guan", "Varun Gulshan", "Andrew M. Dai", "Geoffrey E. Hinton"], "authorids": ["melodyguan@google.com", "varungulshan@google.com", "adai@google.com", "geoffhinton@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489396940168, "id": "ICLR.cc/2017/workshop/-/paper83/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper83/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper83/AnonReviewer1", "ICLR.cc/2017/workshop/paper83/AnonReviewer2"], "reply": {"forum": "BJdmMd4Yg", "replyto": "BJdmMd4Yg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper83/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper83/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489396940168}}}], "count": 6}