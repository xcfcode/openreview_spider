{"notes": [{"id": "lTbOR2kg0Tc", "original": "EbtGJBvnUN", "number": 23, "cdate": 1615310253001, "ddate": null, "tcdate": 1615310253001, "tmdate": 1615313022569, "tddate": null, "forum": "lTbOR2kg0Tc", "replyto": null, "invitation": "ICLR.cc/2021/Workshop/SSL-RL/-/Blind_Submission", "content": {"title": "Variational Model-Based Imitation Learning in High-Dimensional Observation Spaces", "authorids": ["ICLR.cc/2021/Workshop/SSL-RL/Paper23/Authors"], "authors": ["Anonymous"], "keywords": ["imitation learning", "generative models", "vision", "POMDP", "self-supervised reinforcement learning"], "TL;DR": "We train a distribution-matching imitation-learning algorithm using variational models of image-based environments.", "abstract": "We consider the problem setting of imitation learning where the agent is provided a fixed dataset of demonstrations. While the agent can interact with the environment for exploration, it is oblivious to the reward function used by the demonstrator. This setting is representative of many applications in robotics where task demonstrations may be straightforward while reward shaping or conveying stylistic aspects of human motion may be difficult. For this setting, we develop a variational model-based imitation learning algorithm (VMIL) that is capable of learning policies from visual observations. Through experiments, we find that VMIL is more sample efficient compared to prior algorithms in several challenging vision-based locomotion and manipulation tasks, including a high-dimensional in-hand dexterous manipulation task.", "pdf": "/pdf/edeb29f296e900fd737521df6b43cdc28575b40e.pdf", "paperhash": "anonymous|variational_modelbased_imitation_learning_in_highdimensional_observation_spaces", "_bibtex": "@inproceedings{\nanonymous2021variational,\ntitle={Variational Model-Based Imitation Learning in High-Dimensional Observation Spaces},\nauthor={Anonymous},\nbooktitle={Submitted to Self-Supervision for Reinforcement Learning Workshop - ICLR 2021},\nyear={2021},\nurl={https://openreview.net/forum?id=lTbOR2kg0Tc},\nnote={under review}\n}"}, "signatures": ["ICLR.cc/2021/Workshop/SSL-RL"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Workshop/SSL-RL"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Workshop/SSL-RL"]}, "signatures": {"values": ["ICLR.cc/2021/Workshop/SSL-RL"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Workshop/SSL-RL"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Workshop/SSL-RL"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1615310247528, "tmdate": 1615313016556, "id": "ICLR.cc/2021/Workshop/SSL-RL/-/Blind_Submission"}}, "tauthor": "~Super_User1"}], "count": 1}