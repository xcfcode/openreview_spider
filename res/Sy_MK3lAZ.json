{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730179155, "tcdate": 1509112079617, "number": 397, "cdate": 1518730179144, "id": "Sy_MK3lAZ", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "Sy_MK3lAZ", "original": "S1PzFngRW", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "PARAMETRIZED DEEP Q-NETWORKS LEARNING: PLAYING ONLINE BATTLE ARENA WITH DISCRETE-CONTINUOUS HYBRID ACTION SPACE", "abstract": "Most existing deep reinforcement learning (DRL) frameworks consider action spaces that are either\ndiscrete or continuous space. Motivated by the project of design Game AI for King of Glory\n(KOG), one the world\u2019s most popular mobile game, we consider the scenario with the discrete-continuous\nhybrid action space. To directly apply existing DLR frameworks, existing approaches\neither approximate the hybrid space by a discrete set or relaxing it into a continuous set, which is\nusually less efficient and robust. In this paper, we propose a parametrized deep Q-network (P-DQN)\nfor the hybrid action space without approximation or relaxation. Our algorithm combines DQN and\nDDPG and can be viewed as an extension of the DQN to hybrid actions. The empirical study on the\ngame KOG validates the efficiency and effectiveness of our method.", "pdf": "/pdf/b2c8c78c8f14a68ba1145b8537a7d884f273bc77.pdf", "TL;DR": "A DQN and DDPG hybrid algorithm is proposed to deal with the discrete-continuous hybrid action space.", "paperhash": "xiong|parametrized_deep_qnetworks_learning_playing_online_battle_arena_with_discretecontinuous_hybrid_action_space", "_bibtex": "@misc{\nxiong2018parametrized,\ntitle={{PARAMETRIZED} {DEEP} Q-{NETWORKS} {LEARNING}: {PLAYING} {ONLINE} {BATTLE} {ARENA} {WITH} {DISCRETE}-{CONTINUOUS} {HYBRID} {ACTION} {SPACE}},\nauthor={Jiechao Xiong and Qing Wang and Zhuoran Yang and Peng Sun and Yang Zheng and Lei Han and Haobo Fu and Xiangru Lian and Carson Eisenach and Haichuan Yang and Emmanuel Ekwedike and Bei Peng and Haoyue Gao and Tong Zhang and Ji Liu and Han Liu},\nyear={2018},\nurl={https://openreview.net/forum?id=Sy_MK3lAZ},\n}", "keywords": ["Deep reinforcement learning", "Hybrid action space", "DQN", "DDPG"], "authors": ["Jiechao Xiong", "Qing Wang", "Zhuoran Yang", "Peng Sun", "Yang Zheng", "Lei Han", "Haobo Fu", "Xiangru Lian", "Carson Eisenach", "Haichuan Yang", "Emmanuel Ekwedike", "Bei Peng", "Haoyue Gao", "Tong Zhang", "Ji Liu", "Han Liu"], "authorids": ["jcxiong@tencent.com", "drwang@tencent.com", "pythonsun@tencent.com", "zakzheng@tencent.com", "lxhan@tencent.com", "haobofu@tencent.com", "tongzhang@tongzhang-ml.org", "ji.liu.uwisc@gmail.com"]}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260075739, "tcdate": 1517250235383, "number": 881, "cdate": 1517250235369, "id": "Bk76U16SG", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "Sy_MK3lAZ", "replyto": "Sy_MK3lAZ", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "The idea studied here is fairly incremental and the empirical evaluation could be improved."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PARAMETRIZED DEEP Q-NETWORKS LEARNING: PLAYING ONLINE BATTLE ARENA WITH DISCRETE-CONTINUOUS HYBRID ACTION SPACE", "abstract": "Most existing deep reinforcement learning (DRL) frameworks consider action spaces that are either\ndiscrete or continuous space. Motivated by the project of design Game AI for King of Glory\n(KOG), one the world\u2019s most popular mobile game, we consider the scenario with the discrete-continuous\nhybrid action space. To directly apply existing DLR frameworks, existing approaches\neither approximate the hybrid space by a discrete set or relaxing it into a continuous set, which is\nusually less efficient and robust. In this paper, we propose a parametrized deep Q-network (P-DQN)\nfor the hybrid action space without approximation or relaxation. Our algorithm combines DQN and\nDDPG and can be viewed as an extension of the DQN to hybrid actions. The empirical study on the\ngame KOG validates the efficiency and effectiveness of our method.", "pdf": "/pdf/b2c8c78c8f14a68ba1145b8537a7d884f273bc77.pdf", "TL;DR": "A DQN and DDPG hybrid algorithm is proposed to deal with the discrete-continuous hybrid action space.", "paperhash": "xiong|parametrized_deep_qnetworks_learning_playing_online_battle_arena_with_discretecontinuous_hybrid_action_space", "_bibtex": "@misc{\nxiong2018parametrized,\ntitle={{PARAMETRIZED} {DEEP} Q-{NETWORKS} {LEARNING}: {PLAYING} {ONLINE} {BATTLE} {ARENA} {WITH} {DISCRETE}-{CONTINUOUS} {HYBRID} {ACTION} {SPACE}},\nauthor={Jiechao Xiong and Qing Wang and Zhuoran Yang and Peng Sun and Yang Zheng and Lei Han and Haobo Fu and Xiangru Lian and Carson Eisenach and Haichuan Yang and Emmanuel Ekwedike and Bei Peng and Haoyue Gao and Tong Zhang and Ji Liu and Han Liu},\nyear={2018},\nurl={https://openreview.net/forum?id=Sy_MK3lAZ},\n}", "keywords": ["Deep reinforcement learning", "Hybrid action space", "DQN", "DDPG"], "authors": ["Jiechao Xiong", "Qing Wang", "Zhuoran Yang", "Peng Sun", "Yang Zheng", "Lei Han", "Haobo Fu", "Xiangru Lian", "Carson Eisenach", "Haichuan Yang", "Emmanuel Ekwedike", "Bei Peng", "Haoyue Gao", "Tong Zhang", "Ji Liu", "Han Liu"], "authorids": ["jcxiong@tencent.com", "drwang@tencent.com", "pythonsun@tencent.com", "zakzheng@tencent.com", "lxhan@tencent.com", "haobofu@tencent.com", "tongzhang@tongzhang-ml.org", "ji.liu.uwisc@gmail.com"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642443648, "tcdate": 1510265659342, "number": 1, "cdate": 1510265659342, "id": "SkQBXUfJG", "invitation": "ICLR.cc/2018/Conference/-/Paper397/Official_Review", "forum": "Sy_MK3lAZ", "replyto": "Sy_MK3lAZ", "signatures": ["ICLR.cc/2018/Conference/Paper397/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Interesting idea but needs more analysis of results", "rating": "5: Marginally below acceptance threshold", "review": "This paper examines a modified NN architecture and algorithm (P-DQN) for learning in hybrid discrete/continuous action spaces. The authors come up with a clever way of modifying the architecture of parameterized-action-space DDPG (as in Hausknecht & Stone 16) in such a way that the actor only outputs values for the continuous actions and the critic outputs values for all discrete actions, parameterized by the actor\u2019s choice of continuous actions.  Overall, I think this is an interesting and valid modification to the DDPG architecture, with results to show improved sample complexity. However, there is no quantitative analysis of why the new architecture works better, insufficient understanding of the new domain and learning task, and overall rough presentation.\n\nClarity: The writing clarity is rough, but understandable, with numerous minor grammar mistakes. The paper is overly long and could be improved by a more compact presentation of background, algorithms, and results.\n\nOriginality: The paper builds on DDPG and explores a novel modification to the architecture. \n\nSignificance: It\u2019s hard to evaluate the significance of this result because of the lack of videos + information on the Moba environment. The proposed P-DQN architecture is interesting and, if the results on the Moba environment are general, could be of use in future hybrid-discrete-continuous action space domains. \n\nPros:\n\u2022\tThe modification to DDPG is genuinely interesting and does result in an algorithm that is a hybrid between DQN and DDPG.\n\u2022\tThe learning curves show evidence of faster learning using the P-DQN architecture.\n\nCons:\n\u2022\tIt\u2019s difficult to confidently evaluate the merits of P-DQN vs DDPG based only on learning curves from a single, new domain. It would be nice to have explored results on additional domains such as Robot Soccer (HFO), where algorithm could have been compared directly to DDPG.\n\u2022\tThere is very little analysis of why P-DQN exhibits better sample complexity. The authors claim the difference stems from explicit computation over the discrete actions, but this is never analyzed.\n\u2022\tVery difficult to read the axes on the plots in Fig 3.\n\u2022\tNot much detail is given about the domain \u2013 who or what is the agent playing against? Is the agent playing against a bot or just learning to kill creeps? Would be great to have a video of the learned policy (or evaluation against human / scripted opponent) so that others can understand the quality of the learned policy.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PARAMETRIZED DEEP Q-NETWORKS LEARNING: PLAYING ONLINE BATTLE ARENA WITH DISCRETE-CONTINUOUS HYBRID ACTION SPACE", "abstract": "Most existing deep reinforcement learning (DRL) frameworks consider action spaces that are either\ndiscrete or continuous space. Motivated by the project of design Game AI for King of Glory\n(KOG), one the world\u2019s most popular mobile game, we consider the scenario with the discrete-continuous\nhybrid action space. To directly apply existing DLR frameworks, existing approaches\neither approximate the hybrid space by a discrete set or relaxing it into a continuous set, which is\nusually less efficient and robust. In this paper, we propose a parametrized deep Q-network (P-DQN)\nfor the hybrid action space without approximation or relaxation. Our algorithm combines DQN and\nDDPG and can be viewed as an extension of the DQN to hybrid actions. The empirical study on the\ngame KOG validates the efficiency and effectiveness of our method.", "pdf": "/pdf/b2c8c78c8f14a68ba1145b8537a7d884f273bc77.pdf", "TL;DR": "A DQN and DDPG hybrid algorithm is proposed to deal with the discrete-continuous hybrid action space.", "paperhash": "xiong|parametrized_deep_qnetworks_learning_playing_online_battle_arena_with_discretecontinuous_hybrid_action_space", "_bibtex": "@misc{\nxiong2018parametrized,\ntitle={{PARAMETRIZED} {DEEP} Q-{NETWORKS} {LEARNING}: {PLAYING} {ONLINE} {BATTLE} {ARENA} {WITH} {DISCRETE}-{CONTINUOUS} {HYBRID} {ACTION} {SPACE}},\nauthor={Jiechao Xiong and Qing Wang and Zhuoran Yang and Peng Sun and Yang Zheng and Lei Han and Haobo Fu and Xiangru Lian and Carson Eisenach and Haichuan Yang and Emmanuel Ekwedike and Bei Peng and Haoyue Gao and Tong Zhang and Ji Liu and Han Liu},\nyear={2018},\nurl={https://openreview.net/forum?id=Sy_MK3lAZ},\n}", "keywords": ["Deep reinforcement learning", "Hybrid action space", "DQN", "DDPG"], "authors": ["Jiechao Xiong", "Qing Wang", "Zhuoran Yang", "Peng Sun", "Yang Zheng", "Lei Han", "Haobo Fu", "Xiangru Lian", "Carson Eisenach", "Haichuan Yang", "Emmanuel Ekwedike", "Bei Peng", "Haoyue Gao", "Tong Zhang", "Ji Liu", "Han Liu"], "authorids": ["jcxiong@tencent.com", "drwang@tencent.com", "pythonsun@tencent.com", "zakzheng@tencent.com", "lxhan@tencent.com", "haobofu@tencent.com", "tongzhang@tongzhang-ml.org", "ji.liu.uwisc@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642443562, "id": "ICLR.cc/2018/Conference/-/Paper397/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper397/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper397/AnonReviewer2", "ICLR.cc/2018/Conference/Paper397/AnonReviewer3", "ICLR.cc/2018/Conference/Paper397/AnonReviewer1"], "reply": {"forum": "Sy_MK3lAZ", "replyto": "Sy_MK3lAZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper397/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642443562}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642443612, "tcdate": 1511793203406, "number": 2, "cdate": 1511793203406, "id": "r1j4GjKeM", "invitation": "ICLR.cc/2018/Conference/-/Paper397/Official_Review", "forum": "Sy_MK3lAZ", "replyto": "Sy_MK3lAZ", "signatures": ["ICLR.cc/2018/Conference/Paper397/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Review", "rating": "5: Marginally below acceptance threshold", "review": "In this paper, the authors investigate RL agents whose action space contains discrete dimensions, and some continuous dimensions. They approach the problem by tackling the continuous dimensions with DDPG, max-marginalizing out the continuous actions, and tackling the remaining dimensions with classical Q-learning. They apply their method to a MOBA-game, King of Glory.\nMethodologically, the method is a somewhat straightforward combination of DDPG and Q-learning; experimentally, they demonstrate improved performance (2-3x sample efficiency) compared to a modified DDPG algorithm from Hausknecht and Stone. Overall, methodologically, the paper is on the incremental side; experimentally, the authors attack a hard problem, and obtain moderate improvements. The most interesting part of the paper in my mind is the challenging domain of application; maybe trying their algorithm on slightly more difficult settings (different 'heroes', higher AI level) would have made the benefits of their method more evident.\n\nMinor:\n- Paper is significantly over the page limit; in many places, writing could be improved, many typos in paper\n(in the first page: \"project of design\"-> \"project of designing\"; \"farmework\"->\"framework\"; \"we consider the scenario\" \"problems that are once\" are clumsy).\nThe used of 'parameters' for what is effectively a continuous action is a bit confusing; I realize this is borrowed from Hausknecht and Stone, but the use of the term deserves a bit more clarification (they are effectively continuous actions, but in this particular game, they parametrize a particular discrete action).\n- Equation 2.2: Note that the term (r_t+ \\gamma ...) is not differentiated even though it appears in the loss, various paper use different notations to denote this. As it is, the loss is slightly incorrect; same issue with the last equation on page 3.\n- just after equation 2.3, the multiplier of \\grad \\log p_\\theta for REINFORCE is not the reward r_t but the return R_t.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PARAMETRIZED DEEP Q-NETWORKS LEARNING: PLAYING ONLINE BATTLE ARENA WITH DISCRETE-CONTINUOUS HYBRID ACTION SPACE", "abstract": "Most existing deep reinforcement learning (DRL) frameworks consider action spaces that are either\ndiscrete or continuous space. Motivated by the project of design Game AI for King of Glory\n(KOG), one the world\u2019s most popular mobile game, we consider the scenario with the discrete-continuous\nhybrid action space. To directly apply existing DLR frameworks, existing approaches\neither approximate the hybrid space by a discrete set or relaxing it into a continuous set, which is\nusually less efficient and robust. In this paper, we propose a parametrized deep Q-network (P-DQN)\nfor the hybrid action space without approximation or relaxation. Our algorithm combines DQN and\nDDPG and can be viewed as an extension of the DQN to hybrid actions. The empirical study on the\ngame KOG validates the efficiency and effectiveness of our method.", "pdf": "/pdf/b2c8c78c8f14a68ba1145b8537a7d884f273bc77.pdf", "TL;DR": "A DQN and DDPG hybrid algorithm is proposed to deal with the discrete-continuous hybrid action space.", "paperhash": "xiong|parametrized_deep_qnetworks_learning_playing_online_battle_arena_with_discretecontinuous_hybrid_action_space", "_bibtex": "@misc{\nxiong2018parametrized,\ntitle={{PARAMETRIZED} {DEEP} Q-{NETWORKS} {LEARNING}: {PLAYING} {ONLINE} {BATTLE} {ARENA} {WITH} {DISCRETE}-{CONTINUOUS} {HYBRID} {ACTION} {SPACE}},\nauthor={Jiechao Xiong and Qing Wang and Zhuoran Yang and Peng Sun and Yang Zheng and Lei Han and Haobo Fu and Xiangru Lian and Carson Eisenach and Haichuan Yang and Emmanuel Ekwedike and Bei Peng and Haoyue Gao and Tong Zhang and Ji Liu and Han Liu},\nyear={2018},\nurl={https://openreview.net/forum?id=Sy_MK3lAZ},\n}", "keywords": ["Deep reinforcement learning", "Hybrid action space", "DQN", "DDPG"], "authors": ["Jiechao Xiong", "Qing Wang", "Zhuoran Yang", "Peng Sun", "Yang Zheng", "Lei Han", "Haobo Fu", "Xiangru Lian", "Carson Eisenach", "Haichuan Yang", "Emmanuel Ekwedike", "Bei Peng", "Haoyue Gao", "Tong Zhang", "Ji Liu", "Han Liu"], "authorids": ["jcxiong@tencent.com", "drwang@tencent.com", "pythonsun@tencent.com", "zakzheng@tencent.com", "lxhan@tencent.com", "haobofu@tencent.com", "tongzhang@tongzhang-ml.org", "ji.liu.uwisc@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642443562, "id": "ICLR.cc/2018/Conference/-/Paper397/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper397/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper397/AnonReviewer2", "ICLR.cc/2018/Conference/Paper397/AnonReviewer3", "ICLR.cc/2018/Conference/Paper397/AnonReviewer1"], "reply": {"forum": "Sy_MK3lAZ", "replyto": "Sy_MK3lAZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper397/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642443562}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642443577, "tcdate": 1511817680379, "number": 3, "cdate": 1511817680379, "id": "BkO0bWclz", "invitation": "ICLR.cc/2018/Conference/-/Paper397/Official_Review", "forum": "Sy_MK3lAZ", "replyto": "Sy_MK3lAZ", "signatures": ["ICLR.cc/2018/Conference/Paper397/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Interesting paper but lacking in depth", "rating": "4: Ok but not good enough - rejection", "review": "This paper presents a new reinforcement learning approach to handle environments with a mix of discrete and\ncontinuous action spaces. The authors propose a parameterized deep Q-network (P-DQN) and leverage learning\nschemes from existing algorithms such as DQN and DDPG to train the network. The proposed loss function and\nalternating optimization of the parameters are pretty intuitive and easy to follow. My main concern is\nwith lack of sufficient depth in empirical evaluation and analysis of the method.\n\nPros:\n1. The setup is an interesting and practically useful one to investigate. Many real-world environments require individual actions\n that are further parameterized over a continuous space.\n2. The proposed method is simple and intuitive.\n\nCons:\n1. The evaluation is performed only on a single environment in a restricted fashion. I understand the authors are restricted in the choice of environments which require a hybrid action space. However,\n even domains like Atari could be used in a setting where the continuous parameter x_k refers to the number of\n repetitions for action k. This is similar to the work of Lakshminarayanan et al. (2017). Could you test your algorithm in such a setting?\n2. Comparison of the algorithm is performed only against DDPG. Have you tried other options like PPO (Schulman et al., 2017)?\n Also, considering that the action space is simplified in the experimental setup (\"we use the default parameters of skills provided by the game environment, usually pointing to\nthe opponent hero's location\"), with only the move(\\alpha) action being a hybrid, one could imagine discretizing the move\ndirection \\alpha and training a DQN (or any other algorithms over discrete action spaces) as another baseline.\n3. The reward structure seems to be highly engineered. With so many components in the reward, it is not clear\nwhat the individual contributions are and what policies are actually learned.\n4. The authors don't provide any analysis of the empirical results. Do the P-DQN and DDPG converge to the same policy?\nWhat factor(s) contribute most to the faster learning of P-DQN? Do the values of \\alpha and \\beta for the two-timescale\nupdates affect the results considerably?\n5. (minor) The writing contains a lot of grammatical errors which makes this draft below par for an ICLR paper.\n\n\nOther Questions:\n1. In eq. 5.3, the loss over \\theta is defined as the sum of Q values over different k. Did you try other formulations of\nthe loss? (say, product of the Q values for instance) One potential issue with the sum could be that if some values of k dominate this sum, Q(s, k, x_k; w) might not be maximized for all k.\n2. Some terms of the reward function seem to be overly dependent on historic actions (ex. difference in gold and hitpoints). This could swamp the\ninfluence of the other terms which are more dependent on the current action a_t, which might be an issue, especially with the Markovian assumption?\n\nReferences:\n Lakshminarayanan et al, 2017; Dynamic Action Repetition for Deep Reinforcement Learning; AAAI\n Schulman et al., 2017; Proximal Policy Optimization Algorithms; Arxiv\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PARAMETRIZED DEEP Q-NETWORKS LEARNING: PLAYING ONLINE BATTLE ARENA WITH DISCRETE-CONTINUOUS HYBRID ACTION SPACE", "abstract": "Most existing deep reinforcement learning (DRL) frameworks consider action spaces that are either\ndiscrete or continuous space. Motivated by the project of design Game AI for King of Glory\n(KOG), one the world\u2019s most popular mobile game, we consider the scenario with the discrete-continuous\nhybrid action space. To directly apply existing DLR frameworks, existing approaches\neither approximate the hybrid space by a discrete set or relaxing it into a continuous set, which is\nusually less efficient and robust. In this paper, we propose a parametrized deep Q-network (P-DQN)\nfor the hybrid action space without approximation or relaxation. Our algorithm combines DQN and\nDDPG and can be viewed as an extension of the DQN to hybrid actions. The empirical study on the\ngame KOG validates the efficiency and effectiveness of our method.", "pdf": "/pdf/b2c8c78c8f14a68ba1145b8537a7d884f273bc77.pdf", "TL;DR": "A DQN and DDPG hybrid algorithm is proposed to deal with the discrete-continuous hybrid action space.", "paperhash": "xiong|parametrized_deep_qnetworks_learning_playing_online_battle_arena_with_discretecontinuous_hybrid_action_space", "_bibtex": "@misc{\nxiong2018parametrized,\ntitle={{PARAMETRIZED} {DEEP} Q-{NETWORKS} {LEARNING}: {PLAYING} {ONLINE} {BATTLE} {ARENA} {WITH} {DISCRETE}-{CONTINUOUS} {HYBRID} {ACTION} {SPACE}},\nauthor={Jiechao Xiong and Qing Wang and Zhuoran Yang and Peng Sun and Yang Zheng and Lei Han and Haobo Fu and Xiangru Lian and Carson Eisenach and Haichuan Yang and Emmanuel Ekwedike and Bei Peng and Haoyue Gao and Tong Zhang and Ji Liu and Han Liu},\nyear={2018},\nurl={https://openreview.net/forum?id=Sy_MK3lAZ},\n}", "keywords": ["Deep reinforcement learning", "Hybrid action space", "DQN", "DDPG"], "authors": ["Jiechao Xiong", "Qing Wang", "Zhuoran Yang", "Peng Sun", "Yang Zheng", "Lei Han", "Haobo Fu", "Xiangru Lian", "Carson Eisenach", "Haichuan Yang", "Emmanuel Ekwedike", "Bei Peng", "Haoyue Gao", "Tong Zhang", "Ji Liu", "Han Liu"], "authorids": ["jcxiong@tencent.com", "drwang@tencent.com", "pythonsun@tencent.com", "zakzheng@tencent.com", "lxhan@tencent.com", "haobofu@tencent.com", "tongzhang@tongzhang-ml.org", "ji.liu.uwisc@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642443562, "id": "ICLR.cc/2018/Conference/-/Paper397/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper397/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper397/AnonReviewer2", "ICLR.cc/2018/Conference/Paper397/AnonReviewer3", "ICLR.cc/2018/Conference/Paper397/AnonReviewer1"], "reply": {"forum": "Sy_MK3lAZ", "replyto": "Sy_MK3lAZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper397/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642443562}}}], "count": 5}