{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730170627, "tcdate": 1509129648069, "number": 631, "cdate": 1518730170616, "id": "HyunpgbR-", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "HyunpgbR-", "original": "ByPhTxZ0b", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Structured Exploration via Hierarchical Variational Policy Networks", "abstract": "Reinforcement learning in environments with large state-action spaces is challenging, as exploration can be highly inefficient. Even if the dynamics are simple, the optimal policy can be combinatorially hard to discover. In this work, we propose a hierarchical approach to structured exploration to improve the sample efficiency of on-policy exploration in large state-action spaces. The key idea is to model a stochastic policy as a hierarchical latent variable model, which can learn low-dimensional structure in the state-action space, and to define exploration by sampling from the low-dimensional latent space. This approach enables lower sample complexity, while preserving policy expressivity. In order to make learning tractable, we derive a joint learning and exploration strategy by combining hierarchical variational inference with actor-critic learning. The benefits of our learning approach are that 1) it is principled, 2) simple to implement, 3) easily scalable to settings with many actions and 4) easily composable with existing deep learning approaches. We demonstrate the effectiveness of our approach on learning a deep centralized multi-agent policy, as multi-agent environments naturally have an exponentially large state-action space. In this setting, the latent hierarchy implements a form of multi-agent coordination during exploration and execution (MACE). We demonstrate empirically that MACE can more efficiently learn optimal policies in challenging multi-agent games with a large number (~20) of agents, compared to conventional baselines. Moreover, we show that our hierarchical structure leads to meaningful agent coordination.", "pdf": "/pdf/c919771a95ab0c405ebc28a3734b750277b51dbd.pdf", "TL;DR": "Make deep reinforcement learning in large state-action spaces more efficient using structured exploration with deep hierarchical policies.", "paperhash": "zheng|structured_exploration_via_hierarchical_variational_policy_networks", "_bibtex": "@misc{\nzheng2018structured,\ntitle={Structured Exploration via Hierarchical Variational Policy Networks},\nauthor={Stephan Zheng and Yisong Yue},\nyear={2018},\nurl={https://openreview.net/forum?id=HyunpgbR-},\n}", "keywords": ["Deep Reinforcement Learning", "Structured Variational Inference", "Multi-agent Coordination", "Multi-agent Learning"], "authors": ["Stephan Zheng", "Yisong Yue"], "authorids": ["stephan@caltech.edu", "yyue@caltech.edu"]}, "nonreaders": [], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260075795, "tcdate": 1517250233035, "number": 879, "cdate": 1517250233019, "id": "S1W6U1aHM", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "HyunpgbR-", "replyto": "HyunpgbR-", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "The reviewers feel there are two issues that make this paper fall short of acceptance: first, the\nlack of a clear emphasis and focus (evidenced by the significant revisions) and second, a lack of\ncomparison to similar, existing methods for multi-agent reinforcement learning."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Exploration via Hierarchical Variational Policy Networks", "abstract": "Reinforcement learning in environments with large state-action spaces is challenging, as exploration can be highly inefficient. Even if the dynamics are simple, the optimal policy can be combinatorially hard to discover. In this work, we propose a hierarchical approach to structured exploration to improve the sample efficiency of on-policy exploration in large state-action spaces. The key idea is to model a stochastic policy as a hierarchical latent variable model, which can learn low-dimensional structure in the state-action space, and to define exploration by sampling from the low-dimensional latent space. This approach enables lower sample complexity, while preserving policy expressivity. In order to make learning tractable, we derive a joint learning and exploration strategy by combining hierarchical variational inference with actor-critic learning. The benefits of our learning approach are that 1) it is principled, 2) simple to implement, 3) easily scalable to settings with many actions and 4) easily composable with existing deep learning approaches. We demonstrate the effectiveness of our approach on learning a deep centralized multi-agent policy, as multi-agent environments naturally have an exponentially large state-action space. In this setting, the latent hierarchy implements a form of multi-agent coordination during exploration and execution (MACE). We demonstrate empirically that MACE can more efficiently learn optimal policies in challenging multi-agent games with a large number (~20) of agents, compared to conventional baselines. Moreover, we show that our hierarchical structure leads to meaningful agent coordination.", "pdf": "/pdf/c919771a95ab0c405ebc28a3734b750277b51dbd.pdf", "TL;DR": "Make deep reinforcement learning in large state-action spaces more efficient using structured exploration with deep hierarchical policies.", "paperhash": "zheng|structured_exploration_via_hierarchical_variational_policy_networks", "_bibtex": "@misc{\nzheng2018structured,\ntitle={Structured Exploration via Hierarchical Variational Policy Networks},\nauthor={Stephan Zheng and Yisong Yue},\nyear={2018},\nurl={https://openreview.net/forum?id=HyunpgbR-},\n}", "keywords": ["Deep Reinforcement Learning", "Structured Variational Inference", "Multi-agent Coordination", "Multi-agent Learning"], "authors": ["Stephan Zheng", "Yisong Yue"], "authorids": ["stephan@caltech.edu", "yyue@caltech.edu"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642482844, "tcdate": 1511467228346, "number": 1, "cdate": 1511467228346, "id": "Hk4yYjNef", "invitation": "ICLR.cc/2018/Conference/-/Paper631/Official_Review", "forum": "HyunpgbR-", "replyto": "HyunpgbR-", "signatures": ["ICLR.cc/2018/Conference/Paper631/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Seems interesting, but with significant weaknesses", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposes an approach to improve exploration in multiagent reinforcement learning by allowing the policies of the individual agents to be conditioned on an external coordination signal \\lambda. In order to find such parametrized policies, the approach combines deep RL with a variational inference approach (ELBO optimization). The paper presents an empirical evaluation, which seems encouraging, but that is also somewhat difficult to interpret given the lack of comparison to other state-of-the-art methods.\n\nOverall, the paper seems interesting, but (in addition to the not completely convincing empirical evaluation), it has two main weaknesses: lack of clarity and grounding in related literature.\n\n=Issues with clarity=\n\n-\"This problem has two equivalent solutions\". \nThis is not so clear; depending on the movement of the preys it might well be that the optimal solution will switch to the other prey in certain cases?\n\n-It is not clear what is really meant with the term \"structured exploration\". It just seems to mean 'improved'?\n\n-It is not clear that the improvements are due to exploration; my feeling is that is is due to improved statistical strength on a more abstract state feature (which is learned), not unlike:\nGeramifard, Alborz, et al. \"Online discovery of feature dependencies.\" Proceedings of the 28th International Conference on Machine Learning (ICML-11). 2011.\nHowever, there is no clear indication that there is an improved exploration policy.\n\n-The problem setting is not quite clear:\nThe paper first introduces \"multi-agent RL\", which seems to correspond to a \"stochastic game\" (also \"Markov game\"), but then moves on to restrict to the \"fully cooperative setting\" (which would make it a \"Multiagent MDP\", Boutilier '96).\n\nIt subsequently says it deals only with deterministic problems (which would reduce the problem further to a learning version of a multiagent classical planning problem), but in the experiments do consider stochastically moving preys.\n\n-The paper says the problem is fully observable, but fails to make explicit if this is *individually* fully observable, or jointly. I am assuming the former, but is it not clear how the agents observe this full state in the experimental evaluation.\n\nThis is actually a crucial confusion, as it completely changes the interpretation of what the approach does: in the individually observable case, the approach is adding a redundant source of information which is more abstract and thus seems to facilitate faster learning. In the latter case, where agents would have individual observations, it is actually providing the agents with more information.\n\nAs such, I would really encourage the authors to better define the task they are considering. E.g., by building on the taxonomies of problems that researchers have developed in the community focusing on decentralized POMDPs, such as:\nGoldman, Claudia V., and Shlomo Zilberstein. \"Decentralized control of cooperative systems: Categorization and complexity analysis.\" (2004).\n\n-\"Compared to the single-agent RL setting, multi-agent RL poses unique difficulties. A central issue\nis the exploration-exploitation trade-off\"\nThat now in particular happens to be a central issue in single agent RL too.\n\n-\"Finding the true posteriors P (\u03bb t |s t ) \u221d P (s t |\u03bb t )P (\u03bb t ) is intractable in general\"\nThe paper did not explain how this inference task is required to solve the RL problem.\n\n-In general, I found the technical description impossible to follow, even after carefully looking at the appending. For instance, (also) there the term P (\u03bb |s ) is suddenly introduced without explaining what the term exactly is? Why is the term P(a|\u03bb) not popping up here? That also needs to be optimized, right? I suppose \\phi is the parameter vector of the variational approximation, but it is never really stated. The various shorthand notations introduced for clarity do not help at all, but only make the formulas very cryptic.\n\n-The main text is not readable since definitions, e.g., L(Q_r,\\tehta,\\phi), that are in the appendix are now missing.\n\n-It is not clear to me how the second term of (10) is now estimated?\n\n-\"Shared (shared actor-critic): agents share a deterministic hidden layer,\"\nWhat kind of layer is this exactly? How does it relate to \\lambda ?\n\n-\"The key difference is that this model does not sample from the shared hidden layer\"\nWhy would sampling help? Given that we are dealing with a fully observable multiagent MDP, there is no inherent need to randomize at all? (there should be a optimal deterministic joint policy?)\n\n-\"There is shared information between the agents\"\nWhat information is referred to exactly? \nAlso: It is not quite clear if for these domains cloned would be better than completely independent learners (without shared weights)?\n\n-I can't seem to find anywhere what is the actual shape (or type? I am assuming a vector of reals) of the used \\lambda.\n\n-in figure 5, rhs, what is being shown exactly? What do the colors mean? Why does there seem to be a \\lambda *per* agent now?\n\n\n\n=Related work=\n\nI think the paper could/should be hugely improved in this respect. \n\nThe idea of casting MARL as inference has also been considered by:\n\nLearning for Decentralized Control of Multiagent Systems in Large, Partially-Observable Stochastic Environments.\nM Liu, C Amato, EP Anesta, JD Griffith, JP How - AAAI, 2016\n\nStick-breaking policy learning in Dec-POMDPs\nM Liu, C Amato, X Liao, L Carin, JP How\nInternational Joint Conference on Artificial Intelligence (IJCAI) 2015\n\nWu, F.; Zilberstein, S.; and Jennings, N. R. 2013. Monte-carlo\nexpectation maximization for decentralized POMDPs. In Proc.\nof the 23rd Int\u2019l Joint Conf. on Artificial Intelligence (IJCAI-\n13).\n\nI do not think that these explicitly make use of a mechanism to coordinate the policies, since they address to true Dec-POMDP setting where each agent only gets its own observations, but in the Dec-POMDP literature, there also is the notion of 'correlation device', which is an additional controller (say corresponding to a dummy agent), which of which the states can be observed by other agents and used to condition their actions on:\n\nBernstein DS, Hansen EA, Zilberstein S. Bounded policy iteration for decentralized POMDPs. InProceedings of the nineteenth international joint conference on artificial intelligence (IJCAI) 2005 Jun 6 (pp. 52-57).\n\n(and clearly this could be directly included in the aforementioned learning approaches). \n\n\nThis notion of a correlation device also highlights to potential relation to methods to learn/compute correlated equilibria. E.g.,:\n\nGreenwald A, Hall K, Serrano R. Correlated Q-learning. In ICML 2003 Aug 21 (Vol. 3, pp. 242-249).\n\n\nA different connection between MARL and inference can be found in:\n\nZhang, Xinhua and Aberdeen, Douglas and Vishwanathan, S. V. N., \"Conditional Random Fields for Multi-agent Reinforcement Learning\", in (New York, NY, USA: ACM, 2007), pp. 1143--1150.\n\n\nThe idea of doing something hierarchical of course makes sense, but also here there are a number of related papers:\n\n-putting \"hierarchical multiagent\" in google scholar finds works by Ghavamzadeh et al., Saira & Mahadevan, etc.\n\n-Victor Lesser has pursued coordination for better exploration with a number of students.\n\nI suppose that Guestrin et al.'s classical paper:\nGuestrin, Carlos, Michail Lagoudakis, and Ronald Parr. \"Coordinated reinforcement learning.\" ICML. Vol. 2. 2002.\nwould deserve a citation, and the MARL field is moving ahead fast, an explanation of the differences with COMA:\nCounterfactual Multi-Agent Policy Gradients\nJ Foerster, G Farquhar, T Afouras, N Nardelli, S Whiteson\nAAAI 2018\nis probably also warranted.\n\n\n\n\n\n\n\n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Exploration via Hierarchical Variational Policy Networks", "abstract": "Reinforcement learning in environments with large state-action spaces is challenging, as exploration can be highly inefficient. Even if the dynamics are simple, the optimal policy can be combinatorially hard to discover. In this work, we propose a hierarchical approach to structured exploration to improve the sample efficiency of on-policy exploration in large state-action spaces. The key idea is to model a stochastic policy as a hierarchical latent variable model, which can learn low-dimensional structure in the state-action space, and to define exploration by sampling from the low-dimensional latent space. This approach enables lower sample complexity, while preserving policy expressivity. In order to make learning tractable, we derive a joint learning and exploration strategy by combining hierarchical variational inference with actor-critic learning. The benefits of our learning approach are that 1) it is principled, 2) simple to implement, 3) easily scalable to settings with many actions and 4) easily composable with existing deep learning approaches. We demonstrate the effectiveness of our approach on learning a deep centralized multi-agent policy, as multi-agent environments naturally have an exponentially large state-action space. In this setting, the latent hierarchy implements a form of multi-agent coordination during exploration and execution (MACE). We demonstrate empirically that MACE can more efficiently learn optimal policies in challenging multi-agent games with a large number (~20) of agents, compared to conventional baselines. Moreover, we show that our hierarchical structure leads to meaningful agent coordination.", "pdf": "/pdf/c919771a95ab0c405ebc28a3734b750277b51dbd.pdf", "TL;DR": "Make deep reinforcement learning in large state-action spaces more efficient using structured exploration with deep hierarchical policies.", "paperhash": "zheng|structured_exploration_via_hierarchical_variational_policy_networks", "_bibtex": "@misc{\nzheng2018structured,\ntitle={Structured Exploration via Hierarchical Variational Policy Networks},\nauthor={Stephan Zheng and Yisong Yue},\nyear={2018},\nurl={https://openreview.net/forum?id=HyunpgbR-},\n}", "keywords": ["Deep Reinforcement Learning", "Structured Variational Inference", "Multi-agent Coordination", "Multi-agent Learning"], "authors": ["Stephan Zheng", "Yisong Yue"], "authorids": ["stephan@caltech.edu", "yyue@caltech.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642482749, "id": "ICLR.cc/2018/Conference/-/Paper631/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper631/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper631/AnonReviewer1", "ICLR.cc/2018/Conference/Paper631/AnonReviewer3", "ICLR.cc/2018/Conference/Paper631/AnonReviewer2"], "reply": {"forum": "HyunpgbR-", "replyto": "HyunpgbR-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper631/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642482749}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642482799, "tcdate": 1511802939412, "number": 2, "cdate": 1511802939412, "id": "SJmr_aFgf", "invitation": "ICLR.cc/2018/Conference/-/Paper631/Official_Review", "forum": "HyunpgbR-", "replyto": "HyunpgbR-", "signatures": ["ICLR.cc/2018/Conference/Paper631/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Paper presents interesting and potentially novel method.", "rating": "7: Good paper, accept", "review": "The paper proposes a method to coordinate agent behaviour  by using policies that have a shared latent structure. The authors derive a variational policy optimisation method to optimise the coordinated policies. The approach is investigated empirically on 2 predator prey type games.\n\nThe method presented in the paper seems quite novel. The authors present a derivation of their variational, hierarchical  update. Not all steps in this derivation are equally well explained, especially the introduction of the variational posterior could be more detailed. The appendix also offers very little extra information compared to the main text, most paragraphs concerning the derivations are identical. The comparison to existing approaches using variational inference is quite brief. It would be nice to have a more detailed explanation of the novel steps in this approach.\n\n It also seems that by assuming a shared model, shared global state and a fully cooperative problem, the authors remove many of the complexities of a multi-agent system. This also brings the derivations closer to the single agent case.\n\nA related potential criticism is the feasibility of using this approach in a multi-agent system. The authors are essentially creating a (partially) centralised learner. The cooperative rewards and shared structure assumptions structures mentioned above seem limiting in a multi-agent system. Even giving each agent local state observations is known to potentially create coordination problems. The predator prey games where agents with agents physically distributed over the environment are probably not the best motivational examples.\n\nOther remarks: \n\nEmpirical result show a clear advantage for this method over the baselines. The evaluation domains are relatively simple, but it was nice to see that the authors also make an attempt to investigate the qualitative behaviour of their method.\n\nThe overview of related work was relatively brief and focused mostly on recent deep MARL approaches. There is a very large body on coordination in multi-agent RL. It would be nice to situate the research somewhat better within this field (or at least refer to an overview such as Busoniu et al, 2010).\n\nIt seems like a completely factorised approach (i.e. independent agents) would make a nice baseline for the experiments, in addition to the shared architecture approaches.\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Exploration via Hierarchical Variational Policy Networks", "abstract": "Reinforcement learning in environments with large state-action spaces is challenging, as exploration can be highly inefficient. Even if the dynamics are simple, the optimal policy can be combinatorially hard to discover. In this work, we propose a hierarchical approach to structured exploration to improve the sample efficiency of on-policy exploration in large state-action spaces. The key idea is to model a stochastic policy as a hierarchical latent variable model, which can learn low-dimensional structure in the state-action space, and to define exploration by sampling from the low-dimensional latent space. This approach enables lower sample complexity, while preserving policy expressivity. In order to make learning tractable, we derive a joint learning and exploration strategy by combining hierarchical variational inference with actor-critic learning. The benefits of our learning approach are that 1) it is principled, 2) simple to implement, 3) easily scalable to settings with many actions and 4) easily composable with existing deep learning approaches. We demonstrate the effectiveness of our approach on learning a deep centralized multi-agent policy, as multi-agent environments naturally have an exponentially large state-action space. In this setting, the latent hierarchy implements a form of multi-agent coordination during exploration and execution (MACE). We demonstrate empirically that MACE can more efficiently learn optimal policies in challenging multi-agent games with a large number (~20) of agents, compared to conventional baselines. Moreover, we show that our hierarchical structure leads to meaningful agent coordination.", "pdf": "/pdf/c919771a95ab0c405ebc28a3734b750277b51dbd.pdf", "TL;DR": "Make deep reinforcement learning in large state-action spaces more efficient using structured exploration with deep hierarchical policies.", "paperhash": "zheng|structured_exploration_via_hierarchical_variational_policy_networks", "_bibtex": "@misc{\nzheng2018structured,\ntitle={Structured Exploration via Hierarchical Variational Policy Networks},\nauthor={Stephan Zheng and Yisong Yue},\nyear={2018},\nurl={https://openreview.net/forum?id=HyunpgbR-},\n}", "keywords": ["Deep Reinforcement Learning", "Structured Variational Inference", "Multi-agent Coordination", "Multi-agent Learning"], "authors": ["Stephan Zheng", "Yisong Yue"], "authorids": ["stephan@caltech.edu", "yyue@caltech.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642482749, "id": "ICLR.cc/2018/Conference/-/Paper631/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper631/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper631/AnonReviewer1", "ICLR.cc/2018/Conference/Paper631/AnonReviewer3", "ICLR.cc/2018/Conference/Paper631/AnonReviewer2"], "reply": {"forum": "HyunpgbR-", "replyto": "HyunpgbR-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper631/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642482749}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642482763, "tcdate": 1511834356238, "number": 3, "cdate": 1511834356238, "id": "B1hxXS9xM", "invitation": "ICLR.cc/2018/Conference/-/Paper631/Official_Review", "forum": "HyunpgbR-", "replyto": "HyunpgbR-", "signatures": ["ICLR.cc/2018/Conference/Paper631/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Interesting method, missing related work and baselines, very limited experiments. Unclear if this is really a 'multi-agent' paper", "rating": "5: Marginally below acceptance threshold", "review": "This paper suggests an interesting algorithmic innovation, consisting of hierarchical latent variables for coordinated exploration in multi-agent settings. \n\nMain concern: This work heavily relies on the multi-agent aspect for novelty : \n\"Houthooft et al. (2016) learned exploration policies via information gain using variational methods. However, these only consider 1 agent\".  However, in the current form of the paper this is a questionable claim. As the problems investigated combine fully observable states, purely cooperative payouts and global latent variables, they reduce to single agent problems with a large action space. Effectively the 'different agents' are nothing but a parameterized action space of a central controller. \nUsing hierarchical latent variables for large action spaces is like a good idea, but placing the work into multi-agent seems like a red herring. \n\nGiven that this is a centralized controller, it would be really helpful to compare quantitatively to other approaches for structured exploration, eg [3] and [4].\n\nDetailed comments:\n-\"we restrict to fully cooperative MDPs that are fully observable, deterministic and episodic.\" Since the rewards are also positive, a very relevant baseline (from a MARL point of view) is distributed Q-learning [1].\n-Figure 3: Showing total cumulative terminal rewards is difficult to interpret. I would be interested in seeing standard 'training curves' which show the average return per episode after a given amount of training episodes. Currently it is difficult to judge whether training has converged on not.\n-Related work is missing a lot of relevant research. Apart from references below, please see [2] for a relevant, if dated, overview.\n-\"Table 1: Total terminal reward (averaged over 5 best runs)\" - how does the mean and median compare across methods for all runs rather than just the top 5? \n\n\nReferences:\n[1] Lauer, M., Riedmiller, M.: An algorithm for distributed reinforcement learning in cooperative\nmulti-agent systems. In: Proceedings 17th International Conference on Machine Learning\n(ICML-00), pp. 535\u2013542. Stanford University, US (2000)\n[2] L. Bus\u00b8oniu, R. Babuska, and B. De Schutter: Multi-Agent Reinforcement Learning: An Overview\n[3] Nicolas Usunier, Gabriel Synnaeve, Zeming Lin, Soumith Chintala: Episodic Exploration for Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks\n[4] Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y. Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, Marcin Andrychowicz: Parameter Space Noise for Exploration\n\n\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Exploration via Hierarchical Variational Policy Networks", "abstract": "Reinforcement learning in environments with large state-action spaces is challenging, as exploration can be highly inefficient. Even if the dynamics are simple, the optimal policy can be combinatorially hard to discover. In this work, we propose a hierarchical approach to structured exploration to improve the sample efficiency of on-policy exploration in large state-action spaces. The key idea is to model a stochastic policy as a hierarchical latent variable model, which can learn low-dimensional structure in the state-action space, and to define exploration by sampling from the low-dimensional latent space. This approach enables lower sample complexity, while preserving policy expressivity. In order to make learning tractable, we derive a joint learning and exploration strategy by combining hierarchical variational inference with actor-critic learning. The benefits of our learning approach are that 1) it is principled, 2) simple to implement, 3) easily scalable to settings with many actions and 4) easily composable with existing deep learning approaches. We demonstrate the effectiveness of our approach on learning a deep centralized multi-agent policy, as multi-agent environments naturally have an exponentially large state-action space. In this setting, the latent hierarchy implements a form of multi-agent coordination during exploration and execution (MACE). We demonstrate empirically that MACE can more efficiently learn optimal policies in challenging multi-agent games with a large number (~20) of agents, compared to conventional baselines. Moreover, we show that our hierarchical structure leads to meaningful agent coordination.", "pdf": "/pdf/c919771a95ab0c405ebc28a3734b750277b51dbd.pdf", "TL;DR": "Make deep reinforcement learning in large state-action spaces more efficient using structured exploration with deep hierarchical policies.", "paperhash": "zheng|structured_exploration_via_hierarchical_variational_policy_networks", "_bibtex": "@misc{\nzheng2018structured,\ntitle={Structured Exploration via Hierarchical Variational Policy Networks},\nauthor={Stephan Zheng and Yisong Yue},\nyear={2018},\nurl={https://openreview.net/forum?id=HyunpgbR-},\n}", "keywords": ["Deep Reinforcement Learning", "Structured Variational Inference", "Multi-agent Coordination", "Multi-agent Learning"], "authors": ["Stephan Zheng", "Yisong Yue"], "authorids": ["stephan@caltech.edu", "yyue@caltech.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642482749, "id": "ICLR.cc/2018/Conference/-/Paper631/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper631/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper631/AnonReviewer1", "ICLR.cc/2018/Conference/Paper631/AnonReviewer3", "ICLR.cc/2018/Conference/Paper631/AnonReviewer2"], "reply": {"forum": "HyunpgbR-", "replyto": "HyunpgbR-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper631/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642482749}}}, {"tddate": null, "ddate": null, "tmdate": 1515175758282, "tcdate": 1515175758282, "number": 11, "cdate": 1515175758282, "id": "HkULJrp7M", "invitation": "ICLR.cc/2018/Conference/-/Paper631/Official_Comment", "forum": "HyunpgbR-", "replyto": "HyunpgbR-", "signatures": ["ICLR.cc/2018/Conference/Paper631/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper631/Authors"], "content": {"title": "Revision", "comment": "We'd like to notify reviewers that based on the reviews, we have uploaded a revision of our paper that addresses their comments and suggestions. Revised sections: \n\n1. Title\n2. Abstract\n3. Introduction\n4. Theory \n5. Related work\n6. Appendix"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Exploration via Hierarchical Variational Policy Networks", "abstract": "Reinforcement learning in environments with large state-action spaces is challenging, as exploration can be highly inefficient. Even if the dynamics are simple, the optimal policy can be combinatorially hard to discover. In this work, we propose a hierarchical approach to structured exploration to improve the sample efficiency of on-policy exploration in large state-action spaces. The key idea is to model a stochastic policy as a hierarchical latent variable model, which can learn low-dimensional structure in the state-action space, and to define exploration by sampling from the low-dimensional latent space. This approach enables lower sample complexity, while preserving policy expressivity. In order to make learning tractable, we derive a joint learning and exploration strategy by combining hierarchical variational inference with actor-critic learning. The benefits of our learning approach are that 1) it is principled, 2) simple to implement, 3) easily scalable to settings with many actions and 4) easily composable with existing deep learning approaches. We demonstrate the effectiveness of our approach on learning a deep centralized multi-agent policy, as multi-agent environments naturally have an exponentially large state-action space. In this setting, the latent hierarchy implements a form of multi-agent coordination during exploration and execution (MACE). We demonstrate empirically that MACE can more efficiently learn optimal policies in challenging multi-agent games with a large number (~20) of agents, compared to conventional baselines. Moreover, we show that our hierarchical structure leads to meaningful agent coordination.", "pdf": "/pdf/c919771a95ab0c405ebc28a3734b750277b51dbd.pdf", "TL;DR": "Make deep reinforcement learning in large state-action spaces more efficient using structured exploration with deep hierarchical policies.", "paperhash": "zheng|structured_exploration_via_hierarchical_variational_policy_networks", "_bibtex": "@misc{\nzheng2018structured,\ntitle={Structured Exploration via Hierarchical Variational Policy Networks},\nauthor={Stephan Zheng and Yisong Yue},\nyear={2018},\nurl={https://openreview.net/forum?id=HyunpgbR-},\n}", "keywords": ["Deep Reinforcement Learning", "Structured Variational Inference", "Multi-agent Coordination", "Multi-agent Learning"], "authors": ["Stephan Zheng", "Yisong Yue"], "authorids": ["stephan@caltech.edu", "yyue@caltech.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825730353, "id": "ICLR.cc/2018/Conference/-/Paper631/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HyunpgbR-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper631/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper631/Authors|ICLR.cc/2018/Conference/Paper631/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper631/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper631/Authors|ICLR.cc/2018/Conference/Paper631/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper631/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper631/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper631/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper631/Reviewers", "ICLR.cc/2018/Conference/Paper631/Authors", "ICLR.cc/2018/Conference/Paper631/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825730353}}}, {"tddate": null, "ddate": null, "tmdate": 1514860253367, "tcdate": 1514859929390, "number": 10, "cdate": 1514859929390, "id": "r1WspD_7f", "invitation": "ICLR.cc/2018/Conference/-/Paper631/Official_Comment", "forum": "HyunpgbR-", "replyto": "SJHUqM2MM", "signatures": ["ICLR.cc/2018/Conference/Paper631/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper631/Authors"], "content": {"title": "Partial observability", "comment": "Thank you for your interest and response! \n\nWe think the partial observable setting is very interesting to study wrt modeling policies as deep graphical models. In the partial observable case, one can choose a multi-agent learning problem definition to operate under. This has implications for how the joint / individual policies factorize / are shared. However, our method of modeling the hidden structure of the joint / individual policies can largely be generalized to these settings and are complementary to other works that operate under partial observability.\n\n1. For instance, one can provide only partial observations to the individual agent's policies, but give the model for lambda access to the full state. Then, the model for lambda can be interpreted as a sort of \"correlation device\". In this case, our variational approach and policy factorization can still be applied.\n\n2. Agent policies can be fully decoupled, where each agent now maintains their own coordination model (e.g. Q(lambda | observation_i). In this case, each agent can e.g. predict the observations / actions of other agents as well (see e.g. \"Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments\", Lowe et al.). This could give the individual agents a way to learn an implied coordination model from their imputations of other agents' state. Here, the latent variables could compactly encode distributions that are efficient for state/action imputation for many agents simultaneously.\n\nNote that in our paper, the policy support centralized learning and decentralized execution, where the agents only have to share a random seed to sample lambdas during execution. Similarly, in the fully decoupled case, agents could still agree on a random seed during training and execution. \n\n\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Exploration via Hierarchical Variational Policy Networks", "abstract": "Reinforcement learning in environments with large state-action spaces is challenging, as exploration can be highly inefficient. Even if the dynamics are simple, the optimal policy can be combinatorially hard to discover. In this work, we propose a hierarchical approach to structured exploration to improve the sample efficiency of on-policy exploration in large state-action spaces. The key idea is to model a stochastic policy as a hierarchical latent variable model, which can learn low-dimensional structure in the state-action space, and to define exploration by sampling from the low-dimensional latent space. This approach enables lower sample complexity, while preserving policy expressivity. In order to make learning tractable, we derive a joint learning and exploration strategy by combining hierarchical variational inference with actor-critic learning. The benefits of our learning approach are that 1) it is principled, 2) simple to implement, 3) easily scalable to settings with many actions and 4) easily composable with existing deep learning approaches. We demonstrate the effectiveness of our approach on learning a deep centralized multi-agent policy, as multi-agent environments naturally have an exponentially large state-action space. In this setting, the latent hierarchy implements a form of multi-agent coordination during exploration and execution (MACE). We demonstrate empirically that MACE can more efficiently learn optimal policies in challenging multi-agent games with a large number (~20) of agents, compared to conventional baselines. Moreover, we show that our hierarchical structure leads to meaningful agent coordination.", "pdf": "/pdf/c919771a95ab0c405ebc28a3734b750277b51dbd.pdf", "TL;DR": "Make deep reinforcement learning in large state-action spaces more efficient using structured exploration with deep hierarchical policies.", "paperhash": "zheng|structured_exploration_via_hierarchical_variational_policy_networks", "_bibtex": "@misc{\nzheng2018structured,\ntitle={Structured Exploration via Hierarchical Variational Policy Networks},\nauthor={Stephan Zheng and Yisong Yue},\nyear={2018},\nurl={https://openreview.net/forum?id=HyunpgbR-},\n}", "keywords": ["Deep Reinforcement Learning", "Structured Variational Inference", "Multi-agent Coordination", "Multi-agent Learning"], "authors": ["Stephan Zheng", "Yisong Yue"], "authorids": ["stephan@caltech.edu", "yyue@caltech.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825730353, "id": "ICLR.cc/2018/Conference/-/Paper631/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HyunpgbR-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper631/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper631/Authors|ICLR.cc/2018/Conference/Paper631/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper631/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper631/Authors|ICLR.cc/2018/Conference/Paper631/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper631/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper631/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper631/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper631/Reviewers", "ICLR.cc/2018/Conference/Paper631/Authors", "ICLR.cc/2018/Conference/Paper631/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825730353}}}, {"tddate": null, "ddate": null, "tmdate": 1514858903699, "tcdate": 1514858319106, "number": 9, "cdate": 1514858319106, "id": "BywLPDO7M", "invitation": "ICLR.cc/2018/Conference/-/Paper631/Official_Comment", "forum": "HyunpgbR-", "replyto": "rJ_4ESsff", "signatures": ["ICLR.cc/2018/Conference/Paper631/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper631/Authors"], "content": {"title": "Clarifications", "comment": "Thanks for your interest in our paper! \n\n\"We think this paper is not a clear paper. First, the definition of symbols in the paper is not clear. Many symbols used in the paper is not defined.\"\n- We've uploaded a paper revision that clarifies the writing and definitions. Please let us know if there are further details that are unclear, so we can clarify further if needed.\n\n\"Second, the decomposition of distribution of latent variables in the paper is not clear and looks less reasonable.\"\n- We model the policy distribution P(a_t|s_t) = \\int dlambda P(a_t, lambda_t | s_t) using a latent variable lambda_t for each time-step t. The distribution of the latent variables lambda_t is learned by Q(lambda_t | s_t), which gives a principled variational lower bound on the log-likelihood of the policy distribution (and hence the expected reward). This is (one of) the simplest latent structure that one can assume, and that can capture a wide range of complex policy distributions P(a|s), given powerful neural network models for Q(lambda_t | s_t) and the \"decoder\" P(a|lambda, s).\n\n\"Third, it is not clear how the agents observe this full states of the environment, etc. The paper states that the architecture of their approach helps the coordination of agents. But we think it is nothing but a centralized controller and we do not know the advantage of it over centralized controller method.\"\n\n- The centralized controller observes the positions of all agents (predators and prey). The coordination of the predators' actions is correlated through the latent variable lambda. \n\nIt is unclear to us what is meant by \"centralized controller method\". We invite the commenter to clarify this. \n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Exploration via Hierarchical Variational Policy Networks", "abstract": "Reinforcement learning in environments with large state-action spaces is challenging, as exploration can be highly inefficient. Even if the dynamics are simple, the optimal policy can be combinatorially hard to discover. In this work, we propose a hierarchical approach to structured exploration to improve the sample efficiency of on-policy exploration in large state-action spaces. The key idea is to model a stochastic policy as a hierarchical latent variable model, which can learn low-dimensional structure in the state-action space, and to define exploration by sampling from the low-dimensional latent space. This approach enables lower sample complexity, while preserving policy expressivity. In order to make learning tractable, we derive a joint learning and exploration strategy by combining hierarchical variational inference with actor-critic learning. The benefits of our learning approach are that 1) it is principled, 2) simple to implement, 3) easily scalable to settings with many actions and 4) easily composable with existing deep learning approaches. We demonstrate the effectiveness of our approach on learning a deep centralized multi-agent policy, as multi-agent environments naturally have an exponentially large state-action space. In this setting, the latent hierarchy implements a form of multi-agent coordination during exploration and execution (MACE). We demonstrate empirically that MACE can more efficiently learn optimal policies in challenging multi-agent games with a large number (~20) of agents, compared to conventional baselines. Moreover, we show that our hierarchical structure leads to meaningful agent coordination.", "pdf": "/pdf/c919771a95ab0c405ebc28a3734b750277b51dbd.pdf", "TL;DR": "Make deep reinforcement learning in large state-action spaces more efficient using structured exploration with deep hierarchical policies.", "paperhash": "zheng|structured_exploration_via_hierarchical_variational_policy_networks", "_bibtex": "@misc{\nzheng2018structured,\ntitle={Structured Exploration via Hierarchical Variational Policy Networks},\nauthor={Stephan Zheng and Yisong Yue},\nyear={2018},\nurl={https://openreview.net/forum?id=HyunpgbR-},\n}", "keywords": ["Deep Reinforcement Learning", "Structured Variational Inference", "Multi-agent Coordination", "Multi-agent Learning"], "authors": ["Stephan Zheng", "Yisong Yue"], "authorids": ["stephan@caltech.edu", "yyue@caltech.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825730353, "id": "ICLR.cc/2018/Conference/-/Paper631/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HyunpgbR-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper631/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper631/Authors|ICLR.cc/2018/Conference/Paper631/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper631/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper631/Authors|ICLR.cc/2018/Conference/Paper631/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper631/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper631/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper631/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper631/Reviewers", "ICLR.cc/2018/Conference/Paper631/Authors", "ICLR.cc/2018/Conference/Paper631/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825730353}}}, {"tddate": null, "ddate": null, "tmdate": 1514857763114, "tcdate": 1514857763114, "number": 8, "cdate": 1514857763114, "id": "rJiQSDumf", "invitation": "ICLR.cc/2018/Conference/-/Paper631/Official_Comment", "forum": "HyunpgbR-", "replyto": "HJITw-LWf", "signatures": ["ICLR.cc/2018/Conference/Paper631/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper631/Authors"], "content": {"title": "re", "comment": "Thank you for your interest! We're working on cleaning the code and making it easy to use. We will let you know as soon as possible when we have a version that can be shared. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Exploration via Hierarchical Variational Policy Networks", "abstract": "Reinforcement learning in environments with large state-action spaces is challenging, as exploration can be highly inefficient. Even if the dynamics are simple, the optimal policy can be combinatorially hard to discover. In this work, we propose a hierarchical approach to structured exploration to improve the sample efficiency of on-policy exploration in large state-action spaces. The key idea is to model a stochastic policy as a hierarchical latent variable model, which can learn low-dimensional structure in the state-action space, and to define exploration by sampling from the low-dimensional latent space. This approach enables lower sample complexity, while preserving policy expressivity. In order to make learning tractable, we derive a joint learning and exploration strategy by combining hierarchical variational inference with actor-critic learning. The benefits of our learning approach are that 1) it is principled, 2) simple to implement, 3) easily scalable to settings with many actions and 4) easily composable with existing deep learning approaches. We demonstrate the effectiveness of our approach on learning a deep centralized multi-agent policy, as multi-agent environments naturally have an exponentially large state-action space. In this setting, the latent hierarchy implements a form of multi-agent coordination during exploration and execution (MACE). We demonstrate empirically that MACE can more efficiently learn optimal policies in challenging multi-agent games with a large number (~20) of agents, compared to conventional baselines. Moreover, we show that our hierarchical structure leads to meaningful agent coordination.", "pdf": "/pdf/c919771a95ab0c405ebc28a3734b750277b51dbd.pdf", "TL;DR": "Make deep reinforcement learning in large state-action spaces more efficient using structured exploration with deep hierarchical policies.", "paperhash": "zheng|structured_exploration_via_hierarchical_variational_policy_networks", "_bibtex": "@misc{\nzheng2018structured,\ntitle={Structured Exploration via Hierarchical Variational Policy Networks},\nauthor={Stephan Zheng and Yisong Yue},\nyear={2018},\nurl={https://openreview.net/forum?id=HyunpgbR-},\n}", "keywords": ["Deep Reinforcement Learning", "Structured Variational Inference", "Multi-agent Coordination", "Multi-agent Learning"], "authors": ["Stephan Zheng", "Yisong Yue"], "authorids": ["stephan@caltech.edu", "yyue@caltech.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825730353, "id": "ICLR.cc/2018/Conference/-/Paper631/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HyunpgbR-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper631/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper631/Authors|ICLR.cc/2018/Conference/Paper631/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper631/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper631/Authors|ICLR.cc/2018/Conference/Paper631/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper631/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper631/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper631/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper631/Reviewers", "ICLR.cc/2018/Conference/Paper631/Authors", "ICLR.cc/2018/Conference/Paper631/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825730353}}}, {"tddate": null, "ddate": null, "tmdate": 1514052172572, "tcdate": 1514052172572, "number": 4, "cdate": 1514052172572, "id": "SJHUqM2MM", "invitation": "ICLR.cc/2018/Conference/-/Paper631/Public_Comment", "forum": "HyunpgbR-", "replyto": "HyunpgbR-", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Beautiful Paper", "comment": "Really enjoyed reading the paper, one of better papers at ICLR in our opinion. It seems to us that you should be able to extend this to partially observable scenarios as well, and not just fully observable. Could you comment where things break down when assuming partially observability? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Exploration via Hierarchical Variational Policy Networks", "abstract": "Reinforcement learning in environments with large state-action spaces is challenging, as exploration can be highly inefficient. Even if the dynamics are simple, the optimal policy can be combinatorially hard to discover. In this work, we propose a hierarchical approach to structured exploration to improve the sample efficiency of on-policy exploration in large state-action spaces. The key idea is to model a stochastic policy as a hierarchical latent variable model, which can learn low-dimensional structure in the state-action space, and to define exploration by sampling from the low-dimensional latent space. This approach enables lower sample complexity, while preserving policy expressivity. In order to make learning tractable, we derive a joint learning and exploration strategy by combining hierarchical variational inference with actor-critic learning. The benefits of our learning approach are that 1) it is principled, 2) simple to implement, 3) easily scalable to settings with many actions and 4) easily composable with existing deep learning approaches. We demonstrate the effectiveness of our approach on learning a deep centralized multi-agent policy, as multi-agent environments naturally have an exponentially large state-action space. In this setting, the latent hierarchy implements a form of multi-agent coordination during exploration and execution (MACE). We demonstrate empirically that MACE can more efficiently learn optimal policies in challenging multi-agent games with a large number (~20) of agents, compared to conventional baselines. Moreover, we show that our hierarchical structure leads to meaningful agent coordination.", "pdf": "/pdf/c919771a95ab0c405ebc28a3734b750277b51dbd.pdf", "TL;DR": "Make deep reinforcement learning in large state-action spaces more efficient using structured exploration with deep hierarchical policies.", "paperhash": "zheng|structured_exploration_via_hierarchical_variational_policy_networks", "_bibtex": "@misc{\nzheng2018structured,\ntitle={Structured Exploration via Hierarchical Variational Policy Networks},\nauthor={Stephan Zheng and Yisong Yue},\nyear={2018},\nurl={https://openreview.net/forum?id=HyunpgbR-},\n}", "keywords": ["Deep Reinforcement Learning", "Structured Variational Inference", "Multi-agent Coordination", "Multi-agent Learning"], "authors": ["Stephan Zheng", "Yisong Yue"], "authorids": ["stephan@caltech.edu", "yyue@caltech.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791682757, "id": "ICLR.cc/2018/Conference/-/Paper631/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "HyunpgbR-", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper631/Authors", "ICLR.cc/2018/Conference/Paper631/Reviewers", "ICLR.cc/2018/Conference/Paper631/Area_Chair"], "cdate": 1512791682757}}}, {"tddate": null, "ddate": null, "tmdate": 1513997361321, "tcdate": 1513997361321, "number": 3, "cdate": 1513997361321, "id": "rJ_4ESsff", "invitation": "ICLR.cc/2018/Conference/-/Paper631/Public_Comment", "forum": "HyunpgbR-", "replyto": "HyunpgbR-", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Unclear in many ways", "comment": "We think this paper is not a clear paper. First, the definition of symbols in the paper is not clear. Many symbols used in the paper is not defined. Second, the decomposition of distribution of latent variables in the paper is not clear and looks less reasonable. Third, it is not clear how the agents observe this full states of the environment, etc. The paper states that the architecture of their approach helps the coordination of agents. But we think it is nothing but a centralized controller and we do not know the advantage of it over centralized controller method. To sum up, we think this paper is not in a high quality.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Exploration via Hierarchical Variational Policy Networks", "abstract": "Reinforcement learning in environments with large state-action spaces is challenging, as exploration can be highly inefficient. Even if the dynamics are simple, the optimal policy can be combinatorially hard to discover. In this work, we propose a hierarchical approach to structured exploration to improve the sample efficiency of on-policy exploration in large state-action spaces. The key idea is to model a stochastic policy as a hierarchical latent variable model, which can learn low-dimensional structure in the state-action space, and to define exploration by sampling from the low-dimensional latent space. This approach enables lower sample complexity, while preserving policy expressivity. In order to make learning tractable, we derive a joint learning and exploration strategy by combining hierarchical variational inference with actor-critic learning. The benefits of our learning approach are that 1) it is principled, 2) simple to implement, 3) easily scalable to settings with many actions and 4) easily composable with existing deep learning approaches. We demonstrate the effectiveness of our approach on learning a deep centralized multi-agent policy, as multi-agent environments naturally have an exponentially large state-action space. In this setting, the latent hierarchy implements a form of multi-agent coordination during exploration and execution (MACE). We demonstrate empirically that MACE can more efficiently learn optimal policies in challenging multi-agent games with a large number (~20) of agents, compared to conventional baselines. Moreover, we show that our hierarchical structure leads to meaningful agent coordination.", "pdf": "/pdf/c919771a95ab0c405ebc28a3734b750277b51dbd.pdf", "TL;DR": "Make deep reinforcement learning in large state-action spaces more efficient using structured exploration with deep hierarchical policies.", "paperhash": "zheng|structured_exploration_via_hierarchical_variational_policy_networks", "_bibtex": "@misc{\nzheng2018structured,\ntitle={Structured Exploration via Hierarchical Variational Policy Networks},\nauthor={Stephan Zheng and Yisong Yue},\nyear={2018},\nurl={https://openreview.net/forum?id=HyunpgbR-},\n}", "keywords": ["Deep Reinforcement Learning", "Structured Variational Inference", "Multi-agent Coordination", "Multi-agent Learning"], "authors": ["Stephan Zheng", "Yisong Yue"], "authorids": ["stephan@caltech.edu", "yyue@caltech.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791682757, "id": "ICLR.cc/2018/Conference/-/Paper631/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "HyunpgbR-", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper631/Authors", "ICLR.cc/2018/Conference/Paper631/Reviewers", "ICLR.cc/2018/Conference/Paper631/Area_Chair"], "cdate": 1512791682757}}}, {"tddate": null, "ddate": null, "tmdate": 1512988602159, "tcdate": 1512988602159, "number": 2, "cdate": 1512988602159, "id": "Bkz6yknZf", "invitation": "ICLR.cc/2018/Conference/-/Paper631/Public_Comment", "forum": "HyunpgbR-", "replyto": "HJITw-LWf", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "zhanzhao@umich.edu", "comment": "zhanzhao@umich.edu"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Exploration via Hierarchical Variational Policy Networks", "abstract": "Reinforcement learning in environments with large state-action spaces is challenging, as exploration can be highly inefficient. Even if the dynamics are simple, the optimal policy can be combinatorially hard to discover. In this work, we propose a hierarchical approach to structured exploration to improve the sample efficiency of on-policy exploration in large state-action spaces. The key idea is to model a stochastic policy as a hierarchical latent variable model, which can learn low-dimensional structure in the state-action space, and to define exploration by sampling from the low-dimensional latent space. This approach enables lower sample complexity, while preserving policy expressivity. In order to make learning tractable, we derive a joint learning and exploration strategy by combining hierarchical variational inference with actor-critic learning. The benefits of our learning approach are that 1) it is principled, 2) simple to implement, 3) easily scalable to settings with many actions and 4) easily composable with existing deep learning approaches. We demonstrate the effectiveness of our approach on learning a deep centralized multi-agent policy, as multi-agent environments naturally have an exponentially large state-action space. In this setting, the latent hierarchy implements a form of multi-agent coordination during exploration and execution (MACE). We demonstrate empirically that MACE can more efficiently learn optimal policies in challenging multi-agent games with a large number (~20) of agents, compared to conventional baselines. Moreover, we show that our hierarchical structure leads to meaningful agent coordination.", "pdf": "/pdf/c919771a95ab0c405ebc28a3734b750277b51dbd.pdf", "TL;DR": "Make deep reinforcement learning in large state-action spaces more efficient using structured exploration with deep hierarchical policies.", "paperhash": "zheng|structured_exploration_via_hierarchical_variational_policy_networks", "_bibtex": "@misc{\nzheng2018structured,\ntitle={Structured Exploration via Hierarchical Variational Policy Networks},\nauthor={Stephan Zheng and Yisong Yue},\nyear={2018},\nurl={https://openreview.net/forum?id=HyunpgbR-},\n}", "keywords": ["Deep Reinforcement Learning", "Structured Variational Inference", "Multi-agent Coordination", "Multi-agent Learning"], "authors": ["Stephan Zheng", "Yisong Yue"], "authorids": ["stephan@caltech.edu", "yyue@caltech.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791682757, "id": "ICLR.cc/2018/Conference/-/Paper631/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "HyunpgbR-", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper631/Authors", "ICLR.cc/2018/Conference/Paper631/Reviewers", "ICLR.cc/2018/Conference/Paper631/Area_Chair"], "cdate": 1512791682757}}}, {"tddate": null, "ddate": null, "tmdate": 1512605630358, "tcdate": 1512605630358, "number": 1, "cdate": 1512605630358, "id": "HJITw-LWf", "invitation": "ICLR.cc/2018/Conference/-/Paper631/Public_Comment", "forum": "HyunpgbR-", "replyto": "HyunpgbR-", "signatures": ["~Zhanzhan_Zhao1"], "readers": ["everyone"], "writers": ["~Zhanzhan_Zhao1"], "content": {"title": "asking for the code ", "comment": "I am a master student at Umich. We are now taking part in ICLR 2018 Reproducibility Challenge. So I wonder if I can get your code for the reproduce of the results in your paper. It is encouraged to get the code in this challenge and do some further investigations. So it will be great if you could offer us the code. My Email address is zhanzhao@umich.edu.  Thank you so much!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Exploration via Hierarchical Variational Policy Networks", "abstract": "Reinforcement learning in environments with large state-action spaces is challenging, as exploration can be highly inefficient. Even if the dynamics are simple, the optimal policy can be combinatorially hard to discover. In this work, we propose a hierarchical approach to structured exploration to improve the sample efficiency of on-policy exploration in large state-action spaces. The key idea is to model a stochastic policy as a hierarchical latent variable model, which can learn low-dimensional structure in the state-action space, and to define exploration by sampling from the low-dimensional latent space. This approach enables lower sample complexity, while preserving policy expressivity. In order to make learning tractable, we derive a joint learning and exploration strategy by combining hierarchical variational inference with actor-critic learning. The benefits of our learning approach are that 1) it is principled, 2) simple to implement, 3) easily scalable to settings with many actions and 4) easily composable with existing deep learning approaches. We demonstrate the effectiveness of our approach on learning a deep centralized multi-agent policy, as multi-agent environments naturally have an exponentially large state-action space. In this setting, the latent hierarchy implements a form of multi-agent coordination during exploration and execution (MACE). We demonstrate empirically that MACE can more efficiently learn optimal policies in challenging multi-agent games with a large number (~20) of agents, compared to conventional baselines. Moreover, we show that our hierarchical structure leads to meaningful agent coordination.", "pdf": "/pdf/c919771a95ab0c405ebc28a3734b750277b51dbd.pdf", "TL;DR": "Make deep reinforcement learning in large state-action spaces more efficient using structured exploration with deep hierarchical policies.", "paperhash": "zheng|structured_exploration_via_hierarchical_variational_policy_networks", "_bibtex": "@misc{\nzheng2018structured,\ntitle={Structured Exploration via Hierarchical Variational Policy Networks},\nauthor={Stephan Zheng and Yisong Yue},\nyear={2018},\nurl={https://openreview.net/forum?id=HyunpgbR-},\n}", "keywords": ["Deep Reinforcement Learning", "Structured Variational Inference", "Multi-agent Coordination", "Multi-agent Learning"], "authors": ["Stephan Zheng", "Yisong Yue"], "authorids": ["stephan@caltech.edu", "yyue@caltech.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791682757, "id": "ICLR.cc/2018/Conference/-/Paper631/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "HyunpgbR-", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper631/Authors", "ICLR.cc/2018/Conference/Paper631/Reviewers", "ICLR.cc/2018/Conference/Paper631/Area_Chair"], "cdate": 1512791682757}}}], "count": 13}