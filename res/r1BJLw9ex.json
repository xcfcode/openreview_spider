{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396554476, "tcdate": 1486396554476, "number": 1, "id": "Syf53zIOg", "invitation": "ICLR.cc/2017/conference/-/paper383/acceptance", "forum": "r1BJLw9ex", "replyto": "r1BJLw9ex", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "This was a borderline paper. However, no reviewers were willing to champion the acceptance of the paper during the deliberation period. Furthermore, in practice, initialization itself is a hyperparameter that gets tuned automatically. To be a compelling empirical result, it would be useful for the paper to include a comparison between the proposed initialization and a tuned arbitrary initialization scale with various tuning budgets. Additionally, other issues with the empirical evaluation brought up by the reviewers were only partially resolved in the revisions. For these reasons, the paper has been recommended for rejection."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.", "pdf": "/pdf/aed672a70c2793d881f5a2d0b526f73b60103d40.pdf", "TL;DR": "Batch Norm Incorrectly Estimates Variance When Dropout Is On", "paperhash": "hendrycks|adjusting_for_dropout_variance_in_batch_normalization_and_weight_initialization", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396555006, "id": "ICLR.cc/2017/conference/-/paper383/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "r1BJLw9ex", "replyto": "r1BJLw9ex", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396555006}}}, {"tddate": null, "tmdate": 1485183911039, "tcdate": 1484423981931, "number": 5, "id": "BJLVQWOLx", "invitation": "ICLR.cc/2017/conference/-/paper383/public/comment", "forum": "r1BJLw9ex", "replyto": "BJvkqW8El", "signatures": ["~Dan_Hendrycks1"], "readers": ["everyone"], "writers": ["~Dan_Hendrycks1"], "content": {"title": "Response", "comment": "Thank you for your analysis of our paper.\n\nWe have added more detail to our synthetic experiment set-up in the new paper.\n\nFollowing your suggestion about more random hyperparameter search, we re-ran the MNIST experiments tuning over {10^-3, 10^-4, 10^-5} and randomly drew 7 learning rates from 10^Unif[-1,-5]. We then selected the best curves based on validation results. The plots have been updated to include test log loss curves as well.\n\nYour suggestion about tuning the learning rate with random hyperparameter search was indeed executed for the Adam and Nesterov experiments. Other parameters (depth, number of filters, etc.) were inherited from the popular VGG Net implementation. Note that He et al. [1] demonstrate that small weight initialization scalars leave some networks untrainable when there is no batch normalization (Figure 3), so to us it is not unreasonable to expect that different initializations in our Nesterov experiment have noticeably distinct convergence curves.\n\n> However there is no result for 'original with BN update' therefore it is not clear whether the BN update helps in general or not. SVHN also does not have result for original with BN update.\n\nThis is because the \"original with BN update\" values are from the original DenseNet paper [2], and we do not have their model saved at the midpoint of training. For that reason we re-ran their experiments with and without the Batch Normalization update all while using the setup described in their paper. The \"Ours\" rows show the consequence of using Batch Normalization re-estimation method and the test error without the re-estimation. Note that this lets us see the exact effects of Batch Normalization variance re-estimation because we show how the model performs on the test set and how that _same_ model performs if we only modify the model's Batch Normalization variance parameters keeping all other parameters the same.\n\n> The CIFAR100 difference is not significant without including batch normalization variance re-estimation. \n\nThe CIFAR-100 experiment is designed to show the effect of batch normalization variance re-estimation. Without the batch normalization variance re-estimation, there are no appreciable differences between the original setup and our experiments. We are not testing our weight initialization in this section, only the batch normalization variance re-estimation.\n\nThank you for helping the paper become stronger.\n\n\nUpdate: Anonymous Reviewer 2 edited their initial review on Jan 23rd to include new concerns.\n----------------------------------------------------------------------------------------------------------------------------\n\n> compare the initialization to batch-normalization\nWe compare our initialization to other initializations involving simple scalars, as many still use \"Xavier\" and \"He\" initializations. Batch Normalization significantly reduces the need for a good initialization, so we give the initialization and batch normalization dropout correction methods separate sections. It is still worthwhile to consider the simple scalar initializations because people may opt not to use Batch Normalization if they want to save computational time, do online learning, or train a ConvNet for reinforcement learning (since Batch Normalization introduces much noise to training in RL). This is why we show how to correct for dropout's influence against other initializations with simple scalars (Xavier, He), and then we show how to correct for dropout's influence in Batch Normalization separately.\n\n> Dense Net scale to many depths, ... it is not clear if there is going to be any empirical gain for that \nWe trained on a 100-layer DenseNet with growth rate k=12, shown in Table 2, and this has 7 million parameters. Larger DenseNets are almost never more than twice this depth. Their larger DenseNet required 26GB of memory and to be trained on 4 Titan X's, according to a correspondence with Zhuang Liu, the DenseNet author. This was simply too bloated for us to train, and we were still able to surpass its performance with batch normalization variance re-estimation.\n\n> Table 2 is not significant (paraphrase)\nWe respectfully disagree. Going from 5.77% to 5.38% (note we are approaching an accuracy ceiling) and 23.79% to 22.17% (both state of the art) with a simple, quick, and highly general trick is significant to us.\n\n\n[1] Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. https://arxiv.org/abs/1502.01852\n[2] Densely Connected Convolutional Networks. https://arxiv.org/pdf/1608.06993v1.pdf"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.", "pdf": "/pdf/aed672a70c2793d881f5a2d0b526f73b60103d40.pdf", "TL;DR": "Batch Norm Incorrectly Estimates Variance When Dropout Is On", "paperhash": "hendrycks|adjusting_for_dropout_variance_in_batch_normalization_and_weight_initialization", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287597641, "id": "ICLR.cc/2017/conference/-/paper383/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1BJLw9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper383/reviewers", "ICLR.cc/2017/conference/paper383/areachairs"], "cdate": 1485287597641}}}, {"tddate": null, "tmdate": 1485161425212, "tcdate": 1485161425212, "number": 6, "id": "S1FRQr7Px", "invitation": "ICLR.cc/2017/conference/-/paper383/official/comment", "forum": "r1BJLw9ex", "replyto": "BJLVQWOLx", "signatures": ["ICLR.cc/2017/conference/paper383/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper383/AnonReviewer2"], "content": {"title": "response after the rebuttal ", "comment": "Despite clarifications and revisions, the empirical analysis of the paper is still unsatisfactory. For initialization experiments, Figure 1 and 2 are restricted to mnist;  only considering VGG  Net for Figure 3 and 4 is restrictive. The initialization should be also compared to previous work   (Ioffe & Szegedy, 2015; Mishkin & Matas, 2015; Salimans & Kingma, 2016). For the normalization variance reestimation, Table 2 does not demonstrate significant performance gain;  ImageNet results could help.  Also results with data augmentation should be given for the sake of completeness. A comprehensive analysis like the one in (Mishkin et al, 2016 ) is required to judge whether proposed approaches give a significant gain in practice."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.", "pdf": "/pdf/aed672a70c2793d881f5a2d0b526f73b60103d40.pdf", "TL;DR": "Batch Norm Incorrectly Estimates Variance When Dropout Is On", "paperhash": "hendrycks|adjusting_for_dropout_variance_in_batch_normalization_and_weight_initialization", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287597488, "id": "ICLR.cc/2017/conference/-/paper383/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "r1BJLw9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper383/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper383/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper383/reviewers", "ICLR.cc/2017/conference/paper383/areachairs"], "cdate": 1485287597488}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1485031066416, "tcdate": 1478288861357, "number": 383, "id": "r1BJLw9ex", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "r1BJLw9ex", "signatures": ["~Dan_Hendrycks1"], "readers": ["everyone"], "content": {"title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.", "pdf": "/pdf/aed672a70c2793d881f5a2d0b526f73b60103d40.pdf", "TL;DR": "Batch Norm Incorrectly Estimates Variance When Dropout Is On", "paperhash": "hendrycks|adjusting_for_dropout_variance_in_batch_normalization_and_weight_initialization", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 15, "writable": false, "overwriting": ["S1AtgaPug"], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1485030199624, "tcdate": 1485030199624, "number": 7, "id": "S1eS7S-vx", "invitation": "ICLR.cc/2017/conference/-/paper383/public/comment", "forum": "r1BJLw9ex", "replyto": "B10FtuC8e", "signatures": ["~Dan_Hendrycks1"], "readers": ["everyone"], "writers": ["~Dan_Hendrycks1"], "content": {"title": "Response", "comment": "> This can be seen as a more sophisticated version of the naive standard practice of multiplying weights by 1/p at test time.\n\nWe multiply by 1/p during training and not during testing, as it is the default in Tensorflow's and Lasagne's dropout implementation. Multiplying by 1/p during training corrects the mean but not the variance, as shown in our derivation section. Thus before we adjust Batch Normalization variance parameters we already have the mean corrected, and this mean correction could have been accomplished with the naive 1/p rescale or Monte Carlo dropout. Consequently, our variance adjustment is not serving as a sophisticated stand-in for a 1/p rescale since that rescale accounts the mean and our BN variance adjustment accounts for the variance."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.", "pdf": "/pdf/aed672a70c2793d881f5a2d0b526f73b60103d40.pdf", "TL;DR": "Batch Norm Incorrectly Estimates Variance When Dropout Is On", "paperhash": "hendrycks|adjusting_for_dropout_variance_in_batch_normalization_and_weight_initialization", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287597641, "id": "ICLR.cc/2017/conference/-/paper383/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1BJLw9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper383/reviewers", "ICLR.cc/2017/conference/paper383/areachairs"], "cdate": 1485287597641}}}, {"tddate": null, "tmdate": 1484847493561, "tcdate": 1484847493561, "number": 5, "id": "B10FtuC8e", "invitation": "ICLR.cc/2017/conference/-/paper383/official/comment", "forum": "r1BJLw9ex", "replyto": "ryfwNeOUl", "signatures": ["ICLR.cc/2017/conference/paper383/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper383/AnonReviewer3"], "content": {"title": "clarification", "comment": "You adjust the batch normalization parameters after training, to account for the fact that you use dropout during training but not during testing. This can be seen as a more sophisticated version of the naive standard practice of multiplying weights by 1/p at test time, for dropout probability p. For the latter, we know that Monte Carlo dropout (using dropout at test time, but averaging over many random draws) works better: how does this compare to your method of adjusting the parameters?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.", "pdf": "/pdf/aed672a70c2793d881f5a2d0b526f73b60103d40.pdf", "TL;DR": "Batch Norm Incorrectly Estimates Variance When Dropout Is On", "paperhash": "hendrycks|adjusting_for_dropout_variance_in_batch_normalization_and_weight_initialization", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287597488, "id": "ICLR.cc/2017/conference/-/paper383/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "r1BJLw9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper383/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper383/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper383/reviewers", "ICLR.cc/2017/conference/paper383/areachairs"], "cdate": 1485287597488}}}, {"tddate": null, "tmdate": 1484424402385, "tcdate": 1484424402385, "number": 6, "id": "rJc0NZdUx", "invitation": "ICLR.cc/2017/conference/-/paper383/public/comment", "forum": "r1BJLw9ex", "replyto": "r1BJLw9ex", "signatures": ["~Dan_Hendrycks1"], "readers": ["everyone"], "writers": ["~Dan_Hendrycks1"], "content": {"title": "Paper Update", "comment": "We have updated the paper. For the updated paper, we re-ran the MNIST experiments with random hyperparameter search and now plot the log-loss of the test set in addition to the training set log-loss. We also added more detail about the synthetic experiment set-up. Last, we included a plot showing the Frobenius norm of the gradient as training progresses under different weight initializations.\nThank you all for your suggestions!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.", "pdf": "/pdf/aed672a70c2793d881f5a2d0b526f73b60103d40.pdf", "TL;DR": "Batch Norm Incorrectly Estimates Variance When Dropout Is On", "paperhash": "hendrycks|adjusting_for_dropout_variance_in_batch_normalization_and_weight_initialization", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287597641, "id": "ICLR.cc/2017/conference/-/paper383/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1BJLw9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper383/reviewers", "ICLR.cc/2017/conference/paper383/areachairs"], "cdate": 1485287597641}}}, {"tddate": null, "tmdate": 1484420186503, "tcdate": 1484420186503, "number": 4, "id": "ryfwNeOUl", "invitation": "ICLR.cc/2017/conference/-/paper383/public/comment", "forum": "r1BJLw9ex", "replyto": "r10W8r-4e", "signatures": ["~Dan_Hendrycks1"], "readers": ["everyone"], "writers": ["~Dan_Hendrycks1"], "content": {"title": "Response", "comment": "Thank you for your analysis of our paper.\n\nIn our experiments, we do not enable dropout during test time since, as we all know, dropout at test time hurts performance. Then we do not need to correct for variance _during_ testing. We correct for the variance of dropout applied in training either by weight initialization used during training, or batch normalization parameter re-estimation after normal training but before testing. These corrections allow for better training (weight initialization correction) or fixes the trained parameters (batch normalization correction), which affect test time performance. If we are misunderstanding your last two points, let us know."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.", "pdf": "/pdf/aed672a70c2793d881f5a2d0b526f73b60103d40.pdf", "TL;DR": "Batch Norm Incorrectly Estimates Variance When Dropout Is On", "paperhash": "hendrycks|adjusting_for_dropout_variance_in_batch_normalization_and_weight_initialization", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287597641, "id": "ICLR.cc/2017/conference/-/paper383/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1BJLw9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper383/reviewers", "ICLR.cc/2017/conference/paper383/areachairs"], "cdate": 1485287597641}}}, {"tddate": null, "tmdate": 1484418932176, "tcdate": 1484418932176, "number": 3, "id": "S12uJld8x", "invitation": "ICLR.cc/2017/conference/-/paper383/public/comment", "forum": "r1BJLw9ex", "replyto": "H1CLEkeEg", "signatures": ["~Dan_Hendrycks1"], "readers": ["everyone"], "writers": ["~Dan_Hendrycks1"], "content": {"title": "Response", "comment": "Thank you for your analysis of our paper.\n\nIn light of your review, we added a figure showing the differences in gradient norms using distinct initializations."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.", "pdf": "/pdf/aed672a70c2793d881f5a2d0b526f73b60103d40.pdf", "TL;DR": "Batch Norm Incorrectly Estimates Variance When Dropout Is On", "paperhash": "hendrycks|adjusting_for_dropout_variance_in_batch_normalization_and_weight_initialization", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287597641, "id": "ICLR.cc/2017/conference/-/paper383/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1BJLw9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper383/reviewers", "ICLR.cc/2017/conference/paper383/areachairs"], "cdate": 1485287597641}}}, {"tddate": null, "tmdate": 1482197471217, "tcdate": 1482197471217, "number": 3, "id": "BJvkqW8El", "invitation": "ICLR.cc/2017/conference/-/paper383/official/review", "forum": "r1BJLw9ex", "replyto": "r1BJLw9ex", "signatures": ["ICLR.cc/2017/conference/paper383/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper383/AnonReviewer2"], "content": {"title": "results are not convincing", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposes new initialization for particular architectures and a correction trick to batch normalization to correct variance introduced by dropout. While authors state interesting observations, the claims are not supported with convincing results.\n\nI guess Figure 1 is only for mnist and for only two values of p with one particular network architecture, the dataset and empirical setup is not clear.\n\nThe convergence is demonstrated only for three dropout values in Figure 2 which may cause an unfair comparison. For instance how does the convergence compare for the best dropout rate after cross-validation (three figures each figure has three results for one method with different dropouts [bests cv result for each one])? Also how is the corresponding validation error and test iterations?  Also only mnist does not have to generalize to other benchmarks.\n\nFigure 3 gives closer results for Adam optimizer, learning rate is not selected with random search or bayesian optimization, learning decay iterations fixed and regularization coefficient is set to a small value without tuning. A slightly better tuning of parameters may close the current gap. Also Nesterov based competitor gives unreasonably worse accuracy compared to recent results which may indicate that this experiment should not be taken into account. \n\nIn Table 2, there is no significant improvement on CIFAR10. The CIFAR100 difference is not significant without including batch normalization variance re-estimation. However there is no result for 'original with BN update' therefore it is not clear whether the BN update helps in general or not. SVHN also does not have result for original with BN update.\n\nThere should be baselines with batch normalizations for Figure 1,2 3 to support the claims convincingly.  The main criticism about batch normalization is additional computational cost by giving (Mishkin et al, 2016 ) as reference however this should not be a reason to not to compare the initialization to batch-normalization.  In fact, (Mishkin et al, 2016) performs comparison to batch normalization and also with and without data augmentation with recent state of art architectures.\n\nNone of the empirical results have data augmentation. It is not clear if the initialization or  batch normalization update will help or make it worse for that case.\n\nRecent state of art methods methods like Res Net variant and Dense Net scale to many depths and report result for ImageNet. Although the authors claim that this can be extended to residual network variants, it is not clear if there is going to be any empirical gain for that architectures.  \n\nThis work requires a comprehensive and fair comparison. Otherwise the contribution is not significant.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.", "pdf": "/pdf/aed672a70c2793d881f5a2d0b526f73b60103d40.pdf", "TL;DR": "Batch Norm Incorrectly Estimates Variance When Dropout Is On", "paperhash": "hendrycks|adjusting_for_dropout_variance_in_batch_normalization_and_weight_initialization", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512603073, "id": "ICLR.cc/2017/conference/-/paper383/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper383/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper383/AnonReviewer1", "ICLR.cc/2017/conference/paper383/AnonReviewer3", "ICLR.cc/2017/conference/paper383/AnonReviewer2"], "reply": {"forum": "r1BJLw9ex", "replyto": "r1BJLw9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper383/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper383/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512603073}}}, {"tddate": null, "tmdate": 1481885189936, "tcdate": 1481885189936, "number": 2, "id": "r10W8r-4e", "invitation": "ICLR.cc/2017/conference/-/paper383/official/review", "forum": "r1BJLw9ex", "replyto": "r1BJLw9ex", "signatures": ["ICLR.cc/2017/conference/paper383/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper383/AnonReviewer3"], "content": {"title": "Interesting observation about the usefulness of adjusting for dropout variance that people should know", "rating": "7: Good paper, accept", "review": "The main observation made in the paper is that the use of dropout increases the variance of neurons. Correcting for this increase in variance, in the parameter initialization, and in the test-time statistics of batch normalization, improves performance, as is shown reasonably convincingly in the experiments.\n\nThis observation is important, as it applies to many of the models used in the literature. It's not extremely novel (it's been observed in the literature before that our simple dropout approximations at test time do not achieve the accuracy obtained by full Monte Carlo dropout)\n\nThe paper could use more experimental validation. Specifically:\n\n- I'm guessing the correction for dropout variance at test time is not only specific to batch normalization: Standard dropout, in networks without batch normalization, corrects only for the mean at test time (by dividing activations by one minus the dropout probability). This work suggests it would be beneficial to also correct for the variance. Has this been tested?\n\n-  How does the dropout variance correction compare to using Monte Carlo dropout at test time? (i.e. just averaging over a large number of random dropout masks)", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.", "pdf": "/pdf/aed672a70c2793d881f5a2d0b526f73b60103d40.pdf", "TL;DR": "Batch Norm Incorrectly Estimates Variance When Dropout Is On", "paperhash": "hendrycks|adjusting_for_dropout_variance_in_batch_normalization_and_weight_initialization", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512603073, "id": "ICLR.cc/2017/conference/-/paper383/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper383/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper383/AnonReviewer1", "ICLR.cc/2017/conference/paper383/AnonReviewer3", "ICLR.cc/2017/conference/paper383/AnonReviewer2"], "reply": {"forum": "r1BJLw9ex", "replyto": "r1BJLw9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper383/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper383/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512603073}}}, {"tddate": null, "tmdate": 1481794646474, "tcdate": 1481794646465, "number": 1, "id": "H1CLEkeEg", "invitation": "ICLR.cc/2017/conference/-/paper383/official/review", "forum": "r1BJLw9ex", "replyto": "r1BJLw9ex", "signatures": ["ICLR.cc/2017/conference/paper383/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper383/AnonReviewer1"], "content": {"title": "Interesting initialization approach together with a simple inference trick to improve accuracy. Due to limitied experimental validation and theoritical analysis, hard to judge the contribution.", "rating": "6: Marginally above acceptance threshold", "review": "The paper presents an approach for compensating the input/activation variance introduced by dropout in a network. Additionally, a practical inference trick of re-estimating the batch normalization parameters with dropout turned off before testing. \n\nThe authors very well show how dropout influences the input/activation variance and then scale the initial weights accordingly to achieve unit variance which helps in avoiding activation outputs exploding or vanishing. It is shown that the presented approach serves as a good initialization technique for deep networks and results in performances op par or slightly better than the existing approaches. The limited experimental validation and only small difference in accuracies compared to existing methods makes it difficult to judge the effectiveness of presented approach. Perhaps observing the statistics of output activations and gradients over training epochs in multiple experiments can better support the argument of stability of the network using proposed approach.\n\nAuthors might consider adding some validation for considering the backpropagation variance. On multiple occasions comparison is drawn against batch normalization which I believe does much more than a weight initialization technique. The presented approach is a good initialization technique just not sure if its better than existing ones.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.", "pdf": "/pdf/aed672a70c2793d881f5a2d0b526f73b60103d40.pdf", "TL;DR": "Batch Norm Incorrectly Estimates Variance When Dropout Is On", "paperhash": "hendrycks|adjusting_for_dropout_variance_in_batch_normalization_and_weight_initialization", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512603073, "id": "ICLR.cc/2017/conference/-/paper383/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper383/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper383/AnonReviewer1", "ICLR.cc/2017/conference/paper383/AnonReviewer3", "ICLR.cc/2017/conference/paper383/AnonReviewer2"], "reply": {"forum": "r1BJLw9ex", "replyto": "r1BJLw9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper383/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper383/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512603073}}}, {"tddate": null, "tmdate": 1481600656520, "tcdate": 1481600656514, "number": 2, "id": "H1Oc0JTQl", "invitation": "ICLR.cc/2017/conference/-/paper383/public/comment", "forum": "r1BJLw9ex", "replyto": "BJRjfRgXg", "signatures": ["~Dan_Hendrycks1"], "readers": ["everyone"], "writers": ["~Dan_Hendrycks1"], "content": {"title": "RE: about the setup and comparisons", "comment": "Thank you for the questions. To our understanding, He and Xavier weight initializations are not at all suited for ResNets unless one uses Batch Normalization. This is because the argument behind the design of the He initialization assumes each layer depends only on the single previous one, but ResNets receive inputs from the main connection and the residual connection. Should anyone develop a weight initialization for ResNets, this work would still be of help because we specify how to adjust for an arbitrary nonlinearity's contractiveness and how to adjust for dropout.\n\nWe do not have ImageNet results due to ImageNet\u2019s computational demands, and this is why the manuscript was not submitted to a more numbers-based conference.\n\nThe CIFAR-10 VGGNet inherited most of its hyperparameters (l2 decay, dropout, network structure, etc.) from this popular implementation: https://github.com/szagoruyko/cifar.torch/blob/master/models/vgg_bn_drop.lua But in light of your comment we now trained with 7 randomly selected learning rates and 3 determinsitically chosen rates per optimizer, and the results are in the updated manuscript. We have improved the manuscript thanks to your comment."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.", "pdf": "/pdf/aed672a70c2793d881f5a2d0b526f73b60103d40.pdf", "TL;DR": "Batch Norm Incorrectly Estimates Variance When Dropout Is On", "paperhash": "hendrycks|adjusting_for_dropout_variance_in_batch_normalization_and_weight_initialization", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287597641, "id": "ICLR.cc/2017/conference/-/paper383/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1BJLw9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper383/reviewers", "ICLR.cc/2017/conference/paper383/areachairs"], "cdate": 1485287597641}}}, {"tddate": null, "tmdate": 1481600568985, "tcdate": 1481600568980, "number": 1, "id": "HkZSA1Tml", "invitation": "ICLR.cc/2017/conference/-/paper383/public/comment", "forum": "r1BJLw9ex", "replyto": "Bk0fmWJ7e", "signatures": ["~Dan_Hendrycks1"], "readers": ["everyone"], "writers": ["~Dan_Hendrycks1"], "content": {"title": "RE: questions on experimental observations and writing", "comment": "Thank you for the questions. The results in Figure 1 are from a deeper fully-connected network, while those from Figure 2 are from a shallower network trained on MNIST. Figure 1 illustrates that He and Xavier initializations can lead to exponential blowups and decays. As a result, deeper networks may take longer to train because the network must learn to stabilize the signal or the network will not learn. For this reason, Figure 3 shows that it takes longer for the He initialization to converge because several epochs are spent learning to stabilize the feedforward signal, and the He initialization failed to converge at more learning rates than when we used our initialization in the CIFAR-10 experiment.\n\nWe saw that the variance of a neuron's input grows when dropout is on inside the Weight Initialization derivation section. Consequently, Batch Normalization's variance estimates are greater when dropout is on because a neuron's input variance is greater with dropout. But dropout is off during testing, and the variance estimates BN normally uses are valid when dropout is on not off. For this reason we re-estimate the batch normalization variance by simply turning dropout off and feeding forward the training data. Then the variance re-estimates are accurate for when dropout is off, so the variance estimates are accurate for testing. We have added an expanded explanation in the manuscript thanks to your comment."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.", "pdf": "/pdf/aed672a70c2793d881f5a2d0b526f73b60103d40.pdf", "TL;DR": "Batch Norm Incorrectly Estimates Variance When Dropout Is On", "paperhash": "hendrycks|adjusting_for_dropout_variance_in_batch_normalization_and_weight_initialization", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287597641, "id": "ICLR.cc/2017/conference/-/paper383/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1BJLw9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper383/reviewers", "ICLR.cc/2017/conference/paper383/areachairs"], "cdate": 1485287597641}}}, {"tddate": null, "tmdate": 1480807077807, "tcdate": 1480807077799, "number": 2, "id": "BJRjfRgXg", "invitation": "ICLR.cc/2017/conference/-/paper383/official/comment", "forum": "r1BJLw9ex", "replyto": "r1BJLw9ex", "signatures": ["ICLR.cc/2017/conference/paper383/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper383/AnonReviewer2"], "content": {"title": "about the setup and comparisons", "comment": "-Why don't you compare to also recent state of art methods like resnet variants or denseNet  for Section 2?\n- The parameters are set to fix values of selected from small set of values. However tuning with random search or bayesian optimization is the two common ways to obtain meaningful comparisons. At this stage, it is hard to see whether the difference is coming from proposed approaches since the parameters are not fine tuned reasonably.\n\n- Are there any results on Imagenet?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.", "pdf": "/pdf/aed672a70c2793d881f5a2d0b526f73b60103d40.pdf", "TL;DR": "Batch Norm Incorrectly Estimates Variance When Dropout Is On", "paperhash": "hendrycks|adjusting_for_dropout_variance_in_batch_normalization_and_weight_initialization", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287597488, "id": "ICLR.cc/2017/conference/-/paper383/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "r1BJLw9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper383/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper383/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper383/reviewers", "ICLR.cc/2017/conference/paper383/areachairs"], "cdate": 1485287597488}}}, {"tddate": null, "tmdate": 1480688406587, "tcdate": 1480688406431, "number": 1, "id": "Bk0fmWJ7e", "invitation": "ICLR.cc/2017/conference/-/paper383/pre-review/question", "forum": "r1BJLw9ex", "replyto": "r1BJLw9ex", "signatures": ["ICLR.cc/2017/conference/paper383/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper383/AnonReviewer1"], "content": {"title": "questions on experimental observations and writing", "question": "Figure 1 shows exponential blowups and decay for Xavier initialization but the Log-loss decay over epochs from Figure 2 looks very smooth and similar to that of presented approach. Can you please comment on this?\n\nWhy does batch normalization variance needs to be updated before testing?\n\nHow re-estimation of batch normalization variance after finishing training help perform better on test set?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.", "pdf": "/pdf/aed672a70c2793d881f5a2d0b526f73b60103d40.pdf", "TL;DR": "Batch Norm Incorrectly Estimates Variance When Dropout Is On", "paperhash": "hendrycks|adjusting_for_dropout_variance_in_batch_normalization_and_weight_initialization", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959309566, "id": "ICLR.cc/2017/conference/-/paper383/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper383/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper383/AnonReviewer1"], "reply": {"forum": "r1BJLw9ex", "replyto": "r1BJLw9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper383/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper383/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959309566}}}], "count": 16}