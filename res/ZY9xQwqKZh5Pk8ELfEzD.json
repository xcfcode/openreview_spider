{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458242395234, "tcdate": 1458242395234, "id": "ROV4j1QAWCvnM0J1Ip0q", "invitation": "ICLR.cc/2016/workshop/-/paper/129/comment", "forum": "ZY9xQwqKZh5Pk8ELfEzD", "replyto": "VAVwRrNJ2Tx0Wk76TAQ7", "signatures": ["~Timothy_Draelos1"], "readers": ["everyone"], "writers": ["~Timothy_Draelos1"], "content": {"title": "Response to review", "comment": "The authors thank this reviewer for noting the positive contributions of our work. We do intend to explore our basic neurogenic deep learning ideas applied to convolutional neural networks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Neurogenic Deep Learning", "abstract": "Deep neural networks (DNNs) have achieved remarkable success on complex data processing tasks. In contrast to biological neural systems, capable of learning continuously, DNNs have a limited ability to incorporate new information in a trained network. Therefore, methods for continuous learning are potentially highly impactful in enabling the application of DNNs to dynamic data sets. Inspired by adult neurogenesis in the hippocampus, we explore the potential for adding new nodes to layers of artificial neural networks to facilitate their acquisition of novel information while preserving previously trained data representations. Our results demonstrate that neurogenesis is well suited for addressing the stability-plasticity dilemma that has long challenged adaptive machine learning algorithms. ", "pdf": "/pdf/ZY9xQwqKZh5Pk8ELfEzD.pdf", "paperhash": "draelos|neurogenic_deep_learning", "conflicts": ["sandia.gov"], "authors": ["Timothy J. Draelos", "Nadine E. Miner", "Jonathan A. Cox", "Christopher C. Lamb", "Conrad D. James", "James B. Aimone"], "authorids": ["tjdrael@sandia.gov", "nrminer@sandia.gov", "jacox@sandia.gov", "cclamb@sandia.gov", "cdjame@sandia.gov", "jbaimong@sandia.gov"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455825056765, "ddate": null, "super": null, "final": null, "tcdate": 1455825056765, "id": "ICLR.cc/2016/workshop/-/paper/129/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "ZY9xQwqKZh5Pk8ELfEzD", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/129/reviewer"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458242237061, "tcdate": 1458242237061, "id": "p8jOLkwwYSnQVOGWfpkG", "invitation": "ICLR.cc/2016/workshop/-/paper/129/comment", "forum": "ZY9xQwqKZh5Pk8ELfEzD", "replyto": "OM0WY6zK4ip57ZJjtN5Y", "signatures": ["~Timothy_Draelos1"], "readers": ["everyone"], "writers": ["~Timothy_Draelos1"], "content": {"title": "Responses to weaknesses.", "comment": "The authors thank this reviewer for the positive comments and the identification of a couple weakness of our work being the lack of testing of hyperparameters and the lack of another dataset beyond MNIST. It is true that we have not rigorously tested our hyperparameter values, so we do not fully understand the sensitivities to these parameters nor do we know whether we have reached optimal performance. We intend to perform additional experiments to resolve this issue. Regarding additional datasets to use for this work, we are preparing to apply neurogenic deep learning to the CIFAR-10 dataset and hope to present results in the poster presented at ICLR. We will revise the paper to include references related to biological connections of our work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Neurogenic Deep Learning", "abstract": "Deep neural networks (DNNs) have achieved remarkable success on complex data processing tasks. In contrast to biological neural systems, capable of learning continuously, DNNs have a limited ability to incorporate new information in a trained network. Therefore, methods for continuous learning are potentially highly impactful in enabling the application of DNNs to dynamic data sets. Inspired by adult neurogenesis in the hippocampus, we explore the potential for adding new nodes to layers of artificial neural networks to facilitate their acquisition of novel information while preserving previously trained data representations. Our results demonstrate that neurogenesis is well suited for addressing the stability-plasticity dilemma that has long challenged adaptive machine learning algorithms. ", "pdf": "/pdf/ZY9xQwqKZh5Pk8ELfEzD.pdf", "paperhash": "draelos|neurogenic_deep_learning", "conflicts": ["sandia.gov"], "authors": ["Timothy J. Draelos", "Nadine E. Miner", "Jonathan A. Cox", "Christopher C. Lamb", "Conrad D. James", "James B. Aimone"], "authorids": ["tjdrael@sandia.gov", "nrminer@sandia.gov", "jacox@sandia.gov", "cclamb@sandia.gov", "cdjame@sandia.gov", "jbaimong@sandia.gov"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455825056765, "ddate": null, "super": null, "final": null, "tcdate": 1455825056765, "id": "ICLR.cc/2016/workshop/-/paper/129/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "ZY9xQwqKZh5Pk8ELfEzD", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/129/reviewer"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458242092308, "tcdate": 1458242092308, "id": "gZ9BMZ87QtAPowrRUAK2", "invitation": "ICLR.cc/2016/workshop/-/paper/129/comment", "forum": "ZY9xQwqKZh5Pk8ELfEzD", "replyto": "ZY9xQwqKZh5Pk8ELfEzD", "signatures": ["~Timothy_Draelos1"], "readers": ["everyone"], "writers": ["~Timothy_Draelos1"], "content": {"title": "Clarifications, responses to 3 concerns", "comment": "This reviewer raises some concerns of the paper regarding our communication of: 1) transfer learning (TL), 2) intrinsic replay (IR), and 3) control experiments. We will amend the paper in line with the following comments.\n\n1) On further reflection, we believe our experiments, as well as the application of our work, is related more to Online Learning (OL) than to TL. Our objective is to adaptively learn to represent all new presented data instances from old and/or new data classes. Our experiments demonstrate this in batch form by presenting all data instances in a given class, but it is just as applicable to a streaming scenario. We are aware of DNNs successes in transfer learning, but our focus has been on situations where that isn\u2019t the case. DNNs have definite advantages in transfer learning problems, with their general-purpose feature detectors at shallow layers of a network, but our goal is to identify inputs that a trained network finds difficult to represent, whether from a different, new class or not. In this regard, we are addressing OL and have revised the paper accordingly.\n\n2) The choice of using a multivariate Gaussian distribution to represent top layer codes is an initial conjecture and the subject of a separate research effort, but it worked well in support of our neurogenesis work. Before neurogenesis is performed, data samples from each old data class are generated and used in the neurogenesis process AND in updating the distribution of the top layer codes after neurogenesis. We revised the paper to include information related to the previous sentence.\n\n3) Regarding the experimental control networks, there is one control network for the neurogenesis network when using IR (NG+IR) and a different control network for neurogenesis network when not using IR (NG). We first train a control network using all training samples of digits 1 and 7, starting with random weights. This network is the same size as the one created during neurogenesis. Then, to perform OL, all samples from a single, new digit (first 0, then 2, then 3, then 4, then 5, then 6, then 8, and finally 9) are presented and training occurs for all weights in the autoencoder for a fixed number of epochs. In the paper, the network (NG) trained with neurogenesis, but not with IR, is also considered a control network for the NG+IR network. The paper has been revised to clarify the experiments.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Neurogenic Deep Learning", "abstract": "Deep neural networks (DNNs) have achieved remarkable success on complex data processing tasks. In contrast to biological neural systems, capable of learning continuously, DNNs have a limited ability to incorporate new information in a trained network. Therefore, methods for continuous learning are potentially highly impactful in enabling the application of DNNs to dynamic data sets. Inspired by adult neurogenesis in the hippocampus, we explore the potential for adding new nodes to layers of artificial neural networks to facilitate their acquisition of novel information while preserving previously trained data representations. Our results demonstrate that neurogenesis is well suited for addressing the stability-plasticity dilemma that has long challenged adaptive machine learning algorithms. ", "pdf": "/pdf/ZY9xQwqKZh5Pk8ELfEzD.pdf", "paperhash": "draelos|neurogenic_deep_learning", "conflicts": ["sandia.gov"], "authors": ["Timothy J. Draelos", "Nadine E. Miner", "Jonathan A. Cox", "Christopher C. Lamb", "Conrad D. James", "James B. Aimone"], "authorids": ["tjdrael@sandia.gov", "nrminer@sandia.gov", "jacox@sandia.gov", "cclamb@sandia.gov", "cdjame@sandia.gov", "jbaimong@sandia.gov"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455825056765, "ddate": null, "super": null, "final": null, "tcdate": 1455825056765, "id": "ICLR.cc/2016/workshop/-/paper/129/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "ZY9xQwqKZh5Pk8ELfEzD", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/129/reviewer"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457731497707, "tcdate": 1457731497707, "id": "Qn8YNovLBIkB2l8pUYAl", "invitation": "ICLR.cc/2016/workshop/-/paper/129/review/11", "forum": "ZY9xQwqKZh5Pk8ELfEzD", "replyto": "ZY9xQwqKZh5Pk8ELfEzD", "signatures": ["ICLR.cc/2016/workshop/paper/129/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/129/reviewer/11"], "content": {"title": "Paper on interesting heuristic idea but with experimental details missing.", "rating": "5: Marginally below acceptance threshold", "review": "This paper presents an algorithm for growing the representational capacity of an autoencoder (AE) network in a data-dependent way. The idea is interesting, but the presentation is rough (as might be expected for a workshop submission). For example, see the several below sections:\n\n> DNNs are \u2026 [not] well suited for transfer learning (TL)\n\nActually, many of the results shown in hundreds of papers over the last few years have been built on the effectiveness of transfer learning using DNNs.\n\n> Samples from old classes are generated via hippocampus- inspired \u201dintrinsic replay\u201d (IR) by retrieving a high-level representation through sampling from the multivariate Normal and Cholesky decomposition of the top layer of the full encoder network and then leveraging the full decoder to reconstruct new data points from that previously trained class.\n\nPlease fill in additional details here. Is a multivariate Gaussian distribution being fit to the top layer code? If so, how is it updated to account for the new units, particularly if the old data is not used? Is there any reason to suspect a Gaussian distribution is remotely a good fit to the layer code?\n\n> Control 1 (TL+IR) - an AE trained first on the subset digits 1 and 7 and then retrained with one new single digit at a time with standard TL,\n\nWhat is \u201cstandard TL\u201d?? There is not a single standard transfer learning setup. More details need to be provided about what is actually happening in the Control experiments; without them the results are hard or impossible to interpret.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Neurogenic Deep Learning", "abstract": "Deep neural networks (DNNs) have achieved remarkable success on complex data processing tasks. In contrast to biological neural systems, capable of learning continuously, DNNs have a limited ability to incorporate new information in a trained network. Therefore, methods for continuous learning are potentially highly impactful in enabling the application of DNNs to dynamic data sets. Inspired by adult neurogenesis in the hippocampus, we explore the potential for adding new nodes to layers of artificial neural networks to facilitate their acquisition of novel information while preserving previously trained data representations. Our results demonstrate that neurogenesis is well suited for addressing the stability-plasticity dilemma that has long challenged adaptive machine learning algorithms. ", "pdf": "/pdf/ZY9xQwqKZh5Pk8ELfEzD.pdf", "paperhash": "draelos|neurogenic_deep_learning", "conflicts": ["sandia.gov"], "authors": ["Timothy J. Draelos", "Nadine E. Miner", "Jonathan A. Cox", "Christopher C. Lamb", "Conrad D. James", "James B. Aimone"], "authorids": ["tjdrael@sandia.gov", "nrminer@sandia.gov", "jacox@sandia.gov", "cclamb@sandia.gov", "cdjame@sandia.gov", "jbaimong@sandia.gov"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580020055, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580020055, "id": "ICLR.cc/2016/workshop/-/paper/129/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "ZY9xQwqKZh5Pk8ELfEzD", "replyto": "ZY9xQwqKZh5Pk8ELfEzD", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/129/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457699488600, "tcdate": 1457699488600, "id": "OM0WY6zK4ip57ZJjtN5Y", "invitation": "ICLR.cc/2016/workshop/-/paper/129/review/12", "forum": "ZY9xQwqKZh5Pk8ELfEzD", "replyto": "ZY9xQwqKZh5Pk8ELfEzD", "signatures": ["ICLR.cc/2016/workshop/paper/129/reviewer/12"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/129/reviewer/12"], "content": {"title": "Interesting heuristic idea", "rating": "6: Marginally above acceptance threshold", "review": "Interesting idea about how to increase the representation power of a DNN as we receive new (different) data. The paper is brief and clear which is welcome. The weakness of the paper is the different heuristics (hyper parameters ) that use in order to decide when to increase the DNN and the \"lower learning rate\" to maintain stability in the old weights. The influence of these parameters has not been tested although. Moreover would be interesting to use a different dataset beyond the mnist that use to be use to \"sanity check\".\n\nThere are several sentences like \"This step relates to the notion of plasticity in biological NG\" that would need some reference.\n\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Neurogenic Deep Learning", "abstract": "Deep neural networks (DNNs) have achieved remarkable success on complex data processing tasks. In contrast to biological neural systems, capable of learning continuously, DNNs have a limited ability to incorporate new information in a trained network. Therefore, methods for continuous learning are potentially highly impactful in enabling the application of DNNs to dynamic data sets. Inspired by adult neurogenesis in the hippocampus, we explore the potential for adding new nodes to layers of artificial neural networks to facilitate their acquisition of novel information while preserving previously trained data representations. Our results demonstrate that neurogenesis is well suited for addressing the stability-plasticity dilemma that has long challenged adaptive machine learning algorithms. ", "pdf": "/pdf/ZY9xQwqKZh5Pk8ELfEzD.pdf", "paperhash": "draelos|neurogenic_deep_learning", "conflicts": ["sandia.gov"], "authors": ["Timothy J. Draelos", "Nadine E. Miner", "Jonathan A. Cox", "Christopher C. Lamb", "Conrad D. James", "James B. Aimone"], "authorids": ["tjdrael@sandia.gov", "nrminer@sandia.gov", "jacox@sandia.gov", "cclamb@sandia.gov", "cdjame@sandia.gov", "jbaimong@sandia.gov"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580019900, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580019900, "id": "ICLR.cc/2016/workshop/-/paper/129/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "ZY9xQwqKZh5Pk8ELfEzD", "replyto": "ZY9xQwqKZh5Pk8ELfEzD", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/129/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457647327945, "tcdate": 1457647327945, "id": "VAVwRrNJ2Tx0Wk76TAQ7", "invitation": "ICLR.cc/2016/workshop/-/paper/129/review/10", "forum": "ZY9xQwqKZh5Pk8ELfEzD", "replyto": "ZY9xQwqKZh5Pk8ELfEzD", "signatures": ["ICLR.cc/2016/workshop/paper/129/reviewer/10"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/129/reviewer/10"], "content": {"title": "An interesting idea for adapting neural network models to new input data. The research topic is particularly relevant in the light of the large neural nets that have been trained recently.", "rating": "7: Good paper, accept", "review": "The authors consider the problem of dynamically adapting a neural network to new input data (e.g. new classes), for which the required features might not have been learned. A neurogenesis method is proposed, that progressively introduces new neurons to the model as more data is presented to the network.\n\nThe method is motivated from a cost perspective, by explaining that adapting an existing model to new data is cheaper than storing the previous data and training a new model as soon as new data is being observed. The method and motivations are clearly explained and summarized.\n\nThe considered MNIST deep autoencoder model with sequentially introduced digit classes is a reasonable choice for testing, although the retraining costs are probably becoming more important when considering more complex models such as large convnets.\n\nA technique called \"intrinsic replay\" is applied in addition to the neurogenesis process. It seeks to generate examples that are similar to those observed previously, and feeding them to the neural network in addition to the newly observed data.\n\nThe properties of neural network learning dynamics with time-dependent data distributions is still an open question, that has not been very extensively studied, but a highly relevant one.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Neurogenic Deep Learning", "abstract": "Deep neural networks (DNNs) have achieved remarkable success on complex data processing tasks. In contrast to biological neural systems, capable of learning continuously, DNNs have a limited ability to incorporate new information in a trained network. Therefore, methods for continuous learning are potentially highly impactful in enabling the application of DNNs to dynamic data sets. Inspired by adult neurogenesis in the hippocampus, we explore the potential for adding new nodes to layers of artificial neural networks to facilitate their acquisition of novel information while preserving previously trained data representations. Our results demonstrate that neurogenesis is well suited for addressing the stability-plasticity dilemma that has long challenged adaptive machine learning algorithms. ", "pdf": "/pdf/ZY9xQwqKZh5Pk8ELfEzD.pdf", "paperhash": "draelos|neurogenic_deep_learning", "conflicts": ["sandia.gov"], "authors": ["Timothy J. Draelos", "Nadine E. Miner", "Jonathan A. Cox", "Christopher C. Lamb", "Conrad D. James", "James B. Aimone"], "authorids": ["tjdrael@sandia.gov", "nrminer@sandia.gov", "jacox@sandia.gov", "cclamb@sandia.gov", "cdjame@sandia.gov", "jbaimong@sandia.gov"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580020532, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580020532, "id": "ICLR.cc/2016/workshop/-/paper/129/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "ZY9xQwqKZh5Pk8ELfEzD", "replyto": "ZY9xQwqKZh5Pk8ELfEzD", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/129/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455825055094, "tcdate": 1455825055094, "id": "ZY9xQwqKZh5Pk8ELfEzD", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "ZY9xQwqKZh5Pk8ELfEzD", "signatures": ["~Timothy_Draelos1"], "readers": ["everyone"], "writers": ["~Timothy_Draelos1"], "content": {"CMT_id": "", "title": "Neurogenic Deep Learning", "abstract": "Deep neural networks (DNNs) have achieved remarkable success on complex data processing tasks. In contrast to biological neural systems, capable of learning continuously, DNNs have a limited ability to incorporate new information in a trained network. Therefore, methods for continuous learning are potentially highly impactful in enabling the application of DNNs to dynamic data sets. Inspired by adult neurogenesis in the hippocampus, we explore the potential for adding new nodes to layers of artificial neural networks to facilitate their acquisition of novel information while preserving previously trained data representations. Our results demonstrate that neurogenesis is well suited for addressing the stability-plasticity dilemma that has long challenged adaptive machine learning algorithms. ", "pdf": "/pdf/ZY9xQwqKZh5Pk8ELfEzD.pdf", "paperhash": "draelos|neurogenic_deep_learning", "conflicts": ["sandia.gov"], "authors": ["Timothy J. Draelos", "Nadine E. Miner", "Jonathan A. Cox", "Christopher C. Lamb", "Conrad D. James", "James B. Aimone"], "authorids": ["tjdrael@sandia.gov", "nrminer@sandia.gov", "jacox@sandia.gov", "cclamb@sandia.gov", "cdjame@sandia.gov", "jbaimong@sandia.gov"]}, "nonreaders": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 7}