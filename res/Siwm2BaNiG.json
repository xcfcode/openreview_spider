{"notes": [{"id": "Siwm2BaNiG", "original": "OeT3_FDvL14", "number": 89, "cdate": 1601308019043, "ddate": null, "tcdate": 1601308019043, "tmdate": 1614985660769, "tddate": null, "forum": "Siwm2BaNiG", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Modal Uncertainty Estimation via Discrete Latent Representations", "authorids": ["~Di_Qiu1", "~Zhanghan_Ke1", "~Peng_Su1", "~Lok_Ming_Lui2"], "authors": ["Di Qiu", "Zhanghan Ke", "Peng Su", "Lok Ming Lui"], "keywords": ["uncertainty estimation", "one -to-many mapping", "conditional generative model", "discrete latent space", "medical image segmentation"], "abstract": "Many important problems in the real world don't have unique solutions. It is thus important for machine learning models to be capable of proposing different plausible solutions with meaningful probability measures.\nIn this work we propose a novel deep learning based framework, named {\\it modal uncertainty estimation}  (MUE), to learn the one-to-many mappings between the inputs and outputs, together with faithful uncertainty estimation.\nMotivated by the multi-modal posterior collapse problem in current conditional generative models, MUE uses a set of discrete latent variables, each representing a latent mode hypothesis that explains one type of input-output relationship, to generate the one-to-many mappings. Benefit from the discrete nature of the latent representations, MUE can estimate any input the conditional probability distribution of the outputs effectively. Moreover, MUE is efficient during training since the discrete latent space and its uncertainty estimation are jointly learned.\nWe also develop the theoretical background of MUE and extensively validate it on both synthetic and realistic tasks. MUE demonstrates (1) significantly more accurate uncertainty estimation than the current state-of-the-art, and (2) its informativeness for practical use.\n\n", "one-sentence_summary": "We use a conditional generative model with discrete latent representation to solve the one-to-many mapping problem with faithful uncertainty estimates.", "pdf": "/pdf/33557352a7ba4632d3afc9b06de3be30f2eab419.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qiu|modal_uncertainty_estimation_via_discrete_latent_representations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pzhr0QhGs7", "_bibtex": "@misc{\nqiu2021modal,\ntitle={Modal Uncertainty Estimation via Discrete Latent Representations},\nauthor={Di Qiu and Zhanghan Ke and Peng Su and Lok Ming Lui},\nyear={2021},\nurl={https://openreview.net/forum?id=Siwm2BaNiG}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "7REIKlmJ9Ml", "original": null, "number": 1, "cdate": 1610040501145, "ddate": null, "tcdate": 1610040501145, "tmdate": 1610474107889, "tddate": null, "forum": "Siwm2BaNiG", "replyto": "Siwm2BaNiG", "invitation": "ICLR.cc/2021/Conference/Paper89/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper introduces a conditional discrete VAE for uncertainty estimation on high-dimensional data. Reviewers found the paper borderline, and two of the three reviewers stated it doesn't meet the acceptance bar due to lack of clarity in several aspects and limited technical novelty."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Modal Uncertainty Estimation via Discrete Latent Representations", "authorids": ["~Di_Qiu1", "~Zhanghan_Ke1", "~Peng_Su1", "~Lok_Ming_Lui2"], "authors": ["Di Qiu", "Zhanghan Ke", "Peng Su", "Lok Ming Lui"], "keywords": ["uncertainty estimation", "one -to-many mapping", "conditional generative model", "discrete latent space", "medical image segmentation"], "abstract": "Many important problems in the real world don't have unique solutions. It is thus important for machine learning models to be capable of proposing different plausible solutions with meaningful probability measures.\nIn this work we propose a novel deep learning based framework, named {\\it modal uncertainty estimation}  (MUE), to learn the one-to-many mappings between the inputs and outputs, together with faithful uncertainty estimation.\nMotivated by the multi-modal posterior collapse problem in current conditional generative models, MUE uses a set of discrete latent variables, each representing a latent mode hypothesis that explains one type of input-output relationship, to generate the one-to-many mappings. Benefit from the discrete nature of the latent representations, MUE can estimate any input the conditional probability distribution of the outputs effectively. Moreover, MUE is efficient during training since the discrete latent space and its uncertainty estimation are jointly learned.\nWe also develop the theoretical background of MUE and extensively validate it on both synthetic and realistic tasks. MUE demonstrates (1) significantly more accurate uncertainty estimation than the current state-of-the-art, and (2) its informativeness for practical use.\n\n", "one-sentence_summary": "We use a conditional generative model with discrete latent representation to solve the one-to-many mapping problem with faithful uncertainty estimates.", "pdf": "/pdf/33557352a7ba4632d3afc9b06de3be30f2eab419.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qiu|modal_uncertainty_estimation_via_discrete_latent_representations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pzhr0QhGs7", "_bibtex": "@misc{\nqiu2021modal,\ntitle={Modal Uncertainty Estimation via Discrete Latent Representations},\nauthor={Di Qiu and Zhanghan Ke and Peng Su and Lok Ming Lui},\nyear={2021},\nurl={https://openreview.net/forum?id=Siwm2BaNiG}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Siwm2BaNiG", "replyto": "Siwm2BaNiG", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040501132, "tmdate": 1610474107874, "id": "ICLR.cc/2021/Conference/Paper89/-/Decision"}}}, {"id": "vMfjvaJQypg", "original": null, "number": 3, "cdate": 1603918396561, "ddate": null, "tcdate": 1603918396561, "tmdate": 1606783502653, "tddate": null, "forum": "Siwm2BaNiG", "replyto": "Siwm2BaNiG", "invitation": "ICLR.cc/2021/Conference/Paper89/-/Official_Review", "content": {"title": "Review 4", "review": "This manuscript proposes to measure the \"modal uncertainty\" in conditional generative models by forcing a discrete latent intermediate representation (here, C), between inputs X and outputs Y. By then manipulating the estimated categorical distribution likelihoods, an uncertainty estimate can be produced.\n\nThis is, in essence, the kludge of the VQ-VAE (van den Oord et al. 2017) categorical latent variable model into the Probabilistic U-Net (Kohl et al. 2018), which had previously used the Gaussian latent variable model. Empirically this difference is important, as the Gaussian VAE specifies a single mode latent distribution, which is incorrect in many cases.\n\nStrong points:  \n*Presents a clear argument for VQ-VAE style latents vs. Gaussians (...non-unimodal uncertainty representation).  \n*Constructs and executes examples from a subset of previous literature (LeCun & Cortez 2010, Kohl et al. 2018) that illustrate empirical effectiveness.  \nWeak points:  \n*Limited characterization of uncertainty (i.e. uncertainty of a label image is one-of-K, or combinations thereof, and not uncertainty w.r.t. model/training/etc).  \n*Possibly incorrect claims (prior-free, disentanglement, Prob. U-Net ordering, VQ-VAE sampling(?)).  \n*Possible limited scope from the chosen definition of \"uncertainty\" (no model uncertainty, no parameter uncertainty).  \n\nI think this paper is marginal for this venue. It improves on the Prob. U-Net by borrowing improvements to VAE from VQ-VAE. While it does seem to better characterize label uncertainty, it is better suited to a venue where such improvements have intrinsic importance (e.g. radiology), and moreover importance that can be measured (i.e. experiments asking whether radiologists will use Non-Top-1 segmentations?). However, it appears to be an improvement on the prior art in my opinion, and I believe it would have some interest to the community as a poster.\n\nOne overall question: are we learning uncertainty of segmentations/labels, or are we simply learning the biases of the varying raters/radiologists/human segmenters, or are these two concepts indistinguishable? Supposing the number of codes equals the number of raters, why should $p(c|x)$ differ from $p(r)$? In an imbalanced rater case, why would an ordering of $p(c|x)$ not simply reflect the likelihood of one rater or another.\n\nThis seems to speak less about uncertainty in the segmentation w.r.t. the image and model parameters, and instead speak about biases in the segmentation w.r.t. the raters. While this is clearly also important, does this capture notions of uncertainty beyond the uncertainty of who is rating a certain image? Or is that only and exactly the uncertainty that the authors are attempting to capture?\n\nDiscrete Representation and Associated Loss\n----\n\nAs expected, there is significant discussion and derivation devoted to the discrete representation. This essentially mirrors the short derivations in the probabilistic u-net paper, except using a discrete likelihood.\n\nFrom the loss in Eq. (4) we have 1) a distribution matching term between $p_\\theta(c|x)$ and $q_\\phi(c|x,y)$, 2) a reconstruction term $D_\\theta$ given the output of the E(x,y) encoder and $x$ (using $\\arg\\min || c' - E_\\phi (c|x,y)||$ for $c$), and then 3) the VQ-VAE \"stop gradient\" term.\n\nIn the derivation slightly higher on the page, we assumed that $q_\\phi(c|x,y)$ is deterministic (i.e. $q_\\phi(c|x,y)$ is $1$ for one value of $c$ and otherwise $0$). While this results in a nice cross entropy term, why is it necessary? We otherwise have a still tractable difference between $q_\\phi(c|x,y)$ and $p_\\theta(c|x)$, summed over $c$. Is there added benefit to the forced discretization here?\n\nPerhaps it is useful to sample $c \\sim q_\\phi(c|x,y)$ or use the mode $c = \\arg\\min || c' - E_\\phi (c|x,y)||$ (this inducing an implicit gaussian likelihood structure on the embedding space), but it's not clear from just the theory that this is true.\n\nWhy is it reasonable to always use the mode reconstruction when training a loss (again, the argmax from VQ-VAE's embedding scheme), and then look at the other categories? Why should the outputs from the non-mode categories be reasonable (up to their likelihood)? (this question is in contrast to the convergence of p(c|x) to a rater distribution p(r)).\n\nThe authors claim that as in van den Oord et al 2017 there is no shrinkage to a prior over $c$. Is the regularization term $\\beta || E(x,y) - sg[c]||$ not such a prior? Doesn't this enforce a structure on the co-domain of E(x,y)? Are the embedding vectors determining $c'$s learned? If not, isn't this a prior?\n\n[minor] The authors use the terms \"prior encoder\" to describe $p_{\\theta}(c|x)$, a learned network, and similarly \"posterior encoder\" to describe $q_\\phi(c|x,y)$, also a learned network. Reading the probabilistic U-Net paper, it appears that the authors of that paper also use this terminology. It is a matter of viewpoint, but I personally think these terms conflict with \"the usual terminology\", where $p(c)$ would be \"the prior\", to which we shrink $p(c|x)$ to. Further, this implies a causal order to $x$ and $y$; this is the case with the empirical examples (tumor labels on CT images, etc.), but may not always be the case.\n\nU-Net Comparison\n----\n\nThe authors claim that \"Probabilistic U-Net\" outputs cannot be ranked; is this actually the case? It is my understanding that the difference between the proposed method and the Prob. U-Net method in architecture is the replacement of their Gaussian latent C (in their paper, Z) with a discrete categorical C. This, alongside the different loss, contains the majority of the changes.\n\nCould the Prob. U-Net have outputs ranked by their point-likelihoods? The C output are conditionally Gaussian, so why not use the $p(c|x)$ likelihood? Understandably, generating diverse samples from this is not as simple as querying the different codes in the proposed method, but it seems incorrect to say that the samples from the Prob. U-Net are unordered.\n\nOther Questions\n----\n\nThere are several mentions of \"disentanglement\". In what way are these representations disentangled? They're certainly categorical by construction, but both the outputs and the actual labels are highly correlated between different categories.\n\nAt the bottom of page 3 there is a claim about sampling from a VQ-VAE being auto-regressive? Is the particular decoder architecture in VQ-VAE important w.r.t. the theory discussion directly adjacent to that statement? Surely VQ refers to the Vector-Quantization phase, which is agnostic of the decoder architecture, auto-regressive or otherwise? This seems incorrect. Similarly, there is a claim that VQ-VAE was introduced to avoid\n\n> noise sampling, which is a different cause than ours which usually results in blurriness  \n\nIs this the cause of blurriness (not, e.g. the L2 loss in the decoder?)? And is the introduction of VQ not the same issue the authors here are attempting to address (misspecification of a single-mode latent distribution), not the additive noise sampling?\n\nRecommendations for improvement\n----\n-Remove the disentanglement sentences.  \n-Resolve questionable claims.  \n-Include an experimental case where the number of raters is larger than the number of latent categories.  \n\nEdit after Rebuttal/Response Period:\n----\n\nFirst, an apology to the authors that a dialogue did not occur during the response period; the authors response was prompt, and my (R4) response was not, thus they were not given an opportunity to respond this response to their response to the review (...the number of recurrences may indicate why this was not possible, given limited reviewer time resources).\n\nI think the authors misunderstand my questions about the nature of the uncertainty they're capturing:  \nIs there intrinsic uncertainty in the observed phenomena (e.g. medical images of tissues), are we capturing mixture proportions of deterministic states which have been mixed due to quantization, OR is the uncertainty due to the raters, i.e. found in the labels ONLY due to differences in label generation?  OR, a third case, is this moot because it does not change the outcome?\n\nI understand that there is no explicit modelling of raters. However, my concern was that what we are capturing is intrinsically the uncertainty due to raters, even though no actual rater indicator variable was provided. This would be analogous to learning, unsupervised, the writers of the various MNISTs digits. While for MNIST this is surely difficult due to the number of writers (and their anonymity), for medical images we will likely have a limited number of raters. Having the posterior code collapse to a rater indicator appears problematic, not a desirable outcome, and likely if any one rater has correlated outputs across samples (which seems reasonable; some raters may be more or less conservative with their tissue labeling, boundaries, better/more careful at delineating curves etc). What prevents the capture of this signal, or is this the actual variation we intend to capture in the first place?\n\nR3 further included this interesting question in their initial review:\n> In the shown samples, for a given input, many of the different outputs seem very similar and could be considered from the same mode. This can potentially make interpreting the probabilities more difficult than claimed in the paper, especially since the model is trained with a large number of codes. A very plausible scenario could be that one of the most likely modes is split between multiple low probability outputs and thus doesn't show up on the top ouput [sic]. Can the authors comment on this potential issue?\nIf outputs are correlated i.e. overlapping in the original image domain, should the measured uncertainty be aggregated across codes?\n\nI disagree with the characterization of the Gaussian VAE calibration in the \"Ranking Probabilistic U-NET\" response section. Simply because it does not accurately fit the function (one mode vs. many) does not mean we can't evaluate the learned unimodal beliefs. Yes, it is misspecified. Does this mean the rankings are meaningless? Surely the discrete model is misspecified (there are more possible masks than codes), but the ranking is claimed to be meaningful. The discrete model may have a better fit, and make the argument that it is better specified, but this doesn't mean you _can't_ evaluate the Gaussian VAE.\n\nI stand by my initial rating and reasoning, though I note to the AC that, given space, this could make an acceptable poster. It is, in my opinion and in gross summation, an improvement on the Prob. U-Net by way of improving the Gaussian VAE sub-model of the Prob. U-Net to the VQ-VAE. This allows for sampling from a discrete set of codes which hopefully correspond with modes of the generating distribution in the data domain, instead of sampling from a parametric density, which, while continuous, has only one local maximum.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper89/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper89/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Modal Uncertainty Estimation via Discrete Latent Representations", "authorids": ["~Di_Qiu1", "~Zhanghan_Ke1", "~Peng_Su1", "~Lok_Ming_Lui2"], "authors": ["Di Qiu", "Zhanghan Ke", "Peng Su", "Lok Ming Lui"], "keywords": ["uncertainty estimation", "one -to-many mapping", "conditional generative model", "discrete latent space", "medical image segmentation"], "abstract": "Many important problems in the real world don't have unique solutions. It is thus important for machine learning models to be capable of proposing different plausible solutions with meaningful probability measures.\nIn this work we propose a novel deep learning based framework, named {\\it modal uncertainty estimation}  (MUE), to learn the one-to-many mappings between the inputs and outputs, together with faithful uncertainty estimation.\nMotivated by the multi-modal posterior collapse problem in current conditional generative models, MUE uses a set of discrete latent variables, each representing a latent mode hypothesis that explains one type of input-output relationship, to generate the one-to-many mappings. Benefit from the discrete nature of the latent representations, MUE can estimate any input the conditional probability distribution of the outputs effectively. Moreover, MUE is efficient during training since the discrete latent space and its uncertainty estimation are jointly learned.\nWe also develop the theoretical background of MUE and extensively validate it on both synthetic and realistic tasks. MUE demonstrates (1) significantly more accurate uncertainty estimation than the current state-of-the-art, and (2) its informativeness for practical use.\n\n", "one-sentence_summary": "We use a conditional generative model with discrete latent representation to solve the one-to-many mapping problem with faithful uncertainty estimates.", "pdf": "/pdf/33557352a7ba4632d3afc9b06de3be30f2eab419.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qiu|modal_uncertainty_estimation_via_discrete_latent_representations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pzhr0QhGs7", "_bibtex": "@misc{\nqiu2021modal,\ntitle={Modal Uncertainty Estimation via Discrete Latent Representations},\nauthor={Di Qiu and Zhanghan Ke and Peng Su and Lok Ming Lui},\nyear={2021},\nurl={https://openreview.net/forum?id=Siwm2BaNiG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Siwm2BaNiG", "replyto": "Siwm2BaNiG", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper89/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538150539, "tmdate": 1606915800438, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper89/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper89/-/Official_Review"}}}, {"id": "YgQP_SEo5yi", "original": null, "number": 6, "cdate": 1605531342328, "ddate": null, "tcdate": 1605531342328, "tmdate": 1605540269173, "tddate": null, "forum": "Siwm2BaNiG", "replyto": "wr02ZBmizNL", "invitation": "ICLR.cc/2021/Conference/Paper89/-/Official_Comment", "content": {"title": "Response", "comment": "Thanks for the comments and questions! Please find below our answers.\n\n**Definition of posterior collapse**\nThanks for the interesting question. By successful minimization, we mean that the KL divergence is small, note the situation in Fig.1(a) will have a larger KL divergence than that in Fig.1(b). Note that both prior and posterior distributions are optimized. By posterior collapse we mean negligible difference between the prior and the posterior, as is the case in Fig.1(b). The definition is consistent with the existing literature [1] cited in the paper.\n\n**Comparing with the method proposed in the neural dialogue paper**\nThanks for bringing up the work which we were not aware of! After looking at their paper, we note there are key differences. Using our notation, we believe they would like their learned discrete code $c$ for an utterance to be \"context free\" at the first place in their design, meaning that the code $c$ of the response $y$ should not be influenced by the context $x$. This is in contrast to our assumption that the hypothesis $c$ of input $x$ should depend on $x$. Taking the LIDC-IDRI benchmark for an example, if we encode the hypotheses from the segmentation alone, likely there will either be two modes (benign vs malignant) or a huge number of modes if the shape of the segmentation is taken into account. Moreover, it will not contain any information about what kinds of actual biological tissue they might be, which on the other hand can be judged from the actual scan image. In our case, we have deliberately separated the recognition task learning, e.g. segmenting the image, and the hypothesis learning, so that together they can approximate the variation of $y$ given $x$. Therefore, we believe the quoted statement in their paper does not apply to our setting. \nWe acknowledge that the use of discrete latent space with VAE has been explored by others, we have added in Section 2 a discussion about it. We believe that we are the first to justify the use of discrete latent space from the perspective of the multi-modal posterior collapse problem due to Gaussian parametrization of latent variables, and developed a framework to produce faithful uncertainty estimation for the one-to-many mapping problem.\n\n[1] Ali Razavi, Aaron van den Oord, Ben Poole, and Oriol Vinyals.  Preventing posterior collapse with delta-vaes. In International Conference on Learning Representations, 2018.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper89/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper89/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Modal Uncertainty Estimation via Discrete Latent Representations", "authorids": ["~Di_Qiu1", "~Zhanghan_Ke1", "~Peng_Su1", "~Lok_Ming_Lui2"], "authors": ["Di Qiu", "Zhanghan Ke", "Peng Su", "Lok Ming Lui"], "keywords": ["uncertainty estimation", "one -to-many mapping", "conditional generative model", "discrete latent space", "medical image segmentation"], "abstract": "Many important problems in the real world don't have unique solutions. It is thus important for machine learning models to be capable of proposing different plausible solutions with meaningful probability measures.\nIn this work we propose a novel deep learning based framework, named {\\it modal uncertainty estimation}  (MUE), to learn the one-to-many mappings between the inputs and outputs, together with faithful uncertainty estimation.\nMotivated by the multi-modal posterior collapse problem in current conditional generative models, MUE uses a set of discrete latent variables, each representing a latent mode hypothesis that explains one type of input-output relationship, to generate the one-to-many mappings. Benefit from the discrete nature of the latent representations, MUE can estimate any input the conditional probability distribution of the outputs effectively. Moreover, MUE is efficient during training since the discrete latent space and its uncertainty estimation are jointly learned.\nWe also develop the theoretical background of MUE and extensively validate it on both synthetic and realistic tasks. MUE demonstrates (1) significantly more accurate uncertainty estimation than the current state-of-the-art, and (2) its informativeness for practical use.\n\n", "one-sentence_summary": "We use a conditional generative model with discrete latent representation to solve the one-to-many mapping problem with faithful uncertainty estimates.", "pdf": "/pdf/33557352a7ba4632d3afc9b06de3be30f2eab419.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qiu|modal_uncertainty_estimation_via_discrete_latent_representations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pzhr0QhGs7", "_bibtex": "@misc{\nqiu2021modal,\ntitle={Modal Uncertainty Estimation via Discrete Latent Representations},\nauthor={Di Qiu and Zhanghan Ke and Peng Su and Lok Ming Lui},\nyear={2021},\nurl={https://openreview.net/forum?id=Siwm2BaNiG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Siwm2BaNiG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper89/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper89/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper89/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper89/Authors|ICLR.cc/2021/Conference/Paper89/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper89/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874669, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper89/-/Official_Comment"}}}, {"id": "yh8b5YiXtj-", "original": null, "number": 5, "cdate": 1605529820414, "ddate": null, "tcdate": 1605529820414, "tmdate": 1605540162190, "tddate": null, "forum": "Siwm2BaNiG", "replyto": "d47KsYwBrLc", "invitation": "ICLR.cc/2021/Conference/Paper89/-/Official_Comment", "content": {"title": "Response", "comment": "Thanks for the comments and questions! Please refer to our answers below.\n\n**On single mode splitting into multiple codes and thus affecting uncertainty estimation**\nThis is a very good question!  Indeed we were concerned about the scenario where many codes correspond to the same mode, and making the uncertainty estimation less useful.  However, we found in both of our synthetic and real experiments that the extent of such a scenario is very mild. This could be explained by the fact that when it starts training, usually only one code is used, and then the number gradually increases, and then decreases. We believe there will be an interesting theory behind, in the spirit of the information bottleneck theory developed by Tishby et al. We left the theoretical aspects for the future work.  We have added corresponding discussion and references.\n\n**\"I am curious about the properties of the latent space succinctly mentioned at the end of section 4. What other properties have the authors observed?\"**\nWe believe there are more interesting observations if we go in the reverse direction to see what kind of outputs a particular code can generate. But in order to develop a metric it requires us to have more information or professional knowledge about the scan and the patient, which unfortunately we don\u2019t have. We believe our methodology may be of interest to people with various applications in mind.\n\n**Do similar outputs tend to be represented with codes that are close in the latent space even in the case of discrete representations?**\nWe confirm that for the same image, codes that are close in the latent space produce similar outputs, since the code is constantly being updated while the results do not change significantly during the later phase of training.  We do not rule out the possibility where different codes produce similar outputs.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper89/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper89/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Modal Uncertainty Estimation via Discrete Latent Representations", "authorids": ["~Di_Qiu1", "~Zhanghan_Ke1", "~Peng_Su1", "~Lok_Ming_Lui2"], "authors": ["Di Qiu", "Zhanghan Ke", "Peng Su", "Lok Ming Lui"], "keywords": ["uncertainty estimation", "one -to-many mapping", "conditional generative model", "discrete latent space", "medical image segmentation"], "abstract": "Many important problems in the real world don't have unique solutions. It is thus important for machine learning models to be capable of proposing different plausible solutions with meaningful probability measures.\nIn this work we propose a novel deep learning based framework, named {\\it modal uncertainty estimation}  (MUE), to learn the one-to-many mappings between the inputs and outputs, together with faithful uncertainty estimation.\nMotivated by the multi-modal posterior collapse problem in current conditional generative models, MUE uses a set of discrete latent variables, each representing a latent mode hypothesis that explains one type of input-output relationship, to generate the one-to-many mappings. Benefit from the discrete nature of the latent representations, MUE can estimate any input the conditional probability distribution of the outputs effectively. Moreover, MUE is efficient during training since the discrete latent space and its uncertainty estimation are jointly learned.\nWe also develop the theoretical background of MUE and extensively validate it on both synthetic and realistic tasks. MUE demonstrates (1) significantly more accurate uncertainty estimation than the current state-of-the-art, and (2) its informativeness for practical use.\n\n", "one-sentence_summary": "We use a conditional generative model with discrete latent representation to solve the one-to-many mapping problem with faithful uncertainty estimates.", "pdf": "/pdf/33557352a7ba4632d3afc9b06de3be30f2eab419.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qiu|modal_uncertainty_estimation_via_discrete_latent_representations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pzhr0QhGs7", "_bibtex": "@misc{\nqiu2021modal,\ntitle={Modal Uncertainty Estimation via Discrete Latent Representations},\nauthor={Di Qiu and Zhanghan Ke and Peng Su and Lok Ming Lui},\nyear={2021},\nurl={https://openreview.net/forum?id=Siwm2BaNiG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Siwm2BaNiG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper89/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper89/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper89/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper89/Authors|ICLR.cc/2021/Conference/Paper89/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper89/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874669, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper89/-/Official_Comment"}}}, {"id": "BX2jMU_4DNC", "original": null, "number": 3, "cdate": 1605528000944, "ddate": null, "tcdate": 1605528000944, "tmdate": 1605539725815, "tddate": null, "forum": "Siwm2BaNiG", "replyto": "Siwm2BaNiG", "invitation": "ICLR.cc/2021/Conference/Paper89/-/Official_Comment", "content": {"title": "Overall response to the reviewers", "comment": "Thanks for the feedback from the reviewers! Overall, we have made the following improvements in the updated manuscript.\n1. We have modified and highlighted our contribution in Section 1, and added in Section 2 two more paragraphs to discuss previous works using discrete latent variables in VAE, including the one noted by Review 2, and other kind of uncertainty estimation, noted by Reviewer 4, to compare ours with them and emphasize the key differences.\n1. We have added more explanation about:\n    (a) How the posterior distribution is not regularized by the prior distribution and the effect of the regularization term, in Section 3.\n    (b) How different parts of the model work together to give good results, in Section 3.\n    (c). Why scenarios where modes split into multiple codes do not become a serious problem in our approach, in Section 3.\n3. We have also improved the clarity of exposition in various places, thanks to the questions and suggestions from the reviewers.\n\nPlease refer to the individual response for more details. "}, "signatures": ["ICLR.cc/2021/Conference/Paper89/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper89/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Modal Uncertainty Estimation via Discrete Latent Representations", "authorids": ["~Di_Qiu1", "~Zhanghan_Ke1", "~Peng_Su1", "~Lok_Ming_Lui2"], "authors": ["Di Qiu", "Zhanghan Ke", "Peng Su", "Lok Ming Lui"], "keywords": ["uncertainty estimation", "one -to-many mapping", "conditional generative model", "discrete latent space", "medical image segmentation"], "abstract": "Many important problems in the real world don't have unique solutions. It is thus important for machine learning models to be capable of proposing different plausible solutions with meaningful probability measures.\nIn this work we propose a novel deep learning based framework, named {\\it modal uncertainty estimation}  (MUE), to learn the one-to-many mappings between the inputs and outputs, together with faithful uncertainty estimation.\nMotivated by the multi-modal posterior collapse problem in current conditional generative models, MUE uses a set of discrete latent variables, each representing a latent mode hypothesis that explains one type of input-output relationship, to generate the one-to-many mappings. Benefit from the discrete nature of the latent representations, MUE can estimate any input the conditional probability distribution of the outputs effectively. Moreover, MUE is efficient during training since the discrete latent space and its uncertainty estimation are jointly learned.\nWe also develop the theoretical background of MUE and extensively validate it on both synthetic and realistic tasks. MUE demonstrates (1) significantly more accurate uncertainty estimation than the current state-of-the-art, and (2) its informativeness for practical use.\n\n", "one-sentence_summary": "We use a conditional generative model with discrete latent representation to solve the one-to-many mapping problem with faithful uncertainty estimates.", "pdf": "/pdf/33557352a7ba4632d3afc9b06de3be30f2eab419.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qiu|modal_uncertainty_estimation_via_discrete_latent_representations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pzhr0QhGs7", "_bibtex": "@misc{\nqiu2021modal,\ntitle={Modal Uncertainty Estimation via Discrete Latent Representations},\nauthor={Di Qiu and Zhanghan Ke and Peng Su and Lok Ming Lui},\nyear={2021},\nurl={https://openreview.net/forum?id=Siwm2BaNiG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Siwm2BaNiG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper89/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper89/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper89/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper89/Authors|ICLR.cc/2021/Conference/Paper89/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper89/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874669, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper89/-/Official_Comment"}}}, {"id": "-EY-QU-luS2", "original": null, "number": 4, "cdate": 1605529463907, "ddate": null, "tcdate": 1605529463907, "tmdate": 1605529463907, "tddate": null, "forum": "Siwm2BaNiG", "replyto": "vMfjvaJQypg", "invitation": "ICLR.cc/2021/Conference/Paper89/-/Official_Comment", "content": {"title": "Response", "comment": "Thanks for the detailed questions! Please check below for our answers.\n\n**On characterization of uncertainty**\nWe have been focusing on the uncertainty from \u201cone-to-many mapping problem\u201d, which originates from the ambiguous label problem of the datasets, which is different in nature from model or training uncertainty. But we note that those uncertainties could be taken into consideration by using existing approaches jointly with our framework. We have updated the paper in Section 2.\n\n**On the relationship between prior distribution and posterior distribution in MUE**\nIn Section 3.1, we described the fact that the *probability values* of the posterior over the discrete latent space is not pulled to the *probability values* of any prior distribution over the discrete latent space. The reason is that we maintain a delta distribution of a given input-output pair $(x,y)$ during training, which is not affected by whatever prior distribution $p(c|x)$ over $\\mathcal{C}$. Note that the discrete latent space here should be considered just as a finite set of indices, without any other structure between these indices. \nAs for the regularization term, it deals with how we represent the latent feature in $\\mathbb{R}^n$, so that they can be utilized well by the decoder. We have made corresponding updates in Section 3.\n\n**Rater distribution vs. MUE**\nWe did not explicitly model the pair $(x, y)$ to be coming from different raters. The latent code c is designed to capture the variation of the labels $y$\u2019s conditioned on an input $x$, as provided in the dataset.  Our framework captures that overall, what probability will the scan image be compatible with $c$. \nThe number of codes and the number of raters need not be the same. In the case of LIDC-IDRI benchmark, two raters may agree (the raters share the same code) or disagree (the raters hold different codes)  with each other for a certain scan image. Such ambiguous situations happen very often since no further information of the patient is provided. \n\n**On the choice of deterministic posterior**\nBecause the mapping $(x, y)$ is given to the posterior encoder, there should be no \u201cmodal uncertainty\u201d for the posterior network.  As a result, we can let the posterior encoder produce a deterministic output e in R^n for the given input-output pair (x,y). We have added more descriptions in Section 3.\n\n**How the model can produce good results, non-mode code produces possibly reasonable output**\nMUEl can be conceptually divided into two parts. The first part is an auto-encoding part: posterior encoder + decoder; the second part is a conditional generation part: prior encoder + decoder, where the prior encoder is a classification model. The prior encoder is trained by the posterior encoder. So the model can learn a good representation of the latent code c from the successfully trained auto-encoder, and at the same time faithful uncertainty estimation by the prior encoder. \nNote that although we enforced the code of mode categories to correspond to correct outputs, we didn\u2019t enforce what output a non-mode category should give, other than they have small probabilities, because non-mode pairs don\u2019t appear in the training. We have added more explanations in Section 3.\n\n**Ranking Probabilistic U-Net**\nIn Section 1 we explained why Gaussian latent variables density value cannot be used for calibrated uncertainty estimation. We did a comparison experiment, shown in Fig.2(b), where the left axis is the point-likelihoods of the Prob.U-Net samples. We can note that the likelihood values cannot reflect the true uncertainty level and thus the order defined by these values are essentially useless.  We have added more explanations in Section 1.\n\n**On disentangled representation**\nWe agree this word may lead to some confusion and we have modified them. By disentanglement we mean learning two kinds of complementary features: one that is necessary for the recognition task and one that is necessary to explain the variation of the outputs given input. \n\n**Claims regarding vq-VAE**\nThe statement about sampling depends on the application of VQ-VAE. In their original paper, they considered mainly unconditional generation tasks, and the latent codes for an image in VQ-VAE are pixel-wise, hence the joint distribution of the codes cannot be obtained directly. Our sampling is easier because of the MUE setting. \nWe recognize the sentence \"noise sampling... blurriness\" is confusing here. We would like to claim that VQ-VAE uses discrete latent variables so that it can get rid of the noise sampling, which enables the latent variable to be more effectively utilized by the decoder and produce outputs with better visual quality. While we focused on the multi-modal posterior collapse problem, which is particular to the one-to-many mapping problem.\nWe have modified them accordingly. \n\n**Terminology of prior encoding networks**\nWe have adopted the standard terminology, where we added citation in corresponding places.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper89/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper89/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Modal Uncertainty Estimation via Discrete Latent Representations", "authorids": ["~Di_Qiu1", "~Zhanghan_Ke1", "~Peng_Su1", "~Lok_Ming_Lui2"], "authors": ["Di Qiu", "Zhanghan Ke", "Peng Su", "Lok Ming Lui"], "keywords": ["uncertainty estimation", "one -to-many mapping", "conditional generative model", "discrete latent space", "medical image segmentation"], "abstract": "Many important problems in the real world don't have unique solutions. It is thus important for machine learning models to be capable of proposing different plausible solutions with meaningful probability measures.\nIn this work we propose a novel deep learning based framework, named {\\it modal uncertainty estimation}  (MUE), to learn the one-to-many mappings between the inputs and outputs, together with faithful uncertainty estimation.\nMotivated by the multi-modal posterior collapse problem in current conditional generative models, MUE uses a set of discrete latent variables, each representing a latent mode hypothesis that explains one type of input-output relationship, to generate the one-to-many mappings. Benefit from the discrete nature of the latent representations, MUE can estimate any input the conditional probability distribution of the outputs effectively. Moreover, MUE is efficient during training since the discrete latent space and its uncertainty estimation are jointly learned.\nWe also develop the theoretical background of MUE and extensively validate it on both synthetic and realistic tasks. MUE demonstrates (1) significantly more accurate uncertainty estimation than the current state-of-the-art, and (2) its informativeness for practical use.\n\n", "one-sentence_summary": "We use a conditional generative model with discrete latent representation to solve the one-to-many mapping problem with faithful uncertainty estimates.", "pdf": "/pdf/33557352a7ba4632d3afc9b06de3be30f2eab419.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qiu|modal_uncertainty_estimation_via_discrete_latent_representations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pzhr0QhGs7", "_bibtex": "@misc{\nqiu2021modal,\ntitle={Modal Uncertainty Estimation via Discrete Latent Representations},\nauthor={Di Qiu and Zhanghan Ke and Peng Su and Lok Ming Lui},\nyear={2021},\nurl={https://openreview.net/forum?id=Siwm2BaNiG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Siwm2BaNiG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper89/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper89/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper89/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper89/Authors|ICLR.cc/2021/Conference/Paper89/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper89/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874669, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper89/-/Official_Comment"}}}, {"id": "wr02ZBmizNL", "original": null, "number": 1, "cdate": 1603616027437, "ddate": null, "tcdate": 1603616027437, "tmdate": 1605024765534, "tddate": null, "forum": "Siwm2BaNiG", "replyto": "Siwm2BaNiG", "invitation": "ICLR.cc/2021/Conference/Paper89/-/Official_Review", "content": {"title": "Extension of conditional VAE for uncertainty estimation, but need polishing", "review": "The paper proposes a conditional VAE like framework to learn the one-to-many mappings between input and output, leading to an application of uncertainty estimation. Technically, the novel part is to utilize a deterministic (delta) distribution for approximate posterior.\n\nSome flaws may need future attention:\n\n1. In the introduction paragraph staring with \"Let us recall that one key ingredient of the VAE framework\", the main idea is understandable: discrete latent code has definitely advantages in coping with multimodal distributions than a Gaussian distribution which is in nature single mode.  However, its explanation is confusing as following: \n\tIn VAE, let's say we minimize KL divergence KL(P(c|x,y)|P(c|x)), where P(c|x,y) has two modes, P(c|x) has single mode. When minimization is successful, p(c|x) will spread out like figure 1(a), rather than 1(b). This is related to the difference between forward KL and reverse KL. However, it seems this paragraph suggests 1(b) as posterior collapse.\n\n2. Using discrete latent code is not new in VAE community. There are previous works (e.g. https://arxiv.org/pdf/1804.08069.pdf) noting that naively learned discrete code c for p(y|x,c) can not be interpreted alone, but need interpreted together with input x. Such statement argues this paper's novelty and contribution. \n\n3. Some typos and minor flaws, such as unclear references of figure rows in figure 3.\n\nOverall, the reviewer thinks this paper need some revisions for it to be more shining.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper89/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper89/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Modal Uncertainty Estimation via Discrete Latent Representations", "authorids": ["~Di_Qiu1", "~Zhanghan_Ke1", "~Peng_Su1", "~Lok_Ming_Lui2"], "authors": ["Di Qiu", "Zhanghan Ke", "Peng Su", "Lok Ming Lui"], "keywords": ["uncertainty estimation", "one -to-many mapping", "conditional generative model", "discrete latent space", "medical image segmentation"], "abstract": "Many important problems in the real world don't have unique solutions. It is thus important for machine learning models to be capable of proposing different plausible solutions with meaningful probability measures.\nIn this work we propose a novel deep learning based framework, named {\\it modal uncertainty estimation}  (MUE), to learn the one-to-many mappings between the inputs and outputs, together with faithful uncertainty estimation.\nMotivated by the multi-modal posterior collapse problem in current conditional generative models, MUE uses a set of discrete latent variables, each representing a latent mode hypothesis that explains one type of input-output relationship, to generate the one-to-many mappings. Benefit from the discrete nature of the latent representations, MUE can estimate any input the conditional probability distribution of the outputs effectively. Moreover, MUE is efficient during training since the discrete latent space and its uncertainty estimation are jointly learned.\nWe also develop the theoretical background of MUE and extensively validate it on both synthetic and realistic tasks. MUE demonstrates (1) significantly more accurate uncertainty estimation than the current state-of-the-art, and (2) its informativeness for practical use.\n\n", "one-sentence_summary": "We use a conditional generative model with discrete latent representation to solve the one-to-many mapping problem with faithful uncertainty estimates.", "pdf": "/pdf/33557352a7ba4632d3afc9b06de3be30f2eab419.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qiu|modal_uncertainty_estimation_via_discrete_latent_representations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pzhr0QhGs7", "_bibtex": "@misc{\nqiu2021modal,\ntitle={Modal Uncertainty Estimation via Discrete Latent Representations},\nauthor={Di Qiu and Zhanghan Ke and Peng Su and Lok Ming Lui},\nyear={2021},\nurl={https://openreview.net/forum?id=Siwm2BaNiG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Siwm2BaNiG", "replyto": "Siwm2BaNiG", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper89/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538150539, "tmdate": 1606915800438, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper89/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper89/-/Official_Review"}}}, {"id": "d47KsYwBrLc", "original": null, "number": 2, "cdate": 1603897729187, "ddate": null, "tcdate": 1603897729187, "tmdate": 1605024765468, "tddate": null, "forum": "Siwm2BaNiG", "replyto": "Siwm2BaNiG", "invitation": "ICLR.cc/2021/Conference/Paper89/-/Official_Review", "content": {"title": "An interesting way to apply discrete latent space VAEs, for multimodal outputs. ", "review": "This paper introduces a novel conditional generative model for high dimensional data with multimodal output distributions. The proposed method, called modal uncertainty estimation (MUE), is a conditional VAE but with discrete latent representations. This discrete latent space allows the model to better handle multimodal outputs and provide confidence scores for the different modes predicted by the model. These capabilities are applied to the task of segmenting lesions in medical scans.\n\n\n################################################\n\nStrong points:\n\n- The paper is clear and easy to follow. It is well-motivated and does a good job at highlighting the multi-modal posterior collapse problem.\n\n- The model outperforms the prior state-of-the-art on both a synthetic and realistic task.\n\n\nWeaknesses:\n\n- Although the application is very different, the proposed model is very similar, albeit lighter, to a prior work (cited by the authors).\n\n- In the shown samples, for a given input, many of the different outputs seem very similar and could be considered from the same mode. This can potentially make interpreting the probabilities more difficult than claimed in the paper, especially since the model is trained with a large number of codes. A very plausible scenario could be that one of the most likely modes is split between multiple low probability outputs and thus doesn't show up on the top ouput. Can the authors comment on this potential issue?\n\n\n################################################\n\nScore motivation:\n\nWhile the method is not particularly novel, the authors apply it in a way that could be of interest to the community.\nBesides, the behavior of VAEs with discrete latent space is a relevant topic that is little explored in the literature.\n\n\n################################################\n\nOther question:\n\nI am curious about the properties of the latent space succinctly mentioned at the end of section 4. What other properties have the authors observed? Also, do similar outputs tend to be represented with codes that are close in the latent space even in the case of discrete representations?\n\n\nMinor typo:\n\n3rd paragraph of the introduction: two consecutive commas", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper89/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper89/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Modal Uncertainty Estimation via Discrete Latent Representations", "authorids": ["~Di_Qiu1", "~Zhanghan_Ke1", "~Peng_Su1", "~Lok_Ming_Lui2"], "authors": ["Di Qiu", "Zhanghan Ke", "Peng Su", "Lok Ming Lui"], "keywords": ["uncertainty estimation", "one -to-many mapping", "conditional generative model", "discrete latent space", "medical image segmentation"], "abstract": "Many important problems in the real world don't have unique solutions. It is thus important for machine learning models to be capable of proposing different plausible solutions with meaningful probability measures.\nIn this work we propose a novel deep learning based framework, named {\\it modal uncertainty estimation}  (MUE), to learn the one-to-many mappings between the inputs and outputs, together with faithful uncertainty estimation.\nMotivated by the multi-modal posterior collapse problem in current conditional generative models, MUE uses a set of discrete latent variables, each representing a latent mode hypothesis that explains one type of input-output relationship, to generate the one-to-many mappings. Benefit from the discrete nature of the latent representations, MUE can estimate any input the conditional probability distribution of the outputs effectively. Moreover, MUE is efficient during training since the discrete latent space and its uncertainty estimation are jointly learned.\nWe also develop the theoretical background of MUE and extensively validate it on both synthetic and realistic tasks. MUE demonstrates (1) significantly more accurate uncertainty estimation than the current state-of-the-art, and (2) its informativeness for practical use.\n\n", "one-sentence_summary": "We use a conditional generative model with discrete latent representation to solve the one-to-many mapping problem with faithful uncertainty estimates.", "pdf": "/pdf/33557352a7ba4632d3afc9b06de3be30f2eab419.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qiu|modal_uncertainty_estimation_via_discrete_latent_representations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pzhr0QhGs7", "_bibtex": "@misc{\nqiu2021modal,\ntitle={Modal Uncertainty Estimation via Discrete Latent Representations},\nauthor={Di Qiu and Zhanghan Ke and Peng Su and Lok Ming Lui},\nyear={2021},\nurl={https://openreview.net/forum?id=Siwm2BaNiG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Siwm2BaNiG", "replyto": "Siwm2BaNiG", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper89/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538150539, "tmdate": 1606915800438, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper89/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper89/-/Official_Review"}}}], "count": 9}