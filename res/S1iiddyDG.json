{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124458000, "tcdate": 1518467235510, "number": 249, "cdate": 1518467235510, "id": "S1iiddyDG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "S1iiddyDG", "signatures": ["~Guillaume_Alain1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Negative eigenvalues of the Hessian in deep neural networks", "abstract": "We study the loss function of a deep neural network through the eigendecomposition of its Hessian matrix. We focus on negative eigenvalues, how important they are, and how to best deal with them. The goal is to develop an optimization method specifically tailored for deep neural networks.", "paperhash": "alain|negative_eigenvalues_of_the_hessian_in_deep_neural_networks", "keywords": ["Optimization", "Hessian matrix", "Neural Networks", "Negative Curvature"], "_bibtex": "@misc{\n  alain2018negative,\n  title={Negative eigenvalues of the Hessian in deep neural networks},\n  author={Guillaume Alain and Nicolas Le Roux and Pierre-Antoine Manzagol},\n  year={2018},\n  url={https://openreview.net/forum?id=S1iiddyDG}\n}", "authorids": ["guillaume.alain.umontreal@gmail.com", "nicolas@le-roux.name", "manzagop@google.com"], "authors": ["Guillaume Alain", "Nicolas Le Roux", "Pierre-Antoine Manzagol"], "TL;DR": "We study negative curvature of the loss of neural networks. We want to develop a better optimization method.", "pdf": "/pdf/2c184c7fec416cce2b96eefe6dc2db9536c60902.pdf"}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582892497, "tcdate": 1520518144005, "number": 1, "cdate": 1520518144005, "id": "rkd-ETR_M", "invitation": "ICLR.cc/2018/Workshop/-/Paper249/Official_Review", "forum": "S1iiddyDG", "replyto": "S1iiddyDG", "signatures": ["ICLR.cc/2018/Workshop/Paper249/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper249/AnonReviewer3"], "content": {"title": "Interesting suggestions but way to preliminary", "rating": "4: Ok but not good enough - rejection", "review": "The main result reported in this contribution is that in a specific convolutional network the loss function decreases more in directions corresponding to negative eigenvalues of the Hessian. The authors suggest that this should be taken into account when designing the step-size used in learning. \n\nIt is not clear whether the authors expect their result to hold in learning in other types of neural networks, they set this as a goal for future work. But more importantly it is not clear whether this should hold in a more generic class of non-convex optimization problems  or only for the loss optimization in neural networks. \n\nThe authors speculate about ways how to take advantage of their observation and obtained improved algorithms, but do not test them in any setting. \n\nWhile the questions they are asking are interesting, they do not seem to be sufficiently advanced for this work to be interesting for presentation. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Negative eigenvalues of the Hessian in deep neural networks", "abstract": "We study the loss function of a deep neural network through the eigendecomposition of its Hessian matrix. We focus on negative eigenvalues, how important they are, and how to best deal with them. The goal is to develop an optimization method specifically tailored for deep neural networks.", "paperhash": "alain|negative_eigenvalues_of_the_hessian_in_deep_neural_networks", "keywords": ["Optimization", "Hessian matrix", "Neural Networks", "Negative Curvature"], "_bibtex": "@misc{\n  alain2018negative,\n  title={Negative eigenvalues of the Hessian in deep neural networks},\n  author={Guillaume Alain and Nicolas Le Roux and Pierre-Antoine Manzagol},\n  year={2018},\n  url={https://openreview.net/forum?id=S1iiddyDG}\n}", "authorids": ["guillaume.alain.umontreal@gmail.com", "nicolas@le-roux.name", "manzagop@google.com"], "authors": ["Guillaume Alain", "Nicolas Le Roux", "Pierre-Antoine Manzagol"], "TL;DR": "We study negative curvature of the loss of neural networks. We want to develop a better optimization method.", "pdf": "/pdf/2c184c7fec416cce2b96eefe6dc2db9536c60902.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582892303, "id": "ICLR.cc/2018/Workshop/-/Paper249/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper249/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper249/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper249/AnonReviewer1"], "reply": {"forum": "S1iiddyDG", "replyto": "S1iiddyDG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper249/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper249/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582892303}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582805730, "tcdate": 1520620625666, "number": 2, "cdate": 1520620625666, "id": "Hy5LN8eFG", "invitation": "ICLR.cc/2018/Workshop/-/Paper249/Official_Review", "forum": "S1iiddyDG", "replyto": "S1iiddyDG", "signatures": ["ICLR.cc/2018/Workshop/Paper249/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper249/AnonReviewer1"], "content": {"title": "finds existence proof (for 1 model and 1 dataset) where learning rate should be higher than Dauphin et al's suggested 1 / abs(eigenval) for negative curvature directions", "rating": "6: Marginally above acceptance threshold", "review": "The main contribution of this work is their simple empirical observation in Figure 4, that the optimal alpha (learning rate) appears to often NOT be 1 / | eigval | when the eigval is negative (contradicting the hypothesis of Dauphin et al. (2014), but instead should be much larger. This seems to this reader to be the only new insight offered by this paper, so not mentioning that connection to that well known related work until Appendix C is concerning (it should be mentioned in the main body).  They mention a way to estimate the negative eigenvector, but it is disappointing that they have \u201cnot yet tried this method in practice\u201d.  This paper is clearly premature / under-developed for a standard conference publication; whether suitable for a workshop is debatable.  On the one hand, it would be likely to prompt useful discussions at the workshop.  However, overall, given the lack of trying to use this insight in any actual optimization algorithm on any data, it could easily be seen as more flag planting at this point than a solid contribution.  It could be more suitable if their insights (Figure 4) were established for more than \u201conly one model and one dataset\u201d (as the authors admit).  Also, there has been a fair number of related past work on examining the Hessian (and the tracking of eigenvals over time), but the authors do not cite any.  As one example, LeCun has periodically shown such experiments, including seminal work at NIPS  https://papers.nips.cc/paper/589-automatic-learning-rate-maximization-by-on-line-estimation-of-the-hessians-eigenvectors.pdf and more recent ones on ArXiv.\n\nIn short, this might be marginally suitable as a workshop paper, but is overall under-developed and should do a better job of being very clear about its very modest contribution to date (and cite more related work).\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Negative eigenvalues of the Hessian in deep neural networks", "abstract": "We study the loss function of a deep neural network through the eigendecomposition of its Hessian matrix. We focus on negative eigenvalues, how important they are, and how to best deal with them. The goal is to develop an optimization method specifically tailored for deep neural networks.", "paperhash": "alain|negative_eigenvalues_of_the_hessian_in_deep_neural_networks", "keywords": ["Optimization", "Hessian matrix", "Neural Networks", "Negative Curvature"], "_bibtex": "@misc{\n  alain2018negative,\n  title={Negative eigenvalues of the Hessian in deep neural networks},\n  author={Guillaume Alain and Nicolas Le Roux and Pierre-Antoine Manzagol},\n  year={2018},\n  url={https://openreview.net/forum?id=S1iiddyDG}\n}", "authorids": ["guillaume.alain.umontreal@gmail.com", "nicolas@le-roux.name", "manzagop@google.com"], "authors": ["Guillaume Alain", "Nicolas Le Roux", "Pierre-Antoine Manzagol"], "TL;DR": "We study negative curvature of the loss of neural networks. We want to develop a better optimization method.", "pdf": "/pdf/2c184c7fec416cce2b96eefe6dc2db9536c60902.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582892303, "id": "ICLR.cc/2018/Workshop/-/Paper249/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper249/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper249/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper249/AnonReviewer1"], "reply": {"forum": "S1iiddyDG", "replyto": "S1iiddyDG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper249/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper249/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582892303}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573587247, "tcdate": 1521573587247, "number": 189, "cdate": 1521573586903, "id": "rJiR0RCKz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "S1iiddyDG", "replyto": "S1iiddyDG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop. While this work hasn't tried a new algorithm yet, the preliminary findings will make a really interesting poster."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Negative eigenvalues of the Hessian in deep neural networks", "abstract": "We study the loss function of a deep neural network through the eigendecomposition of its Hessian matrix. We focus on negative eigenvalues, how important they are, and how to best deal with them. The goal is to develop an optimization method specifically tailored for deep neural networks.", "paperhash": "alain|negative_eigenvalues_of_the_hessian_in_deep_neural_networks", "keywords": ["Optimization", "Hessian matrix", "Neural Networks", "Negative Curvature"], "_bibtex": "@misc{\n  alain2018negative,\n  title={Negative eigenvalues of the Hessian in deep neural networks},\n  author={Guillaume Alain and Nicolas Le Roux and Pierre-Antoine Manzagol},\n  year={2018},\n  url={https://openreview.net/forum?id=S1iiddyDG}\n}", "authorids": ["guillaume.alain.umontreal@gmail.com", "nicolas@le-roux.name", "manzagop@google.com"], "authors": ["Guillaume Alain", "Nicolas Le Roux", "Pierre-Antoine Manzagol"], "TL;DR": "We study negative curvature of the loss of neural networks. We want to develop a better optimization method.", "pdf": "/pdf/2c184c7fec416cce2b96eefe6dc2db9536c60902.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 4}