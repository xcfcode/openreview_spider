{"notes": [{"id": "xCm8kiWRiBT", "original": "wPxr8KYUs-e", "number": 1181, "cdate": 1601308132558, "ddate": null, "tcdate": 1601308132558, "tmdate": 1614985629613, "tddate": null, "forum": "xCm8kiWRiBT", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Adversarial Attacks on Binary Image Recognition Systems", "authorids": ["~Eric_Balkanski2", "harrison@robustintelligence.com", "kojin@robustintelligence.com", "rilee@robustintelligence.com", "yaron@robustintelligence.com", "richard@robustintelligence.com"], "authors": ["Eric Balkanski", "Harrison Chase", "Kojin Oshiba", "Alexander Rilee", "Yaron Singer", "Richard Wang"], "keywords": ["Adversarial attacks", "Binary images", "Image Recognition", "Check processing systems"], "abstract": "We initiate the study of adversarial attacks on models for binary (i.e. black and white) image classification. Although there has been a great deal of work on attacking models for colored and grayscale images, little is known about attacks on models for binary images. Models trained to classify binary images are used in text recognition applications such as check processing, license plate recognition, invoice processing, and many others. In contrast to colored and grayscale images, the search space of attacks on binary images is extremely restricted and noise cannot be hidden with minor perturbations in each pixel. Thus, the optimization landscape of attacks on binary images introduces new fundamental challenges.\n\nIn this paper we introduce a new attack algorithm called Scar, designed to fool classifiers of binary images. We show that Scar significantly outperforms existing L0 attacks applied to the binary setting and use it to demonstrate the vulnerability of real-world text recognition systems. Scar\u2019s strong performance in practice contrasts with hardness results that show the existence of worst-case classifiers for binary images that are robust to large perturbations. In many cases, altering a single pixel is sufficient to trick Tesseract, a popular open-source text recognition system, to misclassify a word as a different word in the English dictionary. We also demonstrate the vulnerability of check recognition by fooling commercial check processing systems used by major US banks for mobile deposits. These systems are substantially harder to fool since they classify both the handwritten amounts in digits and letters, independently. Nevertheless, we generalize Scar to design attacks that fool state-of-the-art check processing systems using unnoticeable perturbations that lead to misclassification of deposit amounts. Consequently, this is a powerful method to perform financial fraud.", "one-sentence_summary": "We study adversarial attacks on models designed to classify binary (i.e. black and white) images.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "balkanski|adversarial_attacks_on_binary_image_recognition_systems", "pdf": "/pdf/3f21ccc27b4d08f6b5124ce802699df8807aa788.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=lGBRJc3fCg", "_bibtex": "@misc{\nbalkanski2021adversarial,\ntitle={Adversarial Attacks on Binary Image Recognition Systems},\nauthor={Eric Balkanski and Harrison Chase and Kojin Oshiba and Alexander Rilee and Yaron Singer and Richard Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=xCm8kiWRiBT}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "aF5RLPB4ku-", "original": null, "number": 1, "cdate": 1610040530771, "ddate": null, "tcdate": 1610040530771, "tmdate": 1610474140308, "tddate": null, "forum": "xCm8kiWRiBT", "replyto": "xCm8kiWRiBT", "invitation": "ICLR.cc/2021/Conference/Paper1181/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper was referred to the ICLR 2021 Ethics Review Committee based on concerns about a potential violation of the ICLR 2021 Code of Ethics (https://iclr.cc/public/CodeOfEthics) raised by reviewers. The paper was carefully reviewed by two committee members, who provided a binding decision. The decision is \"Significant concerns (Do not publish)\". Details are provided in the Ethics Meta Review. As a result, the paper is rejected based Ethics Review Committee's decision .\n\nThe technical review and meta reviewing process moved proceeded independently of the ethics review. The result is as follows:\n\nThis paper considers sparse (L0) attacks against binary images analysis systems, in particular OCR.  The major concern of the reviewers seems to be similarity to other methods in the literature, but reviewers did not specify any specific methods to compare to.  Because it was not possible for reviewers to address such vague concerns, and because I believe the authors did a good job differentiating their work in the rebuttal, I think the paper is of good merit.  "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Binary Image Recognition Systems", "authorids": ["~Eric_Balkanski2", "harrison@robustintelligence.com", "kojin@robustintelligence.com", "rilee@robustintelligence.com", "yaron@robustintelligence.com", "richard@robustintelligence.com"], "authors": ["Eric Balkanski", "Harrison Chase", "Kojin Oshiba", "Alexander Rilee", "Yaron Singer", "Richard Wang"], "keywords": ["Adversarial attacks", "Binary images", "Image Recognition", "Check processing systems"], "abstract": "We initiate the study of adversarial attacks on models for binary (i.e. black and white) image classification. Although there has been a great deal of work on attacking models for colored and grayscale images, little is known about attacks on models for binary images. Models trained to classify binary images are used in text recognition applications such as check processing, license plate recognition, invoice processing, and many others. In contrast to colored and grayscale images, the search space of attacks on binary images is extremely restricted and noise cannot be hidden with minor perturbations in each pixel. Thus, the optimization landscape of attacks on binary images introduces new fundamental challenges.\n\nIn this paper we introduce a new attack algorithm called Scar, designed to fool classifiers of binary images. We show that Scar significantly outperforms existing L0 attacks applied to the binary setting and use it to demonstrate the vulnerability of real-world text recognition systems. Scar\u2019s strong performance in practice contrasts with hardness results that show the existence of worst-case classifiers for binary images that are robust to large perturbations. In many cases, altering a single pixel is sufficient to trick Tesseract, a popular open-source text recognition system, to misclassify a word as a different word in the English dictionary. We also demonstrate the vulnerability of check recognition by fooling commercial check processing systems used by major US banks for mobile deposits. These systems are substantially harder to fool since they classify both the handwritten amounts in digits and letters, independently. Nevertheless, we generalize Scar to design attacks that fool state-of-the-art check processing systems using unnoticeable perturbations that lead to misclassification of deposit amounts. Consequently, this is a powerful method to perform financial fraud.", "one-sentence_summary": "We study adversarial attacks on models designed to classify binary (i.e. black and white) images.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "balkanski|adversarial_attacks_on_binary_image_recognition_systems", "pdf": "/pdf/3f21ccc27b4d08f6b5124ce802699df8807aa788.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=lGBRJc3fCg", "_bibtex": "@misc{\nbalkanski2021adversarial,\ntitle={Adversarial Attacks on Binary Image Recognition Systems},\nauthor={Eric Balkanski and Harrison Chase and Kojin Oshiba and Alexander Rilee and Yaron Singer and Richard Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=xCm8kiWRiBT}\n}"}, "tags": [], "invitation": {"reply": {"forum": "xCm8kiWRiBT", "replyto": "xCm8kiWRiBT", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040530757, "tmdate": 1610474140292, "id": "ICLR.cc/2021/Conference/Paper1181/-/Decision"}}}, {"id": "0R-iEJVin4b", "original": null, "number": 1, "cdate": 1609959258460, "ddate": null, "tcdate": 1609959258460, "tmdate": 1609959258460, "tddate": null, "forum": "xCm8kiWRiBT", "replyto": "xCm8kiWRiBT", "invitation": "ICLR.cc/2021/Conference/Paper1181/-/Ethics_Meta_Review", "content": {"decision": "Significant concerns (Do not publish)", "ethics_review": "This paper was referred to the Ethics Panel because of the comment by AnonReviewer4: \u201cThe authors make the case that this is a realistic attack on a high-value target. I believe it is worth opening a discussion on whether this raises ethical issues for publication.\u201d The primary ethical issue is whether the authors have followed procedures for responsible disclosure of risks (see https://en.wikipedia.org/wiki/Responsible_disclosure). This relates to the ICLR Code of Ethics principle of \u201cAvoid Harm\u201d. We have the following questions for the authors:\n1.\tHow serious is the vulnerability? How difficult would it be for someone to exploit this vulnerability in practice? The paper claims that it would be easy to hijack the camera on an Android phone to feed it the modified image. If this is true, then disclosing this vulnerability by submitting to ICLR is already a violation of the ICLR Code of Ethics because all submissions are publicly available via OpenReview. We note that the manuscript was also released on arXiv on October 22, 2020.\n2.\tAre there inexpensive counter-measures to the attack? Are the confidence scores of the attacked images low so that a confidence threshold could be applied to detect the attack? It is reasonable to expect that banks have additional verification steps in the check processing pipeline. How easy do you believe it would be for banks to detect and evade these attacks?\n3.\tThe paper does not identify the company that sells the \u201ccommercial check processing system\u201d, so we were not able to determine whether that company has a Responsible Disclosure Policy for security vulnerabilities. Have the authors determined whether the company has such a policy?\n4.\tHave the authors contacted the company and informed them of the vulnerability? \n5.\tIf so, have the authors given the company a reasonable time period to assess the impact of the vulnerability on actual banking operations? \nWithout knowing the answers to these questions, we cannot make a final recommendation concerning this paper. We would like to see evidence that the authors have followed procedures (formal or informal) for responsible disclosure prior to submitting the manuscript to ICLR 2021. If they have done so, there are no further ethical considerations. If they have not done so, then we recommend rejecting the paper. We realize that rejecting the paper will not prevent the harms resulting from the disclosure, because the manuscript has already been disclosed on OpenReview and arXiv. But rejecting the paper is the only mechanism available to ICLR for incentivizing authors to comply with its Code of Ethics. \n\nNotes for ICLR: ICLR should consider whether it needs to modify its submission processes to prevent irresponsible disclosure on OpenReview. For example, authors should be asked whether the manuscript discloses any novel vulnerabilities in deployed software systems. If the authors answer affirmatively, the authors should be required to certify that they have followed procedures for responsible disclosure. Responsible disclosure practices should be addressed explicitly in the Code of Ethics. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1181/Ethics_Committee"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Paper1181/Ethics_Committee"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Binary Image Recognition Systems", "authorids": ["~Eric_Balkanski2", "harrison@robustintelligence.com", "kojin@robustintelligence.com", "rilee@robustintelligence.com", "yaron@robustintelligence.com", "richard@robustintelligence.com"], "authors": ["Eric Balkanski", "Harrison Chase", "Kojin Oshiba", "Alexander Rilee", "Yaron Singer", "Richard Wang"], "keywords": ["Adversarial attacks", "Binary images", "Image Recognition", "Check processing systems"], "abstract": "We initiate the study of adversarial attacks on models for binary (i.e. black and white) image classification. Although there has been a great deal of work on attacking models for colored and grayscale images, little is known about attacks on models for binary images. Models trained to classify binary images are used in text recognition applications such as check processing, license plate recognition, invoice processing, and many others. In contrast to colored and grayscale images, the search space of attacks on binary images is extremely restricted and noise cannot be hidden with minor perturbations in each pixel. Thus, the optimization landscape of attacks on binary images introduces new fundamental challenges.\n\nIn this paper we introduce a new attack algorithm called Scar, designed to fool classifiers of binary images. We show that Scar significantly outperforms existing L0 attacks applied to the binary setting and use it to demonstrate the vulnerability of real-world text recognition systems. Scar\u2019s strong performance in practice contrasts with hardness results that show the existence of worst-case classifiers for binary images that are robust to large perturbations. In many cases, altering a single pixel is sufficient to trick Tesseract, a popular open-source text recognition system, to misclassify a word as a different word in the English dictionary. We also demonstrate the vulnerability of check recognition by fooling commercial check processing systems used by major US banks for mobile deposits. These systems are substantially harder to fool since they classify both the handwritten amounts in digits and letters, independently. Nevertheless, we generalize Scar to design attacks that fool state-of-the-art check processing systems using unnoticeable perturbations that lead to misclassification of deposit amounts. Consequently, this is a powerful method to perform financial fraud.", "one-sentence_summary": "We study adversarial attacks on models designed to classify binary (i.e. black and white) images.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "balkanski|adversarial_attacks_on_binary_image_recognition_systems", "pdf": "/pdf/3f21ccc27b4d08f6b5124ce802699df8807aa788.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=lGBRJc3fCg", "_bibtex": "@misc{\nbalkanski2021adversarial,\ntitle={Adversarial Attacks on Binary Image Recognition Systems},\nauthor={Eric Balkanski and Harrison Chase and Kojin Oshiba and Alexander Rilee and Yaron Singer and Richard Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=xCm8kiWRiBT}\n}"}, "tags": [], "invitation": {"reply": {"forum": "xCm8kiWRiBT", "replyto": "xCm8kiWRiBT", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Paper1181/Ethics_Committee"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Paper1181/Ethics_Committee"]}, "content": {"decision": {"value-radio": ["Significant concerns (Do not publish)", "Concerns raised (can publish with adjustment)", "No judgement (proceed with normal process)"], "order": 1, "required": true}, "ethics_review": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Your review (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "duedate": 1578844800000, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1181/Ethics_Committee"], "tcdate": 1606763195233, "tmdate": 1606769758550, "id": "ICLR.cc/2021/Conference/Paper1181/-/Ethics_Meta_Review"}}}, {"id": "e_Ib5HiR4g7", "original": null, "number": 6, "cdate": 1605383207332, "ddate": null, "tcdate": 1605383207332, "tmdate": 1605383207332, "tddate": null, "forum": "xCm8kiWRiBT", "replyto": "KI6euUN2-K", "invitation": "ICLR.cc/2021/Conference/Paper1181/-/Official_Comment", "content": {"title": "Addressing the list of Cons", "comment": "Thank you for your careful review and your positive comments --- we believe we address your reservations here and would appreciate it if you would consider improving your score in light of our response.\n\n\u201cslightly increasing the font size of the same letter would incur a large L0 distance while making the modification imperceptible.\u201d: We note that attacks on MNIST, for example, have been extensively studied for years and that this issue with the font size holds, to the best of our knowledge, for all the metrics (L0, L2, L_infty, \u2026) that have been proposed. The L0, L2, and L_infty metrics are the three standard distance metrics and that, out of these three metrics, only the L0 distance is relevant in the binary setting.\n\n\u201cthe classifiers used in section 6.2 seems too simple\u201d: We evaluate our algorithm on a wide range of classifiers, including some simple classifiers (LogReg, MLP2, SVM) and some more complex classifiers (CNN, LeNet5). As shown in the appendix B.1, the CNN and LeNet5 classifiers achieve over 99% accuracy on MNIST, which is comparable to the state-of-the-art. Finally, we also note that more complex classifiers are not necessarily more robust to attacks. In fact, in Appendix B.2, we observe that the SVM classifier is more robust to attacks than CNN and LeNet5.\n\n\u201cit would make the case stronger if simple defense algorithms are also considered\u201d: Evaluating simple defense algorithms is an interesting direction for future work. As a first step to understanding the vulnerabilities of binary classifiers, we think it is most relevant to evaluate classifiers without defenses, which corresponds to the classifiers used in industry such as for check processing systems.\n\n\u201ctau is set to 0.1. How is tau selected and how does tau impact the performance?\u201d: A larger threshold tau would produce a better attack, but at the cost of a larger number of queries. A smaller tau would improve the number of queries to the classifier, but would produce a worse attack. We pick tau such that it achieves a good balance between the two extremes. We will make this clearer.\n\n\u201cPlease consider rewriting the last sentence of the abstract.\u201d: Thank you for the suggestion, we suggest changing it to \u201cConsequently, it is important to raise awareness of the vulnerabilities of check processing systems and other binary image recognition systems.\u201d"}, "signatures": ["ICLR.cc/2021/Conference/Paper1181/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1181/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Binary Image Recognition Systems", "authorids": ["~Eric_Balkanski2", "harrison@robustintelligence.com", "kojin@robustintelligence.com", "rilee@robustintelligence.com", "yaron@robustintelligence.com", "richard@robustintelligence.com"], "authors": ["Eric Balkanski", "Harrison Chase", "Kojin Oshiba", "Alexander Rilee", "Yaron Singer", "Richard Wang"], "keywords": ["Adversarial attacks", "Binary images", "Image Recognition", "Check processing systems"], "abstract": "We initiate the study of adversarial attacks on models for binary (i.e. black and white) image classification. Although there has been a great deal of work on attacking models for colored and grayscale images, little is known about attacks on models for binary images. Models trained to classify binary images are used in text recognition applications such as check processing, license plate recognition, invoice processing, and many others. In contrast to colored and grayscale images, the search space of attacks on binary images is extremely restricted and noise cannot be hidden with minor perturbations in each pixel. Thus, the optimization landscape of attacks on binary images introduces new fundamental challenges.\n\nIn this paper we introduce a new attack algorithm called Scar, designed to fool classifiers of binary images. We show that Scar significantly outperforms existing L0 attacks applied to the binary setting and use it to demonstrate the vulnerability of real-world text recognition systems. Scar\u2019s strong performance in practice contrasts with hardness results that show the existence of worst-case classifiers for binary images that are robust to large perturbations. In many cases, altering a single pixel is sufficient to trick Tesseract, a popular open-source text recognition system, to misclassify a word as a different word in the English dictionary. We also demonstrate the vulnerability of check recognition by fooling commercial check processing systems used by major US banks for mobile deposits. These systems are substantially harder to fool since they classify both the handwritten amounts in digits and letters, independently. Nevertheless, we generalize Scar to design attacks that fool state-of-the-art check processing systems using unnoticeable perturbations that lead to misclassification of deposit amounts. Consequently, this is a powerful method to perform financial fraud.", "one-sentence_summary": "We study adversarial attacks on models designed to classify binary (i.e. black and white) images.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "balkanski|adversarial_attacks_on_binary_image_recognition_systems", "pdf": "/pdf/3f21ccc27b4d08f6b5124ce802699df8807aa788.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=lGBRJc3fCg", "_bibtex": "@misc{\nbalkanski2021adversarial,\ntitle={Adversarial Attacks on Binary Image Recognition Systems},\nauthor={Eric Balkanski and Harrison Chase and Kojin Oshiba and Alexander Rilee and Yaron Singer and Richard Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=xCm8kiWRiBT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "xCm8kiWRiBT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1181/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1181/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1181/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1181/Authors|ICLR.cc/2021/Conference/Paper1181/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs|ICLR.cc/2021/Conference/Paper1181/Ethics_Committee", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1181/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862714, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1181/-/Official_Comment"}}}, {"id": "W_Zz0P__9Cc", "original": null, "number": 5, "cdate": 1605383094119, "ddate": null, "tcdate": 1605383094119, "tmdate": 1605383094119, "tddate": null, "forum": "xCm8kiWRiBT", "replyto": "5pfEZ1-sJj", "invitation": "ICLR.cc/2021/Conference/Paper1181/-/Official_Comment", "content": {"title": "Previous L0 attacks are ineffective in the binary setting", "comment": "Thank you for your careful review and your positive comments --- we believe we address your main reservation that existing attacks can be employed in the binary setting. We would appreciate it if you would consider improving your score in light of our response.\n\n\u201cattacking binary image classifiers is a special case of L0 attacks\u201d: See paragraph titled \u201cPrevious attacks are ineffective in the binary setting\u201d on page 2 for an explanation of why L0 attacks cannot be applied to the binary setting. If the reviewer believes that L0 attacks can be adapted to the binary setting, we would appreciate it if the reviewer would describe how this is possible.\n\n\u201cSimilar heuristic algorithms have also been developed in previous black-box attack algorithms.\u201d If the reviewer believes that similar algorithms have been previously proposed, it would be helpful to include citations to these similar algorithms."}, "signatures": ["ICLR.cc/2021/Conference/Paper1181/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1181/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Binary Image Recognition Systems", "authorids": ["~Eric_Balkanski2", "harrison@robustintelligence.com", "kojin@robustintelligence.com", "rilee@robustintelligence.com", "yaron@robustintelligence.com", "richard@robustintelligence.com"], "authors": ["Eric Balkanski", "Harrison Chase", "Kojin Oshiba", "Alexander Rilee", "Yaron Singer", "Richard Wang"], "keywords": ["Adversarial attacks", "Binary images", "Image Recognition", "Check processing systems"], "abstract": "We initiate the study of adversarial attacks on models for binary (i.e. black and white) image classification. Although there has been a great deal of work on attacking models for colored and grayscale images, little is known about attacks on models for binary images. Models trained to classify binary images are used in text recognition applications such as check processing, license plate recognition, invoice processing, and many others. In contrast to colored and grayscale images, the search space of attacks on binary images is extremely restricted and noise cannot be hidden with minor perturbations in each pixel. Thus, the optimization landscape of attacks on binary images introduces new fundamental challenges.\n\nIn this paper we introduce a new attack algorithm called Scar, designed to fool classifiers of binary images. We show that Scar significantly outperforms existing L0 attacks applied to the binary setting and use it to demonstrate the vulnerability of real-world text recognition systems. Scar\u2019s strong performance in practice contrasts with hardness results that show the existence of worst-case classifiers for binary images that are robust to large perturbations. In many cases, altering a single pixel is sufficient to trick Tesseract, a popular open-source text recognition system, to misclassify a word as a different word in the English dictionary. We also demonstrate the vulnerability of check recognition by fooling commercial check processing systems used by major US banks for mobile deposits. These systems are substantially harder to fool since they classify both the handwritten amounts in digits and letters, independently. Nevertheless, we generalize Scar to design attacks that fool state-of-the-art check processing systems using unnoticeable perturbations that lead to misclassification of deposit amounts. Consequently, this is a powerful method to perform financial fraud.", "one-sentence_summary": "We study adversarial attacks on models designed to classify binary (i.e. black and white) images.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "balkanski|adversarial_attacks_on_binary_image_recognition_systems", "pdf": "/pdf/3f21ccc27b4d08f6b5124ce802699df8807aa788.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=lGBRJc3fCg", "_bibtex": "@misc{\nbalkanski2021adversarial,\ntitle={Adversarial Attacks on Binary Image Recognition Systems},\nauthor={Eric Balkanski and Harrison Chase and Kojin Oshiba and Alexander Rilee and Yaron Singer and Richard Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=xCm8kiWRiBT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "xCm8kiWRiBT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1181/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1181/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1181/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1181/Authors|ICLR.cc/2021/Conference/Paper1181/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs|ICLR.cc/2021/Conference/Paper1181/Ethics_Committee", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1181/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862714, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1181/-/Official_Comment"}}}, {"id": "RXFkyG06U4", "original": null, "number": 4, "cdate": 1605382958636, "ddate": null, "tcdate": 1605382958636, "tmdate": 1605382958636, "tddate": null, "forum": "xCm8kiWRiBT", "replyto": "VKbkCFx1_UA", "invitation": "ICLR.cc/2021/Conference/Paper1181/-/Official_Comment", "content": {"title": "Addressing two main issues ", "comment": "Thank you for your careful review and your positive comments --- we believe we address your two main issues here and would appreciate it if you would consider improving your score in light of our response.\n\n\u201cwhether this raises ethical issues for publication.\u201d: The main issue is that banks and other industries that use binary image classification systems are not aware of the vulnerabilities that we demonstrate in this paper. In many areas, such as cybersecurity, academic papers have played an important role to advance the security of different systems by uncovering vulnerabilities before fraudsters and hackers start taking advantage of them.\n\nConnections to sparse adversarial attacks: Thank you for pointing out these two papers, we will include them in our discussion of related work. See the relation to our paper below:\n\nConnection to \u201cSparseFool: a few pixels make a big difference\u201d: SparseFool relies on an L1 relaxation of the L0 problem. L1 relaxations are not applicable in the binary setting since there are only two possible values for each pixel.\n\nConnection to \u201cOne Pixel Attack for Fooling Deep Neural Networks\u201d: This paper focuses on attacks which modify a single pixel. In the binary setting, if it was possible to attack a classifier by modifying a single pixel, Vanilla SCAR would have found that pixel in the first iteration, since it attempts to flip every pixel. However, as we see in the experiments, in the overwhelming number of cases, Vanilla SCAR flips more than one pixel. Therefore, multiple pixel flips are necessary in order to fool binary classifiers in most cases and one pixel attacks do not apply.\n\nNumerical results are hard to read: Thank you for the suggestion, we will add a table in the next version of the manuscript.\n\n\u201cIf L_0 distance isn't the goal, why not?\u201d: We believe that the L0 distance is a simple and fair objective for comparing the performance of SCAR to other algorithms which aim to minimize L0 distance. However, as discussed, and similarly to any distance metric we are aware of for adversarial attacks, the L0 distance is not perfect and we discuss why. We don\u2019t believe that there exists a universal metric which perfectly measures the perceptibility of any noise added to any image. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1181/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1181/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Binary Image Recognition Systems", "authorids": ["~Eric_Balkanski2", "harrison@robustintelligence.com", "kojin@robustintelligence.com", "rilee@robustintelligence.com", "yaron@robustintelligence.com", "richard@robustintelligence.com"], "authors": ["Eric Balkanski", "Harrison Chase", "Kojin Oshiba", "Alexander Rilee", "Yaron Singer", "Richard Wang"], "keywords": ["Adversarial attacks", "Binary images", "Image Recognition", "Check processing systems"], "abstract": "We initiate the study of adversarial attacks on models for binary (i.e. black and white) image classification. Although there has been a great deal of work on attacking models for colored and grayscale images, little is known about attacks on models for binary images. Models trained to classify binary images are used in text recognition applications such as check processing, license plate recognition, invoice processing, and many others. In contrast to colored and grayscale images, the search space of attacks on binary images is extremely restricted and noise cannot be hidden with minor perturbations in each pixel. Thus, the optimization landscape of attacks on binary images introduces new fundamental challenges.\n\nIn this paper we introduce a new attack algorithm called Scar, designed to fool classifiers of binary images. We show that Scar significantly outperforms existing L0 attacks applied to the binary setting and use it to demonstrate the vulnerability of real-world text recognition systems. Scar\u2019s strong performance in practice contrasts with hardness results that show the existence of worst-case classifiers for binary images that are robust to large perturbations. In many cases, altering a single pixel is sufficient to trick Tesseract, a popular open-source text recognition system, to misclassify a word as a different word in the English dictionary. We also demonstrate the vulnerability of check recognition by fooling commercial check processing systems used by major US banks for mobile deposits. These systems are substantially harder to fool since they classify both the handwritten amounts in digits and letters, independently. Nevertheless, we generalize Scar to design attacks that fool state-of-the-art check processing systems using unnoticeable perturbations that lead to misclassification of deposit amounts. Consequently, this is a powerful method to perform financial fraud.", "one-sentence_summary": "We study adversarial attacks on models designed to classify binary (i.e. black and white) images.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "balkanski|adversarial_attacks_on_binary_image_recognition_systems", "pdf": "/pdf/3f21ccc27b4d08f6b5124ce802699df8807aa788.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=lGBRJc3fCg", "_bibtex": "@misc{\nbalkanski2021adversarial,\ntitle={Adversarial Attacks on Binary Image Recognition Systems},\nauthor={Eric Balkanski and Harrison Chase and Kojin Oshiba and Alexander Rilee and Yaron Singer and Richard Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=xCm8kiWRiBT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "xCm8kiWRiBT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1181/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1181/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1181/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1181/Authors|ICLR.cc/2021/Conference/Paper1181/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs|ICLR.cc/2021/Conference/Paper1181/Ethics_Committee", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1181/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862714, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1181/-/Official_Comment"}}}, {"id": "FAMnjwtZPuS", "original": null, "number": 3, "cdate": 1605382785305, "ddate": null, "tcdate": 1605382785305, "tmdate": 1605382785305, "tddate": null, "forum": "xCm8kiWRiBT", "replyto": "3B2lt5tJyn", "invitation": "ICLR.cc/2021/Conference/Paper1181/-/Official_Comment", "content": {"title": "Clarification about connection between provably robust classifiers and the rest of paper", "comment": "Thank you for your overall positive review. \n\n\u201cit is not clear to me how this is related to the rest of paper\u2019s discussion\u201d: A natural question is whether we can upper bound the number of pixel inversions needed by our SCAR algorithm to attack any classifier. The existence of provably robust classifiers show that, in general, there is no algorithm, including SCAR, that can attack any classifier by inverting a small number of pixels. We will make this connection clearer. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1181/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1181/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Binary Image Recognition Systems", "authorids": ["~Eric_Balkanski2", "harrison@robustintelligence.com", "kojin@robustintelligence.com", "rilee@robustintelligence.com", "yaron@robustintelligence.com", "richard@robustintelligence.com"], "authors": ["Eric Balkanski", "Harrison Chase", "Kojin Oshiba", "Alexander Rilee", "Yaron Singer", "Richard Wang"], "keywords": ["Adversarial attacks", "Binary images", "Image Recognition", "Check processing systems"], "abstract": "We initiate the study of adversarial attacks on models for binary (i.e. black and white) image classification. Although there has been a great deal of work on attacking models for colored and grayscale images, little is known about attacks on models for binary images. Models trained to classify binary images are used in text recognition applications such as check processing, license plate recognition, invoice processing, and many others. In contrast to colored and grayscale images, the search space of attacks on binary images is extremely restricted and noise cannot be hidden with minor perturbations in each pixel. Thus, the optimization landscape of attacks on binary images introduces new fundamental challenges.\n\nIn this paper we introduce a new attack algorithm called Scar, designed to fool classifiers of binary images. We show that Scar significantly outperforms existing L0 attacks applied to the binary setting and use it to demonstrate the vulnerability of real-world text recognition systems. Scar\u2019s strong performance in practice contrasts with hardness results that show the existence of worst-case classifiers for binary images that are robust to large perturbations. In many cases, altering a single pixel is sufficient to trick Tesseract, a popular open-source text recognition system, to misclassify a word as a different word in the English dictionary. We also demonstrate the vulnerability of check recognition by fooling commercial check processing systems used by major US banks for mobile deposits. These systems are substantially harder to fool since they classify both the handwritten amounts in digits and letters, independently. Nevertheless, we generalize Scar to design attacks that fool state-of-the-art check processing systems using unnoticeable perturbations that lead to misclassification of deposit amounts. Consequently, this is a powerful method to perform financial fraud.", "one-sentence_summary": "We study adversarial attacks on models designed to classify binary (i.e. black and white) images.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "balkanski|adversarial_attacks_on_binary_image_recognition_systems", "pdf": "/pdf/3f21ccc27b4d08f6b5124ce802699df8807aa788.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=lGBRJc3fCg", "_bibtex": "@misc{\nbalkanski2021adversarial,\ntitle={Adversarial Attacks on Binary Image Recognition Systems},\nauthor={Eric Balkanski and Harrison Chase and Kojin Oshiba and Alexander Rilee and Yaron Singer and Richard Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=xCm8kiWRiBT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "xCm8kiWRiBT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1181/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1181/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1181/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1181/Authors|ICLR.cc/2021/Conference/Paper1181/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs|ICLR.cc/2021/Conference/Paper1181/Ethics_Committee", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1181/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862714, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1181/-/Official_Comment"}}}, {"id": "KI6euUN2-K", "original": null, "number": 1, "cdate": 1603767335779, "ddate": null, "tcdate": 1603767335779, "tmdate": 1605024509768, "tddate": null, "forum": "xCm8kiWRiBT", "replyto": "xCm8kiWRiBT", "invitation": "ICLR.cc/2021/Conference/Paper1181/-/Official_Review", "content": {"title": "A borderline paper with clear motivation and method description but lacks rigorous evaluation", "review": "##########################################################################\n\nSummary:\n\nAdversarial attacks for binary image classification are unique from traditional attacks on color images due to its limited available space for perturbation.  This paper proposes an algorithm that efficiently searches for valid attacks (both targeted and untargeted) which cause minimum flipped pixels. The proposed method is evaluated on digit classification, letter classification, and check processing systems. The baselines compared are mostly methods that were originally developed for color images.\n\n\n##########################################################################\n\nReasons for score:\n\nThis is a borderline paper which I tend to vote for rejection. The problem is well-defined and the method is clearly introduced. However, the evaluation of the proposed method is flawed. In particular, the classifiers being attacked seem to be simple and the perceptibility metric is questionable.\n\n\n##########################################################################\n\nPros:\n\nThis paper proposes a novel problem with a non-trivial impact. The problem of binary image classification is also well-motivated. The authors provide a clear explanation of how attacks on the binary images are different from those on color images.\n \nThe authors provide theoretical analysis about the upper limit of pixels that need to be flipped in order to confuse the classifier.\n\nThe related work section is well-organized and the authors clearly state why previous works on color images do not apply to the binary image settings.\n\nBoth the problem formulation and description of the method are well-organized. \n\n\n\n##########################################################################\n\nCons:\n\nIn the problem formulation, one constraint is that the attacks should be imperceptible to humans. This is measured by D_x(x'). However, D_x is parameterized as L0 distance which is not convincing. For instance, slightly increasing the font size of the same letter would incur a large L0 distance while making the modification imperceptible. On the other hand, transforming the letter \"I\" to \"T\" might incur a very small L0 distance while causing perceptible change. The authors should justify the use of L0 or consider experimenting with other parameterizations of D_x.\n\nWhile the classifier used in section 6.3 is carefully selected and well justified, the classifiers used in section 6.2 seems too simple. In addition, instead of attacking plain classifiers, it would make the case stronger if simple defense algorithms are also considered.\n\nThe results in Section 6.5 do not have baseline algorithms to compare with. Also, it would be helpful if a reference to the check processing system is provided.\n\nThis paper lacks a conclusion.\n\n\n##########################################################################\n\nSuggestions & Questions:\n\nIn section 6.1, tau is set to 0.1. How is tau selected and how does tau impact the performance?\n\nPlease consider rewriting the last sentence of the abstract.\n\nPlease address the comments in the cons.\n\n\n#########################################################################", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1181/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1181/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Binary Image Recognition Systems", "authorids": ["~Eric_Balkanski2", "harrison@robustintelligence.com", "kojin@robustintelligence.com", "rilee@robustintelligence.com", "yaron@robustintelligence.com", "richard@robustintelligence.com"], "authors": ["Eric Balkanski", "Harrison Chase", "Kojin Oshiba", "Alexander Rilee", "Yaron Singer", "Richard Wang"], "keywords": ["Adversarial attacks", "Binary images", "Image Recognition", "Check processing systems"], "abstract": "We initiate the study of adversarial attacks on models for binary (i.e. black and white) image classification. Although there has been a great deal of work on attacking models for colored and grayscale images, little is known about attacks on models for binary images. Models trained to classify binary images are used in text recognition applications such as check processing, license plate recognition, invoice processing, and many others. In contrast to colored and grayscale images, the search space of attacks on binary images is extremely restricted and noise cannot be hidden with minor perturbations in each pixel. Thus, the optimization landscape of attacks on binary images introduces new fundamental challenges.\n\nIn this paper we introduce a new attack algorithm called Scar, designed to fool classifiers of binary images. We show that Scar significantly outperforms existing L0 attacks applied to the binary setting and use it to demonstrate the vulnerability of real-world text recognition systems. Scar\u2019s strong performance in practice contrasts with hardness results that show the existence of worst-case classifiers for binary images that are robust to large perturbations. In many cases, altering a single pixel is sufficient to trick Tesseract, a popular open-source text recognition system, to misclassify a word as a different word in the English dictionary. We also demonstrate the vulnerability of check recognition by fooling commercial check processing systems used by major US banks for mobile deposits. These systems are substantially harder to fool since they classify both the handwritten amounts in digits and letters, independently. Nevertheless, we generalize Scar to design attacks that fool state-of-the-art check processing systems using unnoticeable perturbations that lead to misclassification of deposit amounts. Consequently, this is a powerful method to perform financial fraud.", "one-sentence_summary": "We study adversarial attacks on models designed to classify binary (i.e. black and white) images.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "balkanski|adversarial_attacks_on_binary_image_recognition_systems", "pdf": "/pdf/3f21ccc27b4d08f6b5124ce802699df8807aa788.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=lGBRJc3fCg", "_bibtex": "@misc{\nbalkanski2021adversarial,\ntitle={Adversarial Attacks on Binary Image Recognition Systems},\nauthor={Eric Balkanski and Harrison Chase and Kojin Oshiba and Alexander Rilee and Yaron Singer and Richard Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=xCm8kiWRiBT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "xCm8kiWRiBT", "replyto": "xCm8kiWRiBT", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1181/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124816, "tmdate": 1606915808662, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1181/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1181/-/Official_Review"}}}, {"id": "5pfEZ1-sJj", "original": null, "number": 2, "cdate": 1603906132113, "ddate": null, "tcdate": 1603906132113, "tmdate": 1605024509707, "tddate": null, "forum": "xCm8kiWRiBT", "replyto": "xCm8kiWRiBT", "invitation": "ICLR.cc/2021/Conference/Paper1181/-/Official_Review", "content": {"title": "Official Blind Review2", "review": "##################################################################\nSummary:\nThis paper targets at a challenging task of attacking binary image classification. A score-based black-box attack algorithm SCAR is designed to fool Tesseract and US banks\u2019 commercial check processing systems.\n\n##################################################################\nPros:\n(1) This paper introduces a challenging task of attacking binary image classifiers.\n(2) The proposed method SCAR is simple yet effective, which successfully fools Tesseract and the state-of-the-art check processing systems. \n(3) The paper reads smooth and is mostly well-written.\n\n##################################################################\nCons:\n(1)\tThe task of attacking binary image classifiers is a special case of L0 attacks, which also aim at perturbing a small number of pixels. The reviewer thinks that most L0 attack algorithms can be adapted to this binary attack task. The reviewer has concerns that since L0 attack has been widely explored, do we really need to study a much narrow task of binary attack? \n(2)\tThe novelty of the proposed method is somewhat limited. The proposed SCAR greedily selects the modified pixels, by considering spatial and temporal correlations. However, this method is not new enough. Similar heuristic algorithms have also been developed in previous black-box attack algorithms.  \n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1181/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1181/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Binary Image Recognition Systems", "authorids": ["~Eric_Balkanski2", "harrison@robustintelligence.com", "kojin@robustintelligence.com", "rilee@robustintelligence.com", "yaron@robustintelligence.com", "richard@robustintelligence.com"], "authors": ["Eric Balkanski", "Harrison Chase", "Kojin Oshiba", "Alexander Rilee", "Yaron Singer", "Richard Wang"], "keywords": ["Adversarial attacks", "Binary images", "Image Recognition", "Check processing systems"], "abstract": "We initiate the study of adversarial attacks on models for binary (i.e. black and white) image classification. Although there has been a great deal of work on attacking models for colored and grayscale images, little is known about attacks on models for binary images. Models trained to classify binary images are used in text recognition applications such as check processing, license plate recognition, invoice processing, and many others. In contrast to colored and grayscale images, the search space of attacks on binary images is extremely restricted and noise cannot be hidden with minor perturbations in each pixel. Thus, the optimization landscape of attacks on binary images introduces new fundamental challenges.\n\nIn this paper we introduce a new attack algorithm called Scar, designed to fool classifiers of binary images. We show that Scar significantly outperforms existing L0 attacks applied to the binary setting and use it to demonstrate the vulnerability of real-world text recognition systems. Scar\u2019s strong performance in practice contrasts with hardness results that show the existence of worst-case classifiers for binary images that are robust to large perturbations. In many cases, altering a single pixel is sufficient to trick Tesseract, a popular open-source text recognition system, to misclassify a word as a different word in the English dictionary. We also demonstrate the vulnerability of check recognition by fooling commercial check processing systems used by major US banks for mobile deposits. These systems are substantially harder to fool since they classify both the handwritten amounts in digits and letters, independently. Nevertheless, we generalize Scar to design attacks that fool state-of-the-art check processing systems using unnoticeable perturbations that lead to misclassification of deposit amounts. Consequently, this is a powerful method to perform financial fraud.", "one-sentence_summary": "We study adversarial attacks on models designed to classify binary (i.e. black and white) images.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "balkanski|adversarial_attacks_on_binary_image_recognition_systems", "pdf": "/pdf/3f21ccc27b4d08f6b5124ce802699df8807aa788.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=lGBRJc3fCg", "_bibtex": "@misc{\nbalkanski2021adversarial,\ntitle={Adversarial Attacks on Binary Image Recognition Systems},\nauthor={Eric Balkanski and Harrison Chase and Kojin Oshiba and Alexander Rilee and Yaron Singer and Richard Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=xCm8kiWRiBT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "xCm8kiWRiBT", "replyto": "xCm8kiWRiBT", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1181/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124816, "tmdate": 1606915808662, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1181/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1181/-/Official_Review"}}}, {"id": "VKbkCFx1_UA", "original": null, "number": 3, "cdate": 1604013419736, "ddate": null, "tcdate": 1604013419736, "tmdate": 1605024509638, "tddate": null, "forum": "xCm8kiWRiBT", "replyto": "xCm8kiWRiBT", "invitation": "ICLR.cc/2021/Conference/Paper1181/-/Official_Review", "content": {"title": "Interesting paper, but need clarity on consequences and previous work", "review": "Summary: The paper describes adversarial attacks on binary (black/white) image classifiers, flipping a classifier result by changing just a few pixels. The authors go on to show that these attacks work on a real-world financial application. \n\nTechnical contributions: \n\n* A query-optimized method for finding adversarial attacks (SCAR).\n* An empirical demonstration of the method\n* A proof (via a straightforward argument) that there are certain types of binary image classifiers that are provably somewhat robust to attack. \n\nRecommendation: reject, but with weak confidence.\n\nReason for score: I see two main issues with this paper, both of which may be cleared up with further discussion.\n\n* The authors make the case that this is a realistic attack on a high-value target. I believe it is worth opening a discussion on whether this raises ethical issues for publication.\n* What is the relation to previous work on sparse adversarial attacks? For example, \"SparseFool: a few pixels make a big difference\" (Modas et al., CVPR 2019) and \"One Pixel Attack for Fooling Deep Neural Networks,\" (Su et al., https://arxiv.org/pdf/1710.08864.pdf). While these do not relate to binary images, they seem to explore a closely related direction. The SCAR algorithm is different, but it seems important to compare with this work. The existence of other sparse methods also calls into question whether this represents a fundamentally new advance.\n\nAreas for improvement:\n* The numeric results (e.g., section 6.2) are hard to read. Putting them in a table would be helpful.\n* A key goal of SCAR is \"hiding the noise\" (p. 4). It would be nice to have more discussion of this goal. If L_0 distance isn't the goal, why not? If there's an implicit perceptual metric at play, why not make it explicit? \n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper1181/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1181/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Binary Image Recognition Systems", "authorids": ["~Eric_Balkanski2", "harrison@robustintelligence.com", "kojin@robustintelligence.com", "rilee@robustintelligence.com", "yaron@robustintelligence.com", "richard@robustintelligence.com"], "authors": ["Eric Balkanski", "Harrison Chase", "Kojin Oshiba", "Alexander Rilee", "Yaron Singer", "Richard Wang"], "keywords": ["Adversarial attacks", "Binary images", "Image Recognition", "Check processing systems"], "abstract": "We initiate the study of adversarial attacks on models for binary (i.e. black and white) image classification. Although there has been a great deal of work on attacking models for colored and grayscale images, little is known about attacks on models for binary images. Models trained to classify binary images are used in text recognition applications such as check processing, license plate recognition, invoice processing, and many others. In contrast to colored and grayscale images, the search space of attacks on binary images is extremely restricted and noise cannot be hidden with minor perturbations in each pixel. Thus, the optimization landscape of attacks on binary images introduces new fundamental challenges.\n\nIn this paper we introduce a new attack algorithm called Scar, designed to fool classifiers of binary images. We show that Scar significantly outperforms existing L0 attacks applied to the binary setting and use it to demonstrate the vulnerability of real-world text recognition systems. Scar\u2019s strong performance in practice contrasts with hardness results that show the existence of worst-case classifiers for binary images that are robust to large perturbations. In many cases, altering a single pixel is sufficient to trick Tesseract, a popular open-source text recognition system, to misclassify a word as a different word in the English dictionary. We also demonstrate the vulnerability of check recognition by fooling commercial check processing systems used by major US banks for mobile deposits. These systems are substantially harder to fool since they classify both the handwritten amounts in digits and letters, independently. Nevertheless, we generalize Scar to design attacks that fool state-of-the-art check processing systems using unnoticeable perturbations that lead to misclassification of deposit amounts. Consequently, this is a powerful method to perform financial fraud.", "one-sentence_summary": "We study adversarial attacks on models designed to classify binary (i.e. black and white) images.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "balkanski|adversarial_attacks_on_binary_image_recognition_systems", "pdf": "/pdf/3f21ccc27b4d08f6b5124ce802699df8807aa788.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=lGBRJc3fCg", "_bibtex": "@misc{\nbalkanski2021adversarial,\ntitle={Adversarial Attacks on Binary Image Recognition Systems},\nauthor={Eric Balkanski and Harrison Chase and Kojin Oshiba and Alexander Rilee and Yaron Singer and Richard Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=xCm8kiWRiBT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "xCm8kiWRiBT", "replyto": "xCm8kiWRiBT", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1181/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124816, "tmdate": 1606915808662, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1181/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1181/-/Official_Review"}}}, {"id": "3B2lt5tJyn", "original": null, "number": 4, "cdate": 1604050315872, "ddate": null, "tcdate": 1604050315872, "tmdate": 1605024509569, "tddate": null, "forum": "xCm8kiWRiBT", "replyto": "xCm8kiWRiBT", "invitation": "ICLR.cc/2021/Conference/Paper1181/-/Official_Review", "content": {"title": "Interesting and practically important work", "review": "The main question this paper aims to answer is how vulnerable binary image classification systems are.  This is an important question because of the application of such binary image classifications for check processing, invoice processing, and license plate registration. One also would think that such systems are less vulnerable to adversarial attacks given the simplicity of their inputs and the fact that most adversarial attacks are based on color or grey scale images.  The authors propose an adversarial attack algorithm called SCAR that efficiently flips the binary pixels with reasonable number of queries in order to confuse the classifier to return a desirable label with high confidence. The authors show that the proposed method outperforms the existing baselines on multiple data sets. Very interestingly, they also showed that their algorithm is able to attack the online deposit systems of US bank with a high success rate. Their example of a check with the amount of $401 that is minimally modified for the amount of $701  is quite significant given that the model had to change both the word and numbers on the check. \n\nThe authors also provided some general theories on the existence of binary image classifier provably robust to any attack that modifies large, bounded number of pixels. However, it is not clear to me how this is related to the rest of paper\u2019s discussion. The connection is missing in the paper. ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1181/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1181/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Binary Image Recognition Systems", "authorids": ["~Eric_Balkanski2", "harrison@robustintelligence.com", "kojin@robustintelligence.com", "rilee@robustintelligence.com", "yaron@robustintelligence.com", "richard@robustintelligence.com"], "authors": ["Eric Balkanski", "Harrison Chase", "Kojin Oshiba", "Alexander Rilee", "Yaron Singer", "Richard Wang"], "keywords": ["Adversarial attacks", "Binary images", "Image Recognition", "Check processing systems"], "abstract": "We initiate the study of adversarial attacks on models for binary (i.e. black and white) image classification. Although there has been a great deal of work on attacking models for colored and grayscale images, little is known about attacks on models for binary images. Models trained to classify binary images are used in text recognition applications such as check processing, license plate recognition, invoice processing, and many others. In contrast to colored and grayscale images, the search space of attacks on binary images is extremely restricted and noise cannot be hidden with minor perturbations in each pixel. Thus, the optimization landscape of attacks on binary images introduces new fundamental challenges.\n\nIn this paper we introduce a new attack algorithm called Scar, designed to fool classifiers of binary images. We show that Scar significantly outperforms existing L0 attacks applied to the binary setting and use it to demonstrate the vulnerability of real-world text recognition systems. Scar\u2019s strong performance in practice contrasts with hardness results that show the existence of worst-case classifiers for binary images that are robust to large perturbations. In many cases, altering a single pixel is sufficient to trick Tesseract, a popular open-source text recognition system, to misclassify a word as a different word in the English dictionary. We also demonstrate the vulnerability of check recognition by fooling commercial check processing systems used by major US banks for mobile deposits. These systems are substantially harder to fool since they classify both the handwritten amounts in digits and letters, independently. Nevertheless, we generalize Scar to design attacks that fool state-of-the-art check processing systems using unnoticeable perturbations that lead to misclassification of deposit amounts. Consequently, this is a powerful method to perform financial fraud.", "one-sentence_summary": "We study adversarial attacks on models designed to classify binary (i.e. black and white) images.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "balkanski|adversarial_attacks_on_binary_image_recognition_systems", "pdf": "/pdf/3f21ccc27b4d08f6b5124ce802699df8807aa788.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=lGBRJc3fCg", "_bibtex": "@misc{\nbalkanski2021adversarial,\ntitle={Adversarial Attacks on Binary Image Recognition Systems},\nauthor={Eric Balkanski and Harrison Chase and Kojin Oshiba and Alexander Rilee and Yaron Singer and Richard Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=xCm8kiWRiBT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "xCm8kiWRiBT", "replyto": "xCm8kiWRiBT", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1181/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124816, "tmdate": 1606915808662, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1181/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1181/-/Official_Review"}}}], "count": 11}