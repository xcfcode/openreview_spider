{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396481677, "tcdate": 1486396481677, "number": 1, "id": "Skqr2fUdx", "invitation": "ICLR.cc/2017/conference/-/paper287/acceptance", "forum": "BycCx8qex", "replyto": "BycCx8qex", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "This work proved to be a controversial submission. This paper has two main components: 1) a neural framework TBRU/DRAGNN, 2) experimental results on some NLP tasks. Generally there was lack of consensus about the originality of (1) and a general feeling that even if there are aspect of novelties, that the paper was lacking clarity about is contributions One reviewer was an outlier, highlighting the benefit of the ability \"incorporate dynamic recurrent connections through the definition of the transition system\" which is claimed to be really novel. Others claim this is specific to the framework used. Negative reviewers felt that (1) is probably not novel within itself and represents a slight departure from stack-lstm. The controversy here is whether DRAGNN is simple \"software engineering with no inherent \"free things\" that would lead to impact within the community. This question of impact is also inherent to (2), in particular whether the authors really got new benefit of using DragNN or whether these are \"reimplementations of things in the literature\". I felt the reviewers did seem to put in due diligence here, so the recommendation would be to put further effort into clarification and further back up of novelty claims in future versions."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DRAGNN: A Transition-Based Framework for Dynamically Connected Neural Networks", "abstract": "In this work, we present a compact, modular framework for constructing new recurrent neural architectures. Our basic module is a new generic unit, the Transition Based Recurrent Unit (TBRU). In addition to hidden layer activations, TBRUs have discrete state dynamics that allow network connections to be built dynamically as a function of intermediate activations. By connecting multiple TBRUs, we can extend and combine commonly used architectures such as sequence-to-sequence, attention mechanisms, and recursive tree-structured models. A TBRU can also serve as both an {\\em encoder} for downstream tasks and as a {\\em decoder} for its own task simultaneously, resulting in more accurate multi-task learning. We call our approach Dynamic Recurrent Acyclic Graphical Neural Networks, or DRAGNN. We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multi-task learning for extractive summarization tasks.\n", "pdf": "/pdf/c50d4ca22fbc45f74f428cb4772daea52cab438a.pdf", "TL;DR": "Modular framework for dynamically unrolled neural architectures improves structured prediction tasks", "paperhash": "kong|dragnn_a_transitionbased_framework_for_dynamically_connected_neural_networks", "keywords": ["Natural language processing", "Deep learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["cmu.edu", "google.com"], "authors": ["Lingpeng Kong", "Chris Alberti", "Daniel Andor", "Ivan Bogatyy", "David Weiss"], "authorids": ["lingpenk@cs.cmu.edu", "chrisalberti@google.com", "andor@google.com", "bogatyy@google.com", "djweiss@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396482181, "id": "ICLR.cc/2017/conference/-/paper287/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BycCx8qex", "replyto": "BycCx8qex", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396482181}}}, {"tddate": null, "tmdate": 1482197990427, "tcdate": 1482197990427, "number": 3, "id": "BkAyn-8Ex", "invitation": "ICLR.cc/2017/conference/-/paper287/official/review", "forum": "BycCx8qex", "replyto": "BycCx8qex", "signatures": ["ICLR.cc/2017/conference/paper287/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper287/AnonReviewer3"], "content": {"title": "official review", "rating": "6: Marginally above acceptance threshold", "review": "The paper proposes a new neural architecture, called DRAGNN, for the transition-based framework. A DRAGNN uses TBRUs which are neural units to compute hidden activations for the current state of a transition-based system. The paper proves that DRAGNNs can cover a wide range of transition-based methods in the literature. In addition, one can easily implement multitask learning systems with DRAGNNs. The experimental results shows that using DRAGNNs the authors built (near) state-of-the-art systems for 2 tasks: parsing and summarization. \n\nThe paper contains two major parts: DRAGNN and demonstrations of its usages. \n\nRegarding to the first part, the proposed DRAGNN is a neat tool for building any transition-based systems. However, it is difficult to say whether the DRAGNN is novel. Transition-based framework is already well defined and there's a huge trend in NLP using neural networks to implement transition-based systems. In my opinion, the difference between the Stack-LSTM (Dyer et al., 2015) and DRAGNN is slight. Of course, the DRAGNN is a powerful architecture but the contribution here should be considered mainly in terms of software engineering.\n\nIn the second part, the authors used DRAGNN to implement new transition-based systems for different (multi-)tasks. The implementations are neat, confirming that DRAGNN is a powerful architecture, especially for multitask learning. However, we should bear in mind that the solutions employed are already there in the literature, thus making difficult to judge the novelty of this part w.r.t. the theme of the conference.  ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DRAGNN: A Transition-Based Framework for Dynamically Connected Neural Networks", "abstract": "In this work, we present a compact, modular framework for constructing new recurrent neural architectures. Our basic module is a new generic unit, the Transition Based Recurrent Unit (TBRU). In addition to hidden layer activations, TBRUs have discrete state dynamics that allow network connections to be built dynamically as a function of intermediate activations. By connecting multiple TBRUs, we can extend and combine commonly used architectures such as sequence-to-sequence, attention mechanisms, and recursive tree-structured models. A TBRU can also serve as both an {\\em encoder} for downstream tasks and as a {\\em decoder} for its own task simultaneously, resulting in more accurate multi-task learning. We call our approach Dynamic Recurrent Acyclic Graphical Neural Networks, or DRAGNN. We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multi-task learning for extractive summarization tasks.\n", "pdf": "/pdf/c50d4ca22fbc45f74f428cb4772daea52cab438a.pdf", "TL;DR": "Modular framework for dynamically unrolled neural architectures improves structured prediction tasks", "paperhash": "kong|dragnn_a_transitionbased_framework_for_dynamically_connected_neural_networks", "keywords": ["Natural language processing", "Deep learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["cmu.edu", "google.com"], "authors": ["Lingpeng Kong", "Chris Alberti", "Daniel Andor", "Ivan Bogatyy", "David Weiss"], "authorids": ["lingpenk@cs.cmu.edu", "chrisalberti@google.com", "andor@google.com", "bogatyy@google.com", "djweiss@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512636046, "id": "ICLR.cc/2017/conference/-/paper287/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper287/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper287/AnonReviewer6", "ICLR.cc/2017/conference/paper287/AnonReviewer5", "ICLR.cc/2017/conference/paper287/AnonReviewer3"], "reply": {"forum": "BycCx8qex", "replyto": "BycCx8qex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper287/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper287/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512636046}}}, {"tddate": null, "tmdate": 1482095922022, "tcdate": 1482095922022, "number": 2, "id": "H1qE6OVVg", "invitation": "ICLR.cc/2017/conference/-/paper287/official/review", "forum": "BycCx8qex", "replyto": "BycCx8qex", "signatures": ["ICLR.cc/2017/conference/paper287/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper287/AnonReviewer5"], "content": {"title": "", "rating": "7: Good paper, accept", "review": "The authors present a general framework for defining a wide variety of recurrent neural network architectures, including seq2seq models, tree-structured models, attention, and a new family of dynamically connected architectures. The framework defines a new, general-purpose recurrent unit called the TBRU, which takes a transition system, defining and constraining its inputs and outputs, and input function which defines the mapping between raw inputs and fixed-width vector representations, and recurrence function that defines the inputs to each recurrent step as a function of the current state, and an RNN cell that computes the output from the input (fixed and recurrent). Many example instantiations of this framework are provided, including sequential tagging RNNs, Google\u2019s Parsey McParseface parser, encoder/decoder networks, tree LSTMs and less familiar examples that demonstrate the power this framework. \n\nThe most interesting contribution of this work is the ease by which it can be used to incorporate dynamic recurrent connections through the definition of the transition system. In particular, this paper explores the application of these dynamic connections to syntactic dependency parsing, both as a standalone task, and by multitasking parsing with extractive summarization, using the same compositional phrase representations as features for the parser and summarization (previous work used discrete parse features), which is particularly simple/elegant in this framework. In experimental results, the authors demonstrate that such multitasking leads to more accurate summarization models, and using the framework to incorporate more structure into existing parsing models also leads to increased accuracy with no big-oh efficiency loss (compared with e.g. attention). \n\nThe \u201craison d\u2019etre,\u201d in particular the example, perhaps described even more thoroughly/explicitly, should be made as clear as possible as soon as possible. This is the most important contribution, but it gets lost in the description and presentation as a framework \u2014 emphasizing that attention, seq2seq, etc can be represented in the framework is distracting and makes it seem less novel than it is. AnonReviewer6 clearly missed this point, as did I in my first pass over the paper. To get this idea across and to emphasize the benefits of this representation, I\u2019d love to see more detailed analysis of these representations and their importance to achieving your experimental results. I think it would also be helpful to emphasize the difference between a stack LSTM and Example 6. \n\nOverall I think this paper presents a valuable contribution, though the exposition could be improved and analysis of experimental results expanded. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DRAGNN: A Transition-Based Framework for Dynamically Connected Neural Networks", "abstract": "In this work, we present a compact, modular framework for constructing new recurrent neural architectures. Our basic module is a new generic unit, the Transition Based Recurrent Unit (TBRU). In addition to hidden layer activations, TBRUs have discrete state dynamics that allow network connections to be built dynamically as a function of intermediate activations. By connecting multiple TBRUs, we can extend and combine commonly used architectures such as sequence-to-sequence, attention mechanisms, and recursive tree-structured models. A TBRU can also serve as both an {\\em encoder} for downstream tasks and as a {\\em decoder} for its own task simultaneously, resulting in more accurate multi-task learning. We call our approach Dynamic Recurrent Acyclic Graphical Neural Networks, or DRAGNN. We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multi-task learning for extractive summarization tasks.\n", "pdf": "/pdf/c50d4ca22fbc45f74f428cb4772daea52cab438a.pdf", "TL;DR": "Modular framework for dynamically unrolled neural architectures improves structured prediction tasks", "paperhash": "kong|dragnn_a_transitionbased_framework_for_dynamically_connected_neural_networks", "keywords": ["Natural language processing", "Deep learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["cmu.edu", "google.com"], "authors": ["Lingpeng Kong", "Chris Alberti", "Daniel Andor", "Ivan Bogatyy", "David Weiss"], "authorids": ["lingpenk@cs.cmu.edu", "chrisalberti@google.com", "andor@google.com", "bogatyy@google.com", "djweiss@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512636046, "id": "ICLR.cc/2017/conference/-/paper287/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper287/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper287/AnonReviewer6", "ICLR.cc/2017/conference/paper287/AnonReviewer5", "ICLR.cc/2017/conference/paper287/AnonReviewer3"], "reply": {"forum": "BycCx8qex", "replyto": "BycCx8qex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper287/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper287/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512636046}}}, {"tddate": null, "tmdate": 1481916505342, "tcdate": 1481916112241, "number": 3, "id": "B1dR02-Eg", "invitation": "ICLR.cc/2017/conference/-/paper287/public/comment", "forum": "BycCx8qex", "replyto": "rkqw-ubNg", "signatures": ["~David_J_Weiss1"], "readers": ["everyone"], "writers": ["~David_J_Weiss1"], "content": {"title": "Reponse to final review", "comment": "Thank you for the detailed review. We want to clarify our statement about the \"goal of DRAGNN\" -- when we said \"the goal of DRAGNN is modularity\" in our comment, we should have said \"**a** goal of the DRAGNN **implementation** is modularity\". This may have led to some confusion about why we wrote this paper and what our contributions are. This purpose of this paper is not to describe a software system or a means to unify existing neural architectures: We are proposing a framework for neural architectures that allows **new** types of cross-task representation learning and dynamic neural models.\n\nIn particular:\n\nRe: The purpose of DRAGNN\n\n- The primary purpose of our framework is **not code re-use or engineering**, but (1) dynamic neural connections and (2) new, better multi-task learning through re-use of learned representations between models.  We show empirically that both of these lead to improvements over popular architectures and enable new types of stacked neural architectures:\n\n * Table 1 shows that seq2seq+attention is less accurate (and quadratic) vs. an explicit dynamic input connection (which is linear)\n * Table 2 shows that stack-propagation is better than single encoder->multiple decoders. This model is a new dynamic neural architecture enabled by our framework (backpropagation across tasks along connections as a function of shift/reduce decisions.) \n * Table 3 shows we can build new types of stacked models by including multiple TBRU's that produce dependency trees in different directions. This is another new dynamic neural architecture enabled by our framework.\n\n- Again, the primary purpose of DRAGNN is not about the implementation, nor to make it easy to re-implement other papers. The goal is to provide *new* types of multi-task learning and compositional representations. The modularity of TBRU's makes this easy to do, but it's not required if one is just using something like bi-LSTMS (see for example: https://openreview.net/pdf?id=SJZAb5cel).\n\n- We discussed the implementation because TensorFlow does not efficiently support building a compute graph dynamically. TBRU's provide a way to express dynamic connections that can be implemented efficiently with a fixed compute graph (assuming access to while loops). Like we said before, we are actively pursuing an open source release in TensorFlow, not proposing an alternative to TensorFlow, Dynet, etc. DRAGNN is a meta-framework that must be implemented on top of an existing NN toolbox; it requires automatic differentiation, and it does not compete with NN toolboxes.\n\nRe: DRAGNN vs. VW\n\nWe claimed VW is orthogonal for two reasons: (1) recurrent, dynamic neural networks and (2) multi-task learning with shared multi-task representations.\n\n- VW's credit assignment compiler is an excellent tool for learning structured predictors for a single task. Like we said, DRAGNN is aimed to enable multi-task learning and learn compositional representations. How to do this properly is still an open research question.\n\n- According to VW's github page, VW supports feed-forward networks of a particular structure, with non-differentiable links (i.e. links through the features). We apologize if we were mistaken -- can you please clarify how to implement recurrent models such as LSTMs in VW?\n\nRe: Limitations of DRAGNN\n\n- In terms of implementation, DRAGNN is built on TensorFlow, and can be interspersed with vanilla TF code. Therefore, any model implemented in TensorFlow can utilize DRAGNN, and vis versa.\n\n- Dynamic programming is not something that fits into TF easily, and unless the program can be easily mapped to a TBRU it won't fit into DRAGNN easily either. For example, if the CRF functions to compute structured attention in https://openreview.net/pdf?id=HkE0Nvqlg are not implemented in TF, you would need to port them first. However,\n\n- If you're not doing multi-task learning and/or not trying to use dynamic connections in the network, there's not much point to shoehorning your model into the DRAGNN framework to begin with. If you are, then DRAGNN provides a useful foundation to build your model. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DRAGNN: A Transition-Based Framework for Dynamically Connected Neural Networks", "abstract": "In this work, we present a compact, modular framework for constructing new recurrent neural architectures. Our basic module is a new generic unit, the Transition Based Recurrent Unit (TBRU). In addition to hidden layer activations, TBRUs have discrete state dynamics that allow network connections to be built dynamically as a function of intermediate activations. By connecting multiple TBRUs, we can extend and combine commonly used architectures such as sequence-to-sequence, attention mechanisms, and recursive tree-structured models. A TBRU can also serve as both an {\\em encoder} for downstream tasks and as a {\\em decoder} for its own task simultaneously, resulting in more accurate multi-task learning. We call our approach Dynamic Recurrent Acyclic Graphical Neural Networks, or DRAGNN. We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multi-task learning for extractive summarization tasks.\n", "pdf": "/pdf/c50d4ca22fbc45f74f428cb4772daea52cab438a.pdf", "TL;DR": "Modular framework for dynamically unrolled neural architectures improves structured prediction tasks", "paperhash": "kong|dragnn_a_transitionbased_framework_for_dynamically_connected_neural_networks", "keywords": ["Natural language processing", "Deep learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["cmu.edu", "google.com"], "authors": ["Lingpeng Kong", "Chris Alberti", "Daniel Andor", "Ivan Bogatyy", "David Weiss"], "authorids": ["lingpenk@cs.cmu.edu", "chrisalberti@google.com", "andor@google.com", "bogatyy@google.com", "djweiss@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287640005, "id": "ICLR.cc/2017/conference/-/paper287/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BycCx8qex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper287/reviewers", "ICLR.cc/2017/conference/paper287/areachairs"], "cdate": 1485287640005}}}, {"tddate": null, "tmdate": 1481896289831, "tcdate": 1481896289831, "number": 1, "id": "rkqw-ubNg", "invitation": "ICLR.cc/2017/conference/-/paper287/official/review", "forum": "BycCx8qex", "replyto": "BycCx8qex", "signatures": ["ICLR.cc/2017/conference/paper287/AnonReviewer6"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper287/AnonReviewer6"], "content": {"title": "Final review", "rating": "5: Marginally below acceptance threshold", "review": "Overall, this is a nice paper. Developing a unifying framework for these newer\nneural models is a worthwhile endeavor.\n\nHowever, it's unclear if the DRAGNN framework (in its current form) is a\nsignificant standalone contribution. The main idea is straightforward: use a\ntransition system to unroll a computation graph. When you implement models in\nthis way you can reuse code because modules can be mixed and matched. This is\nnice, but (in my opinion) is just good software engineering, not machine \nlearning research.\n\nMoreover, there appears to be little incentive to use DRAGNN, as there are no\n'free things' (benefits) that you get by using the framework. For example:\n\n- If you write your neuralnet in an automatic differentiation library (e.g.,\n  tensorflow or dynet) you get gradients for 'free'.\n\n- In the VW framework, there are efficiency tricks that 'the credit assignment\n  compiler' provides for you, which would be tedious to implement on your\n  own. There is also a variety of algorithms for training the model in a\n  principled way (i.e., without exposure bias).\n\nI don't feel that my question about the limitations of the framework has been\nsatisfactorily addressed. Let me ask it in a different way: Can you give me\nexamples of a few models that I can't (nicely) express in the DRAGNN framework?\nWhat if I wanted to implement https://openreview.net/pdf?id=HkE0Nvqlg or\nhttp://www.cs.jhu.edu/~jason/papers/rastogi+al.naacl16.pdf? Can I implement the\ndynamic programming components as transition units and (importantly) would it be\nefficient?\n\n disagree that the VW framework is orthogonal, it is a *competing* way to\nimplement recurrent models. The main different to me appears to be that VW's\nimperative framework is more general, but less modular.\n\nThe experimental contribution seems useful as does the emphasis on how easy it\nis to incorporate multi-task learning.\n\nMinor:\n\n- It would be useful to see actual code snippets (possibly in an\n  appendix). Otherwise, its unclear how modular DRAGNN really are.\n\n- The introduction states that (unlike seq2seq+attention) inference remains\n  linear. Is this *necessarily* the case? Users define a transition system that\n  is quadratic, just let attention be over all previous states. I recommend that\n  authors rephrase statement more carefully.\n\n- It seems strange to use A() as in \"actions\", then use d as \"decision\" for its\n  elements.\n\n- I recommend adding i as an argument to the definition of the recurrence\n  function r(s) to make it clear that it's the subset of previous states at time\n  i, otherwise it looks like an undefined variable. A nice terse option is to\n  write r(s_i).\n\n- Real numbers should be \\mathbb{R} not \\mathcal{R}.\n\n- It's more conventional to use t for a time-step instead of i.\n\n- Example 2: \"52 feature embeddings\" -> did you mean \"52-DIMENSIONAL feature\n  embeddings\"?\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DRAGNN: A Transition-Based Framework for Dynamically Connected Neural Networks", "abstract": "In this work, we present a compact, modular framework for constructing new recurrent neural architectures. Our basic module is a new generic unit, the Transition Based Recurrent Unit (TBRU). In addition to hidden layer activations, TBRUs have discrete state dynamics that allow network connections to be built dynamically as a function of intermediate activations. By connecting multiple TBRUs, we can extend and combine commonly used architectures such as sequence-to-sequence, attention mechanisms, and recursive tree-structured models. A TBRU can also serve as both an {\\em encoder} for downstream tasks and as a {\\em decoder} for its own task simultaneously, resulting in more accurate multi-task learning. We call our approach Dynamic Recurrent Acyclic Graphical Neural Networks, or DRAGNN. We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multi-task learning for extractive summarization tasks.\n", "pdf": "/pdf/c50d4ca22fbc45f74f428cb4772daea52cab438a.pdf", "TL;DR": "Modular framework for dynamically unrolled neural architectures improves structured prediction tasks", "paperhash": "kong|dragnn_a_transitionbased_framework_for_dynamically_connected_neural_networks", "keywords": ["Natural language processing", "Deep learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["cmu.edu", "google.com"], "authors": ["Lingpeng Kong", "Chris Alberti", "Daniel Andor", "Ivan Bogatyy", "David Weiss"], "authorids": ["lingpenk@cs.cmu.edu", "chrisalberti@google.com", "andor@google.com", "bogatyy@google.com", "djweiss@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512636046, "id": "ICLR.cc/2017/conference/-/paper287/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper287/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper287/AnonReviewer6", "ICLR.cc/2017/conference/paper287/AnonReviewer5", "ICLR.cc/2017/conference/paper287/AnonReviewer3"], "reply": {"forum": "BycCx8qex", "replyto": "BycCx8qex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper287/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper287/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512636046}}}, {"tddate": null, "tmdate": 1481469211683, "tcdate": 1481469211676, "number": 2, "id": "rk4QakoQe", "invitation": "ICLR.cc/2017/conference/-/paper287/public/comment", "forum": "BycCx8qex", "replyto": "HyI9jXyXe", "signatures": ["~Lingpeng_Kong1"], "readers": ["everyone"], "writers": ["~Lingpeng_Kong1"], "content": {"title": "response", "comment": "1. What are the limitations of the DRAGNN framework? (e.g., Can we implement seq2seq with soft attention?)\n\nThe goal of DRAGNN is to express complex neural network architectures in terms of pluggable modules; this is also the main limitation. In other words, given an arbitrary equation to implement, one needs to translate it into a TBRU specification if it doesn\u2019t fit into an existing library of modules. \n\nThat being said, seq2seq with soft attention is easily represented in DRAGNN and we actually included it in the experiments. We simply set the recurrence function to pull from all input tokens and include an attention mechanism in the network cell. We actually compare to this in our experiments; rows 3 & 4 in Table 1 correspond to soft attention\n\n2. How does DRAGNN related to Vowpal Wabbit's imperative learning-to-search learning framework? See recent paper at NIPS https://arxiv.org/abs/1406.1837 (which has been on arxiv since 2014) as well as the author's tutorial at ICML 2015. \n\nVW has a flexible approach for learning-to-search built on a fixed model structure for a single structured prediction task. In contrast, DRAGNN is a simple approach for learning-to-search built on a flexible, modular model structure that handles multiple tasks and heterogenous datasets. Note that \u201cjoint\u201d in the VW framework refers to jointly reasoning about multiple variables, not jointly reasoning about multiple tasks with different annotations/datasets/loss functions. In that sense, the VW framework is orthogonal to DRAGNN, and we can combine the learning approach described in that work to DRAGNN in the future. \n\n3. Will a framework be released accompanying paper? The examples given are a little hand-wavey, which got me wondering: How many lines of code are required to expresses each of the models? Also, How long does it take to train and run? \n\nSee answer above. We\u2019ll add more detailed descriptions of the models. \u201cLines of code\u201d is a bit hard to measure: given the set of transition systems, recurrent units, and recurrence functions we implemented, expressing the models are just filling out a configuration file (e.g. TBRU #1 are these four things, TBRU #2 are these four things.) Typically more code is required to specify the training configuration (e.g. which datasets / loss functions map to which components) than the actual model itself.\n\nTraining a parsing model like in Table #1 took about 5-6 hours on a single CPU; the summarization models took about a day to train the most complex multi-task models.\n\n4. How does one include beam search or dynamic programming inference with a DRAGNN? To implement the full Parsey McParseFace model, users need this to implement approx global normalization and decoding via beam search.\n\nDRAGNN is designed to allow batching, and beam can be considered a batch with a bit more constraints and book-keeping. The structure of computation does not change. We are currently porting the SyntaxNet style beam training to DRAGNN so we can implement a recurrent version of Parsey. \n\n5. Why limit training to maximum likelihood training, especially given that we know such training is prone to failure due to \"exposure bias\" and many good alternatives exist, including DAgger (Ross et. al, 2011), LOLS (Chang et al., 2015) and reinforcement learning algorithms (e.g., policy gradient).\n\nThis framework is extendable to these RL style algorithms and we intend to explore these in future work. We chose ML style training because it was the simplest starting point for this framework. Furthermore, since we use iterative training rather than training with a global loss for all the components, some errors of this kind have been taken care of since the previous components are dynamically unrolled (instead of requiring a joint set of gold annotations for every task on every example.)\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DRAGNN: A Transition-Based Framework for Dynamically Connected Neural Networks", "abstract": "In this work, we present a compact, modular framework for constructing new recurrent neural architectures. Our basic module is a new generic unit, the Transition Based Recurrent Unit (TBRU). In addition to hidden layer activations, TBRUs have discrete state dynamics that allow network connections to be built dynamically as a function of intermediate activations. By connecting multiple TBRUs, we can extend and combine commonly used architectures such as sequence-to-sequence, attention mechanisms, and recursive tree-structured models. A TBRU can also serve as both an {\\em encoder} for downstream tasks and as a {\\em decoder} for its own task simultaneously, resulting in more accurate multi-task learning. We call our approach Dynamic Recurrent Acyclic Graphical Neural Networks, or DRAGNN. We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multi-task learning for extractive summarization tasks.\n", "pdf": "/pdf/c50d4ca22fbc45f74f428cb4772daea52cab438a.pdf", "TL;DR": "Modular framework for dynamically unrolled neural architectures improves structured prediction tasks", "paperhash": "kong|dragnn_a_transitionbased_framework_for_dynamically_connected_neural_networks", "keywords": ["Natural language processing", "Deep learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["cmu.edu", "google.com"], "authors": ["Lingpeng Kong", "Chris Alberti", "Daniel Andor", "Ivan Bogatyy", "David Weiss"], "authorids": ["lingpenk@cs.cmu.edu", "chrisalberti@google.com", "andor@google.com", "bogatyy@google.com", "djweiss@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287640005, "id": "ICLR.cc/2017/conference/-/paper287/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BycCx8qex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper287/reviewers", "ICLR.cc/2017/conference/paper287/areachairs"], "cdate": 1485287640005}}}, {"tddate": null, "tmdate": 1481469167894, "tcdate": 1481469167881, "number": 1, "id": "rkdgTJiQx", "invitation": "ICLR.cc/2017/conference/-/paper287/public/comment", "forum": "BycCx8qex", "replyto": "S13PQBeme", "signatures": ["~Lingpeng_Kong1"], "readers": ["everyone"], "writers": ["~Lingpeng_Kong1"], "content": {"title": "response", "comment": "1. Explain the differences between a stack LSTM and Example 6 (compositional representation from arc-standard dependency parsing). It\u2019s clear to me that they are indeed different, but I\u2019d love to better understand the nuances there, and it seems like an important comparison to draw explicitly in the paper.\n\nThe main difference is in the composition function. Stack LSTM uses a composition function which combines the embedding of head (h), the dependent (d), and the syntactic relation (r) to be the compositional representation for the arc.  In contrast, we use dynamic recurrences to represent composition: the hidden representation of the last decision that modified the i\u2019th token is used as the compositional representation. This makes the model quite a bit simpler and more efficient without sacrificing accuracy. \n\n2. Any plans to release accompanying code? \n\nWe\u2019re actively pursuing an open-source release in TensorFlow.\n\n3. How does the concurrent ICLR submission https://arxiv.org/pdf/1611.01734v2.pdf fit into this framework, if at all?\n\nIt is quite easy to fit the deep biaffine attention into DRAGNN. Consider the following setup with three TBRU\u2019s:\n\nTBRU #1: Left-to-right shift-only LSTM\nTBRU #2: Right-to-left shift-only LSTM\nTBRU #3: Head-selection transition system, defined as follows:\n\nState = list of heads 1, \u2026, i \nDecision = select a head for i+1\nRecurrence = Representations for all tokens from Bi-LSTM models (TBRU\u2019s #1 and #2)\nNetwork cell = Biaffine score between head and all possible modifiers\n\nNote that we will include this implementation with any release of the framework, and with our framework, we can include training objectives such as POS tagging quite easily."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DRAGNN: A Transition-Based Framework for Dynamically Connected Neural Networks", "abstract": "In this work, we present a compact, modular framework for constructing new recurrent neural architectures. Our basic module is a new generic unit, the Transition Based Recurrent Unit (TBRU). In addition to hidden layer activations, TBRUs have discrete state dynamics that allow network connections to be built dynamically as a function of intermediate activations. By connecting multiple TBRUs, we can extend and combine commonly used architectures such as sequence-to-sequence, attention mechanisms, and recursive tree-structured models. A TBRU can also serve as both an {\\em encoder} for downstream tasks and as a {\\em decoder} for its own task simultaneously, resulting in more accurate multi-task learning. We call our approach Dynamic Recurrent Acyclic Graphical Neural Networks, or DRAGNN. We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multi-task learning for extractive summarization tasks.\n", "pdf": "/pdf/c50d4ca22fbc45f74f428cb4772daea52cab438a.pdf", "TL;DR": "Modular framework for dynamically unrolled neural architectures improves structured prediction tasks", "paperhash": "kong|dragnn_a_transitionbased_framework_for_dynamically_connected_neural_networks", "keywords": ["Natural language processing", "Deep learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["cmu.edu", "google.com"], "authors": ["Lingpeng Kong", "Chris Alberti", "Daniel Andor", "Ivan Bogatyy", "David Weiss"], "authorids": ["lingpenk@cs.cmu.edu", "chrisalberti@google.com", "andor@google.com", "bogatyy@google.com", "djweiss@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287640005, "id": "ICLR.cc/2017/conference/-/paper287/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BycCx8qex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper287/reviewers", "ICLR.cc/2017/conference/paper287/areachairs"], "cdate": 1485287640005}}}, {"tddate": null, "tmdate": 1480770403757, "tcdate": 1480770403727, "number": 2, "id": "S13PQBeme", "invitation": "ICLR.cc/2017/conference/-/paper287/pre-review/question", "forum": "BycCx8qex", "replyto": "BycCx8qex", "signatures": ["ICLR.cc/2017/conference/paper287/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper287/AnonReviewer5"], "content": {"title": "Questions", "question": "\u2014 Explain the differences between a stack LSTM and Example 6 (compositional representation from arc-standard dependency parsing). It\u2019s clear to me that they are indeed different, but I\u2019d love to better understand the nuances there, and it seems like an important comparison to draw explicitly in the paper.\n\u2014 Any plans to release accompanying code?\n\u2014 How does the concurrent ICLR submission https://arxiv.org/pdf/1611.01734v2.pdf fit into this framework, if at all?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DRAGNN: A Transition-Based Framework for Dynamically Connected Neural Networks", "abstract": "In this work, we present a compact, modular framework for constructing new recurrent neural architectures. Our basic module is a new generic unit, the Transition Based Recurrent Unit (TBRU). In addition to hidden layer activations, TBRUs have discrete state dynamics that allow network connections to be built dynamically as a function of intermediate activations. By connecting multiple TBRUs, we can extend and combine commonly used architectures such as sequence-to-sequence, attention mechanisms, and recursive tree-structured models. A TBRU can also serve as both an {\\em encoder} for downstream tasks and as a {\\em decoder} for its own task simultaneously, resulting in more accurate multi-task learning. We call our approach Dynamic Recurrent Acyclic Graphical Neural Networks, or DRAGNN. We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multi-task learning for extractive summarization tasks.\n", "pdf": "/pdf/c50d4ca22fbc45f74f428cb4772daea52cab438a.pdf", "TL;DR": "Modular framework for dynamically unrolled neural architectures improves structured prediction tasks", "paperhash": "kong|dragnn_a_transitionbased_framework_for_dynamically_connected_neural_networks", "keywords": ["Natural language processing", "Deep learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["cmu.edu", "google.com"], "authors": ["Lingpeng Kong", "Chris Alberti", "Daniel Andor", "Ivan Bogatyy", "David Weiss"], "authorids": ["lingpenk@cs.cmu.edu", "chrisalberti@google.com", "andor@google.com", "bogatyy@google.com", "djweiss@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959360765, "id": "ICLR.cc/2017/conference/-/paper287/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper287/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper287/AnonReviewer6", "ICLR.cc/2017/conference/paper287/AnonReviewer5"], "reply": {"forum": "BycCx8qex", "replyto": "BycCx8qex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper287/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper287/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959360765}}}, {"tddate": null, "tmdate": 1480698977046, "tcdate": 1480698765819, "number": 1, "id": "HyI9jXyXe", "invitation": "ICLR.cc/2017/conference/-/paper287/pre-review/question", "forum": "BycCx8qex", "replyto": "BycCx8qex", "signatures": ["ICLR.cc/2017/conference/paper287/AnonReviewer6"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper287/AnonReviewer6"], "content": {"title": "Limitation of the framework", "question": "1. What are the limitations of the DRAGNN framework? (e.g., Can we implement seq2seq with soft attention?)\n\n2. How does DRAGNN related to Vowpal Wabbit's imperative learning-to-search learning framework?\nSee recent paper at NIPS https://arxiv.org/abs/1406.1837 (which has been on arxiv since 2014) as well as the author's tutorial at ICML 2015.\n\n3. Will a framework be released accompanying paper? The examples given are a little hand-wavey, which got me wondering: How many lines of code are required to expresses each of the models? Also, How long does it take to train and run?\n\n4. How does one include beam search or dynamic programming inference with a DRAGNN? To implement the full Parsey McParseFace model, users need this to implement approx global normalization and decoding via beam search.\n\n5. Why limit training to maximum likelihood training, especially given that we know such training is prone to failure due to \"exposure bias\" and many good alternatives exist, including DAgger (Ross et. al, 2011), LOLS (Chang et al., 2015) and reinforcement learning algorithms (e.g., policy gradient)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DRAGNN: A Transition-Based Framework for Dynamically Connected Neural Networks", "abstract": "In this work, we present a compact, modular framework for constructing new recurrent neural architectures. Our basic module is a new generic unit, the Transition Based Recurrent Unit (TBRU). In addition to hidden layer activations, TBRUs have discrete state dynamics that allow network connections to be built dynamically as a function of intermediate activations. By connecting multiple TBRUs, we can extend and combine commonly used architectures such as sequence-to-sequence, attention mechanisms, and recursive tree-structured models. A TBRU can also serve as both an {\\em encoder} for downstream tasks and as a {\\em decoder} for its own task simultaneously, resulting in more accurate multi-task learning. We call our approach Dynamic Recurrent Acyclic Graphical Neural Networks, or DRAGNN. We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multi-task learning for extractive summarization tasks.\n", "pdf": "/pdf/c50d4ca22fbc45f74f428cb4772daea52cab438a.pdf", "TL;DR": "Modular framework for dynamically unrolled neural architectures improves structured prediction tasks", "paperhash": "kong|dragnn_a_transitionbased_framework_for_dynamically_connected_neural_networks", "keywords": ["Natural language processing", "Deep learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["cmu.edu", "google.com"], "authors": ["Lingpeng Kong", "Chris Alberti", "Daniel Andor", "Ivan Bogatyy", "David Weiss"], "authorids": ["lingpenk@cs.cmu.edu", "chrisalberti@google.com", "andor@google.com", "bogatyy@google.com", "djweiss@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959360765, "id": "ICLR.cc/2017/conference/-/paper287/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper287/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper287/AnonReviewer6", "ICLR.cc/2017/conference/paper287/AnonReviewer5"], "reply": {"forum": "BycCx8qex", "replyto": "BycCx8qex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper287/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper287/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959360765}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478283474393, "tcdate": 1478283474384, "number": 287, "id": "BycCx8qex", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BycCx8qex", "signatures": ["~Daniel_Andor1"], "readers": ["everyone"], "content": {"title": "DRAGNN: A Transition-Based Framework for Dynamically Connected Neural Networks", "abstract": "In this work, we present a compact, modular framework for constructing new recurrent neural architectures. Our basic module is a new generic unit, the Transition Based Recurrent Unit (TBRU). In addition to hidden layer activations, TBRUs have discrete state dynamics that allow network connections to be built dynamically as a function of intermediate activations. By connecting multiple TBRUs, we can extend and combine commonly used architectures such as sequence-to-sequence, attention mechanisms, and recursive tree-structured models. A TBRU can also serve as both an {\\em encoder} for downstream tasks and as a {\\em decoder} for its own task simultaneously, resulting in more accurate multi-task learning. We call our approach Dynamic Recurrent Acyclic Graphical Neural Networks, or DRAGNN. We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multi-task learning for extractive summarization tasks.\n", "pdf": "/pdf/c50d4ca22fbc45f74f428cb4772daea52cab438a.pdf", "TL;DR": "Modular framework for dynamically unrolled neural architectures improves structured prediction tasks", "paperhash": "kong|dragnn_a_transitionbased_framework_for_dynamically_connected_neural_networks", "keywords": ["Natural language processing", "Deep learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["cmu.edu", "google.com"], "authors": ["Lingpeng Kong", "Chris Alberti", "Daniel Andor", "Ivan Bogatyy", "David Weiss"], "authorids": ["lingpenk@cs.cmu.edu", "chrisalberti@google.com", "andor@google.com", "bogatyy@google.com", "djweiss@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 10}