{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396489443, "tcdate": 1486396489443, "number": 1, "id": "r1b8hzLdg", "invitation": "ICLR.cc/2017/conference/-/paper298/acceptance", "forum": "rkmDI85ge", "replyto": "rkmDI85ge", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This is a solidly executed paper that received good reviews. However, the originality is a bit lacking. In addition, the paper would have been stronger with a comparison to the method proposed in Zweig et al. (2013). We recommend this paper for the workshop.", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Efficient Softmax Approximation for GPUs", "abstract": "We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax.", "pdf": "/pdf/e63702cce988e4f72ab323968211fbd3d85a30c0.pdf", "paperhash": "grave|efficient_softmax_approximation_for_gpus", "keywords": ["Natural language processing"], "conflicts": ["fb.com", "kaust.edu.sa", "columbia.edu"], "authors": ["\u00c9douard Grave", "Armand Joulin", "Moustapha Ciss\u00e9", "David Grangier", "Herv\u00e9 J\u00e9gou"], "authorids": ["egrave@fb.com", "ajoulin@fb.com", "moustaphacisse@fb.com", "grangier@fb.com", "rvj@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396490010, "id": "ICLR.cc/2017/conference/-/paper298/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rkmDI85ge", "replyto": "rkmDI85ge", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396490010}}}, {"tddate": null, "tmdate": 1485367426062, "tcdate": 1482265659240, "number": 3, "id": "B1mHNfPEx", "invitation": "ICLR.cc/2017/conference/-/paper298/official/review", "forum": "rkmDI85ge", "replyto": "rkmDI85ge", "signatures": ["ICLR.cc/2017/conference/paper298/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper298/AnonReviewer4"], "content": {"title": "Final Review: nice practical speed optimization of softmax for GPUs", "rating": "7: Good paper, accept", "review": "The authors introduce an adaptive softmax approximation tailored for faster performance on GPUs. The key idea, which is very sensible, is to use a class-based hierarchical softmax, but where the clusters/hierarchy are distributed such that the resulting matrix multiplications are optimally-sized for GPU computation, based on their empirical tests. Their results indicate that the system does indeed work very well.\n\nIn terms of presentation, I found the paper to have both clear and unclear elements. Fortunately, the underlying concepts and logic seem quite clear. Unfortunately, at various points, the writing is not. There are various minor typos (as mentioned by AnonReviewer2, in addition to some other spots, e.g. the notation describing recurrent network in Section 3 mentions an x_t which is surely different from the x_t used in the previous paragraph on regular feedforward NN's, i think it belonged in the equation for h_t; the use of the two matrices A and P in Eq2 is strange, etc). Also, while Section 4.2 (Intuition for 2-cluster case) was a good idea to include and helpful, and while the *concepts* underlying the complexity analysis were straightforward, it could be made a lot clearer by (a) adding an additional figure such as Figure 2, along with (b) a few well-placed additional sentences unpacking the logic of the argument into easier-to-follow steps. For example, it was only when I saw Eq (6) and (7) combined with Fig(2) that the analysis on the previous page made more sense in terms of arriving at the eq for the complexity of putting the head of the distribution in the root of the tree. (Perhaps an Appendix might be the most appropriate place to add such an explanation).\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Efficient Softmax Approximation for GPUs", "abstract": "We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax.", "pdf": "/pdf/e63702cce988e4f72ab323968211fbd3d85a30c0.pdf", "paperhash": "grave|efficient_softmax_approximation_for_gpus", "keywords": ["Natural language processing"], "conflicts": ["fb.com", "kaust.edu.sa", "columbia.edu"], "authors": ["\u00c9douard Grave", "Armand Joulin", "Moustapha Ciss\u00e9", "David Grangier", "Herv\u00e9 J\u00e9gou"], "authorids": ["egrave@fb.com", "ajoulin@fb.com", "moustaphacisse@fb.com", "grangier@fb.com", "rvj@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512632462, "id": "ICLR.cc/2017/conference/-/paper298/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper298/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper298/AnonReviewer2", "ICLR.cc/2017/conference/paper298/AnonReviewer3", "ICLR.cc/2017/conference/paper298/AnonReviewer4"], "reply": {"forum": "rkmDI85ge", "replyto": "rkmDI85ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper298/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper298/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512632462}}}, {"tddate": null, "tmdate": 1484970195506, "tcdate": 1484970195506, "number": 6, "id": "SJiAOLlvx", "invitation": "ICLR.cc/2017/conference/-/paper298/public/comment", "forum": "rkmDI85ge", "replyto": "rkmDI85ge", "signatures": ["~Edouard_Grave1"], "readers": ["everyone"], "writers": ["~Edouard_Grave1"], "content": {"title": "Response", "comment": "First and foremost, we would like to thank the reviewers for their insightful and great comments. We will edit the paper to take into account their remarks, improve its clarity and add the missing references. Second, as suggested by reviewer 2, we compared our approach to the hierarchical softmax with perplexity based clustering (referred as HSM(PPL)):\n\n\t\tHSM(PPL)\t\tOURS\nbg\t\t39 (29 min)\t\t37 (18 min)\ncs\t\t67 (55 min)\t\t62 (30 min)\nda\t\t37 (228 min)\t\t35 (105 min)\nde\t\t44 (207 min)\t\t40 (110 min)\nel\t\t39 (136 min)\t\t36 (72 min)\nes\t\t30 (194 min)\t\t29 (103 min)\n\nOur method obtain a slightly better perplexity, while being significantly faster. Finally, the code for our method is publicly available at: https://github.com/facebookresearch/adaptive-softmax."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Efficient Softmax Approximation for GPUs", "abstract": "We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax.", "pdf": "/pdf/e63702cce988e4f72ab323968211fbd3d85a30c0.pdf", "paperhash": "grave|efficient_softmax_approximation_for_gpus", "keywords": ["Natural language processing"], "conflicts": ["fb.com", "kaust.edu.sa", "columbia.edu"], "authors": ["\u00c9douard Grave", "Armand Joulin", "Moustapha Ciss\u00e9", "David Grangier", "Herv\u00e9 J\u00e9gou"], "authorids": ["egrave@fb.com", "ajoulin@fb.com", "moustaphacisse@fb.com", "grangier@fb.com", "rvj@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287633013, "id": "ICLR.cc/2017/conference/-/paper298/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkmDI85ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper298/reviewers", "ICLR.cc/2017/conference/paper298/areachairs"], "cdate": 1485287633013}}}, {"tddate": null, "tmdate": 1482422787285, "tcdate": 1482422787285, "number": 5, "id": "SJjWcOF4g", "invitation": "ICLR.cc/2017/conference/-/paper298/public/comment", "forum": "rkmDI85ge", "replyto": "S1OAIZi7x", "signatures": ["~Edouard_Grave1"], "readers": ["everyone"], "writers": ["~Edouard_Grave1"], "content": {"title": "Additional baseline", "comment": "Thank you for your review and detailed comments!\n\nAs requested, we ran experiments with the hierarchical softmax with perplexity-based clustering. More precisely, we ran Brown clustering (Percy Liang's implementation, https://github.com/percyliang/brown-cluster) on the training set to obtain the clustering used in the HSM. We set the number of clusters to sqrt(|V|).\n\nCompared to frequency binning, we observe important improvement in perplexity, with an increased runtime. Compared to our approach, the perplexity obtained with this approach is slightly worse, while the runtime is significantly bigger (more than 50% slower in all cases). We will add these results to the paper.\n\n         HSM PPL             OURS\nbg  39 (29 min)       37 (18 min)\ncs  67 (55 min)        62 (30 min)\nda  37 (228 min)     35 (105 min)\nde  44 (207 min)     40 (110 min)\nel   39 (136 min)      36  (72 min)\nes  30 (194 min)      29 (103 min)\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Efficient Softmax Approximation for GPUs", "abstract": "We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax.", "pdf": "/pdf/e63702cce988e4f72ab323968211fbd3d85a30c0.pdf", "paperhash": "grave|efficient_softmax_approximation_for_gpus", "keywords": ["Natural language processing"], "conflicts": ["fb.com", "kaust.edu.sa", "columbia.edu"], "authors": ["\u00c9douard Grave", "Armand Joulin", "Moustapha Ciss\u00e9", "David Grangier", "Herv\u00e9 J\u00e9gou"], "authorids": ["egrave@fb.com", "ajoulin@fb.com", "moustaphacisse@fb.com", "grangier@fb.com", "rvj@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287633013, "id": "ICLR.cc/2017/conference/-/paper298/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkmDI85ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper298/reviewers", "ICLR.cc/2017/conference/paper298/areachairs"], "cdate": 1485287633013}}}, {"tddate": null, "tmdate": 1482005442072, "tcdate": 1482004382348, "number": 2, "id": "ryLjvz7Ne", "invitation": "ICLR.cc/2017/conference/-/paper298/official/review", "forum": "rkmDI85ge", "replyto": "rkmDI85ge", "signatures": ["ICLR.cc/2017/conference/paper298/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper298/AnonReviewer3"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "SYNOPSIS:\nThe authors introduce an efficient approximation to the softmax function that speeds up the empirical calculation of the softmax on GPUs. They leverage the unbalanced distribution of words and specific empirical timings of matrix multiplies on GPUs to devise an algorithm that selects an optimal placement of the vocabulary into clusters.  They show empirical results that show speedups over alternative methods, while not losing much accuracy compared to the full softmax. \n\nTHOUGHTS:\n\nSince the goal of this work is to speed up training, I'm curious why you compare only to the flat 2-level HSM (O(sqrt(V)) speedup at best), and not the deeper binary-tree HSM (O(lgV) speedup at best)?\n\nOverall, the paper is clear, easy to understand, and well written, bar a few notation issues as pointed out by other reviewers. It adds an interesting extra tool in the language modeling toolbox. The idea is based on several previous works that aim to optimize vocabulary clustering to improve the speed-accuracy tradeoff often experienced in practice with hierarchical methods. The interesting result here seems to be that this particular clustering objective improves speed (what it was designed for), while apparently not losing much i.t.o. accuracy (what it wasn't designed for). Although the authors do not speculate  reasons for the latter part at all, I suspect it is largely related to the fact that the flat region on the timing graph (Fig 1) means that the head group V_h can actually include a sizeable portion of the most frequent words in the vocabulary at constant cost. This reduces the approximation error (regions of no support in P_approx(next | previous) compared to P_real ), which in turn mitigates the hit in perplexity compared to the full softmax. \n\nHowever, since the method is intimately related to the speed-optimal method proposed by Zweig et al. (2013) (albeit without the explicit tailoring towards GPU), I feel that a direct comparison is warranted (I understand this is underway). If the performance and accuracy improvements still hold, I will update my rating to a 7.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Efficient Softmax Approximation for GPUs", "abstract": "We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax.", "pdf": "/pdf/e63702cce988e4f72ab323968211fbd3d85a30c0.pdf", "paperhash": "grave|efficient_softmax_approximation_for_gpus", "keywords": ["Natural language processing"], "conflicts": ["fb.com", "kaust.edu.sa", "columbia.edu"], "authors": ["\u00c9douard Grave", "Armand Joulin", "Moustapha Ciss\u00e9", "David Grangier", "Herv\u00e9 J\u00e9gou"], "authorids": ["egrave@fb.com", "ajoulin@fb.com", "moustaphacisse@fb.com", "grangier@fb.com", "rvj@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512632462, "id": "ICLR.cc/2017/conference/-/paper298/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper298/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper298/AnonReviewer2", "ICLR.cc/2017/conference/paper298/AnonReviewer3", "ICLR.cc/2017/conference/paper298/AnonReviewer4"], "reply": {"forum": "rkmDI85ge", "replyto": "rkmDI85ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper298/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper298/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512632462}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481665628201, "tcdate": 1478284891410, "number": 298, "id": "rkmDI85ge", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rkmDI85ge", "signatures": ["~Edouard_Grave1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Efficient Softmax Approximation for GPUs", "abstract": "We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax.", "pdf": "/pdf/e63702cce988e4f72ab323968211fbd3d85a30c0.pdf", "paperhash": "grave|efficient_softmax_approximation_for_gpus", "keywords": ["Natural language processing"], "conflicts": ["fb.com", "kaust.edu.sa", "columbia.edu"], "authors": ["\u00c9douard Grave", "Armand Joulin", "Moustapha Ciss\u00e9", "David Grangier", "Herv\u00e9 J\u00e9gou"], "authorids": ["egrave@fb.com", "ajoulin@fb.com", "moustaphacisse@fb.com", "grangier@fb.com", "rvj@fb.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481640985922, "tcdate": 1481640966547, "number": 4, "id": "SyR-nFpXe", "invitation": "ICLR.cc/2017/conference/-/paper298/public/comment", "forum": "rkmDI85ge", "replyto": "HyVo2Cafe", "signatures": ["~Edouard_Grave1"], "readers": ["everyone"], "writers": ["~Edouard_Grave1"], "content": {"title": "re: Clustering: complexity vs. performance", "comment": "Thank you for your comment.\n\nFor small numbers of clusters (between 2 and 5 in the paper), we did not observe significant degradation in performance compared to the full softmax. This is also true for configurations close to the optimal one obtained by dynamic programming.\n\nWe are currently implementing/running HSM(PPL), based on Brown clustering (similarly to Zweig et al. 2013), on our benchmarks, and hopefully will be able to report the results before the end of the week."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Efficient Softmax Approximation for GPUs", "abstract": "We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax.", "pdf": "/pdf/e63702cce988e4f72ab323968211fbd3d85a30c0.pdf", "paperhash": "grave|efficient_softmax_approximation_for_gpus", "keywords": ["Natural language processing"], "conflicts": ["fb.com", "kaust.edu.sa", "columbia.edu"], "authors": ["\u00c9douard Grave", "Armand Joulin", "Moustapha Ciss\u00e9", "David Grangier", "Herv\u00e9 J\u00e9gou"], "authorids": ["egrave@fb.com", "ajoulin@fb.com", "moustaphacisse@fb.com", "grangier@fb.com", "rvj@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287633013, "id": "ICLR.cc/2017/conference/-/paper298/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkmDI85ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper298/reviewers", "ICLR.cc/2017/conference/paper298/areachairs"], "cdate": 1485287633013}}}, {"tddate": null, "tmdate": 1481627851543, "tcdate": 1481627851538, "number": 3, "id": "BJX0dIamg", "invitation": "ICLR.cc/2017/conference/-/paper298/public/comment", "forum": "rkmDI85ge", "replyto": "SJttYzfQx", "signatures": ["~Edouard_Grave1"], "readers": ["everyone"], "writers": ["~Edouard_Grave1"], "content": {"title": "re: Clarifications", "comment": "Thanks for your comment.\n\nYes, we meant using the standard Huffman-coded HSM where all words are in the leaves. Assigning some words to multiple leaves could also be used in combination to our approach. It would be interesting to investigate if it also leads to improved performance when combined with the adaptive softmax. However, we believe it is beyond the scope of this paper, as it requires to re-compute the hierarchy during training (as opposed to the methods considered in the paper). As reported by Mnih & Hinton, it also leads to slower computation, while we mainly focus on efficiency (which is critical for large datasets such as the billion word benchmark)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Efficient Softmax Approximation for GPUs", "abstract": "We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax.", "pdf": "/pdf/e63702cce988e4f72ab323968211fbd3d85a30c0.pdf", "paperhash": "grave|efficient_softmax_approximation_for_gpus", "keywords": ["Natural language processing"], "conflicts": ["fb.com", "kaust.edu.sa", "columbia.edu"], "authors": ["\u00c9douard Grave", "Armand Joulin", "Moustapha Ciss\u00e9", "David Grangier", "Herv\u00e9 J\u00e9gou"], "authorids": ["egrave@fb.com", "ajoulin@fb.com", "moustaphacisse@fb.com", "grangier@fb.com", "rvj@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287633013, "id": "ICLR.cc/2017/conference/-/paper298/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkmDI85ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper298/reviewers", "ICLR.cc/2017/conference/paper298/areachairs"], "cdate": 1485287633013}}}, {"tddate": null, "tmdate": 1481627740556, "tcdate": 1481627740550, "number": 2, "id": "r1VDdU67e", "invitation": "ICLR.cc/2017/conference/-/paper298/public/comment", "forum": "rkmDI85ge", "replyto": "rkQECRaMx", "signatures": ["~Edouard_Grave1"], "readers": ["everyone"], "writers": ["~Edouard_Grave1"], "content": {"title": "re: Notation", "comment": "Thanks for your comment.\n\nThe computational cost g can be generalized to take into account the batch size B and the dimension d by using the cost function g(k, B, d) = c_1 + lambda * max(0, k * B * d - c_2). This crude model of computation can be extended without invalidating the method described in the paper. In practice, we found it was able to well capture the computational time we observed empirically.\n\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Efficient Softmax Approximation for GPUs", "abstract": "We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax.", "pdf": "/pdf/e63702cce988e4f72ab323968211fbd3d85a30c0.pdf", "paperhash": "grave|efficient_softmax_approximation_for_gpus", "keywords": ["Natural language processing"], "conflicts": ["fb.com", "kaust.edu.sa", "columbia.edu"], "authors": ["\u00c9douard Grave", "Armand Joulin", "Moustapha Ciss\u00e9", "David Grangier", "Herv\u00e9 J\u00e9gou"], "authorids": ["egrave@fb.com", "ajoulin@fb.com", "moustaphacisse@fb.com", "grangier@fb.com", "rvj@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287633013, "id": "ICLR.cc/2017/conference/-/paper298/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkmDI85ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper298/reviewers", "ICLR.cc/2017/conference/paper298/areachairs"], "cdate": 1485287633013}}}, {"tddate": null, "tmdate": 1481475792314, "tcdate": 1481475792307, "number": 1, "id": "S1OAIZi7x", "invitation": "ICLR.cc/2017/conference/-/paper298/official/review", "forum": "rkmDI85ge", "replyto": "rkmDI85ge", "signatures": ["ICLR.cc/2017/conference/paper298/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper298/AnonReviewer2"], "content": {"title": "Review", "rating": "7: Good paper, accept", "review": "he authors provide an interesting, computational-complexity-driven approach for efficient softmax computation for language modeling based on GPUs. An adaptive softmax approach is proposed based on a hierarchical model. Dynamic programming is applied to optimize the structure of the hierarchical approach chosen here w.r.t. computational complexity based on GPUs. \n\nHowever, it remains unclear, how robust the specific configuration obtained from dynamic programming is w.r.t. performance/perplexity. Corresponding comparative results with perplexity-based clustering would be desirable. Especially, in Sec. 5, Paragraph Baselines, and Table 1, respectively, it would be interesting to see a result on HSM(PPL) (cf. Zweig et al. 2013).\n\nAFAIK, the first successful application of an LSTM-based language model for large vocabulary was published by Sundermeyer et al. 2012 (see below), which is missing in the sumary of prior work on the bottom of p. 3.\n\nMainly, the paper is well written and accessible, though notation in some cases should be improved, see detailed comments below.\n\nPrior work on LSTM language modeling: \n - Sundermeyer et al.: LSTM Neural Networks for Language Modeling, Interspeech, pp. 194-197, 2012.\n\nNotation:\n - use of g(k) vs. g(k,B,d): g(k) should be clearly defined (constant B and d?)\n - notation should not be reused (B is matrix in Eq. (3), and batch size in Sec. 4.1).\n - notation p_{i+j} (Eq. (10) and before) is kind of misleading, as p_{i+j} is not the same as p_{(i+j)}\n\nMinor comments:\n - p. 1, item list at bottom, first item: take -> takes\n - p. 5, second paragraph: will then contained -> will then contain\n - p. 5, third paragaph: to associated -> to associate\n - Sec. 4.3, first paragraph: At the time being -> For the time being\n - below Eq. (9): most-right -> right-most\n - below Eq. (10): the second term of this equation -> the second term of the right-hand side of this equation\n - p. 6, second to last line: smaller that the -> smaller than the\n - p. 7, Sec. 5, itemize, first item: 100 millions -> 100 million\n - p. 8, last sentence: we are the -> ours is the\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Efficient Softmax Approximation for GPUs", "abstract": "We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax.", "pdf": "/pdf/e63702cce988e4f72ab323968211fbd3d85a30c0.pdf", "paperhash": "grave|efficient_softmax_approximation_for_gpus", "keywords": ["Natural language processing"], "conflicts": ["fb.com", "kaust.edu.sa", "columbia.edu"], "authors": ["\u00c9douard Grave", "Armand Joulin", "Moustapha Ciss\u00e9", "David Grangier", "Herv\u00e9 J\u00e9gou"], "authorids": ["egrave@fb.com", "ajoulin@fb.com", "moustaphacisse@fb.com", "grangier@fb.com", "rvj@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512632462, "id": "ICLR.cc/2017/conference/-/paper298/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper298/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper298/AnonReviewer2", "ICLR.cc/2017/conference/paper298/AnonReviewer3", "ICLR.cc/2017/conference/paper298/AnonReviewer4"], "reply": {"forum": "rkmDI85ge", "replyto": "rkmDI85ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper298/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper298/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512632462}}}, {"tddate": null, "tmdate": 1480890753423, "tcdate": 1480890753418, "number": 2, "id": "SJttYzfQx", "invitation": "ICLR.cc/2017/conference/-/paper298/pre-review/question", "forum": "rkmDI85ge", "replyto": "rkmDI85ge", "signatures": ["ICLR.cc/2017/conference/paper298/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper298/AnonReviewer3"], "content": {"title": "Clarifications", "question": "I'm not sure I understand the \"Compromising between efficiency and accuracy\" on p5, especially: \"We observe empirically that putting all the clusters in the leaves of the tree leads to a significant drop of performance\". Do you mean using the standard Huffman-coded HSM (where all words are in the leaves) leads to a 5-10% drop in accuracy? If so, did you consider the case where nodes are replicated (words have multiple leave nodes and total probability is summed over all of these), and that Mnih & Hinton showed this to actually improve over results obtained with a full softmax?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Efficient Softmax Approximation for GPUs", "abstract": "We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax.", "pdf": "/pdf/e63702cce988e4f72ab323968211fbd3d85a30c0.pdf", "paperhash": "grave|efficient_softmax_approximation_for_gpus", "keywords": ["Natural language processing"], "conflicts": ["fb.com", "kaust.edu.sa", "columbia.edu"], "authors": ["\u00c9douard Grave", "Armand Joulin", "Moustapha Ciss\u00e9", "David Grangier", "Herv\u00e9 J\u00e9gou"], "authorids": ["egrave@fb.com", "ajoulin@fb.com", "moustaphacisse@fb.com", "grangier@fb.com", "rvj@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959353848, "id": "ICLR.cc/2017/conference/-/paper298/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper298/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper298/AnonReviewer2", "ICLR.cc/2017/conference/paper298/AnonReviewer3"], "reply": {"forum": "rkmDI85ge", "replyto": "rkmDI85ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper298/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper298/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959353848}}}, {"tddate": null, "tmdate": 1480613791209, "tcdate": 1480613791204, "number": 1, "id": "Hyvi1JCfx", "invitation": "ICLR.cc/2017/conference/-/paper298/public/comment", "forum": "rkmDI85ge", "replyto": "Hkob6RpGx", "signatures": ["~Edouard_Grave1"], "readers": ["everyone"], "writers": ["~Edouard_Grave1"], "content": {"title": "Re: Missing references/citations", "comment": "Thanks for your comment. We updated the paper to add the missing references."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Efficient Softmax Approximation for GPUs", "abstract": "We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax.", "pdf": "/pdf/e63702cce988e4f72ab323968211fbd3d85a30c0.pdf", "paperhash": "grave|efficient_softmax_approximation_for_gpus", "keywords": ["Natural language processing"], "conflicts": ["fb.com", "kaust.edu.sa", "columbia.edu"], "authors": ["\u00c9douard Grave", "Armand Joulin", "Moustapha Ciss\u00e9", "David Grangier", "Herv\u00e9 J\u00e9gou"], "authorids": ["egrave@fb.com", "ajoulin@fb.com", "moustaphacisse@fb.com", "grangier@fb.com", "rvj@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287633013, "id": "ICLR.cc/2017/conference/-/paper298/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkmDI85ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper298/reviewers", "ICLR.cc/2017/conference/paper298/areachairs"], "cdate": 1485287633013}}}, {"tddate": null, "tmdate": 1480613418915, "tcdate": 1480613418910, "number": 3, "id": "rkQECRaMx", "invitation": "ICLR.cc/2017/conference/-/paper298/official/comment", "forum": "rkmDI85ge", "replyto": "rkmDI85ge", "signatures": ["ICLR.cc/2017/conference/paper298/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper298/AnonReviewer2"], "content": {"title": "Notation", "comment": "g() is used in a number of different configurations with different variables. Please provide corresponding definitions (e.g. g(k,B,d) vs. g(k) same apart from keeping B and d constant in g(k)?)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Efficient Softmax Approximation for GPUs", "abstract": "We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax.", "pdf": "/pdf/e63702cce988e4f72ab323968211fbd3d85a30c0.pdf", "paperhash": "grave|efficient_softmax_approximation_for_gpus", "keywords": ["Natural language processing"], "conflicts": ["fb.com", "kaust.edu.sa", "columbia.edu"], "authors": ["\u00c9douard Grave", "Armand Joulin", "Moustapha Ciss\u00e9", "David Grangier", "Herv\u00e9 J\u00e9gou"], "authorids": ["egrave@fb.com", "ajoulin@fb.com", "moustaphacisse@fb.com", "grangier@fb.com", "rvj@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287632883, "id": "ICLR.cc/2017/conference/-/paper298/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "rkmDI85ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper298/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper298/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper298/reviewers", "ICLR.cc/2017/conference/paper298/areachairs"], "cdate": 1485287632883}}}, {"tddate": null, "tmdate": 1480613123067, "tcdate": 1480613123063, "number": 2, "id": "Hkob6RpGx", "invitation": "ICLR.cc/2017/conference/-/paper298/official/comment", "forum": "rkmDI85ge", "replyto": "rkmDI85ge", "signatures": ["ICLR.cc/2017/conference/paper298/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper298/AnonReviewer2"], "content": {"title": "Missing references/citations", "comment": "In many places, references/citations seem to be erroneous/missing (cf. \"(?)\" in the text). Please provide these."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Efficient Softmax Approximation for GPUs", "abstract": "We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax.", "pdf": "/pdf/e63702cce988e4f72ab323968211fbd3d85a30c0.pdf", "paperhash": "grave|efficient_softmax_approximation_for_gpus", "keywords": ["Natural language processing"], "conflicts": ["fb.com", "kaust.edu.sa", "columbia.edu"], "authors": ["\u00c9douard Grave", "Armand Joulin", "Moustapha Ciss\u00e9", "David Grangier", "Herv\u00e9 J\u00e9gou"], "authorids": ["egrave@fb.com", "ajoulin@fb.com", "moustaphacisse@fb.com", "grangier@fb.com", "rvj@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287632883, "id": "ICLR.cc/2017/conference/-/paper298/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "rkmDI85ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper298/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper298/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper298/reviewers", "ICLR.cc/2017/conference/paper298/areachairs"], "cdate": 1485287632883}}}, {"tddate": null, "tmdate": 1480613020136, "tcdate": 1480613020131, "number": 1, "id": "HyVo2Cafe", "invitation": "ICLR.cc/2017/conference/-/paper298/pre-review/question", "forum": "rkmDI85ge", "replyto": "rkmDI85ge", "signatures": ["ICLR.cc/2017/conference/paper298/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper298/AnonReviewer2"], "content": {"title": "Clustering: complexity vs. performance", "question": "The objective for your clustering approach is clearly computational complexity, as opposed to performance. Did you analyse how robust the specific configuration you obtain for GPU computation by dynamic programming is w.r.t. recognition performance or perplexity? \n\nEspecially, in Sec. 5/Baselines and Table 1 it would be interesting to see a result on HSM(PPL) (cf. Zweig et al. 2013). Did you also check this?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Efficient Softmax Approximation for GPUs", "abstract": "We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax.", "pdf": "/pdf/e63702cce988e4f72ab323968211fbd3d85a30c0.pdf", "paperhash": "grave|efficient_softmax_approximation_for_gpus", "keywords": ["Natural language processing"], "conflicts": ["fb.com", "kaust.edu.sa", "columbia.edu"], "authors": ["\u00c9douard Grave", "Armand Joulin", "Moustapha Ciss\u00e9", "David Grangier", "Herv\u00e9 J\u00e9gou"], "authorids": ["egrave@fb.com", "ajoulin@fb.com", "moustaphacisse@fb.com", "grangier@fb.com", "rvj@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959353848, "id": "ICLR.cc/2017/conference/-/paper298/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper298/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper298/AnonReviewer2", "ICLR.cc/2017/conference/paper298/AnonReviewer3"], "reply": {"forum": "rkmDI85ge", "replyto": "rkmDI85ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper298/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper298/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959353848}}}], "count": 15}