{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028636780, "tcdate": 1490028636780, "number": 1, "id": "Skrvdtpog", "invitation": "ICLR.cc/2017/workshop/-/paper155/acceptance", "forum": "ByWhxeHtx", "replyto": "ByWhxeHtx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bottom Up or Top Down? Dynamics of Deep Representations via Canonical Correlation Analysis", "abstract": "We present a versatile quantitative framework for comparing representations in deep neural networks, based on Canonical Correlation Analysis, and use it to analyze  the  dynamics  of  representation  learning  during  the  training  process  of  a deep network.  We find that layers converge to their final representation from the bottom-up, but that the representations themselves migrate downwards in the net-work over the course of learning.", "pdf": "/pdf/b873bb7f6d1d8216e4e1e8e29e157cd225a8b78b.pdf", "TL;DR": "Use CCA to look at representation learning dynamics of neural networks, and find bottom up convergence, top down representation crawling.", "paperhash": "raghu|bottom_up_or_top_down_dynamics_of_deep_representations_via_canonical_correlation_analysis", "keywords": ["Theory", "Deep learning"], "conflicts": ["google.com", "cs.cornell.edu", "uber.com"], "authors": ["Maithra Raghu", "Jason Yosinski", "Jascha Sohl-Dickstein"], "authorids": ["maithrar@gmail.com", "jason@yosinski.com", "jaschasd@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028637348, "id": "ICLR.cc/2017/workshop/-/paper155/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "ByWhxeHtx", "replyto": "ByWhxeHtx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028637348}}}, {"tddate": null, "tmdate": 1489171909924, "tcdate": 1489171909924, "number": 2, "id": "HJRpSdese", "invitation": "ICLR.cc/2017/workshop/-/paper155/official/review", "forum": "ByWhxeHtx", "replyto": "ByWhxeHtx", "signatures": ["ICLR.cc/2017/workshop/paper155/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper155/AnonReviewer2"], "content": {"title": "Interesting approach to study dynamics of DNNs with a rather incomplete presentation", "rating": "5: Marginally below acceptance threshold", "review": "This work studies similarities between data representations of DNNs during training using canonical correlation analysis (CCA). Authors present two conclusions based on this analysis framework. First, during training, the lower layers converge faster to the final distribution (up to affine transformation) compared to the upper layers. Then, authors observe that The final layer correlates more with the lower layers during the early stages of training compared to the final stages.\n\nThe observed properties are rather interesting and especially the second observation would be a quite surprising observation. It is known that for DNN training, the neural networks need to be over-parametrised, but little is known about the reasons why. The proposed explanation of a low-level image representations crawling down from upper layers sounds like an intriguing explanation, however it is not clear whether the observed effect is not only an artifact of the non-linear operation of the logit layer (as it seems from the Figure 1).\n\nFrom the technical perspective, the paper is really brief and unfortunately is missing out some important details (what final layer is used in the Figure 3 experiment, how are convolutional features handled, reason for non-symmetry of the tensors in Figure 1). The structure of the manuscript is rather unusual as it does not contain final discussion/conclusions.\n\nIn general, it is a quite interesting idea, however feels a bit unfinished. Furthermore, considering the goals of the ICLR Workshop, it does not seem to fall to any of the \"late-breaking developments, very novel ideas and position papers\" categories. If these requirements were relaxed and the work was a bit extended, I believe it would be an interesting workshop submission paper.\n\nPros:\n- Neat and simple idea how to study properties of image representations during training\n- Interesting perspective on the hidden units as vectors in function space which nicely fits to the CCA analysis\n\nCons:\n- Seems to be unfinished, missing some important details\n- Unfortunately, does not fit the requirements of the ICLR 2017 Workshops\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bottom Up or Top Down? Dynamics of Deep Representations via Canonical Correlation Analysis", "abstract": "We present a versatile quantitative framework for comparing representations in deep neural networks, based on Canonical Correlation Analysis, and use it to analyze  the  dynamics  of  representation  learning  during  the  training  process  of  a deep network.  We find that layers converge to their final representation from the bottom-up, but that the representations themselves migrate downwards in the net-work over the course of learning.", "pdf": "/pdf/b873bb7f6d1d8216e4e1e8e29e157cd225a8b78b.pdf", "TL;DR": "Use CCA to look at representation learning dynamics of neural networks, and find bottom up convergence, top down representation crawling.", "paperhash": "raghu|bottom_up_or_top_down_dynamics_of_deep_representations_via_canonical_correlation_analysis", "keywords": ["Theory", "Deep learning"], "conflicts": ["google.com", "cs.cornell.edu", "uber.com"], "authors": ["Maithra Raghu", "Jason Yosinski", "Jascha Sohl-Dickstein"], "authorids": ["maithrar@gmail.com", "jason@yosinski.com", "jaschasd@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489171910528, "id": "ICLR.cc/2017/workshop/-/paper155/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper155/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper155/AnonReviewer3", "ICLR.cc/2017/workshop/paper155/AnonReviewer2"], "reply": {"forum": "ByWhxeHtx", "replyto": "ByWhxeHtx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper155/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper155/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489171910528}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489164734598, "tcdate": 1489164662448, "number": 1, "id": "HyR_KUgjx", "invitation": "ICLR.cc/2017/workshop/-/paper155/official/review", "forum": "ByWhxeHtx", "replyto": "ByWhxeHtx", "signatures": ["ICLR.cc/2017/workshop/paper155/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper155/AnonReviewer3"], "content": {"title": "Interesting direction for future research, currently too preliminary for ICLR workshop focus areas ", "rating": "3: Clear rejection", "review": "Thanks to the authors for sharing this technique and the direction they're heading with their research.\n\nThis work applies canonical-correlation analysis between layers of a deep neural network to its own intermediate layers, in a FCN and CNN setting. Through this visualization, the authors observe a bottom-up convergence pattern in two networks trained for classification, an FCN for  MNIST and CNN for CIFAR-10. This is interpreted as the network learning converging to low-level representations quickly, and building upwards toward higher-level representations more slowly during training.\n\nThe authors also make an observation about what they describe as the \"1% rows / higher layers of the network\" being similar to their final representations. This is interpreted as the network learning final representations most quickly which are then \"squeezed from the top down\" to fit into lower layers through training.\n\nThis point is unclear, as there is no label corresponding to 1% rows on the diagrams, but it likely refers to the stage at 3% in the training where the \"out\" layer has correlation between 0.7 and 0.9 with all layers for the MNIST example, and 0.1- 0.65 in the CIFAR-10 example.\n\nSince the gradient signal is strongest at the top layer, the phenomenon may be simply a characteristic of gradient descent rather than a feature of representation learning by deep networks. Moreover, initialization and training algorithm will heavily influence this pattern in the visualization. These points are not explored in the current version of the paper, weakening the conjectures about representation learning by the network.\n\nCCA as a method of studying correlation patterns among layers in a deep network is interesting, and I look forward to seeing more work from the authors in this area. For the purposes of the ICLR workshop track, which seeks to emphasize late-breaking developments, very novel ideas and position papers, I assess this as not appropriate.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bottom Up or Top Down? Dynamics of Deep Representations via Canonical Correlation Analysis", "abstract": "We present a versatile quantitative framework for comparing representations in deep neural networks, based on Canonical Correlation Analysis, and use it to analyze  the  dynamics  of  representation  learning  during  the  training  process  of  a deep network.  We find that layers converge to their final representation from the bottom-up, but that the representations themselves migrate downwards in the net-work over the course of learning.", "pdf": "/pdf/b873bb7f6d1d8216e4e1e8e29e157cd225a8b78b.pdf", "TL;DR": "Use CCA to look at representation learning dynamics of neural networks, and find bottom up convergence, top down representation crawling.", "paperhash": "raghu|bottom_up_or_top_down_dynamics_of_deep_representations_via_canonical_correlation_analysis", "keywords": ["Theory", "Deep learning"], "conflicts": ["google.com", "cs.cornell.edu", "uber.com"], "authors": ["Maithra Raghu", "Jason Yosinski", "Jascha Sohl-Dickstein"], "authorids": ["maithrar@gmail.com", "jason@yosinski.com", "jaschasd@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489171910528, "id": "ICLR.cc/2017/workshop/-/paper155/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper155/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper155/AnonReviewer3", "ICLR.cc/2017/workshop/paper155/AnonReviewer2"], "reply": {"forum": "ByWhxeHtx", "replyto": "ByWhxeHtx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper155/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper155/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489171910528}}}, {"tddate": null, "replyto": null, "nonreaders": null, "ddate": null, "tmdate": 1488675543938, "tcdate": 1487368360994, "number": 155, "id": "ByWhxeHtx", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "ByWhxeHtx", "signatures": ["~Maithra_Raghu1"], "readers": ["everyone"], "content": {"title": "Bottom Up or Top Down? Dynamics of Deep Representations via Canonical Correlation Analysis", "abstract": "We present a versatile quantitative framework for comparing representations in deep neural networks, based on Canonical Correlation Analysis, and use it to analyze  the  dynamics  of  representation  learning  during  the  training  process  of  a deep network.  We find that layers converge to their final representation from the bottom-up, but that the representations themselves migrate downwards in the net-work over the course of learning.", "pdf": "/pdf/b873bb7f6d1d8216e4e1e8e29e157cd225a8b78b.pdf", "TL;DR": "Use CCA to look at representation learning dynamics of neural networks, and find bottom up convergence, top down representation crawling.", "paperhash": "raghu|bottom_up_or_top_down_dynamics_of_deep_representations_via_canonical_correlation_analysis", "keywords": ["Theory", "Deep learning"], "conflicts": ["google.com", "cs.cornell.edu", "uber.com"], "authors": ["Maithra Raghu", "Jason Yosinski", "Jascha Sohl-Dickstein"], "authorids": ["maithrar@gmail.com", "jason@yosinski.com", "jaschasd@google.com"]}, "writers": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 4}