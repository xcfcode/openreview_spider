{"notes": [{"id": "HJej3s09Km", "original": "HJx9AIjqt7", "number": 744, "cdate": 1538087859398, "ddate": null, "tcdate": 1538087859398, "tmdate": 1545355415387, "tddate": null, "forum": "HJej3s09Km", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "On the effect of the activation function on the distribution of hidden nodes in a deep network", "abstract": "We analyze the joint probability distribution on the lengths of the\nvectors of hidden variables in different layers of a fully connected\ndeep network, when the weights and biases are chosen randomly according to\nGaussian distributions, and the input is binary-valued.  We show\nthat, if the activation function satisfies a minimal set of\nassumptions, satisfied by all activation functions that we know that\nare used in practice, then, as the width of the network gets large,\nthe ``length process'' converges in probability to a length map\nthat is determined as a simple function of the variances of the\nrandom weights and biases, and the activation function.\n\nWe also show that this convergence may fail for activation functions \nthat violate our assumptions.", "keywords": ["theory", "length map", "initialization"], "authorids": ["plong@google.com", "hsedghi@google.com"], "authors": ["Philip M. Long and Hanie Sedghi"], "TL;DR": "We prove that, for activation functions satisfying some conditions, as a deep network gets wide, the lengths of the vectors of hidden variables converge to a length map.", "pdf": "/pdf/7fb78879ecb9d621a1a1df79fbff9165bf7b8650.pdf", "paperhash": "sedghi|on_the_effect_of_the_activation_function_on_the_distribution_of_hidden_nodes_in_a_deep_network", "_bibtex": "@misc{\nsedghi2019on,\ntitle={On the effect of the activation function on the distribution of hidden nodes in a deep network},\nauthor={Philip M. Long and Hanie Sedghi},\nyear={2019},\nurl={https://openreview.net/forum?id=HJej3s09Km},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "S1xrFvOTk4", "original": null, "number": 1, "cdate": 1544550269209, "ddate": null, "tcdate": 1544550269209, "tmdate": 1545354499451, "tddate": null, "forum": "HJej3s09Km", "replyto": "HJej3s09Km", "invitation": "ICLR.cc/2019/Conference/-/Paper744/Meta_Review", "content": {"metareview": "I appreciate that the authors are refuting a technical claim in Poole et al., however the paper has garnered zero enthusiasm the way it is written. I suggest to the authors that they rewrite the paper as a refutation of Poole et al., and name it as such.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Too narrow"}, "signatures": ["ICLR.cc/2019/Conference/Paper744/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper744/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the effect of the activation function on the distribution of hidden nodes in a deep network", "abstract": "We analyze the joint probability distribution on the lengths of the\nvectors of hidden variables in different layers of a fully connected\ndeep network, when the weights and biases are chosen randomly according to\nGaussian distributions, and the input is binary-valued.  We show\nthat, if the activation function satisfies a minimal set of\nassumptions, satisfied by all activation functions that we know that\nare used in practice, then, as the width of the network gets large,\nthe ``length process'' converges in probability to a length map\nthat is determined as a simple function of the variances of the\nrandom weights and biases, and the activation function.\n\nWe also show that this convergence may fail for activation functions \nthat violate our assumptions.", "keywords": ["theory", "length map", "initialization"], "authorids": ["plong@google.com", "hsedghi@google.com"], "authors": ["Philip M. Long and Hanie Sedghi"], "TL;DR": "We prove that, for activation functions satisfying some conditions, as a deep network gets wide, the lengths of the vectors of hidden variables converge to a length map.", "pdf": "/pdf/7fb78879ecb9d621a1a1df79fbff9165bf7b8650.pdf", "paperhash": "sedghi|on_the_effect_of_the_activation_function_on_the_distribution_of_hidden_nodes_in_a_deep_network", "_bibtex": "@misc{\nsedghi2019on,\ntitle={On the effect of the activation function on the distribution of hidden nodes in a deep network},\nauthor={Philip M. Long and Hanie Sedghi},\nyear={2019},\nurl={https://openreview.net/forum?id=HJej3s09Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper744/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353101977, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJej3s09Km", "replyto": "HJej3s09Km", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper744/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper744/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper744/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353101977}}}, {"id": "HyeYsYKPh7", "original": null, "number": 1, "cdate": 1541015969277, "ddate": null, "tcdate": 1541015969277, "tmdate": 1543173004448, "tddate": null, "forum": "HJej3s09Km", "replyto": "HJej3s09Km", "invitation": "ICLR.cc/2019/Conference/-/Paper744/Official_Review", "content": {"title": "Technically correct, not well-written", "review": "Summary: the paper proves the convergence of empirical length map (length process) in NN to the length map for a permissible activation functions in a wide-network limit. The authors also show why the assumptions on the permissible functions can not be relaxed.\n\nQuality: the paper seems to be technically correct. However, the authors do not discuss any consequence of their result. Why was it important to prove it? What does it tell us about the networks? While the proof may be of interest to the authors of [14] to correct their (possible) mistakes, I think the paper will go under the radar for most people and thus encourage the authors to heavily revise the paper.\n\nClarity: the writing is clear in general. The proofs sometimes jump over non-trivial things and explain easy steps, but that maybe subjective. The paper spends no effort explaining the contribution and its consequences.\n\nOriginality: the proven statements are novel and extend/fix the claims of [14]\n\nSignificance: as said above, I believe that in the current form the paper will have little to no impact. The importance of proving the main statement under more general conditions on activation functions is doubtful and the authors do not comment on that.\n\nMinor comments:\n* when introducing T{i,:,:} the <> notation is not clear. I could guess it from the later usage of the symbol, but these brackets can mean a lot of things, e.g. bracket mean (Section 2.1)\n* it would be beneficial to define the main objects, wide-network limits, in a more formal way (Section 2.3)\n* how the wide regime (large N) is interesting for studying deep NNs? [14] discusses that to some extent, but this should be explained here as well\n* q_0 is never defined\n* it's good practice to add numbers to all equations\n* I believe the claim in the appendix of [14] was meant to be conditionally independent (see also the reviews of [14]). It's clear that preactivations should not be independent and, while technically interesting, spending a page of theorem 10 and on plots seems unnecessary. Even in the paper's example preactivations are uncorrelated in the limit of large N.\n* I don't see the point of having experiments in this paper. The authors have already proven the fact. Also, it is not clear how to read the plots (no axes, little description) and come to the statements from page 9.\n\n********************\nAfter the authors' response:\nIf the main motivation of the paper is to fix the mistakes in [14], then the paper should clearly state so, in addition to explaining why fixing is necessary. While I believe that pointing out other paper's mistakes and correcting them is important, the current state of the paper leads me to keeping my initial score and recommending to reject the paper.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper744/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "On the effect of the activation function on the distribution of hidden nodes in a deep network", "abstract": "We analyze the joint probability distribution on the lengths of the\nvectors of hidden variables in different layers of a fully connected\ndeep network, when the weights and biases are chosen randomly according to\nGaussian distributions, and the input is binary-valued.  We show\nthat, if the activation function satisfies a minimal set of\nassumptions, satisfied by all activation functions that we know that\nare used in practice, then, as the width of the network gets large,\nthe ``length process'' converges in probability to a length map\nthat is determined as a simple function of the variances of the\nrandom weights and biases, and the activation function.\n\nWe also show that this convergence may fail for activation functions \nthat violate our assumptions.", "keywords": ["theory", "length map", "initialization"], "authorids": ["plong@google.com", "hsedghi@google.com"], "authors": ["Philip M. Long and Hanie Sedghi"], "TL;DR": "We prove that, for activation functions satisfying some conditions, as a deep network gets wide, the lengths of the vectors of hidden variables converge to a length map.", "pdf": "/pdf/7fb78879ecb9d621a1a1df79fbff9165bf7b8650.pdf", "paperhash": "sedghi|on_the_effect_of_the_activation_function_on_the_distribution_of_hidden_nodes_in_a_deep_network", "_bibtex": "@misc{\nsedghi2019on,\ntitle={On the effect of the activation function on the distribution of hidden nodes in a deep network},\nauthor={Philip M. Long and Hanie Sedghi},\nyear={2019},\nurl={https://openreview.net/forum?id=HJej3s09Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper744/Official_Review", "cdate": 1542234386175, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJej3s09Km", "replyto": "HJej3s09Km", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper744/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335793474, "tmdate": 1552335793474, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper744/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJxmsF5kp7", "original": null, "number": 3, "cdate": 1541544347214, "ddate": null, "tcdate": 1541544347214, "tmdate": 1541544451848, "tddate": null, "forum": "HJej3s09Km", "replyto": "HyeYsYKPh7", "invitation": "ICLR.cc/2019/Conference/-/Paper744/Official_Comment", "content": {"title": "significance, and independence vs. near-independence", "comment": "We thank this reviewer for the valuable detailed suggestions regarding presentation.  Thanks also for pointing out the need to define q_0 before the statement of Theorem 2;  q_0 = 1.\n\nRegarding motivation, as we pointed out to Reviewer 1, the length map studied in our paper, itself published in NIPS\u201916, has been repeatedly applied in a long series of papers published in NIPS, ICLR and ICML; we provide a list in the fourth paragraph of our paper.  It is therefore noteworthy that the logic supporting this length map published in [14] is not valid, and the claim made about it in that paper is incorrect.  This then motivates the question of what similar statement is correct. \n\nWe have spoken to two of the authors of [14] about our findings, and neither of them claimed that they meant \u201cconditionally independent\u201d when they wrote \u201cindependent\u201d.  While, on a first reading, this claim in their paper struck us as highly implausible, we felt that it was necessary to prove our claim that their claim was incorrect.   The experiments illustrate the strength of some of the effects analyzed in our paper.\n\nYou are correct that, as N gets large, the dependence between pairs of preactivation values becomes weaker.  But then, when analyzing the next layer, there are competing effects: as N gets larger, the dependence between individual pairs of hidden nodes is approaching zero but the number of such interferences is approaching infinity, but the improved stability obtained by averaging more cases is improving.  A rigorous analysis must take account of all of these.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper744/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper744/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper744/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the effect of the activation function on the distribution of hidden nodes in a deep network", "abstract": "We analyze the joint probability distribution on the lengths of the\nvectors of hidden variables in different layers of a fully connected\ndeep network, when the weights and biases are chosen randomly according to\nGaussian distributions, and the input is binary-valued.  We show\nthat, if the activation function satisfies a minimal set of\nassumptions, satisfied by all activation functions that we know that\nare used in practice, then, as the width of the network gets large,\nthe ``length process'' converges in probability to a length map\nthat is determined as a simple function of the variances of the\nrandom weights and biases, and the activation function.\n\nWe also show that this convergence may fail for activation functions \nthat violate our assumptions.", "keywords": ["theory", "length map", "initialization"], "authorids": ["plong@google.com", "hsedghi@google.com"], "authors": ["Philip M. Long and Hanie Sedghi"], "TL;DR": "We prove that, for activation functions satisfying some conditions, as a deep network gets wide, the lengths of the vectors of hidden variables converge to a length map.", "pdf": "/pdf/7fb78879ecb9d621a1a1df79fbff9165bf7b8650.pdf", "paperhash": "sedghi|on_the_effect_of_the_activation_function_on_the_distribution_of_hidden_nodes_in_a_deep_network", "_bibtex": "@misc{\nsedghi2019on,\ntitle={On the effect of the activation function on the distribution of hidden nodes in a deep network},\nauthor={Philip M. Long and Hanie Sedghi},\nyear={2019},\nurl={https://openreview.net/forum?id=HJej3s09Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper744/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624533, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJej3s09Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper744/Authors", "ICLR.cc/2019/Conference/Paper744/Reviewers", "ICLR.cc/2019/Conference/Paper744/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper744/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper744/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper744/Authors|ICLR.cc/2019/Conference/Paper744/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper744/Reviewers", "ICLR.cc/2019/Conference/Paper744/Authors", "ICLR.cc/2019/Conference/Paper744/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624533}}}, {"id": "HJlETOcJTX", "original": null, "number": 2, "cdate": 1541544124350, "ddate": null, "tcdate": 1541544124350, "tmdate": 1541544124350, "tddate": null, "forum": "HJej3s09Km", "replyto": "S1gSYsX52Q", "invitation": "ICLR.cc/2019/Conference/-/Paper744/Official_Comment", "content": {"title": "motivation, and discussion of normalization", "comment": "Theorem 10 demonstrates a flaw in the logic provided in [14], motivating a new analysis.  As discussed in some earlier papers in this line of research, if sigma_w and sigma_b are chosen so that q_{ell} = q_{ell-1}, very deep networks can be trained.  \n\nWe agree that extending a rigorous analysis to concern batch normalization and weight normalization is an interesting direction for further research.  It seems that the most interesting effects would occur during training.  Framing this for tractable rigorous analysis looks like an interesting and important challenge.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper744/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper744/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper744/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the effect of the activation function on the distribution of hidden nodes in a deep network", "abstract": "We analyze the joint probability distribution on the lengths of the\nvectors of hidden variables in different layers of a fully connected\ndeep network, when the weights and biases are chosen randomly according to\nGaussian distributions, and the input is binary-valued.  We show\nthat, if the activation function satisfies a minimal set of\nassumptions, satisfied by all activation functions that we know that\nare used in practice, then, as the width of the network gets large,\nthe ``length process'' converges in probability to a length map\nthat is determined as a simple function of the variances of the\nrandom weights and biases, and the activation function.\n\nWe also show that this convergence may fail for activation functions \nthat violate our assumptions.", "keywords": ["theory", "length map", "initialization"], "authorids": ["plong@google.com", "hsedghi@google.com"], "authors": ["Philip M. Long and Hanie Sedghi"], "TL;DR": "We prove that, for activation functions satisfying some conditions, as a deep network gets wide, the lengths of the vectors of hidden variables converge to a length map.", "pdf": "/pdf/7fb78879ecb9d621a1a1df79fbff9165bf7b8650.pdf", "paperhash": "sedghi|on_the_effect_of_the_activation_function_on_the_distribution_of_hidden_nodes_in_a_deep_network", "_bibtex": "@misc{\nsedghi2019on,\ntitle={On the effect of the activation function on the distribution of hidden nodes in a deep network},\nauthor={Philip M. Long and Hanie Sedghi},\nyear={2019},\nurl={https://openreview.net/forum?id=HJej3s09Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper744/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624533, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJej3s09Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper744/Authors", "ICLR.cc/2019/Conference/Paper744/Reviewers", "ICLR.cc/2019/Conference/Paper744/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper744/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper744/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper744/Authors|ICLR.cc/2019/Conference/Paper744/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper744/Reviewers", "ICLR.cc/2019/Conference/Paper744/Authors", "ICLR.cc/2019/Conference/Paper744/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624533}}}, {"id": "rygxqdckaX", "original": null, "number": 1, "cdate": 1541544072243, "ddate": null, "tcdate": 1541544072243, "tmdate": 1541544072243, "tddate": null, "forum": "HJej3s09Km", "replyto": "BklpNmRoh7", "invitation": "ICLR.cc/2019/Conference/-/Paper744/Official_Comment", "content": {"title": "motivation for our analysis", "comment": "The length map studied in our paper, itself published in NIPS\u201916, has been repeatedly applied in a long series of papers published in NIPS, ICLR and ICML; we provide a list in the fourth paragraph of our paper.  It is therefore noteworthy that the logic supporting this length map published in [14] is not valid, and the claim made about it in that paper is incorrect.  This in turn motivates the question of what similar statement is correct. "}, "signatures": ["ICLR.cc/2019/Conference/Paper744/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper744/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper744/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the effect of the activation function on the distribution of hidden nodes in a deep network", "abstract": "We analyze the joint probability distribution on the lengths of the\nvectors of hidden variables in different layers of a fully connected\ndeep network, when the weights and biases are chosen randomly according to\nGaussian distributions, and the input is binary-valued.  We show\nthat, if the activation function satisfies a minimal set of\nassumptions, satisfied by all activation functions that we know that\nare used in practice, then, as the width of the network gets large,\nthe ``length process'' converges in probability to a length map\nthat is determined as a simple function of the variances of the\nrandom weights and biases, and the activation function.\n\nWe also show that this convergence may fail for activation functions \nthat violate our assumptions.", "keywords": ["theory", "length map", "initialization"], "authorids": ["plong@google.com", "hsedghi@google.com"], "authors": ["Philip M. Long and Hanie Sedghi"], "TL;DR": "We prove that, for activation functions satisfying some conditions, as a deep network gets wide, the lengths of the vectors of hidden variables converge to a length map.", "pdf": "/pdf/7fb78879ecb9d621a1a1df79fbff9165bf7b8650.pdf", "paperhash": "sedghi|on_the_effect_of_the_activation_function_on_the_distribution_of_hidden_nodes_in_a_deep_network", "_bibtex": "@misc{\nsedghi2019on,\ntitle={On the effect of the activation function on the distribution of hidden nodes in a deep network},\nauthor={Philip M. Long and Hanie Sedghi},\nyear={2019},\nurl={https://openreview.net/forum?id=HJej3s09Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper744/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624533, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJej3s09Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper744/Authors", "ICLR.cc/2019/Conference/Paper744/Reviewers", "ICLR.cc/2019/Conference/Paper744/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper744/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper744/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper744/Authors|ICLR.cc/2019/Conference/Paper744/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper744/Reviewers", "ICLR.cc/2019/Conference/Paper744/Authors", "ICLR.cc/2019/Conference/Paper744/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624533}}}, {"id": "BklpNmRoh7", "original": null, "number": 3, "cdate": 1541296949318, "ddate": null, "tcdate": 1541296949318, "tmdate": 1541533724423, "tddate": null, "forum": "HJej3s09Km", "replyto": "HJej3s09Km", "invitation": "ICLR.cc/2019/Conference/-/Paper744/Official_Review", "content": {"title": "an abstract analysis that does not aim to derive any conclusions", "review": "This paper performs an analysis of the length scale of activations for deep fully-connected neural networks with respect to the activation function in neural networks. The authors show that for a very large class of activation functions, the length process converges in probability.\n\nI am listing my main concerns about this manuscript below.\n\n1. The paper is poorly motivated and does not make an attempt to relate its results to observations in practice or the design of new techniques. It is an abstract analysis of the probability distribution of the activations.\n\n2. Theorem 2, which is the main theoretical contribution of the paper, hinges on fixing the inputs of the neural network with weights sampled randomly from a Gaussian distribution. It is difficult to connect this with practice. This is not unreasonable and indeed common in mean-field analyses. However such analyses go further in their implications, e.g., https://arxiv.org/abs/1606.05340, https://arxiv.org/abs/1806.05393 etc. This is my main concern about the paper, its lack of concrete implications despite the simplifying assumptions.\n\n3. It would be very interesting if the analysis in this manuscript informs new activation functions or new initialization methods for training deep networks.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper744/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the effect of the activation function on the distribution of hidden nodes in a deep network", "abstract": "We analyze the joint probability distribution on the lengths of the\nvectors of hidden variables in different layers of a fully connected\ndeep network, when the weights and biases are chosen randomly according to\nGaussian distributions, and the input is binary-valued.  We show\nthat, if the activation function satisfies a minimal set of\nassumptions, satisfied by all activation functions that we know that\nare used in practice, then, as the width of the network gets large,\nthe ``length process'' converges in probability to a length map\nthat is determined as a simple function of the variances of the\nrandom weights and biases, and the activation function.\n\nWe also show that this convergence may fail for activation functions \nthat violate our assumptions.", "keywords": ["theory", "length map", "initialization"], "authorids": ["plong@google.com", "hsedghi@google.com"], "authors": ["Philip M. Long and Hanie Sedghi"], "TL;DR": "We prove that, for activation functions satisfying some conditions, as a deep network gets wide, the lengths of the vectors of hidden variables converge to a length map.", "pdf": "/pdf/7fb78879ecb9d621a1a1df79fbff9165bf7b8650.pdf", "paperhash": "sedghi|on_the_effect_of_the_activation_function_on_the_distribution_of_hidden_nodes_in_a_deep_network", "_bibtex": "@misc{\nsedghi2019on,\ntitle={On the effect of the activation function on the distribution of hidden nodes in a deep network},\nauthor={Philip M. Long and Hanie Sedghi},\nyear={2019},\nurl={https://openreview.net/forum?id=HJej3s09Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper744/Official_Review", "cdate": 1542234386175, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJej3s09Km", "replyto": "HJej3s09Km", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper744/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335793474, "tmdate": 1552335793474, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper744/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1gSYsX52Q", "original": null, "number": 2, "cdate": 1541188477422, "ddate": null, "tcdate": 1541188477422, "tmdate": 1541533724219, "tddate": null, "forum": "HJej3s09Km", "replyto": "HJej3s09Km", "invitation": "ICLR.cc/2019/Conference/-/Paper744/Official_Review", "content": {"title": "In this paper, the authors studied how the activation function affects the behavior of randomized deep networks. ", "review": "* summary\nIn this paper, the authors studied how the activation function affects the behavior of randomized deep networks. \nWhen the activation function is permissible and the weights of DNN are generated from the Gaussian distribution,\nthe output of each layer was related to the so-called length process. When the permissibility is violated,\nthe convergence property may not hold. Some numerical experiments confirm the theoretical findings. \n\n\n* comments\nHowever, The randomized DNN is not clear whether theoretical results in this paper is related to the practical DNN. \nThe authors showed intensive proofs of theorems.\nI think that the relation between DNN in practice and the results in this paper should be pursued more. \n\n* The meaning of Theorem 10 is not clear. What does the theorem reveal about the ReLU function in the practical usage?\n\n* In this paper, a limit theorem in terms of the dimension N is considered.\n  However, the limit theorem in terms of the depth D is also important for the DNN.\n  Some comments on that would be helpful for readers. \n\n* Is there any relation between the analysis in this paper and batch normalization or weight normalization? \n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper744/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the effect of the activation function on the distribution of hidden nodes in a deep network", "abstract": "We analyze the joint probability distribution on the lengths of the\nvectors of hidden variables in different layers of a fully connected\ndeep network, when the weights and biases are chosen randomly according to\nGaussian distributions, and the input is binary-valued.  We show\nthat, if the activation function satisfies a minimal set of\nassumptions, satisfied by all activation functions that we know that\nare used in practice, then, as the width of the network gets large,\nthe ``length process'' converges in probability to a length map\nthat is determined as a simple function of the variances of the\nrandom weights and biases, and the activation function.\n\nWe also show that this convergence may fail for activation functions \nthat violate our assumptions.", "keywords": ["theory", "length map", "initialization"], "authorids": ["plong@google.com", "hsedghi@google.com"], "authors": ["Philip M. Long and Hanie Sedghi"], "TL;DR": "We prove that, for activation functions satisfying some conditions, as a deep network gets wide, the lengths of the vectors of hidden variables converge to a length map.", "pdf": "/pdf/7fb78879ecb9d621a1a1df79fbff9165bf7b8650.pdf", "paperhash": "sedghi|on_the_effect_of_the_activation_function_on_the_distribution_of_hidden_nodes_in_a_deep_network", "_bibtex": "@misc{\nsedghi2019on,\ntitle={On the effect of the activation function on the distribution of hidden nodes in a deep network},\nauthor={Philip M. Long and Hanie Sedghi},\nyear={2019},\nurl={https://openreview.net/forum?id=HJej3s09Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper744/Official_Review", "cdate": 1542234386175, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJej3s09Km", "replyto": "HJej3s09Km", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper744/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335793474, "tmdate": 1552335793474, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper744/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}