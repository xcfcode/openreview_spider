{"notes": [{"id": "H1x8b6EtvH", "original": "BJlbDLwIwr", "number": 375, "cdate": 1569438973581, "ddate": null, "tcdate": 1569438973581, "tmdate": 1577168250163, "tddate": null, "forum": "H1x8b6EtvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"abstract": "  Pruning is an efficient model compression technique to remove redundancy in the connectivity of deep neural networks (DNNs). A critical problem to represent sparse matrices after pruning is that if fewer bits are used for quantization and pruning rate is enhanced, then the amount of index becomes relatively larger. Moreover, an irregular index form leads to low parallelism for convolutions and matrix multiplications. In this paper, we propose a new network pruning technique that generates a low-rank binary index matrix to compress index data significantly. Specifically, the proposed compression method finds a particular fine-grained pruning mask that can be decomposed into two binary matrices while decompressing index data is performed by simple binary matrix multiplication. We also propose a tile-based factorization technique that not only lowers memory requirements but also enhances compression ratio. Various DNN models (including conv layers and LSTM layers) can be pruned with much fewer indices compared to previous sparse matrix formats while maintaining the same pruning rate.", "title": "Network Pruning for Low-Rank Binary Index", "keywords": ["Pruning", "Model compression", "Index compression", "low-rank", "binary matrix decomposition"], "pdf": "/pdf/a5117a23ed74265a14607fbfb7a4cb0362c6dd96.pdf", "authors": ["Dongsoo Lee", "Se Jung Kwon", "Byeongwook Kim", "Parichay Kapoor", "Gu-Yeon Wei"], "TL;DR": "We propose a new pruning technique to generate a low-rank binary index matrix.", "authorids": ["dslee3@gmail.com", "mogndrewk@gmail.com", "quddnr145@gmail.com", "kparichay@gmail.com", "gywei@g.harvard.edu"], "paperhash": "lee|network_pruning_for_lowrank_binary_index", "original_pdf": "/attachment/a5117a23ed74265a14607fbfb7a4cb0362c6dd96.pdf", "_bibtex": "@misc{\nlee2020network,\ntitle={Network Pruning for Low-Rank Binary Index},\nauthor={Dongsoo Lee and Se Jung Kwon and Byeongwook Kim and Parichay Kapoor and Gu-Yeon Wei},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x8b6EtvH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "EleP33_Rrd", "original": null, "number": 1, "cdate": 1576798694641, "ddate": null, "tcdate": 1576798694641, "tmdate": 1576800940900, "tddate": null, "forum": "H1x8b6EtvH", "replyto": "H1x8b6EtvH", "invitation": "ICLR.cc/2020/Conference/Paper375/-/Decision", "content": {"decision": "Reject", "comment": "The submission proposes a method to improve over a standard binary network pruning strategy by the inclusion of a structured matrix product to encourage network weight sparsification that can have better memory and computational properties.  The idea is well motivated, but there were reviewer concerns about the quality of writing and in particular the quality of the experiments.  The reviewers were unanimous that the paper is not suitable for acceptance at ICLR, and no rebuttal was provided.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "  Pruning is an efficient model compression technique to remove redundancy in the connectivity of deep neural networks (DNNs). A critical problem to represent sparse matrices after pruning is that if fewer bits are used for quantization and pruning rate is enhanced, then the amount of index becomes relatively larger. Moreover, an irregular index form leads to low parallelism for convolutions and matrix multiplications. In this paper, we propose a new network pruning technique that generates a low-rank binary index matrix to compress index data significantly. Specifically, the proposed compression method finds a particular fine-grained pruning mask that can be decomposed into two binary matrices while decompressing index data is performed by simple binary matrix multiplication. We also propose a tile-based factorization technique that not only lowers memory requirements but also enhances compression ratio. Various DNN models (including conv layers and LSTM layers) can be pruned with much fewer indices compared to previous sparse matrix formats while maintaining the same pruning rate.", "title": "Network Pruning for Low-Rank Binary Index", "keywords": ["Pruning", "Model compression", "Index compression", "low-rank", "binary matrix decomposition"], "pdf": "/pdf/a5117a23ed74265a14607fbfb7a4cb0362c6dd96.pdf", "authors": ["Dongsoo Lee", "Se Jung Kwon", "Byeongwook Kim", "Parichay Kapoor", "Gu-Yeon Wei"], "TL;DR": "We propose a new pruning technique to generate a low-rank binary index matrix.", "authorids": ["dslee3@gmail.com", "mogndrewk@gmail.com", "quddnr145@gmail.com", "kparichay@gmail.com", "gywei@g.harvard.edu"], "paperhash": "lee|network_pruning_for_lowrank_binary_index", "original_pdf": "/attachment/a5117a23ed74265a14607fbfb7a4cb0362c6dd96.pdf", "_bibtex": "@misc{\nlee2020network,\ntitle={Network Pruning for Low-Rank Binary Index},\nauthor={Dongsoo Lee and Se Jung Kwon and Byeongwook Kim and Parichay Kapoor and Gu-Yeon Wei},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x8b6EtvH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1x8b6EtvH", "replyto": "H1x8b6EtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795729696, "tmdate": 1576800282339, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper375/-/Decision"}}}, {"id": "rJeAsR7iYB", "original": null, "number": 1, "cdate": 1571663525899, "ddate": null, "tcdate": 1571663525899, "tmdate": 1572972603196, "tddate": null, "forum": "H1x8b6EtvH", "replyto": "H1x8b6EtvH", "invitation": "ICLR.cc/2020/Conference/Paper375/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper addresses the problem of reducing the computational complexity of neural network pruning. Main idea is to compute a low-rank approximation of the binary index matrix used to represent the structure of the pruned network. In the considered setup, the binary index matrix is the (sparse) boolean matrix associated with the nonzero network's weights. As low-rank decomposition of binary matrices is a hard problem, the authors propose a method to approximate the solution by computing a more standard non-negative matrix factorization. \n\nThis is a nice problem and the proposed approach is interesting  but I would tend to reject the paper. The main problem, in my opinion, is a substantial lack of theoretical justifications.  What are the intuitive motivations for considering the proposed method? It is hard to understand why the approach should be preferred to others.  The idea of converting the binary low-rank factorization problem into a real-valued non-negative factorization problem is interesting but \ncould have been better justified (from an intuitive/theoretical and computational perspective). For example, why is it convenient to convert a hard matrix factorization problem (BMF) into another hard matrix factorization problem (NMF). \nAn empirical or theoretical analysis of the algorithm's computational complexity would help in this sense.\n\nAlso, it is not completely clear what is the relationship of the proposed approach with the neural network framework. Would the impact of the paper increase if presented as a method for binary matrix factorization without links to neural networks pruning? Perhaps a quantitative comparison between gains associated with sparse matrix storage versus other computational costs (related to training or pruning) would help to better collocate the proposed approach in the deep learning framework.\n\nQuestions:\n- The general idea of approximating BMF with NMF is interesting and could be investigated independently and more deeply. Have the problem and the proposed solution appeared before in the matrix factorization literature (without connections to neural network pruning )?\n- Is there any intuitive justification of why thresholding the matrix before and after the factorization leads to consistent results?\n- The cost of the proposed approach seems to depend on the rank. Could such explicit dependence be estimated quantitatively and compared with the computational complexity of solving the problem (BMF) directly? \n\n- In the experiments, the value of the pruning objective seems to decrease as the rank of the factorization increases. Is this an expected result? What is even less clear to me is the behaviour of the test accuracy. Is it normal for the pruned network accuracy to increase with sparsity? And why does the performance look independent from the rank?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper375/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper375/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "  Pruning is an efficient model compression technique to remove redundancy in the connectivity of deep neural networks (DNNs). A critical problem to represent sparse matrices after pruning is that if fewer bits are used for quantization and pruning rate is enhanced, then the amount of index becomes relatively larger. Moreover, an irregular index form leads to low parallelism for convolutions and matrix multiplications. In this paper, we propose a new network pruning technique that generates a low-rank binary index matrix to compress index data significantly. Specifically, the proposed compression method finds a particular fine-grained pruning mask that can be decomposed into two binary matrices while decompressing index data is performed by simple binary matrix multiplication. We also propose a tile-based factorization technique that not only lowers memory requirements but also enhances compression ratio. Various DNN models (including conv layers and LSTM layers) can be pruned with much fewer indices compared to previous sparse matrix formats while maintaining the same pruning rate.", "title": "Network Pruning for Low-Rank Binary Index", "keywords": ["Pruning", "Model compression", "Index compression", "low-rank", "binary matrix decomposition"], "pdf": "/pdf/a5117a23ed74265a14607fbfb7a4cb0362c6dd96.pdf", "authors": ["Dongsoo Lee", "Se Jung Kwon", "Byeongwook Kim", "Parichay Kapoor", "Gu-Yeon Wei"], "TL;DR": "We propose a new pruning technique to generate a low-rank binary index matrix.", "authorids": ["dslee3@gmail.com", "mogndrewk@gmail.com", "quddnr145@gmail.com", "kparichay@gmail.com", "gywei@g.harvard.edu"], "paperhash": "lee|network_pruning_for_lowrank_binary_index", "original_pdf": "/attachment/a5117a23ed74265a14607fbfb7a4cb0362c6dd96.pdf", "_bibtex": "@misc{\nlee2020network,\ntitle={Network Pruning for Low-Rank Binary Index},\nauthor={Dongsoo Lee and Se Jung Kwon and Byeongwook Kim and Parichay Kapoor and Gu-Yeon Wei},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x8b6EtvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1x8b6EtvH", "replyto": "H1x8b6EtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper375/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper375/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575835649267, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper375/Reviewers"], "noninvitees": [], "tcdate": 1570237753067, "tmdate": 1575835649282, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper375/-/Official_Review"}}}, {"id": "SyxuPw3RFB", "original": null, "number": 2, "cdate": 1571895135896, "ddate": null, "tcdate": 1571895135896, "tmdate": 1572972603162, "tddate": null, "forum": "H1x8b6EtvH", "replyto": "H1x8b6EtvH", "invitation": "ICLR.cc/2020/Conference/Paper375/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an algorithm to compress index data representing the sparsity of a (deep) neural network. The ultimate goal is to reduce the storage requirements for the masks.\n\nThe algorithm is based on non-negative matrix factorization so the original mask is decomposed into two smaller matrices leading to memory savings depending on the rank of the matrices.\nIn addition, the paper proposes a tiling approach to further reduce storage requirements. \nResults on selected architectures and datasets show some improvements compared to naive binary index representations\n\nOn the positive side, I see an interesting approach to pruning a neural network, assuming the storage cost is guiding the pruning algorithm. If this was integrated into the training process, the optimizer could lead to an optimal solution. \n\n\nOn the negative side, I find the paper not easy to follow/read. A few comments on this regard:\n\n- The motivation is not clear. The paper mostly targets unstructured sparsity (thus the need for sparse matrix representation). However, not really sure if just addressing the storage requirements would have the proper impact. In the end, the original matrix needs to be recovered from the factorization.\n- The organization of the paper is also confusing.  Section 2 is about factorization, NMS, then the MNIST case study \n- I missed a clear algorithmic section. The title/ conclusions suggest pruning, while the introduction suggests index compression (to me quite different). \nAlgorithm 1 is for the matrix factorization part but how is this integrated into the entire training / fine-tuning process? That is very confusing to me.  \n\n- Interestingly, page 3 starts by suggesting magnitude-based pruning is sub-optimal. However, during the experimental section and the last part of section 2, the paper suggests pruning weights with a large magnitude will damage the performance. That is unclear to me.\n\n- To my understanding, the experiments are not very convincing. Why only those selected architectures and within those pruning only FC layers (AlexNet)? Or, according to table 2 caption, part of the network is pruned using magnitude-based methods and the rest based on the new pruning. That is very confusing. This means the proposal is for pruning or for representing the indexes? If for pruning, why not applying to all the layers in the network? How do we distinguish between the relevance of magnitude pruning and the proposed method?\n- How are the different hyperparameters set (S, Sp, k..). \n- How does section 3.2 fits with the rest? Methods are vaguely described and then one of them used in the experimental section. Very confusing.\n- Table 5 suggests non-zero weights are quantized... I guess I am completely lost now :).  "}, "signatures": ["ICLR.cc/2020/Conference/Paper375/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper375/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "  Pruning is an efficient model compression technique to remove redundancy in the connectivity of deep neural networks (DNNs). A critical problem to represent sparse matrices after pruning is that if fewer bits are used for quantization and pruning rate is enhanced, then the amount of index becomes relatively larger. Moreover, an irregular index form leads to low parallelism for convolutions and matrix multiplications. In this paper, we propose a new network pruning technique that generates a low-rank binary index matrix to compress index data significantly. Specifically, the proposed compression method finds a particular fine-grained pruning mask that can be decomposed into two binary matrices while decompressing index data is performed by simple binary matrix multiplication. We also propose a tile-based factorization technique that not only lowers memory requirements but also enhances compression ratio. Various DNN models (including conv layers and LSTM layers) can be pruned with much fewer indices compared to previous sparse matrix formats while maintaining the same pruning rate.", "title": "Network Pruning for Low-Rank Binary Index", "keywords": ["Pruning", "Model compression", "Index compression", "low-rank", "binary matrix decomposition"], "pdf": "/pdf/a5117a23ed74265a14607fbfb7a4cb0362c6dd96.pdf", "authors": ["Dongsoo Lee", "Se Jung Kwon", "Byeongwook Kim", "Parichay Kapoor", "Gu-Yeon Wei"], "TL;DR": "We propose a new pruning technique to generate a low-rank binary index matrix.", "authorids": ["dslee3@gmail.com", "mogndrewk@gmail.com", "quddnr145@gmail.com", "kparichay@gmail.com", "gywei@g.harvard.edu"], "paperhash": "lee|network_pruning_for_lowrank_binary_index", "original_pdf": "/attachment/a5117a23ed74265a14607fbfb7a4cb0362c6dd96.pdf", "_bibtex": "@misc{\nlee2020network,\ntitle={Network Pruning for Low-Rank Binary Index},\nauthor={Dongsoo Lee and Se Jung Kwon and Byeongwook Kim and Parichay Kapoor and Gu-Yeon Wei},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x8b6EtvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1x8b6EtvH", "replyto": "H1x8b6EtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper375/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper375/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575835649267, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper375/Reviewers"], "noninvitees": [], "tcdate": 1570237753067, "tmdate": 1575835649282, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper375/-/Official_Review"}}}, {"id": "rye0hcA15H", "original": null, "number": 3, "cdate": 1571969718059, "ddate": null, "tcdate": 1571969718059, "tmdate": 1572972603118, "tddate": null, "forum": "H1x8b6EtvH", "replyto": "H1x8b6EtvH", "invitation": "ICLR.cc/2020/Conference/Paper375/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposed a new network pruning method that generates a low-rank binary index matrix to compress index data, and a tile-based factorization technique to save memory. The binary index can achieve larger compression ratio than the CSR index, and the  low-rank binary index can further reduce memory usage. The results for various networks, including DNN, CNN and LSTM, have shown the effectiveness of the propsoed method. The paper is well-written and easy to follow.\n\nIn addition, I have some concerns:\n- Discussion about the relationship between your method with binary neural networks [1,2], especially the networks with binary weights [1].\n- There are no comparison with the state-of-art methods on pruning and index saving, such as deep compression [3] and CNNPack [4].\n- The experiments are not convincing, e.g. only pruning FC5 and FC6 layers in AlexNet on ImageNet dataset.\n\n[1] Courbariaux, Matthieu, Yoshua Bengio, and Jean-Pierre David. \"Binaryconnect: Training deep neural networks with binary weights during propagations.\" Advances in neural information processing systems. 2015.\n[2] Hubara, Itay, et al. \"Binarized neural networks.\" Advances in neural information processing systems. 2016.\n[3] Han, Song, Huizi Mao, and William J. Dally. \"Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding.\" arXiv preprint arXiv:1510.00149 (2015).\n[4] Wang, Yunhe, et al. \"Cnnpack: Packing convolutional neural networks in the frequency domain.\" Advances in neural information processing systems. 2016."}, "signatures": ["ICLR.cc/2020/Conference/Paper375/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper375/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "  Pruning is an efficient model compression technique to remove redundancy in the connectivity of deep neural networks (DNNs). A critical problem to represent sparse matrices after pruning is that if fewer bits are used for quantization and pruning rate is enhanced, then the amount of index becomes relatively larger. Moreover, an irregular index form leads to low parallelism for convolutions and matrix multiplications. In this paper, we propose a new network pruning technique that generates a low-rank binary index matrix to compress index data significantly. Specifically, the proposed compression method finds a particular fine-grained pruning mask that can be decomposed into two binary matrices while decompressing index data is performed by simple binary matrix multiplication. We also propose a tile-based factorization technique that not only lowers memory requirements but also enhances compression ratio. Various DNN models (including conv layers and LSTM layers) can be pruned with much fewer indices compared to previous sparse matrix formats while maintaining the same pruning rate.", "title": "Network Pruning for Low-Rank Binary Index", "keywords": ["Pruning", "Model compression", "Index compression", "low-rank", "binary matrix decomposition"], "pdf": "/pdf/a5117a23ed74265a14607fbfb7a4cb0362c6dd96.pdf", "authors": ["Dongsoo Lee", "Se Jung Kwon", "Byeongwook Kim", "Parichay Kapoor", "Gu-Yeon Wei"], "TL;DR": "We propose a new pruning technique to generate a low-rank binary index matrix.", "authorids": ["dslee3@gmail.com", "mogndrewk@gmail.com", "quddnr145@gmail.com", "kparichay@gmail.com", "gywei@g.harvard.edu"], "paperhash": "lee|network_pruning_for_lowrank_binary_index", "original_pdf": "/attachment/a5117a23ed74265a14607fbfb7a4cb0362c6dd96.pdf", "_bibtex": "@misc{\nlee2020network,\ntitle={Network Pruning for Low-Rank Binary Index},\nauthor={Dongsoo Lee and Se Jung Kwon and Byeongwook Kim and Parichay Kapoor and Gu-Yeon Wei},\nyear={2020},\nurl={https://openreview.net/forum?id=H1x8b6EtvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1x8b6EtvH", "replyto": "H1x8b6EtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper375/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper375/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575835649267, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper375/Reviewers"], "noninvitees": [], "tcdate": 1570237753067, "tmdate": 1575835649282, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper375/-/Official_Review"}}}], "count": 5}