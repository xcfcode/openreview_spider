{"notes": [{"id": "rylIy3R9K7", "original": "SygKkVaqtm", "number": 987, "cdate": 1538087901840, "ddate": null, "tcdate": 1538087901840, "tmdate": 1545355425975, "tddate": null, "forum": "rylIy3R9K7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Understand the dynamics of GANs via Primal-Dual Optimization", "abstract": "Generative adversarial network (GAN) is one of the best known unsupervised learning techniques these days due to its superior ability to learn data distributions. In spite of its great success in applications, GAN is known to be notoriously hard to train. The tremendous amount of time it takes to run the training algorithm and its sensitivity to hyper-parameter tuning have been haunting researchers in this area. To resolve these issues, we need to first understand how GANs work. Herein, we take a step toward this direction by examining the dynamics of GANs. We relate a large class of GANs including the Wasserstein GANs to max-min optimization problems with the coupling term being linear over the discriminator. By developing new primal-dual optimization tools, we show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate. The same framework also applies to multi-task learning and distributional robust learning problems. We verify our analysis on numerical examples with both synthetic and real data sets. We hope our analysis shed light on future studies on the theoretical properties of relevant machine learning problems.", "keywords": ["non-convex optimization", "generative adversarial network", "primal dual algorithm"], "authorids": ["lus@umn.edu", "rasingh@gatech.edu", "chen5719@umn.edu", "yongchen@gatech.edu", "mhong@umn.edu"], "authors": ["Songtao Lu", "Rahul Singh", "Xiangyi Chen", "Yongxin Chen", "Mingyi Hong"], "TL;DR": "We show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate.", "pdf": "/pdf/4b90db1a141a6c9fdc9f385855f76f970f1a7ff9.pdf", "paperhash": "lu|understand_the_dynamics_of_gans_via_primaldual_optimization", "_bibtex": "@misc{\nlu2019understand,\ntitle={Understand the dynamics of {GAN}s via Primal-Dual Optimization},\nauthor={Songtao Lu and Rahul Singh and Xiangyi Chen and Yongxin Chen and Mingyi Hong},\nyear={2019},\nurl={https://openreview.net/forum?id=rylIy3R9K7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Bke8K55QeE", "original": null, "number": 1, "cdate": 1544952446050, "ddate": null, "tcdate": 1544952446050, "tmdate": 1545354489832, "tddate": null, "forum": "rylIy3R9K7", "replyto": "rylIy3R9K7", "invitation": "ICLR.cc/2019/Conference/-/Paper987/Meta_Review", "content": {"metareview": "The paper studies the convergence of a primal-dual algorithm on a special min-max problem in WGAN where the maximization is with respect to linear variables (linear discriminator) and minimization is over non-convex generators. Experiments with both simulated and real world data are conducted to show that the algorithm works for WGANs and multi-task learning.\n\nThe major concern of reviewers lies in that the linear discriminator assumption in WGAN is too restrictive to general non-convex mini-max saddle point problem in GANs. Linear discriminator implies that the maximization part in min-max problem is concave, and it is thus not surprise that under this assumption the paper converts the original problem to a non-convex optimization instance and proves its first order convergence with descent lemma. This technique however can\u2019t be applied to general non-convex saddle point problem in GANs. Also the experimental studies are also not strong enough. Therefore, current version of the paper is proposed as borderline lean reject. \n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "A primal-dual algorithm for linear discriminator WGANs with first order convergence, as a special non-convex optimization problem."}, "signatures": ["ICLR.cc/2019/Conference/Paper987/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper987/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understand the dynamics of GANs via Primal-Dual Optimization", "abstract": "Generative adversarial network (GAN) is one of the best known unsupervised learning techniques these days due to its superior ability to learn data distributions. In spite of its great success in applications, GAN is known to be notoriously hard to train. The tremendous amount of time it takes to run the training algorithm and its sensitivity to hyper-parameter tuning have been haunting researchers in this area. To resolve these issues, we need to first understand how GANs work. Herein, we take a step toward this direction by examining the dynamics of GANs. We relate a large class of GANs including the Wasserstein GANs to max-min optimization problems with the coupling term being linear over the discriminator. By developing new primal-dual optimization tools, we show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate. The same framework also applies to multi-task learning and distributional robust learning problems. We verify our analysis on numerical examples with both synthetic and real data sets. We hope our analysis shed light on future studies on the theoretical properties of relevant machine learning problems.", "keywords": ["non-convex optimization", "generative adversarial network", "primal dual algorithm"], "authorids": ["lus@umn.edu", "rasingh@gatech.edu", "chen5719@umn.edu", "yongchen@gatech.edu", "mhong@umn.edu"], "authors": ["Songtao Lu", "Rahul Singh", "Xiangyi Chen", "Yongxin Chen", "Mingyi Hong"], "TL;DR": "We show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate.", "pdf": "/pdf/4b90db1a141a6c9fdc9f385855f76f970f1a7ff9.pdf", "paperhash": "lu|understand_the_dynamics_of_gans_via_primaldual_optimization", "_bibtex": "@misc{\nlu2019understand,\ntitle={Understand the dynamics of {GAN}s via Primal-Dual Optimization},\nauthor={Songtao Lu and Rahul Singh and Xiangyi Chen and Yongxin Chen and Mingyi Hong},\nyear={2019},\nurl={https://openreview.net/forum?id=rylIy3R9K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper987/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353010402, "tddate": null, "super": null, "final": null, "reply": {"forum": "rylIy3R9K7", "replyto": "rylIy3R9K7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper987/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper987/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper987/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353010402}}}, {"id": "SkgeCzEiT7", "original": null, "number": 5, "cdate": 1542304455823, "ddate": null, "tcdate": 1542304455823, "tmdate": 1542304455823, "tddate": null, "forum": "rylIy3R9K7", "replyto": "B1eiAIPInQ", "invitation": "ICLR.cc/2019/Conference/-/Paper987/Official_Comment", "content": {"title": "Response of \"Interesting Analysis GANs' Learning Dynamics in a Limited Setting\"", "comment": "Thanks sincerely for the positive feedback from the reviewer. We greatly appreciate the time and effort that have been made by the reviewer. \n\nThe general GANs can be formulated as primal-dual optimization problems with both the primal and the dual are nonconvex. Proving the convergence of any algorithm on these problems is extremely challenging. To the best of our knowledge, in these general settings of GANs, the convergence of the algorithms is still an open problem. Herein, we made a reasonable assumption on the structure of the discriminator and obtained significant results under this assumption. \nThe proof we have now doesn\u2019t apply to the general cases directly as our proof relies heavily on the convexity of the inner loop maximization problem. It might be possible to generalize the proof to the general case by modifying the potential functions used in the proof, but the path is not clear at this moment.\n\nWe have added one more example with MNIST data (Section 4.2). From this example, we can see the algorithm converge with a proper choice of step-size. The generated samples are not as good as those generated by general GANs. This is expected due to the linear structure of the discriminator. Nevertheless, the samples are reasonable and the quality improves as we increase the number of bases of the discriminator.\n\nThe focus of our paper is the dynamics of a first-order iterative algorithm on problem (4), which includes multi-task machine learning as a special case. Our intention is not to show the formulations we adopt are better. We would refer the reviewer to (Qian et al, 2018, Namkoong & Duchi 2016) for more discussions on the advantages of the problem formulations. We have moved the multi-task learning part (including both theory and numerical results) into the supplemental materials as an extension of this work. \n\nWe have commented on the stability issue in the introduction part in the revised version of the paper. We don\u2019t see any direct relation between techniques such as spectral normalization and our assumptions on the discriminator at this moment. It could be an interesting research direction to further unravel the dynamics of GANs.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper987/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper987/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper987/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understand the dynamics of GANs via Primal-Dual Optimization", "abstract": "Generative adversarial network (GAN) is one of the best known unsupervised learning techniques these days due to its superior ability to learn data distributions. In spite of its great success in applications, GAN is known to be notoriously hard to train. The tremendous amount of time it takes to run the training algorithm and its sensitivity to hyper-parameter tuning have been haunting researchers in this area. To resolve these issues, we need to first understand how GANs work. Herein, we take a step toward this direction by examining the dynamics of GANs. We relate a large class of GANs including the Wasserstein GANs to max-min optimization problems with the coupling term being linear over the discriminator. By developing new primal-dual optimization tools, we show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate. The same framework also applies to multi-task learning and distributional robust learning problems. We verify our analysis on numerical examples with both synthetic and real data sets. We hope our analysis shed light on future studies on the theoretical properties of relevant machine learning problems.", "keywords": ["non-convex optimization", "generative adversarial network", "primal dual algorithm"], "authorids": ["lus@umn.edu", "rasingh@gatech.edu", "chen5719@umn.edu", "yongchen@gatech.edu", "mhong@umn.edu"], "authors": ["Songtao Lu", "Rahul Singh", "Xiangyi Chen", "Yongxin Chen", "Mingyi Hong"], "TL;DR": "We show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate.", "pdf": "/pdf/4b90db1a141a6c9fdc9f385855f76f970f1a7ff9.pdf", "paperhash": "lu|understand_the_dynamics_of_gans_via_primaldual_optimization", "_bibtex": "@misc{\nlu2019understand,\ntitle={Understand the dynamics of {GAN}s via Primal-Dual Optimization},\nauthor={Songtao Lu and Rahul Singh and Xiangyi Chen and Yongxin Chen and Mingyi Hong},\nyear={2019},\nurl={https://openreview.net/forum?id=rylIy3R9K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper987/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619601, "tddate": null, "super": null, "final": null, "reply": {"forum": "rylIy3R9K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper987/Authors", "ICLR.cc/2019/Conference/Paper987/Reviewers", "ICLR.cc/2019/Conference/Paper987/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper987/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper987/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper987/Authors|ICLR.cc/2019/Conference/Paper987/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper987/Reviewers", "ICLR.cc/2019/Conference/Paper987/Authors", "ICLR.cc/2019/Conference/Paper987/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619601}}}, {"id": "rJxd8fVo6m", "original": null, "number": 4, "cdate": 1542304336498, "ddate": null, "tcdate": 1542304336498, "tmdate": 1542304336498, "tddate": null, "forum": "rylIy3R9K7", "replyto": "HJeodzPtnm", "invitation": "ICLR.cc/2019/Conference/-/Paper987/Official_Comment", "content": {"title": "Response of \"Interesting theory, advantage over baseline min-max algorithms unclear\"", "comment": "We thank the reviewer\u2019s time and effort of reading our work. We would like to emphasize that the theoretical result is one of the most important parts of this paper. To the best of our knowledge, it is the first convergence rate result about nonconvex min-max problems. Further, the convergence rate matches the rate that the ordinary gradient descent achieves for only the minimization problems.  \n \n1, We consider Wasserstein GAN or GANs with similar structure. The major assumption we made is that the discriminator is a linear combination of predefined basis functions. We agree that this assumption might be restrictive for general GANs, but in principle, any function can be approximated to an arbitrary precision with large enough bases. Further, we would like to remark that no constraint has been imposed on the generator G; it can be any general neural network. Therefore, by using a sufficiently large set of bases, this GAN model should have the capacity to generate samples in any distribution.  Once assuming the linear structure of the bases, then it is natural to add a convex regularizer, see e.g., Sanjabi et al 2018. Hence, we don\u2019t think strict concavity is a strong assumption. \nThough our analysis doesn\u2019t apply to the most general framework of GANs, we believe it is big step forward as it allows us to consider general generators which introduce nonconvexity in the resulting optimization problems.\nTo the best of our knowledge, all the existing works (e.g., Chen et al [2018]) used the convex-concave primal-dual dynamic to interpret GANs. Under this framework, the problem is convex and the algorithm converges to the global optimal solution of the problem, which deviates from the empirical results observed in the GANs problem. The reason is that this kind of analysis omits the inherent nature of the GANs problem which is nonconvexity. From an optimization viewpoint, our paper is the first result that shows the convergence rate of the primal-dual algorithm for nonconvex min-max problems. This part is independent of applying the primal-dual algorithm in applications of GANs. \n\nThe numerical results verified the effectiveness of the proposed primal-dual algorithm in the sense that the algorithm converges stably under the different size of the regularizers, and also shows that the convergence behavior of the proposed algorithm is consistent with the theoretical analysis. \n\n\n2, The improvement is due to the formulation of the problem shown eq. 12 which is a harder problem than the traditional minimization problem with fixed weights, in the sense that problem in Eq. 12 basically minimizes the worst case of the original problem. It can also be contributed to the fact that our proposed algorithm is able to solve the formulation well. Note that it is not possible to directly apply gradient descent to solve a min-max problem, where we definitely need some ascent technique to deal with the maximization problem.\n\n \n3, There are few works on the nonconvex min-max problems. Prior to our work, the only algorithm that can solve the nonconvex min-max problem is proposed in the reference Sanjabi et al [2018], which can be considered a baseline work. From the theoretical point of view, the primal-dual algorithm solves the problem in an alternating way rather than the baseline method which solves the dual problem up to some high accuracy and then solve the primal problem. The proposed primal-dual algorithm is a different strategy of generating the iterations compared with the baseline work, and it has a significant advantage in terms of computational consumption. In the revised version, we also added the results that compare the proposed primal-dual algorithm with the baseline method in terms of the running time, which shows that the primal-dual method has the similar performance in terms of the number of iterations as the baseline method but uses much less computational time. Please see page 7.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper987/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper987/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper987/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understand the dynamics of GANs via Primal-Dual Optimization", "abstract": "Generative adversarial network (GAN) is one of the best known unsupervised learning techniques these days due to its superior ability to learn data distributions. In spite of its great success in applications, GAN is known to be notoriously hard to train. The tremendous amount of time it takes to run the training algorithm and its sensitivity to hyper-parameter tuning have been haunting researchers in this area. To resolve these issues, we need to first understand how GANs work. Herein, we take a step toward this direction by examining the dynamics of GANs. We relate a large class of GANs including the Wasserstein GANs to max-min optimization problems with the coupling term being linear over the discriminator. By developing new primal-dual optimization tools, we show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate. The same framework also applies to multi-task learning and distributional robust learning problems. We verify our analysis on numerical examples with both synthetic and real data sets. We hope our analysis shed light on future studies on the theoretical properties of relevant machine learning problems.", "keywords": ["non-convex optimization", "generative adversarial network", "primal dual algorithm"], "authorids": ["lus@umn.edu", "rasingh@gatech.edu", "chen5719@umn.edu", "yongchen@gatech.edu", "mhong@umn.edu"], "authors": ["Songtao Lu", "Rahul Singh", "Xiangyi Chen", "Yongxin Chen", "Mingyi Hong"], "TL;DR": "We show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate.", "pdf": "/pdf/4b90db1a141a6c9fdc9f385855f76f970f1a7ff9.pdf", "paperhash": "lu|understand_the_dynamics_of_gans_via_primaldual_optimization", "_bibtex": "@misc{\nlu2019understand,\ntitle={Understand the dynamics of {GAN}s via Primal-Dual Optimization},\nauthor={Songtao Lu and Rahul Singh and Xiangyi Chen and Yongxin Chen and Mingyi Hong},\nyear={2019},\nurl={https://openreview.net/forum?id=rylIy3R9K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper987/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619601, "tddate": null, "super": null, "final": null, "reply": {"forum": "rylIy3R9K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper987/Authors", "ICLR.cc/2019/Conference/Paper987/Reviewers", "ICLR.cc/2019/Conference/Paper987/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper987/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper987/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper987/Authors|ICLR.cc/2019/Conference/Paper987/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper987/Reviewers", "ICLR.cc/2019/Conference/Paper987/Authors", "ICLR.cc/2019/Conference/Paper987/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619601}}}, {"id": "Byeb4WVoa7", "original": null, "number": 3, "cdate": 1542304041328, "ddate": null, "tcdate": 1542304041328, "tmdate": 1542304084147, "tddate": null, "forum": "rylIy3R9K7", "replyto": "ryeU9NyT2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper987/Official_Comment", "content": {"title": "Response of the official review \"Understand the dynamics of GANs via Primal-Dual Optimization\" ", "comment": "Thanks for the reviewer\u2019s comments. The response of each point is listed below.\n1, Thanks for bringing to our attention the related works. In Chen et al. (2018), the authors related a class of GANs to constrained convex optimization problems. More specifically, such GANs can be viewed as Lagrangian forms of these convex optimization problems. The optimization variables in their formulation are the probability density of the generator and the function values of the discriminator. Many issues like nonconvexity do not show up. This is essentially a nonparametric model, which doesn\u2019t apply directly to cases when the discriminator and the generator are represented by parametric models. On the other hand, our analysis is carried out on the parametric models directly and we have to deal with the nonconvexity of neural networks.  We have added the comments on this issue in the revised version of the paper and the reference Chen et al (2018) has been cited as part of the literature review. Please see page 2.\n \n2, The GANs problem motivates us to study the dynamics of solving the nonconvex min-max saddle point problem. It turns out this formulation is very general, which also covers the problem of multi-task learning models. \nThe connection between these problems is that: the problems of GANs and multi-task learning can be both formulated in the form of eq.13 under some conditions. In the revised manuscript we have added a new example for GAN and moved the multi-task learning section to the supplemental material. Please see page 8.\n \n3, The experimental results mainly show that the convergence behavior of the proposed primal-dual algorithm is consistent with the theoretical analysis. Our intention is by no means to show our algorithm generates superior samples than other methods. Instead, due to the linear features used in the discriminator, it is expected that our generated samples are going to be worse in quality for real dataset. In the revised version, we added an example with MNIST data to further support our results. Please see page 8.\nThanks for the appreciation of our theoretical results. We would like to remark that our goal is to understand the properties of the first-order algorithm in GAN training. Our paper presents a first theoretical result in analyzing the primal-dual algorithm for the nonconvex min-max problem that appears in GAN training."}, "signatures": ["ICLR.cc/2019/Conference/Paper987/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper987/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper987/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understand the dynamics of GANs via Primal-Dual Optimization", "abstract": "Generative adversarial network (GAN) is one of the best known unsupervised learning techniques these days due to its superior ability to learn data distributions. In spite of its great success in applications, GAN is known to be notoriously hard to train. The tremendous amount of time it takes to run the training algorithm and its sensitivity to hyper-parameter tuning have been haunting researchers in this area. To resolve these issues, we need to first understand how GANs work. Herein, we take a step toward this direction by examining the dynamics of GANs. We relate a large class of GANs including the Wasserstein GANs to max-min optimization problems with the coupling term being linear over the discriminator. By developing new primal-dual optimization tools, we show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate. The same framework also applies to multi-task learning and distributional robust learning problems. We verify our analysis on numerical examples with both synthetic and real data sets. We hope our analysis shed light on future studies on the theoretical properties of relevant machine learning problems.", "keywords": ["non-convex optimization", "generative adversarial network", "primal dual algorithm"], "authorids": ["lus@umn.edu", "rasingh@gatech.edu", "chen5719@umn.edu", "yongchen@gatech.edu", "mhong@umn.edu"], "authors": ["Songtao Lu", "Rahul Singh", "Xiangyi Chen", "Yongxin Chen", "Mingyi Hong"], "TL;DR": "We show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate.", "pdf": "/pdf/4b90db1a141a6c9fdc9f385855f76f970f1a7ff9.pdf", "paperhash": "lu|understand_the_dynamics_of_gans_via_primaldual_optimization", "_bibtex": "@misc{\nlu2019understand,\ntitle={Understand the dynamics of {GAN}s via Primal-Dual Optimization},\nauthor={Songtao Lu and Rahul Singh and Xiangyi Chen and Yongxin Chen and Mingyi Hong},\nyear={2019},\nurl={https://openreview.net/forum?id=rylIy3R9K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper987/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619601, "tddate": null, "super": null, "final": null, "reply": {"forum": "rylIy3R9K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper987/Authors", "ICLR.cc/2019/Conference/Paper987/Reviewers", "ICLR.cc/2019/Conference/Paper987/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper987/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper987/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper987/Authors|ICLR.cc/2019/Conference/Paper987/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper987/Reviewers", "ICLR.cc/2019/Conference/Paper987/Authors", "ICLR.cc/2019/Conference/Paper987/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619601}}}, {"id": "rJxuNaMj6Q", "original": null, "number": 2, "cdate": 1542298927583, "ddate": null, "tcdate": 1542298927583, "tmdate": 1542298927583, "tddate": null, "forum": "rylIy3R9K7", "replyto": "H1xMvWvMhQ", "invitation": "ICLR.cc/2019/Conference/-/Paper987/Official_Comment", "content": {"title": "The related work is included", "comment": "Thanks for the comment. Yadav et al. (2018) considered convex-concave primal-dual optimization problems. This is considerably different to our setup where GANs, as they should be, are formulated as nonconvex saddle point problems. We have included this reference in the revised version of this paper."}, "signatures": ["ICLR.cc/2019/Conference/Paper987/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper987/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper987/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understand the dynamics of GANs via Primal-Dual Optimization", "abstract": "Generative adversarial network (GAN) is one of the best known unsupervised learning techniques these days due to its superior ability to learn data distributions. In spite of its great success in applications, GAN is known to be notoriously hard to train. The tremendous amount of time it takes to run the training algorithm and its sensitivity to hyper-parameter tuning have been haunting researchers in this area. To resolve these issues, we need to first understand how GANs work. Herein, we take a step toward this direction by examining the dynamics of GANs. We relate a large class of GANs including the Wasserstein GANs to max-min optimization problems with the coupling term being linear over the discriminator. By developing new primal-dual optimization tools, we show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate. The same framework also applies to multi-task learning and distributional robust learning problems. We verify our analysis on numerical examples with both synthetic and real data sets. We hope our analysis shed light on future studies on the theoretical properties of relevant machine learning problems.", "keywords": ["non-convex optimization", "generative adversarial network", "primal dual algorithm"], "authorids": ["lus@umn.edu", "rasingh@gatech.edu", "chen5719@umn.edu", "yongchen@gatech.edu", "mhong@umn.edu"], "authors": ["Songtao Lu", "Rahul Singh", "Xiangyi Chen", "Yongxin Chen", "Mingyi Hong"], "TL;DR": "We show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate.", "pdf": "/pdf/4b90db1a141a6c9fdc9f385855f76f970f1a7ff9.pdf", "paperhash": "lu|understand_the_dynamics_of_gans_via_primaldual_optimization", "_bibtex": "@misc{\nlu2019understand,\ntitle={Understand the dynamics of {GAN}s via Primal-Dual Optimization},\nauthor={Songtao Lu and Rahul Singh and Xiangyi Chen and Yongxin Chen and Mingyi Hong},\nyear={2019},\nurl={https://openreview.net/forum?id=rylIy3R9K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper987/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619601, "tddate": null, "super": null, "final": null, "reply": {"forum": "rylIy3R9K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper987/Authors", "ICLR.cc/2019/Conference/Paper987/Reviewers", "ICLR.cc/2019/Conference/Paper987/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper987/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper987/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper987/Authors|ICLR.cc/2019/Conference/Paper987/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper987/Reviewers", "ICLR.cc/2019/Conference/Paper987/Authors", "ICLR.cc/2019/Conference/Paper987/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619601}}}, {"id": "ryeU9NyT2Q", "original": null, "number": 3, "cdate": 1541366926331, "ddate": null, "tcdate": 1541366926331, "tmdate": 1541533519309, "tddate": null, "forum": "rylIy3R9K7", "replyto": "rylIy3R9K7", "invitation": "ICLR.cc/2019/Conference/-/Paper987/Official_Review", "content": {"title": "official review of \"Understand the dynamics of GANs via Primal-Dual Optimization\" ", "review": "The paper proposed a primal-dual optimization framework for GANs and multi-task learning. It also analyzes the convergence rate of models. Some results are conducted on both real and synthetic data.\n\nHere are some concerns for the paper:\n\n1. The idea of the model is pretty similar with Xu et al. [2018] (Training Generative Adversarial Networks via Primal-Dual Subgradient Methods: A Lagrangian Perspective on GAN), especially the primal-dual setting. The author totally ignored it. \n\n2. The motivation of the paper is not clear. GANs and multi-task learning are two different perspectives. Which one is your focus and what is the connection between them? \n\n3. The experimental results are not good. The convergence analysis is good. However we also need more empirical evidence to support. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper987/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understand the dynamics of GANs via Primal-Dual Optimization", "abstract": "Generative adversarial network (GAN) is one of the best known unsupervised learning techniques these days due to its superior ability to learn data distributions. In spite of its great success in applications, GAN is known to be notoriously hard to train. The tremendous amount of time it takes to run the training algorithm and its sensitivity to hyper-parameter tuning have been haunting researchers in this area. To resolve these issues, we need to first understand how GANs work. Herein, we take a step toward this direction by examining the dynamics of GANs. We relate a large class of GANs including the Wasserstein GANs to max-min optimization problems with the coupling term being linear over the discriminator. By developing new primal-dual optimization tools, we show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate. The same framework also applies to multi-task learning and distributional robust learning problems. We verify our analysis on numerical examples with both synthetic and real data sets. We hope our analysis shed light on future studies on the theoretical properties of relevant machine learning problems.", "keywords": ["non-convex optimization", "generative adversarial network", "primal dual algorithm"], "authorids": ["lus@umn.edu", "rasingh@gatech.edu", "chen5719@umn.edu", "yongchen@gatech.edu", "mhong@umn.edu"], "authors": ["Songtao Lu", "Rahul Singh", "Xiangyi Chen", "Yongxin Chen", "Mingyi Hong"], "TL;DR": "We show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate.", "pdf": "/pdf/4b90db1a141a6c9fdc9f385855f76f970f1a7ff9.pdf", "paperhash": "lu|understand_the_dynamics_of_gans_via_primaldual_optimization", "_bibtex": "@misc{\nlu2019understand,\ntitle={Understand the dynamics of {GAN}s via Primal-Dual Optimization},\nauthor={Songtao Lu and Rahul Singh and Xiangyi Chen and Yongxin Chen and Mingyi Hong},\nyear={2019},\nurl={https://openreview.net/forum?id=rylIy3R9K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper987/Official_Review", "cdate": 1542234332001, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rylIy3R9K7", "replyto": "rylIy3R9K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper987/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335847637, "tmdate": 1552335847637, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper987/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJeodzPtnm", "original": null, "number": 2, "cdate": 1541137010666, "ddate": null, "tcdate": 1541137010666, "tmdate": 1541533519102, "tddate": null, "forum": "rylIy3R9K7", "replyto": "rylIy3R9K7", "invitation": "ICLR.cc/2019/Conference/-/Paper987/Official_Review", "content": {"title": "Interesting theory, advantage over baseline min-max algorithms unclear", "review": "This paper studies the convergence of a primal-dual algorithm on a certain min-max problem and experimentally shows that it works in GANs and multi-task learning.\n\nThis paper is clear and well-written. The convergence guarantee looks neat, and convergence to stationary points is a sensible thing on non convex-concave problems. I am not super familiar with the literature of saddle-point optimization and may not have a good sense about the significance of the theoretical result.\n\nMy main concern is that the assumptions in the theory are rather over-restrictive and it\u2019s not clear what intuitions or new messages they bring in for the practice of GANs. The convergence theorem requires the maximization problem (over discriminators) to be strictly concave. On GANs, this assumption is not (near) satisfied beyond the simple case of the LQG setting (PSD quadratic discriminators). On the other hand, the experiments on GANs just seem to say that the algorithm works but not much more beyond that. There is a brief discussion about the improvement in time consumption but it doesn\u2019t have a report a quantitative comparison in the wall time.\n\nOn multi-task learning, the proposed algorithm shows improvement over the baseline. However it is also unclear whether it is the *formulation* (12) that brings in the improvement, or it is the actual primal-dual *algorithm*. Perhaps it might be good to try gradient descent on (12) and see if it also works well. \n\nIn general, I would recommend the authors to have a more convincing demonstration of the strength of this algorithm over baseline methods on min-max problems, either theoretical or empirical. ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper987/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understand the dynamics of GANs via Primal-Dual Optimization", "abstract": "Generative adversarial network (GAN) is one of the best known unsupervised learning techniques these days due to its superior ability to learn data distributions. In spite of its great success in applications, GAN is known to be notoriously hard to train. The tremendous amount of time it takes to run the training algorithm and its sensitivity to hyper-parameter tuning have been haunting researchers in this area. To resolve these issues, we need to first understand how GANs work. Herein, we take a step toward this direction by examining the dynamics of GANs. We relate a large class of GANs including the Wasserstein GANs to max-min optimization problems with the coupling term being linear over the discriminator. By developing new primal-dual optimization tools, we show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate. The same framework also applies to multi-task learning and distributional robust learning problems. We verify our analysis on numerical examples with both synthetic and real data sets. We hope our analysis shed light on future studies on the theoretical properties of relevant machine learning problems.", "keywords": ["non-convex optimization", "generative adversarial network", "primal dual algorithm"], "authorids": ["lus@umn.edu", "rasingh@gatech.edu", "chen5719@umn.edu", "yongchen@gatech.edu", "mhong@umn.edu"], "authors": ["Songtao Lu", "Rahul Singh", "Xiangyi Chen", "Yongxin Chen", "Mingyi Hong"], "TL;DR": "We show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate.", "pdf": "/pdf/4b90db1a141a6c9fdc9f385855f76f970f1a7ff9.pdf", "paperhash": "lu|understand_the_dynamics_of_gans_via_primaldual_optimization", "_bibtex": "@misc{\nlu2019understand,\ntitle={Understand the dynamics of {GAN}s via Primal-Dual Optimization},\nauthor={Songtao Lu and Rahul Singh and Xiangyi Chen and Yongxin Chen and Mingyi Hong},\nyear={2019},\nurl={https://openreview.net/forum?id=rylIy3R9K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper987/Official_Review", "cdate": 1542234332001, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rylIy3R9K7", "replyto": "rylIy3R9K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper987/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335847637, "tmdate": 1552335847637, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper987/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1eiAIPInQ", "original": null, "number": 1, "cdate": 1540941522591, "ddate": null, "tcdate": 1540941522591, "tmdate": 1541533518894, "tddate": null, "forum": "rylIy3R9K7", "replyto": "rylIy3R9K7", "invitation": "ICLR.cc/2019/Conference/-/Paper987/Official_Review", "content": {"title": "Interesting Analysis GANs' Learning Dynamics in a Limited Setting", "review": "This paper analyses the learning dynamics of GANs by formulating the problem as a primal-dual optimisation problem. This formulation assumes a limited class of models -- Wasserstein GANs with discriminators using linear combinations of base functions. Although this setting is limited, it advanced our understanding of a central problem related to GANs, and provides intuition for more general cases. The paper further shows the same analysis can be applied to multi-task learning and distributed learning.\n\nPros:\n\n* The paper is well written and well motivated\n* The theoretical analysis is solid and provide intuition for more complex problems\n\nCons:\n\n* The primal-dual formulation assumes Wasserstein GANs using linear discriminator. This simplification is understandable, but it would be helpful to at least comment on more general cases.\n\n* Experiments are limited: only results from GANs with LQG setting were presented. Since the assumption of linear discriminator (in basis) is already strong, it would be helpful to show the experimental results from this more general setting.\n\n* The results on multi-task learning were interesting, but the advantage of optimising the mixing weights was unclear compared with the even mixture baseline. This weakens the analysis of the learning dynamics, since learning the mixing did not seem to be important.\n\nIt would also be helpful to comment on recently proposed stabilising methods. For example, would spectral normalisation bring learning dynamics closer to the assumed model?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper987/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understand the dynamics of GANs via Primal-Dual Optimization", "abstract": "Generative adversarial network (GAN) is one of the best known unsupervised learning techniques these days due to its superior ability to learn data distributions. In spite of its great success in applications, GAN is known to be notoriously hard to train. The tremendous amount of time it takes to run the training algorithm and its sensitivity to hyper-parameter tuning have been haunting researchers in this area. To resolve these issues, we need to first understand how GANs work. Herein, we take a step toward this direction by examining the dynamics of GANs. We relate a large class of GANs including the Wasserstein GANs to max-min optimization problems with the coupling term being linear over the discriminator. By developing new primal-dual optimization tools, we show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate. The same framework also applies to multi-task learning and distributional robust learning problems. We verify our analysis on numerical examples with both synthetic and real data sets. We hope our analysis shed light on future studies on the theoretical properties of relevant machine learning problems.", "keywords": ["non-convex optimization", "generative adversarial network", "primal dual algorithm"], "authorids": ["lus@umn.edu", "rasingh@gatech.edu", "chen5719@umn.edu", "yongchen@gatech.edu", "mhong@umn.edu"], "authors": ["Songtao Lu", "Rahul Singh", "Xiangyi Chen", "Yongxin Chen", "Mingyi Hong"], "TL;DR": "We show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate.", "pdf": "/pdf/4b90db1a141a6c9fdc9f385855f76f970f1a7ff9.pdf", "paperhash": "lu|understand_the_dynamics_of_gans_via_primaldual_optimization", "_bibtex": "@misc{\nlu2019understand,\ntitle={Understand the dynamics of {GAN}s via Primal-Dual Optimization},\nauthor={Songtao Lu and Rahul Singh and Xiangyi Chen and Yongxin Chen and Mingyi Hong},\nyear={2019},\nurl={https://openreview.net/forum?id=rylIy3R9K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper987/Official_Review", "cdate": 1542234332001, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rylIy3R9K7", "replyto": "rylIy3R9K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper987/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335847637, "tmdate": 1552335847637, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper987/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1xMvWvMhQ", "original": null, "number": 2, "cdate": 1540677977528, "ddate": null, "tcdate": 1540677977528, "tmdate": 1540677977528, "tddate": null, "forum": "rylIy3R9K7", "replyto": "Hyl428xe9X", "invitation": "ICLR.cc/2019/Conference/-/Paper987/Public_Comment", "content": {"comment": "This work is also related to the following paper, which uses prediction steps (which can be primal-dual optimization with extra gradients):\n\nAbhay Yadav, Sohil Shah, Zheng Xu, David Jacobs, Tom Goldstein, \"Stabilizing Adversarial Nets with Prediction Methods\", ICLR 2018.\n\nThe authors are encouraged to cite and discuss the differences/contribution of their work.", "title": "Related work"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper987/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understand the dynamics of GANs via Primal-Dual Optimization", "abstract": "Generative adversarial network (GAN) is one of the best known unsupervised learning techniques these days due to its superior ability to learn data distributions. In spite of its great success in applications, GAN is known to be notoriously hard to train. The tremendous amount of time it takes to run the training algorithm and its sensitivity to hyper-parameter tuning have been haunting researchers in this area. To resolve these issues, we need to first understand how GANs work. Herein, we take a step toward this direction by examining the dynamics of GANs. We relate a large class of GANs including the Wasserstein GANs to max-min optimization problems with the coupling term being linear over the discriminator. By developing new primal-dual optimization tools, we show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate. The same framework also applies to multi-task learning and distributional robust learning problems. We verify our analysis on numerical examples with both synthetic and real data sets. We hope our analysis shed light on future studies on the theoretical properties of relevant machine learning problems.", "keywords": ["non-convex optimization", "generative adversarial network", "primal dual algorithm"], "authorids": ["lus@umn.edu", "rasingh@gatech.edu", "chen5719@umn.edu", "yongchen@gatech.edu", "mhong@umn.edu"], "authors": ["Songtao Lu", "Rahul Singh", "Xiangyi Chen", "Yongxin Chen", "Mingyi Hong"], "TL;DR": "We show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate.", "pdf": "/pdf/4b90db1a141a6c9fdc9f385855f76f970f1a7ff9.pdf", "paperhash": "lu|understand_the_dynamics_of_gans_via_primaldual_optimization", "_bibtex": "@misc{\nlu2019understand,\ntitle={Understand the dynamics of {GAN}s via Primal-Dual Optimization},\nauthor={Songtao Lu and Rahul Singh and Xiangyi Chen and Yongxin Chen and Mingyi Hong},\nyear={2019},\nurl={https://openreview.net/forum?id=rylIy3R9K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper987/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311705555, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rylIy3R9K7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper987/Authors", "ICLR.cc/2019/Conference/Paper987/Reviewers", "ICLR.cc/2019/Conference/Paper987/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper987/Authors", "ICLR.cc/2019/Conference/Paper987/Reviewers", "ICLR.cc/2019/Conference/Paper987/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311705555}}}, {"id": "Hyl428xe9X", "original": null, "number": 1, "cdate": 1538422444171, "ddate": null, "tcdate": 1538422444171, "tmdate": 1538422458907, "tddate": null, "forum": "rylIy3R9K7", "replyto": "Skx2YTK0Km", "invitation": "ICLR.cc/2019/Conference/-/Paper987/Official_Comment", "content": {"title": "response to the comments about \"related works\"", "comment": "Thanks for the reviewers\u2019 comments. These references will be included in the literature review part of this paper when a revision is allowed. Generally speaking, our response is that the main contributions of the above mentioned papers are not directly related to the those of this paper.\n\nIn Zhao et al. (2018), the authors unified several generative models, e.g., VAE, infoGAN, in the Lagrangian framework. The Lagrangian problem they considered is different to ours. For one thing, the dual variable in their problem is a Lagrangian multiplier, while in our problem, it is the discriminator of GAN. Besides, the focus of their paper is not the optimization algorithm. The algorithm design and convergence analysis were not mentioned much. Our main contribution, on the other hand, is a convergence proof of a first-order primal-dual algorithm for GANs.\n\nIn Chen et al. (2018), the authors related a class of GANs to constrained convex optimization problems. More specifically, such GANs can be viewed as Lagrangian forms of these convex optimization problems. The optimization variables in their formulation are the probability density of the generator and the function values of the discriminator. Many issues like nonconvexity do not show up. This is essentially a nonparametric model, which doesn\u2019t apply to cases when the discriminator and the generator are represented by parametric models. On the other hand, our analysis is carried out on the parametric models directly and we have to deal with the nonconvexity of neural networks.\n\nWe couldn\u2019t find the preprint of Farnia et al. (2018). Based on the abstract on the NIPS website, we believe the primal-dual formulations we considered are similar in the sense that the discriminator is constrained to a convex set. However, the focus of Farnia et al. (2018) is not the convergence properties of optimization algorithms, instead, they investigated the properties of the optimal solutions. We think our convergence analysis of the first-order primal-dual algorithm is complementary to their results.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper987/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper987/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper987/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understand the dynamics of GANs via Primal-Dual Optimization", "abstract": "Generative adversarial network (GAN) is one of the best known unsupervised learning techniques these days due to its superior ability to learn data distributions. In spite of its great success in applications, GAN is known to be notoriously hard to train. The tremendous amount of time it takes to run the training algorithm and its sensitivity to hyper-parameter tuning have been haunting researchers in this area. To resolve these issues, we need to first understand how GANs work. Herein, we take a step toward this direction by examining the dynamics of GANs. We relate a large class of GANs including the Wasserstein GANs to max-min optimization problems with the coupling term being linear over the discriminator. By developing new primal-dual optimization tools, we show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate. The same framework also applies to multi-task learning and distributional robust learning problems. We verify our analysis on numerical examples with both synthetic and real data sets. We hope our analysis shed light on future studies on the theoretical properties of relevant machine learning problems.", "keywords": ["non-convex optimization", "generative adversarial network", "primal dual algorithm"], "authorids": ["lus@umn.edu", "rasingh@gatech.edu", "chen5719@umn.edu", "yongchen@gatech.edu", "mhong@umn.edu"], "authors": ["Songtao Lu", "Rahul Singh", "Xiangyi Chen", "Yongxin Chen", "Mingyi Hong"], "TL;DR": "We show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate.", "pdf": "/pdf/4b90db1a141a6c9fdc9f385855f76f970f1a7ff9.pdf", "paperhash": "lu|understand_the_dynamics_of_gans_via_primaldual_optimization", "_bibtex": "@misc{\nlu2019understand,\ntitle={Understand the dynamics of {GAN}s via Primal-Dual Optimization},\nauthor={Songtao Lu and Rahul Singh and Xiangyi Chen and Yongxin Chen and Mingyi Hong},\nyear={2019},\nurl={https://openreview.net/forum?id=rylIy3R9K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper987/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619601, "tddate": null, "super": null, "final": null, "reply": {"forum": "rylIy3R9K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper987/Authors", "ICLR.cc/2019/Conference/Paper987/Reviewers", "ICLR.cc/2019/Conference/Paper987/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper987/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper987/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper987/Authors|ICLR.cc/2019/Conference/Paper987/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper987/Reviewers", "ICLR.cc/2019/Conference/Paper987/Authors", "ICLR.cc/2019/Conference/Paper987/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619601}}}, {"id": "Skx2YTK0Km", "original": null, "number": 1, "cdate": 1538329988099, "ddate": null, "tcdate": 1538329988099, "tmdate": 1538329988099, "tddate": null, "forum": "rylIy3R9K7", "replyto": "rylIy3R9K7", "invitation": "ICLR.cc/2019/Conference/-/Paper987/Public_Comment", "content": {"comment": "This is a very interesting work that analyzes the convergence of GAN.  I would like to point out that the following works also consider GAN via primal-dual optimization:\n  \nShengjia Zhao, Jiaming Song, Stefano Ermon, \"The Information Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Models\", UAI 2018.\nXu Chen, Jiang Wang, Hao Ge, \"Training Generative Adversarial Networks via Primal-Dual Subgradient Methods: A Lagrangian Perspective on GAN\", ICLR 2018.\nFarzan Farnia, David Tse, \"A Convex Duality Framework for GANs\", NIPS 2018.\n\nThe authors are encouraged to include these latest related papers in the literature review and point out the differences/contributions of their work.", "title": "related works"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper987/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understand the dynamics of GANs via Primal-Dual Optimization", "abstract": "Generative adversarial network (GAN) is one of the best known unsupervised learning techniques these days due to its superior ability to learn data distributions. In spite of its great success in applications, GAN is known to be notoriously hard to train. The tremendous amount of time it takes to run the training algorithm and its sensitivity to hyper-parameter tuning have been haunting researchers in this area. To resolve these issues, we need to first understand how GANs work. Herein, we take a step toward this direction by examining the dynamics of GANs. We relate a large class of GANs including the Wasserstein GANs to max-min optimization problems with the coupling term being linear over the discriminator. By developing new primal-dual optimization tools, we show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate. The same framework also applies to multi-task learning and distributional robust learning problems. We verify our analysis on numerical examples with both synthetic and real data sets. We hope our analysis shed light on future studies on the theoretical properties of relevant machine learning problems.", "keywords": ["non-convex optimization", "generative adversarial network", "primal dual algorithm"], "authorids": ["lus@umn.edu", "rasingh@gatech.edu", "chen5719@umn.edu", "yongchen@gatech.edu", "mhong@umn.edu"], "authors": ["Songtao Lu", "Rahul Singh", "Xiangyi Chen", "Yongxin Chen", "Mingyi Hong"], "TL;DR": "We show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate.", "pdf": "/pdf/4b90db1a141a6c9fdc9f385855f76f970f1a7ff9.pdf", "paperhash": "lu|understand_the_dynamics_of_gans_via_primaldual_optimization", "_bibtex": "@misc{\nlu2019understand,\ntitle={Understand the dynamics of {GAN}s via Primal-Dual Optimization},\nauthor={Songtao Lu and Rahul Singh and Xiangyi Chen and Yongxin Chen and Mingyi Hong},\nyear={2019},\nurl={https://openreview.net/forum?id=rylIy3R9K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper987/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311705555, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rylIy3R9K7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper987/Authors", "ICLR.cc/2019/Conference/Paper987/Reviewers", "ICLR.cc/2019/Conference/Paper987/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper987/Authors", "ICLR.cc/2019/Conference/Paper987/Reviewers", "ICLR.cc/2019/Conference/Paper987/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311705555}}}], "count": 12}