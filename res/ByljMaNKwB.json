{"notes": [{"id": "ByljMaNKwB", "original": "HyxajU38DB", "number": 425, "cdate": 1569438995159, "ddate": null, "tcdate": 1569438995159, "tmdate": 1577168242227, "tddate": null, "forum": "ByljMaNKwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Domain Aggregation Networks for Multi-Source Domain Adaptation", "authors": ["Junfeng Wen", "Russell Greiner", "Dale Schuurmans"], "authorids": ["junfengwen@gmail.com", "rgreiner@ualberta.ca", "daes@ualberta.ca"], "keywords": ["Domain Adaptation", "Transfer Learning", "Deep Learning"], "abstract": "In many real-world applications, we want to exploit multiple source datasets of similar tasks to learn a model for a different but related target dataset -- e.g.,  recognizing characters of a new font using a set of different fonts. While most recent research has considered ad-hoc combination rules to address this problem, we extend previous work on domain discrepancy minimization to develop a finite-sample generalization bound, and accordingly propose a theoretically justified optimization procedure. The algorithm we develop, Domain AggRegation Network (DARN), is able to effectively adjust the weight of each source domain during training to ensure relevant domains are given more importance for adaptation. We evaluate the proposed method on real-world sentiment analysis and digit recognition datasets and show that DARN can significantly outperform the state-of-the-art alternatives.", "pdf": "/pdf/45504454df5025b6b845a596acbc11429c9d3661.pdf", "paperhash": "wen|domain_aggregation_networks_for_multisource_domain_adaptation", "original_pdf": "/attachment/cb30ebb7e535a7a2cd717387955da8ed50001a07.pdf", "_bibtex": "@misc{\nwen2020domain,\ntitle={Domain Aggregation Networks for Multi-Source Domain Adaptation},\nauthor={Junfeng Wen and Russell Greiner and Dale Schuurmans},\nyear={2020},\nurl={https://openreview.net/forum?id=ByljMaNKwB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "b3LFvGfBb5", "original": null, "number": 1, "cdate": 1576798696055, "ddate": null, "tcdate": 1576798696055, "tmdate": 1576800939581, "tddate": null, "forum": "ByljMaNKwB", "replyto": "ByljMaNKwB", "invitation": "ICLR.cc/2020/Conference/Paper425/-/Decision", "content": {"decision": "Reject", "comment": "Thanks for the detailed replies to the reviewers.\nTheir score was slightly improved, this paper is still below the bar given high competition of ICLR2020.\nFor this reason, we decided not to accept this paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain Aggregation Networks for Multi-Source Domain Adaptation", "authors": ["Junfeng Wen", "Russell Greiner", "Dale Schuurmans"], "authorids": ["junfengwen@gmail.com", "rgreiner@ualberta.ca", "daes@ualberta.ca"], "keywords": ["Domain Adaptation", "Transfer Learning", "Deep Learning"], "abstract": "In many real-world applications, we want to exploit multiple source datasets of similar tasks to learn a model for a different but related target dataset -- e.g.,  recognizing characters of a new font using a set of different fonts. While most recent research has considered ad-hoc combination rules to address this problem, we extend previous work on domain discrepancy minimization to develop a finite-sample generalization bound, and accordingly propose a theoretically justified optimization procedure. The algorithm we develop, Domain AggRegation Network (DARN), is able to effectively adjust the weight of each source domain during training to ensure relevant domains are given more importance for adaptation. We evaluate the proposed method on real-world sentiment analysis and digit recognition datasets and show that DARN can significantly outperform the state-of-the-art alternatives.", "pdf": "/pdf/45504454df5025b6b845a596acbc11429c9d3661.pdf", "paperhash": "wen|domain_aggregation_networks_for_multisource_domain_adaptation", "original_pdf": "/attachment/cb30ebb7e535a7a2cd717387955da8ed50001a07.pdf", "_bibtex": "@misc{\nwen2020domain,\ntitle={Domain Aggregation Networks for Multi-Source Domain Adaptation},\nauthor={Junfeng Wen and Russell Greiner and Dale Schuurmans},\nyear={2020},\nurl={https://openreview.net/forum?id=ByljMaNKwB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "ByljMaNKwB", "replyto": "ByljMaNKwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795720583, "tmdate": 1576800271440, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper425/-/Decision"}}}, {"id": "Skxz_jDjjB", "original": null, "number": 6, "cdate": 1573776233920, "ddate": null, "tcdate": 1573776233920, "tmdate": 1573776233920, "tddate": null, "forum": "ByljMaNKwB", "replyto": "Bkg-NyGFsr", "invitation": "ICLR.cc/2020/Conference/Paper425/-/Official_Comment", "content": {"title": "Thank you", "comment": "Thank you for your additional comments! We will include and discuss the reference in the revised version."}, "signatures": ["ICLR.cc/2020/Conference/Paper425/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper425/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain Aggregation Networks for Multi-Source Domain Adaptation", "authors": ["Junfeng Wen", "Russell Greiner", "Dale Schuurmans"], "authorids": ["junfengwen@gmail.com", "rgreiner@ualberta.ca", "daes@ualberta.ca"], "keywords": ["Domain Adaptation", "Transfer Learning", "Deep Learning"], "abstract": "In many real-world applications, we want to exploit multiple source datasets of similar tasks to learn a model for a different but related target dataset -- e.g.,  recognizing characters of a new font using a set of different fonts. While most recent research has considered ad-hoc combination rules to address this problem, we extend previous work on domain discrepancy minimization to develop a finite-sample generalization bound, and accordingly propose a theoretically justified optimization procedure. The algorithm we develop, Domain AggRegation Network (DARN), is able to effectively adjust the weight of each source domain during training to ensure relevant domains are given more importance for adaptation. We evaluate the proposed method on real-world sentiment analysis and digit recognition datasets and show that DARN can significantly outperform the state-of-the-art alternatives.", "pdf": "/pdf/45504454df5025b6b845a596acbc11429c9d3661.pdf", "paperhash": "wen|domain_aggregation_networks_for_multisource_domain_adaptation", "original_pdf": "/attachment/cb30ebb7e535a7a2cd717387955da8ed50001a07.pdf", "_bibtex": "@misc{\nwen2020domain,\ntitle={Domain Aggregation Networks for Multi-Source Domain Adaptation},\nauthor={Junfeng Wen and Russell Greiner and Dale Schuurmans},\nyear={2020},\nurl={https://openreview.net/forum?id=ByljMaNKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByljMaNKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper425/Authors", "ICLR.cc/2020/Conference/Paper425/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper425/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper425/Reviewers", "ICLR.cc/2020/Conference/Paper425/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper425/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper425/Authors|ICLR.cc/2020/Conference/Paper425/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171669, "tmdate": 1576860550299, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper425/Authors", "ICLR.cc/2020/Conference/Paper425/Reviewers", "ICLR.cc/2020/Conference/Paper425/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper425/-/Official_Comment"}}}, {"id": "r1lvRFUliS", "original": null, "number": 3, "cdate": 1573050830650, "ddate": null, "tcdate": 1573050830650, "tmdate": 1573621599349, "tddate": null, "forum": "ByljMaNKwB", "replyto": "ByljMaNKwB", "invitation": "ICLR.cc/2020/Conference/Paper425/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #4", "review": "This paper proposed an aggregation algorithm (DARN) for the multi-source domain adaptation problem which is highly useful in real-world applications.\nThe proposed method is based on the theoretical extension of the single-source domain discrepancy measure proposed by Mansour et al. 2009 to the multi-source setting.\nThis paper also showed the effectiveness of the proposed method on some real-world datasets.\n\nStrengths\nThe paper introduces new technical insights to understand their bound, e.g. effective sample size.\nThe paper proposed the way to estimate coefficient, optimal \\alpha, with theoretical justification, and I think this is the biggest contribution of this paper and is interesting.\nThe proposed method is also able to be used in the regression task since it is based on the disc which can be estimated in the regression task.\n\nWeakness\nThe main theorem of this paper is an extension of existing methods, so the novelty of theoretical analysis is somewhat limited.\nA naive approach to estimate coefficient with single-source domain discrepancy measures such as [1]Mansour (2009), [2,3] Ben-David(2007, 2010), [4] Kuroki et al (2019), and W1-distance is not considered.\nExperimental results itself are fine but not complete.\n  - Although disc can be easily estimated in the regression task (differently from d_A distance which is a special case of disc), there are no experimental results of the regression task even in the synthetic data.\n  - It would be also better to show the coefficient of existing methods that have no theoretical justification.\n  - It would be better to compare with a naive approach that uses domain discrepancy between each source and target as (fixed) coefficient since this approach such as Mansour (2009), Ben-David(2007, 2010) and Kuroki et al (2019) which explicitly consider the hypothesis class has theoretical justification in the form of generalization error bound in the target domain.\n\nOverall, I like the approach of the proposed method, especially tuning coefficient during training procedure although novelty in the theoretical analysis is somewhat limited.\nSo this work has to be supported with more detailed experimental results to express the potential of this approach fully.\nFor this reason, I think it is okay but not good enough at this time.\n\n****After the authors' response****\nIncrease rating.\n\n[1] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and\nalgorithms. In COLT, 2009.\n[2] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for\ndomain adaptation. In NeurIPS, 2007.\n[3] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman\nVaughan. A theory of learning from different domains. Machine Learning, 2010.\n[4] Seiichi Kuroki, Nontawat Charoenphakdee, Han Bao, Junya Honda, Issei Sato, and Masashi Sugiyama.\nUnsupervised domain adaptation based on source-guided discrepancy. In AAAI, 2019.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper425/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper425/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain Aggregation Networks for Multi-Source Domain Adaptation", "authors": ["Junfeng Wen", "Russell Greiner", "Dale Schuurmans"], "authorids": ["junfengwen@gmail.com", "rgreiner@ualberta.ca", "daes@ualberta.ca"], "keywords": ["Domain Adaptation", "Transfer Learning", "Deep Learning"], "abstract": "In many real-world applications, we want to exploit multiple source datasets of similar tasks to learn a model for a different but related target dataset -- e.g.,  recognizing characters of a new font using a set of different fonts. While most recent research has considered ad-hoc combination rules to address this problem, we extend previous work on domain discrepancy minimization to develop a finite-sample generalization bound, and accordingly propose a theoretically justified optimization procedure. The algorithm we develop, Domain AggRegation Network (DARN), is able to effectively adjust the weight of each source domain during training to ensure relevant domains are given more importance for adaptation. We evaluate the proposed method on real-world sentiment analysis and digit recognition datasets and show that DARN can significantly outperform the state-of-the-art alternatives.", "pdf": "/pdf/45504454df5025b6b845a596acbc11429c9d3661.pdf", "paperhash": "wen|domain_aggregation_networks_for_multisource_domain_adaptation", "original_pdf": "/attachment/cb30ebb7e535a7a2cd717387955da8ed50001a07.pdf", "_bibtex": "@misc{\nwen2020domain,\ntitle={Domain Aggregation Networks for Multi-Source Domain Adaptation},\nauthor={Junfeng Wen and Russell Greiner and Dale Schuurmans},\nyear={2020},\nurl={https://openreview.net/forum?id=ByljMaNKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ByljMaNKwB", "replyto": "ByljMaNKwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper425/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper425/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574748762117, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper425/Reviewers"], "noninvitees": [], "tcdate": 1570237752325, "tmdate": 1574748762130, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper425/-/Official_Review"}}}, {"id": "Bkg-NyGFsr", "original": null, "number": 5, "cdate": 1573621545218, "ddate": null, "tcdate": 1573621545218, "tmdate": 1573621545218, "tddate": null, "forum": "ByljMaNKwB", "replyto": "HklAo1JujB", "invitation": "ICLR.cc/2020/Conference/Paper425/-/Official_Comment", "content": {"title": "Response to authors", "comment": "Thank you for your careful response and additional experimental results!\nMost of my concerns are solved in the revised paper and by the authors' response.\n\nSo, I decided to increase my score, but I also have to consider the situation that I make a quick assessment.\n\nComments:\nObviously, \\eta_{\\mathcal{H}} is impossible to estimate in the unsupervised domain adaptation.\nSo many researches assume that it is small, and also in this work.\nSince there might exist some concerns on this term, I think it is better to add research by [1] Ben-David(2010b) which showed that the assumption of small \\eta_{\\mathcal{H} is necessary for domain adaptation to succeed as a reference.\nIt means that we cannot guarantee the success of domain adaptation algorithms always \"only with small discrepancy measure\".\nHowever, I believe that it is quite natural to assume \\eta_{\\mathcal{H} is small in real-world applications and that is why we use domain adaptation.\nIn other words, if we do not have any prior knowledge between the source and the target domains, we might not try to adapt using specific domains as a source.)\n\n[1] Shai Ben-David, Tyler Lu, Teresa Luu, and D\u00e1vid P\u00e1l. Impossibility theorems for domain adaptation.\nIn AISTATS, 2010b."}, "signatures": ["ICLR.cc/2020/Conference/Paper425/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper425/AnonReviewer4", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain Aggregation Networks for Multi-Source Domain Adaptation", "authors": ["Junfeng Wen", "Russell Greiner", "Dale Schuurmans"], "authorids": ["junfengwen@gmail.com", "rgreiner@ualberta.ca", "daes@ualberta.ca"], "keywords": ["Domain Adaptation", "Transfer Learning", "Deep Learning"], "abstract": "In many real-world applications, we want to exploit multiple source datasets of similar tasks to learn a model for a different but related target dataset -- e.g.,  recognizing characters of a new font using a set of different fonts. While most recent research has considered ad-hoc combination rules to address this problem, we extend previous work on domain discrepancy minimization to develop a finite-sample generalization bound, and accordingly propose a theoretically justified optimization procedure. The algorithm we develop, Domain AggRegation Network (DARN), is able to effectively adjust the weight of each source domain during training to ensure relevant domains are given more importance for adaptation. We evaluate the proposed method on real-world sentiment analysis and digit recognition datasets and show that DARN can significantly outperform the state-of-the-art alternatives.", "pdf": "/pdf/45504454df5025b6b845a596acbc11429c9d3661.pdf", "paperhash": "wen|domain_aggregation_networks_for_multisource_domain_adaptation", "original_pdf": "/attachment/cb30ebb7e535a7a2cd717387955da8ed50001a07.pdf", "_bibtex": "@misc{\nwen2020domain,\ntitle={Domain Aggregation Networks for Multi-Source Domain Adaptation},\nauthor={Junfeng Wen and Russell Greiner and Dale Schuurmans},\nyear={2020},\nurl={https://openreview.net/forum?id=ByljMaNKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByljMaNKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper425/Authors", "ICLR.cc/2020/Conference/Paper425/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper425/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper425/Reviewers", "ICLR.cc/2020/Conference/Paper425/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper425/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper425/Authors|ICLR.cc/2020/Conference/Paper425/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171669, "tmdate": 1576860550299, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper425/Authors", "ICLR.cc/2020/Conference/Paper425/Reviewers", "ICLR.cc/2020/Conference/Paper425/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper425/-/Official_Comment"}}}, {"id": "HklAo1JujB", "original": null, "number": 4, "cdate": 1573543846216, "ddate": null, "tcdate": 1573543846216, "tmdate": 1573543846216, "tddate": null, "forum": "ByljMaNKwB", "replyto": "r1lvRFUliS", "invitation": "ICLR.cc/2020/Conference/Paper425/-/Official_Comment", "content": {"title": "Additional Explanation and Results", "comment": "Thank you for your insightful feedback and suggestions! To address your comments:\n\n- Following your suggestion, we have added one experiment on a synthetic regression task in Appendix C. Here, we show that our method can learn meaningful models for target domains, and also learn the source domain weights in a way that selects only relevant source domains for training. More importantly, we can learn the model and the domain weights simultaneously, unlike many existing works that use two-stage learning (learn the weights then the model).\n\n- We also followed your suggestion and show the domain weights of MDMN on the Amazon dataset for comparison (see the new Section 5.4). As you mentioned, our weights are theoretically justified while theirs are only heuristically computed. We see that the weights provided by MDMN are not very stable, changing from one source domain to another drastically during training. This instability makes their weights difficult to interpret.\n\n- W.r.t. the naive method of using (fixed) coefficients. Note that whichever domain discrepancy is used, it has to depend on the data representation. For complicated models like deep neural networks, the feature representations (hidden layers) are changing constantly during training and arguably there is no \"right\" way to compute *fixed* coefficients/weights based on ever-changing representations. Computing the coefficients directly from the images is extremely difficult, if not impossible, because calculating the domain discrepancy using such high-dimensional data is not feasible. \nNote that MDMN DOES use W1-distance to compute domain weights, and our comparison in Section 5.4 shows that it is not very stable, as mentioned above. To summarize, there is no easy way to compute meaningful fixed coefficients and our method is indeed suitable for dynamic representations during neural network training.\n\nWe hope our explanation and additional results address and resolve your concerns. If you have any other comments, we are happy to have further discussions. Thank you!"}, "signatures": ["ICLR.cc/2020/Conference/Paper425/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper425/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain Aggregation Networks for Multi-Source Domain Adaptation", "authors": ["Junfeng Wen", "Russell Greiner", "Dale Schuurmans"], "authorids": ["junfengwen@gmail.com", "rgreiner@ualberta.ca", "daes@ualberta.ca"], "keywords": ["Domain Adaptation", "Transfer Learning", "Deep Learning"], "abstract": "In many real-world applications, we want to exploit multiple source datasets of similar tasks to learn a model for a different but related target dataset -- e.g.,  recognizing characters of a new font using a set of different fonts. While most recent research has considered ad-hoc combination rules to address this problem, we extend previous work on domain discrepancy minimization to develop a finite-sample generalization bound, and accordingly propose a theoretically justified optimization procedure. The algorithm we develop, Domain AggRegation Network (DARN), is able to effectively adjust the weight of each source domain during training to ensure relevant domains are given more importance for adaptation. We evaluate the proposed method on real-world sentiment analysis and digit recognition datasets and show that DARN can significantly outperform the state-of-the-art alternatives.", "pdf": "/pdf/45504454df5025b6b845a596acbc11429c9d3661.pdf", "paperhash": "wen|domain_aggregation_networks_for_multisource_domain_adaptation", "original_pdf": "/attachment/cb30ebb7e535a7a2cd717387955da8ed50001a07.pdf", "_bibtex": "@misc{\nwen2020domain,\ntitle={Domain Aggregation Networks for Multi-Source Domain Adaptation},\nauthor={Junfeng Wen and Russell Greiner and Dale Schuurmans},\nyear={2020},\nurl={https://openreview.net/forum?id=ByljMaNKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByljMaNKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper425/Authors", "ICLR.cc/2020/Conference/Paper425/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper425/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper425/Reviewers", "ICLR.cc/2020/Conference/Paper425/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper425/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper425/Authors|ICLR.cc/2020/Conference/Paper425/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171669, "tmdate": 1576860550299, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper425/Authors", "ICLR.cc/2020/Conference/Paper425/Reviewers", "ICLR.cc/2020/Conference/Paper425/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper425/-/Official_Comment"}}}, {"id": "HJeT3CRDoH", "original": null, "number": 3, "cdate": 1573543604822, "ddate": null, "tcdate": 1573543604822, "tmdate": 1573543604822, "tddate": null, "forum": "ByljMaNKwB", "replyto": "H1l3mdg7qS", "invitation": "ICLR.cc/2020/Conference/Paper425/-/Official_Comment", "content": {"title": "Additional Explanation and Results", "comment": "Thank you for your feedback and additional references! To address the comments:\n\n- As we explained in Thm.1, \\eta_{\\mathcal{H}} is a constant measuring how well the model family \\mathcal{H} can fit the true models from both domains. Estimating this term requires *labelled* target samples, which is usually unavailable in domain adaptation. However, when we have access to a handful of labelled target data, we can certainly estimate this term and perform model selection (e.g., choosing neural network models \\mathcal{H}) better, meaning that we can find better values for alphas, and so achieve even better adaptation performance.\n\n- Yes, there are many methods that conduct single-source to single-target adaption in the literature. However, our main focus is *multi-source* to single-target adaptation. This is why our comparisons focus on similar methods, that also use multiple sources at the same time. They are generally more competitive than single-source methods. Following Reviewer #1's suggestions, we added one additional state-of-the-art competitor, Moment Matching for Multi-Source Domain Adaptation (M3SDA) (Peng et al., 2019), to our experiments. Moreover, we also add the challenging Office-Home dataset as you suggested[D]; results can be found in the new Section 5.3. The results with the new competitor, and on both the earlier datasets and the new one, show that our method outperforms the competition, especially on the Office-Home dataset, in which we achieve state-of-the-art performance.\n\nIf you have any comments or concerns, feel free to leave a message here and we can discuss further. Thank you!"}, "signatures": ["ICLR.cc/2020/Conference/Paper425/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper425/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain Aggregation Networks for Multi-Source Domain Adaptation", "authors": ["Junfeng Wen", "Russell Greiner", "Dale Schuurmans"], "authorids": ["junfengwen@gmail.com", "rgreiner@ualberta.ca", "daes@ualberta.ca"], "keywords": ["Domain Adaptation", "Transfer Learning", "Deep Learning"], "abstract": "In many real-world applications, we want to exploit multiple source datasets of similar tasks to learn a model for a different but related target dataset -- e.g.,  recognizing characters of a new font using a set of different fonts. While most recent research has considered ad-hoc combination rules to address this problem, we extend previous work on domain discrepancy minimization to develop a finite-sample generalization bound, and accordingly propose a theoretically justified optimization procedure. The algorithm we develop, Domain AggRegation Network (DARN), is able to effectively adjust the weight of each source domain during training to ensure relevant domains are given more importance for adaptation. We evaluate the proposed method on real-world sentiment analysis and digit recognition datasets and show that DARN can significantly outperform the state-of-the-art alternatives.", "pdf": "/pdf/45504454df5025b6b845a596acbc11429c9d3661.pdf", "paperhash": "wen|domain_aggregation_networks_for_multisource_domain_adaptation", "original_pdf": "/attachment/cb30ebb7e535a7a2cd717387955da8ed50001a07.pdf", "_bibtex": "@misc{\nwen2020domain,\ntitle={Domain Aggregation Networks for Multi-Source Domain Adaptation},\nauthor={Junfeng Wen and Russell Greiner and Dale Schuurmans},\nyear={2020},\nurl={https://openreview.net/forum?id=ByljMaNKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByljMaNKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper425/Authors", "ICLR.cc/2020/Conference/Paper425/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper425/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper425/Reviewers", "ICLR.cc/2020/Conference/Paper425/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper425/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper425/Authors|ICLR.cc/2020/Conference/Paper425/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171669, "tmdate": 1576860550299, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper425/Authors", "ICLR.cc/2020/Conference/Paper425/Reviewers", "ICLR.cc/2020/Conference/Paper425/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper425/-/Official_Comment"}}}, {"id": "BkeUECAvsr", "original": null, "number": 2, "cdate": 1573543470226, "ddate": null, "tcdate": 1573543470226, "tmdate": 1573543470226, "tddate": null, "forum": "ByljMaNKwB", "replyto": "Hyxp9P_RuH", "invitation": "ICLR.cc/2020/Conference/Paper425/-/Official_Comment", "content": {"title": "Additional Explanation and Results", "comment": "Thank you for your thorough assessment and helpful comments! To answer your two questions:\n\n1) Upper bound\nYou are right that it would be ideal to optimize the target loss L_T(h, f_T) directly. However, this is not possible because we do not have labelled target data (i.e., f_T is unknown).  Minimizing an upper bound is arguably the only viable option *with theoretical generalization guarantee*. It is a common practice in the domain adaptation community (Mansour et al., 2009a, 2009b; Ben-David et al. 2007, 2010; Cortes and Mohri, 2011), and it is essentially the key idea of PAC learning and generalization analysis (Gy\u00f6rfi et al., 2006; Sch\u00f6lkopf et al., 2002; Vapnik, 2013). Besides, our method *directly* optimizes the upper bound without resorting to heuristics, unlike prior methods. \n\nRef:\n- Gy\u00f6rfi, L., Kohler, M., Krzyzak, A. and Walk, H., 2006. A distribution-free theory of nonparametric regression. Springer Science & Business Media.\n- Sch\u00f6lkopf, B., Smola, A.J. and Bach, F., 2002. Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press.\n- Vapnik, V., 2013. The nature of statistical learning theory. Springer science & business media.\n\n2) More experiment\nThank you for pointing out these datasets. We have conducted further experiments on the Office-Home dataset,  using the ResNet50 as the backbone architecture and changing the classification head to 65 classes. As we can see in the new Section 5.3, our method achieve state-of-the-art performance and outperform all alternatives; these results are statistically significant.\n\nIn addition, we have added one more competitive baseline (M3SDA) from the DomainNet paper you mentioned, using their public code with a few necessary adjustments for each dataset (e.g., network architecture, etc). We ensure that all methods use the same backbone architecture for a fair comparison. Again, our method outperform M3SDA in all datasets.\n\nIf you have any other comments or concerns, we are happy to provide further feedback. Thank you!"}, "signatures": ["ICLR.cc/2020/Conference/Paper425/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper425/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain Aggregation Networks for Multi-Source Domain Adaptation", "authors": ["Junfeng Wen", "Russell Greiner", "Dale Schuurmans"], "authorids": ["junfengwen@gmail.com", "rgreiner@ualberta.ca", "daes@ualberta.ca"], "keywords": ["Domain Adaptation", "Transfer Learning", "Deep Learning"], "abstract": "In many real-world applications, we want to exploit multiple source datasets of similar tasks to learn a model for a different but related target dataset -- e.g.,  recognizing characters of a new font using a set of different fonts. While most recent research has considered ad-hoc combination rules to address this problem, we extend previous work on domain discrepancy minimization to develop a finite-sample generalization bound, and accordingly propose a theoretically justified optimization procedure. The algorithm we develop, Domain AggRegation Network (DARN), is able to effectively adjust the weight of each source domain during training to ensure relevant domains are given more importance for adaptation. We evaluate the proposed method on real-world sentiment analysis and digit recognition datasets and show that DARN can significantly outperform the state-of-the-art alternatives.", "pdf": "/pdf/45504454df5025b6b845a596acbc11429c9d3661.pdf", "paperhash": "wen|domain_aggregation_networks_for_multisource_domain_adaptation", "original_pdf": "/attachment/cb30ebb7e535a7a2cd717387955da8ed50001a07.pdf", "_bibtex": "@misc{\nwen2020domain,\ntitle={Domain Aggregation Networks for Multi-Source Domain Adaptation},\nauthor={Junfeng Wen and Russell Greiner and Dale Schuurmans},\nyear={2020},\nurl={https://openreview.net/forum?id=ByljMaNKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByljMaNKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper425/Authors", "ICLR.cc/2020/Conference/Paper425/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper425/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper425/Reviewers", "ICLR.cc/2020/Conference/Paper425/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper425/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper425/Authors|ICLR.cc/2020/Conference/Paper425/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171669, "tmdate": 1576860550299, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper425/Authors", "ICLR.cc/2020/Conference/Paper425/Reviewers", "ICLR.cc/2020/Conference/Paper425/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper425/-/Official_Comment"}}}, {"id": "S1eJF60wor", "original": null, "number": 1, "cdate": 1573543287118, "ddate": null, "tcdate": 1573543287118, "tmdate": 1573543287118, "tddate": null, "forum": "ByljMaNKwB", "replyto": "ByljMaNKwB", "invitation": "ICLR.cc/2020/Conference/Paper425/-/Official_Comment", "content": {"title": "Revision", "comment": "We thank the reviewers for the constructive feedback and insightful comments. We revised our paper accordingly with additional results. The major revision includes:\n\n1. We added the Office-Home dataset and compared our method to the competing alternatives in Section 5.3. Our method achieves state-of-the-art performance on this multi-source adaptation problem.\n\n2. We added one more competing method (Moment Matching for Multi-Source Domain Adaptation, M3SDA for short) from the literature in all three experiments. We use the original authors' implementation with a few necessary adjustments (using the same neural network architecture for a fair comparison, modify the classification head to according to the number of classes, etc). We find that our method outperforms M3SDA over all three experiments.\n\n3. In Section 5.4, we compare the domain weights of our method and MDMN. We can see that the weights of MDMN are less stable and hard to interpret.\n\n4. Appendix C now demonstrates how our method works for a synthetic regression problem. Our method can simultaneously learn meaningful models for the target domains, and the domain weights, to focus on relevant source domains during training.\n\nPlease also see the individual feedback we provided for each reviewer."}, "signatures": ["ICLR.cc/2020/Conference/Paper425/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper425/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain Aggregation Networks for Multi-Source Domain Adaptation", "authors": ["Junfeng Wen", "Russell Greiner", "Dale Schuurmans"], "authorids": ["junfengwen@gmail.com", "rgreiner@ualberta.ca", "daes@ualberta.ca"], "keywords": ["Domain Adaptation", "Transfer Learning", "Deep Learning"], "abstract": "In many real-world applications, we want to exploit multiple source datasets of similar tasks to learn a model for a different but related target dataset -- e.g.,  recognizing characters of a new font using a set of different fonts. While most recent research has considered ad-hoc combination rules to address this problem, we extend previous work on domain discrepancy minimization to develop a finite-sample generalization bound, and accordingly propose a theoretically justified optimization procedure. The algorithm we develop, Domain AggRegation Network (DARN), is able to effectively adjust the weight of each source domain during training to ensure relevant domains are given more importance for adaptation. We evaluate the proposed method on real-world sentiment analysis and digit recognition datasets and show that DARN can significantly outperform the state-of-the-art alternatives.", "pdf": "/pdf/45504454df5025b6b845a596acbc11429c9d3661.pdf", "paperhash": "wen|domain_aggregation_networks_for_multisource_domain_adaptation", "original_pdf": "/attachment/cb30ebb7e535a7a2cd717387955da8ed50001a07.pdf", "_bibtex": "@misc{\nwen2020domain,\ntitle={Domain Aggregation Networks for Multi-Source Domain Adaptation},\nauthor={Junfeng Wen and Russell Greiner and Dale Schuurmans},\nyear={2020},\nurl={https://openreview.net/forum?id=ByljMaNKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByljMaNKwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper425/Authors", "ICLR.cc/2020/Conference/Paper425/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper425/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper425/Reviewers", "ICLR.cc/2020/Conference/Paper425/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper425/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper425/Authors|ICLR.cc/2020/Conference/Paper425/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171669, "tmdate": 1576860550299, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper425/Authors", "ICLR.cc/2020/Conference/Paper425/Reviewers", "ICLR.cc/2020/Conference/Paper425/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper425/-/Official_Comment"}}}, {"id": "Hyxp9P_RuH", "original": null, "number": 1, "cdate": 1570830229380, "ddate": null, "tcdate": 1570830229380, "tmdate": 1572972596794, "tddate": null, "forum": "ByljMaNKwB", "replyto": "ByljMaNKwB", "invitation": "ICLR.cc/2020/Conference/Paper425/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\n\n###Summary###\nThis paper tackles the multi-source domain adaptation by aggregate multiple source domains dynamically during the training phase. The observation is that in many real-world applications, we want to exploit multiple source datasets of similar tasks to learn a model for a different but related target datasets.\n\nFirstly, the paper derives a multiple-source domain adaptation upper-bound from single-to-single domain adaptation generalization bound, based on the theoretical work from Cortes et al (2019). The idea is similar to Zhao et al (2019), which introduces a weighted parameter \\alpha to combine the source domains together. \n\nSecondly, based on the theoretical result, the paper proposes an algorithm to minimize the upper bound of the theoretical result. The upper bound can be simplified as the quartic form (Eq. 4) and can be optimized with the Lagrangian form. Since no closed-form expression for the optimal v can be derived, the authors propose to use binary search to find it. \n\nBased on the theoretical results and the algorithm, the paper introduces Domain AggRegation Network (DARN), which contains a base network for feature extraction, h_y to minimize the task loss and h_d to evaluate the discrepancy between each source domain and target domain.  The loss is aggregation with the parameter \\alpha.\n\nFinally, the paper conduct experiments on sentimental analysis benchmark, Amazon Review and digit datasets. The paper selects MDAN, DANN, MDMN as the baselines. On the amazon review dataset, the performance of the proposed DARN model is comparable with the MDMN baseline. On the digit dataset, the model can outperform the baselines. \n\n\n### Novelty ###\n\nThe theoretical results in this paper are extended from Cortes et al (2019) and Zhao et al (2018).  Thus, the theoretical contribution of this paper is limited. \n\nThe algorithm proposed in this paper is interesting. However, the motivation of the proposed method is to minimize the upper bound, not the loss itself, i.e. L_T(h, f_T). Intuitively, when the upper bound of the loss is minimized, it will be beneficial to minimize the loss itself. But it's not guaranteed as the upper bound contains other variables, such as the number of training samples and model complexity. If the training samples and model complexity (think about the parameters in the deep models) are significantly large, the upper bound of the loss might be also very large. \n\nAs for the experimental results, the paper only provides results on the sentimental analysis results and digit datasets, which are small benchmarks. The selected baselines are not sufficient. The improvement from the baselines is also limited. \n\n\n\n###Clarity###\n\nOverall, the paper is well organized and logically clear. The images are well-presented and well-explained by the captions and the text. \n\nThe derivation of the algorithm in Sec 3.2 is logically clear and easy to follow. \n\n###Pros###\n\n1) The paper proposes a new theoretical upper-bound based on the prior works, the upper-bound and its derivation are interesting and heuristic to the domain adaptation research community. \n2) The paper is applicable to many practical scenarios since the data from the real-world application is typically collected from multiple sources.\n3) The paper is overall well-organized and well-written. The claims of the paper are verified by the experimental results.\n\n###Cons###\n\n1) The critical issue of this paper is that the algorithm is designed to minimize the upper bound. The idea is intuitive when the upper bound is small. However, the proposed upper bound in the paper involves other parameters, such as the model complexity and the number of training samples. \nIt's an intuitive idea to weight different source domains in multi-source domain adaptation. The paper derives the weight by the Lagrangian form to minimize the upper bound. While another trivial trick is to evaluate \\alpha by the domain closeness between each source domain with the target domain. \n2) The experimental results provided in this paper are weak. In the abstract and introduction,  the paper motivates the multi-source domain adaptation (MSDA) problem by arguing that the MSDA has a lot of real applications.  But the paper only provides empirical results on sentimental analysis and digit recognition.  Besides, the results on the sentimental analysis are comparable with the compared baselines. \n\nIt will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:\nDomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/\nOffice-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/\n\n3) The novelty of this paper is incremental as the theoretical results are extended from Cortes et al (2019) and Zhao et al (2018). \n\nBased on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.\nTo improve the rating, the author should explain the following questions:\n1). Why minimizing the upper bound in this scenario would help to minimize the loss on the target domain, when the upper bound can be substantially huge? How about just evaluate \\alpha by the closeness of the source domain with the target domain?\n2). In the introduction, the paper motivates the multi-source domain adaptation (MSDA) problem by arguing that the MSDA has a lot of real applications. While the experiments are only performed on sentimental analysis and digit recognition. How about evaluating the proposed methods on real image recognition such as DomainNet or Office-Home? \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper425/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper425/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain Aggregation Networks for Multi-Source Domain Adaptation", "authors": ["Junfeng Wen", "Russell Greiner", "Dale Schuurmans"], "authorids": ["junfengwen@gmail.com", "rgreiner@ualberta.ca", "daes@ualberta.ca"], "keywords": ["Domain Adaptation", "Transfer Learning", "Deep Learning"], "abstract": "In many real-world applications, we want to exploit multiple source datasets of similar tasks to learn a model for a different but related target dataset -- e.g.,  recognizing characters of a new font using a set of different fonts. While most recent research has considered ad-hoc combination rules to address this problem, we extend previous work on domain discrepancy minimization to develop a finite-sample generalization bound, and accordingly propose a theoretically justified optimization procedure. The algorithm we develop, Domain AggRegation Network (DARN), is able to effectively adjust the weight of each source domain during training to ensure relevant domains are given more importance for adaptation. We evaluate the proposed method on real-world sentiment analysis and digit recognition datasets and show that DARN can significantly outperform the state-of-the-art alternatives.", "pdf": "/pdf/45504454df5025b6b845a596acbc11429c9d3661.pdf", "paperhash": "wen|domain_aggregation_networks_for_multisource_domain_adaptation", "original_pdf": "/attachment/cb30ebb7e535a7a2cd717387955da8ed50001a07.pdf", "_bibtex": "@misc{\nwen2020domain,\ntitle={Domain Aggregation Networks for Multi-Source Domain Adaptation},\nauthor={Junfeng Wen and Russell Greiner and Dale Schuurmans},\nyear={2020},\nurl={https://openreview.net/forum?id=ByljMaNKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ByljMaNKwB", "replyto": "ByljMaNKwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper425/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper425/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574748762117, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper425/Reviewers"], "noninvitees": [], "tcdate": 1570237752325, "tmdate": 1574748762130, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper425/-/Official_Review"}}}, {"id": "H1l3mdg7qS", "original": null, "number": 2, "cdate": 1572173860049, "ddate": null, "tcdate": 1572173860049, "tmdate": 1572972596762, "tddate": null, "forum": "ByljMaNKwB", "replyto": "ByljMaNKwB", "invitation": "ICLR.cc/2020/Conference/Paper425/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper studies multi-source domain adaptation problem. First this paper proposes a new theory for this domain that extends generalized discrepancy theory to multi-source setting. After derive a new generalization bound, this paper also proposes a new method based on the theory. Evaluation on real world datasets are proposed to show the efficiency of the proposed method.\n\n+ The theory in this paper improve bounds for multi-source DA in previous paper. The new bound provides new insight and helps the design of algorithm.\n+ This paper proposes elegant method to tackle new terms in the loss function and gives its complexity analysis. \n+ The evaluation results show that the algorithm is efficient.\n\nComments:\n- The main contribution of the proposed theory is the alpha term. Is \\eta_{\\mathcal{H}} is a better estimation that previous adaptability term in multi-source DA? What is the role of \\eta_{\\mathcal{H}} in the algorithm design? How to control it in empirical algorithm?\n- The evaluation of the proposed method is not complete. Some baseline DA methods [A, B] and datasets [C, D] are not considered. \n\n[A] S. Sankaranarayanan, Y. Balaji, C. D. Castillo, and R. Chellappa. Generate to adapt: Aligning domains using generative adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. \n[B] Saito, Kuniaki, et al. \"Maximum classifier discrepancy for unsupervised domain adaptation.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.\n[C] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual category models to new domains. In European Conference on Computer Vision (ECCV), 2010. \n[D]H.Venkateswara, J.Eusebio, S.Chakraborty, and S.Panchanathan. Deep hashing network for unsupervised domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper425/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper425/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain Aggregation Networks for Multi-Source Domain Adaptation", "authors": ["Junfeng Wen", "Russell Greiner", "Dale Schuurmans"], "authorids": ["junfengwen@gmail.com", "rgreiner@ualberta.ca", "daes@ualberta.ca"], "keywords": ["Domain Adaptation", "Transfer Learning", "Deep Learning"], "abstract": "In many real-world applications, we want to exploit multiple source datasets of similar tasks to learn a model for a different but related target dataset -- e.g.,  recognizing characters of a new font using a set of different fonts. While most recent research has considered ad-hoc combination rules to address this problem, we extend previous work on domain discrepancy minimization to develop a finite-sample generalization bound, and accordingly propose a theoretically justified optimization procedure. The algorithm we develop, Domain AggRegation Network (DARN), is able to effectively adjust the weight of each source domain during training to ensure relevant domains are given more importance for adaptation. We evaluate the proposed method on real-world sentiment analysis and digit recognition datasets and show that DARN can significantly outperform the state-of-the-art alternatives.", "pdf": "/pdf/45504454df5025b6b845a596acbc11429c9d3661.pdf", "paperhash": "wen|domain_aggregation_networks_for_multisource_domain_adaptation", "original_pdf": "/attachment/cb30ebb7e535a7a2cd717387955da8ed50001a07.pdf", "_bibtex": "@misc{\nwen2020domain,\ntitle={Domain Aggregation Networks for Multi-Source Domain Adaptation},\nauthor={Junfeng Wen and Russell Greiner and Dale Schuurmans},\nyear={2020},\nurl={https://openreview.net/forum?id=ByljMaNKwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ByljMaNKwB", "replyto": "ByljMaNKwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper425/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper425/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574748762117, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper425/Reviewers"], "noninvitees": [], "tcdate": 1570237752325, "tmdate": 1574748762130, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper425/-/Official_Review"}}}], "count": 11}