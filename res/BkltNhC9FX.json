{"notes": [{"id": "BkltNhC9FX", "original": "r1xhUNY5tX", "number": 1473, "cdate": 1538087985481, "ddate": null, "tcdate": 1538087985481, "tmdate": 1552676461249, "tddate": null, "forum": "BkltNhC9FX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Posterior Attention Models for Sequence to Sequence Learning", "abstract": "Modern neural architectures critically rely on attention for mapping structured inputs to sequences. In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.\nWe present an alternative architecture called  Posterior Attention Models that after a principled factorization of the full joint distribution of the attention and output variables, proposes two major changes.  First, the position where attention is marginalized is changed from the input to the output. Second, the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output. Empirically on five translation and two morphological inflection tasks the proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models.", "keywords": ["posterior inference", "attention", "seq2seq learning", "translation"], "authorids": ["sshankar@umass.edu", "sunita@iitb.ac.in"], "authors": ["Shiv Shankar", "Sunita Sarawagi"], "TL;DR": "Computing attention based on posterior distribution leads to more meaningful attention and better performance", "pdf": "/pdf/8d8546e36cc0abf49cd85fa8d7eb4dde967fb34e.pdf", "paperhash": "shankar|posterior_attention_models_for_sequence_to_sequence_learning", "_bibtex": "@inproceedings{\nshankar2018posterior,\ntitle={Posterior Attention Models for Sequence to Sequence Learning},\nauthor={Shiv Shankar and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkltNhC9FX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Hkl0IAVxgN", "original": null, "number": 1, "cdate": 1544732245527, "ddate": null, "tcdate": 1544732245527, "tmdate": 1545354482404, "tddate": null, "forum": "BkltNhC9FX", "replyto": "BkltNhC9FX", "invitation": "ICLR.cc/2019/Conference/-/Paper1473/Meta_Review", "content": {"metareview": "The reviewers of this paper agreed that it has done a stellar job of presenting a novel and principled approach to attention as a latent variable, providing a new and sound set of inference techniques to this end. This builds on top of a discussion of the limitations of existing deterministic approaches to attention, and frames the contribution well in relation to other recurrent and stochastic approaches to attention. While there are a few issues with clarity surrounding some aspects of the proposed method, which the authors are encouraged to fine-tune in their final version, paying careful attention to the review comments, this paper is more or less ready for publication with a few tweaks. It makes a clear, significant, and well-evaluate contribution to the field of attention models in sequence to sequence architectures, and will be of great interest to many attendees at ICLR.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "One of the better papers at the conference"}, "signatures": ["ICLR.cc/2019/Conference/Paper1473/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1473/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Posterior Attention Models for Sequence to Sequence Learning", "abstract": "Modern neural architectures critically rely on attention for mapping structured inputs to sequences. In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.\nWe present an alternative architecture called  Posterior Attention Models that after a principled factorization of the full joint distribution of the attention and output variables, proposes two major changes.  First, the position where attention is marginalized is changed from the input to the output. Second, the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output. Empirically on five translation and two morphological inflection tasks the proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models.", "keywords": ["posterior inference", "attention", "seq2seq learning", "translation"], "authorids": ["sshankar@umass.edu", "sunita@iitb.ac.in"], "authors": ["Shiv Shankar", "Sunita Sarawagi"], "TL;DR": "Computing attention based on posterior distribution leads to more meaningful attention and better performance", "pdf": "/pdf/8d8546e36cc0abf49cd85fa8d7eb4dde967fb34e.pdf", "paperhash": "shankar|posterior_attention_models_for_sequence_to_sequence_learning", "_bibtex": "@inproceedings{\nshankar2018posterior,\ntitle={Posterior Attention Models for Sequence to Sequence Learning},\nauthor={Shiv Shankar and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkltNhC9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1473/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352828496, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkltNhC9FX", "replyto": "BkltNhC9FX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1473/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1473/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1473/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352828496}}}, {"id": "HkeNhpWNyE", "original": null, "number": 7, "cdate": 1543933356116, "ddate": null, "tcdate": 1543933356116, "tmdate": 1543933356116, "tddate": null, "forum": "BkltNhC9FX", "replyto": "H1lZgtGmyN", "invitation": "ICLR.cc/2019/Conference/-/Paper1473/Official_Comment", "content": {"title": " ", "comment": "Thanks for the suggestion. We will take this into account and contextualize better in the next draft."}, "signatures": ["ICLR.cc/2019/Conference/Paper1473/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1473/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Posterior Attention Models for Sequence to Sequence Learning", "abstract": "Modern neural architectures critically rely on attention for mapping structured inputs to sequences. In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.\nWe present an alternative architecture called  Posterior Attention Models that after a principled factorization of the full joint distribution of the attention and output variables, proposes two major changes.  First, the position where attention is marginalized is changed from the input to the output. Second, the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output. Empirically on five translation and two morphological inflection tasks the proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models.", "keywords": ["posterior inference", "attention", "seq2seq learning", "translation"], "authorids": ["sshankar@umass.edu", "sunita@iitb.ac.in"], "authors": ["Shiv Shankar", "Sunita Sarawagi"], "TL;DR": "Computing attention based on posterior distribution leads to more meaningful attention and better performance", "pdf": "/pdf/8d8546e36cc0abf49cd85fa8d7eb4dde967fb34e.pdf", "paperhash": "shankar|posterior_attention_models_for_sequence_to_sequence_learning", "_bibtex": "@inproceedings{\nshankar2018posterior,\ntitle={Posterior Attention Models for Sequence to Sequence Learning},\nauthor={Shiv Shankar and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkltNhC9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1473/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607111, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkltNhC9FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference/Paper1473/Reviewers", "ICLR.cc/2019/Conference/Paper1473/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1473/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1473/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1473/Authors|ICLR.cc/2019/Conference/Paper1473/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1473/Reviewers", "ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference/Paper1473/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607111}}}, {"id": "H1lZgtGmyN", "original": null, "number": 6, "cdate": 1543870696739, "ddate": null, "tcdate": 1543870696739, "tmdate": 1543870696739, "tddate": null, "forum": "BkltNhC9FX", "replyto": "B1l0ox750X", "invitation": "ICLR.cc/2019/Conference/-/Paper1473/Official_Comment", "content": {"title": "Thanks for your response", "comment": "I thank the authors for improving the clarity of the model derivation and updating the paper to mention related work and alternative derivations. I agree that the author's formulation provides novel and interesting insights. However, I would just like the final version of the paper to be more explicit - preferable in both the introduction and model derivation - about the relation of their models to the latent/hard attention models that have been discussed here. Just mentioning these papers in the related work section is not sufficient to fully contextualize this work (as was asked for by the other reviewers and commenters as well). Mentioning that these models are essentially neural generalizations of the classical IBM alignment models (Brown et al., 1993) is also helpful for contextualization. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1473/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1473/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1473/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Posterior Attention Models for Sequence to Sequence Learning", "abstract": "Modern neural architectures critically rely on attention for mapping structured inputs to sequences. In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.\nWe present an alternative architecture called  Posterior Attention Models that after a principled factorization of the full joint distribution of the attention and output variables, proposes two major changes.  First, the position where attention is marginalized is changed from the input to the output. Second, the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output. Empirically on five translation and two morphological inflection tasks the proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models.", "keywords": ["posterior inference", "attention", "seq2seq learning", "translation"], "authorids": ["sshankar@umass.edu", "sunita@iitb.ac.in"], "authors": ["Shiv Shankar", "Sunita Sarawagi"], "TL;DR": "Computing attention based on posterior distribution leads to more meaningful attention and better performance", "pdf": "/pdf/8d8546e36cc0abf49cd85fa8d7eb4dde967fb34e.pdf", "paperhash": "shankar|posterior_attention_models_for_sequence_to_sequence_learning", "_bibtex": "@inproceedings{\nshankar2018posterior,\ntitle={Posterior Attention Models for Sequence to Sequence Learning},\nauthor={Shiv Shankar and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkltNhC9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1473/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607111, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkltNhC9FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference/Paper1473/Reviewers", "ICLR.cc/2019/Conference/Paper1473/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1473/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1473/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1473/Authors|ICLR.cc/2019/Conference/Paper1473/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1473/Reviewers", "ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference/Paper1473/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607111}}}, {"id": "HkeaY7QqR7", "original": null, "number": 4, "cdate": 1543283588565, "ddate": null, "tcdate": 1543283588565, "tmdate": 1543819869465, "tddate": null, "forum": "BkltNhC9FX", "replyto": "HklfYhUe2m", "invitation": "ICLR.cc/2019/Conference/-/Paper1473/Official_Comment", "content": {"title": " Author response to reviewer", "comment": "We thank the reviewer for their feedback.\nWe have rewritten the derivation of our factorization and made the assumptions clearer in Section 2.2 .\nSection 2.2.1 has also been revised describing the different variants and their intuition, deriving them all from Eqn 4.\nWe have also fixed some notational discrepancies as pointed out by the reviewer for which we are thankful.\n\nQA\n1)\nWe have rewritten that section, but the simplification comes about because of the Markovian assumption that P(a_t|a_{<t}) = P(a_t|a_{t-1}).  This makes \\sum_{a_{t-1}} P(a_t|a_{<t})P(a_{<t}|y_{<t})  =  \\sum_{a_{t-1}} P(a_t|a_{<t}) P(a_{t-1}|y_{<t}). \n\n\n2)\nThe Taylor trick was used by [1] to simplify the expectation computation. Essentially if the average value of a function is computed at different points, one can compute the Taylor expansion of the function at average of the points leaving only second order terms.\n\n\\Sigma f(x_i) = \\Sigma f( xm + x_i - xm) = \\Sigma [ f(xm) + f\u2019(xm)(x_i - xm) + second order terms ] = \\Sigma f(xm) + df(xm)\\Sigma(x_i - xm) + second order =  \\Sigma f(xm) + df(xm)*0 + second order \\approx \\Sigma f(xm)\n\n3)\ns_t is the decoder state after feeding in output y_{t-1} and attention at step {t-1}. Like in standard seq2seq literature, we rely on the decoding RNN state to capture the dependence on history of output tokens. Under the assumption that y_t depends directly on attention 'a' at t and previous tokens, we use the decoder state s_t and the encoder state x_{a}. Indeed as pointed 'j' was a typo.\n\n4)\nThe main difference between the prior-joint and postr-joint model is which attention gets propagated further down. The prior-joint model behaves analogously to the standard soft-attention in ignoring any interaction between output and attention. In fact, it is a version of an IBM model 1. We have expanded on this in Section3 paragraph 7 and Section4 paragraph 1\n\n[1] Xu et al; Show, attend and tell: Neural image caption generation with visual attention , 2015\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1473/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1473/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Posterior Attention Models for Sequence to Sequence Learning", "abstract": "Modern neural architectures critically rely on attention for mapping structured inputs to sequences. In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.\nWe present an alternative architecture called  Posterior Attention Models that after a principled factorization of the full joint distribution of the attention and output variables, proposes two major changes.  First, the position where attention is marginalized is changed from the input to the output. Second, the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output. Empirically on five translation and two morphological inflection tasks the proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models.", "keywords": ["posterior inference", "attention", "seq2seq learning", "translation"], "authorids": ["sshankar@umass.edu", "sunita@iitb.ac.in"], "authors": ["Shiv Shankar", "Sunita Sarawagi"], "TL;DR": "Computing attention based on posterior distribution leads to more meaningful attention and better performance", "pdf": "/pdf/8d8546e36cc0abf49cd85fa8d7eb4dde967fb34e.pdf", "paperhash": "shankar|posterior_attention_models_for_sequence_to_sequence_learning", "_bibtex": "@inproceedings{\nshankar2018posterior,\ntitle={Posterior Attention Models for Sequence to Sequence Learning},\nauthor={Shiv Shankar and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkltNhC9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1473/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607111, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkltNhC9FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference/Paper1473/Reviewers", "ICLR.cc/2019/Conference/Paper1473/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1473/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1473/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1473/Authors|ICLR.cc/2019/Conference/Paper1473/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1473/Reviewers", "ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference/Paper1473/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607111}}}, {"id": "BJe8NreK2m", "original": null, "number": 2, "cdate": 1541109037960, "ddate": null, "tcdate": 1541109037960, "tmdate": 1543600174301, "tddate": null, "forum": "BkltNhC9FX", "replyto": "BkltNhC9FX", "invitation": "ICLR.cc/2019/Conference/-/Paper1473/Official_Review", "content": {"title": "Posterior attention improves sequence to sequence learning", "review": "Originality: Existing attention models do not statistically express interactions among multiple attentions. The authors of this manuscript reformulate p(y|x) and define prior attention distribution (a_t depends on previous outputs y_<t) and posterior attention distribution (a_t depends on current output y_t as well), and essentially compute the prior attention at current position using posterior attention at the previous position. The hypothesis and derivations make statistical sense, and a couple of assumptions/approximations seem to be mild. \n\nQuality: The overall quality of this paper is technically sound. It pushs forward the development of attention models in sequence to sequence mapping.\n\nClarity: The ideas are presented well, if the readers go through it slowly or twice. However, the authors need to clarify the following issues: \nx_a is not well defined. \nIn Section 2.2, P(y) as a short form of Pr(y|x_1:m) could be problematic and confusing in interpretation of dependency over which variables.  \nPage 3: line 19 of Section 2.2.1, should s_{n-1} be s_{t-1}?\nIn Postr-Joint, Eq. (5) and others, I believe a'_{t-1} is better than a', because the former indicate it is attention for position t-1.\n\nI am a bit lost in the description of coupling energies. The two formulas for proximity biased coupling and monotonicity biased coupling are not well explained. \n\nIn addition to the above major issues, I also identified a few minors: \nsignificant find -> significant finding\nLast line of page 2: should P(y_t|y_<t, a_<n, a_n) be P(y_t|y_<t, a_<t, a_t)?\ntop-k -> top-K\na equally weighted combination -> an equally weighted combination\nSome citations are not used properly, such as last 3rd line of page 4, and brackets are forgotten in some places, etc.\nEnd of Section 3, x should be in boldface.\nnon-differentiability , -> non-differentiability,\nFull stop \".\" is missing in some places.\nLuong attention is not defined.\n\nSignificance: comparisons with an existing soft-attention model and an sparse-attention model on five machine translation datasets show that the performance of using posterior attention indeed are better than benchmark models. \n\nUpdate: I have read the authors' response. My current rating is final.\n", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1473/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Posterior Attention Models for Sequence to Sequence Learning", "abstract": "Modern neural architectures critically rely on attention for mapping structured inputs to sequences. In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.\nWe present an alternative architecture called  Posterior Attention Models that after a principled factorization of the full joint distribution of the attention and output variables, proposes two major changes.  First, the position where attention is marginalized is changed from the input to the output. Second, the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output. Empirically on five translation and two morphological inflection tasks the proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models.", "keywords": ["posterior inference", "attention", "seq2seq learning", "translation"], "authorids": ["sshankar@umass.edu", "sunita@iitb.ac.in"], "authors": ["Shiv Shankar", "Sunita Sarawagi"], "TL;DR": "Computing attention based on posterior distribution leads to more meaningful attention and better performance", "pdf": "/pdf/8d8546e36cc0abf49cd85fa8d7eb4dde967fb34e.pdf", "paperhash": "shankar|posterior_attention_models_for_sequence_to_sequence_learning", "_bibtex": "@inproceedings{\nshankar2018posterior,\ntitle={Posterior Attention Models for Sequence to Sequence Learning},\nauthor={Shiv Shankar and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkltNhC9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1473/Official_Review", "cdate": 1542234222588, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkltNhC9FX", "replyto": "BkltNhC9FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1473/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335953866, "tmdate": 1552335953866, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1473/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1l0ox750X", "original": null, "number": 2, "cdate": 1543282854207, "ddate": null, "tcdate": 1543282854207, "tmdate": 1543361809007, "tddate": null, "forum": "BkltNhC9FX", "replyto": "B1xBIw_93X", "invitation": "ICLR.cc/2019/Conference/-/Paper1473/Official_Comment", "content": {"title": "Author response to reviewer", "comment": "We thank the reviewer for the feedback.  We have discussed the papers mentioned by you and other reviewers in the Related work section, and also added new empirical comparisons.  \n\nWe are also very grateful for suggesting the alternative derivation.  We have added a discussion regarding your suggestion in Section 2.4 .  We have also simplified our derivation by explicitly stating and pulling up the\nMarkov assumption about attention dependencies earlier.\n\n\nThe prior joint model is indeed related to a neural IBM model 1, and has been used in multiple recent works as also pointed out by Yoon Kim.\n\nFrom an efficiency perspective, the various posterior attention models are only marginally slower than prior-joint which does the more compute-intensive part of calculating P(y_t) for each of the top-K attentions.  Thereafter, for tasks like translation, the coupled attention computation almost comes for \"free\". In fact, we observed no measurable difference in the average time per step between the two models\n\nMost seq2seq models rely upon attention feeding at all timesteps, and so we had not experimented with that model. We are providing some of the results of the experiment in the response here.\n Dataset      B=4  B=10\nde-en          28.8  28.6\nen-de          24.0  23.9\nen-vi            26.9  26.6\n\nThese numbers are roughly on par with soft-attention and show the importance of feeding the attention context.\n\nWe also ran some experiments with the suggestion of feeding the prior attention, which are as follows\n            B=4     B=10\nen-vi    27.3    27.0\nvi-en    25.7    25.7\n\nThese results are similar to or slightly worse than the prior-joint model. We are currently in the process of evaluating this on more tasks."}, "signatures": ["ICLR.cc/2019/Conference/Paper1473/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1473/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Posterior Attention Models for Sequence to Sequence Learning", "abstract": "Modern neural architectures critically rely on attention for mapping structured inputs to sequences. In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.\nWe present an alternative architecture called  Posterior Attention Models that after a principled factorization of the full joint distribution of the attention and output variables, proposes two major changes.  First, the position where attention is marginalized is changed from the input to the output. Second, the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output. Empirically on five translation and two morphological inflection tasks the proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models.", "keywords": ["posterior inference", "attention", "seq2seq learning", "translation"], "authorids": ["sshankar@umass.edu", "sunita@iitb.ac.in"], "authors": ["Shiv Shankar", "Sunita Sarawagi"], "TL;DR": "Computing attention based on posterior distribution leads to more meaningful attention and better performance", "pdf": "/pdf/8d8546e36cc0abf49cd85fa8d7eb4dde967fb34e.pdf", "paperhash": "shankar|posterior_attention_models_for_sequence_to_sequence_learning", "_bibtex": "@inproceedings{\nshankar2018posterior,\ntitle={Posterior Attention Models for Sequence to Sequence Learning},\nauthor={Shiv Shankar and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkltNhC9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1473/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607111, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkltNhC9FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference/Paper1473/Reviewers", "ICLR.cc/2019/Conference/Paper1473/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1473/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1473/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1473/Authors|ICLR.cc/2019/Conference/Paper1473/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1473/Reviewers", "ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference/Paper1473/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607111}}}, {"id": "ByebxyX9C7", "original": null, "number": 1, "cdate": 1543282408586, "ddate": null, "tcdate": 1543282408586, "tmdate": 1543361521982, "tddate": null, "forum": "BkltNhC9FX", "replyto": "HyezVpjwpm", "invitation": "ICLR.cc/2019/Conference/-/Paper1473/Official_Comment", "content": {"title": " ", "comment": "\n1)\nYes, we have used the straight through estimator. On our larger datasets we were not able to do full enumeration because of memory constraint.  For En-Vi we can run the exact enumeration and for that task the top-k marginalization reduced time per-step by around 50\\%  (0.354s vs 0.655s per step) and the required memory by a factor of 4 with very minor impact on BLEU\n\n2)\nWe thank you for giving pointers to related work. The reviewers also pointed similar works. We have discussed them in the Related Work section of the revised version.  Also, we have included some experimental comparisons with all of these.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1473/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1473/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Posterior Attention Models for Sequence to Sequence Learning", "abstract": "Modern neural architectures critically rely on attention for mapping structured inputs to sequences. In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.\nWe present an alternative architecture called  Posterior Attention Models that after a principled factorization of the full joint distribution of the attention and output variables, proposes two major changes.  First, the position where attention is marginalized is changed from the input to the output. Second, the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output. Empirically on five translation and two morphological inflection tasks the proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models.", "keywords": ["posterior inference", "attention", "seq2seq learning", "translation"], "authorids": ["sshankar@umass.edu", "sunita@iitb.ac.in"], "authors": ["Shiv Shankar", "Sunita Sarawagi"], "TL;DR": "Computing attention based on posterior distribution leads to more meaningful attention and better performance", "pdf": "/pdf/8d8546e36cc0abf49cd85fa8d7eb4dde967fb34e.pdf", "paperhash": "shankar|posterior_attention_models_for_sequence_to_sequence_learning", "_bibtex": "@inproceedings{\nshankar2018posterior,\ntitle={Posterior Attention Models for Sequence to Sequence Learning},\nauthor={Shiv Shankar and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkltNhC9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1473/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607111, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkltNhC9FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference/Paper1473/Reviewers", "ICLR.cc/2019/Conference/Paper1473/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1473/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1473/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1473/Authors|ICLR.cc/2019/Conference/Paper1473/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1473/Reviewers", "ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference/Paper1473/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607111}}}, {"id": "r1lWSVVj0Q", "original": null, "number": 4, "cdate": 1543353400950, "ddate": null, "tcdate": 1543353400950, "tmdate": 1543353400950, "tddate": null, "forum": "BkltNhC9FX", "replyto": "ByebxyX9C7", "invitation": "ICLR.cc/2019/Conference/-/Paper1473/Public_Comment", "content": {"comment": "Thanks for the detailed response!", "title": "thanks!"}, "signatures": ["~Yoon_Kim1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Yoon_Kim1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Posterior Attention Models for Sequence to Sequence Learning", "abstract": "Modern neural architectures critically rely on attention for mapping structured inputs to sequences. In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.\nWe present an alternative architecture called  Posterior Attention Models that after a principled factorization of the full joint distribution of the attention and output variables, proposes two major changes.  First, the position where attention is marginalized is changed from the input to the output. Second, the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output. Empirically on five translation and two morphological inflection tasks the proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models.", "keywords": ["posterior inference", "attention", "seq2seq learning", "translation"], "authorids": ["sshankar@umass.edu", "sunita@iitb.ac.in"], "authors": ["Shiv Shankar", "Sunita Sarawagi"], "TL;DR": "Computing attention based on posterior distribution leads to more meaningful attention and better performance", "pdf": "/pdf/8d8546e36cc0abf49cd85fa8d7eb4dde967fb34e.pdf", "paperhash": "shankar|posterior_attention_models_for_sequence_to_sequence_learning", "_bibtex": "@inproceedings{\nshankar2018posterior,\ntitle={Posterior Attention Models for Sequence to Sequence Learning},\nauthor={Shiv Shankar and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkltNhC9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1473/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311589126, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "BkltNhC9FX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference/Paper1473/Reviewers", "ICLR.cc/2019/Conference/Paper1473/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference/Paper1473/Reviewers", "ICLR.cc/2019/Conference/Paper1473/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311589126}}}, {"id": "r1xb9bQ9AX", "original": null, "number": 3, "cdate": 1543283080812, "ddate": null, "tcdate": 1543283080812, "tmdate": 1543330618023, "tddate": null, "forum": "BkltNhC9FX", "replyto": "BJe8NreK2m", "invitation": "ICLR.cc/2019/Conference/-/Paper1473/Official_Comment", "content": {"title": " Author response to reviewer", "comment": "We thank the reviewer for the comments.\nIn light of comments about some of the notation and description from all reviewers, we have revised the model description considerably. We have also fixed some notational inconsistencies as pointed out.\n\nWe have also revised Section 2.2.1 to better explain the formula and intuition of the coupling energies."}, "signatures": ["ICLR.cc/2019/Conference/Paper1473/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1473/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Posterior Attention Models for Sequence to Sequence Learning", "abstract": "Modern neural architectures critically rely on attention for mapping structured inputs to sequences. In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.\nWe present an alternative architecture called  Posterior Attention Models that after a principled factorization of the full joint distribution of the attention and output variables, proposes two major changes.  First, the position where attention is marginalized is changed from the input to the output. Second, the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output. Empirically on five translation and two morphological inflection tasks the proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models.", "keywords": ["posterior inference", "attention", "seq2seq learning", "translation"], "authorids": ["sshankar@umass.edu", "sunita@iitb.ac.in"], "authors": ["Shiv Shankar", "Sunita Sarawagi"], "TL;DR": "Computing attention based on posterior distribution leads to more meaningful attention and better performance", "pdf": "/pdf/8d8546e36cc0abf49cd85fa8d7eb4dde967fb34e.pdf", "paperhash": "shankar|posterior_attention_models_for_sequence_to_sequence_learning", "_bibtex": "@inproceedings{\nshankar2018posterior,\ntitle={Posterior Attention Models for Sequence to Sequence Learning},\nauthor={Shiv Shankar and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkltNhC9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1473/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607111, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkltNhC9FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference/Paper1473/Reviewers", "ICLR.cc/2019/Conference/Paper1473/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1473/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1473/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1473/Authors|ICLR.cc/2019/Conference/Paper1473/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1473/Reviewers", "ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference/Paper1473/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607111}}}, {"id": "H1lBWBQcCQ", "original": null, "number": 5, "cdate": 1543283964795, "ddate": null, "tcdate": 1543283964795, "tmdate": 1543283964795, "tddate": null, "forum": "BkltNhC9FX", "replyto": "SylcXSSt6m", "invitation": "ICLR.cc/2019/Conference/-/Paper1473/Official_Comment", "content": {"title": " ", "comment": "We have rewritten Section 2.2 of the paper, which simplifies the presentation and makes the need for posterior attention more obvious. The network architecture and connections are the same as standard soft attention model. The difference is entirely on how attention is computed."}, "signatures": ["ICLR.cc/2019/Conference/Paper1473/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1473/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Posterior Attention Models for Sequence to Sequence Learning", "abstract": "Modern neural architectures critically rely on attention for mapping structured inputs to sequences. In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.\nWe present an alternative architecture called  Posterior Attention Models that after a principled factorization of the full joint distribution of the attention and output variables, proposes two major changes.  First, the position where attention is marginalized is changed from the input to the output. Second, the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output. Empirically on five translation and two morphological inflection tasks the proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models.", "keywords": ["posterior inference", "attention", "seq2seq learning", "translation"], "authorids": ["sshankar@umass.edu", "sunita@iitb.ac.in"], "authors": ["Shiv Shankar", "Sunita Sarawagi"], "TL;DR": "Computing attention based on posterior distribution leads to more meaningful attention and better performance", "pdf": "/pdf/8d8546e36cc0abf49cd85fa8d7eb4dde967fb34e.pdf", "paperhash": "shankar|posterior_attention_models_for_sequence_to_sequence_learning", "_bibtex": "@inproceedings{\nshankar2018posterior,\ntitle={Posterior Attention Models for Sequence to Sequence Learning},\nauthor={Shiv Shankar and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkltNhC9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1473/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607111, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkltNhC9FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference/Paper1473/Reviewers", "ICLR.cc/2019/Conference/Paper1473/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1473/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1473/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1473/Authors|ICLR.cc/2019/Conference/Paper1473/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1473/Reviewers", "ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference/Paper1473/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607111}}}, {"id": "SylcXSSt6m", "original": null, "number": 3, "cdate": 1542178082109, "ddate": null, "tcdate": 1542178082109, "tmdate": 1542178082109, "tddate": null, "forum": "BkltNhC9FX", "replyto": "BkltNhC9FX", "invitation": "ICLR.cc/2019/Conference/-/Paper1473/Public_Comment", "content": {"comment": "The contribution is interesting, but besides the experimental part is a little bit too dry. The paper would immensely benefit of a more high level description and insights about the architecture proposed, as well as a graphical representation (such as a block diagram) to make the architecture understandable at a first glance.", "title": "More high level insights needed"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Posterior Attention Models for Sequence to Sequence Learning", "abstract": "Modern neural architectures critically rely on attention for mapping structured inputs to sequences. In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.\nWe present an alternative architecture called  Posterior Attention Models that after a principled factorization of the full joint distribution of the attention and output variables, proposes two major changes.  First, the position where attention is marginalized is changed from the input to the output. Second, the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output. Empirically on five translation and two morphological inflection tasks the proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models.", "keywords": ["posterior inference", "attention", "seq2seq learning", "translation"], "authorids": ["sshankar@umass.edu", "sunita@iitb.ac.in"], "authors": ["Shiv Shankar", "Sunita Sarawagi"], "TL;DR": "Computing attention based on posterior distribution leads to more meaningful attention and better performance", "pdf": "/pdf/8d8546e36cc0abf49cd85fa8d7eb4dde967fb34e.pdf", "paperhash": "shankar|posterior_attention_models_for_sequence_to_sequence_learning", "_bibtex": "@inproceedings{\nshankar2018posterior,\ntitle={Posterior Attention Models for Sequence to Sequence Learning},\nauthor={Shiv Shankar and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkltNhC9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1473/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311589126, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "BkltNhC9FX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference/Paper1473/Reviewers", "ICLR.cc/2019/Conference/Paper1473/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference/Paper1473/Reviewers", "ICLR.cc/2019/Conference/Paper1473/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311589126}}}, {"id": "HyezVpjwpm", "original": null, "number": 2, "cdate": 1542073641897, "ddate": null, "tcdate": 1542073641897, "tmdate": 1542073641897, "tddate": null, "forum": "BkltNhC9FX", "replyto": "rJlU-8qJTX", "invitation": "ICLR.cc/2019/Conference/-/Paper1473/Public_Comment", "content": {"comment": "Yes it'd be nice to see a comparison of this work to (Deng et al., 2018) which also models attention as a latent variable and has released code here: https://github.com/harvardnlp/var-attn", "title": "Empirical Comparison to (Deng et al., 2018) ?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Posterior Attention Models for Sequence to Sequence Learning", "abstract": "Modern neural architectures critically rely on attention for mapping structured inputs to sequences. In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.\nWe present an alternative architecture called  Posterior Attention Models that after a principled factorization of the full joint distribution of the attention and output variables, proposes two major changes.  First, the position where attention is marginalized is changed from the input to the output. Second, the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output. Empirically on five translation and two morphological inflection tasks the proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models.", "keywords": ["posterior inference", "attention", "seq2seq learning", "translation"], "authorids": ["sshankar@umass.edu", "sunita@iitb.ac.in"], "authors": ["Shiv Shankar", "Sunita Sarawagi"], "TL;DR": "Computing attention based on posterior distribution leads to more meaningful attention and better performance", "pdf": "/pdf/8d8546e36cc0abf49cd85fa8d7eb4dde967fb34e.pdf", "paperhash": "shankar|posterior_attention_models_for_sequence_to_sequence_learning", "_bibtex": "@inproceedings{\nshankar2018posterior,\ntitle={Posterior Attention Models for Sequence to Sequence Learning},\nauthor={Shiv Shankar and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkltNhC9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1473/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311589126, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "BkltNhC9FX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference/Paper1473/Reviewers", "ICLR.cc/2019/Conference/Paper1473/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference/Paper1473/Reviewers", "ICLR.cc/2019/Conference/Paper1473/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311589126}}}, {"id": "rJlU-8qJTX", "original": null, "number": 1, "cdate": 1541543421809, "ddate": null, "tcdate": 1541543421809, "tmdate": 1541543421809, "tddate": null, "forum": "BkltNhC9FX", "replyto": "BkltNhC9FX", "invitation": "ICLR.cc/2019/Conference/-/Paper1473/Public_Comment", "content": {"comment": "Hi there, thanks for a very nice paper. It is great to see that posterior inference substantially increases alignment accuracy! I also liked the application of the model across a diverse range of languages/tasks.\n\nI had one quick question, and one comment:\n\nQuestion: \n- How do you differentiate through the top-K approximation? Do you use the straight through estimator? How much faster was top K vs actually enumerating?\n\nComment:\n- There are several recent works that have also formalized attention as a latent variable and have exactly/approximately optimized the log marginal likelihood. It would be great to see this work put in context of existing work!\n\nWu et al. Hard Non-Monotnic Attention for Character-Level Transduction. EMNLP 2018\nShankar et al. Surprisingly Easy Hard-Attention for Sequence to Sequence Learning. EMNLP 2018.\nDeng et al. Latent Alignment and Variational Attention. NIPS 2018.", "title": "one question/comment"}, "signatures": ["~Yoon_Kim1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Yoon_Kim1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Posterior Attention Models for Sequence to Sequence Learning", "abstract": "Modern neural architectures critically rely on attention for mapping structured inputs to sequences. In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.\nWe present an alternative architecture called  Posterior Attention Models that after a principled factorization of the full joint distribution of the attention and output variables, proposes two major changes.  First, the position where attention is marginalized is changed from the input to the output. Second, the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output. Empirically on five translation and two morphological inflection tasks the proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models.", "keywords": ["posterior inference", "attention", "seq2seq learning", "translation"], "authorids": ["sshankar@umass.edu", "sunita@iitb.ac.in"], "authors": ["Shiv Shankar", "Sunita Sarawagi"], "TL;DR": "Computing attention based on posterior distribution leads to more meaningful attention and better performance", "pdf": "/pdf/8d8546e36cc0abf49cd85fa8d7eb4dde967fb34e.pdf", "paperhash": "shankar|posterior_attention_models_for_sequence_to_sequence_learning", "_bibtex": "@inproceedings{\nshankar2018posterior,\ntitle={Posterior Attention Models for Sequence to Sequence Learning},\nauthor={Shiv Shankar and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkltNhC9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1473/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311589126, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "BkltNhC9FX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference/Paper1473/Reviewers", "ICLR.cc/2019/Conference/Paper1473/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1473/Authors", "ICLR.cc/2019/Conference/Paper1473/Reviewers", "ICLR.cc/2019/Conference/Paper1473/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311589126}}}, {"id": "B1xBIw_93X", "original": null, "number": 3, "cdate": 1541207885173, "ddate": null, "tcdate": 1541207885173, "tmdate": 1541533106802, "tddate": null, "forum": "BkltNhC9FX", "replyto": "BkltNhC9FX", "invitation": "ICLR.cc/2019/Conference/-/Paper1473/Official_Review", "content": {"title": "Very interesting contribution", "review": "This paper proposes a new sequence to sequence model where attention is treated as a latent variable, and derive novel inference procedures for this model. The approach obtains significant improvements in machine translation and morphological inflection generation tasks. An approximation is also used to make hard attention more efficient by reducing the number of softmaxes that have to be computed.  \n\nStrengths:\n- Novel, principled sequence to sequence model.\n- Strong experimental results in machine translation and morphological inflection.\nWeaknesses:\n- Connections can be made with previous closely related architectures.\n- Further ablation experiments could be included. \n\nThe derivation of the model would be more clear if it is first derived without attention feeding: The assumption that output is dependent only on the current attention variable is then valid. The Markov assumption on the attention variable should also be stated as an assumption, rather than an approximation: Given that assumption, as far as I can tell the (posterior) inference procedure that is derived is exact: It is indeed equivalent to the using the forward computation of the classic forward-backward algorithm for HMMs to do inference. \nThe model\u2019s overall distribution can then be defined in a somewhat different way than the authors\u2019 presentation, which I think makes more clear what the model is doing:\np(y | x) = \\sum_a \\prod_{t=1}^n p(y_t | y_{<t}, x, a_t) p(a_t | y_{<t}, x_ a_{t-1}).  \nThe equations derived in the paper for computing the prior and posterior attention is then just a dynamic program for computing this distribution, and is equivalent to using the forward algorithm, which in this context is:\n \\alpha_t(a) = p(a_t = a, y_{<=t}) = p(y_t | s_t, a_t =a) \\sum_{a\u2019} \\alpha_{t-1}(a\u2019) p(a_t = a | s_t, a_{t-1} = a\u2019) \n\nThe only substantial difference in the inference procedure is then that the posterior attention probability is fed into the decoder RNN, which means that the independence assumptions are not strictly valid any more, even though the structural assumptions are still encoded through the way inference is done. \n[1] recently proposed a model with a similar factorization, although that model did not feed the attention distribution, and performed EM-like inference with the forward-backward algorithm, while this model is effectively computing forward probabilities and performing inference through automatic differentiation.\n\nThe Prior-Joint variant, though its definition is not as clear as it should be, seems to be assuming that the attention distribution at each time step is independent of the previous attention (similar to the way standard soft attention is computed) - the equations then reduce to a (neural) version of IBM alignment model 1, similar to another recently proposed model [2]. These papers can be seen as concurrent work, and this paper provides important insights, but it would strengthen rather than weaken the paper to make these connections clear. \n\nThe results clearly show the advantages of the proposed approach over soft and sparse attention baselines. However, the difference in BLEU score between the variants of the prior or posterior attention models is very small across all translation datasets, so to make claims about which of the variants are better, at a minimum statistical significance testing should be done. Given that the \u201cPrior-Joint\u201d model performs competitively, is it computationally more efficient that the full model? \n\nThe main missing experiment is not doing attention feeding at all. The other experiment that is not included (as I understood it) is to compute prior and posterior attention, but feed the prior attention rather than the posterior attention. \n\nThe paper is mostly written very clearly, there are just a few typos and grammatical errors in sections 4.2 and 4.3. \n\nOverall, I really like this paper and would like to see it accepted, although I hope that a revised version would make the assumptions the model is making clearer and make connections to related models clearer. \n \n[1] Neural Hidden Markov Model for Machine Translation, Wang et al, ACL 2018. \n[2] Hard Non-Monotonic Attention for Character-Level Transduction, Wu, Shapiro and Cotterell, EMNLP 2018. ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1473/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Posterior Attention Models for Sequence to Sequence Learning", "abstract": "Modern neural architectures critically rely on attention for mapping structured inputs to sequences. In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.\nWe present an alternative architecture called  Posterior Attention Models that after a principled factorization of the full joint distribution of the attention and output variables, proposes two major changes.  First, the position where attention is marginalized is changed from the input to the output. Second, the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output. Empirically on five translation and two morphological inflection tasks the proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models.", "keywords": ["posterior inference", "attention", "seq2seq learning", "translation"], "authorids": ["sshankar@umass.edu", "sunita@iitb.ac.in"], "authors": ["Shiv Shankar", "Sunita Sarawagi"], "TL;DR": "Computing attention based on posterior distribution leads to more meaningful attention and better performance", "pdf": "/pdf/8d8546e36cc0abf49cd85fa8d7eb4dde967fb34e.pdf", "paperhash": "shankar|posterior_attention_models_for_sequence_to_sequence_learning", "_bibtex": "@inproceedings{\nshankar2018posterior,\ntitle={Posterior Attention Models for Sequence to Sequence Learning},\nauthor={Shiv Shankar and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkltNhC9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1473/Official_Review", "cdate": 1542234222588, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkltNhC9FX", "replyto": "BkltNhC9FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1473/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335953866, "tmdate": 1552335953866, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1473/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HklfYhUe2m", "original": null, "number": 1, "cdate": 1540545658467, "ddate": null, "tcdate": 1540545658467, "tmdate": 1541533106349, "tddate": null, "forum": "BkltNhC9FX", "replyto": "BkltNhC9FX", "invitation": "ICLR.cc/2019/Conference/-/Paper1473/Official_Review", "content": {"title": "This paper presents a novel posterior attention model for seq2seq problems. The PAM exploits the dependencies among attention and output variables, unlike existing attention models that only gives ad-hoc design of attention vectors. The experiments demonstrate their claimed advantages.", "review": "Pros:\n1. This work presents a novel construction of the popularly-used attention modules. It points out the problems lied in existing design that attention vectors are only computed based on parametric functions, instead of considering the interactions among each attention step and output variables. To achieve that, the authors re-write the joint distribution as a product of tractable terms at each timestamp and fully exploit the dependencies among attention and output variables across the sequence. The motivation is clear, and the proposed strategy is original and to the point. This makes the work relative solid and interesting for a publication. Furthermore, the authors propose 3 different formulation for prior attention, making the work even stronger.\n2. The technical content looks good, with each formula written clearly and with sufficient deductive steps. Figure 1 provides clear illustration on the comparison with traditional attentions and shows the advantage of the proposed model.\n3. Extensive experiments are conducted including 5 machine translation tasks as well as another morphological inflection task. These results make the statement more convincing. The authors also conducted further experiments to analyze the effectiveness, including attention entropy evaluation.\n\nCons:\n1. The rich information contained in the paper is not very well-organized. It takes some time to digest, due to some unclear or missing statements. Specifically, the computation for prior attention should be ordered in a subsection with a section name. The 3 different formulations should be first summarized and started with the same core formula as (4). In this way, it will become more clear of where does eq(6) come from or used for. Currently, this part is confusing.\n2. Many substitutions of variables take place without detailed explanation, e.g., y_{<t} with s_t, a with x_{a} in (11) etc. Could you explain before making these substitutions?\n3. As mentioned, the PAM actually computes hard attentions. It should be better to make the statement more clear by explicitly explaining eq(11) on how it assembles hard attention computation.\n\nQA:\n1. In the equation above (3) that computes prior(a_t), can you explain how P(a_{t-1}|y_{<t}) approximates P(a_{<t}|y_{<t})? What's the assumption?\n2. How is eq(5) computed using first order Taylor expansion? How to make Postr inside the probability? And where does x_a' come from?\n3. Transferring from P(y) on top of page 3 to eq(11), how do you substitute y_{<t}, a_t with s_t, x_j? Is there a typo for x_j?\n4. Can you explain how is the baseline Prior-Joint constructed? Specifically, how to compute prior using soft attention without postr?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1473/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Posterior Attention Models for Sequence to Sequence Learning", "abstract": "Modern neural architectures critically rely on attention for mapping structured inputs to sequences. In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.\nWe present an alternative architecture called  Posterior Attention Models that after a principled factorization of the full joint distribution of the attention and output variables, proposes two major changes.  First, the position where attention is marginalized is changed from the input to the output. Second, the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output. Empirically on five translation and two morphological inflection tasks the proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models.", "keywords": ["posterior inference", "attention", "seq2seq learning", "translation"], "authorids": ["sshankar@umass.edu", "sunita@iitb.ac.in"], "authors": ["Shiv Shankar", "Sunita Sarawagi"], "TL;DR": "Computing attention based on posterior distribution leads to more meaningful attention and better performance", "pdf": "/pdf/8d8546e36cc0abf49cd85fa8d7eb4dde967fb34e.pdf", "paperhash": "shankar|posterior_attention_models_for_sequence_to_sequence_learning", "_bibtex": "@inproceedings{\nshankar2018posterior,\ntitle={Posterior Attention Models for Sequence to Sequence Learning},\nauthor={Shiv Shankar and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkltNhC9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1473/Official_Review", "cdate": 1542234222588, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkltNhC9FX", "replyto": "BkltNhC9FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1473/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335953866, "tmdate": 1552335953866, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1473/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 16}