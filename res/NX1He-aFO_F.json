{"notes": [{"id": "NX1He-aFO_F", "original": "InhfCvl-sRI", "number": 3292, "cdate": 1601308365644, "ddate": null, "tcdate": 1601308365644, "tmdate": 1615834367634, "tddate": null, "forum": "NX1He-aFO_F", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Learning Value Functions in Deep Policy Gradients using Residual Variance", "authorids": ["~Yannis_Flet-Berliac1", "~reda_ouhamma1", "~odalric-ambrym_maillard1", "~Philippe_Preux1"], "authors": ["Yannis Flet-Berliac", "reda ouhamma", "odalric-ambrym maillard", "Philippe Preux"], "keywords": [], "abstract": "Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights.", "one-sentence_summary": "We introduce a method to improve the learning of the critic in the actor-critic framework.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fletberliac|learning_value_functions_in_deep_policy_gradients_using_residual_variance", "supplementary_material": "", "pdf": "/pdf/d19c38b4919b1481e2aa3972a928c866f4502b44.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nflet-berliac2021learning,\ntitle={Learning Value Functions in Deep Policy Gradients using Residual Variance},\nauthor={Yannis Flet-Berliac and reda ouhamma and odalric-ambrym maillard and Philippe Preux},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NX1He-aFO_F}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 21, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "DRuPOdTs5st", "original": null, "number": 1, "cdate": 1610040365049, "ddate": null, "tcdate": 1610040365049, "tmdate": 1610473955541, "tddate": null, "forum": "NX1He-aFO_F", "replyto": "NX1He-aFO_F", "invitation": "ICLR.cc/2021/Conference/Paper3292/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper is accepted, however, it could be much stronger by addressing the concerns below.\n\nThe theoretical analysis of the proposed methods is weak.\n* As far as I can tell, the proposition has more to do with the compatible feature assumption than their method. Furthermore the compatible feature assumption is very strong and not satisfied in any of their experiments.\n* Sec 4.2 does not provide strong support for their method. R2 points out issues with their statements about variance and the next subsection argues from an overly simplistic diagram.\n\nThe experimental results are promising, however, R3 brought up important issues in the private discussion:\n* Their implementation of SAC systematically produces results worse than reported in the original paper (they use a version of SAC with automatically tuned temperature https://arxiv.org/pdf/1812.05905.pdf); 1a) Their SAC gets average returns of 2.5k at 500k steps while the original implementation gets 3k at 500k steps; 1b) Their SAC on HalfCheetah 10k at 1M steps, original paper - 11k at 1M steps; 1c) The same applies to Humanoid, there is no improvement with respect to the original SAC;\n* Their approach degrades performance on Hopper. \n* They use non-standard hyper parameters for SAC. 0.98 instead of 0.99 for the discount and 0.01 instead of 0.005 for the soft target updates. That might be the main reason why their SAC works worse than the original implementation. \n* The authors use the hyper-parameters suggested for HalfCheetahBulletEnv for all continuous control tasks. For HalfCheetah, however, the authors of the stable-baselines repository (which this paper uses) suggest to use the hyper parameters from the original SAC paper (https://github.com/araffin/rl-baselines-zoo/blob/master/hyperparams/sac.yml#L48). Nonetheless, the results for the unmodified SAC reported in this work for HalfCheetah/Hopper/Walker/Ant are subpar to the original results, suggesting that the hyper-parameters for HalfCheetahBulletEnv are suboptimal for these tasks.\n\nGiven the simplicity of the change and the promising experimental results (with some caveats), I believe the community will find this paper interesting and will lead to followup work that can patch the theoretical gaps."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Value Functions in Deep Policy Gradients using Residual Variance", "authorids": ["~Yannis_Flet-Berliac1", "~reda_ouhamma1", "~odalric-ambrym_maillard1", "~Philippe_Preux1"], "authors": ["Yannis Flet-Berliac", "reda ouhamma", "odalric-ambrym maillard", "Philippe Preux"], "keywords": [], "abstract": "Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights.", "one-sentence_summary": "We introduce a method to improve the learning of the critic in the actor-critic framework.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fletberliac|learning_value_functions_in_deep_policy_gradients_using_residual_variance", "supplementary_material": "", "pdf": "/pdf/d19c38b4919b1481e2aa3972a928c866f4502b44.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nflet-berliac2021learning,\ntitle={Learning Value Functions in Deep Policy Gradients using Residual Variance},\nauthor={Yannis Flet-Berliac and reda ouhamma and odalric-ambrym maillard and Philippe Preux},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NX1He-aFO_F}\n}"}, "tags": [], "invitation": {"reply": {"forum": "NX1He-aFO_F", "replyto": "NX1He-aFO_F", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040365035, "tmdate": 1610473955524, "id": "ICLR.cc/2021/Conference/Paper3292/-/Decision"}}}, {"id": "D2va7mvewTd", "original": null, "number": 1, "cdate": 1603096974099, "ddate": null, "tcdate": 1603096974099, "tmdate": 1606739949426, "tddate": null, "forum": "NX1He-aFO_F", "replyto": "NX1He-aFO_F", "invitation": "ICLR.cc/2021/Conference/Paper3292/-/Official_Review", "content": {"title": "New critic loss with good theoretical and empirical motivations", "review": "This paper presents AVEC, a new critic loss for model-free actor-critic Reinforcement Learning algorithms. The AVEC loss can be used with any actor-critic algorithm, with PPO, TRPO and SAC being evaluated in the paper. The loss builds on the mean-squared-error, and adds a term that minimizes $E_s [f_{\\\\phi}(s) - \\\\hat{V}^{\\\\pi_{\\\\theta_k}}(s) ]$. The addition of that extra term is motivated by recent research on the stability of actor-critic algorithms, and the benefits obtained by the AVEC loss are empirically demonstrated in numerous environments, with AVEC+PPO, AVEC+SAC and AVEC+TRPO.\n\nQuality: the paper presents an interesting idea, that is simple but well-motivated, and leads to encouraging empirical results. Both the theoretical and empirical motivations are strong.\n\nClarity: the paper flows well and is quite clear. However, an intuition for what the added term in the AVEC loss is missing. Section 4.2 motivates the added term in a mathematical way, but a few sentences explaining what the added term does, in simple terms, may help the readers understand why AVEC is a better loss than simple MSE.\n\nOriginality: the contribution of this paper seems original. It builds on recent work, but the recent work identifies problems while this paper offers an original solution to these problems.\n\nSignificance: the fact that AVEC provides good empirical results, and can be used as the critic loss of any actor-critic Reinforcement Learning algorithm, points at the high significance of this work. Many actor-critic implementations can easily be improved by using the AVEC loss. Another positive point is that the paper discusses how to implement the AVEC loss in algorithms that fit a neural network on batches of samples. This really helps implementing the proposed loss, that contains an expectation in an expectation and is therefore not trivial to properly implement.\n\nIn general, I like this paper and recommend acceptance.\n\nA few questions/issues:\n\n- An explicit mention of the gradient of the loss, or at least a discussion of where to stop back-propagating gradients, would have been interesting. $f_{\\phi}$ appears two times in the AVEC loss, and it is unclear whether the loss contributes to gradients in $f_{\\phi}$ two times, or if the expectation over states is first computed (without computing any gradients), and then used as a constant in the rest of the evaluation of the loss.\n- As mentioned in \"clarity\", an intuition of what the added term of the AVEC loss does, especially since it is \"inserted\" in the mean-squared-error (inside the square), would help the less mathematics-savvy readers. It is not crucial to understand the paper, but the generality of the approach proposed in the paper may lead it to be used often by students, and so an intuition of why AVEC works and what it does would greatly help.\n\nAuthor response: the authors clarified my questions, so I maintain my recommendation for acceptance.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3292/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Value Functions in Deep Policy Gradients using Residual Variance", "authorids": ["~Yannis_Flet-Berliac1", "~reda_ouhamma1", "~odalric-ambrym_maillard1", "~Philippe_Preux1"], "authors": ["Yannis Flet-Berliac", "reda ouhamma", "odalric-ambrym maillard", "Philippe Preux"], "keywords": [], "abstract": "Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights.", "one-sentence_summary": "We introduce a method to improve the learning of the critic in the actor-critic framework.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fletberliac|learning_value_functions_in_deep_policy_gradients_using_residual_variance", "supplementary_material": "", "pdf": "/pdf/d19c38b4919b1481e2aa3972a928c866f4502b44.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nflet-berliac2021learning,\ntitle={Learning Value Functions in Deep Policy Gradients using Residual Variance},\nauthor={Yannis Flet-Berliac and reda ouhamma and odalric-ambrym maillard and Philippe Preux},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NX1He-aFO_F}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "NX1He-aFO_F", "replyto": "NX1He-aFO_F", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3292/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538078369, "tmdate": 1606915762641, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3292/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3292/-/Official_Review"}}}, {"id": "QY9bYuNLk8O", "original": null, "number": 3, "cdate": 1603823386469, "ddate": null, "tcdate": 1603823386469, "tmdate": 1606355303836, "tddate": null, "forum": "NX1He-aFO_F", "replyto": "NX1He-aFO_F", "invitation": "ICLR.cc/2021/Conference/Paper3292/-/Official_Review", "content": {"title": "Interesting approach but not sufficient empirical and theoretical evidence to confirm the effectiveness of the approach", "review": "The paper explores an alternative loss function for fitting critic in Reinforcement Learning. Instead of using the standard mean squared loss between critic predictions and value estimates, the authors propose to use a loss function that also incorporates a variance term. The authors dub the approach AVEC. The authors combine their approach with popular RL algorithms such as SAC and PPO and evaluated on the standard benchmarks for continuous control.\n\nAlthough the paper demonstrates interesting empirical results, I think that the current experimental evaluation has a number of flaws that prevent me from recommending this paper for acceptance. The paper provides basic motivation but it is lacking thorough theoretical investigation of the phenomena. Also the proposed loss is biased in the stochastic mini batch optimization due to the expectation under the squared term that is not addressed in the paper either. Finally, I have major concerns regarding the experimental evaluation. The set of OpenAI mujoco tasks is different from commonly used tasks in literature. In particular, Hopper and Walker2d, which are used in the vast majority of the literature, are ignored in table 1 and figure 2. This fact raises major concerns regarding generality of the approach.\n\nIn conclusion, the paper presents interesting results on some tasks for continuous control. However, the paper requires more thorough experimental evaluation to confirm the statements. Also a deeper theoretical analysis will greatly benefit this work. I strongly encourage the authors to continuous working this approach and revise the paper to improve the theoretical and empirical analysis. This paper presents a very interesting idea but in the current form it is not ready for acceptance.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3292/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Value Functions in Deep Policy Gradients using Residual Variance", "authorids": ["~Yannis_Flet-Berliac1", "~reda_ouhamma1", "~odalric-ambrym_maillard1", "~Philippe_Preux1"], "authors": ["Yannis Flet-Berliac", "reda ouhamma", "odalric-ambrym maillard", "Philippe Preux"], "keywords": [], "abstract": "Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights.", "one-sentence_summary": "We introduce a method to improve the learning of the critic in the actor-critic framework.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fletberliac|learning_value_functions_in_deep_policy_gradients_using_residual_variance", "supplementary_material": "", "pdf": "/pdf/d19c38b4919b1481e2aa3972a928c866f4502b44.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nflet-berliac2021learning,\ntitle={Learning Value Functions in Deep Policy Gradients using Residual Variance},\nauthor={Yannis Flet-Berliac and reda ouhamma and odalric-ambrym maillard and Philippe Preux},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NX1He-aFO_F}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "NX1He-aFO_F", "replyto": "NX1He-aFO_F", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3292/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538078369, "tmdate": 1606915762641, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3292/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3292/-/Official_Review"}}}, {"id": "RSFgpQje_7g", "original": null, "number": 17, "cdate": 1606243258803, "ddate": null, "tcdate": 1606243258803, "tmdate": 1606294098214, "tddate": null, "forum": "NX1He-aFO_F", "replyto": "BwYLKvGlO7", "invitation": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment", "content": {"title": "3rd response to Area Chair", "comment": "We thank the Area Chair for their interest and additional questions. We answer the remaining questions below.  \n* Indeed, this does not change the policy gradient. Our answer to the initial question of the Area Chair is that the motivation for defining $g_\\phi$ is that among the estimators minimizing $\\mathcal{L}_{\\text{AVEC}}$, it is the one with minimal MSE.  \n* Indeed, in [2] they show that it is possible to evaluate variances, we will study their approach to understand why their estimators are unbiased. If their procedure is suitable we will try and provide similar results to strengthen the motivation of our loss in the camera-ready version.  \n* We do understand there might be some confusion arising from Fig.5. In fact, Fig. 5 presents the L2 distance to $\\hat{V}^\\pi$, hence a scaled (by the number of samples) MSE, which itself equals $\\text{Bias}^2 + \\text{Var}$. So Fig. 5 shows the sum of the bias term and the variance term. The intuition is that $\\hat{V}\u2019^\\pi$ have slightly higher biases than $\\hat{V}^\\pi$ but significantly lower variances. This leads to $||\\hat{V}\u2019^\\pi - V^\\pi|| \\leq ||\\hat{V}^\\pi - V^\\pi||$ even though $\\hat{V}\u2019^\\pi$ are meant to fit $V\u2019^\\pi$. We emphasize that this intuition of a higher bias is purely mathematical and simply due to the fact that we do not minimize the bias as in traditional actor-critic.  \n* **EDIT**: The bias and variance are the measures that come from the decomposition of the distance to the true value function $V^\\pi$: $\\mathbb{E}[\\|\\|g_\\phi-V^\\pi\\|\\|_2^2]=\\text{Bias}(\\mathtt{AVEC})^2+\\text{Var}(\\mathtt{AVEC})$ and $\\mathbb{E}[\\|\\|V_\\phi(\\mathtt{PPO})-V^\\pi\\|\\|_2^2]=\\text{Bias}(\\mathtt{PPO})^2+\\text{Var}(\\mathtt{PPO})$ where $V_\\phi(\\mathtt{PPO})$ is the value function estimator in PPO. We understand the confusion and make notations clearer in Appendix B.2.   \n* To clarify further, we would like to emphasize that the sentence \u201cthe points where the value function should be improved\u201d does not refer to points which we think are optimal but to points that need to be explored to reduce the estimation variability (due to stochastic rollouts) until they are no longer extreme, or forever if those points are truly optimal. We believe that this claim is not counterintuitive because the variability due to rollouts is unavoidable and symmetrical for state-values unless one constructs specific indicators to quantify the estimation variability in each state, which is beyond the scope of this paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3292/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Value Functions in Deep Policy Gradients using Residual Variance", "authorids": ["~Yannis_Flet-Berliac1", "~reda_ouhamma1", "~odalric-ambrym_maillard1", "~Philippe_Preux1"], "authors": ["Yannis Flet-Berliac", "reda ouhamma", "odalric-ambrym maillard", "Philippe Preux"], "keywords": [], "abstract": "Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights.", "one-sentence_summary": "We introduce a method to improve the learning of the critic in the actor-critic framework.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fletberliac|learning_value_functions_in_deep_policy_gradients_using_residual_variance", "supplementary_material": "", "pdf": "/pdf/d19c38b4919b1481e2aa3972a928c866f4502b44.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nflet-berliac2021learning,\ntitle={Learning Value Functions in Deep Policy Gradients using Residual Variance},\nauthor={Yannis Flet-Berliac and reda ouhamma and odalric-ambrym maillard and Philippe Preux},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NX1He-aFO_F}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NX1He-aFO_F", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3292/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3292/Authors|ICLR.cc/2021/Conference/Paper3292/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839007, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment"}}}, {"id": "4w3MxBerlK9", "original": null, "number": 19, "cdate": 1606293902277, "ddate": null, "tcdate": 1606293902277, "tmdate": 1606293902277, "tddate": null, "forum": "NX1He-aFO_F", "replyto": "b_i9L_9zyF", "invitation": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment", "content": {"title": "4th response to Area Chair", "comment": "Indeed, the MSE, Bias and Var are computed for $g_\\phi$, there was a typo in the equation of the reply. We edited the reply (\u201c**EDIT**: [...]\u201d) and rewrite the correct equations below.\n\n* The bias and variance are the measures that come from the decomposition of the distance to the true value function $V^\\pi$: $\\mathbb{E}[||g_\\phi-V^\\pi||_2^2]=\\text{Bias}(\\mathtt{AVEC})^2+\\text{Var}(\\mathtt{AVEC})$ and $\\mathbb{E}[||V_\\phi(\\mathtt{PPO})-V^\\pi||_2^2]=\\text{Bias}(\\mathtt{PPO})^2+\\text{Var}(\\mathtt{PPO})$ where $V_\\phi(\\mathtt{PPO})$ is the value function estimator in PPO. We also fix the equations in Appendix B.2."}, "signatures": ["ICLR.cc/2021/Conference/Paper3292/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Value Functions in Deep Policy Gradients using Residual Variance", "authorids": ["~Yannis_Flet-Berliac1", "~reda_ouhamma1", "~odalric-ambrym_maillard1", "~Philippe_Preux1"], "authors": ["Yannis Flet-Berliac", "reda ouhamma", "odalric-ambrym maillard", "Philippe Preux"], "keywords": [], "abstract": "Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights.", "one-sentence_summary": "We introduce a method to improve the learning of the critic in the actor-critic framework.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fletberliac|learning_value_functions_in_deep_policy_gradients_using_residual_variance", "supplementary_material": "", "pdf": "/pdf/d19c38b4919b1481e2aa3972a928c866f4502b44.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nflet-berliac2021learning,\ntitle={Learning Value Functions in Deep Policy Gradients using Residual Variance},\nauthor={Yannis Flet-Berliac and reda ouhamma and odalric-ambrym maillard and Philippe Preux},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NX1He-aFO_F}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NX1He-aFO_F", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3292/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3292/Authors|ICLR.cc/2021/Conference/Paper3292/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839007, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment"}}}, {"id": "b_i9L_9zyF", "original": null, "number": 18, "cdate": 1606256200152, "ddate": null, "tcdate": 1606256200152, "tmdate": 1606256200152, "tddate": null, "forum": "NX1He-aFO_F", "replyto": "RSFgpQje_7g", "invitation": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment", "content": {"title": "RE:", "comment": "The MSE, Bias, and Var terms are being computed for $g_\\phi$ or the targets $\\hat{V}$ and $\\hat{V}'$? The text suggests it is the former, but the equations suggest the latter."}, "signatures": ["ICLR.cc/2021/Conference/Paper3292/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Value Functions in Deep Policy Gradients using Residual Variance", "authorids": ["~Yannis_Flet-Berliac1", "~reda_ouhamma1", "~odalric-ambrym_maillard1", "~Philippe_Preux1"], "authors": ["Yannis Flet-Berliac", "reda ouhamma", "odalric-ambrym maillard", "Philippe Preux"], "keywords": [], "abstract": "Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights.", "one-sentence_summary": "We introduce a method to improve the learning of the critic in the actor-critic framework.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fletberliac|learning_value_functions_in_deep_policy_gradients_using_residual_variance", "supplementary_material": "", "pdf": "/pdf/d19c38b4919b1481e2aa3972a928c866f4502b44.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nflet-berliac2021learning,\ntitle={Learning Value Functions in Deep Policy Gradients using Residual Variance},\nauthor={Yannis Flet-Berliac and reda ouhamma and odalric-ambrym maillard and Philippe Preux},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NX1He-aFO_F}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NX1He-aFO_F", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3292/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3292/Authors|ICLR.cc/2021/Conference/Paper3292/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839007, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment"}}}, {"id": "BqFs9XNKrII", "original": null, "number": 16, "cdate": 1606243252318, "ddate": null, "tcdate": 1606243252318, "tmdate": 1606243252318, "tddate": null, "forum": "NX1He-aFO_F", "replyto": "008YA1ahDoJ", "invitation": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment", "content": {"title": "2nd response to Reviewer 3", "comment": "We thank the reviewer for their reply.\n\nAccording to your request, we have run the experiments on Hopper and have collected the results for PPO and AVEC-PPO which we add to Appendix B.1. Due to the time limitation, we are not yet able to collect the results for SAC (which is more time-consuming due to the update frequency), but we commit to adding the corresponding graph in the camera-ready version of the paper. Note that with the results with PPO, the average improvement rate goes from 33% to 31%, which remains significant."}, "signatures": ["ICLR.cc/2021/Conference/Paper3292/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Value Functions in Deep Policy Gradients using Residual Variance", "authorids": ["~Yannis_Flet-Berliac1", "~reda_ouhamma1", "~odalric-ambrym_maillard1", "~Philippe_Preux1"], "authors": ["Yannis Flet-Berliac", "reda ouhamma", "odalric-ambrym maillard", "Philippe Preux"], "keywords": [], "abstract": "Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights.", "one-sentence_summary": "We introduce a method to improve the learning of the critic in the actor-critic framework.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fletberliac|learning_value_functions_in_deep_policy_gradients_using_residual_variance", "supplementary_material": "", "pdf": "/pdf/d19c38b4919b1481e2aa3972a928c866f4502b44.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nflet-berliac2021learning,\ntitle={Learning Value Functions in Deep Policy Gradients using Residual Variance},\nauthor={Yannis Flet-Berliac and reda ouhamma and odalric-ambrym maillard and Philippe Preux},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NX1He-aFO_F}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NX1He-aFO_F", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3292/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3292/Authors|ICLR.cc/2021/Conference/Paper3292/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839007, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment"}}}, {"id": "008YA1ahDoJ", "original": null, "number": 11, "cdate": 1606143381588, "ddate": null, "tcdate": 1606143381588, "tmdate": 1606172127194, "tddate": null, "forum": "NX1He-aFO_F", "replyto": "T_7Rp8qmd_s", "invitation": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment", "content": {"title": "Thanks for the update", "comment": "Thanks for the feedback. Some of my concerns have been addressed and I will raise my score to 5.\n\n1) I agree with the authors that this part might be out of scope of this paper and I will not base my final score on this concern.\n\n2)  I still believe that due to empirical nature of this paper it's extremely important to provide results on this same set of benchmark tasks as prior work. In particular, TD3, SAC, PPO, AWR  and MBPO use Hopper for comparisons with prior work. If Hopper is added, I would raise my score to 6."}, "signatures": ["ICLR.cc/2021/Conference/Paper3292/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Value Functions in Deep Policy Gradients using Residual Variance", "authorids": ["~Yannis_Flet-Berliac1", "~reda_ouhamma1", "~odalric-ambrym_maillard1", "~Philippe_Preux1"], "authors": ["Yannis Flet-Berliac", "reda ouhamma", "odalric-ambrym maillard", "Philippe Preux"], "keywords": [], "abstract": "Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights.", "one-sentence_summary": "We introduce a method to improve the learning of the critic in the actor-critic framework.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fletberliac|learning_value_functions_in_deep_policy_gradients_using_residual_variance", "supplementary_material": "", "pdf": "/pdf/d19c38b4919b1481e2aa3972a928c866f4502b44.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nflet-berliac2021learning,\ntitle={Learning Value Functions in Deep Policy Gradients using Residual Variance},\nauthor={Yannis Flet-Berliac and reda ouhamma and odalric-ambrym maillard and Philippe Preux},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NX1He-aFO_F}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NX1He-aFO_F", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3292/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3292/Authors|ICLR.cc/2021/Conference/Paper3292/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839007, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment"}}}, {"id": "BwYLKvGlO7", "original": null, "number": 15, "cdate": 1606170605152, "ddate": null, "tcdate": 1606170605152, "tmdate": 1606170605152, "tddate": null, "forum": "NX1He-aFO_F", "replyto": "hmmDWeOsNcu", "invitation": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment", "content": {"title": "RE:", "comment": "3. To clarify, by translation, I meant adding a constant (that possibly depends on s, but not a). I believe this is also what you meant. This does not change the policy gradient for the same reason that a baseline does not cause bias. \n\nI'm not sure why continuous states and actions mean that variance cannot be measured? It seems that in [2], they quantify variance in similar environments.\n\n5. If the estimator fits the targets well (as suggested by previous studies), then I would expect the estimator to approximately match the expected value of the targets. So, Fig 5 would suggest that the bias of the AVEC targets is smaller than the standard targets. Can you explain where my understanding is going wrong? Also, in App B.2, can you clarify exactly how the Bias and Var terms are being computed and of what quantities.\n\n6. With stochastic rollouts, the value function can be perfect and the \"estimation error\" can be high, so I would be careful about concluding that these are \"the points where the value function should be improved.\" I think the claim about exploration needs further justification if you would like to make such a claim."}, "signatures": ["ICLR.cc/2021/Conference/Paper3292/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Value Functions in Deep Policy Gradients using Residual Variance", "authorids": ["~Yannis_Flet-Berliac1", "~reda_ouhamma1", "~odalric-ambrym_maillard1", "~Philippe_Preux1"], "authors": ["Yannis Flet-Berliac", "reda ouhamma", "odalric-ambrym maillard", "Philippe Preux"], "keywords": [], "abstract": "Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights.", "one-sentence_summary": "We introduce a method to improve the learning of the critic in the actor-critic framework.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fletberliac|learning_value_functions_in_deep_policy_gradients_using_residual_variance", "supplementary_material": "", "pdf": "/pdf/d19c38b4919b1481e2aa3972a928c866f4502b44.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nflet-berliac2021learning,\ntitle={Learning Value Functions in Deep Policy Gradients using Residual Variance},\nauthor={Yannis Flet-Berliac and reda ouhamma and odalric-ambrym maillard and Philippe Preux},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NX1He-aFO_F}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NX1He-aFO_F", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3292/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3292/Authors|ICLR.cc/2021/Conference/Paper3292/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839007, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment"}}}, {"id": "hmmDWeOsNcu", "original": null, "number": 12, "cdate": 1606154147724, "ddate": null, "tcdate": 1606154147724, "tmdate": 1606154709785, "tddate": null, "forum": "NX1He-aFO_F", "replyto": "Ldu5EdO8Hi", "invitation": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment", "content": {"title": "2nd response to Area Chair", "comment": "We would like to thank the Area Chair for their added questions and remarks. We answer the remaining questions below and upload a revised version of the paper.\n\n\n$3.$ $g_\\phi$ is a bias-corrected version of $f_\\phi$ to guarantee that the gradient *of the policy* is unbiased, not the gradient of $f_\\phi$ (cf. \u201cAVEC Policy Gradient\u201d Proposition). With this respect, translating $f_\\phi$ does change the policy gradient. While we give a theoretical intuition behind the improvement suggesting that the targets $V\u2019$ have lower variance, it is unclear how we can additionally confirm this statement empirically as states and state-actions are continuous. Nevertheless, the empirical results added to answer Q5 below do suggest that this is the case: we confirm that AVEC reduces the MSE by reducing the variance term (while collaterally increasing the bias term marginally), which indicates that the variance of $\\hat{V}'$ is very likely reduced compared to that of $\\hat{V}$.\n\n$5.$ The claim is that the variance is reduced and the bias is increased, but the variance reduction is far more substantial than the added bias. While it is trivial that $\\mathcal{L}_{\\text{AVEC}}$ provides an estimator with lower variance, the second part of the claim is not a general mathematical property: the variance reduction is not necessarily enough to counterbalance the bias increase. Thus, this claim does not hold for random supervised learning scenarios*. However in our case, since the bias of the empirical targets is quite substantial (cf. Fig. 5 of this paper and [1,2]), we hypothesized that focusing on relative errors (the variance term of the MSE) could be beneficial. To validate this intuition, we provide tangible evidence by running some additional experiments: in Appendix B.2 we plot the relative change of the bias term and the variance term when using AVEC, we remark very clearly that the variance reduction is far more substantial than that of the bias, and Fig. 5 confirms that it counterbalances the bias increase since the distance to $V^\\pi$ is lower when using AVEC.\n\n$6.$ Indeed, the rollouts are stochastic, and as such high estimation errors indicate the points where the value function should be improved. Put differently, the estimation error contains a variance term due to the randomness of the rollouts, and this variance should be considered as an indication of states to be explored further rather than of possible outliers arising from noise in the reward function or transition matrix.\n\n\nWe made the necessary adjustments in the revised version of the paper and would like to thank the Area Chair of their questions - we think this new study further strengthens the overall argumentation for the approach.\n\n\n*To open on your remark concerning supervised settings, we know that in reinforcement learning tasks, the bias of empirical estimators is generally high [1], and as such, we believe that this intuition of focusing on the variance is an interesting research direction that merits additional study, e.g. a time depending loss where the bias term could be increased following annealing strategies based on some indicators of the estimator\u2019s bias might be beneficial.\n\n\n[1] Ilyas Andrew, Logan Engstrom, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, and Aleksander Madry. \"A Closer Look at Deep Policy Gradients.\" In International Conference on Learning Representations. 2019.  \n[2] Tucker George, Surya Bhupatiraju, Shixiang Gu, Richard Turner, Zoubin Ghahramani, and Sergey Levine. \"The Mirage of Action-Dependent Baselines in Reinforcement Learning.\" In International Conference on Machine Learning, pp. 5015-5024. 2018.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3292/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Value Functions in Deep Policy Gradients using Residual Variance", "authorids": ["~Yannis_Flet-Berliac1", "~reda_ouhamma1", "~odalric-ambrym_maillard1", "~Philippe_Preux1"], "authors": ["Yannis Flet-Berliac", "reda ouhamma", "odalric-ambrym maillard", "Philippe Preux"], "keywords": [], "abstract": "Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights.", "one-sentence_summary": "We introduce a method to improve the learning of the critic in the actor-critic framework.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fletberliac|learning_value_functions_in_deep_policy_gradients_using_residual_variance", "supplementary_material": "", "pdf": "/pdf/d19c38b4919b1481e2aa3972a928c866f4502b44.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nflet-berliac2021learning,\ntitle={Learning Value Functions in Deep Policy Gradients using Residual Variance},\nauthor={Yannis Flet-Berliac and reda ouhamma and odalric-ambrym maillard and Philippe Preux},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NX1He-aFO_F}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NX1He-aFO_F", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3292/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3292/Authors|ICLR.cc/2021/Conference/Paper3292/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839007, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment"}}}, {"id": "nuG2FCOz1QW", "original": null, "number": 13, "cdate": 1606154356885, "ddate": null, "tcdate": 1606154356885, "tmdate": 1606154502903, "tddate": null, "forum": "NX1He-aFO_F", "replyto": "36FlceR5q5j", "invitation": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment", "content": {"title": "2nd response to Reviewer 1", "comment": "We thank the reviewer for their answers and comments.\n\n* In the latest version, we now mention that the gradient flows in two times below Equation 3.\n* We now write \u201cthe value of the states (resp. state-action pairs) *relative to their mean value* rather than the absolute value\u201d in the abstract.\n* We agree with the reviewer that the paragraph added in Section 4.2 does seem out of context. We rework it and move it to the Introduction in the newest version.\n* Indeed, [1] and [2] are related to this relative value intuition. We add those in the Introduction along with the ranking literature to further introduce the insight.\n\nIn addition to the changes taking effect in the newest revision of the paper, we agree with the reviewer and will take the time at the end of the review process to proof-read and unify the overall flow of the paper.\n\n\n[1] Wang Ziyu, et al. \"Dueling network architectures for deep reinforcement learning.\" International conference on machine learning. 2016.  \n[2] Harmon Mance E., and Leemon C. Baird III. \"Multi-player residual advantage learning with general function approximation.\" Wright Laboratory, WL/AACF, Wright-Patterson Air Force Base, OH (1996): 45433-7308.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3292/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Value Functions in Deep Policy Gradients using Residual Variance", "authorids": ["~Yannis_Flet-Berliac1", "~reda_ouhamma1", "~odalric-ambrym_maillard1", "~Philippe_Preux1"], "authors": ["Yannis Flet-Berliac", "reda ouhamma", "odalric-ambrym maillard", "Philippe Preux"], "keywords": [], "abstract": "Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights.", "one-sentence_summary": "We introduce a method to improve the learning of the critic in the actor-critic framework.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fletberliac|learning_value_functions_in_deep_policy_gradients_using_residual_variance", "supplementary_material": "", "pdf": "/pdf/d19c38b4919b1481e2aa3972a928c866f4502b44.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nflet-berliac2021learning,\ntitle={Learning Value Functions in Deep Policy Gradients using Residual Variance},\nauthor={Yannis Flet-Berliac and reda ouhamma and odalric-ambrym maillard and Philippe Preux},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NX1He-aFO_F}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NX1He-aFO_F", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3292/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3292/Authors|ICLR.cc/2021/Conference/Paper3292/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839007, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment"}}}, {"id": "iMi-8SkE9Zr", "original": null, "number": 14, "cdate": 1606154431392, "ddate": null, "tcdate": 1606154431392, "tmdate": 1606154431392, "tddate": null, "forum": "NX1He-aFO_F", "replyto": "V370wABgna1", "invitation": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment", "content": {"title": "2nd response to Reviewer 2", "comment": "We thank the reviewer for their reply and comments.\n\n* We agree with the reviewer that the variance reduction for estimators approaching two different targets should not be a key focus. However, we choose to perform this comparison because a proof that the residual variance for an estimator $f_\\phi$ obtained using AVEC is less than that of $f_\\phi$ obtained using the MSE implies that the new estimator very likely has a reduced residual variance to $V^\\pi$. While this does not prove that the distance to the true target $V^\\pi$ is improved under AVEC, our intuition is that since the bias is already very high [1], the variance reduction (while collaterally increasing the bias) might be significant enough to reduce the MSE. We further elaborate this point in Section 4.2 and provide empirical evidence in Appendix B.2 of the revised version.\n* We would like to recall that, in practice, most actor-critic algorithms (in particular the ones considered in the paper) are trained using, at each update, the empirical targets corresponding to the transitions which were seen during the last few episodes (i.e. batch $\\mathcal{B}$ in the paper, composed of the latest 2048 transitions in the case of PPO and TRPO). The critic fits the new targets corresponding to the states visited during those episodes. This is why the $T$ term appearing in $\\mathcal{L}_{\\text{AVEC}}$ corresponds to the size of $\\mathcal{B}$. Note that a batch of transitions can be composed of several episodes: when a terminal state is encountered before $T$ transitions have been collected, the agent is reset to an initial state and continues to perform actions in the environment until the number of transitions in the batch equals $T$.\n\n[1] Ilyas Andrew, Logan Engstrom, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, and Aleksander Madry. \"A Closer Look at Deep Policy Gradients.\" In International Conference on Learning Representations. 2019."}, "signatures": ["ICLR.cc/2021/Conference/Paper3292/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Value Functions in Deep Policy Gradients using Residual Variance", "authorids": ["~Yannis_Flet-Berliac1", "~reda_ouhamma1", "~odalric-ambrym_maillard1", "~Philippe_Preux1"], "authors": ["Yannis Flet-Berliac", "reda ouhamma", "odalric-ambrym maillard", "Philippe Preux"], "keywords": [], "abstract": "Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights.", "one-sentence_summary": "We introduce a method to improve the learning of the critic in the actor-critic framework.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fletberliac|learning_value_functions_in_deep_policy_gradients_using_residual_variance", "supplementary_material": "", "pdf": "/pdf/d19c38b4919b1481e2aa3972a928c866f4502b44.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nflet-berliac2021learning,\ntitle={Learning Value Functions in Deep Policy Gradients using Residual Variance},\nauthor={Yannis Flet-Berliac and reda ouhamma and odalric-ambrym maillard and Philippe Preux},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NX1He-aFO_F}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NX1He-aFO_F", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3292/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3292/Authors|ICLR.cc/2021/Conference/Paper3292/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839007, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment"}}}, {"id": "V370wABgna1", "original": null, "number": 9, "cdate": 1605994465417, "ddate": null, "tcdate": 1605994465417, "tmdate": 1605994465417, "tddate": null, "forum": "NX1He-aFO_F", "replyto": "400fQ2Y_RMe", "invitation": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment", "content": {"title": "Not sure I'm convinced by the correct interpretation of $T$", "comment": "Thanks for the reply.\n\nRegarding the variance reduction comment --  firstly, I'm not sure that the variance reduction should necessarily be a key focus, especially since the objective is being changed and this isn't a case of comparing the variance between two unbiased estimators of the same objective as is typically the case when variance comparisons are invoked. Having said that, I don't fully follow your response. The way I'm understanding it, the variance being referred to is that of the single sample monte carlo estimate for the RHS of Eq (3) (or equivalently, $\\mathcal{L}^{1,2} _{AVEC}$).  And $T$ should be the number of samples used to define the Monte carlo estimate in the inner expectation, which is over $(s, a)$ or $s$. I don't quite see how the trajectory length is coming into the picture here. \n\nBesides the author comment itself, it seems like the presentation could be improved by more clearly relating the intuition presented at the beginning of 4.2 with what is actually implemented in the algorithm."}, "signatures": ["ICLR.cc/2021/Conference/Paper3292/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Value Functions in Deep Policy Gradients using Residual Variance", "authorids": ["~Yannis_Flet-Berliac1", "~reda_ouhamma1", "~odalric-ambrym_maillard1", "~Philippe_Preux1"], "authors": ["Yannis Flet-Berliac", "reda ouhamma", "odalric-ambrym maillard", "Philippe Preux"], "keywords": [], "abstract": "Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights.", "one-sentence_summary": "We introduce a method to improve the learning of the critic in the actor-critic framework.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fletberliac|learning_value_functions_in_deep_policy_gradients_using_residual_variance", "supplementary_material": "", "pdf": "/pdf/d19c38b4919b1481e2aa3972a928c866f4502b44.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nflet-berliac2021learning,\ntitle={Learning Value Functions in Deep Policy Gradients using Residual Variance},\nauthor={Yannis Flet-Berliac and reda ouhamma and odalric-ambrym maillard and Philippe Preux},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NX1He-aFO_F}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NX1He-aFO_F", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3292/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3292/Authors|ICLR.cc/2021/Conference/Paper3292/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839007, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment"}}}, {"id": "Ldu5EdO8Hi", "original": null, "number": 8, "cdate": 1605729369591, "ddate": null, "tcdate": 1605729369591, "tmdate": 1605729422883, "tddate": null, "forum": "NX1He-aFO_F", "replyto": "625hq9k6taA", "invitation": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment", "content": {"title": "RE:", "comment": "Thank you for the clarifications.\n\n1. Great, clear now.\n2. Great, thanks for clarifying.\n3. Translating (by a constant) $g_\\phi$ does not change the gradient. How does this square with your motivation for defining $g_\\phi$. Is lower variance of $\\hat{V}'$ the reason for the improvements? Can you provide empirical support for the claim that the targets have substantially lower variance?\n4. Great, clear now.\n5. I agree that there is an empirical gain, but I do not find the this argument convincing. Is the claim that the bias of the estimator does not change, but the improvement is due to variance reduction? Do these arguments hold for a supervised setting?\n6. The policy is stochastic, so the rollouts are still stochastic even with deterministic dynamics. Given that, please elaborate on the claim that \"As such, there are no outliers and extreme state-action values correspond to learning signals.\""}, "signatures": ["ICLR.cc/2021/Conference/Paper3292/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Value Functions in Deep Policy Gradients using Residual Variance", "authorids": ["~Yannis_Flet-Berliac1", "~reda_ouhamma1", "~odalric-ambrym_maillard1", "~Philippe_Preux1"], "authors": ["Yannis Flet-Berliac", "reda ouhamma", "odalric-ambrym maillard", "Philippe Preux"], "keywords": [], "abstract": "Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights.", "one-sentence_summary": "We introduce a method to improve the learning of the critic in the actor-critic framework.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fletberliac|learning_value_functions_in_deep_policy_gradients_using_residual_variance", "supplementary_material": "", "pdf": "/pdf/d19c38b4919b1481e2aa3972a928c866f4502b44.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nflet-berliac2021learning,\ntitle={Learning Value Functions in Deep Policy Gradients using Residual Variance},\nauthor={Yannis Flet-Berliac and reda ouhamma and odalric-ambrym maillard and Philippe Preux},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NX1He-aFO_F}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NX1He-aFO_F", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3292/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3292/Authors|ICLR.cc/2021/Conference/Paper3292/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839007, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment"}}}, {"id": "36FlceR5q5j", "original": null, "number": 7, "cdate": 1605689102968, "ddate": null, "tcdate": 1605689102968, "tmdate": 1605689102968, "tddate": null, "forum": "NX1He-aFO_F", "replyto": "wpYQp0DGIyN", "invitation": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment", "content": {"title": "The paper is now clearer", "comment": "Thanks for the added details!\n\n- This addresses my question. I would just make sure that the fact that the gradient flows in $f_{\\phi}$ two times is mentioned just below Equation 3.\n- The added intuition helps. I don't exactly remember how the paper was before the update, but I appreciate that the current form of the paper also has some intuition about what the contribution does in the abstract. I simply note that from a grammar perspective, and to allow a better understanding from the reader, something is always relative to *something else*, in \"learns the relative value of the states (resp. state-action pairs) [relative to each other? to the expectation?] rather than their absolute value\".\n- I'm fully aware that modifying a paper following many comments from many reviewers can be a long and challenging task. I would suggest the authors, just before submitting the final version of the paper, to proof-read everything and check that the paper still flows nicely. For instance, the intuition added in Section 4.2 seems to appear a bit out of nowhere, and could have been put just before Equation 3, or maybe even in the introduction.\n\nThe mentions that AVEC loss focuses on the relative value of states instead of their absolute value reminds me of the literature on Dueling [1] and Advantage [2] reinforcement learning. I think that citing these papers could further motivate the argument of focusing on relative values, as they both show strong empirical evidence that what matters is the order of Q-Values in a state, not their absolute values.\n\n[1]: Wang, Ziyu, et al. \"Dueling network architectures for deep reinforcement learning.\" International conference on machine learning. 2016. (the well-known paper on having a network predict advantage values + state value, instead of Q-Values)\n[2]: Harmon, Mance E., and Leemon C. Baird III. \"Multi-player residual advantage learning with general function approximation.\" Wright Laboratory, WL/AACF, Wright-Patterson Air Force Base, OH (1996): 45433-7308. (a far lesser-known paper, but much earlier than the other one, that presents a modified Q-operator that focuses on the relative value of actions, and widens the gap between good and bad actions)\n[3]: Bellemare, Marc G., et al. \"Increasing the action gap: New operators for reinforcement learning.\" arXiv preprint arXiv:1512.04860 (2015). (related to [2] as it considers the gap between Q-Values, but less related to this paper)"}, "signatures": ["ICLR.cc/2021/Conference/Paper3292/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Value Functions in Deep Policy Gradients using Residual Variance", "authorids": ["~Yannis_Flet-Berliac1", "~reda_ouhamma1", "~odalric-ambrym_maillard1", "~Philippe_Preux1"], "authors": ["Yannis Flet-Berliac", "reda ouhamma", "odalric-ambrym maillard", "Philippe Preux"], "keywords": [], "abstract": "Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights.", "one-sentence_summary": "We introduce a method to improve the learning of the critic in the actor-critic framework.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fletberliac|learning_value_functions_in_deep_policy_gradients_using_residual_variance", "supplementary_material": "", "pdf": "/pdf/d19c38b4919b1481e2aa3972a928c866f4502b44.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nflet-berliac2021learning,\ntitle={Learning Value Functions in Deep Policy Gradients using Residual Variance},\nauthor={Yannis Flet-Berliac and reda ouhamma and odalric-ambrym maillard and Philippe Preux},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NX1He-aFO_F}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NX1He-aFO_F", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3292/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3292/Authors|ICLR.cc/2021/Conference/Paper3292/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839007, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment"}}}, {"id": "625hq9k6taA", "original": null, "number": 6, "cdate": 1605630935147, "ddate": null, "tcdate": 1605630935147, "tmdate": 1605630935147, "tddate": null, "forum": "NX1He-aFO_F", "replyto": "PkXByGuKui", "invitation": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment", "content": {"title": "Response to Area Chair", "comment": "We thank the Area Chair for their questions, to which we answer below. We have also uploaded an updated version of the paper.\n\n1. We agree with the area chair that those quantities were not sufficiently well defined. We give a definition of $\\hat{V}$ and $\\hat{Q}$ in Section 3.2 and describe in detail in Section 4.3 how the state-value and state-action value functions are estimated in the implementations of PPO, TRPO and SAC.\n2. According to Ilyas et al. [1], \u201cthe value network does succeed in fitting the given loss function [...]. However, the significant drop in performance [referring to the larger distance to the true value function] indicates that the supervised learning problem [\u2026] does not lead to [the value network] learning the underlying true value function\u201d. This claim is in accordance with our results in Fig. 4 and 5: in the first half of learning for an agent trained with PPO, $V^\\pi$ is not well approximated by $\\hat V^\\pi $ and AVEC approximates $V^\\pi$ better.\n3. Indeed, $g_\\phi$ is a function, we now define it more precisely in Section 4.1. Since translating $f_\\phi$ does not increase the loss of the critic, in AVEC we choose $g_\\phi$, a bias-corrected version of $f_\\phi$ to guarantee that the gradient is unbiased. AVEC leverages the property that $\\hat{V'}$ is a better target than $\\hat{V}$: it has lower variance. In other words, AVEC modifies the critic\u2019s objective so that $f_\\phi$ fits $\\hat{V'}$, which approximates $V\u2019$ better than $\\hat{V}$ approximates $V$.\n4. In the paper, we used \u201cconsistent\u201d as \u201cunbiased\u201d. We acknowledge this term was confusing and replace it in the revised version.\n5. Indeed, $\\hat{V'}$ is defined in terms of expectations. In the analysis for the independent case, we see a variance reduction effect even when the mean estimate contains $X_i$. An in-depth analysis of the general case requires additional assumptions, we do remark however that the $X_i$ term which appears in the expectation scales with $\\frac{1}{T}$ (typically $T=2048$) which means that its influence can be neglected. That being said, our intuition is that since the empirical realizations of $V\u2019$ have lower variance than those of $V$, $\\hat{V'}$ is a better estimator of $V\u2019$ than $\\hat{V}$ is of $V$, i.e. $\\|\\|\\hat{V'}-V\u2019\\|\\| \\leq \\|\\|\\hat{V}-V\\|\\|$. Although this does not completely resolve the approximation issue, we demonstrate empirically that it improves the distance to the true value function.\n6. Noiseless tasks means the transition matrix is deterministic. As such, there are no outliers and extreme state-action values correspond to learning signals. Hence the sentence \u201chigh estimation errors indicate where (in the state or action-state space) the training of the value function should be improved\u201d. We clarify this in the corresponding section. The policy is stochastic (defined in Section 3.1). The application of our method in stochastic environments is anticipated in future investigations (cf. end of Section 6).\n\n[1] Ilyas Andrew, Logan Engstrom, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, and Aleksander Madry. \"A Closer Look at Deep Policy Gradients.\" In International Conference on Learning Representations. 2019."}, "signatures": ["ICLR.cc/2021/Conference/Paper3292/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Value Functions in Deep Policy Gradients using Residual Variance", "authorids": ["~Yannis_Flet-Berliac1", "~reda_ouhamma1", "~odalric-ambrym_maillard1", "~Philippe_Preux1"], "authors": ["Yannis Flet-Berliac", "reda ouhamma", "odalric-ambrym maillard", "Philippe Preux"], "keywords": [], "abstract": "Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights.", "one-sentence_summary": "We introduce a method to improve the learning of the critic in the actor-critic framework.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fletberliac|learning_value_functions_in_deep_policy_gradients_using_residual_variance", "supplementary_material": "", "pdf": "/pdf/d19c38b4919b1481e2aa3972a928c866f4502b44.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nflet-berliac2021learning,\ntitle={Learning Value Functions in Deep Policy Gradients using Residual Variance},\nauthor={Yannis Flet-Berliac and reda ouhamma and odalric-ambrym maillard and Philippe Preux},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NX1He-aFO_F}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NX1He-aFO_F", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3292/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3292/Authors|ICLR.cc/2021/Conference/Paper3292/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839007, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment"}}}, {"id": "PkXByGuKui", "original": null, "number": 5, "cdate": 1605572451330, "ddate": null, "tcdate": 1605572451330, "tmdate": 1605572451330, "tddate": null, "forum": "NX1He-aFO_F", "replyto": "NX1He-aFO_F", "invitation": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment", "content": {"title": "Questions", "comment": "1. $\\hat{A}, \\hat{V}, \\hat{Q}$ are defined as \"bootstrapped Monte Carlo\" estimates. \"Bootstrapped\" suggests that they are based on an estimator, however, they are in Sec 3.1 before function approximators are introduced. Can the authors elaborate on precisely how these quantities are defined?\n2. Sec 4.1 says that the value network fits $\\hat{V}$ well but not $V$. This implies that $V$ is not approximated well by $\\hat{V}$. Is that what the authors are claiming?\n3. Is $g_\\theta$ a function? It is an unbiased estimator of $\\hat{V}$ which does not approximate $V$ well?\n4. What does \"consistent\" in the proposition mean? \n5. Sec 4.2 talks about $\\hat{V}'$ defined in terms of expectations, however, the argument talks about subtracting an empirical estimate of the mean that contains $X_i$. How does that affect the argument? Furthermore, while the variance may be reduced, what is the relation between $||\\hat{V} - V||$ and $||\\hat{V}' - V'||$ as the previous sections argue these are not 0.\n6. Sec 4.2 says that the tasks are noiseless. What does this mean? Is the policy deterministic? Does this method apply to stochastic environments?"}, "signatures": ["ICLR.cc/2021/Conference/Paper3292/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Value Functions in Deep Policy Gradients using Residual Variance", "authorids": ["~Yannis_Flet-Berliac1", "~reda_ouhamma1", "~odalric-ambrym_maillard1", "~Philippe_Preux1"], "authors": ["Yannis Flet-Berliac", "reda ouhamma", "odalric-ambrym maillard", "Philippe Preux"], "keywords": [], "abstract": "Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights.", "one-sentence_summary": "We introduce a method to improve the learning of the critic in the actor-critic framework.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fletberliac|learning_value_functions_in_deep_policy_gradients_using_residual_variance", "supplementary_material": "", "pdf": "/pdf/d19c38b4919b1481e2aa3972a928c866f4502b44.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nflet-berliac2021learning,\ntitle={Learning Value Functions in Deep Policy Gradients using Residual Variance},\nauthor={Yannis Flet-Berliac and reda ouhamma and odalric-ambrym maillard and Philippe Preux},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NX1He-aFO_F}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NX1He-aFO_F", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3292/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3292/Authors|ICLR.cc/2021/Conference/Paper3292/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839007, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment"}}}, {"id": "400fQ2Y_RMe", "original": null, "number": 3, "cdate": 1605540862338, "ddate": null, "tcdate": 1605540862338, "tmdate": 1605541980114, "tddate": null, "forum": "NX1He-aFO_F", "replyto": "pKg71-ogGKd", "invitation": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank the reviewer for their time, positive feedback, and insightful comments.\n\nWe agree that the variance reduction for a given state value (resp. state-action value) scales with $\\frac{1}{T}$. However, we note that to compare AVEC to the squared error case accurately, one should account for the sum of these reductions over visited states in a trajectory, which is equal to $\\frac{2T-1}{T} \\sum_{j=1}^T \\mathbb{V}(X_j)$ and does not scale with $\\frac{1}{T}$.\nFurthermore, we emphasize that $T$ is not very large, it represents the size of the trajectories used to approximate the gradient, for example in our experiments it is equal to $2048$.\n\nConcerning the comment of the reviewer on the improvements of our method, we cannot agree with them being marginal: AVEC brings an improvement over the baseline of on average +26% for SAC and +39% for PPO. Moreover, from Table 1, we find that the coefficients of variation (std/mean) are on average 11% for SAC and 9.5% for PPO. Consequently, we believe that empirical improvement conclusions are reasonable.\n\nThank you for the many additional comments, to which we reply below: \n* It was a typo, we fixed it.\n* The problem depicted in Fig. 1 is a simple example of regression with one variable. We clarify this in Section 4.2.\n* What we would like to highlight in this part of the paper is that the inner empirical expectation is empirically-biased and that it is not possible to propose a bias-corrected version without further restrictive assumptions on the joint law of state-values.\n* We do confirm to the reviewer that Fig. 4 is the L2 distance with respect to the bias-corrected version $g_\\phi$. Indeed it is large, which might be surprising at first sight but is not inconsistent with Fig. 5 which shows that PPO\u2019s value estimator is farther from the true target than AVEC-PPO\u2019s from the empirical target.\n* In the ablation study, we question whether there exists a value of $\\alpha$ that yields better performance than $\\alpha=0$, empirically we find that this is not the case which is why we consider $\\mathcal{L}_{\\text{AVEC}}$. This is also favorable as it suggests that introducing a weighting with the need to be tuned would not be beneficial. Regarding the separation of the plots, indeed for the same task, AVEC-PPO and PPO are the same curves when the weighting is less than one or not. This was done for readability purposes only. We clarify this in Section 5.4.\n* The flatness of the curves is simply a resolution matter: we decided to plot 5 values ($t \\in \\{1,2,4,6,9\\}.10^5$) corresponding to the order of magnitude chosen in Ilyas et al [1]. mainly because it is computationally demanding and because it suffices in order to compare AVEC to the base algorithm.\n\n[1] Ilyas, Andrew, Logan Engstrom, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, and Aleksander Madry. \"A Closer Look at Deep Policy Gradients.\" In International Conference on Learning Representations. 2019."}, "signatures": ["ICLR.cc/2021/Conference/Paper3292/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Value Functions in Deep Policy Gradients using Residual Variance", "authorids": ["~Yannis_Flet-Berliac1", "~reda_ouhamma1", "~odalric-ambrym_maillard1", "~Philippe_Preux1"], "authors": ["Yannis Flet-Berliac", "reda ouhamma", "odalric-ambrym maillard", "Philippe Preux"], "keywords": [], "abstract": "Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights.", "one-sentence_summary": "We introduce a method to improve the learning of the critic in the actor-critic framework.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fletberliac|learning_value_functions_in_deep_policy_gradients_using_residual_variance", "supplementary_material": "", "pdf": "/pdf/d19c38b4919b1481e2aa3972a928c866f4502b44.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nflet-berliac2021learning,\ntitle={Learning Value Functions in Deep Policy Gradients using Residual Variance},\nauthor={Yannis Flet-Berliac and reda ouhamma and odalric-ambrym maillard and Philippe Preux},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NX1He-aFO_F}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NX1He-aFO_F", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3292/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3292/Authors|ICLR.cc/2021/Conference/Paper3292/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839007, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment"}}}, {"id": "wpYQp0DGIyN", "original": null, "number": 4, "cdate": 1605541593519, "ddate": null, "tcdate": 1605541593519, "tmdate": 1605541593519, "tddate": null, "forum": "NX1He-aFO_F", "replyto": "D2va7mvewTd", "invitation": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We would like to thank the reviewer for the positive feedback and thoroughly written review. \n\nConcerning the issues raised by the reviewer:\n* When using AVEC, all the gradients in the total objective remain the same for the coupled method except the gradient of $\\mathcal{L}_{\\text{AVEC}}$. No stop gradient is used for the value (resp. state-value) function loss: both gradients contribute to the total gradient.\n* Thank you for your comment regarding clarity, we now provide a general intuition at the end of Section 4.2 where we emphasize that this added term is used to focus more on the relative values than on absolute values.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3292/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Value Functions in Deep Policy Gradients using Residual Variance", "authorids": ["~Yannis_Flet-Berliac1", "~reda_ouhamma1", "~odalric-ambrym_maillard1", "~Philippe_Preux1"], "authors": ["Yannis Flet-Berliac", "reda ouhamma", "odalric-ambrym maillard", "Philippe Preux"], "keywords": [], "abstract": "Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights.", "one-sentence_summary": "We introduce a method to improve the learning of the critic in the actor-critic framework.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fletberliac|learning_value_functions_in_deep_policy_gradients_using_residual_variance", "supplementary_material": "", "pdf": "/pdf/d19c38b4919b1481e2aa3972a928c866f4502b44.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nflet-berliac2021learning,\ntitle={Learning Value Functions in Deep Policy Gradients using Residual Variance},\nauthor={Yannis Flet-Berliac and reda ouhamma and odalric-ambrym maillard and Philippe Preux},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NX1He-aFO_F}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NX1He-aFO_F", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3292/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3292/Authors|ICLR.cc/2021/Conference/Paper3292/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839007, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment"}}}, {"id": "T_7Rp8qmd_s", "original": null, "number": 2, "cdate": 1605540438496, "ddate": null, "tcdate": 1605540438496, "tmdate": 1605540438496, "tddate": null, "forum": "NX1He-aFO_F", "replyto": "QY9bYuNLk8O", "invitation": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We thank the reviewer for their time and comments. Below we address the remarks of the reviewer.\n\nFirst, we consider it important to insist that the main contribution of this paper is to introduce a new loss to learn the critic; we demonstrate the modification does not bias the gradient. Regarding the remark on the bias in the loss, we address this point at the end of Section 4.3 and in Appendix D: \u201cstate-values for a non-optimal policy are dependent and the variance is not tractable without access to the joint law of state-values. Consequently, to implement AVEC in practice we use the best-known proxy at hand, which is the empirical variance formula assuming independence\u201c. Moreover, we emphasize that theoretical results in deep policy gradient research are merely a source of motivation as even the most basic assumptions (parameterization assumption for example) are unrealistic: Tucker et al. [1] suggest that gradients resulting from these assumptions are always biased in empirical tasks. Hence the focus of this paper is to provide theoretical insights for both cases: when the critic fits a state value function or a state-action value function. Could the reviewer expand on the additional results considered appropriate? We will do our best to pursue such paths to provide further insights for our method. Nevertheless, we believe this would not belong to this work and would be more appropriate for future investigations. \n\nConcerning the experimental evaluation, we respectfully disagree with the remark of the reviewer. For instance, HalfCheetah and Humanoid are used at least as much commonly in literature and Humanoid is more challenging than Hopper or Walker2d (much larger state and action space). In our experimental protocol we chose a representative set of tasks ranging from moderate (Reacher: $\\mathbb{R}^{11} \\times \\mathbb{R}^{2}$) to very large (Humanoid: $\\mathbb{R}^{376} \\times \\mathbb{R}^{17}$) state and action spaces. Although we are confused by this comment as the reviewer stated earlier that we used \u201cstandard benchmarks for continuous control\u201d. Moreover, note that we have already included in Appendix B.2 variance reduction graphs using the (open-source) PyBullet versions of Hopper and Walker2d. Nevertheless, because we sincerely want to address any concern that may come from the reviewer, we include in Table 1 and Appendix B.1 of the revised version of the paper the scores and performance graphs for Walker2d, which compared to Hopper, is more challenging and has a larger state action space ($\\mathbb{R}^{17} \\times \\mathbb{R}^{6}$ vs. $\\mathbb{R}^{11} \\times \\mathbb{R}^{3}$). The results show that using AVEC produces comparable gains in performance: 26% for SAC and 33% for PPO.\n\n[1] Tucker, George, Surya Bhupatiraju, Shixiang Gu, Richard Turner, Zoubin Ghahramani, and Sergey Levine. \"The Mirage of Action-Dependent Baselines in Reinforcement Learning.\" In International Conference on Machine Learning, pp. 5015-5024. 2018."}, "signatures": ["ICLR.cc/2021/Conference/Paper3292/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Value Functions in Deep Policy Gradients using Residual Variance", "authorids": ["~Yannis_Flet-Berliac1", "~reda_ouhamma1", "~odalric-ambrym_maillard1", "~Philippe_Preux1"], "authors": ["Yannis Flet-Berliac", "reda ouhamma", "odalric-ambrym maillard", "Philippe Preux"], "keywords": [], "abstract": "Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights.", "one-sentence_summary": "We introduce a method to improve the learning of the critic in the actor-critic framework.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fletberliac|learning_value_functions_in_deep_policy_gradients_using_residual_variance", "supplementary_material": "", "pdf": "/pdf/d19c38b4919b1481e2aa3972a928c866f4502b44.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nflet-berliac2021learning,\ntitle={Learning Value Functions in Deep Policy Gradients using Residual Variance},\nauthor={Yannis Flet-Berliac and reda ouhamma and odalric-ambrym maillard and Philippe Preux},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NX1He-aFO_F}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NX1He-aFO_F", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3292/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3292/Authors|ICLR.cc/2021/Conference/Paper3292/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839007, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3292/-/Official_Comment"}}}, {"id": "pKg71-ogGKd", "original": null, "number": 2, "cdate": 1603743244161, "ddate": null, "tcdate": 1603743244161, "tmdate": 1605024027327, "tddate": null, "forum": "NX1He-aFO_F", "replyto": "NX1He-aFO_F", "invitation": "ICLR.cc/2021/Conference/Paper3292/-/Official_Review", "content": {"title": "A simple and widely applicable alternative to the squared loss objectives in RL with some evidence of empirical benefits.", "review": "### Strengths\n\nThe paper proposes a simple and elegant idea for changing the value function objectives in deep RL and demonstrates reasonable empirical evidence of it's potential usefulness.  The authors also provide a clearly articulated intuitive motivation and provide experiments to support the proposal.  The idea complements several other algorithms and is therefore quite widely applicable (and easy to try). The analysis of the experiments is also quite interesting and clearly presented. \n\n### Weaknesses\n\nThe paper is mostly well written and has interesting theoretical insights as well as empirical analysis. Here are a some weaknesses.\n\n* The theoretical justification for the variance reduction while technically correct, seems like it should be miniscule in theory. For the $T$ independent RV case being analyzed, the condition required for the improvement is that $\\Delta  \\triangleq 2 \\mathbb{V}(X_i) - \\frac{1}{T} \\sum_{j=1}^T \\mathbb{V}(X_j) > 0$, which seems reasonable unless the sample in question is an outlier with a very small variance to begin with. However, the overall reduction itself has another $\\frac{1}{T}$ scaling, i.e. the variance reduction over the squared error case is equal to $\\frac{\\Delta}{T}$, which seems to be vanishingly small as the number of samples $T$ is large even if $\\Delta \\gg 0$. Note that for the situation where this core idea is being applied, the parameter $T$ is approximately, the number of samples in the expectation over $(s, a)$, which is large in practice.\n* The improvements are a good sanity check, but somewhat marginal in many cases (especially given the error bars).\n\n### Additional comments/feedback\n\n* In Section 4.2 paragraph on State-value function estimation line 3, should the targets be $\\widehat{V}^\\pi$ rather than $V^\\pi$?\n* In Figure 1, some additional detail on the claims seems necessary (e.g. what parameterization is being considered?)\n* In the discussion below the specification for $\\mathcal{L}^1_{AVEC}, \\mathcal{L}^2_{AVEC}$, the authors say \"the reader may have noticed that these equations  slightly differ from Eq. 3\", but I am not able to see what difference is being alluded to.\n* Figure 4 looks quite surprising in terms of the large qualitative difference between the baseline and AVEC-baseline graphs. Just to be sure, do you measure the fit with respect to $f_\\phi$ or the bias corrected version, $g_\\phi$? (obviously, the latter makes more sense?). \n* The Ablation study in Section 5.4 seems intriguing, but what the conclusions imply seems unclear. It appears the authors were expecting to see some non-zero value of $\\alpha$ to improve over $\\alpha=0$ (AVEC), but this isn't the case? Some additional clarification here would be useful. Also, it is a bit confusing to separate the plots into two depending on whether the weighting is less than one; as I'm guessing the exact same plot is used for the non-alpha versions in each pair of these graphs?\n* In Figure 5, the distance to the true value function seems to be relatively flat (or even mildly increasing) through the entire horizon in both graphs. Is this simply due to the resolution, as I'd expect there to be a drop at least in the initial phase over time.\n\n\n\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3292/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3292/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Value Functions in Deep Policy Gradients using Residual Variance", "authorids": ["~Yannis_Flet-Berliac1", "~reda_ouhamma1", "~odalric-ambrym_maillard1", "~Philippe_Preux1"], "authors": ["Yannis Flet-Berliac", "reda ouhamma", "odalric-ambrym maillard", "Philippe Preux"], "keywords": [], "abstract": "Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights.", "one-sentence_summary": "We introduce a method to improve the learning of the critic in the actor-critic framework.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fletberliac|learning_value_functions_in_deep_policy_gradients_using_residual_variance", "supplementary_material": "", "pdf": "/pdf/d19c38b4919b1481e2aa3972a928c866f4502b44.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nflet-berliac2021learning,\ntitle={Learning Value Functions in Deep Policy Gradients using Residual Variance},\nauthor={Yannis Flet-Berliac and reda ouhamma and odalric-ambrym maillard and Philippe Preux},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NX1He-aFO_F}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "NX1He-aFO_F", "replyto": "NX1He-aFO_F", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3292/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538078369, "tmdate": 1606915762641, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3292/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3292/-/Official_Review"}}}], "count": 22}