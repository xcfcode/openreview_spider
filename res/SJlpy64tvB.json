{"notes": [{"id": "SJlpy64tvB", "original": "H1esAYiBPr", "number": 319, "cdate": 1569438949207, "ddate": null, "tcdate": 1569438949207, "tmdate": 1577168261909, "tddate": null, "forum": "SJlpy64tvB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Attacking Lifelong Learning Models with Gradient Reversion", "authors": ["Yunhui Guo", "Mingrui Liu", "Yandong Li", "Liqiang Wang", "Tianbao Yang", "Tajana Rosing"], "authorids": ["yug185@eng.ucsd.edu", "mingrui-liu@uiowa.edu", "lyndon.leeseu@outlook.com", "lwang@cs.ucf.edu", "tianbao-yang@uiowa.edu", "tajana@ucsd.edu"], "keywords": ["lifelong learning", "adversarial learning"], "TL;DR": "Extensive evaluation of the robustness of episodic lifelong learning algorithm under traditional adversarial attacks and the proposed gradient reversion attack. ", "abstract": "Lifelong  learning  aims  at  avoiding  the  catastrophic  forgetting  problem  of  traditional supervised learning  models.   Episodic memory based  lifelong learning methods such as A-GEM (Chaudhry et al., 2018b) are shown to achieve the state-of-the-art results across the benchmarks. In A-GEM, a small episodic memory is utilized to store a random subset of the examples from previous tasks.  While the model is trained on a new task, a reference gradient is computed on the episodic memory to guide the direction of the current update.  While A-GEM has strong continual learning ability,  it is not clear that if it can retain the performance in the presence of adversarial attacks.  In this paper, we examine the robustness ofA-GEM against adversarial attacks to the examples in the episodic memory.  We evaluate the effectiveness of traditional attack methods such as FGSM and PGD.The results show that A-GEM still possesses strong continual learning ability in the  presence  of  adversarial  examples  in  the  memory  and  simple  defense  techniques such as label smoothing can further alleviate the adversarial effects.  We presume that traditional attack methods are specially designed for standard supervised learning models rather than lifelong learning models. we therefore propose a principled way for attacking A-GEM called gradient reversion(GREV) which is shown to be more effective.  Our results indicate that future lifelong learning research should bear adversarial attacks in mind to develop more robust lifelong learning algorithms.", "pdf": "/pdf/ee84e3fa4383a1c2533006233d5cedf831a2aaca.pdf", "code": "https://drive.google.com/file/d/1zdSJ0aZR3KxoH_TDY1vMd5LFiDBS6v43/view?usp=sharing", "paperhash": "guo|attacking_lifelong_learning_models_with_gradient_reversion", "original_pdf": "/attachment/ee84e3fa4383a1c2533006233d5cedf831a2aaca.pdf", "_bibtex": "@misc{\nguo2020attacking,\ntitle={Attacking Lifelong Learning Models with Gradient Reversion},\nauthor={Yunhui Guo and Mingrui Liu and Yandong Li and Liqiang Wang and Tianbao Yang and Tajana Rosing},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlpy64tvB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "JXBb6KtsT4", "original": null, "number": 1, "cdate": 1576798693198, "ddate": null, "tcdate": 1576798693198, "tmdate": 1576800942218, "tddate": null, "forum": "SJlpy64tvB", "replyto": "SJlpy64tvB", "invitation": "ICLR.cc/2020/Conference/Paper319/-/Decision", "content": {"decision": "Reject", "comment": "The paper investigates questions around adversarial attacks in a continual learning algorithm, i.e., A-GEM. While reviewers agree that this is a novel topic of great importance, the contributions are quite narrow, since only a single model (A-GEM) is considered and it is not immediately clear whether this method transfers to other lifelong learning models (or even other models that belong to the same family as A-GEM). This is an interesting submission, but at the moment due to its very narrow scope, it seems more appropriate as a workshop submission investigating a very particular question (that of attacking A-GEM). As such, I cannot recommend acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Attacking Lifelong Learning Models with Gradient Reversion", "authors": ["Yunhui Guo", "Mingrui Liu", "Yandong Li", "Liqiang Wang", "Tianbao Yang", "Tajana Rosing"], "authorids": ["yug185@eng.ucsd.edu", "mingrui-liu@uiowa.edu", "lyndon.leeseu@outlook.com", "lwang@cs.ucf.edu", "tianbao-yang@uiowa.edu", "tajana@ucsd.edu"], "keywords": ["lifelong learning", "adversarial learning"], "TL;DR": "Extensive evaluation of the robustness of episodic lifelong learning algorithm under traditional adversarial attacks and the proposed gradient reversion attack. ", "abstract": "Lifelong  learning  aims  at  avoiding  the  catastrophic  forgetting  problem  of  traditional supervised learning  models.   Episodic memory based  lifelong learning methods such as A-GEM (Chaudhry et al., 2018b) are shown to achieve the state-of-the-art results across the benchmarks. In A-GEM, a small episodic memory is utilized to store a random subset of the examples from previous tasks.  While the model is trained on a new task, a reference gradient is computed on the episodic memory to guide the direction of the current update.  While A-GEM has strong continual learning ability,  it is not clear that if it can retain the performance in the presence of adversarial attacks.  In this paper, we examine the robustness ofA-GEM against adversarial attacks to the examples in the episodic memory.  We evaluate the effectiveness of traditional attack methods such as FGSM and PGD.The results show that A-GEM still possesses strong continual learning ability in the  presence  of  adversarial  examples  in  the  memory  and  simple  defense  techniques such as label smoothing can further alleviate the adversarial effects.  We presume that traditional attack methods are specially designed for standard supervised learning models rather than lifelong learning models. we therefore propose a principled way for attacking A-GEM called gradient reversion(GREV) which is shown to be more effective.  Our results indicate that future lifelong learning research should bear adversarial attacks in mind to develop more robust lifelong learning algorithms.", "pdf": "/pdf/ee84e3fa4383a1c2533006233d5cedf831a2aaca.pdf", "code": "https://drive.google.com/file/d/1zdSJ0aZR3KxoH_TDY1vMd5LFiDBS6v43/view?usp=sharing", "paperhash": "guo|attacking_lifelong_learning_models_with_gradient_reversion", "original_pdf": "/attachment/ee84e3fa4383a1c2533006233d5cedf831a2aaca.pdf", "_bibtex": "@misc{\nguo2020attacking,\ntitle={Attacking Lifelong Learning Models with Gradient Reversion},\nauthor={Yunhui Guo and Mingrui Liu and Yandong Li and Liqiang Wang and Tianbao Yang and Tajana Rosing},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlpy64tvB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJlpy64tvB", "replyto": "SJlpy64tvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795729840, "tmdate": 1576800282514, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper319/-/Decision"}}}, {"id": "BJxR4mo3FS", "original": null, "number": 1, "cdate": 1571758902249, "ddate": null, "tcdate": 1571758902249, "tmdate": 1572972610366, "tddate": null, "forum": "SJlpy64tvB", "replyto": "SJlpy64tvB", "invitation": "ICLR.cc/2020/Conference/Paper319/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper does a good job of raising awareness of adversarial attacks in lifelong learning research with deep neural networks. This is the first time I have considered this problem, but not sure whether any prior work exists in the specific subfield.\n\nAt the conceptual level, many issues can arise when a lifelong learner is attacked, since systematic negative bias could be introduced in the training process and may be very difficult to remove, given the tendency to 'remember everything' which dominates current approaches.\n\nThe paper isolates one lifelong learning approach (A-GEM) which is characteristic of one (of many) different approaches to lifelong learning, and investigates its robustness to standard adversarial attacks and a novel attack developed within this paper, which is stronger, but specific to episodic memory approaches.\n\nI cannot recommend acceptance at this point for the following reasons:\n1) I am not sure what I can generalize away from this paper to the immediate subfield and beyond. The paper claims that the investigated method is SOTA, but it's not clear this is the case, even in restricted class of similar episodic memory based models, see [1] for an independent evaluation of many such approaches. Is there any reasons why conclusions about this particular method are indeed representative of its class?\n2) While the paper does not explicitly make this claim, the title suggests that 'gradient reversion' attacks apply to lifelong learning models in general. Why is this class of approaches particularly informative such that conclusions may hold in general? Are other methods in this class more susceptible to these attacks and can the proposed attack be applied to the whole class, or even other types of approaches? This should be clarified!\n\n\nReferences\n[1] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, Tinne Tuytelaars,  Continual learning: A comparative study on how to defy forgetting in classification tasks, https://arxiv.org/abs/1909.08383"}, "signatures": ["ICLR.cc/2020/Conference/Paper319/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper319/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Attacking Lifelong Learning Models with Gradient Reversion", "authors": ["Yunhui Guo", "Mingrui Liu", "Yandong Li", "Liqiang Wang", "Tianbao Yang", "Tajana Rosing"], "authorids": ["yug185@eng.ucsd.edu", "mingrui-liu@uiowa.edu", "lyndon.leeseu@outlook.com", "lwang@cs.ucf.edu", "tianbao-yang@uiowa.edu", "tajana@ucsd.edu"], "keywords": ["lifelong learning", "adversarial learning"], "TL;DR": "Extensive evaluation of the robustness of episodic lifelong learning algorithm under traditional adversarial attacks and the proposed gradient reversion attack. ", "abstract": "Lifelong  learning  aims  at  avoiding  the  catastrophic  forgetting  problem  of  traditional supervised learning  models.   Episodic memory based  lifelong learning methods such as A-GEM (Chaudhry et al., 2018b) are shown to achieve the state-of-the-art results across the benchmarks. In A-GEM, a small episodic memory is utilized to store a random subset of the examples from previous tasks.  While the model is trained on a new task, a reference gradient is computed on the episodic memory to guide the direction of the current update.  While A-GEM has strong continual learning ability,  it is not clear that if it can retain the performance in the presence of adversarial attacks.  In this paper, we examine the robustness ofA-GEM against adversarial attacks to the examples in the episodic memory.  We evaluate the effectiveness of traditional attack methods such as FGSM and PGD.The results show that A-GEM still possesses strong continual learning ability in the  presence  of  adversarial  examples  in  the  memory  and  simple  defense  techniques such as label smoothing can further alleviate the adversarial effects.  We presume that traditional attack methods are specially designed for standard supervised learning models rather than lifelong learning models. we therefore propose a principled way for attacking A-GEM called gradient reversion(GREV) which is shown to be more effective.  Our results indicate that future lifelong learning research should bear adversarial attacks in mind to develop more robust lifelong learning algorithms.", "pdf": "/pdf/ee84e3fa4383a1c2533006233d5cedf831a2aaca.pdf", "code": "https://drive.google.com/file/d/1zdSJ0aZR3KxoH_TDY1vMd5LFiDBS6v43/view?usp=sharing", "paperhash": "guo|attacking_lifelong_learning_models_with_gradient_reversion", "original_pdf": "/attachment/ee84e3fa4383a1c2533006233d5cedf831a2aaca.pdf", "_bibtex": "@misc{\nguo2020attacking,\ntitle={Attacking Lifelong Learning Models with Gradient Reversion},\nauthor={Yunhui Guo and Mingrui Liu and Yandong Li and Liqiang Wang and Tianbao Yang and Tajana Rosing},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlpy64tvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJlpy64tvB", "replyto": "SJlpy64tvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper319/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper319/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576094631999, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper319/Reviewers"], "noninvitees": [], "tcdate": 1570237753853, "tmdate": 1576094632013, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper319/-/Official_Review"}}}, {"id": "B1x9LHd6tr", "original": null, "number": 2, "cdate": 1571812689604, "ddate": null, "tcdate": 1571812689604, "tmdate": 1572972610324, "tddate": null, "forum": "SJlpy64tvB", "replyto": "SJlpy64tvB", "invitation": "ICLR.cc/2020/Conference/Paper319/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposed a novel approach for robust continual learning model from the adversarial attack. The authors start from one of the state-of-the-art episodic memory based continual learning method, A-GEM. The proposed method, Gradient Reversion (GREV), is specialized on A-GEM. The method perturbed the episodic memory examples on A-GEM, thus modifies the direction of reference gradient. While conventional attack like Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) hardly show the their influence on A-GEM, GREV significantly attacks the performance.\n\n\nThe paper is well written, and easy to follow. Also, attack technique on episodic memory based continual learning is interesting and would be valuable. \n\nBut, I feel that some of the analysis are obvious which are not much meaningful to analyze the model,  and the overall contributions are suggested under A-GEM model, while not to cover generic other episodic-based continual learning. \n\nSo, I hesitate to give the high score even the approach is interesting.\n\nAdditional one question.\nI didn't get the concrete reasons that A-GEM is already robust for famous attack methods. What\u2019s the reason that A-GEM is robust for them?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper319/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper319/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Attacking Lifelong Learning Models with Gradient Reversion", "authors": ["Yunhui Guo", "Mingrui Liu", "Yandong Li", "Liqiang Wang", "Tianbao Yang", "Tajana Rosing"], "authorids": ["yug185@eng.ucsd.edu", "mingrui-liu@uiowa.edu", "lyndon.leeseu@outlook.com", "lwang@cs.ucf.edu", "tianbao-yang@uiowa.edu", "tajana@ucsd.edu"], "keywords": ["lifelong learning", "adversarial learning"], "TL;DR": "Extensive evaluation of the robustness of episodic lifelong learning algorithm under traditional adversarial attacks and the proposed gradient reversion attack. ", "abstract": "Lifelong  learning  aims  at  avoiding  the  catastrophic  forgetting  problem  of  traditional supervised learning  models.   Episodic memory based  lifelong learning methods such as A-GEM (Chaudhry et al., 2018b) are shown to achieve the state-of-the-art results across the benchmarks. In A-GEM, a small episodic memory is utilized to store a random subset of the examples from previous tasks.  While the model is trained on a new task, a reference gradient is computed on the episodic memory to guide the direction of the current update.  While A-GEM has strong continual learning ability,  it is not clear that if it can retain the performance in the presence of adversarial attacks.  In this paper, we examine the robustness ofA-GEM against adversarial attacks to the examples in the episodic memory.  We evaluate the effectiveness of traditional attack methods such as FGSM and PGD.The results show that A-GEM still possesses strong continual learning ability in the  presence  of  adversarial  examples  in  the  memory  and  simple  defense  techniques such as label smoothing can further alleviate the adversarial effects.  We presume that traditional attack methods are specially designed for standard supervised learning models rather than lifelong learning models. we therefore propose a principled way for attacking A-GEM called gradient reversion(GREV) which is shown to be more effective.  Our results indicate that future lifelong learning research should bear adversarial attacks in mind to develop more robust lifelong learning algorithms.", "pdf": "/pdf/ee84e3fa4383a1c2533006233d5cedf831a2aaca.pdf", "code": "https://drive.google.com/file/d/1zdSJ0aZR3KxoH_TDY1vMd5LFiDBS6v43/view?usp=sharing", "paperhash": "guo|attacking_lifelong_learning_models_with_gradient_reversion", "original_pdf": "/attachment/ee84e3fa4383a1c2533006233d5cedf831a2aaca.pdf", "_bibtex": "@misc{\nguo2020attacking,\ntitle={Attacking Lifelong Learning Models with Gradient Reversion},\nauthor={Yunhui Guo and Mingrui Liu and Yandong Li and Liqiang Wang and Tianbao Yang and Tajana Rosing},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlpy64tvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJlpy64tvB", "replyto": "SJlpy64tvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper319/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper319/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576094631999, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper319/Reviewers"], "noninvitees": [], "tcdate": 1570237753853, "tmdate": 1576094632013, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper319/-/Official_Review"}}}, {"id": "SJxFkDtf5H", "original": null, "number": 3, "cdate": 1572144865281, "ddate": null, "tcdate": 1572144865281, "tmdate": 1572972610282, "tddate": null, "forum": "SJlpy64tvB", "replyto": "SJlpy64tvB", "invitation": "ICLR.cc/2020/Conference/Paper319/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "General:\nThe paper first proposes an adversarial attack on the exemplar-based continual learning algorithm, A-GEM. The problem formulation is new and interesting, but I am not sure how practical or realistic the setting is. The attacker assumes to have access not only to the model but also to the episodic memory. I think that is quite a powerful assumption for the attacker and it is not very surprising that the attack would work. So, I am a bit torn with the judgments. \n\nSummary & Pro:\n1. First proposal of adversarial attack of continual learning algorithm. \n2. The conventional attack schemes are shown to not work well, and they devised new one (GREV) tailored for A-GEM. \n3. Experimental results show convincing results that their method works well. \n\nCon & Questions: \n1. It is not clear whether the proposed method will also work well for other exemplar-based methods like iCaRL or GEM (the simpler version than A-GEM), etc. I get that A-GEM can be attacked by their assumption and method, but how general is it?\n2. What exactly is the practical scenario of this method? How can the attacker get access to the model & memory? In the traditional adversarial attack literature, it is shown that white-box attack can also lead to the black-box attack. But, in this case, I am not sure about the practical implication of the proposed methods. \n3. It seems like the entire memory is under attack. What happens when only the memory is partially attacked, e.g., 10% of the data in the memory is attacked? \n4. Table 1/2 only shows the overall average accuracy. Can you also show the per-task average accuracy curves? It would be much better to see such curves to clearly see the effect of the attack rather than the overall average. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper319/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper319/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Attacking Lifelong Learning Models with Gradient Reversion", "authors": ["Yunhui Guo", "Mingrui Liu", "Yandong Li", "Liqiang Wang", "Tianbao Yang", "Tajana Rosing"], "authorids": ["yug185@eng.ucsd.edu", "mingrui-liu@uiowa.edu", "lyndon.leeseu@outlook.com", "lwang@cs.ucf.edu", "tianbao-yang@uiowa.edu", "tajana@ucsd.edu"], "keywords": ["lifelong learning", "adversarial learning"], "TL;DR": "Extensive evaluation of the robustness of episodic lifelong learning algorithm under traditional adversarial attacks and the proposed gradient reversion attack. ", "abstract": "Lifelong  learning  aims  at  avoiding  the  catastrophic  forgetting  problem  of  traditional supervised learning  models.   Episodic memory based  lifelong learning methods such as A-GEM (Chaudhry et al., 2018b) are shown to achieve the state-of-the-art results across the benchmarks. In A-GEM, a small episodic memory is utilized to store a random subset of the examples from previous tasks.  While the model is trained on a new task, a reference gradient is computed on the episodic memory to guide the direction of the current update.  While A-GEM has strong continual learning ability,  it is not clear that if it can retain the performance in the presence of adversarial attacks.  In this paper, we examine the robustness ofA-GEM against adversarial attacks to the examples in the episodic memory.  We evaluate the effectiveness of traditional attack methods such as FGSM and PGD.The results show that A-GEM still possesses strong continual learning ability in the  presence  of  adversarial  examples  in  the  memory  and  simple  defense  techniques such as label smoothing can further alleviate the adversarial effects.  We presume that traditional attack methods are specially designed for standard supervised learning models rather than lifelong learning models. we therefore propose a principled way for attacking A-GEM called gradient reversion(GREV) which is shown to be more effective.  Our results indicate that future lifelong learning research should bear adversarial attacks in mind to develop more robust lifelong learning algorithms.", "pdf": "/pdf/ee84e3fa4383a1c2533006233d5cedf831a2aaca.pdf", "code": "https://drive.google.com/file/d/1zdSJ0aZR3KxoH_TDY1vMd5LFiDBS6v43/view?usp=sharing", "paperhash": "guo|attacking_lifelong_learning_models_with_gradient_reversion", "original_pdf": "/attachment/ee84e3fa4383a1c2533006233d5cedf831a2aaca.pdf", "_bibtex": "@misc{\nguo2020attacking,\ntitle={Attacking Lifelong Learning Models with Gradient Reversion},\nauthor={Yunhui Guo and Mingrui Liu and Yandong Li and Liqiang Wang and Tianbao Yang and Tajana Rosing},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlpy64tvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJlpy64tvB", "replyto": "SJlpy64tvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper319/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper319/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576094631999, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper319/Reviewers"], "noninvitees": [], "tcdate": 1570237753853, "tmdate": 1576094632013, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper319/-/Official_Review"}}}], "count": 5}