{"notes": [{"id": "S1lRg0VKDr", "original": "HJlf0TXuwr", "number": 948, "cdate": 1569439222268, "ddate": null, "tcdate": 1569439222268, "tmdate": 1577168252596, "tddate": null, "forum": "S1lRg0VKDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "On summarized validation curves and generalization", "authors": ["Mohammad Hashir", "Yoshua Bengio", "Joseph Paul Cohen"], "authorids": ["mohammad.hashir.khan@umontreal.ca", "yoshua.bengio@mila.quebec", "joseph@josephpcohen.com"], "keywords": ["model selection", "deep learning", "early stopping", "validation curves"], "abstract": "The validation curve is widely used for model selection and hyper-parameter search with the curve usually summarized over all the training tasks. However, this summarization tends to lose the intricacies of the per-task curves and it isn't able to reflect if all the tasks are at their validation optimum even if the summarized curve might be. In this work, we explore this loss of information, how it affects the model at testing and how to detect it using interval plots. We propose two techniques as a proof-of-concept of the potential gain in the test performance when per-task validation curves are accounted for. Our experiments on three large datasets show up to a 2.5% increase (averaged over multiple trials) in the test accuracy rate when model selection uses the per-task validation maximums instead of the summarized validation maximum. This potential increase is not a result of any modification to the model but rather at what point of training the weights were selected from. This presents an exciting direction for new training and model selection techniques that rely on more than just averaged metrics. ", "pdf": "/pdf/d7a671f64111b00df78c9d0fc274087caacc7803.pdf", "paperhash": "hashir|on_summarized_validation_curves_and_generalization", "original_pdf": "/attachment/c0e71864f4f01447dd90ca4153289eb9cc03bc2c.pdf", "_bibtex": "@misc{\nhashir2020on,\ntitle={On summarized validation curves and generalization},\nauthor={Mohammad Hashir and Yoshua Bengio and Joseph Paul Cohen},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lRg0VKDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "ge6BRZsgr", "original": null, "number": 1, "cdate": 1576798710475, "ddate": null, "tcdate": 1576798710475, "tmdate": 1576800925868, "tddate": null, "forum": "S1lRg0VKDr", "replyto": "S1lRg0VKDr", "invitation": "ICLR.cc/2020/Conference/Paper948/-/Decision", "content": {"decision": "Reject", "comment": "The reviewers reached a consensus that the paper is preliminary and has a very limited contribution. Therefore, I cannot recommend acceptance at this time.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On summarized validation curves and generalization", "authors": ["Mohammad Hashir", "Yoshua Bengio", "Joseph Paul Cohen"], "authorids": ["mohammad.hashir.khan@umontreal.ca", "yoshua.bengio@mila.quebec", "joseph@josephpcohen.com"], "keywords": ["model selection", "deep learning", "early stopping", "validation curves"], "abstract": "The validation curve is widely used for model selection and hyper-parameter search with the curve usually summarized over all the training tasks. However, this summarization tends to lose the intricacies of the per-task curves and it isn't able to reflect if all the tasks are at their validation optimum even if the summarized curve might be. In this work, we explore this loss of information, how it affects the model at testing and how to detect it using interval plots. We propose two techniques as a proof-of-concept of the potential gain in the test performance when per-task validation curves are accounted for. Our experiments on three large datasets show up to a 2.5% increase (averaged over multiple trials) in the test accuracy rate when model selection uses the per-task validation maximums instead of the summarized validation maximum. This potential increase is not a result of any modification to the model but rather at what point of training the weights were selected from. This presents an exciting direction for new training and model selection techniques that rely on more than just averaged metrics. ", "pdf": "/pdf/d7a671f64111b00df78c9d0fc274087caacc7803.pdf", "paperhash": "hashir|on_summarized_validation_curves_and_generalization", "original_pdf": "/attachment/c0e71864f4f01447dd90ca4153289eb9cc03bc2c.pdf", "_bibtex": "@misc{\nhashir2020on,\ntitle={On summarized validation curves and generalization},\nauthor={Mohammad Hashir and Yoshua Bengio and Joseph Paul Cohen},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lRg0VKDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1lRg0VKDr", "replyto": "S1lRg0VKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795717995, "tmdate": 1576800268402, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper948/-/Decision"}}}, {"id": "BkgjwrSBtB", "original": null, "number": 1, "cdate": 1571276131396, "ddate": null, "tcdate": 1571276131396, "tmdate": 1574194198583, "tddate": null, "forum": "S1lRg0VKDr", "replyto": "S1lRg0VKDr", "invitation": "ICLR.cc/2020/Conference/Paper948/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "The paper examines the common practice of performing model selection by choosing the model that maximizes validation accuracy. In a setting where there are multiple tasks, the average validation error hides performance on individual tasks, which may be relevant. The paper casts multi-class image classification as a multi-task problem, where identifying each different class is a different task.\n\nAt different points of training, a model's performance on a task will vary. To make it easier to examine this, they propose displaying an interval plot. The validation accuracy for a single task will be largest at a specific epoch. For each single task, display the epochs where performance is within some threshold of the best validation-set accuracy for that task. Plots on CIFAR-100 demonstrate how some tasks are learned quickly then forgotten, while other tasks are only learned late.\n\nTo examine the difference in performance, they compare the single best CIFAR-100 model, to the classifier defined by \"for class k, forward input to the model with best validation accuracy for class k\". This gives about +2.5% in Top-1 performance, a similar story appears on the other 2 datasets they try (Tiny ImageNet, PadChest). This requires N models, where N is the number of classes. They also examine a clustering approach, where they cluster the N models into K models (using K-means), then forward class k to the best model out of those K models.\n\nOverall, it isn't an especially large contribution but I felt it had some interesting points. Some comments:\n* Citation list feels short. The catastrophic forgetting literature seems relevant here. Rich Caruana's multitask learning thesis (Caruana et al 1997) is also relevant.\n* It's unclear how the K-means is carried out. I assume the features used for each model checkpoint are tied to its performance on individual task but it's never spell out what distance metric is used in the K-means.\n* It feels strange to have no comparisons to ensemble-based baselines. The baseline here would be, for some parameter K, run K independent training runs, take the top average validation accuracy from each run, and average them together, then compare this to the K models found from K-means clustering. For small enough K (like 5 or 10) this seems like a feasible experiment, computation-wise.\n* In a similar vein, I wonder how this compares to the Snapshot Ensembles paper, which has a similar guarantee of doing 1 training run and giving M models for an ensemble. Is the gain from ensembling, or from directly examining which models are better at which classes?\n* A natural follow-up here is to use model distillation to distill the N best models for each individual class into a single model checkpoint - does this give us a better single model checkpoint?\n\nAs-is, I give this paper a weak reject, but would be willing to increase if some of these experiments were tried.\n\nEdit: having read the other reviews and author comments, I still maintain a weak reject rating. I believe this paper needs further work.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper948/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper948/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On summarized validation curves and generalization", "authors": ["Mohammad Hashir", "Yoshua Bengio", "Joseph Paul Cohen"], "authorids": ["mohammad.hashir.khan@umontreal.ca", "yoshua.bengio@mila.quebec", "joseph@josephpcohen.com"], "keywords": ["model selection", "deep learning", "early stopping", "validation curves"], "abstract": "The validation curve is widely used for model selection and hyper-parameter search with the curve usually summarized over all the training tasks. However, this summarization tends to lose the intricacies of the per-task curves and it isn't able to reflect if all the tasks are at their validation optimum even if the summarized curve might be. In this work, we explore this loss of information, how it affects the model at testing and how to detect it using interval plots. We propose two techniques as a proof-of-concept of the potential gain in the test performance when per-task validation curves are accounted for. Our experiments on three large datasets show up to a 2.5% increase (averaged over multiple trials) in the test accuracy rate when model selection uses the per-task validation maximums instead of the summarized validation maximum. This potential increase is not a result of any modification to the model but rather at what point of training the weights were selected from. This presents an exciting direction for new training and model selection techniques that rely on more than just averaged metrics. ", "pdf": "/pdf/d7a671f64111b00df78c9d0fc274087caacc7803.pdf", "paperhash": "hashir|on_summarized_validation_curves_and_generalization", "original_pdf": "/attachment/c0e71864f4f01447dd90ca4153289eb9cc03bc2c.pdf", "_bibtex": "@misc{\nhashir2020on,\ntitle={On summarized validation curves and generalization},\nauthor={Mohammad Hashir and Yoshua Bengio and Joseph Paul Cohen},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lRg0VKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1lRg0VKDr", "replyto": "S1lRg0VKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper948/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper948/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575068860269, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper948/Reviewers"], "noninvitees": [], "tcdate": 1570237744630, "tmdate": 1575068860281, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper948/-/Official_Review"}}}, {"id": "rJg9G_F3sB", "original": null, "number": 5, "cdate": 1573849105794, "ddate": null, "tcdate": 1573849105794, "tmdate": 1573849105794, "tddate": null, "forum": "S1lRg0VKDr", "replyto": "rJewCQJqjS", "invitation": "ICLR.cc/2020/Conference/Paper948/-/Official_Comment", "content": {"title": "Thank you", "comment": "Thank you for your reply. From your comments, we do think now that the paper would benefit from including these baselines but it won't be possible to update this paper due to the revision deadline. Do you have any more suggestions on baselines, especially any that focus particularly on the validation accuracy?\n\nIn your definition of an ensemble, can you clarify this small thing: is it that the train/val split remains the same but the model initialization is different between the training runs? Or it doesn't matter?"}, "signatures": ["ICLR.cc/2020/Conference/Paper948/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper948/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On summarized validation curves and generalization", "authors": ["Mohammad Hashir", "Yoshua Bengio", "Joseph Paul Cohen"], "authorids": ["mohammad.hashir.khan@umontreal.ca", "yoshua.bengio@mila.quebec", "joseph@josephpcohen.com"], "keywords": ["model selection", "deep learning", "early stopping", "validation curves"], "abstract": "The validation curve is widely used for model selection and hyper-parameter search with the curve usually summarized over all the training tasks. However, this summarization tends to lose the intricacies of the per-task curves and it isn't able to reflect if all the tasks are at their validation optimum even if the summarized curve might be. In this work, we explore this loss of information, how it affects the model at testing and how to detect it using interval plots. We propose two techniques as a proof-of-concept of the potential gain in the test performance when per-task validation curves are accounted for. Our experiments on three large datasets show up to a 2.5% increase (averaged over multiple trials) in the test accuracy rate when model selection uses the per-task validation maximums instead of the summarized validation maximum. This potential increase is not a result of any modification to the model but rather at what point of training the weights were selected from. This presents an exciting direction for new training and model selection techniques that rely on more than just averaged metrics. ", "pdf": "/pdf/d7a671f64111b00df78c9d0fc274087caacc7803.pdf", "paperhash": "hashir|on_summarized_validation_curves_and_generalization", "original_pdf": "/attachment/c0e71864f4f01447dd90ca4153289eb9cc03bc2c.pdf", "_bibtex": "@misc{\nhashir2020on,\ntitle={On summarized validation curves and generalization},\nauthor={Mohammad Hashir and Yoshua Bengio and Joseph Paul Cohen},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lRg0VKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lRg0VKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper948/Authors", "ICLR.cc/2020/Conference/Paper948/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper948/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper948/Reviewers", "ICLR.cc/2020/Conference/Paper948/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper948/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper948/Authors|ICLR.cc/2020/Conference/Paper948/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163629, "tmdate": 1576860546321, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper948/Authors", "ICLR.cc/2020/Conference/Paper948/Reviewers", "ICLR.cc/2020/Conference/Paper948/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper948/-/Official_Comment"}}}, {"id": "rJewCQJqjS", "original": null, "number": 4, "cdate": 1573675982811, "ddate": null, "tcdate": 1573675982811, "tmdate": 1573675982811, "tddate": null, "forum": "S1lRg0VKDr", "replyto": "ByxcLX3YiS", "invitation": "ICLR.cc/2020/Conference/Paper948/-/Official_Comment", "content": {"title": "Thanks for your reply", "comment": "Thanks for your reply. Although the comparisons aren't directly equitable, I don't think this means you should avoid them entirely, as inequitable comparisons still provide signal on why your proposed approach helps.\n\nI believe the Table 1 numbers compare K=3 seeds of regular training and K=3 seeds of brute-force comparison. What I wanted was 3 seeds of regular training, defining the ensemble by the average of their predictions, then comparing this to a K-means reduction of the models, rather than to the brute force model. This ensemble averaging is different from just averaging the accuracy of the 3 random seeds (we expect the ensemble of 3 to outperform any individual model.)"}, "signatures": ["ICLR.cc/2020/Conference/Paper948/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper948/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On summarized validation curves and generalization", "authors": ["Mohammad Hashir", "Yoshua Bengio", "Joseph Paul Cohen"], "authorids": ["mohammad.hashir.khan@umontreal.ca", "yoshua.bengio@mila.quebec", "joseph@josephpcohen.com"], "keywords": ["model selection", "deep learning", "early stopping", "validation curves"], "abstract": "The validation curve is widely used for model selection and hyper-parameter search with the curve usually summarized over all the training tasks. However, this summarization tends to lose the intricacies of the per-task curves and it isn't able to reflect if all the tasks are at their validation optimum even if the summarized curve might be. In this work, we explore this loss of information, how it affects the model at testing and how to detect it using interval plots. We propose two techniques as a proof-of-concept of the potential gain in the test performance when per-task validation curves are accounted for. Our experiments on three large datasets show up to a 2.5% increase (averaged over multiple trials) in the test accuracy rate when model selection uses the per-task validation maximums instead of the summarized validation maximum. This potential increase is not a result of any modification to the model but rather at what point of training the weights were selected from. This presents an exciting direction for new training and model selection techniques that rely on more than just averaged metrics. ", "pdf": "/pdf/d7a671f64111b00df78c9d0fc274087caacc7803.pdf", "paperhash": "hashir|on_summarized_validation_curves_and_generalization", "original_pdf": "/attachment/c0e71864f4f01447dd90ca4153289eb9cc03bc2c.pdf", "_bibtex": "@misc{\nhashir2020on,\ntitle={On summarized validation curves and generalization},\nauthor={Mohammad Hashir and Yoshua Bengio and Joseph Paul Cohen},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lRg0VKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lRg0VKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper948/Authors", "ICLR.cc/2020/Conference/Paper948/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper948/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper948/Reviewers", "ICLR.cc/2020/Conference/Paper948/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper948/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper948/Authors|ICLR.cc/2020/Conference/Paper948/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163629, "tmdate": 1576860546321, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper948/Authors", "ICLR.cc/2020/Conference/Paper948/Reviewers", "ICLR.cc/2020/Conference/Paper948/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper948/-/Official_Comment"}}}, {"id": "ByxcLX3YiS", "original": null, "number": 3, "cdate": 1573663569968, "ddate": null, "tcdate": 1573663569968, "tmdate": 1573663569968, "tddate": null, "forum": "S1lRg0VKDr", "replyto": "BkgjwrSBtB", "invitation": "ICLR.cc/2020/Conference/Paper948/-/Official_Comment", "content": {"title": "Thank you for your comments", "comment": "> Citation list feels short. The catastrophic forgetting literature seems relevant here. Rich Caruana's multitask learning thesis (Caruana et al 1997) is also relevant.\n\nWe\u2019ve added the relevant literature.\n\n> It's unclear how the K-means is carried out. I assume the features used for each model checkpoint are tied to its performance on individual task but it's never spell out what distance metric is used in the K-means.\n\nWe use each task\u2019s optimal valid epoch t_i as the distance metric\n\n> It feels strange to have no comparisons to ensemble-based baselines. The baseline here would be, for some parameter K, run K independent training runs, take the top average validation accuracy from each run, and average them together, then compare this to the K models found from K-means clustering. For small enough K (like 5 or 10) this seems like a feasible experiment, computation-wise.\n\nThis does happen to be our baseline, with K=3. The numbers in Table 1 have been averaged over 3 independent runs of different train/valid splits and model initializations. However, the test set for CIFAR100 and TinyImagenet remained the same for all runs. On examining Fig 4, we expect it to be worse than the baseline possibly due to the noisy nature of SGD training (which we address in Sec 5.2, second paragraph). \n\n> In a similar vein, I wonder how this compares to the Snapshot Ensembles paper, which has a similar guarantee of doing 1 training run and giving M models for an ensemble. Is the gain from ensembling, or from directly examining which models are better at which classes? A natural follow-up here is to use model distillation to distill the N best models for each individual class into a single model checkpoint - does this give us a better single model checkpoint?\n\nWe are hesitant to include baselines that involves modifying the training procedure, as our analysis is done post-training so we don\u2019t think it\u2019d be an equitable comparison. We\u2019ve elaborated on our reasons in more detail in response to Reviewer#2\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper948/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper948/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On summarized validation curves and generalization", "authors": ["Mohammad Hashir", "Yoshua Bengio", "Joseph Paul Cohen"], "authorids": ["mohammad.hashir.khan@umontreal.ca", "yoshua.bengio@mila.quebec", "joseph@josephpcohen.com"], "keywords": ["model selection", "deep learning", "early stopping", "validation curves"], "abstract": "The validation curve is widely used for model selection and hyper-parameter search with the curve usually summarized over all the training tasks. However, this summarization tends to lose the intricacies of the per-task curves and it isn't able to reflect if all the tasks are at their validation optimum even if the summarized curve might be. In this work, we explore this loss of information, how it affects the model at testing and how to detect it using interval plots. We propose two techniques as a proof-of-concept of the potential gain in the test performance when per-task validation curves are accounted for. Our experiments on three large datasets show up to a 2.5% increase (averaged over multiple trials) in the test accuracy rate when model selection uses the per-task validation maximums instead of the summarized validation maximum. This potential increase is not a result of any modification to the model but rather at what point of training the weights were selected from. This presents an exciting direction for new training and model selection techniques that rely on more than just averaged metrics. ", "pdf": "/pdf/d7a671f64111b00df78c9d0fc274087caacc7803.pdf", "paperhash": "hashir|on_summarized_validation_curves_and_generalization", "original_pdf": "/attachment/c0e71864f4f01447dd90ca4153289eb9cc03bc2c.pdf", "_bibtex": "@misc{\nhashir2020on,\ntitle={On summarized validation curves and generalization},\nauthor={Mohammad Hashir and Yoshua Bengio and Joseph Paul Cohen},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lRg0VKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lRg0VKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper948/Authors", "ICLR.cc/2020/Conference/Paper948/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper948/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper948/Reviewers", "ICLR.cc/2020/Conference/Paper948/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper948/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper948/Authors|ICLR.cc/2020/Conference/Paper948/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163629, "tmdate": 1576860546321, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper948/Authors", "ICLR.cc/2020/Conference/Paper948/Reviewers", "ICLR.cc/2020/Conference/Paper948/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper948/-/Official_Comment"}}}, {"id": "SygGpMhtsr", "original": null, "number": 2, "cdate": 1573663418193, "ddate": null, "tcdate": 1573663418193, "tmdate": 1573663418193, "tddate": null, "forum": "S1lRg0VKDr", "replyto": "Bkg_5_6hYr", "invitation": "ICLR.cc/2020/Conference/Paper948/-/Official_Comment", "content": {"title": "Thank you for your comments", "comment": "We tried to include more baselines and ultimately chose not to because: \n(i) These baseline methods are done during training and (most) don\u2019t involve validation metrics as the main aspect of their approaches. We perform our analysis post-training with all the focus on validation metrics.\n(ii) We only wanted to consider how the weights change with time keeping all else constant and how these changes affect the validation and test performance. As these baseline methods modify the gradients wrt several factors during the training, it would add more degrees of freedom and would be difficult to compare. \nBecause of these two reasons, it felt it wouldn\u2019t be an equitable comparison. Moreover, we\u2019ve been unable to find any research papers in this niche that we could implement as a baseline.\n\n> How much the problem is due to label imbalancing? \nThere is no imbalance in CIFAR100 and TinyImagenet. PadChest does have an imbalance, but the network had been trained with class weights. \n\n> The proposed approach is not very interesting, brutal force and clustering. They are very straightforward.\nYes, we do remark on the naive and inefficient nature of the two methods we use. We\u2019d like to state that these methods themselves aren\u2019t intended as a contribution; rather their results are used to develop our main contribution: that simply using averaged metrics doesn\u2019t guarantee the best testing performance and there is a potential increase if per-task validation metrics are used. We\u2019ve revised the paper to make sure this is reflected better.\n\nComments not affect rating\nThe paper uses \"task\" which is not defined.\nWe mention our definition of a task in the footnote of Page 2. We\u2019ve moved it to the beginning of the section\n\nWe believe the main reason for your rejection decision was due to the work not including necessary baselines. As we explained above, we believe there are no equitable comparisons. Please let us know if you do not agree.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper948/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper948/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On summarized validation curves and generalization", "authors": ["Mohammad Hashir", "Yoshua Bengio", "Joseph Paul Cohen"], "authorids": ["mohammad.hashir.khan@umontreal.ca", "yoshua.bengio@mila.quebec", "joseph@josephpcohen.com"], "keywords": ["model selection", "deep learning", "early stopping", "validation curves"], "abstract": "The validation curve is widely used for model selection and hyper-parameter search with the curve usually summarized over all the training tasks. However, this summarization tends to lose the intricacies of the per-task curves and it isn't able to reflect if all the tasks are at their validation optimum even if the summarized curve might be. In this work, we explore this loss of information, how it affects the model at testing and how to detect it using interval plots. We propose two techniques as a proof-of-concept of the potential gain in the test performance when per-task validation curves are accounted for. Our experiments on three large datasets show up to a 2.5% increase (averaged over multiple trials) in the test accuracy rate when model selection uses the per-task validation maximums instead of the summarized validation maximum. This potential increase is not a result of any modification to the model but rather at what point of training the weights were selected from. This presents an exciting direction for new training and model selection techniques that rely on more than just averaged metrics. ", "pdf": "/pdf/d7a671f64111b00df78c9d0fc274087caacc7803.pdf", "paperhash": "hashir|on_summarized_validation_curves_and_generalization", "original_pdf": "/attachment/c0e71864f4f01447dd90ca4153289eb9cc03bc2c.pdf", "_bibtex": "@misc{\nhashir2020on,\ntitle={On summarized validation curves and generalization},\nauthor={Mohammad Hashir and Yoshua Bengio and Joseph Paul Cohen},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lRg0VKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lRg0VKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper948/Authors", "ICLR.cc/2020/Conference/Paper948/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper948/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper948/Reviewers", "ICLR.cc/2020/Conference/Paper948/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper948/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper948/Authors|ICLR.cc/2020/Conference/Paper948/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163629, "tmdate": 1576860546321, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper948/Authors", "ICLR.cc/2020/Conference/Paper948/Reviewers", "ICLR.cc/2020/Conference/Paper948/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper948/-/Official_Comment"}}}, {"id": "B1lzvMntjS", "original": null, "number": 1, "cdate": 1573663321601, "ddate": null, "tcdate": 1573663321601, "tmdate": 1573663321601, "tddate": null, "forum": "S1lRg0VKDr", "replyto": "r1xyacKWcH", "invitation": "ICLR.cc/2020/Conference/Paper948/-/Official_Comment", "content": {"title": "Thank you for your comments", "comment": "We believe that the contribution of the paper has been misunderstood. The main message of the paper is that using a summarized validation metric for a given purpose tends to leave out nuances in how each task has been performing throughout training. We demonstrate that accounting for these nuances can lead to better model selection and hence a better test performance through two simple methods. We\u2019d like to stress two things - \n\n(i) The two methods themselves are not a contribution; rather we use the results of these methods to support the contribution\n\n(ii) Model selection is the specific use case we chose for this paper to test our hypothesis. Our contribution can also apply/be tested for other purposes, such as reducing the learning rate based on the validation metric.\n\n> Also there seems to be some confusion re: what accuracy means and is used in some places instead of precision.  (eg in. Sect. 4.1)\n\nThis is unclear, where should we have used the word precision?\n\n> One question I had was to think about is how one would know what model to apply to what samples if they don't know a priori the labels of these samples. This seems to be something your suggested approaches rely on.\n\nThis is a very valid question and a major shortcoming of the two methods. To answer the question, it depends on what level of abstraction of task are the multiple models based on. If prediction tasks are independent, such as in the case of multi-label (like PadChest), then every model needs to be applied. It is similar for the general definition of the multi-task setting (eg. classification, segmentation etc). But for multi-class, it\u2019d be difficult to determine. One could extract just the logit of the class produced by its particular valid-optimal model and take the softmax over the vector of these \u201cvalid-optimal\u201d logits. \nHowever, we\u2019d like to reiterate that our suggested approaches serve as only a proof-of-concept of the gain in performance when per-task metrics are accounted for. These methods aren\u2019t intended to be adopted in current practice but their results are intended as a \u201cstimulus for increasing research\u201d in the area of examining the subtleties in validation curves w.r.t. individual tasks and also as a \u201cbaseline\u201d for any future work in this area.\n\nWe understand there was ambiguity in the paper regarding the exact contribution. We hope our response makes it more clear and we have revised the paper to make sure it isn\u2019t ambiguous anymore. Have we addressed all of your concerns?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper948/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper948/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On summarized validation curves and generalization", "authors": ["Mohammad Hashir", "Yoshua Bengio", "Joseph Paul Cohen"], "authorids": ["mohammad.hashir.khan@umontreal.ca", "yoshua.bengio@mila.quebec", "joseph@josephpcohen.com"], "keywords": ["model selection", "deep learning", "early stopping", "validation curves"], "abstract": "The validation curve is widely used for model selection and hyper-parameter search with the curve usually summarized over all the training tasks. However, this summarization tends to lose the intricacies of the per-task curves and it isn't able to reflect if all the tasks are at their validation optimum even if the summarized curve might be. In this work, we explore this loss of information, how it affects the model at testing and how to detect it using interval plots. We propose two techniques as a proof-of-concept of the potential gain in the test performance when per-task validation curves are accounted for. Our experiments on three large datasets show up to a 2.5% increase (averaged over multiple trials) in the test accuracy rate when model selection uses the per-task validation maximums instead of the summarized validation maximum. This potential increase is not a result of any modification to the model but rather at what point of training the weights were selected from. This presents an exciting direction for new training and model selection techniques that rely on more than just averaged metrics. ", "pdf": "/pdf/d7a671f64111b00df78c9d0fc274087caacc7803.pdf", "paperhash": "hashir|on_summarized_validation_curves_and_generalization", "original_pdf": "/attachment/c0e71864f4f01447dd90ca4153289eb9cc03bc2c.pdf", "_bibtex": "@misc{\nhashir2020on,\ntitle={On summarized validation curves and generalization},\nauthor={Mohammad Hashir and Yoshua Bengio and Joseph Paul Cohen},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lRg0VKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lRg0VKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper948/Authors", "ICLR.cc/2020/Conference/Paper948/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper948/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper948/Reviewers", "ICLR.cc/2020/Conference/Paper948/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper948/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper948/Authors|ICLR.cc/2020/Conference/Paper948/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163629, "tmdate": 1576860546321, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper948/Authors", "ICLR.cc/2020/Conference/Paper948/Reviewers", "ICLR.cc/2020/Conference/Paper948/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper948/-/Official_Comment"}}}, {"id": "Bkg_5_6hYr", "original": null, "number": 2, "cdate": 1571768463746, "ddate": null, "tcdate": 1571768463746, "tmdate": 1572972531982, "tddate": null, "forum": "S1lRg0VKDr", "replyto": "S1lRg0VKDr", "invitation": "ICLR.cc/2020/Conference/Paper948/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary\nModel validation curve typically aggregates accuracies of all labels. This paper investigates the fine-grained per-label model validation curve. It shows that the optimal epoch varies by label. The paper proposes a visualization method to detect if there is a disparity between the per-label curves and the summarized validation curve. It also proposes two methods to exploit per-label metrics into model evaluation and selection. The experiments use three datasets: CIFAR 100, Tiny ImageNet, PadChest.\n\nLimitations\nThe paper is very preliminary in nature. It does not compare with other related work. For example,  the task prioritization during training approach where tasks dynamically change priority or are regularized in some way. It will be good to see how the proposed approach compare with other related ones (three listed in related work) in the literature. \n\nOther approaches not mentioned in the paper can be more effective. For example, \nFocal Loss for Dense Object Detection, https://arxiv.org/abs/1708.02002\nmight automatically handle the problem to a large extent.\n\nHow much the problem is due to label imbalancing? If label imbalancing is one main problem, please first address it.\n\nThe proposed approach is not very interesting, brutal force and clustering. They are very straightforward.\n\nOverall, the quality is far below the bar of ICLR.\n\nComments not affect rating\n\nThe paper uses \"task\" which is not defined. It is confusing until much later in the paper that it just refers to \"class\".\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper948/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper948/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On summarized validation curves and generalization", "authors": ["Mohammad Hashir", "Yoshua Bengio", "Joseph Paul Cohen"], "authorids": ["mohammad.hashir.khan@umontreal.ca", "yoshua.bengio@mila.quebec", "joseph@josephpcohen.com"], "keywords": ["model selection", "deep learning", "early stopping", "validation curves"], "abstract": "The validation curve is widely used for model selection and hyper-parameter search with the curve usually summarized over all the training tasks. However, this summarization tends to lose the intricacies of the per-task curves and it isn't able to reflect if all the tasks are at their validation optimum even if the summarized curve might be. In this work, we explore this loss of information, how it affects the model at testing and how to detect it using interval plots. We propose two techniques as a proof-of-concept of the potential gain in the test performance when per-task validation curves are accounted for. Our experiments on three large datasets show up to a 2.5% increase (averaged over multiple trials) in the test accuracy rate when model selection uses the per-task validation maximums instead of the summarized validation maximum. This potential increase is not a result of any modification to the model but rather at what point of training the weights were selected from. This presents an exciting direction for new training and model selection techniques that rely on more than just averaged metrics. ", "pdf": "/pdf/d7a671f64111b00df78c9d0fc274087caacc7803.pdf", "paperhash": "hashir|on_summarized_validation_curves_and_generalization", "original_pdf": "/attachment/c0e71864f4f01447dd90ca4153289eb9cc03bc2c.pdf", "_bibtex": "@misc{\nhashir2020on,\ntitle={On summarized validation curves and generalization},\nauthor={Mohammad Hashir and Yoshua Bengio and Joseph Paul Cohen},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lRg0VKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1lRg0VKDr", "replyto": "S1lRg0VKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper948/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper948/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575068860269, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper948/Reviewers"], "noninvitees": [], "tcdate": 1570237744630, "tmdate": 1575068860281, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper948/-/Official_Review"}}}, {"id": "r1xyacKWcH", "original": null, "number": 3, "cdate": 1572080311449, "ddate": null, "tcdate": 1572080311449, "tmdate": 1572972531932, "tddate": null, "forum": "S1lRg0VKDr", "replyto": "S1lRg0VKDr", "invitation": "ICLR.cc/2020/Conference/Paper948/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summarize what the paper claims to do/contribute. Be positive and generous.\nThe paper proposes a new method for model selection in the case of classification with multiple labels. The method suggests not relying on average accuracy over all labels to choose a model but choosing multiple models depending on the label and apply them to the related samples.\n\nClearly state your decision (accept or reject) with one or two key reasons for this choice.\nReject.\n- I don't think the paper is contributing something new to the literature. Also there seems to be some confusion re: what accuracy means and is used in some places instead of precision.  (eg in. Sect. 4.1)\n\nProvide supporting arguments for the reasons for the decision.\n- One question I had was to think about is how one would know what model to apply to what samples if they don't know a priori the labels of these samples. This seems to be something your suggested approaches rely on.\n\nProvide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment.\n- \"valid\" is used often instead of validation.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper948/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper948/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On summarized validation curves and generalization", "authors": ["Mohammad Hashir", "Yoshua Bengio", "Joseph Paul Cohen"], "authorids": ["mohammad.hashir.khan@umontreal.ca", "yoshua.bengio@mila.quebec", "joseph@josephpcohen.com"], "keywords": ["model selection", "deep learning", "early stopping", "validation curves"], "abstract": "The validation curve is widely used for model selection and hyper-parameter search with the curve usually summarized over all the training tasks. However, this summarization tends to lose the intricacies of the per-task curves and it isn't able to reflect if all the tasks are at their validation optimum even if the summarized curve might be. In this work, we explore this loss of information, how it affects the model at testing and how to detect it using interval plots. We propose two techniques as a proof-of-concept of the potential gain in the test performance when per-task validation curves are accounted for. Our experiments on three large datasets show up to a 2.5% increase (averaged over multiple trials) in the test accuracy rate when model selection uses the per-task validation maximums instead of the summarized validation maximum. This potential increase is not a result of any modification to the model but rather at what point of training the weights were selected from. This presents an exciting direction for new training and model selection techniques that rely on more than just averaged metrics. ", "pdf": "/pdf/d7a671f64111b00df78c9d0fc274087caacc7803.pdf", "paperhash": "hashir|on_summarized_validation_curves_and_generalization", "original_pdf": "/attachment/c0e71864f4f01447dd90ca4153289eb9cc03bc2c.pdf", "_bibtex": "@misc{\nhashir2020on,\ntitle={On summarized validation curves and generalization},\nauthor={Mohammad Hashir and Yoshua Bengio and Joseph Paul Cohen},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lRg0VKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1lRg0VKDr", "replyto": "S1lRg0VKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper948/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper948/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575068860269, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper948/Reviewers"], "noninvitees": [], "tcdate": 1570237744630, "tmdate": 1575068860281, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper948/-/Official_Review"}}}], "count": 10}