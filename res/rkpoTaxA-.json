{"notes": [{"tddate": null, "ddate": null, "tmdate": 1520376389014, "tcdate": 1520376389014, "number": 9, "cdate": 1520376389014, "id": "Sk6H552uf", "invitation": "ICLR.cc/2018/Conference/-/Paper435/Official_Comment", "forum": "rkpoTaxA-", "replyto": "B1WswBVUM", "signatures": ["ICLR.cc/2018/Conference/Paper435/Area_Chair"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper435/Area_Chair"], "content": {"title": "proposed changes", "comment": "Apologies for missing this message. Yes, the proposed changes sound good to me!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-ensembling for visual domain adaptation", "abstract": "This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et. al 2017) of temporal ensembling (Laine et al. 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.", "pdf": "/pdf/9c94173e9d7bd0bfa05875f7895fa377df6d3cd4.pdf", "TL;DR": "Self-ensembling based algorithm for visual domain adaptation, state of the art results, won VisDA-2017 image classification domain adaptation challenge.", "paperhash": "french|selfensembling_for_visual_domain_adaptation", "_bibtex": "@inproceedings{\nfrench2018selfensembling,\ntitle={Self-ensembling for visual domain adaptation},\nauthor={Geoff French and Michal Mackiewicz and Mark Fisher},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=rkpoTaxA-},\n}", "keywords": ["deep learning", "neural networks", "domain adaptation", "images", "visual", "computer vision"], "authors": ["Geoff French", "Michal Mackiewicz", "Mark Fisher"], "authorids": ["g.french@uea.ac.uk", "m.mackiewicz@uea.ac.uk", "m.fisher@uea.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825733682, "id": "ICLR.cc/2018/Conference/-/Paper435/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rkpoTaxA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper435/Authors|ICLR.cc/2018/Conference/Paper435/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper435/Authors|ICLR.cc/2018/Conference/Paper435/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper435/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper435/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper435/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper435/Reviewers", "ICLR.cc/2018/Conference/Paper435/Authors", "ICLR.cc/2018/Conference/Paper435/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825733682}}}, {"tddate": null, "ddate": null, "tmdate": 1519062978490, "tcdate": 1509117349328, "number": 435, "cdate": 1518730177990, "id": "rkpoTaxA-", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "rkpoTaxA-", "original": "HkTjTaxRW", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Self-ensembling for visual domain adaptation", "abstract": "This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et. al 2017) of temporal ensembling (Laine et al. 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.", "pdf": "/pdf/9c94173e9d7bd0bfa05875f7895fa377df6d3cd4.pdf", "TL;DR": "Self-ensembling based algorithm for visual domain adaptation, state of the art results, won VisDA-2017 image classification domain adaptation challenge.", "paperhash": "french|selfensembling_for_visual_domain_adaptation", "_bibtex": "@inproceedings{\nfrench2018selfensembling,\ntitle={Self-ensembling for visual domain adaptation},\nauthor={Geoff French and Michal Mackiewicz and Mark Fisher},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=rkpoTaxA-},\n}", "keywords": ["deep learning", "neural networks", "domain adaptation", "images", "visual", "computer vision"], "authors": ["Geoff French", "Michal Mackiewicz", "Mark Fisher"], "authorids": ["g.french@uea.ac.uk", "m.mackiewicz@uea.ac.uk", "m.fisher@uea.ac.uk"]}, "nonreaders": [], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "tmdate": 1517733785194, "tcdate": 1517733785194, "number": 8, "cdate": 1517733785194, "id": "B1WswBVUM", "invitation": "ICLR.cc/2018/Conference/-/Paper435/Official_Comment", "forum": "rkpoTaxA-", "replyto": "rJ8eNJTrM", "signatures": ["ICLR.cc/2018/Conference/Paper435/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper435/Authors"], "content": {"title": "Thank you", "comment": "Thank you for accepting our paper! Would you like us to make the following changes to the paper?\n\n- The minimal augmentation results are in the paper for the small image experiments. We are running experiments using minimal augmentation on the VisDa dataset. Would you like us to insert these results into Table 2?\n\n- We could also add some (*) marks to table 1 to indicate minimal augmentation results more clearly if you like.\n\n- We also suggested to AnonReviewer2 that we could note a caveat concerning our class balancing loss.\n\n- We would also like to add web URLs for our source code.\n\nThank you"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-ensembling for visual domain adaptation", "abstract": "This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et. al 2017) of temporal ensembling (Laine et al. 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.", "pdf": "/pdf/9c94173e9d7bd0bfa05875f7895fa377df6d3cd4.pdf", "TL;DR": "Self-ensembling based algorithm for visual domain adaptation, state of the art results, won VisDA-2017 image classification domain adaptation challenge.", "paperhash": "french|selfensembling_for_visual_domain_adaptation", "_bibtex": "@inproceedings{\nfrench2018selfensembling,\ntitle={Self-ensembling for visual domain adaptation},\nauthor={Geoff French and Michal Mackiewicz and Mark Fisher},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=rkpoTaxA-},\n}", "keywords": ["deep learning", "neural networks", "domain adaptation", "images", "visual", "computer vision"], "authors": ["Geoff French", "Michal Mackiewicz", "Mark Fisher"], "authorids": ["g.french@uea.ac.uk", "m.mackiewicz@uea.ac.uk", "m.fisher@uea.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825733682, "id": "ICLR.cc/2018/Conference/-/Paper435/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rkpoTaxA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper435/Authors|ICLR.cc/2018/Conference/Paper435/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper435/Authors|ICLR.cc/2018/Conference/Paper435/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper435/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper435/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper435/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper435/Reviewers", "ICLR.cc/2018/Conference/Paper435/Authors", "ICLR.cc/2018/Conference/Paper435/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825733682}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260093848, "tcdate": 1517249518173, "number": 279, "cdate": 1517249518158, "id": "rJ8eNJTrM", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "rkpoTaxA-", "replyto": "rkpoTaxA-", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "An interesting application of self-ensembling/temporal ensembling for visual domain adaptation that achieves state of the art on the visual domain adaptation challenge. Reviewers noted that the approach is quite engineering-heavy, but I am not sure it's really much worse than making a pixel-to-pixel approach work well for domain adaptation.\n\nI hope the authors follow through with their promise to add experiments to the final version (notably the minimal augmentation experiments to show just how much this domain adaptation technique is tailored towards imagenet-like things).\n\nAs it stands, this paper would be a good contribution to ICLR as it shows an efficient and interesting way to solve a particular visual domain adaptation problem.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-ensembling for visual domain adaptation", "abstract": "This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et. al 2017) of temporal ensembling (Laine et al. 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.", "pdf": "/pdf/9c94173e9d7bd0bfa05875f7895fa377df6d3cd4.pdf", "TL;DR": "Self-ensembling based algorithm for visual domain adaptation, state of the art results, won VisDA-2017 image classification domain adaptation challenge.", "paperhash": "french|selfensembling_for_visual_domain_adaptation", "_bibtex": "@inproceedings{\nfrench2018selfensembling,\ntitle={Self-ensembling for visual domain adaptation},\nauthor={Geoff French and Michal Mackiewicz and Mark Fisher},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=rkpoTaxA-},\n}", "keywords": ["deep learning", "neural networks", "domain adaptation", "images", "visual", "computer vision"], "authors": ["Geoff French", "Michal Mackiewicz", "Mark Fisher"], "authorids": ["g.french@uea.ac.uk", "m.mackiewicz@uea.ac.uk", "m.fisher@uea.ac.uk"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "tmdate": 1516031102646, "tcdate": 1516027822045, "number": 7, "cdate": 1516027822045, "id": "B1Ih1S54G", "invitation": "ICLR.cc/2018/Conference/-/Paper435/Official_Comment", "forum": "rkpoTaxA-", "replyto": "HJ7P8yYNM", "signatures": ["ICLR.cc/2018/Conference/Paper435/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper435/Authors"], "content": {"title": "Good point", "comment": "Thanks for pointing this out, as its' most likely correct; problems will likely arise in situations where there is severe class imbalance in the target dataset.\n\nIf the editors permit it, we may need to add this caveat to our paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-ensembling for visual domain adaptation", "abstract": "This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et. al 2017) of temporal ensembling (Laine et al. 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.", "pdf": "/pdf/9c94173e9d7bd0bfa05875f7895fa377df6d3cd4.pdf", "TL;DR": "Self-ensembling based algorithm for visual domain adaptation, state of the art results, won VisDA-2017 image classification domain adaptation challenge.", "paperhash": "french|selfensembling_for_visual_domain_adaptation", "_bibtex": "@inproceedings{\nfrench2018selfensembling,\ntitle={Self-ensembling for visual domain adaptation},\nauthor={Geoff French and Michal Mackiewicz and Mark Fisher},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=rkpoTaxA-},\n}", "keywords": ["deep learning", "neural networks", "domain adaptation", "images", "visual", "computer vision"], "authors": ["Geoff French", "Michal Mackiewicz", "Mark Fisher"], "authorids": ["g.french@uea.ac.uk", "m.mackiewicz@uea.ac.uk", "m.fisher@uea.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825733682, "id": "ICLR.cc/2018/Conference/-/Paper435/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rkpoTaxA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper435/Authors|ICLR.cc/2018/Conference/Paper435/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper435/Authors|ICLR.cc/2018/Conference/Paper435/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper435/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper435/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper435/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper435/Reviewers", "ICLR.cc/2018/Conference/Paper435/Authors", "ICLR.cc/2018/Conference/Paper435/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825733682}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515939994371, "tcdate": 1511809564598, "number": 1, "cdate": 1511809564598, "id": "S1HXzycxf", "invitation": "ICLR.cc/2018/Conference/-/Paper435/Official_Review", "forum": "rkpoTaxA-", "replyto": "rkpoTaxA-", "signatures": ["ICLR.cc/2018/Conference/Paper435/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Review", "rating": "7: Good paper, accept", "review": "This paper presents a domain adaptation algorithm based on the self-ensembling method proposed by [Tarvainen & Valpola, 2017]. The main idea is to enforce the agreement between the predictions of the teacher and the student classifiers on the target domain samples while training the student to perform well on the source domain. The teacher network is simply an exponential moving average of different versions of the student network over time.   \n\nPros:\n+ The paper is well-written and easy to read\n+ The proposed method is a natural extension of the mean teacher semi-supervised learning model by [Tarvainen & Valpola, 2017]\n+ The model achieves state-of-the-art results on a range of visual domain adaptation benchmarks (including top performance in the VisDA17 challenge)\n\nCons:\n- The model is tailored to the image domain as it makes heavy use of the data augmentation. That restricts its applicability quite significantly. I\u2019m also very interested to know how the proposed method works when no augmentation is employed (for fair comparison with some of the entries in Table 1).\n- I\u2019m not particularly fond of the engineering tricks like confidence thresholding and the class balance loss. They seem to be essential for good performance and thus, in my opinion, reduce the value of the main idea.\n- Related to the previous point, the final VisDA17 model seems to be engineered too heavily to work well on a particular dataset. I\u2019m not sure if it provides many interesting insights for the scientific community at large.\n\nIn my opinion, it\u2019s a borderline paper. While the best reported quantitative results are quite good, it seems that achieving those requires a significant engineering effort beyond just applying the self-ensembling idea. \n\nNotes:\n* The paper somewhat breaks the anonymity of the authors by mentioning the \u201cwinning entry in the VISDA-2017\u201d. Maybe it\u2019s not a big issue but in my opinion it\u2019s better to remove references to the competition entry.\n* Page 2, 2.1, line 2, typo: \u201cstanrdard\u201d -> \u201cstandard\u201d\n\nPost-rebuttal revision:\nAfter reading the authors' response to my review, I decided to increase the score by 2 points. I appreciate the improvements that were made to the paper but still feel that this work a bit too engineering-heavy, and the title does not fully reflect what's going on in the full pipeline.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Self-ensembling for visual domain adaptation", "abstract": "This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et. al 2017) of temporal ensembling (Laine et al. 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.", "pdf": "/pdf/9c94173e9d7bd0bfa05875f7895fa377df6d3cd4.pdf", "TL;DR": "Self-ensembling based algorithm for visual domain adaptation, state of the art results, won VisDA-2017 image classification domain adaptation challenge.", "paperhash": "french|selfensembling_for_visual_domain_adaptation", "_bibtex": "@inproceedings{\nfrench2018selfensembling,\ntitle={Self-ensembling for visual domain adaptation},\nauthor={Geoff French and Michal Mackiewicz and Mark Fisher},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=rkpoTaxA-},\n}", "keywords": ["deep learning", "neural networks", "domain adaptation", "images", "visual", "computer vision"], "authors": ["Geoff French", "Michal Mackiewicz", "Mark Fisher"], "authorids": ["g.french@uea.ac.uk", "m.mackiewicz@uea.ac.uk", "m.fisher@uea.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642448491, "id": "ICLR.cc/2018/Conference/-/Paper435/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper435/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper435/AnonReviewer2", "ICLR.cc/2018/Conference/Paper435/AnonReviewer1", "ICLR.cc/2018/Conference/Paper435/AnonReviewer3"], "reply": {"forum": "rkpoTaxA-", "replyto": "rkpoTaxA-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper435/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642448491}}}, {"tddate": null, "ddate": null, "tmdate": 1515939419559, "tcdate": 1515939419559, "number": 6, "cdate": 1515939419559, "id": "HJ7P8yYNM", "invitation": "ICLR.cc/2018/Conference/-/Paper435/Official_Comment", "forum": "rkpoTaxA-", "replyto": "Syu2DDUmz", "signatures": ["ICLR.cc/2018/Conference/Paper435/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper435/AnonReviewer2"], "content": {"title": "Comment", "comment": "Thank you for you comments! Regarding class balancing loss, I'm wondering if it's safe to force the predictions on the target batch to be similar to the uniform distribution. As you mention in the paper, SVHN is a non-balanced dataset therefore a random batch won't really follow the uniform label distribution. I guess one has to be very careful with the scale of that term."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-ensembling for visual domain adaptation", "abstract": "This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et. al 2017) of temporal ensembling (Laine et al. 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.", "pdf": "/pdf/9c94173e9d7bd0bfa05875f7895fa377df6d3cd4.pdf", "TL;DR": "Self-ensembling based algorithm for visual domain adaptation, state of the art results, won VisDA-2017 image classification domain adaptation challenge.", "paperhash": "french|selfensembling_for_visual_domain_adaptation", "_bibtex": "@inproceedings{\nfrench2018selfensembling,\ntitle={Self-ensembling for visual domain adaptation},\nauthor={Geoff French and Michal Mackiewicz and Mark Fisher},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=rkpoTaxA-},\n}", "keywords": ["deep learning", "neural networks", "domain adaptation", "images", "visual", "computer vision"], "authors": ["Geoff French", "Michal Mackiewicz", "Mark Fisher"], "authorids": ["g.french@uea.ac.uk", "m.mackiewicz@uea.ac.uk", "m.fisher@uea.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825733682, "id": "ICLR.cc/2018/Conference/-/Paper435/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rkpoTaxA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper435/Authors|ICLR.cc/2018/Conference/Paper435/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper435/Authors|ICLR.cc/2018/Conference/Paper435/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper435/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper435/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper435/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper435/Reviewers", "ICLR.cc/2018/Conference/Paper435/Authors", "ICLR.cc/2018/Conference/Paper435/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825733682}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642448543, "tcdate": 1511914256548, "number": 2, "cdate": 1511914256548, "id": "r1uziOjxf", "invitation": "ICLR.cc/2018/Conference/-/Paper435/Official_Review", "forum": "rkpoTaxA-", "replyto": "rkpoTaxA-", "signatures": ["ICLR.cc/2018/Conference/Paper435/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "The paper authors domain adaptation problems using techniques from semi-supervised learning and achieves impressive empirical results.", "rating": "7: Good paper, accept", "review": "The paper addresses the problem of domain adaptation: Say you have a source dataset S of labeled examples and you have a target dataset T of unlabeled examples and you want to label examples from the target dataset. \n\nThe main idea in the paper is to train two parallel networks, a 'teacher network' and a 'student network', where the student network has a loss term that takes into account labeled examples and there is an additional loss term coming from the teacher network that compares the probabilities placed by the two networks on the outputs. This is motivated by a similar network introduced in the context of semi-supervised learning by Tarvainen and Valpola (2017). The parameters are then optimized by gradient descent where the weight of the loss-term associated with the unsupervised learning part follows a Gaussian curve (with time). No clear explanation is provided for why this may be a good thing to try. The authors also use other techniques like data augmentation to enhance their algorithms.\n\nThe experimental results in the paper are quite nice. They apply the methodology to various standard vision datasets with noticeable improvements/gains and in one case by including additional tricks manage to better than other methods for VISDA-2017 domain adaptation challenge. In the latter, the challenge is to use computer-generated labeled examples and use this information to label real photographic images. The present paper does substantially better than the competition for this challenge. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-ensembling for visual domain adaptation", "abstract": "This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et. al 2017) of temporal ensembling (Laine et al. 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.", "pdf": "/pdf/9c94173e9d7bd0bfa05875f7895fa377df6d3cd4.pdf", "TL;DR": "Self-ensembling based algorithm for visual domain adaptation, state of the art results, won VisDA-2017 image classification domain adaptation challenge.", "paperhash": "french|selfensembling_for_visual_domain_adaptation", "_bibtex": "@inproceedings{\nfrench2018selfensembling,\ntitle={Self-ensembling for visual domain adaptation},\nauthor={Geoff French and Michal Mackiewicz and Mark Fisher},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=rkpoTaxA-},\n}", "keywords": ["deep learning", "neural networks", "domain adaptation", "images", "visual", "computer vision"], "authors": ["Geoff French", "Michal Mackiewicz", "Mark Fisher"], "authorids": ["g.french@uea.ac.uk", "m.mackiewicz@uea.ac.uk", "m.fisher@uea.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642448491, "id": "ICLR.cc/2018/Conference/-/Paper435/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper435/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper435/AnonReviewer2", "ICLR.cc/2018/Conference/Paper435/AnonReviewer1", "ICLR.cc/2018/Conference/Paper435/AnonReviewer3"], "reply": {"forum": "rkpoTaxA-", "replyto": "rkpoTaxA-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper435/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642448491}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642448506, "tcdate": 1511927632005, "number": 3, "cdate": 1511927632005, "id": "S1OIJnigM", "invitation": "ICLR.cc/2018/Conference/-/Paper435/Official_Review", "forum": "rkpoTaxA-", "replyto": "rkpoTaxA-", "signatures": ["ICLR.cc/2018/Conference/Paper435/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "The method was not particularly novel but using \"self-ensembling\" seemed to win the VISDA 2017 domain adaptation competition.", "rating": "7: Good paper, accept", "review": "The paper was very well-written, and mostly clear, making it easy to follow. The originality of the main method was not immediately apparent to me. However, the authors clearly outline the tricks they had to do to achieve good performance on multiple domain adaptation tasks: confidence thresholding, particular data augmentation, and a loss to deal with imbalanced target datasets, all of which seem like good tricks-of-the-trade for future work. The experimentation was extensive and convincing.\n\nPros:\n* Winning entry to the VISDA 2017 visual domain adaptation challenge competition.\n* Extensive experimentation on established toy datasets (USPS<>MNIST, SVHN<>MNIST, SVHN, GTSRB) and other more real-world datasets (including the VISDA one)\n\nCons:\n* Literature review on domain adaptation was lacking. Recent CVPR papers on transforming samples from source to target should be referred to, one of them was by Shrivastava et al., Learning from Simulated and Unsupervised Images through Adversarial Training, and another by Bousmalis et al., Unsupervised Pixel-level Domain Adaptation with GANs. Also you might want to mention Domain Separation Networks which uses gradient reversal (Ganin et al.) and autoencoders (Ghifary et al.). There was no mention of MMD-based methods, on which there are a few papers. The authors might want to mention non-Deep Learning methods also, or that this review relates to neural networks,\n* On p. 4 it wasn't clear to me how the semi-supervised tasks by Tarvainen and Laine were different to domain adaptation. Did you want to say that the data distributions are different? How does this make the task different. Having source and target come in different minibatches is purely an implementation decision.\n* It was unclear to me what  footnote a. on p. 6 means. Why would you combine results from Ganin et al. and Ghifary et al. ?\n* To preserve anonymity keep acknowledgements out of blind submissions. (although not a big deal with your acknowledgements)", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-ensembling for visual domain adaptation", "abstract": "This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et. al 2017) of temporal ensembling (Laine et al. 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.", "pdf": "/pdf/9c94173e9d7bd0bfa05875f7895fa377df6d3cd4.pdf", "TL;DR": "Self-ensembling based algorithm for visual domain adaptation, state of the art results, won VisDA-2017 image classification domain adaptation challenge.", "paperhash": "french|selfensembling_for_visual_domain_adaptation", "_bibtex": "@inproceedings{\nfrench2018selfensembling,\ntitle={Self-ensembling for visual domain adaptation},\nauthor={Geoff French and Michal Mackiewicz and Mark Fisher},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=rkpoTaxA-},\n}", "keywords": ["deep learning", "neural networks", "domain adaptation", "images", "visual", "computer vision"], "authors": ["Geoff French", "Michal Mackiewicz", "Mark Fisher"], "authorids": ["g.french@uea.ac.uk", "m.mackiewicz@uea.ac.uk", "m.fisher@uea.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642448491, "id": "ICLR.cc/2018/Conference/-/Paper435/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper435/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper435/AnonReviewer2", "ICLR.cc/2018/Conference/Paper435/AnonReviewer1", "ICLR.cc/2018/Conference/Paper435/AnonReviewer3"], "reply": {"forum": "rkpoTaxA-", "replyto": "rkpoTaxA-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper435/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642448491}}}, {"tddate": null, "ddate": null, "tmdate": 1514727698335, "tcdate": 1514727344213, "number": 4, "cdate": 1514727344213, "id": "Syu2DDUmz", "invitation": "ICLR.cc/2018/Conference/-/Paper435/Official_Comment", "forum": "rkpoTaxA-", "replyto": "S1HXzycxf", "signatures": ["ICLR.cc/2018/Conference/Paper435/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper435/Authors"], "content": {"title": "Thank you for your review and feedback", "comment": "Thank you for your review\n\n* We agree that our work is tailored to the image domain. With a view to addressing your concerns, we have run further experiments to quantify the effects of each part of our approach - including data augmentation - for all of the small image benchmarks. We have therefore removed Table 2 as the information that it presented can be more compactly shown in Table 1, alongside everything else. We have added further discussion of the effect of our affine augmentation to section 3.3 and demonstrated its effect on both domain adaptation and plain supervised experiments. What we currently have is the same augmentation scheme used by Lain et al. and Tarvainen at al, which consists of translations (all) and horizontal flips (CIFAR/STL only). Experiments with minimal augmentation (gaussian noise added to the input only, therefore usable outside the image domain) are currently running; we will add them if the experiments complete on time.\n\nFurthermore, we found that our model performs slightly better on the MNIST <-> SVHN experiments when using RGB images rather than greyscale, so we have replaced our greyscale results with RGB ones. This represents a slightly bigger domain jump, so we hope that this increases your confidence in our work.\n\n* We see what you mean concerning engineering tricks. In defence of confidence thresholding, rather than being a new additional trick it replaces a time-dependent ramp-up curve used by Laine et al. in their work. We have made this a little more explicit in section 3.3. As for class balancing loss, it is similar in purpose and implementation to the entropy maximisation loss used in the IMSAT model of Hu et al. (an unsupervised clustering model that also uses data augmentation). We have mentioned this in section 3.4. We did not cite this paper in our original version as we were unaware of it at the time.\n\n* We have run further VisDA experiments. We found that pairing back our augmentation scheme improved performance on the validation set and made little different on the test set. Our original complex augmentation scheme was tested on a very small subset (1280 samples) of the training and validation sets during the development of our model. It turns out that these results did not generalise to the full set, so lesson learned (we were facing a tight competition deadline too). Our new reduced augmentation scheme consists of random crops, random horizontal flips and random uniform scaling, thus bringing it in line with augmentation schemes commonly used in ImageNet networks, such as He et al.'s ResNets. We have also performed 5 independent runs of each of our newer experiments and given a breakdown of the results."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-ensembling for visual domain adaptation", "abstract": "This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et. al 2017) of temporal ensembling (Laine et al. 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.", "pdf": "/pdf/9c94173e9d7bd0bfa05875f7895fa377df6d3cd4.pdf", "TL;DR": "Self-ensembling based algorithm for visual domain adaptation, state of the art results, won VisDA-2017 image classification domain adaptation challenge.", "paperhash": "french|selfensembling_for_visual_domain_adaptation", "_bibtex": "@inproceedings{\nfrench2018selfensembling,\ntitle={Self-ensembling for visual domain adaptation},\nauthor={Geoff French and Michal Mackiewicz and Mark Fisher},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=rkpoTaxA-},\n}", "keywords": ["deep learning", "neural networks", "domain adaptation", "images", "visual", "computer vision"], "authors": ["Geoff French", "Michal Mackiewicz", "Mark Fisher"], "authorids": ["g.french@uea.ac.uk", "m.mackiewicz@uea.ac.uk", "m.fisher@uea.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825733682, "id": "ICLR.cc/2018/Conference/-/Paper435/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rkpoTaxA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper435/Authors|ICLR.cc/2018/Conference/Paper435/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper435/Authors|ICLR.cc/2018/Conference/Paper435/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper435/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper435/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper435/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper435/Reviewers", "ICLR.cc/2018/Conference/Paper435/Authors", "ICLR.cc/2018/Conference/Paper435/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825733682}}}, {"tddate": null, "ddate": null, "tmdate": 1514725443762, "tcdate": 1514725443762, "number": 3, "cdate": 1514725443762, "id": "ryhrgwLXz", "invitation": "ICLR.cc/2018/Conference/-/Paper435/Official_Comment", "forum": "rkpoTaxA-", "replyto": "r1uziOjxf", "signatures": ["ICLR.cc/2018/Conference/Paper435/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper435/Authors"], "content": {"title": "Thank you", "comment": "Thank you for your review.\n\nWe have clarified our discussion of the Gaussian curve based unsupervised loss scaling that was originally proposed by Laine et al. Beyond stating that the scaling function must ramp up slowly they don't discuss their choice of scaling function, so we present it as is. That said, we would propose replacing it with confidence thresholding, especially as it is more stable that Gaussian ramp-up in more challenging scenarios. We have explicitly clarified this in section 3.2."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-ensembling for visual domain adaptation", "abstract": "This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et. al 2017) of temporal ensembling (Laine et al. 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.", "pdf": "/pdf/9c94173e9d7bd0bfa05875f7895fa377df6d3cd4.pdf", "TL;DR": "Self-ensembling based algorithm for visual domain adaptation, state of the art results, won VisDA-2017 image classification domain adaptation challenge.", "paperhash": "french|selfensembling_for_visual_domain_adaptation", "_bibtex": "@inproceedings{\nfrench2018selfensembling,\ntitle={Self-ensembling for visual domain adaptation},\nauthor={Geoff French and Michal Mackiewicz and Mark Fisher},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=rkpoTaxA-},\n}", "keywords": ["deep learning", "neural networks", "domain adaptation", "images", "visual", "computer vision"], "authors": ["Geoff French", "Michal Mackiewicz", "Mark Fisher"], "authorids": ["g.french@uea.ac.uk", "m.mackiewicz@uea.ac.uk", "m.fisher@uea.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825733682, "id": "ICLR.cc/2018/Conference/-/Paper435/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rkpoTaxA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper435/Authors|ICLR.cc/2018/Conference/Paper435/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper435/Authors|ICLR.cc/2018/Conference/Paper435/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper435/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper435/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper435/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper435/Reviewers", "ICLR.cc/2018/Conference/Paper435/Authors", "ICLR.cc/2018/Conference/Paper435/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825733682}}}, {"tddate": null, "ddate": null, "tmdate": 1514725274115, "tcdate": 1514725234042, "number": 2, "cdate": 1514725234042, "id": "HJqdkDLQG", "invitation": "ICLR.cc/2018/Conference/-/Paper435/Official_Comment", "forum": "rkpoTaxA-", "replyto": "S1OIJnigM", "signatures": ["ICLR.cc/2018/Conference/Paper435/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper435/Authors"], "content": {"title": "Thank you for your review", "comment": "Thank you for your review. We hope that our revision will address your concerns.\n\n* Thanks for pointing out the shortcomings of our literature review. We have stated that we are focusing on neural networks and we have cited the works that you mentioned. We have briefly mentioned MMD based approaches, although not in detail as we do not have an in-depth familiarity with the mathematics behind it. We have had to condense our literature review somewhat in order to not go too far over the page limit.\n\n* We have made the distinction between semi-supervised learning and domain adaptation more clear, as the distributions of the source and target datasets are indeed different. As for having separate source and target mini-batches, we have clarified how this fits in and was inspired by the work of Li et al. (2016). Time permitting, we may be able to run some experiments to quantify the effect this decision has and add the results to Table 1.\n\n* It seems that Ghifary et al. reimplemented Ganin's RevGrad approach. Neither paper had results for all the small image benchmarks that we discuss, so we took results from both papers to get a complete set. We have clarified the footnote.\n\n* We have suppressed the acknowledgements for now.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-ensembling for visual domain adaptation", "abstract": "This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et. al 2017) of temporal ensembling (Laine et al. 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.", "pdf": "/pdf/9c94173e9d7bd0bfa05875f7895fa377df6d3cd4.pdf", "TL;DR": "Self-ensembling based algorithm for visual domain adaptation, state of the art results, won VisDA-2017 image classification domain adaptation challenge.", "paperhash": "french|selfensembling_for_visual_domain_adaptation", "_bibtex": "@inproceedings{\nfrench2018selfensembling,\ntitle={Self-ensembling for visual domain adaptation},\nauthor={Geoff French and Michal Mackiewicz and Mark Fisher},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=rkpoTaxA-},\n}", "keywords": ["deep learning", "neural networks", "domain adaptation", "images", "visual", "computer vision"], "authors": ["Geoff French", "Michal Mackiewicz", "Mark Fisher"], "authorids": ["g.french@uea.ac.uk", "m.mackiewicz@uea.ac.uk", "m.fisher@uea.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825733682, "id": "ICLR.cc/2018/Conference/-/Paper435/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rkpoTaxA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper435/Authors|ICLR.cc/2018/Conference/Paper435/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper435/Authors|ICLR.cc/2018/Conference/Paper435/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper435/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper435/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper435/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper435/Reviewers", "ICLR.cc/2018/Conference/Paper435/Authors", "ICLR.cc/2018/Conference/Paper435/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825733682}}}, {"tddate": null, "ddate": null, "tmdate": 1510152415224, "tcdate": 1510152415224, "number": 1, "cdate": 1510152415224, "id": "B1vyYcxyG", "invitation": "ICLR.cc/2018/Conference/-/Paper435/Official_Comment", "forum": "rkpoTaxA-", "replyto": "rkpoTaxA-", "signatures": ["ICLR.cc/2018/Conference/Paper435/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper435/Authors"], "content": {"title": "Corrections to text in red", "comment": "There are three instances of text in red that indicate items that we would like to correct.\n\nFirstly in the conclusions section on page 9 the word 'check' in red was a 'note to self' to verify the fact that our networks also exhibit strong performance on sample from the source domain. At submission time our experiment logs from our small image benchmarks backed this claim up. At the time we had not managed to verify this claim for the VisDA experiments, hence the 'note to self'. This has since been done and the claim holds. Given our approach to training (simultaneous supervised training on source domain and unsupervised training on target domain) we had a strong reason to believe this claim to be true at the time of submission.\n\nIn tables 1 and 2 on page 6 there are results in red, as they result from averaging less than the 5 independent runs as claimed in the table 1 caption. We have since run more experiments to get the full 5 results. The only substantial change is the 11.11 +/- 0 result for STL -> CIFAR in the 'Mean teacher' row which has now changed to 15.51 +/- 8.7. The rest are within a few tenths of a % of the results shown in the submitted version.\n\nFurthermore, since submission we discovered a bug in our image augmentation code that affects the small colour image benchmarks (STL <-> CIFAR, Syn Digits -> SVHN and SynSigns -> GTSRB). Fixing the bug looks set to yield improved results (so far by looking at the results from the experiments that have completed). We would like to update tables 1 and 2 to reflect this."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-ensembling for visual domain adaptation", "abstract": "This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et. al 2017) of temporal ensembling (Laine et al. 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.", "pdf": "/pdf/9c94173e9d7bd0bfa05875f7895fa377df6d3cd4.pdf", "TL;DR": "Self-ensembling based algorithm for visual domain adaptation, state of the art results, won VisDA-2017 image classification domain adaptation challenge.", "paperhash": "french|selfensembling_for_visual_domain_adaptation", "_bibtex": "@inproceedings{\nfrench2018selfensembling,\ntitle={Self-ensembling for visual domain adaptation},\nauthor={Geoff French and Michal Mackiewicz and Mark Fisher},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=rkpoTaxA-},\n}", "keywords": ["deep learning", "neural networks", "domain adaptation", "images", "visual", "computer vision"], "authors": ["Geoff French", "Michal Mackiewicz", "Mark Fisher"], "authorids": ["g.french@uea.ac.uk", "m.mackiewicz@uea.ac.uk", "m.fisher@uea.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825733682, "id": "ICLR.cc/2018/Conference/-/Paper435/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rkpoTaxA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper435/Authors|ICLR.cc/2018/Conference/Paper435/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper435/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper435/Authors|ICLR.cc/2018/Conference/Paper435/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper435/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper435/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper435/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper435/Reviewers", "ICLR.cc/2018/Conference/Paper435/Authors", "ICLR.cc/2018/Conference/Paper435/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825733682}}}], "count": 13}