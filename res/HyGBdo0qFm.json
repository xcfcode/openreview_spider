{"notes": [{"id": "HyGBdo0qFm", "original": "rkld3_qFtQ", "number": 351, "cdate": 1538087788941, "ddate": null, "tcdate": 1538087788941, "tmdate": 1547162771177, "tddate": null, "forum": "HyGBdo0qFm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "On the Turing Completeness of Modern Neural Network Architectures", "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.", "keywords": ["Transformer", "NeuralGPU", "Turing completeness"], "authorids": ["jperez@dcc.uchile.cl", "javier.marinkovic95@gmail.com", "pbarcelo@dcc.uchile.cl"], "authors": ["Jorge P\u00e9rez", "Javier Marinkovi\u0107", "Pablo Barcel\u00f3"], "TL;DR": "We show that the Transformer architecture and the Neural GPU are Turing complete.", "pdf": "/pdf/37280d2270ccaadd759e5ed2916ae49a647230a3.pdf", "paperhash": "p\u00e9rez|on_the_turing_completeness_of_modern_neural_network_architectures", "_bibtex": "@inproceedings{\np\u00e9rez2018on,\ntitle={On the Turing Completeness of Modern Neural Network Architectures},\nauthor={Jorge P\u00e9rez and Javier Marinkovi\u0107 and Pablo Barcel\u00f3},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGBdo0qFm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 20, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BJxIEOBexN", "original": null, "number": 1, "cdate": 1544734765876, "ddate": null, "tcdate": 1544734765876, "tmdate": 1545354513430, "tddate": null, "forum": "HyGBdo0qFm", "replyto": "HyGBdo0qFm", "invitation": "ICLR.cc/2019/Conference/-/Paper351/Meta_Review", "content": {"metareview": "This paper provides a theoretical analysis of the Turing completeness of popular neural network architectures, specifically Neural Transformers and the Neural GPU. The reviewers agreed that this paper provides a meaningful theoretical contribution and should be accepted to the conference. Work of a theoretical nature is, amongst other types of work, called for by the ICLR CFP, but is not a very popular category for submissions, nor is it an easy one. As such, I am happy to follow the reviewers' recommendation and support this paper.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "Interesting theoretical work"}, "signatures": ["ICLR.cc/2019/Conference/Paper351/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper351/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Turing Completeness of Modern Neural Network Architectures", "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.", "keywords": ["Transformer", "NeuralGPU", "Turing completeness"], "authorids": ["jperez@dcc.uchile.cl", "javier.marinkovic95@gmail.com", "pbarcelo@dcc.uchile.cl"], "authors": ["Jorge P\u00e9rez", "Javier Marinkovi\u0107", "Pablo Barcel\u00f3"], "TL;DR": "We show that the Transformer architecture and the Neural GPU are Turing complete.", "pdf": "/pdf/37280d2270ccaadd759e5ed2916ae49a647230a3.pdf", "paperhash": "p\u00e9rez|on_the_turing_completeness_of_modern_neural_network_architectures", "_bibtex": "@inproceedings{\np\u00e9rez2018on,\ntitle={On the Turing Completeness of Modern Neural Network Architectures},\nauthor={Jorge P\u00e9rez and Javier Marinkovi\u0107 and Pablo Barcel\u00f3},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGBdo0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper351/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353245687, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyGBdo0qFm", "replyto": "HyGBdo0qFm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper351/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper351/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper351/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353245687}}}, {"id": "HkxVcSSeg4", "original": null, "number": 19, "cdate": 1544734092278, "ddate": null, "tcdate": 1544734092278, "tmdate": 1544734092278, "tddate": null, "forum": "HyGBdo0qFm", "replyto": "H1gfQ37Cy4", "invitation": "ICLR.cc/2019/Conference/-/Paper351/Official_Comment", "content": {"title": "On your comment", "comment": "Thanks for your comment. \n\nWe believe that your doubt has been already clarified by the authors of the paper mentioned in your comment (\"Universal Transformers\"), and we thank the authors for their response. We just want to emphasize that our results only hold when unbounded precision is admitted, which is a standard assumption in the theoretical analysis of the computational power of neural networks (see, e.g., the Universal  Approximation Theorem, or Turing Completeness for RNNs). As mentioned in the response provided by the authors of the Universal Transformers paper, when only bounded precision is allowed, then the model is no longer Turing complete. In fact, we formally prove in our paper that the latter holds even if one sees the Transformer as a seq-to-seq network that produces an arbitrary long output.  We will include some further comments about this in the final version of our paper.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper351/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper351/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Turing Completeness of Modern Neural Network Architectures", "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.", "keywords": ["Transformer", "NeuralGPU", "Turing completeness"], "authorids": ["jperez@dcc.uchile.cl", "javier.marinkovic95@gmail.com", "pbarcelo@dcc.uchile.cl"], "authors": ["Jorge P\u00e9rez", "Javier Marinkovi\u0107", "Pablo Barcel\u00f3"], "TL;DR": "We show that the Transformer architecture and the Neural GPU are Turing complete.", "pdf": "/pdf/37280d2270ccaadd759e5ed2916ae49a647230a3.pdf", "paperhash": "p\u00e9rez|on_the_turing_completeness_of_modern_neural_network_architectures", "_bibtex": "@inproceedings{\np\u00e9rez2018on,\ntitle={On the Turing Completeness of Modern Neural Network Architectures},\nauthor={Jorge P\u00e9rez and Javier Marinkovi\u0107 and Pablo Barcel\u00f3},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGBdo0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper351/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609917, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyGBdo0qFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper351/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper351/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper351/Authors|ICLR.cc/2019/Conference/Paper351/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609917}}}, {"id": "r1lxKiAJlV", "original": null, "number": 5, "cdate": 1544706935644, "ddate": null, "tcdate": 1544706935644, "tmdate": 1544706935644, "tddate": null, "forum": "HyGBdo0qFm", "replyto": "H1gfQ37Cy4", "invitation": "ICLR.cc/2019/Conference/-/Paper351/Public_Comment", "content": {"comment": "We are the authors of the Universal Transformer paper ([1] above). As this comment is very similar to what was posted on that submission, please see our response there:  https://openreview.net/forum?id=HyzdRiR9Y7&noteId=HyxfZDmCk4&noteId=rkginvfklN\n\nThe TLDR is that in this work the authors assume arbitrary-precision arithmetic, whereas in our case we focus on the fixed-precision setting and provide a fairly short and intuitive counterexample showing that the Transformer is not universal in that setting, whereas the Universal Transformer is (see our comment above). Our main focus in that work, however, is to show how this increased theoretical capacity leads to significant practical advantages by expanding the number of tasks the Transformer can solve, and by improving accuracies on multiple real-world sequence-to-sequence learning tasks such as MT.\n\n", "title": "Transformer with fixed-precision is not Turing-complete"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Turing Completeness of Modern Neural Network Architectures", "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.", "keywords": ["Transformer", "NeuralGPU", "Turing completeness"], "authorids": ["jperez@dcc.uchile.cl", "javier.marinkovic95@gmail.com", "pbarcelo@dcc.uchile.cl"], "authors": ["Jorge P\u00e9rez", "Javier Marinkovi\u0107", "Pablo Barcel\u00f3"], "TL;DR": "We show that the Transformer architecture and the Neural GPU are Turing complete.", "pdf": "/pdf/37280d2270ccaadd759e5ed2916ae49a647230a3.pdf", "paperhash": "p\u00e9rez|on_the_turing_completeness_of_modern_neural_network_architectures", "_bibtex": "@inproceedings{\np\u00e9rez2018on,\ntitle={On the Turing Completeness of Modern Neural Network Architectures},\nauthor={Jorge P\u00e9rez and Javier Marinkovi\u0107 and Pablo Barcel\u00f3},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGBdo0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper351/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311860319, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HyGBdo0qFm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311860319}}}, {"id": "H1gfQ37Cy4", "original": null, "number": 3, "cdate": 1544596505803, "ddate": null, "tcdate": 1544596505803, "tmdate": 1544597243308, "tddate": null, "forum": "HyGBdo0qFm", "replyto": "HyGBdo0qFm", "invitation": "ICLR.cc/2019/Conference/-/Paper351/Public_Comment", "content": {"comment": "It's observed in [1] that Transformer is not universal. Also, the proofs in this paper are very technical without any intuitive explanation. The results seem very questionable. It is definitely necessary to address this concern before this paper can be accepted.\n\n[1] https://openreview.net/forum?id=HyzdRiR9Y7&noteId=HyxfZDmCk4", "title": "Result in this paper is potentially wrong... "}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Turing Completeness of Modern Neural Network Architectures", "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.", "keywords": ["Transformer", "NeuralGPU", "Turing completeness"], "authorids": ["jperez@dcc.uchile.cl", "javier.marinkovic95@gmail.com", "pbarcelo@dcc.uchile.cl"], "authors": ["Jorge P\u00e9rez", "Javier Marinkovi\u0107", "Pablo Barcel\u00f3"], "TL;DR": "We show that the Transformer architecture and the Neural GPU are Turing complete.", "pdf": "/pdf/37280d2270ccaadd759e5ed2916ae49a647230a3.pdf", "paperhash": "p\u00e9rez|on_the_turing_completeness_of_modern_neural_network_architectures", "_bibtex": "@inproceedings{\np\u00e9rez2018on,\ntitle={On the Turing Completeness of Modern Neural Network Architectures},\nauthor={Jorge P\u00e9rez and Javier Marinkovi\u0107 and Pablo Barcel\u00f3},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGBdo0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper351/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311860319, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HyGBdo0qFm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311860319}}}, {"id": "BygjD2mA1E", "original": null, "number": 4, "cdate": 1544596579394, "ddate": null, "tcdate": 1544596579394, "tmdate": 1544596579394, "tddate": null, "forum": "HyGBdo0qFm", "replyto": "SkxNcv55nm", "invitation": "ICLR.cc/2019/Conference/-/Paper351/Public_Comment", "content": {"comment": "It's observed in [1] that Transformer is not universal. Also, the proofs in this paper are very technical without any intuitive explanation. The results seem very questionable. \n\n[1] https://openreview.net/forum?id=HyzdRiR9Y7&noteId=HyxfZDmCk4", "title": "Result in this paper is potentially wrong... "}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Turing Completeness of Modern Neural Network Architectures", "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.", "keywords": ["Transformer", "NeuralGPU", "Turing completeness"], "authorids": ["jperez@dcc.uchile.cl", "javier.marinkovic95@gmail.com", "pbarcelo@dcc.uchile.cl"], "authors": ["Jorge P\u00e9rez", "Javier Marinkovi\u0107", "Pablo Barcel\u00f3"], "TL;DR": "We show that the Transformer architecture and the Neural GPU are Turing complete.", "pdf": "/pdf/37280d2270ccaadd759e5ed2916ae49a647230a3.pdf", "paperhash": "p\u00e9rez|on_the_turing_completeness_of_modern_neural_network_architectures", "_bibtex": "@inproceedings{\np\u00e9rez2018on,\ntitle={On the Turing Completeness of Modern Neural Network Architectures},\nauthor={Jorge P\u00e9rez and Javier Marinkovi\u0107 and Pablo Barcel\u00f3},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGBdo0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper351/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311860319, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HyGBdo0qFm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311860319}}}, {"id": "BJl39ddR37", "original": null, "number": 3, "cdate": 1541470355757, "ddate": null, "tcdate": 1541470355757, "tmdate": 1543790865179, "tddate": null, "forum": "HyGBdo0qFm", "replyto": "HyGBdo0qFm", "invitation": "ICLR.cc/2019/Conference/-/Paper351/Official_Review", "content": {"title": "Potentially interesting results, very dense and confusing writing", "review": "The paper shows Turing completeness of two modern neural architectures, the Transformer and the Neural GPU. The paper is technically very heavy and gives very little insight and intuition behind the results. Right after surveying the previous work the paper starts stacking definitions and theorems without much explanations.\n\nWhile technical results are potentially quite strong I believe a major revision to the paper might be necessary in order to clarify the ideas. I would even suggest to split the paper into two, one about each architecture as in the current form it is quite long and difficult to follow. \n\nResults are claimed to hold without access to external memory, relying just on the network itself to represent the intermediate results of the computation. I am a bit confused by this statement -- what if the problem at hand is, say EXPSPACE-complete? Then the network would have to be of exponential size (or more generally of arbitrary size which is independent of the input). In this case the claim about not using external memory seems to be kind of vacuous as the network itself has unbounded size. The whole point of Turing-completeness is that the program size is independent of the input size so there seems to be some confusion here.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper351/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "On the Turing Completeness of Modern Neural Network Architectures", "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.", "keywords": ["Transformer", "NeuralGPU", "Turing completeness"], "authorids": ["jperez@dcc.uchile.cl", "javier.marinkovic95@gmail.com", "pbarcelo@dcc.uchile.cl"], "authors": ["Jorge P\u00e9rez", "Javier Marinkovi\u0107", "Pablo Barcel\u00f3"], "TL;DR": "We show that the Transformer architecture and the Neural GPU are Turing complete.", "pdf": "/pdf/37280d2270ccaadd759e5ed2916ae49a647230a3.pdf", "paperhash": "p\u00e9rez|on_the_turing_completeness_of_modern_neural_network_architectures", "_bibtex": "@inproceedings{\np\u00e9rez2018on,\ntitle={On the Turing Completeness of Modern Neural Network Architectures},\nauthor={Jorge P\u00e9rez and Javier Marinkovi\u0107 and Pablo Barcel\u00f3},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGBdo0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper351/Official_Review", "cdate": 1542234481021, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyGBdo0qFm", "replyto": "HyGBdo0qFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper351/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335704921, "tmdate": 1552335704921, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper351/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1lRigk1y4", "original": null, "number": 14, "cdate": 1543594149620, "ddate": null, "tcdate": 1543594149620, "tmdate": 1543594674449, "tddate": null, "forum": "HyGBdo0qFm", "replyto": "BJl39ddR37", "invitation": "ICLR.cc/2019/Conference/-/Paper351/Official_Comment", "content": {"title": "Please consider rebuttal", "comment": "Dear Reviewer 1,\n\nThanks again for you review. As you can see, the authors have written a detailed rebuttal to you and the other reviewers in separate post. Please take the time to consider it, and the other reviews, and respond if needed. I would appreciate it if you can review your own assessment of the paper, and, if you decide to stand by your score, present a short explanation of why you think the paper still falls short in light of the comments made by the authors."}, "signatures": ["ICLR.cc/2019/Conference/Paper351/Area_Chair1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper351/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper351/Area_Chair1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Turing Completeness of Modern Neural Network Architectures", "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.", "keywords": ["Transformer", "NeuralGPU", "Turing completeness"], "authorids": ["jperez@dcc.uchile.cl", "javier.marinkovic95@gmail.com", "pbarcelo@dcc.uchile.cl"], "authors": ["Jorge P\u00e9rez", "Javier Marinkovi\u0107", "Pablo Barcel\u00f3"], "TL;DR": "We show that the Transformer architecture and the Neural GPU are Turing complete.", "pdf": "/pdf/37280d2270ccaadd759e5ed2916ae49a647230a3.pdf", "paperhash": "p\u00e9rez|on_the_turing_completeness_of_modern_neural_network_architectures", "_bibtex": "@inproceedings{\np\u00e9rez2018on,\ntitle={On the Turing Completeness of Modern Neural Network Architectures},\nauthor={Jorge P\u00e9rez and Javier Marinkovi\u0107 and Pablo Barcel\u00f3},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGBdo0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper351/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609917, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyGBdo0qFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper351/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper351/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper351/Authors|ICLR.cc/2019/Conference/Paper351/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609917}}}, {"id": "rygUfWJykN", "original": null, "number": 16, "cdate": 1543594254388, "ddate": null, "tcdate": 1543594254388, "tmdate": 1543594254388, "tddate": null, "forum": "HyGBdo0qFm", "replyto": "rkx3iv83pm", "invitation": "ICLR.cc/2019/Conference/-/Paper351/Official_Comment", "content": {"title": "Score increase", "comment": "As commented above, since the AC says the proof sketch in the body of the paper is admissible, I've increased my score."}, "signatures": ["ICLR.cc/2019/Conference/Paper351/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper351/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper351/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Turing Completeness of Modern Neural Network Architectures", "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.", "keywords": ["Transformer", "NeuralGPU", "Turing completeness"], "authorids": ["jperez@dcc.uchile.cl", "javier.marinkovic95@gmail.com", "pbarcelo@dcc.uchile.cl"], "authors": ["Jorge P\u00e9rez", "Javier Marinkovi\u0107", "Pablo Barcel\u00f3"], "TL;DR": "We show that the Transformer architecture and the Neural GPU are Turing complete.", "pdf": "/pdf/37280d2270ccaadd759e5ed2916ae49a647230a3.pdf", "paperhash": "p\u00e9rez|on_the_turing_completeness_of_modern_neural_network_architectures", "_bibtex": "@inproceedings{\np\u00e9rez2018on,\ntitle={On the Turing Completeness of Modern Neural Network Architectures},\nauthor={Jorge P\u00e9rez and Javier Marinkovi\u0107 and Pablo Barcel\u00f3},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGBdo0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper351/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609917, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyGBdo0qFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper351/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper351/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper351/Authors|ICLR.cc/2019/Conference/Paper351/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609917}}}, {"id": "SkxNcv55nm", "original": null, "number": 2, "cdate": 1541216139874, "ddate": null, "tcdate": 1541216139874, "tmdate": 1543594204255, "tddate": null, "forum": "HyGBdo0qFm", "replyto": "HyGBdo0qFm", "invitation": "ICLR.cc/2019/Conference/-/Paper351/Official_Review", "content": {"title": "Better fit for a journal", "review": "This paper presents interesting theoretical results on Turing completeness of the Transformer and Neural GPU architectures, as modern architectures based on attention and convolutions, under particular assumptions. The basis of proofs in the paper relies on Turing completeness of the seq2seq architecture, which is Turing complete since it contains Turing complete RNNs. Turing completeness of the Transformer and the Neural GPU is proven by showing they can simulate seq2seq architecture.\n\nThe Transformer, using additive hard attention and residual connections, is Turing complete in the case when positional encoding is used. Otherwise, if no positional encoding is used, the model is order-invariant which makes it not Turing complete.\n\nA version of the Neural GPU, dubbed Uniform Neural GPU is proven to be Turing complete. Moreover, the presented theoretical results are backed by a recent publication by Karlis and Liepins. Interestingly, Neural GPUs using circular convolutions are not Turing complete, while the ones using zero padding are.\n\nThe repercussion of the paper for similar architectures is the not just in the theoretical section but also in a set of discoveries of practical importance, like the importance of the use of residual connections, positional coding in Transformers, and zero padding in Neural GPUs.\n\nAlbeit the paper presents an original and significant theoretical progress and is well written, it is not fit for ICLR, primarily as the paper is impossible to review and verify without a thorough perusal and analysis of the appendix. Although the results and the proof sketches fit the body of the paper, the necessity of verifying proofs makes this paper 23 pages long and makes it a better fit for a journal and not a conference.", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper351/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "On the Turing Completeness of Modern Neural Network Architectures", "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.", "keywords": ["Transformer", "NeuralGPU", "Turing completeness"], "authorids": ["jperez@dcc.uchile.cl", "javier.marinkovic95@gmail.com", "pbarcelo@dcc.uchile.cl"], "authors": ["Jorge P\u00e9rez", "Javier Marinkovi\u0107", "Pablo Barcel\u00f3"], "TL;DR": "We show that the Transformer architecture and the Neural GPU are Turing complete.", "pdf": "/pdf/37280d2270ccaadd759e5ed2916ae49a647230a3.pdf", "paperhash": "p\u00e9rez|on_the_turing_completeness_of_modern_neural_network_architectures", "_bibtex": "@inproceedings{\np\u00e9rez2018on,\ntitle={On the Turing Completeness of Modern Neural Network Architectures},\nauthor={Jorge P\u00e9rez and Javier Marinkovi\u0107 and Pablo Barcel\u00f3},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGBdo0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper351/Official_Review", "cdate": 1542234481021, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyGBdo0qFm", "replyto": "HyGBdo0qFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper351/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335704921, "tmdate": 1552335704921, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper351/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SkllClJky4", "original": null, "number": 15, "cdate": 1543594183982, "ddate": null, "tcdate": 1543594183982, "tmdate": 1543594183982, "tddate": null, "forum": "HyGBdo0qFm", "replyto": "rkxzZyk1kN", "invitation": "ICLR.cc/2019/Conference/-/Paper351/Official_Comment", "content": {"title": "Reply to \"Theoretical Work\"", "comment": "Dear Area Chair,\n\nAs per my comment earlier, and given your comment saying the proof sketches are admissible, without the necessity to go through the main proof, I will increase my score. Essentially, that was my only issue with the paper.\n\nOther than that I still stick by what I've stated before - that the paper presents an original and significant theoretical progress with discoveries of practical importance, particularly as it fits well with related work corroborating said discoveries. As such, it should be welcomed to the community."}, "signatures": ["ICLR.cc/2019/Conference/Paper351/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper351/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper351/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Turing Completeness of Modern Neural Network Architectures", "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.", "keywords": ["Transformer", "NeuralGPU", "Turing completeness"], "authorids": ["jperez@dcc.uchile.cl", "javier.marinkovic95@gmail.com", "pbarcelo@dcc.uchile.cl"], "authors": ["Jorge P\u00e9rez", "Javier Marinkovi\u0107", "Pablo Barcel\u00f3"], "TL;DR": "We show that the Transformer architecture and the Neural GPU are Turing complete.", "pdf": "/pdf/37280d2270ccaadd759e5ed2916ae49a647230a3.pdf", "paperhash": "p\u00e9rez|on_the_turing_completeness_of_modern_neural_network_architectures", "_bibtex": "@inproceedings{\np\u00e9rez2018on,\ntitle={On the Turing Completeness of Modern Neural Network Architectures},\nauthor={Jorge P\u00e9rez and Javier Marinkovi\u0107 and Pablo Barcel\u00f3},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGBdo0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper351/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609917, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyGBdo0qFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper351/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper351/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper351/Authors|ICLR.cc/2019/Conference/Paper351/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609917}}}, {"id": "rkxzZyk1kN", "original": null, "number": 13, "cdate": 1543593722329, "ddate": null, "tcdate": 1543593722329, "tmdate": 1543593722329, "tddate": null, "forum": "HyGBdo0qFm", "replyto": "SyemDwc_AX", "invitation": "ICLR.cc/2019/Conference/-/Paper351/Official_Comment", "content": {"title": "Theoretical work", "comment": "Dear Reviewer 3,\n\nTo weigh in, the CFP for the conference calls for\u2014amongst other things\u2014\"theoretical issues in deep learning\", a category under which this work falls.\n\nTheoretical work requiring expansive proofs is indubitably better suited for journals for proper treatment. However, in a fast moving field, the role of conferences is to share work in this more preliminary form, provided suitable rigor has been applied in presenting and framing the work. In the case of theoretical work, this may mean that proof sketches are offered in lieu of proofs, especially in the main body of the paper, with further details to be included in the supplementary materials. This is perfectly acceptable, in my mind, as the role of the main body of the paper is to present the results and motivate them.\n\nIf you feel the paper has done so appropriately, and if you agree there is space for theoretical work at ICLR in line with what i have written and what is in the CFP, I invite you to reconsider your evaluation in light of your own appreciation for the papers contributions.\n\nAC"}, "signatures": ["ICLR.cc/2019/Conference/Paper351/Area_Chair1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper351/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper351/Area_Chair1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Turing Completeness of Modern Neural Network Architectures", "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.", "keywords": ["Transformer", "NeuralGPU", "Turing completeness"], "authorids": ["jperez@dcc.uchile.cl", "javier.marinkovic95@gmail.com", "pbarcelo@dcc.uchile.cl"], "authors": ["Jorge P\u00e9rez", "Javier Marinkovi\u0107", "Pablo Barcel\u00f3"], "TL;DR": "We show that the Transformer architecture and the Neural GPU are Turing complete.", "pdf": "/pdf/37280d2270ccaadd759e5ed2916ae49a647230a3.pdf", "paperhash": "p\u00e9rez|on_the_turing_completeness_of_modern_neural_network_architectures", "_bibtex": "@inproceedings{\np\u00e9rez2018on,\ntitle={On the Turing Completeness of Modern Neural Network Architectures},\nauthor={Jorge P\u00e9rez and Javier Marinkovi\u0107 and Pablo Barcel\u00f3},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGBdo0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper351/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609917, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyGBdo0qFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper351/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper351/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper351/Authors|ICLR.cc/2019/Conference/Paper351/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609917}}}, {"id": "SJedZxpFAX", "original": null, "number": 12, "cdate": 1543258112504, "ddate": null, "tcdate": 1543258112504, "tmdate": 1543258112504, "tddate": null, "forum": "HyGBdo0qFm", "replyto": "SyemDwc_AX", "invitation": "ICLR.cc/2019/Conference/-/Paper351/Official_Comment", "content": {"title": "We empathize with your concern", "comment": "We appreciate the comment. We empathize with your concern about the difficulty of checking long technical proofs in appendices, and in fact we often have to struggle with this ourselves as reviewers. Still, we decided to present our proofs in full detail so that they could be verified exhaustively by reviewers if needed. The only way we could do this was by presenting them as supplementary material. While we believe that in general the writing of the proof in the body of the paper and in the appendix is good and can be more or less easily followed, we will do our best to improve it further if the paper gets accepted."}, "signatures": ["ICLR.cc/2019/Conference/Paper351/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper351/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Turing Completeness of Modern Neural Network Architectures", "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.", "keywords": ["Transformer", "NeuralGPU", "Turing completeness"], "authorids": ["jperez@dcc.uchile.cl", "javier.marinkovic95@gmail.com", "pbarcelo@dcc.uchile.cl"], "authors": ["Jorge P\u00e9rez", "Javier Marinkovi\u0107", "Pablo Barcel\u00f3"], "TL;DR": "We show that the Transformer architecture and the Neural GPU are Turing complete.", "pdf": "/pdf/37280d2270ccaadd759e5ed2916ae49a647230a3.pdf", "paperhash": "p\u00e9rez|on_the_turing_completeness_of_modern_neural_network_architectures", "_bibtex": "@inproceedings{\np\u00e9rez2018on,\ntitle={On the Turing Completeness of Modern Neural Network Architectures},\nauthor={Jorge P\u00e9rez and Javier Marinkovi\u0107 and Pablo Barcel\u00f3},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGBdo0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper351/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609917, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyGBdo0qFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper351/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper351/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper351/Authors|ICLR.cc/2019/Conference/Paper351/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609917}}}, {"id": "SyemDwc_AX", "original": null, "number": 11, "cdate": 1543182170667, "ddate": null, "tcdate": 1543182170667, "tmdate": 1543182170667, "tddate": null, "forum": "HyGBdo0qFm", "replyto": "Hkg1123_pX", "invitation": "ICLR.cc/2019/Conference/-/Paper351/Official_Comment", "content": {"title": "Response to Responses to AnonReviewer3", "comment": "Please, do not get me wrong - I do think your paper is well written, that the insights from it are important for the theoretical understanding of architectures many researchers are using, and that the results from the paper can be of practical significance (and that they should entice other theoretical result).\n\nHowever, given that this is a theoretical paper, of a theoretical contribution, the proofs in it are not akin to 'companion code' to just back up the findings, nor should they be there for 'reproducibility purposes' - they are the core of the paper. Without a verification of these proofs, there is no contribution. This is my main concern. Proof sketches seem ok, the reasoning in the main body of the paper seems sound, but without proofs that have been verified, the conclusions are open to refutation. And the verification of the proofs requires detailed perusal of the appendix, which doesn't fit into the 11 page limit proposed by ICLR.\n\nI would leave the opinion of whether a thorough verification of the proofs is or is not warranted in this case to area chairs. In the case of latter, I support the paper."}, "signatures": ["ICLR.cc/2019/Conference/Paper351/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper351/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper351/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Turing Completeness of Modern Neural Network Architectures", "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.", "keywords": ["Transformer", "NeuralGPU", "Turing completeness"], "authorids": ["jperez@dcc.uchile.cl", "javier.marinkovic95@gmail.com", "pbarcelo@dcc.uchile.cl"], "authors": ["Jorge P\u00e9rez", "Javier Marinkovi\u0107", "Pablo Barcel\u00f3"], "TL;DR": "We show that the Transformer architecture and the Neural GPU are Turing complete.", "pdf": "/pdf/37280d2270ccaadd759e5ed2916ae49a647230a3.pdf", "paperhash": "p\u00e9rez|on_the_turing_completeness_of_modern_neural_network_architectures", "_bibtex": "@inproceedings{\np\u00e9rez2018on,\ntitle={On the Turing Completeness of Modern Neural Network Architectures},\nauthor={Jorge P\u00e9rez and Javier Marinkovi\u0107 and Pablo Barcel\u00f3},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGBdo0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper351/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609917, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyGBdo0qFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper351/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper351/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper351/Authors|ICLR.cc/2019/Conference/Paper351/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609917}}}, {"id": "SJgnoh3dp7", "original": null, "number": 5, "cdate": 1542143140155, "ddate": null, "tcdate": 1542143140155, "tmdate": 1542143140155, "tddate": null, "forum": "HyGBdo0qFm", "replyto": "Syx12inOam", "invitation": "ICLR.cc/2019/Conference/-/Paper351/Official_Comment", "content": {"title": "Responses to AnonReviewer1", "comment": "Responses to AnonReviewer1:\n\n\n** [comment] \u201cResults are claimed to hold without access to external memory [...] what if the problem at hand is, say EXPSPACE-complete? Then the network would have to be of exponential size [...] The whole point of Turing-completeness is that the program size is independent of the input size so there seems to be some confusion here.\u201d\n\n[response] As stated in the paper, Turing completeness for Transformer and Neural GPU is obtained by taking advantage of the internal representations used by both architectures. We prove that the Transformer and the Neural GPU can use the values in their internal activations to carry out the computations while having a network with a fixed number of neurons and connections. For the case of Neural GPUs we even restrict the architecture to ensure a fixed number of parameters (Uniform Neural GPUs). Thus our proof actually uses a \u201cprogram size which is independent of the input size\u201d as mentioned by the reviewer. The confusion might arise because of our assumption that internal representations are rational numbers with arbitrary precision; we are trading external memory by internal precision. This is a classical assumption in the study of the computational power of neural networks (e.g. Universal Approximation Theorem for FFNs and Turing Completeness for RNNs). We mention this property in the Introduction, in the Conclusions, and also when formally proving the results, but we will make it more explicit in the next version of the paper.\n\n** [comment] \u201cThe paper is technically very heavy [...] I believe a major revision to the paper might be necessary in order to clarify the ideas.\u201d\n\n[response] It is true that the paper is a bit dense, but we prove a technically involved result. To be precise in our claims we needed to include all the definitions in the paper. Moreover, our formal definitions can be used in the future to prove more properties for these and similar architectures with theoretical and practical implications. Though technical, the two other reviewers explicitly mention that the paper is well written. \n\n** [comment] \u201cThe paper [...] gives very little insight and intuition behind the results.\u201d\n\n[response] The main intuition in our results is that both architectures can effectively simulate an (Elman)RNN-seq2seq computation, which by Siegelmann and Sontag\u2019s classical result [1] are Turing complete when internal representations are rational numbers of arbitrary precision. We mentioned this in the Introduction and in each proof sketch, but we will make it more explicit in the next version of the paper.\n\n**[comment] \u201cI would even suggest to split the paper into two, one about each architecture\u201d.\n\n[response] We wanted to have both architectures in the paper as they are two of the most popular architectures in use today, yet based on different paradigms; namely, self-attention mechanisms and convolution. We wanted to understand to what extent the use of these features could be exploited in order to show Turing completeness for the models. Moreover, the computational power of Transformers has been compared with that of Neural GPUs in the current literature, but both are only informally used. We wanted to provide a formal way of approaching this comparison.\n\n[1] Siegelmann and Sontag. On the computational power of neural nets. JCSS-95\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper351/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper351/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Turing Completeness of Modern Neural Network Architectures", "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.", "keywords": ["Transformer", "NeuralGPU", "Turing completeness"], "authorids": ["jperez@dcc.uchile.cl", "javier.marinkovic95@gmail.com", "pbarcelo@dcc.uchile.cl"], "authors": ["Jorge P\u00e9rez", "Javier Marinkovi\u0107", "Pablo Barcel\u00f3"], "TL;DR": "We show that the Transformer architecture and the Neural GPU are Turing complete.", "pdf": "/pdf/37280d2270ccaadd759e5ed2916ae49a647230a3.pdf", "paperhash": "p\u00e9rez|on_the_turing_completeness_of_modern_neural_network_architectures", "_bibtex": "@inproceedings{\np\u00e9rez2018on,\ntitle={On the Turing Completeness of Modern Neural Network Architectures},\nauthor={Jorge P\u00e9rez and Javier Marinkovi\u0107 and Pablo Barcel\u00f3},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGBdo0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper351/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609917, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyGBdo0qFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper351/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper351/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper351/Authors|ICLR.cc/2019/Conference/Paper351/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609917}}}, {"id": "Hkg1123_pX", "original": null, "number": 3, "cdate": 1542142935030, "ddate": null, "tcdate": 1542142935030, "tmdate": 1542143089496, "tddate": null, "forum": "HyGBdo0qFm", "replyto": "Syx12inOam", "invitation": "ICLR.cc/2019/Conference/-/Paper351/Official_Comment", "content": {"title": "Responses to AnonReviewer3", "comment": "Responses to AnonReviewer3:\n\n** [comment] \u201cAlbeit the paper presents an original and significant theoretical progress and is well written, it is not fit for ICLR, primarily as the paper is impossible to review and verify without a thorough perusal and analysis of the appendix. Although the results and the proof sketches fit the body of the paper, the necessity of verifying proofs makes this paper 23 pages long and makes it a better fit for a journal and not a conference.\u201d\n\n[response] We included the appendices to allow the interested reader to see the techniques used in our theoretical proof and potentially extend it or apply it to other architectures, to understand the full implications of the results, and to validate the results for themselves. We see the proofs in our appendix more as a \u201ccompanion code to backup our findings\u201d as one usually do for an experimental paper, and we include it mostly for reproducibility purposes. As we stated in the general comments, although submitting to a journal is an option, we do want  to discuss the theoretical implications of our work face-to-face with people of the interested community without waiting for a long journal review process.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper351/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper351/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Turing Completeness of Modern Neural Network Architectures", "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.", "keywords": ["Transformer", "NeuralGPU", "Turing completeness"], "authorids": ["jperez@dcc.uchile.cl", "javier.marinkovic95@gmail.com", "pbarcelo@dcc.uchile.cl"], "authors": ["Jorge P\u00e9rez", "Javier Marinkovi\u0107", "Pablo Barcel\u00f3"], "TL;DR": "We show that the Transformer architecture and the Neural GPU are Turing complete.", "pdf": "/pdf/37280d2270ccaadd759e5ed2916ae49a647230a3.pdf", "paperhash": "p\u00e9rez|on_the_turing_completeness_of_modern_neural_network_architectures", "_bibtex": "@inproceedings{\np\u00e9rez2018on,\ntitle={On the Turing Completeness of Modern Neural Network Architectures},\nauthor={Jorge P\u00e9rez and Javier Marinkovi\u0107 and Pablo Barcel\u00f3},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGBdo0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper351/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609917, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyGBdo0qFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper351/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper351/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper351/Authors|ICLR.cc/2019/Conference/Paper351/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609917}}}, {"id": "S1xAQn3dpX", "original": null, "number": 4, "cdate": 1542143014236, "ddate": null, "tcdate": 1542143014236, "tmdate": 1542143014236, "tddate": null, "forum": "HyGBdo0qFm", "replyto": "Syx12inOam", "invitation": "ICLR.cc/2019/Conference/-/Paper351/Official_Comment", "content": {"title": "Responses to AnonReviewer2", "comment": "Responses to AnonReviewer2:\n\n** [comment] \u201cof the simplifications and approximations used for the proof, how much does that take the model away from what is used in practice?\u201d \n\n[response] Most of our changes are actually simplifications, which means that models as used in practice can have even more space to simulate computations. Take for example the relationship between Uniform Neural GPUs that we use, and (regular) Neural GPUs that are used in practice. Uniform Neural GPUs have a number of parameters that cannot depend on the input size, while (regular) Neural GPUs have a number of parameters that depend linearly on the input size. Transformer on the other hand can use multiple heads per layer but we only use one head. For the case of the Transformer one difference is that we use additive attention in our proof while multiplicative attention is used in practice most of the time. A detailed comparison between both uses in terms of computational power is a good topic for future research.\n\n** [comment] \u201cFor example, the assumption of the piecewise linear sigmoid seems like a quite big change, as there are large regions of the space which now have zero gradients. If you run a real implementation of these models, with the normal sigmoid replaced by this one, does training still work? If not, what are the implications for the proof?\u201d\n\n[response] This is a really interesting question. For the case of the Neural GPU, as we mention in the paper, there is a recent work by Freivalds and Liepins [2] showing that piece-wise linear activations dramatically increase the training performance. These activations (along other changes) allowed the learning of decimal multiplication from examples which was impossible with the original Neural GPU [2]. Thus  having piecewise linear activations actually helps in practice. For the case of the Transformer more experimentation is needed to have a conclusive response. We will add some comments on this in the next version of the paper.\n\n** [comment] \u201c[...] all floating points on a computer represent rationals, but it would be interesting to get a better understanding on how the lack of infinite precision rationals on real hardware affects the main results.\u201d\n\n[response] This is similar to a comparison between a computer with bounded vs unbounded memory. With bounded memory a computer is, theoretically, just a finite state machine. Similarly, with rationals of bounded precision, a Transformer is computationally very weak. Actually, your question made us realize that from our results it follows that bounded precision Transformers cannot even simulate finite automaton (this is a corollary of Proposition 3.1 in our submission). We will add a discussion on this result since it will definitely improve the paper. Thank you for the comment.\n\n** [comment] \u201cDoes the proof rely on the input and output dimensionality being the same? Eg in the preliminaries, x_i and y_i are both d-dimensional - could this be changed?\u201d\n\n[response] The short answer is \u201cyes\u201d it can be changed, as one can always pad the shorter with zeroes as a trick to make them of the same dimension. But having both of the same dimension is more of a practical concern of the architectures we use. For the case of the Transformer, the fact that the decoder puts attention over the output of the encoder, plus the use of residual connections in every layer, forces dimensions to coincide. For the case of the Neural GPU, input vectors are transformed without changing their dimensions, thus input and output vectors have naturally the same size.\n\n** [comment] \u201cCircular convolution definition only appears to define the values directly adjacent to the border, would it be more appropriate to define S_{h+n, :, :} = S{n, :, :}?\u201d\n\n[response] Yes, you are right. We will include this change in the next version, thank you.\n\n\n[2] Freivalds and Liepins. Improving the Neural GPU Architecture for Algorithm Learning. NAMPI-18 (workshop at ICML-18)"}, "signatures": ["ICLR.cc/2019/Conference/Paper351/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper351/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Turing Completeness of Modern Neural Network Architectures", "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.", "keywords": ["Transformer", "NeuralGPU", "Turing completeness"], "authorids": ["jperez@dcc.uchile.cl", "javier.marinkovic95@gmail.com", "pbarcelo@dcc.uchile.cl"], "authors": ["Jorge P\u00e9rez", "Javier Marinkovi\u0107", "Pablo Barcel\u00f3"], "TL;DR": "We show that the Transformer architecture and the Neural GPU are Turing complete.", "pdf": "/pdf/37280d2270ccaadd759e5ed2916ae49a647230a3.pdf", "paperhash": "p\u00e9rez|on_the_turing_completeness_of_modern_neural_network_architectures", "_bibtex": "@inproceedings{\np\u00e9rez2018on,\ntitle={On the Turing Completeness of Modern Neural Network Architectures},\nauthor={Jorge P\u00e9rez and Javier Marinkovi\u0107 and Pablo Barcel\u00f3},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGBdo0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper351/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609917, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyGBdo0qFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper351/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper351/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper351/Authors|ICLR.cc/2019/Conference/Paper351/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609917}}}, {"id": "Syx12inOam", "original": null, "number": 2, "cdate": 1542142887376, "ddate": null, "tcdate": 1542142887376, "tmdate": 1542142887376, "tddate": null, "forum": "HyGBdo0qFm", "replyto": "HyGBdo0qFm", "invitation": "ICLR.cc/2019/Conference/-/Paper351/Official_Comment", "content": {"title": "Rebuttal", "comment": "We thank the reviewers for their comments. We first make some general comments and then answer directly to each reviewer. \n\nAll reviewers appear to agree that our technical results on the Turing completeness of the Transformer and the Neural GPU are potentially interesting/important. In two reviews, there is however a general question about the fit of our results for ICLR. One reviewer advised to go directly to a journal. We did consider submitting to a journal or to a theoretical conference, but we felt it important to discuss the computational properties of the Transformer and the Neural GPU directly with the community involved in their design, implementation, and practical use. We felt that submitting to ICLR would generate more impact noting that the Neural GPU was initially proposed at ICLR2016, and the Transformer architecture (proposed at NIPS2017) is used in several ICLR2018 papers (and also now ICLR2019 submissions). \n\nWe also observe that there is a need for more theoretical foundations with regards to the computational power of modern NN architectures at ICLR, and in particular about the two architectures that we study. Consider for example the following ICLR2019 submission: \u201cUniversal Transformers\u201d (https://openreview.net/forum?id=HyzdRiR9Y7&noteId=HyzdRiR9Y7). Universal Transformers are networks that combine the parallelizability and ease of train of recently proposed feed-forward mechanisms based on self-attention, such as the Transformer, with the learning abilities of recurrent NNs. This is a strong paper, in our opinion, with a thorough experimental part and the potential for significant practical impact. Though it received three positive reviews, two reviewers would like to see a more thorough theoretical analysis of the proposed architecture (which is, admittedly, beyond the scope of the paper). One of the reviewers states \u201cI miss a proof that the Universal Transformer is computationally equivalent to a Turing machine.\u201d while the other states \u201cI am having trouble understanding the universal aspect of the transformer\u201d. Our paper brings light into this, by showing what are some of the minimal sets of features that make self-attention networks, in particular, the Transformer, Turing-complete. Moreover, in that paper, Neural GPUs are used as a yardstick to compare the computational power of the Transformer. Thus our paper presents a formal theoretical basis to address problems that are currently being discussed at ICLR. (We emphasize that we are not involved in any way with the \u201cUniversal Transformer\u201d paper, and that we are not reviewers of it.) \n\nBelow we provide detailed responses to each one of the individual reviews. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper351/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper351/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Turing Completeness of Modern Neural Network Architectures", "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.", "keywords": ["Transformer", "NeuralGPU", "Turing completeness"], "authorids": ["jperez@dcc.uchile.cl", "javier.marinkovic95@gmail.com", "pbarcelo@dcc.uchile.cl"], "authors": ["Jorge P\u00e9rez", "Javier Marinkovi\u0107", "Pablo Barcel\u00f3"], "TL;DR": "We show that the Transformer architecture and the Neural GPU are Turing complete.", "pdf": "/pdf/37280d2270ccaadd759e5ed2916ae49a647230a3.pdf", "paperhash": "p\u00e9rez|on_the_turing_completeness_of_modern_neural_network_architectures", "_bibtex": "@inproceedings{\np\u00e9rez2018on,\ntitle={On the Turing Completeness of Modern Neural Network Architectures},\nauthor={Jorge P\u00e9rez and Javier Marinkovi\u0107 and Pablo Barcel\u00f3},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGBdo0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper351/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609917, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyGBdo0qFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper351/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper351/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper351/Authors|ICLR.cc/2019/Conference/Paper351/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609917}}}, {"id": "r1ldhR0Knm", "original": null, "number": 1, "cdate": 1541168815805, "ddate": null, "tcdate": 1541168815805, "tmdate": 1541534067213, "tddate": null, "forum": "HyGBdo0qFm", "replyto": "HyGBdo0qFm", "invitation": "ICLR.cc/2019/Conference/-/Paper351/Official_Review", "content": {"title": "Clear and well written, some questions about how these results map to training the real models.", "review": "This paper seeks to answer the question of whether models which process sequences, but are not strictly classical RNNs, are Turing complete.\n\nThe authors present proofs that both the Transformer and Neural GPU are turing complete, under certain conditions. I do not consider myself qualified to properly verify the proof but it seems to be presented clearly. The authors note that the conditions involved are not how these models are used in the real world. Given the complex construction required for this more theoretically based proof, it seems reasonable that this should be published now, rather than waiting until the further work discussed in the final section is completed.\n\nI have a number of questions where if a brief answer is possible, this would enhance the manuscript. The main question is, of the simplifications and approximations used for the proof, how much does that take the model away from what is used in practice? For example, the assumption of the piecewise linear sigmoid seems like a quite big change, as there are large regions of the space which now have zero gradients. If you run a real implementation of these models, with the normal sigmoid replaced by this one, does training still work? If not, what are the implications for the proof?\n\nThe rational numbers assumption is interesting - again I wonder how this would affect the model in reality, obviously all floating points on a computer represent rationals, but it would be interesting to get a better understanding on how the lack of infinite precision rationals on real hardware affects the main results.\n\nDoes the proof rely on the input and output dimensionality being the same? Eg in the preliminaries, x_i and y_i are both d-dimensional - could this be changed?\n\nOverall this paper is novel and interesting, I have to give a slightly low confidence score because I'm unfamiliar with a lot of the background here (eg the Siegelamnn & Sontag work). The paper does seem concise and well written.\n\ntypos and minor points:\n\nCircular convolution definition only appears to define the values directly adjacent to the border, would it be more appropriate to define S_{h+n, :, :} = S{n, :, :}?\n\nparagraph above equation 5, 'vectores' -> 'vectors'", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper351/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Turing Completeness of Modern Neural Network Architectures", "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.", "keywords": ["Transformer", "NeuralGPU", "Turing completeness"], "authorids": ["jperez@dcc.uchile.cl", "javier.marinkovic95@gmail.com", "pbarcelo@dcc.uchile.cl"], "authors": ["Jorge P\u00e9rez", "Javier Marinkovi\u0107", "Pablo Barcel\u00f3"], "TL;DR": "We show that the Transformer architecture and the Neural GPU are Turing complete.", "pdf": "/pdf/37280d2270ccaadd759e5ed2916ae49a647230a3.pdf", "paperhash": "p\u00e9rez|on_the_turing_completeness_of_modern_neural_network_architectures", "_bibtex": "@inproceedings{\np\u00e9rez2018on,\ntitle={On the Turing Completeness of Modern Neural Network Architectures},\nauthor={Jorge P\u00e9rez and Javier Marinkovi\u0107 and Pablo Barcel\u00f3},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGBdo0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper351/Official_Review", "cdate": 1542234481021, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyGBdo0qFm", "replyto": "HyGBdo0qFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper351/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335704921, "tmdate": 1552335704921, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper351/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Syla8U8ZcQ", "original": null, "number": 1, "cdate": 1538512468992, "ddate": null, "tcdate": 1538512468992, "tmdate": 1538512468992, "tddate": null, "forum": "HyGBdo0qFm", "replyto": "rJgJkiz-9X", "invitation": "ICLR.cc/2019/Conference/-/Paper351/Official_Comment", "content": {"title": "We need unbounded precision for internal representations", "comment": "Our proofs are based on having unbounded precision for internal representations (neuron values). For weights one can prove that fixed precision (actually very small) is enough.\n\nOur results say nothing about the computational power when fixed precision (like float32) is assumed for internal representations. We actually state the fixed-precision case as an interesting topic for future research."}, "signatures": ["ICLR.cc/2019/Conference/Paper351/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper351/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Turing Completeness of Modern Neural Network Architectures", "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.", "keywords": ["Transformer", "NeuralGPU", "Turing completeness"], "authorids": ["jperez@dcc.uchile.cl", "javier.marinkovic95@gmail.com", "pbarcelo@dcc.uchile.cl"], "authors": ["Jorge P\u00e9rez", "Javier Marinkovi\u0107", "Pablo Barcel\u00f3"], "TL;DR": "We show that the Transformer architecture and the Neural GPU are Turing complete.", "pdf": "/pdf/37280d2270ccaadd759e5ed2916ae49a647230a3.pdf", "paperhash": "p\u00e9rez|on_the_turing_completeness_of_modern_neural_network_architectures", "_bibtex": "@inproceedings{\np\u00e9rez2018on,\ntitle={On the Turing Completeness of Modern Neural Network Architectures},\nauthor={Jorge P\u00e9rez and Javier Marinkovi\u0107 and Pablo Barcel\u00f3},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGBdo0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper351/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609917, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyGBdo0qFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper351/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper351/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper351/Authors|ICLR.cc/2019/Conference/Paper351/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609917}}}, {"id": "rJgJkiz-9X", "original": null, "number": 1, "cdate": 1538497238808, "ddate": null, "tcdate": 1538497238808, "tmdate": 1538497238808, "tddate": null, "forum": "HyGBdo0qFm", "replyto": "HyGBdo0qFm", "invitation": "ICLR.cc/2019/Conference/-/Paper351/Public_Comment", "content": {"comment": "Do I understand correctly that your results only hold for weights in Q, meanining unbounded precision? I suppose it's not true with limited precision, like float32, or am I misunderstanding?", "title": "You assume infinite precision, right?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper351/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Turing Completeness of Modern Neural Network Architectures", "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.", "keywords": ["Transformer", "NeuralGPU", "Turing completeness"], "authorids": ["jperez@dcc.uchile.cl", "javier.marinkovic95@gmail.com", "pbarcelo@dcc.uchile.cl"], "authors": ["Jorge P\u00e9rez", "Javier Marinkovi\u0107", "Pablo Barcel\u00f3"], "TL;DR": "We show that the Transformer architecture and the Neural GPU are Turing complete.", "pdf": "/pdf/37280d2270ccaadd759e5ed2916ae49a647230a3.pdf", "paperhash": "p\u00e9rez|on_the_turing_completeness_of_modern_neural_network_architectures", "_bibtex": "@inproceedings{\np\u00e9rez2018on,\ntitle={On the Turing Completeness of Modern Neural Network Architectures},\nauthor={Jorge P\u00e9rez and Javier Marinkovi\u0107 and Pablo Barcel\u00f3},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyGBdo0qFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper351/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311860319, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HyGBdo0qFm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper351/Authors", "ICLR.cc/2019/Conference/Paper351/Reviewers", "ICLR.cc/2019/Conference/Paper351/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311860319}}}], "count": 21}