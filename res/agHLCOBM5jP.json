{"notes": [{"id": "agHLCOBM5jP", "original": "9I-QItuJqqR", "number": 3284, "cdate": 1601308364771, "ddate": null, "tcdate": 1601308364771, "tmdate": 1614557728679, "tddate": null, "forum": "agHLCOBM5jP", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders", "authorids": ["~Mangal_Prakash1", "~Alexander_Krull3", "~Florian_Jug1"], "authors": ["Mangal Prakash", "Alexander Krull", "Florian Jug"], "keywords": ["Diversity denoising", "Unsupervised denoising", "Variational Autoencoders", "Noise model"], "abstract": "Deep Learning based methods have emerged as the indisputable leaders for virtually all image restoration tasks. Especially in the domain of microscopy images, various content-aware image restoration (CARE) approaches are now used to improve the interpretability of acquired data. Naturally, there are limitations to what can be restored in corrupted images, and like for all inverse problems, many potential solutions exist, and one of them must be chosen. Here, we propose DivNoising, a denoising approach based on fully convolutional variational autoencoders (VAEs), overcoming the problem of having to choose a single solution by predicting a whole distribution of denoised images. First we introduce a principled way of formulating the unsupervised denoising problem within the VAE framework by explicitly incorporating imaging noise models into the decoder. Our approach is fully unsupervised, only requiring noisy images and a suitable description of the imaging noise distribution. We show that such a noise model can either be measured, bootstrapped from noisy data, or co-learned during training. If desired, consensus predictions can be inferred from a set of DivNoising predictions, leading to competitive results with other unsupervised methods and, on occasion, even with the supervised state-of-the-art. DivNoising samples from the posterior enable a plethora of useful applications. We are (i) showing denoising results for 13 datasets, (ii) discussing how optical character recognition (OCR) applications can benefit from diverse predictions, and are (iii) demonstrating how instance cell segmentation improves when using diverse DivNoising predictions.", "one-sentence_summary": "DivNoising performs fully unsupervised diversity denoising using fully convolutional variational autoencoders and achieves SOTA results for a number of well known datasets while also enabling VAE-like sampling", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "prakash|fully_unsupervised_diversity_denoising_with_convolutional_variational_autoencoders", "pdf": "/pdf/2afe972808ebb66f3926468902039c366b274c59.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nprakash2021fully,\ntitle={Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders},\nauthor={Mangal Prakash and Alexander Krull and Florian Jug},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=agHLCOBM5jP}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "lRzIu3VNB6E", "original": null, "number": 1, "cdate": 1610040495541, "ddate": null, "tcdate": 1610040495541, "tmdate": 1610474101803, "tddate": null, "forum": "agHLCOBM5jP", "replyto": "agHLCOBM5jP", "invitation": "ICLR.cc/2021/Conference/Paper3284/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "A simple but sensible idea to improve VAE with good experimental results."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders", "authorids": ["~Mangal_Prakash1", "~Alexander_Krull3", "~Florian_Jug1"], "authors": ["Mangal Prakash", "Alexander Krull", "Florian Jug"], "keywords": ["Diversity denoising", "Unsupervised denoising", "Variational Autoencoders", "Noise model"], "abstract": "Deep Learning based methods have emerged as the indisputable leaders for virtually all image restoration tasks. Especially in the domain of microscopy images, various content-aware image restoration (CARE) approaches are now used to improve the interpretability of acquired data. Naturally, there are limitations to what can be restored in corrupted images, and like for all inverse problems, many potential solutions exist, and one of them must be chosen. Here, we propose DivNoising, a denoising approach based on fully convolutional variational autoencoders (VAEs), overcoming the problem of having to choose a single solution by predicting a whole distribution of denoised images. First we introduce a principled way of formulating the unsupervised denoising problem within the VAE framework by explicitly incorporating imaging noise models into the decoder. Our approach is fully unsupervised, only requiring noisy images and a suitable description of the imaging noise distribution. We show that such a noise model can either be measured, bootstrapped from noisy data, or co-learned during training. If desired, consensus predictions can be inferred from a set of DivNoising predictions, leading to competitive results with other unsupervised methods and, on occasion, even with the supervised state-of-the-art. DivNoising samples from the posterior enable a plethora of useful applications. We are (i) showing denoising results for 13 datasets, (ii) discussing how optical character recognition (OCR) applications can benefit from diverse predictions, and are (iii) demonstrating how instance cell segmentation improves when using diverse DivNoising predictions.", "one-sentence_summary": "DivNoising performs fully unsupervised diversity denoising using fully convolutional variational autoencoders and achieves SOTA results for a number of well known datasets while also enabling VAE-like sampling", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "prakash|fully_unsupervised_diversity_denoising_with_convolutional_variational_autoencoders", "pdf": "/pdf/2afe972808ebb66f3926468902039c366b274c59.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nprakash2021fully,\ntitle={Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders},\nauthor={Mangal Prakash and Alexander Krull and Florian Jug},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=agHLCOBM5jP}\n}"}, "tags": [], "invitation": {"reply": {"forum": "agHLCOBM5jP", "replyto": "agHLCOBM5jP", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040495528, "tmdate": 1610474101787, "id": "ICLR.cc/2021/Conference/Paper3284/-/Decision"}}}, {"id": "hF48vV86MjP", "original": null, "number": 2, "cdate": 1603176093556, "ddate": null, "tcdate": 1603176093556, "tmdate": 1607062093667, "tddate": null, "forum": "agHLCOBM5jP", "replyto": "agHLCOBM5jP", "invitation": "ICLR.cc/2021/Conference/Paper3284/-/Official_Review", "content": {"title": "Discussion about p(s) is preferred ", "review": "This paper proposes a new method of noise removal using convolutional VAE.  An observed image with noise is input to VAE, and after the expression $z$ in the latent space, the noise removed image is finally output.  After that, it is possible to generate a pseudo noisy observation image according to the noise model.  The noise model part is flexibly designed using the Gaussian mixture model.  In the training, VAE and noise model can be learned at the same time. Since VAE is a generative model, and a clean denoising image can be obtained by averaging a large number of candidates of clean images, $s$, sampled from the periphery of the latent space representation $z$.\n\nThe advantages of the proposed method are that it is Fully unsupervised and that the noise model can be learned at the same time.\n\nThe paper itself is interesting, and the proposed method itself is good as one of the image processing methods, but there is a lack in the explanation part.  In this paper, the noise model $p (x|s)$ is well discussed, but I think that the discussion about the image prior, $p(s)$, for clean image is weak. As explained in p3 of the paper, $p(s|x) \\propto p (x | s) p (s)$ is an important formula. In conventional image processing, $p(s)$ has been interpreted as smoothness or non-local similarity. In this method, it is treated as $p (s | z) p (z)$, but I think we should discuss a little more about what this is.\n\nIn the proposed method, there is no special assumption about the prior knowledge of the image, and it seems that it is normally distributed in the latent space. In this case, a large amount of data or some help of a network structure is needed to remove noise. The use of the convolutional structure in the proposed method is an indispensable element in the explanation of $p(s)$.\n\nThe noise reduction effect of the convolutional structure itself is well known after the study of Deep Image Prior, and recent research [a, b] also discusses the relationship between convolutional neural networks and non-local similarity. These may help the discussion about $p(s)$.\n\n[a] Yokota, Tatsuya, et al. \"Manifold Modeling in Embedded Space: A Perspective for Interpreting\" Deep Image Prior \".\" ArXiv preprint arXiv: 1908.02995 (2019).\n[b] Tachella, Juli\u00e1n, Junqi Tang, and Mike Davies. \"CNN Denoisers As Non-Local Filters: The Neural Tangent Denoiser.\" ArXiv preprint arXiv: 2006.02379 (2020).\n\n-------------\nAs a result of the feedback, the part I was concerned  became clear, so I would like to raise the score.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3284/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3284/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders", "authorids": ["~Mangal_Prakash1", "~Alexander_Krull3", "~Florian_Jug1"], "authors": ["Mangal Prakash", "Alexander Krull", "Florian Jug"], "keywords": ["Diversity denoising", "Unsupervised denoising", "Variational Autoencoders", "Noise model"], "abstract": "Deep Learning based methods have emerged as the indisputable leaders for virtually all image restoration tasks. Especially in the domain of microscopy images, various content-aware image restoration (CARE) approaches are now used to improve the interpretability of acquired data. Naturally, there are limitations to what can be restored in corrupted images, and like for all inverse problems, many potential solutions exist, and one of them must be chosen. Here, we propose DivNoising, a denoising approach based on fully convolutional variational autoencoders (VAEs), overcoming the problem of having to choose a single solution by predicting a whole distribution of denoised images. First we introduce a principled way of formulating the unsupervised denoising problem within the VAE framework by explicitly incorporating imaging noise models into the decoder. Our approach is fully unsupervised, only requiring noisy images and a suitable description of the imaging noise distribution. We show that such a noise model can either be measured, bootstrapped from noisy data, or co-learned during training. If desired, consensus predictions can be inferred from a set of DivNoising predictions, leading to competitive results with other unsupervised methods and, on occasion, even with the supervised state-of-the-art. DivNoising samples from the posterior enable a plethora of useful applications. We are (i) showing denoising results for 13 datasets, (ii) discussing how optical character recognition (OCR) applications can benefit from diverse predictions, and are (iii) demonstrating how instance cell segmentation improves when using diverse DivNoising predictions.", "one-sentence_summary": "DivNoising performs fully unsupervised diversity denoising using fully convolutional variational autoencoders and achieves SOTA results for a number of well known datasets while also enabling VAE-like sampling", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "prakash|fully_unsupervised_diversity_denoising_with_convolutional_variational_autoencoders", "pdf": "/pdf/2afe972808ebb66f3926468902039c366b274c59.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nprakash2021fully,\ntitle={Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders},\nauthor={Mangal Prakash and Alexander Krull and Florian Jug},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=agHLCOBM5jP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "agHLCOBM5jP", "replyto": "agHLCOBM5jP", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3284/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538078579, "tmdate": 1606915798938, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3284/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3284/-/Official_Review"}}}, {"id": "M5SQwAT8fyT", "original": null, "number": 1, "cdate": 1603104880523, "ddate": null, "tcdate": 1603104880523, "tmdate": 1606793763766, "tddate": null, "forum": "agHLCOBM5jP", "replyto": "agHLCOBM5jP", "invitation": "ICLR.cc/2021/Conference/Paper3284/-/Official_Review", "content": {"title": "The idea sounds reasonable. But the novelty seems marginal and I have several concerns about the application range of the proposed method.", "review": "This paper proposes a modified VAE for unsupervised image denoising. Unlike existing methods only predicting a single denoised image, the proposed one can generate diverse and plausible results. The experimental results show that this method can outperform existing unsupervised denoising methods. I have several concerns about this paper.\n\n(1) The modifications relative to Kingma & Welling (2014) are marginal. The proposed method more likes an application of Kingma & Welling (2014).\n\n(2) According to Eq. 1, the noises are pixel independent which does not hold for most of the applications e.g. images after demosaicing or ISP denoising. How can the proposed method deal with it?\n\n(3) VAE framework seems only to work for small and constrained images. I am wondering about the denoising results of the proposed method for high-resolution nature images?\n\n(4) At the beginning of Sec. 5, the authors claim that the noise model should be known. How does the proposed method deal with the noises without the noise model for example images after demosaicing or ISP denoising?\n-------------------------------------------------------------------------------------------------------------------------------------\n\nI appreciate that this paper has some merits. But I lower my rating because of the limited usage of the proposed method.\nIt seems it cannot handle high-resolution nature images. Most of the experiments use constrained images e.g. biomedical images or images with small resolution.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3284/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3284/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders", "authorids": ["~Mangal_Prakash1", "~Alexander_Krull3", "~Florian_Jug1"], "authors": ["Mangal Prakash", "Alexander Krull", "Florian Jug"], "keywords": ["Diversity denoising", "Unsupervised denoising", "Variational Autoencoders", "Noise model"], "abstract": "Deep Learning based methods have emerged as the indisputable leaders for virtually all image restoration tasks. Especially in the domain of microscopy images, various content-aware image restoration (CARE) approaches are now used to improve the interpretability of acquired data. Naturally, there are limitations to what can be restored in corrupted images, and like for all inverse problems, many potential solutions exist, and one of them must be chosen. Here, we propose DivNoising, a denoising approach based on fully convolutional variational autoencoders (VAEs), overcoming the problem of having to choose a single solution by predicting a whole distribution of denoised images. First we introduce a principled way of formulating the unsupervised denoising problem within the VAE framework by explicitly incorporating imaging noise models into the decoder. Our approach is fully unsupervised, only requiring noisy images and a suitable description of the imaging noise distribution. We show that such a noise model can either be measured, bootstrapped from noisy data, or co-learned during training. If desired, consensus predictions can be inferred from a set of DivNoising predictions, leading to competitive results with other unsupervised methods and, on occasion, even with the supervised state-of-the-art. DivNoising samples from the posterior enable a plethora of useful applications. We are (i) showing denoising results for 13 datasets, (ii) discussing how optical character recognition (OCR) applications can benefit from diverse predictions, and are (iii) demonstrating how instance cell segmentation improves when using diverse DivNoising predictions.", "one-sentence_summary": "DivNoising performs fully unsupervised diversity denoising using fully convolutional variational autoencoders and achieves SOTA results for a number of well known datasets while also enabling VAE-like sampling", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "prakash|fully_unsupervised_diversity_denoising_with_convolutional_variational_autoencoders", "pdf": "/pdf/2afe972808ebb66f3926468902039c366b274c59.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nprakash2021fully,\ntitle={Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders},\nauthor={Mangal Prakash and Alexander Krull and Florian Jug},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=agHLCOBM5jP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "agHLCOBM5jP", "replyto": "agHLCOBM5jP", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3284/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538078579, "tmdate": 1606915798938, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3284/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3284/-/Official_Review"}}}, {"id": "bl-M8MAdge", "original": null, "number": 11, "cdate": 1605615658795, "ddate": null, "tcdate": 1605615658795, "tmdate": 1606255882608, "tddate": null, "forum": "agHLCOBM5jP", "replyto": "agHLCOBM5jP", "invitation": "ICLR.cc/2021/Conference/Paper3284/-/Official_Comment", "content": {"title": "Revised Paper Uploaded", "comment": "We thank everyone for their constructive feedback. We are happy to see that our paper was generally well received by all reviewers. \n\nWe uploaded a modified PDF containing changes suggested by the reviewers' comments and corresponding to our proposals made in the rebuttal. \n\nPlease note that for easy reviewing, all changes are marked in blue. \n\nAdditionally, we wish to clarify that a remark by Reviewer 3 about the limited resolution of input images in our datasets may have been a result of misunderstanding. Some of our benchmark biomedical datasets are already 1024x1024 and our method works very well at denoising those images. We have explained this in detail in our response below to Reviewer 3's comments. In short, our method is not restricted by the resolution of input images.\n\nIf the reviewers have additional remarks, we are happy to engage in further discussion.\n\nThanks,\n\nthe authors"}, "signatures": ["ICLR.cc/2021/Conference/Paper3284/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3284/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders", "authorids": ["~Mangal_Prakash1", "~Alexander_Krull3", "~Florian_Jug1"], "authors": ["Mangal Prakash", "Alexander Krull", "Florian Jug"], "keywords": ["Diversity denoising", "Unsupervised denoising", "Variational Autoencoders", "Noise model"], "abstract": "Deep Learning based methods have emerged as the indisputable leaders for virtually all image restoration tasks. Especially in the domain of microscopy images, various content-aware image restoration (CARE) approaches are now used to improve the interpretability of acquired data. Naturally, there are limitations to what can be restored in corrupted images, and like for all inverse problems, many potential solutions exist, and one of them must be chosen. Here, we propose DivNoising, a denoising approach based on fully convolutional variational autoencoders (VAEs), overcoming the problem of having to choose a single solution by predicting a whole distribution of denoised images. First we introduce a principled way of formulating the unsupervised denoising problem within the VAE framework by explicitly incorporating imaging noise models into the decoder. Our approach is fully unsupervised, only requiring noisy images and a suitable description of the imaging noise distribution. We show that such a noise model can either be measured, bootstrapped from noisy data, or co-learned during training. If desired, consensus predictions can be inferred from a set of DivNoising predictions, leading to competitive results with other unsupervised methods and, on occasion, even with the supervised state-of-the-art. DivNoising samples from the posterior enable a plethora of useful applications. We are (i) showing denoising results for 13 datasets, (ii) discussing how optical character recognition (OCR) applications can benefit from diverse predictions, and are (iii) demonstrating how instance cell segmentation improves when using diverse DivNoising predictions.", "one-sentence_summary": "DivNoising performs fully unsupervised diversity denoising using fully convolutional variational autoencoders and achieves SOTA results for a number of well known datasets while also enabling VAE-like sampling", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "prakash|fully_unsupervised_diversity_denoising_with_convolutional_variational_autoencoders", "pdf": "/pdf/2afe972808ebb66f3926468902039c366b274c59.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nprakash2021fully,\ntitle={Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders},\nauthor={Mangal Prakash and Alexander Krull and Florian Jug},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=agHLCOBM5jP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "agHLCOBM5jP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3284/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3284/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3284/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3284/Authors|ICLR.cc/2021/Conference/Paper3284/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3284/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839120, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3284/-/Official_Comment"}}}, {"id": "Aof1j7GlDDA", "original": null, "number": 13, "cdate": 1605785610002, "ddate": null, "tcdate": 1605785610002, "tmdate": 1606250008874, "tddate": null, "forum": "agHLCOBM5jP", "replyto": "9yhM7Ovcu0T", "invitation": "ICLR.cc/2021/Conference/Paper3284/-/Official_Comment", "content": {"title": "Resolution concern is a possible misunderstanding; noise model concern is only valid in the natural image domain - our paper has a strong focus on biomedical image data though...", "comment": "In the context of natural images we agree with most of the reviewer's comments but not in the context of data domain we target. Regardless of the domain, our approach already handles high resolution images (1024x1024; explanation follows below as minor comment). But first we address the question of noise models in the context of our target data domain.\n\nOur paper is clearly targeted for the important domain of\u00a0**biomedical images (microscopy images)** in the context of computational biology,\u00a0an officially listed subject area of ICLR (https://iclr.cc/Conferences/2021/CallForPapers). In virtually all practical setups **clean (pixel-noise free) data can simply not be acquired**, thereby invalidating the idea of using the noise models to create paired data for supervised training (which would require access to the unobtainable clean data). But **even if some clean data would be available, a correct noise model is usually not**. Also, this is consequently invalidating the idea of image pair generation for supervised training. Hence, the **ability of DivNoising to bootstrap or co-learn a suitable noise model from noisy images alone** is another **key contribution** of our work.\n\nAdditionally, in microscopy image data, **pixel-wise noise is the most dominant \"real noise\u201d** (see [1-6]), rendering our method very practical and useful for many analysis tasks in computational biology.\n\nMinor comments:\n* While BSD68 images have indeed a limited pixel-resolution, our Convallaria data in the paper, for example, is 1024x1024, but due to the fully convolutional nature of DivNoising this is by no means the upper limit. Hence, our method is not limited by resolution of images.\n* Our work does not focus on denoising of natural images. BSD68 is a very standard natural image denoising dataset and has been used by numerous state-of-the-art methods for benchmarking (see [7-10]). DivNoising results for this dataset was included in the appendix to have a reference to other existing methods in the context of natural images. It is not central to our work. While we could use Urban100 or DIV2K data as well, these are not (yet) commonly used benchmarking denoising datasets. In general, we would prefer not to put additional emphasis on results on natural images. Still, if strongly requested, we can run DivNoising on Urban100 and add results to the appendix in the final version.\n\nIn summary, while DivNoising certainly has merit in multiple domains, **we would like to be evaluated in the context of biomedical data and its specific limitations**.\n\n**References** _(sorry for terrible formatting, but the markdown parser of OpenReview is very limiting):_\n\n[1] Weigert, Martin, et al. \"Content-aware image restoration: pushing the limits of fluorescence microscopy.\" Nature methods 15.12 (2018): 1090-1097. \n\n[2] Krull, Alexander, et al.. \"Probabilistic noise2void: Unsupervised content-aware denoising.\" arXiv preprint arXiv:1906.00651 (2019).  \n\n[3] Prakash, Mangal, et al. \"Fully unsupervised probabilistic noise2void.\" 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI). IEEE, 2020. \n\n[4] Khademi, Wesley, et al. \"Self-Supervised Poisson-Gaussian Denoising.\" arXiv preprint arXiv:2002.09558 (2020). \n\n[5] Zhou, Ruofan, et al. \"W2S: A Joint Denoising and Super-Resolution Dataset.\" arXiv preprint arXiv:2003.05961 (2020). \n\n[6] Zhang, Yide, et al. \u201cA poisson-gaussian denoising dataset with real fluorescence microscopy images.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.\u201d \n\n[7]\u00a0Zhang, Kai et al., \"Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising.\"arXiv preprint arXiv:1608.03981 (2016). \n\n[8] Lehtinen, Jaakko, et al. \"Noise2noise: Learning image restoration without clean data.\" arXiv preprint arXiv:1803.04189 (2018). \n\n[9]\u00a0Krull, Alexander, et al. \"Noise2void-learning denoising from single noisy images.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019. \n\n[10]\u00a0Quan, Yuhui, et al. \"Self2Self With Dropout: Learning Self-Supervised Denoising From Single Image.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020."}, "signatures": ["ICLR.cc/2021/Conference/Paper3284/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3284/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders", "authorids": ["~Mangal_Prakash1", "~Alexander_Krull3", "~Florian_Jug1"], "authors": ["Mangal Prakash", "Alexander Krull", "Florian Jug"], "keywords": ["Diversity denoising", "Unsupervised denoising", "Variational Autoencoders", "Noise model"], "abstract": "Deep Learning based methods have emerged as the indisputable leaders for virtually all image restoration tasks. Especially in the domain of microscopy images, various content-aware image restoration (CARE) approaches are now used to improve the interpretability of acquired data. Naturally, there are limitations to what can be restored in corrupted images, and like for all inverse problems, many potential solutions exist, and one of them must be chosen. Here, we propose DivNoising, a denoising approach based on fully convolutional variational autoencoders (VAEs), overcoming the problem of having to choose a single solution by predicting a whole distribution of denoised images. First we introduce a principled way of formulating the unsupervised denoising problem within the VAE framework by explicitly incorporating imaging noise models into the decoder. Our approach is fully unsupervised, only requiring noisy images and a suitable description of the imaging noise distribution. We show that such a noise model can either be measured, bootstrapped from noisy data, or co-learned during training. If desired, consensus predictions can be inferred from a set of DivNoising predictions, leading to competitive results with other unsupervised methods and, on occasion, even with the supervised state-of-the-art. DivNoising samples from the posterior enable a plethora of useful applications. We are (i) showing denoising results for 13 datasets, (ii) discussing how optical character recognition (OCR) applications can benefit from diverse predictions, and are (iii) demonstrating how instance cell segmentation improves when using diverse DivNoising predictions.", "one-sentence_summary": "DivNoising performs fully unsupervised diversity denoising using fully convolutional variational autoencoders and achieves SOTA results for a number of well known datasets while also enabling VAE-like sampling", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "prakash|fully_unsupervised_diversity_denoising_with_convolutional_variational_autoencoders", "pdf": "/pdf/2afe972808ebb66f3926468902039c366b274c59.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nprakash2021fully,\ntitle={Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders},\nauthor={Mangal Prakash and Alexander Krull and Florian Jug},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=agHLCOBM5jP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "agHLCOBM5jP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3284/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3284/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3284/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3284/Authors|ICLR.cc/2021/Conference/Paper3284/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3284/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839120, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3284/-/Official_Comment"}}}, {"id": "Px2-lv-rPBg", "original": null, "number": 4, "cdate": 1603897681057, "ddate": null, "tcdate": 1603897681057, "tmdate": 1605942495867, "tddate": null, "forum": "agHLCOBM5jP", "replyto": "agHLCOBM5jP", "invitation": "ICLR.cc/2021/Conference/Paper3284/-/Official_Review", "content": {"title": "Estimating a distribution of denoised images instead of a single denoised image is not new (ref missing), but an interesting alternative was proposed. I am concerned on evaluations.", "review": "This manuscript proposed an interesting method to infer the distribution of denoised images for a given input instead of a single denoised image. Then, the final denoise image was generated using MMSE (averaging multiple denoised images) or using MAP (finding the mode of the posterior distribution).\n\n1) Unfortunately, this manuscript missed one very important relevant recent work:\n[Quan2020] Quan et al., Self2Self With Dropout: Learning Self-Supervised Denoising From Single Image, CVPR 2020.\nThis work also infers the distribution of denoised images instead of a single denoised image and yielded state-of-the-art results for both synthetic noise, real noise, salt-and-pepper noise and inpainting. \nThe ways of implementing distribution generations seem similar (\"a principled approach to incorporate explicit models of the imaging noise distribution in the decoder of a VAE\" in this manuscript and also similar structure was implemented in the decoder part of the network in [Quan2020]) except that the proposed method here utilized random sampling in the latent space while [Quan2020] used dropout to implement it. In my view, [Quan2020] and the proposed method share some common aspects that can undermine an important contribution of this manuscript in its current form, and thus this manuscript must explain the advantages and novelty of the proposed method over [Quan2020] properly. \n\nEven though the proposed method with MAP is still new, for now [Quan2020] seems to have advantages over the proposed method such as (a) Self2Self requires a single noisy image while the proposed method requires noisy dataset, thus Self2Self can cover the case in this manuscript while the proposed method may not be able to cover the case of [Quan2020] with a single noisy image, (b) Self2Self yielded state-of-the-art performance on well known datasets such as Set9, BSD68, thus the readers can know that Self2Self is one of the state-of-the-art denoising methods. Even though the proposed method was evaluated on 13 datasets, none of them are used to evaluate denoisers in general. Therefore, it is hard to justify that the current proposed method also achieved state-of-the-art performance. (c) Self2Self was evaluated on both color, gray-scale images with synthetic noises as well as real noise (PolyU) while the proposed method was evaluated only on gray-scale images with real noise.\n\nIn addition, the way of implementing the generation network for samples was already proposed in [Kohl2018] for segmentation task. Thus, the contribution of the proposed method could be weakened substantially according to [Kohl2018], too, even though this manuscript nicely demonstrated that this sampling method in [Kohl2018] worked well for denoising problems.\n\n2) Table 1 shows 13 datasets, but it is hard to use them to compare with other denoiser results in other literature. Many denoising works are using common datasets such as Set9, BSD68 (for synthetic noise), and often PolyU (for real noise) and it is helpful to have these results (at least in the appendix) for the comparison purpose with other previous works. Moreover, most images that were used in this manuscript seem to have specific structures (cells, characters), so I am not convinced that the proposed method will be generalized to regular images and color images. The proposed method has noise estimation components (co-learned, bootstrapped) that may be sensitive to underlying ground truth image structures and properties. Showing some results for common datasets (with complicated textures and patterns) as well as some synthetic noise with different noise levels (ideal cases) will convince readers that the proposed method can be generalized to other cases. The proposed method involves noise estimation and it seems to work well with texts, faces and microscopy imaging. However, these results do not seem to be easily extended to other imaging cases. \n\nIn addition, it seems that generating 1000 images for averaging will be computationally expensive. Thus, in Table 1, it will be fair to indicate computation time for training / testing per image.\n\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3284/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3284/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders", "authorids": ["~Mangal_Prakash1", "~Alexander_Krull3", "~Florian_Jug1"], "authors": ["Mangal Prakash", "Alexander Krull", "Florian Jug"], "keywords": ["Diversity denoising", "Unsupervised denoising", "Variational Autoencoders", "Noise model"], "abstract": "Deep Learning based methods have emerged as the indisputable leaders for virtually all image restoration tasks. Especially in the domain of microscopy images, various content-aware image restoration (CARE) approaches are now used to improve the interpretability of acquired data. Naturally, there are limitations to what can be restored in corrupted images, and like for all inverse problems, many potential solutions exist, and one of them must be chosen. Here, we propose DivNoising, a denoising approach based on fully convolutional variational autoencoders (VAEs), overcoming the problem of having to choose a single solution by predicting a whole distribution of denoised images. First we introduce a principled way of formulating the unsupervised denoising problem within the VAE framework by explicitly incorporating imaging noise models into the decoder. Our approach is fully unsupervised, only requiring noisy images and a suitable description of the imaging noise distribution. We show that such a noise model can either be measured, bootstrapped from noisy data, or co-learned during training. If desired, consensus predictions can be inferred from a set of DivNoising predictions, leading to competitive results with other unsupervised methods and, on occasion, even with the supervised state-of-the-art. DivNoising samples from the posterior enable a plethora of useful applications. We are (i) showing denoising results for 13 datasets, (ii) discussing how optical character recognition (OCR) applications can benefit from diverse predictions, and are (iii) demonstrating how instance cell segmentation improves when using diverse DivNoising predictions.", "one-sentence_summary": "DivNoising performs fully unsupervised diversity denoising using fully convolutional variational autoencoders and achieves SOTA results for a number of well known datasets while also enabling VAE-like sampling", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "prakash|fully_unsupervised_diversity_denoising_with_convolutional_variational_autoencoders", "pdf": "/pdf/2afe972808ebb66f3926468902039c366b274c59.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nprakash2021fully,\ntitle={Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders},\nauthor={Mangal Prakash and Alexander Krull and Florian Jug},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=agHLCOBM5jP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "agHLCOBM5jP", "replyto": "agHLCOBM5jP", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3284/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538078579, "tmdate": 1606915798938, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3284/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3284/-/Official_Review"}}}, {"id": "PaY3ydPm8_Q", "original": null, "number": 5, "cdate": 1605178051357, "ddate": null, "tcdate": 1605178051357, "tmdate": 1605799928285, "tddate": null, "forum": "agHLCOBM5jP", "replyto": "M5SQwAT8fyT", "invitation": "ICLR.cc/2021/Conference/Paper3284/-/Official_Comment", "content": {"title": "Initial Point by Point Rebuttal (and invitation for further discussion)", "comment": "We thank the reviewer for her/his insightful comments and overall encouraging evaluation of our work. We address the concerns raised point by point below.\n\n**Remark 1: Comparison to Kingma & Welling (2014)** \nIt is true that our method relies on the VAE framework by Kingma and Welling (2014). Our novel contribution is the formulation of the unsupervised denoising problem within this framework, by including the imaging noise distribution (noise model) in the decoder. This formulation has, to our knowledge, never been attempted before.\nWhile it is possible to achieve a denoising effect by directly applying Kingma\u2019s VAEs without an explicit noise model in the decoder, the original VAE was not designed for denoising and has to effectively predict a separate noise model for each pixel.\nWe clearly highlight our advantages over this approach in the subsection \u201cDenoising with vanilla VAEs\u201d as well as in Fig. 3, Table 1, and Appendix Figures 27 and 28.\n\nAdditionally, we propose a new fully unsupervised DivNoising setup as well, where the decoder can learn the noise model on the fly as a function of the predicted signal.\nTo summarize, our contributions are threefold: \n*(i)* We are the first to present an unsupervised denoising method enabling the sampling of diverse solutions from an approximated true signal posterior distribution (in contrast to state-of-the-art baselines as well as Quan et al (2020) referenced by Reviewer 4). \n*(ii)* We propose a fully unsupervised DivNoising variant that does not require an apriori known noise model. \n*(iii)* Both of our proposed variants allow us to learn an approximation to the real posterior, from which we can then sample efficiently. These individual samples are useful for downstream processing as shown with applications to OCR and cell segmentation. Additionally, we show how to get point estimates such as MMSE and MAP (which is not obtainable with other methods).\n\n**Remarks 2 and 4: How can the proposed method deal with images after demosaicing or ISP denoising?**\nIn the current form, our method does not deal with such tasks. In this paper we are specifically looking at pixel-noises, which are the dominating sources of noise in biomedical microscopy images. (Note that the domains we focus on are of extraordinarily high practical relevance and that many biomedical research projects will directly benefit from employing DivNoising in their downstream analyses.)\nHowever, if future work describes how suitable noise models $p(x|s)$ can be constructed/learned for other/more complex types of noise, then our approach will be able to use them. We want to emphasize that our contribution is to demonstrate a principled way to use noise models in VAEs for diverse and unsupervised denoising, an application/direction that was never explored before. \n\n**Remark 3: Denoising results of the proposed method for high resolution nature images**\nWe discuss the results for natural images on benchmark BSD68 dataset in our discussion section and also show results of our method on this dataset in Appendix Fig. 25. More discussion about the same is found in Appendix Section A10. \nWe find that for such images, we perform almost on par with our baseline Noise2Void, but we additionally learn the full posterior which gives us access to diverse sampling unlike other methods. Not outperforming baseline methods on diverse natural images is in line with our expectations since natural images have tremendous content diversity and learning a full generative model of such data will require networks with higher capacity. (Note that virtually all image generation methods and applications focus on rather narrow domains such as faces or other limited classes of objects.) However, we strongly believe that with more sophisticated VAE architectures, DivNoising will improve also on rich domains such as natural images.\nStill, we want to remind the reviewer that the image domain we target in our work is of extraordinarily high value for many researchers that analyze microscopy images on a daily basis as well as for OCR and facial recognition applications in forensics which often rely on image restoration.\n\n**Finally**, we want to encourage the reviewer to further comment and discuss our responses. We are very open to further constructive criticism and are happy to include clarifications in the final version of paper. Thanks! :)\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3284/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3284/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders", "authorids": ["~Mangal_Prakash1", "~Alexander_Krull3", "~Florian_Jug1"], "authors": ["Mangal Prakash", "Alexander Krull", "Florian Jug"], "keywords": ["Diversity denoising", "Unsupervised denoising", "Variational Autoencoders", "Noise model"], "abstract": "Deep Learning based methods have emerged as the indisputable leaders for virtually all image restoration tasks. Especially in the domain of microscopy images, various content-aware image restoration (CARE) approaches are now used to improve the interpretability of acquired data. Naturally, there are limitations to what can be restored in corrupted images, and like for all inverse problems, many potential solutions exist, and one of them must be chosen. Here, we propose DivNoising, a denoising approach based on fully convolutional variational autoencoders (VAEs), overcoming the problem of having to choose a single solution by predicting a whole distribution of denoised images. First we introduce a principled way of formulating the unsupervised denoising problem within the VAE framework by explicitly incorporating imaging noise models into the decoder. Our approach is fully unsupervised, only requiring noisy images and a suitable description of the imaging noise distribution. We show that such a noise model can either be measured, bootstrapped from noisy data, or co-learned during training. If desired, consensus predictions can be inferred from a set of DivNoising predictions, leading to competitive results with other unsupervised methods and, on occasion, even with the supervised state-of-the-art. DivNoising samples from the posterior enable a plethora of useful applications. We are (i) showing denoising results for 13 datasets, (ii) discussing how optical character recognition (OCR) applications can benefit from diverse predictions, and are (iii) demonstrating how instance cell segmentation improves when using diverse DivNoising predictions.", "one-sentence_summary": "DivNoising performs fully unsupervised diversity denoising using fully convolutional variational autoencoders and achieves SOTA results for a number of well known datasets while also enabling VAE-like sampling", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "prakash|fully_unsupervised_diversity_denoising_with_convolutional_variational_autoencoders", "pdf": "/pdf/2afe972808ebb66f3926468902039c366b274c59.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nprakash2021fully,\ntitle={Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders},\nauthor={Mangal Prakash and Alexander Krull and Florian Jug},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=agHLCOBM5jP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "agHLCOBM5jP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3284/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3284/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3284/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3284/Authors|ICLR.cc/2021/Conference/Paper3284/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3284/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839120, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3284/-/Official_Comment"}}}, {"id": "8OG-aBgVA0k", "original": null, "number": 4, "cdate": 1605175820048, "ddate": null, "tcdate": 1605175820048, "tmdate": 1605799896716, "tddate": null, "forum": "agHLCOBM5jP", "replyto": "hF48vV86MjP", "invitation": "ICLR.cc/2021/Conference/Paper3284/-/Official_Comment", "content": {"title": "Initial Point by Point Rebuttal (and invitation for further discussion)", "comment": "We thank the reviewer for an encouraging evaluation of our paper and we very much appreciate the insightful feedback.\n\nWe agree that the prior over clean images $p(s)$ deserves more discussion. \nHere, we **want to attempt a clarification, which we suggest could in a similar form be included in the final version of the paper**.\n\nAs you mention in your review, traditional methods often explicitly model $p(s)$ e.g. as a function of smoothness, presuming a higher apriori probability for smooth images.\nAlbeit leading to respectable results, this is obviously an oversimplification. We can expect the true distribution $p(s)$ of clean images to be much more complex (e.g. for a particular experimental setup in a fluorescence microscope). \nInstead of explicitly modelling $p(s)$, our VAE based approach only implicitly describes $p(s)$ as $\\int p_\\theta(s|z) p(z) dz$ over all possible values of $z$.\nWhile the prior $p(z)$ is indeed assumed to be a unit Gaussian distribution, the conditional distribution $p_{\\theta}(s|z)$ is learned by the decoder network. \nIt is a dirac distribution centered at $g_{\\theta}(z)$, i.e. the function implemented by the decoder network.\nDepending on its parameters, the network will implement the function differently, which ultimately leads to a different $p(s)$.\nUnlike with the traditional smoothness prior, we cannot directly calculate a probability density for $p(s)$ for a given $s$.\nHowever, to check the plausibility of the learned distribution, we can generate samples from $p(s)$ by first sampling from the unit Gaussian $p(z)$ in latent space and then feeding the samples through the decoder network. We show this in Figures 8-10 in our appendix.\nAlbeit not perfect, we believe that our implicit prior produces plausible results, especially when considering local structures.\n\nAs you point out in your review, since the work on \u2018deep image prior\u2019, convolutional neural networks are understood to have an inherent denoising effect.\nWe believe this makes them especially suitable for our task and contributes to the quality of our results and the quality of the learned $p(s)$.\nHowever, note that the denoising quality we achieve is more than the sheer result of the convolutional architecture, which can for example be seen by the fact that we improve over all other convolutional baselines.\nInstead, to separate image content and noise, we rely on a generative model of image generation, which includes an appropriate noise model. We argue in Appendix Section A7 and show in Figures 11 and 12 in our appendix that deviating from the correct noise model (by changing $\\beta$) worsens the results. (Also note that DivNoising outperforms results obtained with a regular VAE that does not employ a suitable noise model, as we show in Table 1 and  in Figures 27 and 28 in our appendix.) \n\nLast but not least, **we are happy to include and discuss the mentioned literature in the updated version of our paper** alongside the already cited Deep Image Prior.\n\n**Finally**, we want to encourage the reviewer to further comment and discuss our responses. We are very open to further constructive criticism and are happy to include clarifications in the final version of paper.\nThanks! :)\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3284/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3284/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders", "authorids": ["~Mangal_Prakash1", "~Alexander_Krull3", "~Florian_Jug1"], "authors": ["Mangal Prakash", "Alexander Krull", "Florian Jug"], "keywords": ["Diversity denoising", "Unsupervised denoising", "Variational Autoencoders", "Noise model"], "abstract": "Deep Learning based methods have emerged as the indisputable leaders for virtually all image restoration tasks. Especially in the domain of microscopy images, various content-aware image restoration (CARE) approaches are now used to improve the interpretability of acquired data. Naturally, there are limitations to what can be restored in corrupted images, and like for all inverse problems, many potential solutions exist, and one of them must be chosen. Here, we propose DivNoising, a denoising approach based on fully convolutional variational autoencoders (VAEs), overcoming the problem of having to choose a single solution by predicting a whole distribution of denoised images. First we introduce a principled way of formulating the unsupervised denoising problem within the VAE framework by explicitly incorporating imaging noise models into the decoder. Our approach is fully unsupervised, only requiring noisy images and a suitable description of the imaging noise distribution. We show that such a noise model can either be measured, bootstrapped from noisy data, or co-learned during training. If desired, consensus predictions can be inferred from a set of DivNoising predictions, leading to competitive results with other unsupervised methods and, on occasion, even with the supervised state-of-the-art. DivNoising samples from the posterior enable a plethora of useful applications. We are (i) showing denoising results for 13 datasets, (ii) discussing how optical character recognition (OCR) applications can benefit from diverse predictions, and are (iii) demonstrating how instance cell segmentation improves when using diverse DivNoising predictions.", "one-sentence_summary": "DivNoising performs fully unsupervised diversity denoising using fully convolutional variational autoencoders and achieves SOTA results for a number of well known datasets while also enabling VAE-like sampling", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "prakash|fully_unsupervised_diversity_denoising_with_convolutional_variational_autoencoders", "pdf": "/pdf/2afe972808ebb66f3926468902039c366b274c59.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nprakash2021fully,\ntitle={Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders},\nauthor={Mangal Prakash and Alexander Krull and Florian Jug},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=agHLCOBM5jP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "agHLCOBM5jP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3284/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3284/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3284/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3284/Authors|ICLR.cc/2021/Conference/Paper3284/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3284/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839120, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3284/-/Official_Comment"}}}, {"id": "DCEYcRSiLf", "original": null, "number": 3, "cdate": 1605174126703, "ddate": null, "tcdate": 1605174126703, "tmdate": 1605799774225, "tddate": null, "forum": "agHLCOBM5jP", "replyto": "-g8JIhYEBqN", "invitation": "ICLR.cc/2021/Conference/Paper3284/-/Official_Comment", "content": {"title": "Initial Point by Point Rebuttal (and invitation for further discussion)", "comment": "We thank you for your encouraging comments on our approach. We also thank you for your insightful questions. We address these questions one by one below:\n\n**Remark:** \u201cis there any assumptions about this noise model and which type of noises are mainly considered to be removed?\u201d\nThe scope of this paper is to deal with per pixel noise as it commonly appears in microscopy data.  Such noise is inflicted by limited illumination and detector/camera imperfections and is the dominant source of noise in biomedical imaging [1,2,3,4,5], astronomy [6], as well as raw photography data [7,8]. \nThis being said, our approach can in principle be applied to any image as long as a probabilistic noise model can be described. We show, for the types of noise we consider (i.e. Poisson shot noise, Gaussian readout noise, and salt&pepper noise), how appropriate noise models can be measured, bootstrapped, or co-learned (fully unsupervised).\n\n**Remark:** \u201cWhether this denoiser can be applied to other noisy scenarios, such as a real color noisy image or MRI.\u201d\nIn the context of microscopy image denoising an extension to RGB images is not needed. Still, such an extension is simple engineering and we will eventually include this into our open source framework.\nOther biomedical imaging modalities, such as MRI or CT (tomography) are studied by many groups, and we are ourselves looking into extension of the ideas presented here in order to restore said imagery. The challenge is to define and learn suitable noise models that go beyond per-pixel noise and can capture larger spatial dependencies.\nThe same is true for removing noise from consumer level RGB images that have been subject to compression or demosaicing.\n\n**Remark:** \u201cOpen the source codes.\u201d\nWe did not disclose the source code for submission for reasons of anonymity. Our lab is proud to say that we provide the sources for all our methods and put extra emphasis on reproducibility of all reported results. (For an anonymized preview during the rebuttal period, please request in a comment below.)\n\n**Finally**, we want to encourage the reviewer to further comment and discuss our responses. We are very open to further constructive criticism and are happy to include clarifications in the final version of paper.\nThanks! :)\n\n[1]  Weigert, Martin, et al. \"Content-aware image restoration: pushing the limits of fluorescence microscopy.\" Nature methods 15.12 (2018): 1090-1097.\n\n[2] Krull, Alexander, et al.. \"Probabilistic noise2void: Unsupervised content-aware denoising.\" arXiv preprint arXiv:1906.00651 (2019).\n\n[3] Prakash, Mangal, et al. \"Fully unsupervised probabilistic noise2void.\" 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI). IEEE, 2020.\n\n[4] Khademi, Wesley, et al. \"Self-Supervised Poisson-Gaussian Denoising.\" arXiv preprint arXiv:2002.09558 (2020).\n\n[5] Zhou, Ruofan, et al. \"W2S: A Joint Denoising and Super-Resolution Dataset.\" arXiv preprint arXiv:2003.05961 (2020).\n\n[6] Beckouche, Simon et al. \"Astronomical image denoising using dictionary learning.\" Astronomy & Astrophysics 556 (2013): A132.\n\n[7] Kumar, Prashanth et al. \"Low Rank Poisson Denoising (LRPD): A Low Rank Approach Using Split Bregman Algorithm for Poisson Noise Removal From Images.\" CVPR Workshops. 2019.\n\n[8] Laine, Samuli, et al. \"High-quality self-supervised deep image denoising.\" Advances in Neural Information Processing Systems. 2019."}, "signatures": ["ICLR.cc/2021/Conference/Paper3284/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3284/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders", "authorids": ["~Mangal_Prakash1", "~Alexander_Krull3", "~Florian_Jug1"], "authors": ["Mangal Prakash", "Alexander Krull", "Florian Jug"], "keywords": ["Diversity denoising", "Unsupervised denoising", "Variational Autoencoders", "Noise model"], "abstract": "Deep Learning based methods have emerged as the indisputable leaders for virtually all image restoration tasks. Especially in the domain of microscopy images, various content-aware image restoration (CARE) approaches are now used to improve the interpretability of acquired data. Naturally, there are limitations to what can be restored in corrupted images, and like for all inverse problems, many potential solutions exist, and one of them must be chosen. Here, we propose DivNoising, a denoising approach based on fully convolutional variational autoencoders (VAEs), overcoming the problem of having to choose a single solution by predicting a whole distribution of denoised images. First we introduce a principled way of formulating the unsupervised denoising problem within the VAE framework by explicitly incorporating imaging noise models into the decoder. Our approach is fully unsupervised, only requiring noisy images and a suitable description of the imaging noise distribution. We show that such a noise model can either be measured, bootstrapped from noisy data, or co-learned during training. If desired, consensus predictions can be inferred from a set of DivNoising predictions, leading to competitive results with other unsupervised methods and, on occasion, even with the supervised state-of-the-art. DivNoising samples from the posterior enable a plethora of useful applications. We are (i) showing denoising results for 13 datasets, (ii) discussing how optical character recognition (OCR) applications can benefit from diverse predictions, and are (iii) demonstrating how instance cell segmentation improves when using diverse DivNoising predictions.", "one-sentence_summary": "DivNoising performs fully unsupervised diversity denoising using fully convolutional variational autoencoders and achieves SOTA results for a number of well known datasets while also enabling VAE-like sampling", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "prakash|fully_unsupervised_diversity_denoising_with_convolutional_variational_autoencoders", "pdf": "/pdf/2afe972808ebb66f3926468902039c366b274c59.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nprakash2021fully,\ntitle={Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders},\nauthor={Mangal Prakash and Alexander Krull and Florian Jug},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=agHLCOBM5jP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "agHLCOBM5jP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3284/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3284/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3284/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3284/Authors|ICLR.cc/2021/Conference/Paper3284/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3284/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839120, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3284/-/Official_Comment"}}}, {"id": "9yhM7Ovcu0T", "original": null, "number": 12, "cdate": 1605772551608, "ddate": null, "tcdate": 1605772551608, "tmdate": 1605772551608, "tddate": null, "forum": "agHLCOBM5jP", "replyto": "PaY3ydPm8_Q", "invitation": "ICLR.cc/2021/Conference/Paper3284/-/Official_Comment", "content": {"title": "concerns", "comment": "I admit that the proposed framework is useful in some scenarios and interesting. However, if it can only handle low-resolution images with known pixel independent noises, the usage is quite limited. In the rebuttal, the authors claim that they test their method in BSD. But the resolution of it is somewhat small (less than 500). There are many other datasets with high-resolution (e.g. Urban100, DIV2K). Also, the authors claim that 'if future work describes how suitable noise models can be constructed/learned for other/more complex types of noise, then our approach will be able to use them.' I do not think it is easy to model the real noises. Furthermore, if the noise model is known, we can directly generate noisy and clean image pairs for supervised learning with better performance."}, "signatures": ["ICLR.cc/2021/Conference/Paper3284/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3284/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders", "authorids": ["~Mangal_Prakash1", "~Alexander_Krull3", "~Florian_Jug1"], "authors": ["Mangal Prakash", "Alexander Krull", "Florian Jug"], "keywords": ["Diversity denoising", "Unsupervised denoising", "Variational Autoencoders", "Noise model"], "abstract": "Deep Learning based methods have emerged as the indisputable leaders for virtually all image restoration tasks. Especially in the domain of microscopy images, various content-aware image restoration (CARE) approaches are now used to improve the interpretability of acquired data. Naturally, there are limitations to what can be restored in corrupted images, and like for all inverse problems, many potential solutions exist, and one of them must be chosen. Here, we propose DivNoising, a denoising approach based on fully convolutional variational autoencoders (VAEs), overcoming the problem of having to choose a single solution by predicting a whole distribution of denoised images. First we introduce a principled way of formulating the unsupervised denoising problem within the VAE framework by explicitly incorporating imaging noise models into the decoder. Our approach is fully unsupervised, only requiring noisy images and a suitable description of the imaging noise distribution. We show that such a noise model can either be measured, bootstrapped from noisy data, or co-learned during training. If desired, consensus predictions can be inferred from a set of DivNoising predictions, leading to competitive results with other unsupervised methods and, on occasion, even with the supervised state-of-the-art. DivNoising samples from the posterior enable a plethora of useful applications. We are (i) showing denoising results for 13 datasets, (ii) discussing how optical character recognition (OCR) applications can benefit from diverse predictions, and are (iii) demonstrating how instance cell segmentation improves when using diverse DivNoising predictions.", "one-sentence_summary": "DivNoising performs fully unsupervised diversity denoising using fully convolutional variational autoencoders and achieves SOTA results for a number of well known datasets while also enabling VAE-like sampling", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "prakash|fully_unsupervised_diversity_denoising_with_convolutional_variational_autoencoders", "pdf": "/pdf/2afe972808ebb66f3926468902039c366b274c59.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nprakash2021fully,\ntitle={Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders},\nauthor={Mangal Prakash and Alexander Krull and Florian Jug},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=agHLCOBM5jP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "agHLCOBM5jP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3284/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3284/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3284/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3284/Authors|ICLR.cc/2021/Conference/Paper3284/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3284/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839120, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3284/-/Official_Comment"}}}, {"id": "PUtktqJT-Pn", "original": null, "number": 2, "cdate": 1605126971246, "ddate": null, "tcdate": 1605126971246, "tmdate": 1605713998295, "tddate": null, "forum": "agHLCOBM5jP", "replyto": "Px2-lv-rPBg", "invitation": "ICLR.cc/2021/Conference/Paper3284/-/Official_Comment", "content": {"title": "Initial Point by Point Rebuttal (and invitation for further discussion)", "comment": "We very much appreciate the insightful comments in this review.\nHere, we will comment on all points while maintaining the proposed order and numbering scheme of the review.\n\n1. **Comparison to other methods:**\nQuan et al. (Self2Self) describe an interesting denoising method, but it is very different to our method (DivNoising). Self2Self is not based on VAEs but on a U-Net-like architecture with blind-spots and dropout. Hence, it leverages \u2018model uncertainty\u2019, while we are the first to approximate the real (data) posterior, allowing us to get a grip on the \u2018data uncertainty\u2019. \nWe stress that unlike Self2Self, DivNoising uses an explicit noise model $p(x|s)$.   \n**PROPOSAL**: we discuss Self2Self in our related work section.\n\n**a. Training on single inputs vs. whole datasets:**\nSelf2Self is trained on single images. For 4 randomly chosen single images from 4 datasets, we have now run Self2Self and compared it to DivNoising (i) trained on same single image, and (ii) trained on entire dataset. DivNoising leads to superior results in a fraction of the training time. Applying Self2Self to a full dataset would take an impractical amount of time (about 1000 GPU hours vs. 10 GPU hours for DivNoising). Biomedical research datasets typically consist of many images (GB or TB).\n**PROPOSAL**: we include Self2Self as additional baseline in appendix and refer to it in main text.\n\n**Comparison of Self2Self (left) to DivNoising trained on same single image (middle) and DivNoising trained on full dataset (right) :**\n\nPSNR (dB)\n* Convallaria: 36.23; 36.42; **36.94**\n* Actin: 33.15; 33.80; **33.99**\n* Nuclei: 36.21; 35.99; **36.46**\n* W2S Ch1 av1: 31.40; **31.81**; 31.59\n\nTraining time (hours)\n* Convallaria: 10; **0.44**; 10\n* Actin: 10; **1.09**; 10\n* Nuclei: 10; **0.16**; 7\n* W2S Ch1 av1: 10; **0.48**; 3.75\n\nRequired GPU memory (GB)\n* Convallaria: 11; **1.5**; **1.5**\n* Actin: 11; **1.5**; **1.5**\n* Nuclei: 11; **1.5**; **1.5**\n* W2S Ch1 av1: 11; **1.5**; **1.5**\n\n**b. Baselines and benchmark datasets used:**\nWe propose a method for denoising of biomedical microscopy data, hence we are evaluating our performance on benchmark datasets widely used in this domain (see cited literature in paper) and compare to state-of-the-art baselines. \nThe official call for papers for ICLR 2021 explicitly lists applications in computational biology as a subject area (https://iclr.cc/Conferences/2021/CallForPapers), which initially motivated us to submit our work to ICLR. \nAll this being said, DivNoising is by no means limited to biomedical images. To this end, we show that commonly used applications such as OCR and face recognition in forensics can also benefit from our ideas. (Results on natural images BSD68 dataset are already in Appendix Fig. 25.)\n**PROPOSAL**: we make the scope of the paper clearer in introduction and include a paragraph on the importance of denoising for computational biology.\n\n**c. Single channel vs. RGB data:**\nMicroscopy images can have multiple fluorescence channels, but those are never RGB channels and are typically processed independently to avoid undesired crosstalk. Contrary to reviewer's remark, we show results on both real and synthetic data.\n**PROPOSAL**: we include additional baselines or single-channel (microscopy related) datasets if reviewers think it is strictly required.\n\n**Sampling scheme already proposed in [Kohl2018]?**\nThe paper by Kohl et al. is great, but by no means the first to propose the sampling scheme we use. Initially introduced by Kingma et al. (2014), it has been used by many papers and we discussed this in our Related Work section. Our contribution is the way we model the posterior of VAEs. We ask the reviewer to revisit our related work, where we have also explicitly mentioned and discussed [Kohl2018].\n \n2. **DivNoising on other image domains + computation time**\nExtending (generalizing) DivNoising to other image domains is an interesting future work. Here, we show how we can deal with dominant noises in microscopy: Poisson shot noise and Gaussian readout noise (and also salt&pepper noise). Hence, DivNoising will help many biomedical researchers analyze their data! Going beyond this (within biomedical domain) will extend to e.g. MRI or CT (tomography) data, but with the right noise model, we believe many domains can benefit from our general ideas.\n**PROPOSAL**: we extend the paragraph in the discussion section that points at challenges ahead.\n\n**Runtime for sampling 1000 DivNoised images (e.g. for MMSE estimation)**\nThis sampling, only needed during prediction, requires less than 7 seconds per image even for the largest image in all our datasets. \n**PROPOSAL**: we will include precise sampling time in final paper.\n\n**Finally,** we want to encourage the reviewer to comment on our proposals in order to let us understand if these are adequate measures to address her/his concerns. If we misunderstood some of the voiced criticisms, please critique us again as soon as possible. \n_Thanks!_ :)"}, "signatures": ["ICLR.cc/2021/Conference/Paper3284/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3284/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders", "authorids": ["~Mangal_Prakash1", "~Alexander_Krull3", "~Florian_Jug1"], "authors": ["Mangal Prakash", "Alexander Krull", "Florian Jug"], "keywords": ["Diversity denoising", "Unsupervised denoising", "Variational Autoencoders", "Noise model"], "abstract": "Deep Learning based methods have emerged as the indisputable leaders for virtually all image restoration tasks. Especially in the domain of microscopy images, various content-aware image restoration (CARE) approaches are now used to improve the interpretability of acquired data. Naturally, there are limitations to what can be restored in corrupted images, and like for all inverse problems, many potential solutions exist, and one of them must be chosen. Here, we propose DivNoising, a denoising approach based on fully convolutional variational autoencoders (VAEs), overcoming the problem of having to choose a single solution by predicting a whole distribution of denoised images. First we introduce a principled way of formulating the unsupervised denoising problem within the VAE framework by explicitly incorporating imaging noise models into the decoder. Our approach is fully unsupervised, only requiring noisy images and a suitable description of the imaging noise distribution. We show that such a noise model can either be measured, bootstrapped from noisy data, or co-learned during training. If desired, consensus predictions can be inferred from a set of DivNoising predictions, leading to competitive results with other unsupervised methods and, on occasion, even with the supervised state-of-the-art. DivNoising samples from the posterior enable a plethora of useful applications. We are (i) showing denoising results for 13 datasets, (ii) discussing how optical character recognition (OCR) applications can benefit from diverse predictions, and are (iii) demonstrating how instance cell segmentation improves when using diverse DivNoising predictions.", "one-sentence_summary": "DivNoising performs fully unsupervised diversity denoising using fully convolutional variational autoencoders and achieves SOTA results for a number of well known datasets while also enabling VAE-like sampling", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "prakash|fully_unsupervised_diversity_denoising_with_convolutional_variational_autoencoders", "pdf": "/pdf/2afe972808ebb66f3926468902039c366b274c59.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nprakash2021fully,\ntitle={Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders},\nauthor={Mangal Prakash and Alexander Krull and Florian Jug},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=agHLCOBM5jP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "agHLCOBM5jP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3284/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3284/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3284/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3284/Authors|ICLR.cc/2021/Conference/Paper3284/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3284/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839120, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3284/-/Official_Comment"}}}, {"id": "c7VSaG0nlMX", "original": null, "number": 10, "cdate": 1605589905485, "ddate": null, "tcdate": 1605589905485, "tmdate": 1605589905485, "tddate": null, "forum": "agHLCOBM5jP", "replyto": "8OG-aBgVA0k", "invitation": "ICLR.cc/2021/Conference/Paper3284/-/Official_Comment", "content": {"title": "Thank you for your feedback", "comment": "Thank you for adding the discussion on prior distribution, $p(s)$. I generally agree with the problem that methods defining $p(s)$ explicitly often over-modify natural images. On the other hand, improvements in accuracy have been reported by methods that do not explicitly define $p(s)$, such as deep image prior and the proposed method here.  While these approaches offer the benefits of improved accuracy, I feel they are black-box-like, and I find it difficult to interpret. I hope that by adding this discussion, the black box-like part will become clearer.  The literature [a,b] is some work trying be the clear the black-box in deep image prior and convolutional neural networks.\n\nIn my subjectivity, this paper is very interesting and I support it."}, "signatures": ["ICLR.cc/2021/Conference/Paper3284/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3284/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders", "authorids": ["~Mangal_Prakash1", "~Alexander_Krull3", "~Florian_Jug1"], "authors": ["Mangal Prakash", "Alexander Krull", "Florian Jug"], "keywords": ["Diversity denoising", "Unsupervised denoising", "Variational Autoencoders", "Noise model"], "abstract": "Deep Learning based methods have emerged as the indisputable leaders for virtually all image restoration tasks. Especially in the domain of microscopy images, various content-aware image restoration (CARE) approaches are now used to improve the interpretability of acquired data. Naturally, there are limitations to what can be restored in corrupted images, and like for all inverse problems, many potential solutions exist, and one of them must be chosen. Here, we propose DivNoising, a denoising approach based on fully convolutional variational autoencoders (VAEs), overcoming the problem of having to choose a single solution by predicting a whole distribution of denoised images. First we introduce a principled way of formulating the unsupervised denoising problem within the VAE framework by explicitly incorporating imaging noise models into the decoder. Our approach is fully unsupervised, only requiring noisy images and a suitable description of the imaging noise distribution. We show that such a noise model can either be measured, bootstrapped from noisy data, or co-learned during training. If desired, consensus predictions can be inferred from a set of DivNoising predictions, leading to competitive results with other unsupervised methods and, on occasion, even with the supervised state-of-the-art. DivNoising samples from the posterior enable a plethora of useful applications. We are (i) showing denoising results for 13 datasets, (ii) discussing how optical character recognition (OCR) applications can benefit from diverse predictions, and are (iii) demonstrating how instance cell segmentation improves when using diverse DivNoising predictions.", "one-sentence_summary": "DivNoising performs fully unsupervised diversity denoising using fully convolutional variational autoencoders and achieves SOTA results for a number of well known datasets while also enabling VAE-like sampling", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "prakash|fully_unsupervised_diversity_denoising_with_convolutional_variational_autoencoders", "pdf": "/pdf/2afe972808ebb66f3926468902039c366b274c59.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nprakash2021fully,\ntitle={Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders},\nauthor={Mangal Prakash and Alexander Krull and Florian Jug},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=agHLCOBM5jP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "agHLCOBM5jP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3284/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3284/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3284/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3284/Authors|ICLR.cc/2021/Conference/Paper3284/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3284/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839120, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3284/-/Official_Comment"}}}, {"id": "-g8JIhYEBqN", "original": null, "number": 3, "cdate": 1603804155159, "ddate": null, "tcdate": 1603804155159, "tmdate": 1605024029363, "tddate": null, "forum": "agHLCOBM5jP", "replyto": "agHLCOBM5jP", "invitation": "ICLR.cc/2021/Conference/Paper3284/-/Official_Review", "content": {"title": "Official Blind Review #1", "review": "This paper devises a novel unsupervised denoising paradigm, DIVNOISING, that allows us, for the first time, to generate diverse and plausible denoising solutions, sampled from a learned posterior. This approach only requires noisy images and a suitable description of the imaging noise distribution, providing a new perspective for the image denoising field. It has demonstrated that the quality of denoised images is highly competitive, typically outperforming the unsupervised state-of-the-art, and at times even improving on supervised results. This paper is well-written and good-organized. However, the reviewer has the following concerns.\n\nQ1. In DIVNOISING, noisy images are been created from a clean signal $s$ via a known noise model, i.e., $x\\sim p_{NM}(x|s)$. One main practical concern is that is there any assumptions about this noise model and which type of noises are mainly considered to be removed?\n\nQ2. The authors have demonstrated the excellent performance of DIVNOISING on several datasets, especially on microscopy datasets. Whether this denoiser can be applied to other noisy scenarios, such as a real color noisy image or MRI.\n\nQ3. Open the source codes.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3284/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3284/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders", "authorids": ["~Mangal_Prakash1", "~Alexander_Krull3", "~Florian_Jug1"], "authors": ["Mangal Prakash", "Alexander Krull", "Florian Jug"], "keywords": ["Diversity denoising", "Unsupervised denoising", "Variational Autoencoders", "Noise model"], "abstract": "Deep Learning based methods have emerged as the indisputable leaders for virtually all image restoration tasks. Especially in the domain of microscopy images, various content-aware image restoration (CARE) approaches are now used to improve the interpretability of acquired data. Naturally, there are limitations to what can be restored in corrupted images, and like for all inverse problems, many potential solutions exist, and one of them must be chosen. Here, we propose DivNoising, a denoising approach based on fully convolutional variational autoencoders (VAEs), overcoming the problem of having to choose a single solution by predicting a whole distribution of denoised images. First we introduce a principled way of formulating the unsupervised denoising problem within the VAE framework by explicitly incorporating imaging noise models into the decoder. Our approach is fully unsupervised, only requiring noisy images and a suitable description of the imaging noise distribution. We show that such a noise model can either be measured, bootstrapped from noisy data, or co-learned during training. If desired, consensus predictions can be inferred from a set of DivNoising predictions, leading to competitive results with other unsupervised methods and, on occasion, even with the supervised state-of-the-art. DivNoising samples from the posterior enable a plethora of useful applications. We are (i) showing denoising results for 13 datasets, (ii) discussing how optical character recognition (OCR) applications can benefit from diverse predictions, and are (iii) demonstrating how instance cell segmentation improves when using diverse DivNoising predictions.", "one-sentence_summary": "DivNoising performs fully unsupervised diversity denoising using fully convolutional variational autoencoders and achieves SOTA results for a number of well known datasets while also enabling VAE-like sampling", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "prakash|fully_unsupervised_diversity_denoising_with_convolutional_variational_autoencoders", "pdf": "/pdf/2afe972808ebb66f3926468902039c366b274c59.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nprakash2021fully,\ntitle={Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders},\nauthor={Mangal Prakash and Alexander Krull and Florian Jug},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=agHLCOBM5jP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "agHLCOBM5jP", "replyto": "agHLCOBM5jP", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3284/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538078579, "tmdate": 1606915798938, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3284/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3284/-/Official_Review"}}}], "count": 14}