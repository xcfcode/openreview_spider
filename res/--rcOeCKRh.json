{"notes": [{"id": "--rcOeCKRh", "original": "hDFFL0Ywqv1", "number": 168, "cdate": 1601308027390, "ddate": null, "tcdate": 1601308027390, "tmdate": 1614985714025, "tddate": null, "forum": "--rcOeCKRh", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "CROSS-SUPERVISED OBJECT DETECTION", "authorids": ["~Zitian_Chen1", "~Zhiqiang_Shen1", "~Jiahui_Yu1", "~Erik_Learned-Miller2"], "authors": ["Zitian Chen", "Zhiqiang Shen", "Jiahui Yu", "Erik Learned-Miller"], "keywords": ["Object detection", "weakly supervised", "transfer leaning"], "abstract": "After learning a new object category from image-level annotations (with no object bounding boxes), humans are remarkably good at precisely localizing those objects. However, building good object localizers (i.e., detectors) currently requires expensive instance-level annotations. While some work has been done on learning detectors from weakly labeled samples (with only class labels), these detectors do poorly at localization. In this work, we show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories. We call this learning paradigm cross-supervised object detection. While earlier works investigated this paradigm, they did not apply it to realistic complex images (e.g., COCO), and their performance was poor. We propose a unified framework that combines a detection head trained from instance-level annotations and a recognition head learned from image-level annotations, together with a spatial correlation module that bridges the gap between detection and recognition. These contributions enable us to better detect novel objects with image-level annotations in complex multi-object scenes such as the COCO dataset.", "one-sentence_summary": " We show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories.", "pdf": "/pdf/a6fdc33f91f5b937c25cdda47b871c2ddaf46100.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|crosssupervised_object_detection", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=75s_TpHxy", "_bibtex": "@misc{\nchen2021crosssupervised,\ntitle={{\\{}CROSS{\\}}-{\\{}SUPERVISED{\\}} {\\{}OBJECT{\\}} {\\{}DETECTION{\\}}},\nauthor={Zitian Chen and Zhiqiang Shen and Jiahui Yu and Erik Learned-Miller},\nyear={2021},\nurl={https://openreview.net/forum?id=--rcOeCKRh}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "IUsYqUv9nW1", "original": null, "number": 1, "cdate": 1610040428767, "ddate": null, "tcdate": 1610040428767, "tmdate": 1610474028540, "tddate": null, "forum": "--rcOeCKRh", "replyto": "--rcOeCKRh", "invitation": "ICLR.cc/2021/Conference/Paper168/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "After the rebuttal phase, all scores are borderline (6) or negative (4). Among the most confident reviewers (confidence 5), one gives 6 and one gives 4. The reviewer with confidence 4 gives overall score 6 but states they cannot support the paper. There were several concerns about the novelty of the task and method, the challenge of the experimental settings, missing comparisons to recent prior work in the original paper, etc. While the reviewers see merit, the paper can benefit from another revision before being accepted, including to better position the novelty of its method and perhaps reduce claims of novelty of the task. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CROSS-SUPERVISED OBJECT DETECTION", "authorids": ["~Zitian_Chen1", "~Zhiqiang_Shen1", "~Jiahui_Yu1", "~Erik_Learned-Miller2"], "authors": ["Zitian Chen", "Zhiqiang Shen", "Jiahui Yu", "Erik Learned-Miller"], "keywords": ["Object detection", "weakly supervised", "transfer leaning"], "abstract": "After learning a new object category from image-level annotations (with no object bounding boxes), humans are remarkably good at precisely localizing those objects. However, building good object localizers (i.e., detectors) currently requires expensive instance-level annotations. While some work has been done on learning detectors from weakly labeled samples (with only class labels), these detectors do poorly at localization. In this work, we show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories. We call this learning paradigm cross-supervised object detection. While earlier works investigated this paradigm, they did not apply it to realistic complex images (e.g., COCO), and their performance was poor. We propose a unified framework that combines a detection head trained from instance-level annotations and a recognition head learned from image-level annotations, together with a spatial correlation module that bridges the gap between detection and recognition. These contributions enable us to better detect novel objects with image-level annotations in complex multi-object scenes such as the COCO dataset.", "one-sentence_summary": " We show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories.", "pdf": "/pdf/a6fdc33f91f5b937c25cdda47b871c2ddaf46100.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|crosssupervised_object_detection", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=75s_TpHxy", "_bibtex": "@misc{\nchen2021crosssupervised,\ntitle={{\\{}CROSS{\\}}-{\\{}SUPERVISED{\\}} {\\{}OBJECT{\\}} {\\{}DETECTION{\\}}},\nauthor={Zitian Chen and Zhiqiang Shen and Jiahui Yu and Erik Learned-Miller},\nyear={2021},\nurl={https://openreview.net/forum?id=--rcOeCKRh}\n}"}, "tags": [], "invitation": {"reply": {"forum": "--rcOeCKRh", "replyto": "--rcOeCKRh", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040428753, "tmdate": 1610474028524, "id": "ICLR.cc/2021/Conference/Paper168/-/Decision"}}}, {"id": "y3sAjjDew_3", "original": null, "number": 3, "cdate": 1603918886048, "ddate": null, "tcdate": 1603918886048, "tmdate": 1607066802333, "tddate": null, "forum": "--rcOeCKRh", "replyto": "--rcOeCKRh", "invitation": "ICLR.cc/2021/Conference/Paper168/-/Official_Review", "content": {"title": "Duplicate task settings and limited novelty", "review": "Paper summary:\nThe paper proposes a new task cross-supervised object detection, which trains object detectors on the combination of base class images with instance-level annotations and novel class image with only image-level annotations. A network with a recognition head which is trained by image-level annotations and a detection head which is trained by instance-level annotations is proposed for the task. To generate instance-level annotations for novel class images with only image-level annotations, the paper proposes a spatial correlation module to generate pseudo gt boxes from high-confidence boxes. Results on PASCAL VOC and COCO show that the proposed method obtains very promising object detection results for novel classes.\n\n\nStrengths:\n\n+ The paper is well written.\n\n+ Promising experimental results are obtained by the proposed method.\n\n+ The proposed spatial correlation module is interesting.\n\n\nWeaknesses:\n\n- Duplicate task settings.\nThe proposed new task, cross-supervised object detection, is almost the same as the task defined in (Hoffman et al. 2014, Tang et al. 2016, Uijlings et al. 2018). Both of these previous works study the task of training object detectors on the combination of base class images with instance-level annotations and novel class image with only image-level annotations. The work (Uijlings et al. 2018) also conducts experiments on COCO which contains multi-objects in images.\nIn addition, the work  (Khandelwal et al. 2020) unifies the setting of training object detectors on the combination of fully-labeled data and weakly-labeled data, and conducts experiments on multi-object datasets PASCAL VOC and COCO. The task proposed by this paper could be treated as a special case of the task studied in (Khandelwal et al. 2020).\nWe should avoid duplicate task settings.\n\n- Limited novelty.\nThe novelty of the proposed method is limited.\nCombining recognition head and detection head is not new in weakly supervised object detection. The weakly supervised object detection networks (Yang et al. 2019, Zeng et al. 2019) also generate pseudo instance-level annotations from recognition head to train detection head (i.e., head with bounding box classification and regression) for weakly-labeled data.\n\n\nReview summary:\nIn summary, I would like to give a rejection to this paper due to the duplicate task settings and limited novelty.\n\nKhandelwal et al., Weakly-supervised Any-shot Object Detection, 2020\n\n---------- Post rebuttal ----------\n\nAfter discussions with authors and reading other reviews, I acknowledge the contribution that this paper advances the performance of cross-supervised object detection.\n\nHowever, I would like to keep my original reject score. The reasons are as follows.\n\nExtending datasets from PASCAL VOC to COCO is not a significant change comparing to previous tasks. The general object detection papers also evaluated on PASCAL VOC only about five years ago and now evaluate mainly on COCO. With the development of computer vision techniques, it is natural to try more challenging datasets. So although this paper claims that this paper focuses on more challenging datasets, there is no significant difference between the tasks studied in previous works like [a] and this paper.\n\nIn addition, apart from ImageNet, the work [b] also evaluates their method on the Open Images dataset which is even larger and more challenging than COCO. The difference between the tasks studied in [b] and this paper is only that, [b] adds a constraint that weakly-labeled classes have semantic correlations with fully-labeled classes and this paper doesn't. This difference is also minor.\n\nTherefore, the task itself cannot be one of the main contributions of this paper (especially the most important contribution of this paper). I would like to suggest the authors change their title / introduction / main paper by 1) giving lower wights to the task parts 2) giving higher weights to intuitions of why previous works fail on challenging datasets like COCO and motivations of the proposed method.\n\n[a] YOLO9000: Better, Faster, Stronger, In CVPR, 2017\n\n[b] Detecting 11K Classes: Large Scale Object Detection without Fine-Grained Bounding Boxes, In ICCV, 2019", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper168/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper168/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CROSS-SUPERVISED OBJECT DETECTION", "authorids": ["~Zitian_Chen1", "~Zhiqiang_Shen1", "~Jiahui_Yu1", "~Erik_Learned-Miller2"], "authors": ["Zitian Chen", "Zhiqiang Shen", "Jiahui Yu", "Erik Learned-Miller"], "keywords": ["Object detection", "weakly supervised", "transfer leaning"], "abstract": "After learning a new object category from image-level annotations (with no object bounding boxes), humans are remarkably good at precisely localizing those objects. However, building good object localizers (i.e., detectors) currently requires expensive instance-level annotations. While some work has been done on learning detectors from weakly labeled samples (with only class labels), these detectors do poorly at localization. In this work, we show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories. We call this learning paradigm cross-supervised object detection. While earlier works investigated this paradigm, they did not apply it to realistic complex images (e.g., COCO), and their performance was poor. We propose a unified framework that combines a detection head trained from instance-level annotations and a recognition head learned from image-level annotations, together with a spatial correlation module that bridges the gap between detection and recognition. These contributions enable us to better detect novel objects with image-level annotations in complex multi-object scenes such as the COCO dataset.", "one-sentence_summary": " We show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories.", "pdf": "/pdf/a6fdc33f91f5b937c25cdda47b871c2ddaf46100.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|crosssupervised_object_detection", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=75s_TpHxy", "_bibtex": "@misc{\nchen2021crosssupervised,\ntitle={{\\{}CROSS{\\}}-{\\{}SUPERVISED{\\}} {\\{}OBJECT{\\}} {\\{}DETECTION{\\}}},\nauthor={Zitian Chen and Zhiqiang Shen and Jiahui Yu and Erik Learned-Miller},\nyear={2021},\nurl={https://openreview.net/forum?id=--rcOeCKRh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "--rcOeCKRh", "replyto": "--rcOeCKRh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper168/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538148924, "tmdate": 1606915780565, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper168/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper168/-/Official_Review"}}}, {"id": "6aIxHHNHztI", "original": null, "number": 1, "cdate": 1603225386487, "ddate": null, "tcdate": 1603225386487, "tmdate": 1606764543035, "tddate": null, "forum": "--rcOeCKRh", "replyto": "--rcOeCKRh", "invitation": "ICLR.cc/2021/Conference/Paper168/-/Official_Review", "content": {"title": "Good paper in every aspect, but with limited novelty.", "review": "This paper introduces a new method for training an object detector on a dataset that consists of some object categories with instance-level bounding box annotations, as well as some other object categories with only image-level labels. The topic is interesting, important, and potentially very useful for real applications. The authors propose an idea to transfer knowledge from a weakly supervised (WS) detection head into a fully supervised (FS) detection head, by producing pseudo-ground-truth bounding boxes for classes with image-level labels. The idea is straightforward and interesting. Experiments show significant and consistent gains in various scenarios. The paper is well-written.\n\nThe main drawback of this paper is the lack of substantial novelty. The authors claim to propose a \"novel paradigm\" named cross-supervised object detection, while this task has already been studied as reviewed in Section 2. Particularly, the idea of jointly training WS and FS heads on WS and FS labels with a shared backbone has been explored before (e.g. in [1], which should have been discussed in the related work). The idea of using a WS model to create pseudo-ground-truth bounding boxes for training another model has also been studied before (e.g. in [2]). The only novel idea of this paper seems to be the Spatial Correlation Module (SCM), which is discussed more below.\n\nSCM is used to transform proposal boxes selected by the WS detector head into more accurate bounding boxes to serve as pseudo-ground-truth for WS classes. To this end, the authors train a class-agnostic bounding box refinement module on FS classes, and apply it on the proposals of WS classes. However, a similar result could have been (probably) achieved by simply replacing the bounding box regression head of the Faster R-CNN with a class-agnostic head, and training it on base classes, while using it to refine the proposals of both base and novel classes during test. The authors did not explore such simple alternatives to SCM.\n\nAnother issue is that although the authors cited several existing methods for cross-supervised object detection in Section 2, they did not discuss why the proposed method is superior, and they did not include them in performance tables, claiming \"these methods can only perform object localization in single object scenes.\" I cannot verify the correctness of this claim, as any proposal-based model can detect multiple objects per image and per class. \n\nNevertheless, the paper has a clear motivation and idea, a scientifically sound analysis, and significant results and insights that can be helpful for future work. Therefore, I recommend acceptance. If the authors can convince me that the paper also has substantial novelty and advantages over all existing works, I am willing to raise my rating.\n\nMinor comment: In equation (1), the two terms of the binary cross-entropy should probably be placed in parentheses, so the sum is applied on both terms. Also in the paragraph above eq (1), that loss term should be defined as a multi-label cross-entropy, rather than multi-class.\n\n[1] Yang, Hao, Hao Wu, and Hao Chen. \"Detecting 11k classes: Large scale object detection without fine-grained bounding boxes.\" Proceedings of the IEEE International Conference on Computer Vision. 2019.\n\n[2] Tang, Peng, et al. \"Multiple instance detection network with online instance classifier refinement.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.\n\n######## Post-Rebuttal Updates:\n\nAfter reading the authors' response and other reviewers' opinions (especially R3), I would like to downgrade my rating slightly from 7 to 6. I still think the paper makes valuable contributions, but I also think the contributions are overstated and not precisely justified. Particularly:\n\n1. I agree with R3 that the limitation of novelty should be considered from the two perspectives of \"task\" and \"method\". The task is certainly not new, which should be made clear in the paper. The newest version of the paper still claims \"we define a new task\u2014cross-supervised object detection\"\n\n2. The method is indeed somewhat new, due to the use of multi-task learning and SCM, but its novelty should be clarified, and compared to all similar methods, not only some of them which are weaker. For instance, the authors did not adequately justify whether/why their method is superior to YOLO 9000, or Yang et al ([1] above). They did mention some differences in response to R3's comments and mine, but I am not convinced. Moreover, the authors did not explicitly discuss and compare those distinctions in the paper, neither quantitatively nor intuitively.\n\n3. In response to R3, The authors claim they \"are the first to address this problem in situations of realistic complexity\", which is not accurate. Particularly, the paper reads \"While several works [...] have explored this problem before, [...] They struggle to learn under more complex and realistic scenarios, where there are multiple objects from potentially very different classes\" This is not entirely true, as Yang et al. successfully evaluate on Open Images (and also VOC and COCO in their supplementary materials). The authors do not provide a convincing reason or evidence of existing methods \"struggling\" in realistic settings. YOLO 9000 is open-source, and could have been compared to the proposed method to confirm that claim.\n\nAccordingly, I strongly encourage the authors to refine their claims and lay more emphasis on the actually novel aspects of their method, by thoroughly comparing those novelties with all similar methods. I still hope to see this paper accepted, but cannot endorse it due to insisting on inaccurate claims.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper168/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper168/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CROSS-SUPERVISED OBJECT DETECTION", "authorids": ["~Zitian_Chen1", "~Zhiqiang_Shen1", "~Jiahui_Yu1", "~Erik_Learned-Miller2"], "authors": ["Zitian Chen", "Zhiqiang Shen", "Jiahui Yu", "Erik Learned-Miller"], "keywords": ["Object detection", "weakly supervised", "transfer leaning"], "abstract": "After learning a new object category from image-level annotations (with no object bounding boxes), humans are remarkably good at precisely localizing those objects. However, building good object localizers (i.e., detectors) currently requires expensive instance-level annotations. While some work has been done on learning detectors from weakly labeled samples (with only class labels), these detectors do poorly at localization. In this work, we show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories. We call this learning paradigm cross-supervised object detection. While earlier works investigated this paradigm, they did not apply it to realistic complex images (e.g., COCO), and their performance was poor. We propose a unified framework that combines a detection head trained from instance-level annotations and a recognition head learned from image-level annotations, together with a spatial correlation module that bridges the gap between detection and recognition. These contributions enable us to better detect novel objects with image-level annotations in complex multi-object scenes such as the COCO dataset.", "one-sentence_summary": " We show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories.", "pdf": "/pdf/a6fdc33f91f5b937c25cdda47b871c2ddaf46100.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|crosssupervised_object_detection", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=75s_TpHxy", "_bibtex": "@misc{\nchen2021crosssupervised,\ntitle={{\\{}CROSS{\\}}-{\\{}SUPERVISED{\\}} {\\{}OBJECT{\\}} {\\{}DETECTION{\\}}},\nauthor={Zitian Chen and Zhiqiang Shen and Jiahui Yu and Erik Learned-Miller},\nyear={2021},\nurl={https://openreview.net/forum?id=--rcOeCKRh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "--rcOeCKRh", "replyto": "--rcOeCKRh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper168/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538148924, "tmdate": 1606915780565, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper168/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper168/-/Official_Review"}}}, {"id": "uVowNsXpHK", "original": null, "number": 11, "cdate": 1606163000661, "ddate": null, "tcdate": 1606163000661, "tmdate": 1606163379788, "tddate": null, "forum": "--rcOeCKRh", "replyto": "iPMUAH96CRX", "invitation": "ICLR.cc/2021/Conference/Paper168/-/Official_Comment", "content": {"title": "Response to R3", "comment": "We thank R3 for the reply. Below is our response to the concern.\n\nQ1: I agree with the authors that the tasks studied in Hoffman et al. 2014, Tang et al. 2016, Uijlings et al. 2018, and Khandelwal et al. 2020 focus more on transfer learning and this paper focuses more on multi-task learning. However, this difference is minor.\n\nA1: The reason the machine learning community has developed a large variety of different learning paradigms, including transfer, multi-task, low-shot, semi-supervised and so on, is that there are **important differences in how models are trained under each paradigm to maximize performance**. \n\nQ2: Uijings et.al. 2018 and Khandelwal et al. 2020 could be modified to multi-task learning.\n\nA2: **In our previous reply A2 and A4, we already made a comparison of performance.** For results on COCO compare with Uijlings et al. 2018, \u201cOur CorLoc for IOU>0.5 is 59.4 while theirs are 26.2\u201d. For results on PASCAL VOC compare with Khandelwal et al. 2020, \u201cWe achieve 66.7% mAP whereas they achieve 58.7% mAP\u201d.\nOf course, many previous methods could be modified, but the point is that they have not yet been modified. If it were easy to modify and get such a large gain in performance (from 26.2 to 59.4), the previous authors certainly could have done so. A similar argument, that it would be easy to modify previous methods to get large gains, could be made for most papers in computer vision. We do not believe this is a fair criticism. For example, the VGG deep network was a set of very simple modifications to ZF-net, and yet today it has 47,000 citations. \n\nQ3: There are also other works train object detectors on the combination of base class images with instance-level annotations and novel class images with only image-level annotations under the multi-task learning setting [a, b], and there are only minor differences between the task studied in this paper and the tasks studied in [a, b].\n\nA3:\n\nIn [b], they conduct experiments on ImageNet, which is the same as the ILSVRC data set and we already **discussed it in our previous reply A1**. Also, they claim \u201cour method is the first semi-supervised fine-grained detection framework that explicitly exploit semantic/visual correlations between coarse-grained detection and fine-grained classification data\u201d and they assume a very high coarse-grained relation between detection and classification classes, which is a very different situation. \n\nIn [a], still, their classification categories are ImageNet categories and the classification data is still object-centered and single object image. [b] claim that \u201cYOLO 9000([a]) can also be viewed as a semi-supervised detection framework, but it is no more than a naive combination of detection and classification stream and only relies on the implicit shared feature learning from the network\u201d. In comparison, we add connection between our recognition and detection head. Please refer to our previous reply A5 for the importance of this connection.\n\nQ4: The proposed task has only minor differences compared to the previous tasks and is not novel enough.\n\n**Summary** and A4: \nWe disagree with this statement. R3 has proposed several similar papers. We go through each paper one by one and reply. \nThe papers mentioned by R3 either focus on transfer learning paradigms (Uijlings et al. 2018, Khandelwal et al. 2020) or focus on very simplified settings (Hoffman et al. 2014, Tang et al. 2016, [a], [b]). \n\nWe believe this is not a minor difference and we discuss these differences in our previous and current reply. \n\nHere is a brief summary of our views:\n\n* In A1, we clarify the difference between transfer learning and multi-task learning. We focus on multi-task learning.\n* In A2, we show the efficacy of our methods compared to methods from similar settings. We show substantial increases in performance. \n* In previous reply A1 and second paragraph in Introduction of our paper, we clarified our important contribution of conducting cross-supervised object detection under complex multi-objects scenes. The point is that we are the first to address this problem in situations of realistic complexity.\n\n\n\nWe have revised the abstract and introduction to make it clear that we are not the first to work on cross-supervised object detection, but that we are the first to apply it to realistic settings with complex imagery. We add one line in the Abstract and a short paragraph after the original third paragraph of the Introduction. Other than the abstract and intro, there are only very minor changes to the remainder of the document (such as removing the word \u201cnew\u201d from \u201cnew paradigm\u201d in the conclusion). While R3 suggested that the changes to the document would be \u201cepic\u201d, we believe they are quite modest, and reflect all of the raised issues.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper168/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper168/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CROSS-SUPERVISED OBJECT DETECTION", "authorids": ["~Zitian_Chen1", "~Zhiqiang_Shen1", "~Jiahui_Yu1", "~Erik_Learned-Miller2"], "authors": ["Zitian Chen", "Zhiqiang Shen", "Jiahui Yu", "Erik Learned-Miller"], "keywords": ["Object detection", "weakly supervised", "transfer leaning"], "abstract": "After learning a new object category from image-level annotations (with no object bounding boxes), humans are remarkably good at precisely localizing those objects. However, building good object localizers (i.e., detectors) currently requires expensive instance-level annotations. While some work has been done on learning detectors from weakly labeled samples (with only class labels), these detectors do poorly at localization. In this work, we show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories. We call this learning paradigm cross-supervised object detection. While earlier works investigated this paradigm, they did not apply it to realistic complex images (e.g., COCO), and their performance was poor. We propose a unified framework that combines a detection head trained from instance-level annotations and a recognition head learned from image-level annotations, together with a spatial correlation module that bridges the gap between detection and recognition. These contributions enable us to better detect novel objects with image-level annotations in complex multi-object scenes such as the COCO dataset.", "one-sentence_summary": " We show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories.", "pdf": "/pdf/a6fdc33f91f5b937c25cdda47b871c2ddaf46100.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|crosssupervised_object_detection", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=75s_TpHxy", "_bibtex": "@misc{\nchen2021crosssupervised,\ntitle={{\\{}CROSS{\\}}-{\\{}SUPERVISED{\\}} {\\{}OBJECT{\\}} {\\{}DETECTION{\\}}},\nauthor={Zitian Chen and Zhiqiang Shen and Jiahui Yu and Erik Learned-Miller},\nyear={2021},\nurl={https://openreview.net/forum?id=--rcOeCKRh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "--rcOeCKRh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper168/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper168/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper168/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper168/Authors|ICLR.cc/2021/Conference/Paper168/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper168/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873897, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper168/-/Official_Comment"}}}, {"id": "iPMUAH96CRX", "original": null, "number": 10, "cdate": 1606109875499, "ddate": null, "tcdate": 1606109875499, "tmdate": 1606110070533, "tddate": null, "forum": "--rcOeCKRh", "replyto": "B4utTXV7d_", "invitation": "ICLR.cc/2021/Conference/Paper168/-/Official_Comment", "content": {"title": "More discussions about the duplicate task setting and novelty", "comment": "Thanks for clarifying the differences between the task studied in this paper and the tasks studied in Hoffman et al. 2014, Tang et al. 2016, Uijlings et al. 2018, and Khandelwal et al. 2020, and also thanks for clarifying the novelty.\n\n\n----- For task -----\n\nI agree with the authors that the tasks studied in Hoffman et al. 2014, Tang et al. 2016, Uijlings et al. 2018,  and Khandelwal et al. 2020 focus more on transfer learning and this paper focuses more on multi-task learning. However, this difference is minor, and it is easy to extend the methods by Hoffman et al. 2014, Tang et al. 2016, and Uijlings et al. 2018 to the multi-task learning setting, e.g., for Uijlings et al. 2018, they could train their final round of detector on the data with both base classes and novel classes. \nIn addition, the task studied in Khandelwal et al. 2020 could also be treated as multi-task learning because they show promising results on both base classes and novel classes.\n\nEven without considering the work by Khandelwal et al., there are also other works train object detectors on the combination of base class images with instance-level annotations and novel class images with only image-level annotations under the multi-task learning setting [a, b], and there are only minor differences between the task studied in this paper and the tasks studied in [a, b].\n\nTherefore, the proposed task has only minor differences comparing to the previous tasks and is not novel enough. To downgrade the claims about the novel task, the authors need to change their title, rewrite their abstract and introduction, and re-organize their method part. Considering that epic changes are needed to revise the paper, I would like to suggest the authors rewrite their paper and resubmit the paper to the next conference.\n\n\n----- For method -----\n\nThe main differences between the method proposed in this paper and the methods proposed in (Yang et al. 2019, Zeng et al. 2019) are the proposed spatial correlation module and using real labels from fully labeled data. These methodological differences are also the main contributions of this paper. The authors should revise their paper to show the real contributions of their work.\n\n[a] YOLO9000: Better, Faster, Stronger, In CVPR, 2017\n\n[b] Detecting 11K Classes: Large Scale Object Detection without Fine-Grained Bounding Boxes, In ICCV, 2019"}, "signatures": ["ICLR.cc/2021/Conference/Paper168/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper168/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CROSS-SUPERVISED OBJECT DETECTION", "authorids": ["~Zitian_Chen1", "~Zhiqiang_Shen1", "~Jiahui_Yu1", "~Erik_Learned-Miller2"], "authors": ["Zitian Chen", "Zhiqiang Shen", "Jiahui Yu", "Erik Learned-Miller"], "keywords": ["Object detection", "weakly supervised", "transfer leaning"], "abstract": "After learning a new object category from image-level annotations (with no object bounding boxes), humans are remarkably good at precisely localizing those objects. However, building good object localizers (i.e., detectors) currently requires expensive instance-level annotations. While some work has been done on learning detectors from weakly labeled samples (with only class labels), these detectors do poorly at localization. In this work, we show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories. We call this learning paradigm cross-supervised object detection. While earlier works investigated this paradigm, they did not apply it to realistic complex images (e.g., COCO), and their performance was poor. We propose a unified framework that combines a detection head trained from instance-level annotations and a recognition head learned from image-level annotations, together with a spatial correlation module that bridges the gap between detection and recognition. These contributions enable us to better detect novel objects with image-level annotations in complex multi-object scenes such as the COCO dataset.", "one-sentence_summary": " We show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories.", "pdf": "/pdf/a6fdc33f91f5b937c25cdda47b871c2ddaf46100.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|crosssupervised_object_detection", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=75s_TpHxy", "_bibtex": "@misc{\nchen2021crosssupervised,\ntitle={{\\{}CROSS{\\}}-{\\{}SUPERVISED{\\}} {\\{}OBJECT{\\}} {\\{}DETECTION{\\}}},\nauthor={Zitian Chen and Zhiqiang Shen and Jiahui Yu and Erik Learned-Miller},\nyear={2021},\nurl={https://openreview.net/forum?id=--rcOeCKRh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "--rcOeCKRh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper168/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper168/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper168/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper168/Authors|ICLR.cc/2021/Conference/Paper168/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper168/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873897, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper168/-/Official_Comment"}}}, {"id": "xnJowpRfADS", "original": null, "number": 8, "cdate": 1605978816909, "ddate": null, "tcdate": 1605978816909, "tmdate": 1605979650119, "tddate": null, "forum": "--rcOeCKRh", "replyto": "6aIxHHNHztI", "invitation": "ICLR.cc/2021/Conference/Paper168/-/Official_Comment", "content": {"title": "Response to Reviewer 4(2/2)", "comment": "Q3:  Similar results could have been (probably) achieved by simply replacing the bounding box regression head of the Faster R-CNN with a class-agnostic head, and training it on base classes, while using it to refine the proposals of both base and novel classes during tests. \n\nA3: Good question. In Table 3(b), we provide an ablation study of using a class-agnostic 2/3/4 FC layer to regress and refine the proposal: \u201cusing three fully-connected layers to regress the ground truth location taking the coordinates of high-confidence bounding boxes as input\u201d. However, this setting still has very minor differences from the suggestion of R4 (inputs are different). In our very initial experiments, we did exactly what R4 suggested. However, using a class-agnostic regression head learned on base classes actually decreases the performance compared to not regressing the predicted bounding boxes of novel classes. This is also why we designed our SCM. \n\nQ4: Lack of competitors.\n\nA4: In Table.1, we include two cross-supervised object detection competitors -- MSD (Zhang et al. 2018), and Weight Transfer (Kuen et al. 2019). The methods mentioned in related work, either follow a very different setting, (e.g., Gao et al. 2019 focus on semi-supervised object detection), either only report results on single object dataset ILSVRC2013 (e.g., Hoffman et al. 2014, Tang et al.). \n\nWe also conducted experiments on COCO following Uijlings et al. 2018. Note that they have additional semantic information and their model learned from base classes in stage 1 and learned from novel classes in stage 2, whereas we learn from base and novel classes jointly. We use the same architecture and the same training and testing data. Our CorLoc for IOU>0.5 is 59.4 while theirs are 26.2. \n\nQ5: Why is the proposed method superior?\n\nA5: \n* One advantage is that joint training allows us to benefit from additional base-class labels. We are doing multi-task learning from two types of supervision and different classes may have different strengths of supervision, which has rarely been explored before. \n* The spatial correlation module allows us to transfer knowledge from base classes to novel classes.  \n* Ultimately, the performance on more realistic data sets like COCO demonstrates that this works in more realistic settings. Other methods struggle to perform on these complex multi-object scenarios.\n\nQ6: Minor comment in equation(1).\n\nA6: Thanks! We will fix that."}, "signatures": ["ICLR.cc/2021/Conference/Paper168/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper168/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CROSS-SUPERVISED OBJECT DETECTION", "authorids": ["~Zitian_Chen1", "~Zhiqiang_Shen1", "~Jiahui_Yu1", "~Erik_Learned-Miller2"], "authors": ["Zitian Chen", "Zhiqiang Shen", "Jiahui Yu", "Erik Learned-Miller"], "keywords": ["Object detection", "weakly supervised", "transfer leaning"], "abstract": "After learning a new object category from image-level annotations (with no object bounding boxes), humans are remarkably good at precisely localizing those objects. However, building good object localizers (i.e., detectors) currently requires expensive instance-level annotations. While some work has been done on learning detectors from weakly labeled samples (with only class labels), these detectors do poorly at localization. In this work, we show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories. We call this learning paradigm cross-supervised object detection. While earlier works investigated this paradigm, they did not apply it to realistic complex images (e.g., COCO), and their performance was poor. We propose a unified framework that combines a detection head trained from instance-level annotations and a recognition head learned from image-level annotations, together with a spatial correlation module that bridges the gap between detection and recognition. These contributions enable us to better detect novel objects with image-level annotations in complex multi-object scenes such as the COCO dataset.", "one-sentence_summary": " We show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories.", "pdf": "/pdf/a6fdc33f91f5b937c25cdda47b871c2ddaf46100.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|crosssupervised_object_detection", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=75s_TpHxy", "_bibtex": "@misc{\nchen2021crosssupervised,\ntitle={{\\{}CROSS{\\}}-{\\{}SUPERVISED{\\}} {\\{}OBJECT{\\}} {\\{}DETECTION{\\}}},\nauthor={Zitian Chen and Zhiqiang Shen and Jiahui Yu and Erik Learned-Miller},\nyear={2021},\nurl={https://openreview.net/forum?id=--rcOeCKRh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "--rcOeCKRh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper168/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper168/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper168/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper168/Authors|ICLR.cc/2021/Conference/Paper168/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper168/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873897, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper168/-/Official_Comment"}}}, {"id": "B4utTXV7d_", "original": null, "number": 5, "cdate": 1605978251099, "ddate": null, "tcdate": 1605978251099, "tmdate": 1605979138869, "tddate": null, "forum": "--rcOeCKRh", "replyto": "y3sAjjDew_3", "invitation": "ICLR.cc/2021/Conference/Paper168/-/Official_Comment", "content": {"title": "Response to Reviewer 3(2/2)", "comment": "\nQ4: Khandelwal et al. 2020 unifies the setting of training object detectors on the combination of fully-labeled data and weakly-labeled data, and conducts experiments on multi-object datasets PASCAL VOC and COCO.\n\nA4: We want to highlight that Khandelwal et al. 2020 is a recently released arxiv paper and **had not been published as a peer-reviewed paper** at the time of submission.  The instructions on the ICLR website suggest that it is not essential to compare to non-peer-reviewed works. But we can still discuss and compare with them to address the concern of R3. We will add it to the related work. \n\nTheir method \u201cis divided into two stages: base training and fine-tuning\u201d, \u201cDuring base training, instances from D_base are used to obtain a detector / segmentation network\u201d, and \u201cIn the fine-tuning phase, the network is fine-tuned on D_novel\u201d. Their paradigm is closed to \u201ctransfer learning\u201d and our paradigm is closed to \u201cmulti-task learning\u201d(as mentioned in A2). We discussed the difference and the importance of our paradigm in A2.\n\nAlso, their method uses additional semantic information (300-dimensional GloVe [34] vector embeddings) to calculate the similarity between base and novel classes. \n\nDespite the difference, we can compare their results and ours on PASCAL VOC using the same backbone (VGG-16) and the same training and testing data. We follow their class split setting and run our method. We achieve 66.7% mAP whereas they achieve 58.7% mAP.\n\n\nQ5: Combining a recognition head and a detection head is not new in weakly supervised object detection. The weakly supervised object detection networks (Yang et al. 2019, Zeng et al. 2019) also generate pseudo instance-level annotations from the recognition head to training the detection head (i.e., head with bounding box classification and regression) for weakly-labeled data.\n\nA5: Yes, as we claim \u201cA majority of WSOD methods (Tang et al. (2017); Wan et al. (2019); Wei et al. (2018)) find that re-training a new detector taking the top-scoring bounding boxes from a weakly supervised object detector as ground truth marginally improve the performance\u201d, lots of WSOD methods (including Yang et al. 2019, Zeng et al. 2019) have a \u201cdetection head\u201d that learned from pseudo instance-level annotations generated from an \u201crecognition head\u201d. \n\nHowever, their \u201cdetection head\u201d only learns from pseudo labels and it acts similarly to the \u201crecognition head\u201d. It is more like a slightly improved version of the recognition head. In comparison, our detection head learns from both noisy pseudo labels of novel classes and ground truth of base classes. Our \u201cdetection head\u201d is very powerful when detecting base objects and thus have a better prior knowledge even for novel classes. As a result, it is a true \u201cdetection head\u201d rather than an updated version of the \u201crecognition head\u201d.\n\nThese differences are subtle, but we believe our significantly better performance is evidence that they have a major effect on accuracy. In Table 1, when the detection head does not learn from the recognition head and we predict novel class using the recognition head, the performance is 47.1% (Two Head*). Note that in this case the recognition head still benefits from the shared backbone and has a refinement head inside its structure. When we add the connection between the recognition head and the detection head, even without spatial correlation module, the performance is 52.4%(Ours* w/o SCM).\n\nFinally, part of our contribution is that our architecture allows us to benefit substantially from additional labeled examples on the base classes. What\u2019s more, our pseudo labels are refined by our spatial correlation module before supervising the detection head, whereas other methods do not have this process."}, "signatures": ["ICLR.cc/2021/Conference/Paper168/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper168/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CROSS-SUPERVISED OBJECT DETECTION", "authorids": ["~Zitian_Chen1", "~Zhiqiang_Shen1", "~Jiahui_Yu1", "~Erik_Learned-Miller2"], "authors": ["Zitian Chen", "Zhiqiang Shen", "Jiahui Yu", "Erik Learned-Miller"], "keywords": ["Object detection", "weakly supervised", "transfer leaning"], "abstract": "After learning a new object category from image-level annotations (with no object bounding boxes), humans are remarkably good at precisely localizing those objects. However, building good object localizers (i.e., detectors) currently requires expensive instance-level annotations. While some work has been done on learning detectors from weakly labeled samples (with only class labels), these detectors do poorly at localization. In this work, we show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories. We call this learning paradigm cross-supervised object detection. While earlier works investigated this paradigm, they did not apply it to realistic complex images (e.g., COCO), and their performance was poor. We propose a unified framework that combines a detection head trained from instance-level annotations and a recognition head learned from image-level annotations, together with a spatial correlation module that bridges the gap between detection and recognition. These contributions enable us to better detect novel objects with image-level annotations in complex multi-object scenes such as the COCO dataset.", "one-sentence_summary": " We show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories.", "pdf": "/pdf/a6fdc33f91f5b937c25cdda47b871c2ddaf46100.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|crosssupervised_object_detection", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=75s_TpHxy", "_bibtex": "@misc{\nchen2021crosssupervised,\ntitle={{\\{}CROSS{\\}}-{\\{}SUPERVISED{\\}} {\\{}OBJECT{\\}} {\\{}DETECTION{\\}}},\nauthor={Zitian Chen and Zhiqiang Shen and Jiahui Yu and Erik Learned-Miller},\nyear={2021},\nurl={https://openreview.net/forum?id=--rcOeCKRh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "--rcOeCKRh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper168/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper168/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper168/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper168/Authors|ICLR.cc/2021/Conference/Paper168/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper168/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873897, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper168/-/Official_Comment"}}}, {"id": "xSEsCSMMuWx", "original": null, "number": 3, "cdate": 1605976434996, "ddate": null, "tcdate": 1605976434996, "tmdate": 1605978965480, "tddate": null, "forum": "--rcOeCKRh", "replyto": "sfdV4h2xpzl", "invitation": "ICLR.cc/2021/Conference/Paper168/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We appreciate the valuable comments from you. Please see our responses and clarifications for your questions below. \n\nQ1: What is the meaning of \u201creplacing the backbone and feature pyramid network with five max-pooling layers\u201d?\n\nA1: In FCOS (Tian et al. 2019), they use a backbone network and a feature pyramid network to produce five feature maps with different dimensions. We use 5 max-pooling layers that take the image as input and output 5 feature maps of different dimensions that have the same shape as the feature maps in FCOS. The other part is the same as FCOS. \n\nQ2: In Table 1, are the experimental settings of those competitors such as MSD-VGG16, MSD-Ens, and Weight Transfer et al. exactly the same as those used in this paper?\n\nA2: The experimental settings of MSD-VGG16, MSD-Ens, MSD_Ens +FRCN are exactly the same as in our paper. For Weight Transfer, they have different settings and we reproduce their methods under our experimental setting.\n\nQ3:  In Table 2, it seems like the method is taking the non-VOC as the base classes while Hu et al. (2018) use the non-VOC as those classes without mask annotation. Can you tell me the reasons for this choice?\n\nA3: Hu et al. report two results. One takes non-VOC (60 classes) as base classes and VOC (20 classes) as novel classes. The other one takes VOC as base classes and non-VOC as novel classes. So we are actually following their first split up scheme. We choose this because we want to have a sufficient number of base classes so that our spatial correlation module can successfully learn the class-agnostic mapping from coarse bounding boxes predicted by the recognition head to the ground truth. \n\nQ4: Some similar problem settings are defined in [a] and [b]. The paper fails to compare the problem settings.\n\nA4:\n\n [a] (Wang et al. 2019) focus on semi-supervised object detection. As we claim, cross-supervised object detection (CSOD) \u201chas similarities to both transfer learning and semi-supervised learning, since it transfers knowledge from base classes to novel classes and has more information about some instances than other instances. However, CSOD represents a distinct and novel paradigm for learning.\u201d \nThe major difference between our cross-supervised object detection and semi-supervised object detection is that we have **zero instance-level labels** for new classes. Also, out-of-data classes (base classes) are never present in the training set of semi-supervised learning whereas we need to jointly learn from both base and novel classes.\n\n[b] (Chen et. al 2018) focus on a few-shot object detection setting. The major difference is that we have zero instance-level labels and we need to learn from only class labels for new classes whereas few-shot object detection has a few instance-level labels and does not contain any form of weakly-supervised learning.\n\nWe will add [a] and [b] in our related work and further clarify the difference.\n\nQ5: Justify the usefulness of the proposed problem setting in real application scenarios.\n\nA5: As we claim, \u201cCSOD could be a promising approach for expanding object detection to a much larger number of categories\u201d. We have the same real application scenarios as weakly-supervised object detection, but importantly, \u201cweakly supervised object detectors do not work well in complex multi-object scenes\u201d while our CSOD works better in this situation. R4 also agrees that \u201cThe topic is interesting, important, and potentially very useful for real applications\u201d. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper168/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper168/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CROSS-SUPERVISED OBJECT DETECTION", "authorids": ["~Zitian_Chen1", "~Zhiqiang_Shen1", "~Jiahui_Yu1", "~Erik_Learned-Miller2"], "authors": ["Zitian Chen", "Zhiqiang Shen", "Jiahui Yu", "Erik Learned-Miller"], "keywords": ["Object detection", "weakly supervised", "transfer leaning"], "abstract": "After learning a new object category from image-level annotations (with no object bounding boxes), humans are remarkably good at precisely localizing those objects. However, building good object localizers (i.e., detectors) currently requires expensive instance-level annotations. While some work has been done on learning detectors from weakly labeled samples (with only class labels), these detectors do poorly at localization. In this work, we show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories. We call this learning paradigm cross-supervised object detection. While earlier works investigated this paradigm, they did not apply it to realistic complex images (e.g., COCO), and their performance was poor. We propose a unified framework that combines a detection head trained from instance-level annotations and a recognition head learned from image-level annotations, together with a spatial correlation module that bridges the gap between detection and recognition. These contributions enable us to better detect novel objects with image-level annotations in complex multi-object scenes such as the COCO dataset.", "one-sentence_summary": " We show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories.", "pdf": "/pdf/a6fdc33f91f5b937c25cdda47b871c2ddaf46100.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|crosssupervised_object_detection", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=75s_TpHxy", "_bibtex": "@misc{\nchen2021crosssupervised,\ntitle={{\\{}CROSS{\\}}-{\\{}SUPERVISED{\\}} {\\{}OBJECT{\\}} {\\{}DETECTION{\\}}},\nauthor={Zitian Chen and Zhiqiang Shen and Jiahui Yu and Erik Learned-Miller},\nyear={2021},\nurl={https://openreview.net/forum?id=--rcOeCKRh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "--rcOeCKRh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper168/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper168/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper168/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper168/Authors|ICLR.cc/2021/Conference/Paper168/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper168/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873897, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper168/-/Official_Comment"}}}, {"id": "WQ0KpdtZ6_7", "original": null, "number": 9, "cdate": 1605978891021, "ddate": null, "tcdate": 1605978891021, "tmdate": 1605978891021, "tddate": null, "forum": "--rcOeCKRh", "replyto": "6aIxHHNHztI", "invitation": "ICLR.cc/2021/Conference/Paper168/-/Official_Comment", "content": {"title": "Response to Reviewer 4(1/2)", "comment": "We appreciate the valuable opinions from R4. We will try to address the novelty concern and highlight our advantages over other works. \n\n\nQ1: The idea of jointly training weakly-supervised and fully-supervised heads on weakly-supervised and fully-supervised labels with a shared backbone has been explored before (e.g. in [1], which should have been discussed in the related work). \n\n\nA1: Yes, we agree that the structure proposed in [1] has some similarity to our structure and we will add it in the related work. However, there are some differences. \n\nFirst, they only shared the Conv Layers before the ROI Pooling (see Fig.2 for details) whereas we share the Conv Layers, RoI Pooling layers, and FC Layers. Their sharing structure is closer to our \u201cTwo Branch*\u201d Baseline as they both only shared the Conv Layers before the RoI Pooling layer. We know this difference may look subtle, but it actually has a major effect on accuracy. As shown in Table 1, Two Branch* (same as their methods to share backbone) achieves 44.1%mAP whereas Two Head* (same as our methods to share backbone) achieves 47.1% mAP. Note that the comparison is fair since the only difference between Two Branch* and Two Head* is mentioned above and \u201ceverything else is the same\u201d. \n\nSecond, even though both [1] and we have weakly-supervised and fully-supervised heads, our fully-supervised head can detect both weakly-supervised and fully-supervised classes whereas theirs can only detect fully-supervised classes. The purpose of their fully-supervised head is to improve the shared CNN backbone and thus learn a better weakly-supervised detector. In comparison, our fully-supervised head is expected to detect the novel objects in evaluation. Our weakly-supervised head serves as a tool to extract detection information from class labels and further improve our fully-supervised head. In short, in [1], the fully-supervised head is an auxiliary head and in our work, the weakly-supervised head is an auxiliary head. In Figure 4, we compare the quality of detection results between using the weakly-supervised and fully-supervised heads to detect novel instances. \n\nQ2: The idea of using a weakly-supervised model to create pseudo-ground-truth bounding boxes for training another model has also been studied before (e.g. in [2]).\n\nA2: \nYes, as we claim \u201cA majority of WSOD methods (Tang et al. (2017); Wan et al. (2019); Wei et al. (2018)) find that re-training a new detector taking the top-scoring bounding boxes from a weakly supervised object detector as ground truth marginally improves the performance\u201d. Lots of WSOD methods have a \u201cdetection head\u201d that learned from pseudo instance-level annotations generated from a \u201crecognition head\u201d. \n\nHowever, their \u201cdetection head\u201d only learns from pseudo labels and it acts similarly to the \u201crecognition head\u201d. It is more like a slightly improved version of the recognition head. In comparison, our detection head learns from both noisy pseudo labels of novel classes and the ground truth of base classes. Our \u201cdetection head\u201d is very powerful when detecting base objects and thus have a better prior knowledge even for novel classes. As a result, it is a true \u201cdetection head\u201d rather than an updated version of the \u201crecognition head\u201d.\n\nThese differences may look subtle, but we believe our significantly better performance is evidence that they have a major effect on accuracy. In Table 1, when the detection head does not learn from the recognition head and we predict novel class using the recognition head, the performance is 47.1% (Two Head*). Note that in this case the recognition head still benefits from the shared backbone and has a refinement head inside its structure. When we add the connection between the recognition head and the detection head, even without the spatial correlation module, the performance is 52.4% (Ours* w/o SCM).\n\nFinally, part of our contribution is that our architecture allows us to benefit substantially from additional labeled examples on the base classes. What\u2019s more, our pseudo labels are refined by our spatial correlation module before supervising the detection head, whereas other methods do not have this process."}, "signatures": ["ICLR.cc/2021/Conference/Paper168/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper168/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CROSS-SUPERVISED OBJECT DETECTION", "authorids": ["~Zitian_Chen1", "~Zhiqiang_Shen1", "~Jiahui_Yu1", "~Erik_Learned-Miller2"], "authors": ["Zitian Chen", "Zhiqiang Shen", "Jiahui Yu", "Erik Learned-Miller"], "keywords": ["Object detection", "weakly supervised", "transfer leaning"], "abstract": "After learning a new object category from image-level annotations (with no object bounding boxes), humans are remarkably good at precisely localizing those objects. However, building good object localizers (i.e., detectors) currently requires expensive instance-level annotations. While some work has been done on learning detectors from weakly labeled samples (with only class labels), these detectors do poorly at localization. In this work, we show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories. We call this learning paradigm cross-supervised object detection. While earlier works investigated this paradigm, they did not apply it to realistic complex images (e.g., COCO), and their performance was poor. We propose a unified framework that combines a detection head trained from instance-level annotations and a recognition head learned from image-level annotations, together with a spatial correlation module that bridges the gap between detection and recognition. These contributions enable us to better detect novel objects with image-level annotations in complex multi-object scenes such as the COCO dataset.", "one-sentence_summary": " We show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories.", "pdf": "/pdf/a6fdc33f91f5b937c25cdda47b871c2ddaf46100.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|crosssupervised_object_detection", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=75s_TpHxy", "_bibtex": "@misc{\nchen2021crosssupervised,\ntitle={{\\{}CROSS{\\}}-{\\{}SUPERVISED{\\}} {\\{}OBJECT{\\}} {\\{}DETECTION{\\}}},\nauthor={Zitian Chen and Zhiqiang Shen and Jiahui Yu and Erik Learned-Miller},\nyear={2021},\nurl={https://openreview.net/forum?id=--rcOeCKRh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "--rcOeCKRh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper168/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper168/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper168/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper168/Authors|ICLR.cc/2021/Conference/Paper168/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper168/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873897, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper168/-/Official_Comment"}}}, {"id": "9s2u0o1CLTo", "original": null, "number": 7, "cdate": 1605978582020, "ddate": null, "tcdate": 1605978582020, "tmdate": 1605978619423, "tddate": null, "forum": "--rcOeCKRh", "replyto": "joZJrrjMxV2", "invitation": "ICLR.cc/2021/Conference/Paper168/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We appreciate the valuable opinion from you. Please see our responses and clarifications for your questions below.\n\nQ1: How does this work compare with other WSOD recent methods, e.g. C-MIL, [A, B, C] from the experimental results point of view?\n\nA1: In Table.1, we actually compare our results with [C](MSD-VGG16, MSD-Ens, MSD-Ens+FRCN) on PASCAL VOC. We outperform it by 7.5% mAP. Note that [C] is conducted under exactly the same setting as ours. The best performance of C-MIL, [A], [B], and [C] on COCO is 11.4 AP and 24.3 AP_50 whereas our method achieves 13.9 AP and 36.2 AP_50.  On PASCAL VOC, the best performance of C-MIL, [A], [B], and [C] achieves 52.1 mAP on novel classes whereas we achieve 55.7 mAP.  \n\nQ2: How does this work compare with other WSOD in the amount of supervision?\n\nA2: We have the same amount of supervision on novel classes, but on the base classes, we have bounding boxes and class labels whereas WSOD methods only have class labels. Note that all our results (Table 1,2) are reported on the novel classes. So in general, we have extra supervision on another set of classes that have no overlap with the testing classes. \n\nQ3: The text/figure should be adjusted to better map onto each other: Sec. 3.2: \u201cThe image and proposals are fed into several convolutional layers\u201d, but in the figure, the proposals skip those layers.\n\nA3: Good points! Sorry for the confusion and we will update our figure to fix that.\n\nQ4: Section 3.2 is rather hard to read and understand.\n\nA4: Section 3.2 mainly introduces the prior work Online Instance Classifier Refinement (Tang et.al 2017). We will update and make it easier to understand.  \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper168/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper168/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CROSS-SUPERVISED OBJECT DETECTION", "authorids": ["~Zitian_Chen1", "~Zhiqiang_Shen1", "~Jiahui_Yu1", "~Erik_Learned-Miller2"], "authors": ["Zitian Chen", "Zhiqiang Shen", "Jiahui Yu", "Erik Learned-Miller"], "keywords": ["Object detection", "weakly supervised", "transfer leaning"], "abstract": "After learning a new object category from image-level annotations (with no object bounding boxes), humans are remarkably good at precisely localizing those objects. However, building good object localizers (i.e., detectors) currently requires expensive instance-level annotations. While some work has been done on learning detectors from weakly labeled samples (with only class labels), these detectors do poorly at localization. In this work, we show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories. We call this learning paradigm cross-supervised object detection. While earlier works investigated this paradigm, they did not apply it to realistic complex images (e.g., COCO), and their performance was poor. We propose a unified framework that combines a detection head trained from instance-level annotations and a recognition head learned from image-level annotations, together with a spatial correlation module that bridges the gap between detection and recognition. These contributions enable us to better detect novel objects with image-level annotations in complex multi-object scenes such as the COCO dataset.", "one-sentence_summary": " We show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories.", "pdf": "/pdf/a6fdc33f91f5b937c25cdda47b871c2ddaf46100.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|crosssupervised_object_detection", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=75s_TpHxy", "_bibtex": "@misc{\nchen2021crosssupervised,\ntitle={{\\{}CROSS{\\}}-{\\{}SUPERVISED{\\}} {\\{}OBJECT{\\}} {\\{}DETECTION{\\}}},\nauthor={Zitian Chen and Zhiqiang Shen and Jiahui Yu and Erik Learned-Miller},\nyear={2021},\nurl={https://openreview.net/forum?id=--rcOeCKRh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "--rcOeCKRh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper168/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper168/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper168/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper168/Authors|ICLR.cc/2021/Conference/Paper168/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper168/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873897, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper168/-/Official_Comment"}}}, {"id": "VIhQw76tTY", "original": null, "number": 6, "cdate": 1605978420289, "ddate": null, "tcdate": 1605978420289, "tmdate": 1605978420289, "tddate": null, "forum": "--rcOeCKRh", "replyto": "y3sAjjDew_3", "invitation": "ICLR.cc/2021/Conference/Paper168/-/Official_Comment", "content": {"title": "Response to Reviewer 3(1/2)", "comment": "We appreciate the valuable opinion from AnonReviewer3. Please see our responses and clarifications for your questions below.\n\nQ1: The proposed new task, cross-supervised object detection, is almost the same as the task defined in (Hoffman et al. 2014, Tang et al. 2016).\n\nA1: We carefully read the papers (Hoffman et al. 2014, Tang et al. 2016) and we agree that they worked on cross-supervised object detection.  Thus, we will not claim that it is a completely new paradigm in our final paper. However, we do believe Cross Supervised Detection is a good name for this setting, and, we believe these early attempts were in very simplified settings. So, we maintain that we are the first to address this problem in complex multi-object settings.\n\nAs we claim in the paper \u201cEarly WSOD work (Hoffman et al. (2014)) showed fair performance by directly applying recognition\nnetworks to object detection\u201d. In related work introducing cross-supervised object detection methods, we write \u201cHoffman et al. (2014) and Tang et al. (2016) propose methods of adaptation for knowledge transfer from classification features to detection features\u201d. \n\n\u201cHowever, these weakly supervised detectors perform poorly at localization. Most WSOD experiments have been conducted on the ILSVRC (Russakovsky et al. (2015)) data set, in which images have only a single object\u201d. \n\nWe argue that \u201cThe simplicity of these data sets limits the number and types of distractors in an image, making localization substantially easier\u201d and \u201cLearning from only class labels, it is challenging to detect objects at different scales in an image that contains many distractors\u201d. In comparison, we conduct experiments under \u201ccomplex multi-object scenes, such as the COCO dataset\u201d, which is part of our important contribution. \n\nIn the revised version of the paper, we will further clarify that (Hoffman et al. 2014, Tang et al. 2016) have almost the same setting as us and we focus on this problem in more realistic and challenging scenarios. \n\nQ2:  (Uijlings et al. 2018) has a similar task and also conducts experiments on COCO which contains multi-objects in images. \n\nA2: It is true that there is some similarity between our cross-supervised object detection (CSOD) and the task in Uijlings et al. 2018. However, there is a significant difference between our CSOD and their setting.\n\nConsider the following two paradigms:\n\tParadigm 1 (\u201cTransfer learning\u201d). Train a model on a set of base classes. Then, using only the trained model (and not the original training examples), adapt that model to a new set of classes. \n\tParadigm 2 (\u201cmulti-task learning\u201d). Given training data for a set of base classes and a set of novel classes, where the data for each set of classes may be different, train the best possible model for the novel classes. \n\nUijlings et al. 2018 is using paradigm 1 and our CSOD belongs to paradigm 2. This is similar to the distinction between **\u201cmulti-task learning (our paradigm)\u201d** and **\u201ctransfer learning (their paradigm)\u201d**, where you adapt a model to a new set of classes or situations. We argue that cross-supervised detection is analogous to multi-task learning, where the learning occurs jointly. The fact that the ML community has already distinguished between multi-task learning and transfer learning attests to the important difference between these types of paradigms. \n\nAlso, as we claim, \u201cUijlings et al. 2018 use a proposal generator trained on base classes to transfer knowledge by leveraging a MIL framework, organized in a semantic hierarchy.\u201d Uijlings et al. 2018 have additional semantic information. \n\nCSOD \u201chas similarities to both transfer learning and semi-supervised learning, since it transfers knowledge from base class to novel class and has more information about some instances than other instances. However, CSOD represents a distinct and novel paradigm for learning\u201d. We will further clarify the difference in the later version. \n\nQ3: Uijlings et al. 2018 also conduct experiments on COCO which contains multi-objects in images. \n\nA3: We have discussed the difference between our setting and the setting of  Uijlings et al. 2018 in A2. Despite the difference, we still can compare our methods and theirs using the same backbone and same class split up (taking PASCAL VOC classes as base classes and other classes as novel classes) on COCO. Note that since the setting has differences the comparison is not absolutely fair (as discussed in A2) but it still gives us a reference to the efficiency of our work. Our CorLoc for IOU>0.5 is 59.4 while theirs are 26.2. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper168/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper168/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CROSS-SUPERVISED OBJECT DETECTION", "authorids": ["~Zitian_Chen1", "~Zhiqiang_Shen1", "~Jiahui_Yu1", "~Erik_Learned-Miller2"], "authors": ["Zitian Chen", "Zhiqiang Shen", "Jiahui Yu", "Erik Learned-Miller"], "keywords": ["Object detection", "weakly supervised", "transfer leaning"], "abstract": "After learning a new object category from image-level annotations (with no object bounding boxes), humans are remarkably good at precisely localizing those objects. However, building good object localizers (i.e., detectors) currently requires expensive instance-level annotations. While some work has been done on learning detectors from weakly labeled samples (with only class labels), these detectors do poorly at localization. In this work, we show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories. We call this learning paradigm cross-supervised object detection. While earlier works investigated this paradigm, they did not apply it to realistic complex images (e.g., COCO), and their performance was poor. We propose a unified framework that combines a detection head trained from instance-level annotations and a recognition head learned from image-level annotations, together with a spatial correlation module that bridges the gap between detection and recognition. These contributions enable us to better detect novel objects with image-level annotations in complex multi-object scenes such as the COCO dataset.", "one-sentence_summary": " We show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories.", "pdf": "/pdf/a6fdc33f91f5b937c25cdda47b871c2ddaf46100.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|crosssupervised_object_detection", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=75s_TpHxy", "_bibtex": "@misc{\nchen2021crosssupervised,\ntitle={{\\{}CROSS{\\}}-{\\{}SUPERVISED{\\}} {\\{}OBJECT{\\}} {\\{}DETECTION{\\}}},\nauthor={Zitian Chen and Zhiqiang Shen and Jiahui Yu and Erik Learned-Miller},\nyear={2021},\nurl={https://openreview.net/forum?id=--rcOeCKRh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "--rcOeCKRh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper168/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper168/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper168/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper168/Authors|ICLR.cc/2021/Conference/Paper168/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper168/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873897, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper168/-/Official_Comment"}}}, {"id": "joZJrrjMxV2", "original": null, "number": 2, "cdate": 1603915464600, "ddate": null, "tcdate": 1603915464600, "tmdate": 1605024748862, "tddate": null, "forum": "--rcOeCKRh", "replyto": "--rcOeCKRh", "invitation": "ICLR.cc/2021/Conference/Paper168/-/Official_Review", "content": {"title": "Innovative formulation, but more comparisons are needed", "review": "**Summary and contributions**:\nThe paper presents a new task formulation for transferring knowledge for object detection from fully labeled classes to weakly labeled ones, with better results for complex scenes. So the authors propose to learn object detectors for novel classes (which were not seen at training time but specified apriori), based only on that object class label and the bounding boxes for other objects in the image (from the base classes). Compared with other works, the solution claims to be better on the localization aspect, focusing on the object as a whole, not only on its discriminative parts. Nevertheless, there are very few competitors taken into account. The model combines three components: a detection head and a recognition head, based on the same, unified backbone architecture, and a spatial correlation module that aligns the two heads. The authors test their solution, Cross-Supervised Object Detection, on PASCAL VOC and COCO (multi-object scenes).\n\n**Strengths**:\n- Proposing a model that succeeds in learning both novel and base classes at the same time, on two heads, extracting information from the weak labels (recognition head) and using it as supervision for the second head (detection head).\n- The qualitative results look significantly better compared with single heads, containing more of the entire object, not only the important/discriminatory parts as the authors pointed out.\n- FCOS with only 5 conv layers reaches a good enough performance.\n\n\n**Weaknesses**:\n* How does this work compare with other WSOD recent methods, e.g. C-MIL, [A, B, C] from the experimental results point of view, and the amount of supervision?\n* The text/figure should be adjusted to better map one each other: Sec. 3.2: \u201cThe image and proposals are fed into several convolutional layers\u201d, but in the figure, the proposals skip those layers.\n* Section 3.2 is rather hard to read and understand (it has several inaccuracies in the formulas).\n\n**Quality**:\nThe paper is technically sound.\n\n**Clarity**: \nThe paper is mainly clearly written (except for Section 3.2 Recognition Head).\n\n**Novelty**:\nThe components are not novel, but the proposed learning paradigm is novel.\n\n**Significance of this work**:\nThe impact of the work is significant, enabling a new way (CSOD) of transferring knowledge between classes, using weak labels.\n\n**Typos**:\n- \u201ccorresponds to the respective element of the matrix...\u201d the following formula is wrong (d should be replaced with r) \n- what does the upper index R mean and why is it useful?\n- matrices should be capital letters in bold\n- \u201cwhich is expresses\u201d\n\n\nWhile I have some doubts regarding the comparison with other works, I hope they will be clarified during the rebuttal.\n\n\n[A] High-Quality Proposals for Weakly Supervised Object Detection, Cheng, G., Yang, J., Gao, D., Guo, L., & Han, J..Transactions on Image Processing 2020 \n\n[B] Instance-aware, Context-focused, and Memory-efficient Weakly Supervised Object Detection. Zhongzheng Ren, Zhiding Yu, Xiaodong Yang, Ming-Yu Liu, Yong Jae Lee, Alexander G. Schwing, Jan Kautz. CVPR 2020\n\n[C] Mixed Supervised Object Detection with Robust Objectness Transfer, Yan Li, Junge Zhang, Kaiqi Huang, Jianguo Zhang. PAMI 2019", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper168/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper168/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CROSS-SUPERVISED OBJECT DETECTION", "authorids": ["~Zitian_Chen1", "~Zhiqiang_Shen1", "~Jiahui_Yu1", "~Erik_Learned-Miller2"], "authors": ["Zitian Chen", "Zhiqiang Shen", "Jiahui Yu", "Erik Learned-Miller"], "keywords": ["Object detection", "weakly supervised", "transfer leaning"], "abstract": "After learning a new object category from image-level annotations (with no object bounding boxes), humans are remarkably good at precisely localizing those objects. However, building good object localizers (i.e., detectors) currently requires expensive instance-level annotations. While some work has been done on learning detectors from weakly labeled samples (with only class labels), these detectors do poorly at localization. In this work, we show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories. We call this learning paradigm cross-supervised object detection. While earlier works investigated this paradigm, they did not apply it to realistic complex images (e.g., COCO), and their performance was poor. We propose a unified framework that combines a detection head trained from instance-level annotations and a recognition head learned from image-level annotations, together with a spatial correlation module that bridges the gap between detection and recognition. These contributions enable us to better detect novel objects with image-level annotations in complex multi-object scenes such as the COCO dataset.", "one-sentence_summary": " We show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories.", "pdf": "/pdf/a6fdc33f91f5b937c25cdda47b871c2ddaf46100.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|crosssupervised_object_detection", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=75s_TpHxy", "_bibtex": "@misc{\nchen2021crosssupervised,\ntitle={{\\{}CROSS{\\}}-{\\{}SUPERVISED{\\}} {\\{}OBJECT{\\}} {\\{}DETECTION{\\}}},\nauthor={Zitian Chen and Zhiqiang Shen and Jiahui Yu and Erik Learned-Miller},\nyear={2021},\nurl={https://openreview.net/forum?id=--rcOeCKRh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "--rcOeCKRh", "replyto": "--rcOeCKRh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper168/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538148924, "tmdate": 1606915780565, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper168/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper168/-/Official_Review"}}}, {"id": "sfdV4h2xpzl", "original": null, "number": 4, "cdate": 1604157906207, "ddate": null, "tcdate": 1604157906207, "tmdate": 1605024748733, "tddate": null, "forum": "--rcOeCKRh", "replyto": "--rcOeCKRh", "invitation": "ICLR.cc/2021/Conference/Paper168/-/Official_Review", "content": {"title": "The paper proposes a new problem setting for learning object detectors using weak supervision as well as a deep learning solution. However, the presentation and its relation to previous works should be improved.", "review": "\nThis paper defines cross-supervised object detection which learns a detector from both image-level and instance-level annotations. It proposes a unified framework along with a spatial correlation module for the task. The spatial correlation module is used for transfer mapping information from base categories to novel categories. It conducts experiments on the PASCAL VOC dataset and COCO dataset, demonstrating the effectiveness.\n\nPros:\n(1) The proposed spatial correlation module is a novel and effective transfer module.\n(2) The ablation studies are relatively complete. \n\nCons:\n(1) The structure of the proposed spatial correlation module should be described in more detail. What is the meaning of \u201creplacing the backbone and feature pyramid network with five max-pooling layers.\u201d in the heatmap detection part?\n(2) In Table 1, are the experimental settings of those competitors such as MSD-VGG16, MSD-Ens, and Weight Transfer et al. exactly the same as those used in this paper?\n(3) In Table 2, it seems like the method taking the non-VOC as the base classes while Hu et al. (2018) use the non-VOC as those classes without mask annotation. Can you tell me the reasons for this choice?\n(4) Some similar problem settings are defined in [a] and [b]. The paper fails to compare the problem settings and justify the usefulness of the proposed problem setting in real application scenarios.\n\nOverall evaluation: The major contribution of this paper comes from the spatial correlation module. However, I still have some doubts about the structure of this module. Since this task is first presented, I want to make sure that the comparison is as fair as possible. \n\n[a] Weakly- and Semi-Supervised Fast Region-Based CNN for Object Detection. Journal of Computer Science and Technology (JCST) 34(6): 1269\u20131278 Nov. 2019.\n[b] LSTD: A Low-Shot Transfer Detector for Object Detection, AAAI 2018", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper168/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper168/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CROSS-SUPERVISED OBJECT DETECTION", "authorids": ["~Zitian_Chen1", "~Zhiqiang_Shen1", "~Jiahui_Yu1", "~Erik_Learned-Miller2"], "authors": ["Zitian Chen", "Zhiqiang Shen", "Jiahui Yu", "Erik Learned-Miller"], "keywords": ["Object detection", "weakly supervised", "transfer leaning"], "abstract": "After learning a new object category from image-level annotations (with no object bounding boxes), humans are remarkably good at precisely localizing those objects. However, building good object localizers (i.e., detectors) currently requires expensive instance-level annotations. While some work has been done on learning detectors from weakly labeled samples (with only class labels), these detectors do poorly at localization. In this work, we show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories. We call this learning paradigm cross-supervised object detection. While earlier works investigated this paradigm, they did not apply it to realistic complex images (e.g., COCO), and their performance was poor. We propose a unified framework that combines a detection head trained from instance-level annotations and a recognition head learned from image-level annotations, together with a spatial correlation module that bridges the gap between detection and recognition. These contributions enable us to better detect novel objects with image-level annotations in complex multi-object scenes such as the COCO dataset.", "one-sentence_summary": " We show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories.", "pdf": "/pdf/a6fdc33f91f5b937c25cdda47b871c2ddaf46100.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|crosssupervised_object_detection", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=75s_TpHxy", "_bibtex": "@misc{\nchen2021crosssupervised,\ntitle={{\\{}CROSS{\\}}-{\\{}SUPERVISED{\\}} {\\{}OBJECT{\\}} {\\{}DETECTION{\\}}},\nauthor={Zitian Chen and Zhiqiang Shen and Jiahui Yu and Erik Learned-Miller},\nyear={2021},\nurl={https://openreview.net/forum?id=--rcOeCKRh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "--rcOeCKRh", "replyto": "--rcOeCKRh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper168/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538148924, "tmdate": 1606915780565, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper168/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper168/-/Official_Review"}}}], "count": 14}