{"notes": [{"id": "r1gfQgSFDr", "original": "Skxj0Qetvr", "number": 2203, "cdate": 1569439770400, "ddate": null, "tcdate": 1569439770400, "tmdate": 1583912034810, "tddate": null, "forum": "r1gfQgSFDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["mikbinkowski@gmail.com", "jeffdonahue@google.com", "sedielem@google.com", "aidanclark@google.com", "eriche@google.com", "ncasagrande@google.com", "luisca@google.com", "simonyan@google.com"], "title": "High Fidelity Speech Synthesis with Adversarial Networks", "authors": ["Miko\u0142aj Bi\u0144kowski", "Jeff Donahue", "Sander Dieleman", "Aidan Clark", "Erich Elsen", "Norman Casagrande", "Luis C. Cobo", "Karen Simonyan"], "pdf": "/pdf/ab0ccb39f2206a75dadecc6807d39c1079d9e4de.pdf", "TL;DR": "We introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech, which achieves Mean Opinion Score (MOS) 4.2.", "abstract": "Generative adversarial networks have seen rapid development in recent years and have led to remarkable improvements in generative modelling of images. However, their application in the audio domain has received limited attention,\nand autoregressive models, such as WaveNet, remain the state of the art in generative modelling of audio signals such as human speech. To address this paucity, we introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech.\nOur architecture is composed of a conditional feed-forward generator producing raw speech audio, and an ensemble of discriminators which operate on random windows of different sizes. The discriminators analyse the audio both in terms of general realism, as well as how well the audio corresponds to the utterance that should be pronounced.  To measure the performance of GAN-TTS, we employ both subjective human evaluation (MOS - Mean Opinion Score), as well as novel quantitative metrics (Fr\u00e9chet DeepSpeech Distance and Kernel DeepSpeech Distance), which we find to be well correlated with MOS. We show that GAN-TTS is capable of generating high-fidelity speech with naturalness comparable to the state-of-the-art models, and unlike autoregressive models, it is highly parallelisable thanks to an efficient feed-forward generator. Listen to GAN-TTS reading this abstract at https://storage.googleapis.com/deepmind-media/research/abstract.wav", "keywords": ["texttospeech", "speechsynthesis", "audiosynthesis", "gans", "generativeadversarialnetworks", "implicitgenerativemodels"], "paperhash": "bikowski|high_fidelity_speech_synthesis_with_adversarial_networks", "code": "https://github.com/mbinkowski/DeepSpeechDistances", "_bibtex": "@inproceedings{\nBi\u0144kowski2020High,\ntitle={High Fidelity Speech Synthesis with Adversarial Networks},\nauthor={Miko\u0142aj Bi\u0144kowski and Jeff Donahue and Sander Dieleman and Aidan Clark and Erich Elsen and Norman Casagrande and Luis C. Cobo and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gfQgSFDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a98e874542b617b51a0a7e7c64f880ee43f1d046.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "vmFSpeIS8p", "original": null, "number": 1, "cdate": 1576798743128, "ddate": null, "tcdate": 1576798743128, "tmdate": 1576800893074, "tddate": null, "forum": "r1gfQgSFDr", "replyto": "r1gfQgSFDr", "invitation": "ICLR.cc/2020/Conference/Paper2203/-/Decision", "content": {"decision": "Accept (Talk)", "comment": "The authors design a GAN-based text-to-speech synthesis model that performs competitively with state-of-the-art synthesizers.  The reviewers and I agree that this appears to be the first really successful effort at GAN-based synthesis.  Additional positives are that the model is designed to be highly parallelisable, and that the authors also propose several automatic measures of performance in addition to reporting human mean opinion scores.  The automatic measures correlate well (though far from perfectly) with human judgments, and in any case are a nice contribution to the area of evaluation of generative models.  It would be even more convincing if the authors presented human A/B forced-choice test results (in addition to the mean opinion scores), which are often included in speech synthesis evaluation, but this is a minor quibble.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mikbinkowski@gmail.com", "jeffdonahue@google.com", "sedielem@google.com", "aidanclark@google.com", "eriche@google.com", "ncasagrande@google.com", "luisca@google.com", "simonyan@google.com"], "title": "High Fidelity Speech Synthesis with Adversarial Networks", "authors": ["Miko\u0142aj Bi\u0144kowski", "Jeff Donahue", "Sander Dieleman", "Aidan Clark", "Erich Elsen", "Norman Casagrande", "Luis C. Cobo", "Karen Simonyan"], "pdf": "/pdf/ab0ccb39f2206a75dadecc6807d39c1079d9e4de.pdf", "TL;DR": "We introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech, which achieves Mean Opinion Score (MOS) 4.2.", "abstract": "Generative adversarial networks have seen rapid development in recent years and have led to remarkable improvements in generative modelling of images. However, their application in the audio domain has received limited attention,\nand autoregressive models, such as WaveNet, remain the state of the art in generative modelling of audio signals such as human speech. To address this paucity, we introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech.\nOur architecture is composed of a conditional feed-forward generator producing raw speech audio, and an ensemble of discriminators which operate on random windows of different sizes. The discriminators analyse the audio both in terms of general realism, as well as how well the audio corresponds to the utterance that should be pronounced.  To measure the performance of GAN-TTS, we employ both subjective human evaluation (MOS - Mean Opinion Score), as well as novel quantitative metrics (Fr\u00e9chet DeepSpeech Distance and Kernel DeepSpeech Distance), which we find to be well correlated with MOS. We show that GAN-TTS is capable of generating high-fidelity speech with naturalness comparable to the state-of-the-art models, and unlike autoregressive models, it is highly parallelisable thanks to an efficient feed-forward generator. Listen to GAN-TTS reading this abstract at https://storage.googleapis.com/deepmind-media/research/abstract.wav", "keywords": ["texttospeech", "speechsynthesis", "audiosynthesis", "gans", "generativeadversarialnetworks", "implicitgenerativemodels"], "paperhash": "bikowski|high_fidelity_speech_synthesis_with_adversarial_networks", "code": "https://github.com/mbinkowski/DeepSpeechDistances", "_bibtex": "@inproceedings{\nBi\u0144kowski2020High,\ntitle={High Fidelity Speech Synthesis with Adversarial Networks},\nauthor={Miko\u0142aj Bi\u0144kowski and Jeff Donahue and Sander Dieleman and Aidan Clark and Erich Elsen and Norman Casagrande and Luis C. Cobo and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gfQgSFDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a98e874542b617b51a0a7e7c64f880ee43f1d046.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1gfQgSFDr", "replyto": "r1gfQgSFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795718251, "tmdate": 1576800268700, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2203/-/Decision"}}}, {"id": "rklAPQJOKS", "original": null, "number": 1, "cdate": 1571447654183, "ddate": null, "tcdate": 1571447654183, "tmdate": 1574471166637, "tddate": null, "forum": "r1gfQgSFDr", "replyto": "r1gfQgSFDr", "invitation": "ICLR.cc/2020/Conference/Paper2203/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "I want thank the authors for solving this long-standing GAN challenge in raw waveform synthesis. With all due respect, previous GAN trials for audio synthesis are inspiring, but their audio qualities are far away from the state-of-the-art results. Although the speech fidelity of GAN-TTS is still worse than WaveNet and Parallel WaveNet from the posted sample, it has begun to close the significant performance gap that has existed between autoregressive models and GANs for raw audios. Overall, this is a very good paper with significant contributions to the filed.\n\nDetailed comment:\n\n1, In WaveNet, the conditional features (linguistic / mel-spectrogram) are added as bias terms in the convolutional layers. Did the authors tried this alternative architecture for the generator, which uses the white noisy z as network input (similar as flow-based models, e.g., Parallel WaveNet) and the conditional features as bias term in the convolutional layers? \n\n2, Could the authors comment the importance of serval architecture choices in this work? From Table 1, it seems to me that the ensemble of random window discriminators is the most important (perhaps the only important) contributing factor for the success. For example, the MOS score was boosted from 1.889 to 4.213 by replacing a single full discriminator to the ensemble of RWDs.\n\n3, The notations in Eq. (1) and (2) are messy. Although I can figure their meaning from the context, one may clarify certain notations if they appear at the first time. \n\n4, The stable training (NO model collapses) is pretty impressive. Could the authors shed some light on the potential reason? Does the ensemble of RWD regularizes the training? What's your experience for training FullD (does not have random window ) and cRWD_1 (only has one random window discriminator)? Are they still very stable? Also, could the authors comment on the importance of large batch size -- 1024 for stable training of GAN-TTS? \n\n5, Although there is a notable difference, one may properly mention previous work Yamamoto et al. (2019), which uses GAN as an auxiliary loss within ClariNet and obtains high-fidelity speech ( https://r9y9.github.io/demos/projects/interspeech2019/ ).   \n\nYamamoto et al. Probability Density Distillation with Generative Adversarial Networks for High-Quality Parallel Waveform Generation. 2019.\n\n\n=== update === \n\nThank you for the detailed response.  \n2,  Thanks for the elaboration.    \n4,  It would be very interesting to see an analysis of model stability with smaller batch sizes. \n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2203/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2203/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mikbinkowski@gmail.com", "jeffdonahue@google.com", "sedielem@google.com", "aidanclark@google.com", "eriche@google.com", "ncasagrande@google.com", "luisca@google.com", "simonyan@google.com"], "title": "High Fidelity Speech Synthesis with Adversarial Networks", "authors": ["Miko\u0142aj Bi\u0144kowski", "Jeff Donahue", "Sander Dieleman", "Aidan Clark", "Erich Elsen", "Norman Casagrande", "Luis C. Cobo", "Karen Simonyan"], "pdf": "/pdf/ab0ccb39f2206a75dadecc6807d39c1079d9e4de.pdf", "TL;DR": "We introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech, which achieves Mean Opinion Score (MOS) 4.2.", "abstract": "Generative adversarial networks have seen rapid development in recent years and have led to remarkable improvements in generative modelling of images. However, their application in the audio domain has received limited attention,\nand autoregressive models, such as WaveNet, remain the state of the art in generative modelling of audio signals such as human speech. To address this paucity, we introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech.\nOur architecture is composed of a conditional feed-forward generator producing raw speech audio, and an ensemble of discriminators which operate on random windows of different sizes. The discriminators analyse the audio both in terms of general realism, as well as how well the audio corresponds to the utterance that should be pronounced.  To measure the performance of GAN-TTS, we employ both subjective human evaluation (MOS - Mean Opinion Score), as well as novel quantitative metrics (Fr\u00e9chet DeepSpeech Distance and Kernel DeepSpeech Distance), which we find to be well correlated with MOS. We show that GAN-TTS is capable of generating high-fidelity speech with naturalness comparable to the state-of-the-art models, and unlike autoregressive models, it is highly parallelisable thanks to an efficient feed-forward generator. Listen to GAN-TTS reading this abstract at https://storage.googleapis.com/deepmind-media/research/abstract.wav", "keywords": ["texttospeech", "speechsynthesis", "audiosynthesis", "gans", "generativeadversarialnetworks", "implicitgenerativemodels"], "paperhash": "bikowski|high_fidelity_speech_synthesis_with_adversarial_networks", "code": "https://github.com/mbinkowski/DeepSpeechDistances", "_bibtex": "@inproceedings{\nBi\u0144kowski2020High,\ntitle={High Fidelity Speech Synthesis with Adversarial Networks},\nauthor={Miko\u0142aj Bi\u0144kowski and Jeff Donahue and Sander Dieleman and Aidan Clark and Erich Elsen and Norman Casagrande and Luis C. Cobo and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gfQgSFDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a98e874542b617b51a0a7e7c64f880ee43f1d046.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1gfQgSFDr", "replyto": "r1gfQgSFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2203/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2203/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575306292995, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2203/Reviewers"], "noninvitees": [], "tcdate": 1570237726227, "tmdate": 1575306293008, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2203/-/Official_Review"}}}, {"id": "rkg0V5P9iB", "original": null, "number": 6, "cdate": 1573710389657, "ddate": null, "tcdate": 1573710389657, "tmdate": 1573710389657, "tddate": null, "forum": "r1gfQgSFDr", "replyto": "r1gfQgSFDr", "invitation": "ICLR.cc/2020/Conference/Paper2203/-/Official_Comment", "content": {"title": "Reviewers, any comments on the author responses?", "comment": "Dear Reviewers, thanks for your thoughtful input on this submission! \u00a0The authors have now responded to your comments. \u00a0Please be sure to go through their replies and revisions. \u00a0If you have additional feedback or questions, it would be great to get them this week while the authors still have the opportunity to respond/revise further. \u00a0Thanks!\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2203/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2203/Area_Chair1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mikbinkowski@gmail.com", "jeffdonahue@google.com", "sedielem@google.com", "aidanclark@google.com", "eriche@google.com", "ncasagrande@google.com", "luisca@google.com", "simonyan@google.com"], "title": "High Fidelity Speech Synthesis with Adversarial Networks", "authors": ["Miko\u0142aj Bi\u0144kowski", "Jeff Donahue", "Sander Dieleman", "Aidan Clark", "Erich Elsen", "Norman Casagrande", "Luis C. Cobo", "Karen Simonyan"], "pdf": "/pdf/ab0ccb39f2206a75dadecc6807d39c1079d9e4de.pdf", "TL;DR": "We introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech, which achieves Mean Opinion Score (MOS) 4.2.", "abstract": "Generative adversarial networks have seen rapid development in recent years and have led to remarkable improvements in generative modelling of images. However, their application in the audio domain has received limited attention,\nand autoregressive models, such as WaveNet, remain the state of the art in generative modelling of audio signals such as human speech. To address this paucity, we introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech.\nOur architecture is composed of a conditional feed-forward generator producing raw speech audio, and an ensemble of discriminators which operate on random windows of different sizes. The discriminators analyse the audio both in terms of general realism, as well as how well the audio corresponds to the utterance that should be pronounced.  To measure the performance of GAN-TTS, we employ both subjective human evaluation (MOS - Mean Opinion Score), as well as novel quantitative metrics (Fr\u00e9chet DeepSpeech Distance and Kernel DeepSpeech Distance), which we find to be well correlated with MOS. We show that GAN-TTS is capable of generating high-fidelity speech with naturalness comparable to the state-of-the-art models, and unlike autoregressive models, it is highly parallelisable thanks to an efficient feed-forward generator. Listen to GAN-TTS reading this abstract at https://storage.googleapis.com/deepmind-media/research/abstract.wav", "keywords": ["texttospeech", "speechsynthesis", "audiosynthesis", "gans", "generativeadversarialnetworks", "implicitgenerativemodels"], "paperhash": "bikowski|high_fidelity_speech_synthesis_with_adversarial_networks", "code": "https://github.com/mbinkowski/DeepSpeechDistances", "_bibtex": "@inproceedings{\nBi\u0144kowski2020High,\ntitle={High Fidelity Speech Synthesis with Adversarial Networks},\nauthor={Miko\u0142aj Bi\u0144kowski and Jeff Donahue and Sander Dieleman and Aidan Clark and Erich Elsen and Norman Casagrande and Luis C. Cobo and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gfQgSFDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a98e874542b617b51a0a7e7c64f880ee43f1d046.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1gfQgSFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2203/Authors", "ICLR.cc/2020/Conference/Paper2203/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2203/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2203/Reviewers", "ICLR.cc/2020/Conference/Paper2203/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2203/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2203/Authors|ICLR.cc/2020/Conference/Paper2203/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144826, "tmdate": 1576860540748, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2203/Authors", "ICLR.cc/2020/Conference/Paper2203/Reviewers", "ICLR.cc/2020/Conference/Paper2203/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2203/-/Official_Comment"}}}, {"id": "r1gpl85tjH", "original": null, "number": 5, "cdate": 1573656053500, "ddate": null, "tcdate": 1573656053500, "tmdate": 1573656053500, "tddate": null, "forum": "r1gfQgSFDr", "replyto": "rklAPQJOKS", "invitation": "ICLR.cc/2020/Conference/Paper2203/-/Official_Comment", "content": {"title": "Response to Official Blind Review #3", "comment": "Thank you for the detailed comments.\n\n1. We did not do experiments with such generator architecture. Although we have considered other architectural choices for generator and ways of conditioning, our early experiments showed that our residual-upsampling scheme is more efficient than parallel wavenet\u2019s full-resolution scheme. The correspondence between temporal dimensions of the conditioning and the waveform also seemed important and hence we decided to keep the proposed generator architecture throughout.\n\n2. Indeed we believe that the use of the ensemble of random window discriminators was the main factor behind the performance we obtained. This, however, breaks down to three steps: \n(a) switching from full discriminator to random-window discriminator(s),\n(b) including unconditional random window discriminator(s),\n(c) including several different window sizes in the ensemble.\nAs can be seen in Table 1., (a) already brings a huge improvement (from ~1.9 to ~3.4 MOS). (b) and (c) also seem to be important; we have considered fixing the window size or using only conditional RWDs, but all of such trials turned out considerably worse. Only models combining all of (a) - (c) made it past MOS of 4.1.\n\n3. Indeed D^c_k and D^u_k should have been clearly defined there; we clarified this notation in the updated version of the submission.\n\n4. For the training stability, please see our joint response. As for the role of the batch size, we fixed it throughout all experiments, but we will include analysis of model stability with smaller batch sizes in the final version of the paper.\n\n5. Thank you for pointing out this related work. We refer to it in the updated version of the submission."}, "signatures": ["ICLR.cc/2020/Conference/Paper2203/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2203/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mikbinkowski@gmail.com", "jeffdonahue@google.com", "sedielem@google.com", "aidanclark@google.com", "eriche@google.com", "ncasagrande@google.com", "luisca@google.com", "simonyan@google.com"], "title": "High Fidelity Speech Synthesis with Adversarial Networks", "authors": ["Miko\u0142aj Bi\u0144kowski", "Jeff Donahue", "Sander Dieleman", "Aidan Clark", "Erich Elsen", "Norman Casagrande", "Luis C. Cobo", "Karen Simonyan"], "pdf": "/pdf/ab0ccb39f2206a75dadecc6807d39c1079d9e4de.pdf", "TL;DR": "We introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech, which achieves Mean Opinion Score (MOS) 4.2.", "abstract": "Generative adversarial networks have seen rapid development in recent years and have led to remarkable improvements in generative modelling of images. However, their application in the audio domain has received limited attention,\nand autoregressive models, such as WaveNet, remain the state of the art in generative modelling of audio signals such as human speech. To address this paucity, we introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech.\nOur architecture is composed of a conditional feed-forward generator producing raw speech audio, and an ensemble of discriminators which operate on random windows of different sizes. The discriminators analyse the audio both in terms of general realism, as well as how well the audio corresponds to the utterance that should be pronounced.  To measure the performance of GAN-TTS, we employ both subjective human evaluation (MOS - Mean Opinion Score), as well as novel quantitative metrics (Fr\u00e9chet DeepSpeech Distance and Kernel DeepSpeech Distance), which we find to be well correlated with MOS. We show that GAN-TTS is capable of generating high-fidelity speech with naturalness comparable to the state-of-the-art models, and unlike autoregressive models, it is highly parallelisable thanks to an efficient feed-forward generator. Listen to GAN-TTS reading this abstract at https://storage.googleapis.com/deepmind-media/research/abstract.wav", "keywords": ["texttospeech", "speechsynthesis", "audiosynthesis", "gans", "generativeadversarialnetworks", "implicitgenerativemodels"], "paperhash": "bikowski|high_fidelity_speech_synthesis_with_adversarial_networks", "code": "https://github.com/mbinkowski/DeepSpeechDistances", "_bibtex": "@inproceedings{\nBi\u0144kowski2020High,\ntitle={High Fidelity Speech Synthesis with Adversarial Networks},\nauthor={Miko\u0142aj Bi\u0144kowski and Jeff Donahue and Sander Dieleman and Aidan Clark and Erich Elsen and Norman Casagrande and Luis C. Cobo and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gfQgSFDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a98e874542b617b51a0a7e7c64f880ee43f1d046.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1gfQgSFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2203/Authors", "ICLR.cc/2020/Conference/Paper2203/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2203/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2203/Reviewers", "ICLR.cc/2020/Conference/Paper2203/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2203/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2203/Authors|ICLR.cc/2020/Conference/Paper2203/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144826, "tmdate": 1576860540748, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2203/Authors", "ICLR.cc/2020/Conference/Paper2203/Reviewers", "ICLR.cc/2020/Conference/Paper2203/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2203/-/Official_Comment"}}}, {"id": "HylKa4cYjH", "original": null, "number": 3, "cdate": 1573655745435, "ddate": null, "tcdate": 1573655745435, "tmdate": 1573655807001, "tddate": null, "forum": "r1gfQgSFDr", "replyto": "Hyg3Zr60FS", "invitation": "ICLR.cc/2020/Conference/Paper2203/-/Official_Comment", "content": {"title": "Response to Official Blind Review #2", "comment": "Thank you for your comments. Please refer to the joint response in regards to training stability and mode collapse."}, "signatures": ["ICLR.cc/2020/Conference/Paper2203/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2203/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mikbinkowski@gmail.com", "jeffdonahue@google.com", "sedielem@google.com", "aidanclark@google.com", "eriche@google.com", "ncasagrande@google.com", "luisca@google.com", "simonyan@google.com"], "title": "High Fidelity Speech Synthesis with Adversarial Networks", "authors": ["Miko\u0142aj Bi\u0144kowski", "Jeff Donahue", "Sander Dieleman", "Aidan Clark", "Erich Elsen", "Norman Casagrande", "Luis C. Cobo", "Karen Simonyan"], "pdf": "/pdf/ab0ccb39f2206a75dadecc6807d39c1079d9e4de.pdf", "TL;DR": "We introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech, which achieves Mean Opinion Score (MOS) 4.2.", "abstract": "Generative adversarial networks have seen rapid development in recent years and have led to remarkable improvements in generative modelling of images. However, their application in the audio domain has received limited attention,\nand autoregressive models, such as WaveNet, remain the state of the art in generative modelling of audio signals such as human speech. To address this paucity, we introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech.\nOur architecture is composed of a conditional feed-forward generator producing raw speech audio, and an ensemble of discriminators which operate on random windows of different sizes. The discriminators analyse the audio both in terms of general realism, as well as how well the audio corresponds to the utterance that should be pronounced.  To measure the performance of GAN-TTS, we employ both subjective human evaluation (MOS - Mean Opinion Score), as well as novel quantitative metrics (Fr\u00e9chet DeepSpeech Distance and Kernel DeepSpeech Distance), which we find to be well correlated with MOS. We show that GAN-TTS is capable of generating high-fidelity speech with naturalness comparable to the state-of-the-art models, and unlike autoregressive models, it is highly parallelisable thanks to an efficient feed-forward generator. Listen to GAN-TTS reading this abstract at https://storage.googleapis.com/deepmind-media/research/abstract.wav", "keywords": ["texttospeech", "speechsynthesis", "audiosynthesis", "gans", "generativeadversarialnetworks", "implicitgenerativemodels"], "paperhash": "bikowski|high_fidelity_speech_synthesis_with_adversarial_networks", "code": "https://github.com/mbinkowski/DeepSpeechDistances", "_bibtex": "@inproceedings{\nBi\u0144kowski2020High,\ntitle={High Fidelity Speech Synthesis with Adversarial Networks},\nauthor={Miko\u0142aj Bi\u0144kowski and Jeff Donahue and Sander Dieleman and Aidan Clark and Erich Elsen and Norman Casagrande and Luis C. Cobo and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gfQgSFDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a98e874542b617b51a0a7e7c64f880ee43f1d046.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1gfQgSFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2203/Authors", "ICLR.cc/2020/Conference/Paper2203/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2203/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2203/Reviewers", "ICLR.cc/2020/Conference/Paper2203/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2203/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2203/Authors|ICLR.cc/2020/Conference/Paper2203/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144826, "tmdate": 1576860540748, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2203/Authors", "ICLR.cc/2020/Conference/Paper2203/Reviewers", "ICLR.cc/2020/Conference/Paper2203/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2203/-/Official_Comment"}}}, {"id": "SJglWB9FoS", "original": null, "number": 4, "cdate": 1573655799581, "ddate": null, "tcdate": 1573655799581, "tmdate": 1573655799581, "tddate": null, "forum": "r1gfQgSFDr", "replyto": "BJlJ-YsaKS", "invitation": "ICLR.cc/2020/Conference/Paper2203/-/Official_Comment", "content": {"title": "Response to Official Blind Review #1", "comment": "Thank you for your comments. We have added a pseudo-code description of TTS-GAN training algorithm to the updated submission. We believe that, together with other architectural details present in the paper, it makes our work reproducible."}, "signatures": ["ICLR.cc/2020/Conference/Paper2203/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2203/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mikbinkowski@gmail.com", "jeffdonahue@google.com", "sedielem@google.com", "aidanclark@google.com", "eriche@google.com", "ncasagrande@google.com", "luisca@google.com", "simonyan@google.com"], "title": "High Fidelity Speech Synthesis with Adversarial Networks", "authors": ["Miko\u0142aj Bi\u0144kowski", "Jeff Donahue", "Sander Dieleman", "Aidan Clark", "Erich Elsen", "Norman Casagrande", "Luis C. Cobo", "Karen Simonyan"], "pdf": "/pdf/ab0ccb39f2206a75dadecc6807d39c1079d9e4de.pdf", "TL;DR": "We introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech, which achieves Mean Opinion Score (MOS) 4.2.", "abstract": "Generative adversarial networks have seen rapid development in recent years and have led to remarkable improvements in generative modelling of images. However, their application in the audio domain has received limited attention,\nand autoregressive models, such as WaveNet, remain the state of the art in generative modelling of audio signals such as human speech. To address this paucity, we introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech.\nOur architecture is composed of a conditional feed-forward generator producing raw speech audio, and an ensemble of discriminators which operate on random windows of different sizes. The discriminators analyse the audio both in terms of general realism, as well as how well the audio corresponds to the utterance that should be pronounced.  To measure the performance of GAN-TTS, we employ both subjective human evaluation (MOS - Mean Opinion Score), as well as novel quantitative metrics (Fr\u00e9chet DeepSpeech Distance and Kernel DeepSpeech Distance), which we find to be well correlated with MOS. We show that GAN-TTS is capable of generating high-fidelity speech with naturalness comparable to the state-of-the-art models, and unlike autoregressive models, it is highly parallelisable thanks to an efficient feed-forward generator. Listen to GAN-TTS reading this abstract at https://storage.googleapis.com/deepmind-media/research/abstract.wav", "keywords": ["texttospeech", "speechsynthesis", "audiosynthesis", "gans", "generativeadversarialnetworks", "implicitgenerativemodels"], "paperhash": "bikowski|high_fidelity_speech_synthesis_with_adversarial_networks", "code": "https://github.com/mbinkowski/DeepSpeechDistances", "_bibtex": "@inproceedings{\nBi\u0144kowski2020High,\ntitle={High Fidelity Speech Synthesis with Adversarial Networks},\nauthor={Miko\u0142aj Bi\u0144kowski and Jeff Donahue and Sander Dieleman and Aidan Clark and Erich Elsen and Norman Casagrande and Luis C. Cobo and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gfQgSFDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a98e874542b617b51a0a7e7c64f880ee43f1d046.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1gfQgSFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2203/Authors", "ICLR.cc/2020/Conference/Paper2203/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2203/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2203/Reviewers", "ICLR.cc/2020/Conference/Paper2203/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2203/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2203/Authors|ICLR.cc/2020/Conference/Paper2203/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144826, "tmdate": 1576860540748, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2203/Authors", "ICLR.cc/2020/Conference/Paper2203/Reviewers", "ICLR.cc/2020/Conference/Paper2203/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2203/-/Official_Comment"}}}, {"id": "S1gKvNctjS", "original": null, "number": 2, "cdate": 1573655648911, "ddate": null, "tcdate": 1573655648911, "tmdate": 1573655648911, "tddate": null, "forum": "r1gfQgSFDr", "replyto": "r1gfQgSFDr", "invitation": "ICLR.cc/2020/Conference/Paper2203/-/Official_Comment", "content": {"title": "Joint response to all Reviewers", "comment": "We would like to thank all reviewers for their effort and their useful comments.\n\nWe have updated our submission, adding several references to related work and pseudocode for training GAN-TTS in Appendix D.\n\n*Stability and Mode Collapse*\nThere are two phenomena in GAN training: (i) mode collapse and (ii) model collapse. The first manifests itself in the lack of sample diversity, the second is essentially training instability.\nIn the paper, we didn't claim that our model doesn't have the former (mode collapse). In fact, for conditional generative models like text-to-speech, mode collapse is not necessarily a problem. Having said that, based on our subjective assessment, feeding different noise z samples leads to slightly different speech samples, so the model does capture some sample diversity given the conditioning.\nWhat we did claim in Section 5.2 is that we didn't observe the second phenomenon (model collapse), i.e. training is stable. We attribute this to data augmentation, both explicit - due to training on random crops, and implicit - through discriminating random windows. The only setting in which we observed model collapse was the one with full-window discriminator; settings with even single random window discriminator, on the other hand, led to stable training."}, "signatures": ["ICLR.cc/2020/Conference/Paper2203/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2203/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mikbinkowski@gmail.com", "jeffdonahue@google.com", "sedielem@google.com", "aidanclark@google.com", "eriche@google.com", "ncasagrande@google.com", "luisca@google.com", "simonyan@google.com"], "title": "High Fidelity Speech Synthesis with Adversarial Networks", "authors": ["Miko\u0142aj Bi\u0144kowski", "Jeff Donahue", "Sander Dieleman", "Aidan Clark", "Erich Elsen", "Norman Casagrande", "Luis C. Cobo", "Karen Simonyan"], "pdf": "/pdf/ab0ccb39f2206a75dadecc6807d39c1079d9e4de.pdf", "TL;DR": "We introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech, which achieves Mean Opinion Score (MOS) 4.2.", "abstract": "Generative adversarial networks have seen rapid development in recent years and have led to remarkable improvements in generative modelling of images. However, their application in the audio domain has received limited attention,\nand autoregressive models, such as WaveNet, remain the state of the art in generative modelling of audio signals such as human speech. To address this paucity, we introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech.\nOur architecture is composed of a conditional feed-forward generator producing raw speech audio, and an ensemble of discriminators which operate on random windows of different sizes. The discriminators analyse the audio both in terms of general realism, as well as how well the audio corresponds to the utterance that should be pronounced.  To measure the performance of GAN-TTS, we employ both subjective human evaluation (MOS - Mean Opinion Score), as well as novel quantitative metrics (Fr\u00e9chet DeepSpeech Distance and Kernel DeepSpeech Distance), which we find to be well correlated with MOS. We show that GAN-TTS is capable of generating high-fidelity speech with naturalness comparable to the state-of-the-art models, and unlike autoregressive models, it is highly parallelisable thanks to an efficient feed-forward generator. Listen to GAN-TTS reading this abstract at https://storage.googleapis.com/deepmind-media/research/abstract.wav", "keywords": ["texttospeech", "speechsynthesis", "audiosynthesis", "gans", "generativeadversarialnetworks", "implicitgenerativemodels"], "paperhash": "bikowski|high_fidelity_speech_synthesis_with_adversarial_networks", "code": "https://github.com/mbinkowski/DeepSpeechDistances", "_bibtex": "@inproceedings{\nBi\u0144kowski2020High,\ntitle={High Fidelity Speech Synthesis with Adversarial Networks},\nauthor={Miko\u0142aj Bi\u0144kowski and Jeff Donahue and Sander Dieleman and Aidan Clark and Erich Elsen and Norman Casagrande and Luis C. Cobo and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gfQgSFDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a98e874542b617b51a0a7e7c64f880ee43f1d046.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1gfQgSFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2203/Authors", "ICLR.cc/2020/Conference/Paper2203/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2203/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2203/Reviewers", "ICLR.cc/2020/Conference/Paper2203/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2203/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2203/Authors|ICLR.cc/2020/Conference/Paper2203/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144826, "tmdate": 1576860540748, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2203/Authors", "ICLR.cc/2020/Conference/Paper2203/Reviewers", "ICLR.cc/2020/Conference/Paper2203/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2203/-/Official_Comment"}}}, {"id": "BJlJ-YsaKS", "original": null, "number": 2, "cdate": 1571825910939, "ddate": null, "tcdate": 1571825910939, "tmdate": 1572972369616, "tddate": null, "forum": "r1gfQgSFDr", "replyto": "r1gfQgSFDr", "invitation": "ICLR.cc/2020/Conference/Paper2203/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes to enable GAN based TTS in the time domain with the careful designs of the (non-autoregressive) generator and discriminator. There have been various trials of GAN-TTS but not so many success and I'm glad to hear that the proposed method seems to enable GAN-TTS with fast inference thanks to the non-autoregressive property. The method also proposes new objective measures inspired by the image recognition network based on the high-level features generated by end-to-end ASR, which is also another important contribution of this paper. \n\nMy concern for this paper is reproducibility. Although I really appreciate the authors' efforts on providing implementational details in the appendix, the code and data do not seem to be publicly available, and I'm expecting that the implementation of this technique is relatively hard due to their complex designs of the generator and discriminator. Apart from that, the paper is well written overall by well describing the trend of GAN studies in the image processing and the application of such image processing oriented GAN techniques to TTS."}, "signatures": ["ICLR.cc/2020/Conference/Paper2203/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2203/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mikbinkowski@gmail.com", "jeffdonahue@google.com", "sedielem@google.com", "aidanclark@google.com", "eriche@google.com", "ncasagrande@google.com", "luisca@google.com", "simonyan@google.com"], "title": "High Fidelity Speech Synthesis with Adversarial Networks", "authors": ["Miko\u0142aj Bi\u0144kowski", "Jeff Donahue", "Sander Dieleman", "Aidan Clark", "Erich Elsen", "Norman Casagrande", "Luis C. Cobo", "Karen Simonyan"], "pdf": "/pdf/ab0ccb39f2206a75dadecc6807d39c1079d9e4de.pdf", "TL;DR": "We introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech, which achieves Mean Opinion Score (MOS) 4.2.", "abstract": "Generative adversarial networks have seen rapid development in recent years and have led to remarkable improvements in generative modelling of images. However, their application in the audio domain has received limited attention,\nand autoregressive models, such as WaveNet, remain the state of the art in generative modelling of audio signals such as human speech. To address this paucity, we introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech.\nOur architecture is composed of a conditional feed-forward generator producing raw speech audio, and an ensemble of discriminators which operate on random windows of different sizes. The discriminators analyse the audio both in terms of general realism, as well as how well the audio corresponds to the utterance that should be pronounced.  To measure the performance of GAN-TTS, we employ both subjective human evaluation (MOS - Mean Opinion Score), as well as novel quantitative metrics (Fr\u00e9chet DeepSpeech Distance and Kernel DeepSpeech Distance), which we find to be well correlated with MOS. We show that GAN-TTS is capable of generating high-fidelity speech with naturalness comparable to the state-of-the-art models, and unlike autoregressive models, it is highly parallelisable thanks to an efficient feed-forward generator. Listen to GAN-TTS reading this abstract at https://storage.googleapis.com/deepmind-media/research/abstract.wav", "keywords": ["texttospeech", "speechsynthesis", "audiosynthesis", "gans", "generativeadversarialnetworks", "implicitgenerativemodels"], "paperhash": "bikowski|high_fidelity_speech_synthesis_with_adversarial_networks", "code": "https://github.com/mbinkowski/DeepSpeechDistances", "_bibtex": "@inproceedings{\nBi\u0144kowski2020High,\ntitle={High Fidelity Speech Synthesis with Adversarial Networks},\nauthor={Miko\u0142aj Bi\u0144kowski and Jeff Donahue and Sander Dieleman and Aidan Clark and Erich Elsen and Norman Casagrande and Luis C. Cobo and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gfQgSFDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a98e874542b617b51a0a7e7c64f880ee43f1d046.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1gfQgSFDr", "replyto": "r1gfQgSFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2203/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2203/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575306292995, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2203/Reviewers"], "noninvitees": [], "tcdate": 1570237726227, "tmdate": 1575306293008, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2203/-/Official_Review"}}}, {"id": "Hyg3Zr60FS", "original": null, "number": 3, "cdate": 1571898627686, "ddate": null, "tcdate": 1571898627686, "tmdate": 1572972369571, "tddate": null, "forum": "r1gfQgSFDr", "replyto": "r1gfQgSFDr", "invitation": "ICLR.cc/2020/Conference/Paper2203/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper puts forth adversarial architectures for TTS. Currently, there aren't many examples (e.g. Donahue et al,  Engel et al. referenced in paper) of GANs being used successfully in TTS, so this papers in this area are significant. \n\nThe architectures proposed are convolutional (in the manner of Yu and Koltun), with increasing receptive field sizes taking into account the long term dependency structure inherent in speech signals. The input to the generator are linguistic and pitch signals - extracted externally, and noise. In that sense, we are working with a conditional GAN. \n\nI found the discriminator design very interesting. As the comment below notes, it is a sort of patch GAN discriminator (See pix2pix, and this comment from Philip Isola - https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/39) and that is could be quite significant in that it classifies at different scales. In the image world, having a single discriminator for the whole model would not take into account local structure of the images. Likewise, perhaps we can imagine something similar in the case of audio at varying scales - in fact, audio dependencies are even more long range. That might be one reason why the variable window sizes work here. \n\nThe paper also presents to image analogues for metrics based on FID and the KID, with the features being taken from DeepSpeech2. \n\nI found the speech sample presented very convincing. In general, the architectures are also presented quite clearly, so it seems that we might be able to reproduce these experiments in our own practice. It is also promising that producing good speech could be achieved by a non-autoregressive or attention based architecture.\n\nThe authors mention that they hardly encounter any issues with training stability and mode collapse. Is that because of the design of the multiple discriminator architecture?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2203/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2203/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mikbinkowski@gmail.com", "jeffdonahue@google.com", "sedielem@google.com", "aidanclark@google.com", "eriche@google.com", "ncasagrande@google.com", "luisca@google.com", "simonyan@google.com"], "title": "High Fidelity Speech Synthesis with Adversarial Networks", "authors": ["Miko\u0142aj Bi\u0144kowski", "Jeff Donahue", "Sander Dieleman", "Aidan Clark", "Erich Elsen", "Norman Casagrande", "Luis C. Cobo", "Karen Simonyan"], "pdf": "/pdf/ab0ccb39f2206a75dadecc6807d39c1079d9e4de.pdf", "TL;DR": "We introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech, which achieves Mean Opinion Score (MOS) 4.2.", "abstract": "Generative adversarial networks have seen rapid development in recent years and have led to remarkable improvements in generative modelling of images. However, their application in the audio domain has received limited attention,\nand autoregressive models, such as WaveNet, remain the state of the art in generative modelling of audio signals such as human speech. To address this paucity, we introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech.\nOur architecture is composed of a conditional feed-forward generator producing raw speech audio, and an ensemble of discriminators which operate on random windows of different sizes. The discriminators analyse the audio both in terms of general realism, as well as how well the audio corresponds to the utterance that should be pronounced.  To measure the performance of GAN-TTS, we employ both subjective human evaluation (MOS - Mean Opinion Score), as well as novel quantitative metrics (Fr\u00e9chet DeepSpeech Distance and Kernel DeepSpeech Distance), which we find to be well correlated with MOS. We show that GAN-TTS is capable of generating high-fidelity speech with naturalness comparable to the state-of-the-art models, and unlike autoregressive models, it is highly parallelisable thanks to an efficient feed-forward generator. Listen to GAN-TTS reading this abstract at https://storage.googleapis.com/deepmind-media/research/abstract.wav", "keywords": ["texttospeech", "speechsynthesis", "audiosynthesis", "gans", "generativeadversarialnetworks", "implicitgenerativemodels"], "paperhash": "bikowski|high_fidelity_speech_synthesis_with_adversarial_networks", "code": "https://github.com/mbinkowski/DeepSpeechDistances", "_bibtex": "@inproceedings{\nBi\u0144kowski2020High,\ntitle={High Fidelity Speech Synthesis with Adversarial Networks},\nauthor={Miko\u0142aj Bi\u0144kowski and Jeff Donahue and Sander Dieleman and Aidan Clark and Erich Elsen and Norman Casagrande and Luis C. Cobo and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gfQgSFDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a98e874542b617b51a0a7e7c64f880ee43f1d046.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1gfQgSFDr", "replyto": "r1gfQgSFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2203/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2203/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575306292995, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2203/Reviewers"], "noninvitees": [], "tcdate": 1570237726227, "tmdate": 1575306293008, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2203/-/Official_Review"}}}, {"id": "Hygn03xGFB", "original": null, "number": 1, "cdate": 1571060948364, "ddate": null, "tcdate": 1571060948364, "tmdate": 1571060948364, "tddate": null, "forum": "r1gfQgSFDr", "replyto": "rkxBtBvCuB", "invitation": "ICLR.cc/2020/Conference/Paper2203/-/Official_Comment", "content": {"comment": "Thank you for sharing your related work. As it was made public after our submission and will be published only in the near future, it cannot be considered prior work. We are looking forward to reading the camera-ready version of your paper, and will include a discussion of similarities and differences in a future version of our paper.", "title": "Thanks for reference to parallel work"}, "signatures": ["ICLR.cc/2020/Conference/Paper2203/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2203/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mikbinkowski@gmail.com", "jeffdonahue@google.com", "sedielem@google.com", "aidanclark@google.com", "eriche@google.com", "ncasagrande@google.com", "luisca@google.com", "simonyan@google.com"], "title": "High Fidelity Speech Synthesis with Adversarial Networks", "authors": ["Miko\u0142aj Bi\u0144kowski", "Jeff Donahue", "Sander Dieleman", "Aidan Clark", "Erich Elsen", "Norman Casagrande", "Luis C. Cobo", "Karen Simonyan"], "pdf": "/pdf/ab0ccb39f2206a75dadecc6807d39c1079d9e4de.pdf", "TL;DR": "We introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech, which achieves Mean Opinion Score (MOS) 4.2.", "abstract": "Generative adversarial networks have seen rapid development in recent years and have led to remarkable improvements in generative modelling of images. However, their application in the audio domain has received limited attention,\nand autoregressive models, such as WaveNet, remain the state of the art in generative modelling of audio signals such as human speech. To address this paucity, we introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech.\nOur architecture is composed of a conditional feed-forward generator producing raw speech audio, and an ensemble of discriminators which operate on random windows of different sizes. The discriminators analyse the audio both in terms of general realism, as well as how well the audio corresponds to the utterance that should be pronounced.  To measure the performance of GAN-TTS, we employ both subjective human evaluation (MOS - Mean Opinion Score), as well as novel quantitative metrics (Fr\u00e9chet DeepSpeech Distance and Kernel DeepSpeech Distance), which we find to be well correlated with MOS. We show that GAN-TTS is capable of generating high-fidelity speech with naturalness comparable to the state-of-the-art models, and unlike autoregressive models, it is highly parallelisable thanks to an efficient feed-forward generator. Listen to GAN-TTS reading this abstract at https://storage.googleapis.com/deepmind-media/research/abstract.wav", "keywords": ["texttospeech", "speechsynthesis", "audiosynthesis", "gans", "generativeadversarialnetworks", "implicitgenerativemodels"], "paperhash": "bikowski|high_fidelity_speech_synthesis_with_adversarial_networks", "code": "https://github.com/mbinkowski/DeepSpeechDistances", "_bibtex": "@inproceedings{\nBi\u0144kowski2020High,\ntitle={High Fidelity Speech Synthesis with Adversarial Networks},\nauthor={Miko\u0142aj Bi\u0144kowski and Jeff Donahue and Sander Dieleman and Aidan Clark and Erich Elsen and Norman Casagrande and Luis C. Cobo and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gfQgSFDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a98e874542b617b51a0a7e7c64f880ee43f1d046.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1gfQgSFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2203/Authors", "ICLR.cc/2020/Conference/Paper2203/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2203/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2203/Reviewers", "ICLR.cc/2020/Conference/Paper2203/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2203/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2203/Authors|ICLR.cc/2020/Conference/Paper2203/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144826, "tmdate": 1576860540748, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2203/Authors", "ICLR.cc/2020/Conference/Paper2203/Reviewers", "ICLR.cc/2020/Conference/Paper2203/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2203/-/Official_Comment"}}}, {"id": "rkxBtBvCuB", "original": null, "number": 1, "cdate": 1570825596753, "ddate": null, "tcdate": 1570825596753, "tmdate": 1570828093715, "tddate": null, "forum": "r1gfQgSFDr", "replyto": "r1gfQgSFDr", "invitation": "ICLR.cc/2020/Conference/Paper2203/-/Public_Comment", "content": {"comment": "We would like to point out that our research paper - MelGAN: Conditional Generative Adversarial Networks for Conditional Waveform Synthesis (accepted as poster presentation at NeurIPS 2019) also performs raw audio generation using generative adversarial networks. Our paper primarily targets the problem of mel spectrogram inversion using Conditional GANs and also show that alternate representations such as VQ-VAE latents, Universal Music Translator encodings can be utilized to generate corresponding raw waveform.\n\nMelGAN and the current paper under review (GAN-TTS) have many similarities in their approach. Specifically, both papers use highly similar Generator architectures (residual blocks, dilated convolutions, pattern of upsampling the conditioning information) and Discriminator architectures (multi-scale discriminator and multiple discriminators, patch-discriminator vs random window samping). The difference occurs in the task, where the GAN-TTS model uses text features instead of mel-spectrograms to perform raw audio generation.\n\nWe acknowledge that the authors couldn\u2019t have known this paper since it wasn\u2019t public. It would be nice if the authors could summarize and discuss the additional insights provided by this paper in the light of the existence of this prior work.\n\nWe temporarily share the paper using a google drive link, pending arxiv submission. (https://drive.google.com/file/d/1a_CnqAMkFYEC7pfAkBKvjMeaKiREKPkl/view?usp=sharing)\n\nThe final camera ready version of the paper will be available later this month (October 30).", "title": "Prior work for raw audio generation using Conditional GANs"}, "signatures": ["~Rithesh_Kumar1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Rithesh_Kumar1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mikbinkowski@gmail.com", "jeffdonahue@google.com", "sedielem@google.com", "aidanclark@google.com", "eriche@google.com", "ncasagrande@google.com", "luisca@google.com", "simonyan@google.com"], "title": "High Fidelity Speech Synthesis with Adversarial Networks", "authors": ["Miko\u0142aj Bi\u0144kowski", "Jeff Donahue", "Sander Dieleman", "Aidan Clark", "Erich Elsen", "Norman Casagrande", "Luis C. Cobo", "Karen Simonyan"], "pdf": "/pdf/ab0ccb39f2206a75dadecc6807d39c1079d9e4de.pdf", "TL;DR": "We introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech, which achieves Mean Opinion Score (MOS) 4.2.", "abstract": "Generative adversarial networks have seen rapid development in recent years and have led to remarkable improvements in generative modelling of images. However, their application in the audio domain has received limited attention,\nand autoregressive models, such as WaveNet, remain the state of the art in generative modelling of audio signals such as human speech. To address this paucity, we introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech.\nOur architecture is composed of a conditional feed-forward generator producing raw speech audio, and an ensemble of discriminators which operate on random windows of different sizes. The discriminators analyse the audio both in terms of general realism, as well as how well the audio corresponds to the utterance that should be pronounced.  To measure the performance of GAN-TTS, we employ both subjective human evaluation (MOS - Mean Opinion Score), as well as novel quantitative metrics (Fr\u00e9chet DeepSpeech Distance and Kernel DeepSpeech Distance), which we find to be well correlated with MOS. We show that GAN-TTS is capable of generating high-fidelity speech with naturalness comparable to the state-of-the-art models, and unlike autoregressive models, it is highly parallelisable thanks to an efficient feed-forward generator. Listen to GAN-TTS reading this abstract at https://storage.googleapis.com/deepmind-media/research/abstract.wav", "keywords": ["texttospeech", "speechsynthesis", "audiosynthesis", "gans", "generativeadversarialnetworks", "implicitgenerativemodels"], "paperhash": "bikowski|high_fidelity_speech_synthesis_with_adversarial_networks", "code": "https://github.com/mbinkowski/DeepSpeechDistances", "_bibtex": "@inproceedings{\nBi\u0144kowski2020High,\ntitle={High Fidelity Speech Synthesis with Adversarial Networks},\nauthor={Miko\u0142aj Bi\u0144kowski and Jeff Donahue and Sander Dieleman and Aidan Clark and Erich Elsen and Norman Casagrande and Luis C. Cobo and Karen Simonyan},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gfQgSFDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a98e874542b617b51a0a7e7c64f880ee43f1d046.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1gfQgSFDr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504183657, "tmdate": 1576860574266, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2203/Authors", "ICLR.cc/2020/Conference/Paper2203/Reviewers", "ICLR.cc/2020/Conference/Paper2203/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2203/-/Public_Comment"}}}], "count": 12}