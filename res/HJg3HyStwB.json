{"notes": [{"id": "HJg3HyStwB", "original": "Byl9mX6uPB", "number": 1706, "cdate": 1569439555812, "ddate": null, "tcdate": 1569439555812, "tmdate": 1577168243667, "tddate": null, "forum": "HJg3HyStwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions", "authors": ["He Zhao", "Trung Le", "Paul Montague", "Olivier De Vel", "Tamas Abraham", "Dinh Phung"], "authorids": ["ethanhezhao@gmail.com", "trunglm@monash.edu", "paul.montague@dst.defence.gov.au", "olivier.devel@dst.defence.gov.au", "tamas.abraham@dst.defence.gov.au", "dinh.phung@monash.edu"], "keywords": [], "TL;DR": "A new adversarial attack for images with both perturbations and spatial distortions", "abstract": "Deep neural network image classifiers are reported to be susceptible to adversarial evasion attacks, which use carefully crafted images created to mislead a classifier. Recently, various kinds of adversarial attack methods have been proposed, most of which focus on adding small perturbations to input images. Despite the success of existing approaches, the way to generate realistic adversarial images with small perturbations remains a challenging problem. In this paper, we aim to address this problem by proposing a novel adversarial method, which generates adversarial examples by imposing not only perturbations but also spatial distortions on input images, including scaling, rotation, shear, and translation. As humans are less susceptible to small spatial distortions, the proposed approach can produce visually more realistic attacks with smaller perturbations, able to deceive classifiers without affecting human predictions. We learn our method by amortized techniques with neural networks and generate adversarial examples efficiently by a forward pass of the networks. Extensive experiments on attacking different types of non-robustified classifiers and robust classifiers with defence show that our method has state-of-the-art performance in comparison with advanced attack parallels.", "pdf": "/pdf/170cfc3bc6beae2308e16b2690cba26fcd11f984.pdf", "paperhash": "zhao|perturbations_are_not_enough_generating_adversarial_examples_with_spatial_distortions", "original_pdf": "/attachment/170cfc3bc6beae2308e16b2690cba26fcd11f984.pdf", "_bibtex": "@misc{\nzhao2020perturbations,\ntitle={Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions},\nauthor={He Zhao and Trung Le and Paul Montague and Olivier De Vel and Tamas Abraham and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJg3HyStwB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "pxupoAD8W", "original": null, "number": 1, "cdate": 1576798730405, "ddate": null, "tcdate": 1576798730405, "tmdate": 1576800906111, "tddate": null, "forum": "HJg3HyStwB", "replyto": "HJg3HyStwB", "invitation": "ICLR.cc/2020/Conference/Paper1706/-/Decision", "content": {"decision": "Reject", "comment": "The method proposed and explored here is to introduce small spatial distortions, with the goal of making them undetectable by humans but affecting the classification of the images. As reviewers point out, very similar methods have been tested before. The methods are also only tested on a few low-resolution datasets. \n\nThe reviewers are unanimous in their judgement that the method is not novel enough, and the authors' rebuttals have not convinced the reviewers or me about the opposite.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions", "authors": ["He Zhao", "Trung Le", "Paul Montague", "Olivier De Vel", "Tamas Abraham", "Dinh Phung"], "authorids": ["ethanhezhao@gmail.com", "trunglm@monash.edu", "paul.montague@dst.defence.gov.au", "olivier.devel@dst.defence.gov.au", "tamas.abraham@dst.defence.gov.au", "dinh.phung@monash.edu"], "keywords": [], "TL;DR": "A new adversarial attack for images with both perturbations and spatial distortions", "abstract": "Deep neural network image classifiers are reported to be susceptible to adversarial evasion attacks, which use carefully crafted images created to mislead a classifier. Recently, various kinds of adversarial attack methods have been proposed, most of which focus on adding small perturbations to input images. Despite the success of existing approaches, the way to generate realistic adversarial images with small perturbations remains a challenging problem. In this paper, we aim to address this problem by proposing a novel adversarial method, which generates adversarial examples by imposing not only perturbations but also spatial distortions on input images, including scaling, rotation, shear, and translation. As humans are less susceptible to small spatial distortions, the proposed approach can produce visually more realistic attacks with smaller perturbations, able to deceive classifiers without affecting human predictions. We learn our method by amortized techniques with neural networks and generate adversarial examples efficiently by a forward pass of the networks. Extensive experiments on attacking different types of non-robustified classifiers and robust classifiers with defence show that our method has state-of-the-art performance in comparison with advanced attack parallels.", "pdf": "/pdf/170cfc3bc6beae2308e16b2690cba26fcd11f984.pdf", "paperhash": "zhao|perturbations_are_not_enough_generating_adversarial_examples_with_spatial_distortions", "original_pdf": "/attachment/170cfc3bc6beae2308e16b2690cba26fcd11f984.pdf", "_bibtex": "@misc{\nzhao2020perturbations,\ntitle={Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions},\nauthor={He Zhao and Trung Le and Paul Montague and Olivier De Vel and Tamas Abraham and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJg3HyStwB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJg3HyStwB", "replyto": "HJg3HyStwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795726386, "tmdate": 1576800278511, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1706/-/Decision"}}}, {"id": "HklsdMjcor", "original": null, "number": 5, "cdate": 1573724787236, "ddate": null, "tcdate": 1573724787236, "tmdate": 1573724787236, "tddate": null, "forum": "HJg3HyStwB", "replyto": "r1xnXDVnFH", "invitation": "ICLR.cc/2020/Conference/Paper1706/-/Official_Comment", "content": {"title": "Many thanks for your precious time and valuable comments.", "comment": "Our response is as follows:\n\n1. Please allow us to re-emphasize the main novelty of our method: We focus on generating adversarial examples that look realistic to humans but also attack the classifier well; We achieve this goal by proposing a generator that conducts both spatial distortions and perturbations. Importantly, the proposed generator is fully differentiable so that we can train it to generate spatial distortions and perturbations jointly. In the joint process, spatial distortions and perturbations are \u201caware of\u201d each other and \u201cwork collaboratively\u201d, so that we are able to use small spatial distortions plus small perturbations to achieve better attack performance.\n\n2. We are conducting experiments on CIFAR and CelebA. We will try our best to report the results in the rebuttal. If the experiments cannot be concluded by the rebuttal deadline, we will report them in the revised paper.\n\n3. We have conducted the experiments of adversarial training + PGD, i.e.,  Adv-Train-PGD. The performance results are shown in the following table. It can be observed that Adv-Train-PGD defends well against perturbation-based methods but is less effective than our proposed approaches. \n\n\n+---------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+\n|               |         MNIST, Model A         |         MNIST, Model B         |     Fashion MNIST, Model A     |     Fashion MNIST, Model B     |\n+---------------+----------------+---------------+----------------+---------------+----------------+---------------+----------------+---------------+\n|               | Adv-Train-FGSM | Adv-Train-PGD | Adv-Train-FGSM | Adv-Train-PGD | Adv-Train-FGSM | Adv-Train-PGD | Adv-Train-FGSM | Adv-Train-PGD |\n+---------------+----------------+---------------+----------------+---------------+----------------+---------------+----------------+---------------+\n|   No attack   |     0.9916     |     0.9915    |     0.9757     |     0.9830    |     0.9057     |     0.9060    |     0.8869     |     0.8854    |\n+---------------+----------------+---------------+----------------+---------------+----------------+---------------+----------------+---------------+\n|      STM      |     0.9481     |     0.4241    |     0.1200     |     0.0413    |     0.1296     |     0.1323    |     0.1112     |     0.1132    |\n+---------------+----------------+---------------+----------------+---------------+----------------+---------------+----------------+---------------+\n|  SdAdv (ours) |      0.074     |     0.0631    |     0.0744     |      0.08     |     0.1502     |     0.1049    |     0.1420     |     0.1850    |\n+---------------+----------------+---------------+----------------+---------------+----------------+---------------+----------------+---------------+\n|      FGSM     |     0.9481     |     0.9710    |     0.8753     |     0.8189    |     0.8838     |     0.7499    |     0.8558     |     0.6076    |\n+---------------+----------------+---------------+----------------+---------------+----------------+---------------+----------------+---------------+\n|      PGD      |     0.0926     |     0.9427    |     0.0147     |     0.7419    |     0.0764     |     0.6619    |     0.0431     |     0.5140    |\n+---------------+----------------+---------------+----------------+---------------+----------------+---------------+----------------+---------------+\n|      MIM      |     0.1584     |     0.9358    |     0.0373     |     0.7201    |     0.1054     |     0.5987    |     0.0515     |     0.4401    |\n+---------------+----------------+---------------+----------------+---------------+----------------+---------------+----------------+---------------+\n|     AdvGAN    |     0.9278     |     0.9906    |     0.2868     |     0.8680    |     0.1854     |     0.3762    |     0.1015     |     0.1387    |\n+---------------+----------------+---------------+----------------+---------------+----------------+---------------+----------------+---------------+\n| SdpAdv (ours) |      0.033     |     0.0752    |     0.0741     |     0.092     |     0.0444     |     0.1113    |     0.0392     |     0.1081    |\n+---------------+----------------+---------------+----------------+---------------+----------------+---------------+----------------+---------------+ "}, "signatures": ["ICLR.cc/2020/Conference/Paper1706/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1706/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions", "authors": ["He Zhao", "Trung Le", "Paul Montague", "Olivier De Vel", "Tamas Abraham", "Dinh Phung"], "authorids": ["ethanhezhao@gmail.com", "trunglm@monash.edu", "paul.montague@dst.defence.gov.au", "olivier.devel@dst.defence.gov.au", "tamas.abraham@dst.defence.gov.au", "dinh.phung@monash.edu"], "keywords": [], "TL;DR": "A new adversarial attack for images with both perturbations and spatial distortions", "abstract": "Deep neural network image classifiers are reported to be susceptible to adversarial evasion attacks, which use carefully crafted images created to mislead a classifier. Recently, various kinds of adversarial attack methods have been proposed, most of which focus on adding small perturbations to input images. Despite the success of existing approaches, the way to generate realistic adversarial images with small perturbations remains a challenging problem. In this paper, we aim to address this problem by proposing a novel adversarial method, which generates adversarial examples by imposing not only perturbations but also spatial distortions on input images, including scaling, rotation, shear, and translation. As humans are less susceptible to small spatial distortions, the proposed approach can produce visually more realistic attacks with smaller perturbations, able to deceive classifiers without affecting human predictions. We learn our method by amortized techniques with neural networks and generate adversarial examples efficiently by a forward pass of the networks. Extensive experiments on attacking different types of non-robustified classifiers and robust classifiers with defence show that our method has state-of-the-art performance in comparison with advanced attack parallels.", "pdf": "/pdf/170cfc3bc6beae2308e16b2690cba26fcd11f984.pdf", "paperhash": "zhao|perturbations_are_not_enough_generating_adversarial_examples_with_spatial_distortions", "original_pdf": "/attachment/170cfc3bc6beae2308e16b2690cba26fcd11f984.pdf", "_bibtex": "@misc{\nzhao2020perturbations,\ntitle={Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions},\nauthor={He Zhao and Trung Le and Paul Montague and Olivier De Vel and Tamas Abraham and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJg3HyStwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJg3HyStwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1706/Authors", "ICLR.cc/2020/Conference/Paper1706/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1706/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1706/Reviewers", "ICLR.cc/2020/Conference/Paper1706/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1706/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1706/Authors|ICLR.cc/2020/Conference/Paper1706/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152068, "tmdate": 1576860549724, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1706/Authors", "ICLR.cc/2020/Conference/Paper1706/Reviewers", "ICLR.cc/2020/Conference/Paper1706/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1706/-/Official_Comment"}}}, {"id": "rJxLyGs9sH", "original": null, "number": 4, "cdate": 1573724638373, "ddate": null, "tcdate": 1573724638373, "tmdate": 1573724638373, "tddate": null, "forum": "HJg3HyStwB", "replyto": "r1e3Y-QTFH", "invitation": "ICLR.cc/2020/Conference/Paper1706/-/Official_Comment", "content": {"title": "Many thanks for your precious time and valuable comments.", "comment": "Our responses are as follows:\n\n1. Novelty:\n\n\ta. Please note that we did not claim the idea of \u201cintroducing spatial distortions into adversarial attacks\u201d as our innovation. Instead, we have proposed a new approach with spatial distortions, which is better than existing spatial distortion based attacks.\n\n\tb. We are aware of the existing study in spatial distortion based attacks; the most related ones to ours are discussed in Sections 2.2 and 3.5 in our paper; more importantly, we have compared our approach to this in our experiments.\n\n\tc. The novelties of our paper compared with others are discussed in Sections 3.5 and please allow us to re-emphasize the main novelty of our method: We focus on generating adversarial examples that look realistic to humans but also attack the classifier well; We achieve this goal by proposing a generator that conducts both spatial distortions and perturbations; Importantly, the proposed generator is fully differentiable so that we can train it to generate spatial distortions and perturbations jointly; In the joint process, spatial distortions and perturbations are \u201caware of\u201d each other and \u201cwork collaboratively\u201d, so that we are able to use small spatial distortions plus small perturbations to achieve better attack performance.\n\n\td. We undertook a comprehensive search of the related literature of spatial distortion based attacks. However, we may have missed some papers. It would be great if the reviewer could point out any additional related papers to help us improve our paper. We are more than happy to discuss and compare these papers with our paper.\n\n2. \u201cMisleading\u201d: \n\nIt is unfortunate that the reviewer misunderstood our main claims. We did not make any claims that \u201cwe are the first to use spatial distortions in adversarial attacks.\u201d On the contrary, we have compared the recent advances in spatial distortions in detail by means of both discussions (see Sections 2.2 and 3.5) and experiments. As the writing style and presentation order can be very subjective, we respectfully disagree on judging our paper by using them as major points.\n\n3. Theory:\n\nWe will remove the theory as suggested.\n\n4. Experiments:\n\n\ta. \\gamma controls the magnitude of the spatial distortion. We have conducted experiments using varying \\gamma values for the proposed methods, with performance results shown in the following table. Please also note that we have released the code to the community so as to reproduce our results. It can be seen that SdAdv is relatively sensitive to \\gamma as it only uses spatial distortions. However, SdpAdv is less sensitive to \\gamma as perturbations \u201care aware of\u201d spatial distortions and will help attack.\n\n+--------+--------+--------------------+--------+--------+\n| \\gamma |   0.1  | 0.3 (in the paper) |   0.5  |   1.0  |\n+--------+--------+--------------------+--------+--------+\n|             MNIST, Model A, Non-robustified            |\n+--------+--------+--------------------+--------+--------+\n|  SdAdv | 0.7446 |       0.0517       | 0.0352 | 0.0417 |\n+--------+--------+--------------------+--------+--------+\n| SdpAdv |  0.034 |       0.0204       | 0.0131 | 0.0130 |\n+--------+--------+--------------------+--------+--------+\n|             MNIST, Model B, Non-robustified            |\n+--------+--------+--------------------+--------+--------+\n|  SdAdv | 0.3892 |       0.0502       | 0.0501 | 0.0477 |\n+--------+--------+--------------------+--------+--------+\n| SdpAdv | 0.0117 |       0.0233       | 0.0223 | 0.0222 |\n+--------+--------+--------------------+--------+--------+\n|         Fashion MNIST, Model A, Non-robustified        |\n+--------+--------+--------------------+--------+--------+\n|  SdAdv | 0.4213 |       0.1762       | 0.1369 | 0.1285 |\n+--------+--------+--------------------+--------+--------+\n| SdpAdv | 0.0221 |       0.0121       | 0.0225 | 0.0224 |\n+--------+--------+--------------------+--------+--------+\n|         Fashion MNIST, Model B, Non-robustified        |\n+--------+--------+--------------------+--------+--------+\n|  SdAdv | 0.2976 |       0.1079       | 0.1299 | 0.1210 |\n+--------+--------+--------------------+--------+--------+\n| SdpAdv | 0.0158 |       0.0075       | 0.0120 | 0.0125 |\n+--------+--------+--------------------+--------+--------+\n\n\n\tb. We have conducted experiments with adversarial training + PGD i.e., Adv-Train-PGD. Due to the character limits of our response, please find the results in our response to Reviewer 3.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1706/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1706/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions", "authors": ["He Zhao", "Trung Le", "Paul Montague", "Olivier De Vel", "Tamas Abraham", "Dinh Phung"], "authorids": ["ethanhezhao@gmail.com", "trunglm@monash.edu", "paul.montague@dst.defence.gov.au", "olivier.devel@dst.defence.gov.au", "tamas.abraham@dst.defence.gov.au", "dinh.phung@monash.edu"], "keywords": [], "TL;DR": "A new adversarial attack for images with both perturbations and spatial distortions", "abstract": "Deep neural network image classifiers are reported to be susceptible to adversarial evasion attacks, which use carefully crafted images created to mislead a classifier. Recently, various kinds of adversarial attack methods have been proposed, most of which focus on adding small perturbations to input images. Despite the success of existing approaches, the way to generate realistic adversarial images with small perturbations remains a challenging problem. In this paper, we aim to address this problem by proposing a novel adversarial method, which generates adversarial examples by imposing not only perturbations but also spatial distortions on input images, including scaling, rotation, shear, and translation. As humans are less susceptible to small spatial distortions, the proposed approach can produce visually more realistic attacks with smaller perturbations, able to deceive classifiers without affecting human predictions. We learn our method by amortized techniques with neural networks and generate adversarial examples efficiently by a forward pass of the networks. Extensive experiments on attacking different types of non-robustified classifiers and robust classifiers with defence show that our method has state-of-the-art performance in comparison with advanced attack parallels.", "pdf": "/pdf/170cfc3bc6beae2308e16b2690cba26fcd11f984.pdf", "paperhash": "zhao|perturbations_are_not_enough_generating_adversarial_examples_with_spatial_distortions", "original_pdf": "/attachment/170cfc3bc6beae2308e16b2690cba26fcd11f984.pdf", "_bibtex": "@misc{\nzhao2020perturbations,\ntitle={Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions},\nauthor={He Zhao and Trung Le and Paul Montague and Olivier De Vel and Tamas Abraham and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJg3HyStwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJg3HyStwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1706/Authors", "ICLR.cc/2020/Conference/Paper1706/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1706/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1706/Reviewers", "ICLR.cc/2020/Conference/Paper1706/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1706/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1706/Authors|ICLR.cc/2020/Conference/Paper1706/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152068, "tmdate": 1576860549724, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1706/Authors", "ICLR.cc/2020/Conference/Paper1706/Reviewers", "ICLR.cc/2020/Conference/Paper1706/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1706/-/Official_Comment"}}}, {"id": "SylTVgs5sr", "original": null, "number": 3, "cdate": 1573724213400, "ddate": null, "tcdate": 1573724213400, "tmdate": 1573724213400, "tddate": null, "forum": "HJg3HyStwB", "replyto": "BkgXRYNaYB", "invitation": "ICLR.cc/2020/Conference/Paper1706/-/Official_Comment", "content": {"title": "Many thanks for your precious time and valuable comments.", "comment": "Our responses are as follows:\n\n1. Novelty and related work:\n\n\ta. Our proposed attack falls into the category of \u201cspatial distortion based attacks\u201d, which also belongs to a more general research line of \u201cadversarial attacks for images with visually meaningful transformations\u201d (e.g.  \u201ccolour-shifting\u201d in \u201csemantic adversarial examples\u201d). Please note that we focus on the category of \u201cspatial distortion based attacks.'' In this category, we believe that our approach has significant novelties and advantages over others in the same category in both effectiveness and efficiency. This is discussed in Sections 2.2 and 3.5 of our paper.\n\n\tb. Please allow us to re-emphasize the main novelty of our method: We focus on generating adversarial examples that look realistic to humans but also attack the classifier well; We achieve this goal by proposing a generator that conducts both spatial distortions and perturbations; Importantly, the proposed generator is fully differentiable so that we can train it to generate spatial distortions and perturbations jointly; In the joint process, spatial distortions and perturbations are \u201caware of\u201d each other and \u201cwork collaboratively\u201d, so that we are able to use small spatial distortions plus small perturbations to achieve better attack performance.\n\n\tc. Thanks for pointing out the interesting papers, which we will add as the references for our paper. For those papers, we have the following discussions:\n\n\t\ti. \u201cExploring the landscape of spatial robustness\u201d, ICML19: This paper falls into the same category (spatial distortion based attacks) as ours. Actually, in our discussions and experiments, we compared a spatial distortion method called the \u201cSpatial Transformation Method (STM)\u201d, which is implemented in Cleverhans and is exactly the method proposed in \u201cExploring the landscape of spatial robustness\u201d. We were unaware of this paper because Cleverhans did not give a reference to this method. We re-summarise the differences between ours and STM: STM only allows translations and rotations while ours allows all kinds of affine transformations; STM uses grid searches to find the optimal translation and rotation while ours is a differentiable method can be jointly trained with perturbation-based methods; STM takes grid searches for every test sample which can be inefficient, while ours only takes a pass of neural networks to conduct attacks (Shown in Table 3 of the paper, where our technique is 3 times faster than STM, even when including perturbations). \n\n\t\tii. \u201cCatastrophic Child's Play\", CVPR19: This paper only considers random affine transformations with rotations less than 15 degrees while ours optimises the affine transformations according to the loss of the classifier.\n\n \t\tiii. \"Semantic adversarial examples\", CVPR18 and \"Semantic adversarial attacks\", ICCV19: Spatial distortions are not used in these papers, so they might be relatively less related to ours. But we agree that they fall into the general area of \"adversarial attacks for images with visually meaningful transformations.\"\n\n2. Experimental evaluation:\n\nWe are conducting experiments on CIFAR and CelebA. We will try our best to report the results in the rebuttal. If the experiments cannot be concluded by the rebuttal deadline, we will report them in the revised paper.\n\n3. Weakness of theoretical part:\n\nWe will remove the theory as suggested."}, "signatures": ["ICLR.cc/2020/Conference/Paper1706/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1706/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions", "authors": ["He Zhao", "Trung Le", "Paul Montague", "Olivier De Vel", "Tamas Abraham", "Dinh Phung"], "authorids": ["ethanhezhao@gmail.com", "trunglm@monash.edu", "paul.montague@dst.defence.gov.au", "olivier.devel@dst.defence.gov.au", "tamas.abraham@dst.defence.gov.au", "dinh.phung@monash.edu"], "keywords": [], "TL;DR": "A new adversarial attack for images with both perturbations and spatial distortions", "abstract": "Deep neural network image classifiers are reported to be susceptible to adversarial evasion attacks, which use carefully crafted images created to mislead a classifier. Recently, various kinds of adversarial attack methods have been proposed, most of which focus on adding small perturbations to input images. Despite the success of existing approaches, the way to generate realistic adversarial images with small perturbations remains a challenging problem. In this paper, we aim to address this problem by proposing a novel adversarial method, which generates adversarial examples by imposing not only perturbations but also spatial distortions on input images, including scaling, rotation, shear, and translation. As humans are less susceptible to small spatial distortions, the proposed approach can produce visually more realistic attacks with smaller perturbations, able to deceive classifiers without affecting human predictions. We learn our method by amortized techniques with neural networks and generate adversarial examples efficiently by a forward pass of the networks. Extensive experiments on attacking different types of non-robustified classifiers and robust classifiers with defence show that our method has state-of-the-art performance in comparison with advanced attack parallels.", "pdf": "/pdf/170cfc3bc6beae2308e16b2690cba26fcd11f984.pdf", "paperhash": "zhao|perturbations_are_not_enough_generating_adversarial_examples_with_spatial_distortions", "original_pdf": "/attachment/170cfc3bc6beae2308e16b2690cba26fcd11f984.pdf", "_bibtex": "@misc{\nzhao2020perturbations,\ntitle={Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions},\nauthor={He Zhao and Trung Le and Paul Montague and Olivier De Vel and Tamas Abraham and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJg3HyStwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJg3HyStwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1706/Authors", "ICLR.cc/2020/Conference/Paper1706/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1706/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1706/Reviewers", "ICLR.cc/2020/Conference/Paper1706/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1706/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1706/Authors|ICLR.cc/2020/Conference/Paper1706/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152068, "tmdate": 1576860549724, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1706/Authors", "ICLR.cc/2020/Conference/Paper1706/Reviewers", "ICLR.cc/2020/Conference/Paper1706/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1706/-/Official_Comment"}}}, {"id": "r1xnXDVnFH", "original": null, "number": 1, "cdate": 1571731236362, "ddate": null, "tcdate": 1571731236362, "tmdate": 1572972433840, "tddate": null, "forum": "HJg3HyStwB", "replyto": "HJg3HyStwB", "invitation": "ICLR.cc/2020/Conference/Paper1706/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a new adversarial attack method by combining spatial transformations with perturbation-based noises. The proposed method uses two networks to generate the parameters of spatial transformation and the perturbation noise. The whole architecture is trained by a variant of GAN-loss to make the adversarial examples realistic to humans. Experiments on MNIST prove that the proposed attack method can improve the success rate of white-box attacks against several models.\n\nOverall, this paper considers an important problem of adversarial robustness of classifiers, and present a new approach to craft adversarial examples. The writing is clear. However, I have some concerns about this paper.\n\n1. This paper seems to integrate multiple ideas studied before into a single attack method. Perturbation-based adversarial examples, spatial transformation-based adversarial examples, generating adversarial examples based on the GAN loss are all studied before. And the proposed method integrates them together to form a new attack.\n\n2. The experiments are only conducted on MNIST and Fashion MNIST. More experiments on CIFAR-10 and ImageNet can further prove the effectiveness of the proposed method.\n\n3. More robust defense models should be incorporated in experiments, at least the PGD-based adversarial training model (Madry et al., 2018)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1706/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1706/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions", "authors": ["He Zhao", "Trung Le", "Paul Montague", "Olivier De Vel", "Tamas Abraham", "Dinh Phung"], "authorids": ["ethanhezhao@gmail.com", "trunglm@monash.edu", "paul.montague@dst.defence.gov.au", "olivier.devel@dst.defence.gov.au", "tamas.abraham@dst.defence.gov.au", "dinh.phung@monash.edu"], "keywords": [], "TL;DR": "A new adversarial attack for images with both perturbations and spatial distortions", "abstract": "Deep neural network image classifiers are reported to be susceptible to adversarial evasion attacks, which use carefully crafted images created to mislead a classifier. Recently, various kinds of adversarial attack methods have been proposed, most of which focus on adding small perturbations to input images. Despite the success of existing approaches, the way to generate realistic adversarial images with small perturbations remains a challenging problem. In this paper, we aim to address this problem by proposing a novel adversarial method, which generates adversarial examples by imposing not only perturbations but also spatial distortions on input images, including scaling, rotation, shear, and translation. As humans are less susceptible to small spatial distortions, the proposed approach can produce visually more realistic attacks with smaller perturbations, able to deceive classifiers without affecting human predictions. We learn our method by amortized techniques with neural networks and generate adversarial examples efficiently by a forward pass of the networks. Extensive experiments on attacking different types of non-robustified classifiers and robust classifiers with defence show that our method has state-of-the-art performance in comparison with advanced attack parallels.", "pdf": "/pdf/170cfc3bc6beae2308e16b2690cba26fcd11f984.pdf", "paperhash": "zhao|perturbations_are_not_enough_generating_adversarial_examples_with_spatial_distortions", "original_pdf": "/attachment/170cfc3bc6beae2308e16b2690cba26fcd11f984.pdf", "_bibtex": "@misc{\nzhao2020perturbations,\ntitle={Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions},\nauthor={He Zhao and Trung Le and Paul Montague and Olivier De Vel and Tamas Abraham and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJg3HyStwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJg3HyStwB", "replyto": "HJg3HyStwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1706/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1706/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575761859898, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1706/Reviewers"], "noninvitees": [], "tcdate": 1570237733464, "tmdate": 1575761859931, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1706/-/Official_Review"}}}, {"id": "r1e3Y-QTFH", "original": null, "number": 2, "cdate": 1571791235975, "ddate": null, "tcdate": 1571791235975, "tmdate": 1572972433805, "tddate": null, "forum": "HJg3HyStwB", "replyto": "HJg3HyStwB", "invitation": "ICLR.cc/2020/Conference/Paper1706/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper builds upon the work of AdvGAN and proposes to add spatial transformations on top of it. The resulting attacking framework is demonstrated to outperform AdvGAN on attacking several defense approaches, such as Defense-GAN, AdvCritic and adversarial training. Compared to previous approaches on generating spatially transformed adversarial examples, this approaches amortizes the attacking procedure and can produce spatially transformed adversarial examples much faster. This approach also simultaneously combine spatial transformations and perturbations to make the attack stronger.\n\nI cannot recommend acceptance of this paper because of several reasons:\n\n- The idea is not novel enough. It is simply an A + B paper where A = AdvGAN and B = spatial transformer networks. The idea of adversarial attacks with spatial distortion is not the innovation of this paper and has been proposed and extensively studied by many previous papers. This paper does not have additional innovation and does not lead to additional insight that can warrant an acceptance at ICLR.\n\n- The general narrative of this paper is misleading. The title seems to indicate this paper is the first to discover the importance of considering spatial perturbations, which is misleading. There is no mention of previous work on spatial transformation attacks in either the abstract nor the introduction (except at the very last). The introduction simply analyzes some well-known phenomenon in the literature, does not place this work well in the literature (even true in the related work section as well), and can mislead readers in believing that this work was the first to realize the importance of spatial transformation attacks.\n\n- Theorem 1 is a vacuous statement. It is automatically true based on the universal approximation assumption of neural networks. Including the statement of Theorem 1 is decorative and a waste of space.\n\n- The experiments are not convincing. For example, there is no \\gamma value reported in the tables. Since \\gamma is as important as \\epsilon in the proposed attacking method, the missing of this important variate is suspicious. Also the adversarial training only uses FGSM not PGD. The Defense-GAN is already shown not robust by Athalye et al. and cannot be considered as one of the state-of-the-art defenses. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1706/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1706/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions", "authors": ["He Zhao", "Trung Le", "Paul Montague", "Olivier De Vel", "Tamas Abraham", "Dinh Phung"], "authorids": ["ethanhezhao@gmail.com", "trunglm@monash.edu", "paul.montague@dst.defence.gov.au", "olivier.devel@dst.defence.gov.au", "tamas.abraham@dst.defence.gov.au", "dinh.phung@monash.edu"], "keywords": [], "TL;DR": "A new adversarial attack for images with both perturbations and spatial distortions", "abstract": "Deep neural network image classifiers are reported to be susceptible to adversarial evasion attacks, which use carefully crafted images created to mislead a classifier. Recently, various kinds of adversarial attack methods have been proposed, most of which focus on adding small perturbations to input images. Despite the success of existing approaches, the way to generate realistic adversarial images with small perturbations remains a challenging problem. In this paper, we aim to address this problem by proposing a novel adversarial method, which generates adversarial examples by imposing not only perturbations but also spatial distortions on input images, including scaling, rotation, shear, and translation. As humans are less susceptible to small spatial distortions, the proposed approach can produce visually more realistic attacks with smaller perturbations, able to deceive classifiers without affecting human predictions. We learn our method by amortized techniques with neural networks and generate adversarial examples efficiently by a forward pass of the networks. Extensive experiments on attacking different types of non-robustified classifiers and robust classifiers with defence show that our method has state-of-the-art performance in comparison with advanced attack parallels.", "pdf": "/pdf/170cfc3bc6beae2308e16b2690cba26fcd11f984.pdf", "paperhash": "zhao|perturbations_are_not_enough_generating_adversarial_examples_with_spatial_distortions", "original_pdf": "/attachment/170cfc3bc6beae2308e16b2690cba26fcd11f984.pdf", "_bibtex": "@misc{\nzhao2020perturbations,\ntitle={Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions},\nauthor={He Zhao and Trung Le and Paul Montague and Olivier De Vel and Tamas Abraham and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJg3HyStwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJg3HyStwB", "replyto": "HJg3HyStwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1706/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1706/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575761859898, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1706/Reviewers"], "noninvitees": [], "tcdate": 1570237733464, "tmdate": 1575761859931, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1706/-/Official_Review"}}}, {"id": "BkgXRYNaYB", "original": null, "number": 3, "cdate": 1571797450669, "ddate": null, "tcdate": 1571797450669, "tmdate": 1572972433759, "tddate": null, "forum": "HJg3HyStwB", "replyto": "HJg3HyStwB", "invitation": "ICLR.cc/2020/Conference/Paper1706/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper introduces a new approach to generate adversarial examples for deep classifiers. As opposed to the majority of work on adversarial attack models, which generally limit the attacker on pixel-space distortions measured with respect to an Lp norm, the authors here consider a slightly more general attack model that is a combination of an affine transformation and additive L2 perturbation of the input example. \n\nFinding optimal attacks for this model can be non-trivial (standard due to the highly nonlinear coupling between the affine parameters and the additive perturbation), so the authors instead propose training a surrogate neural network that generates the attack affine-transformation and distortion- parameters sequentially. This can, in principle, be done in a traditional supervised training setup; however, to force the adversarial images to look perceptually close to natural looking images, the authors throw a discriminator loss on top, and train the attack generator network adversarially.\n\nThe paper is well-written in general, the idea is intuitive, and the experiments are well-described. However, I have a few concerns that lead to me to give a low score (at least in the first round of reviews).\n\n- Novelty. \nLeveraging spatial distortions (or other visually meaningful transformations) to generate adversarial attacks is not a new idea, but the authors seem to have been unaware of this very large body of work. See, for example:\n** Engstrom et al, \"Exploring the landscape of spatial robustness\", ICML 2019\n** Poovendran et al, \"Semantic adversarial examples\", CVPR 2018\n** Ho et al, \"Catastrophic Child's Play\", CVPR 2019\n** Joshi et al, \"Semantic adversarial attacks\", ICCV 2019\namong many others.\n\nUsing GAN-like transformation models to generate attacks is also not a new idea. A few of the above papers use this approach, and the authors refer to a few other such papers as well.\n\nSo as such, the conceptual novelty of the contribution seems to be low (beyond the specific choice of combining affine and L2 perturbations).\n\n- Experimental evaluation.\nThe authors do a commendable job thoroughly laying out the experimental setup. However, a couple of red flags emerge in the experiments. First, why not look at L-infty perturbations (as opposed to L2)? Second, why not test on more challenging datasets (CIFAR, CelebA, etc) as opposed to simple black/white datasets such as MNIST/Fashion-MNIST? One would imagine that the smaller, simpler datasets are easier to optimize for, and therefore the \"amortized\" attack generator networks are not necessary here.\n\n- Weakness of theoretical part.\nI am not sure the theorem is saying anything strong or useful (since the underlying transformer neural network is assumed to possess infinite capacity). I would suggest just removing it.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1706/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1706/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions", "authors": ["He Zhao", "Trung Le", "Paul Montague", "Olivier De Vel", "Tamas Abraham", "Dinh Phung"], "authorids": ["ethanhezhao@gmail.com", "trunglm@monash.edu", "paul.montague@dst.defence.gov.au", "olivier.devel@dst.defence.gov.au", "tamas.abraham@dst.defence.gov.au", "dinh.phung@monash.edu"], "keywords": [], "TL;DR": "A new adversarial attack for images with both perturbations and spatial distortions", "abstract": "Deep neural network image classifiers are reported to be susceptible to adversarial evasion attacks, which use carefully crafted images created to mislead a classifier. Recently, various kinds of adversarial attack methods have been proposed, most of which focus on adding small perturbations to input images. Despite the success of existing approaches, the way to generate realistic adversarial images with small perturbations remains a challenging problem. In this paper, we aim to address this problem by proposing a novel adversarial method, which generates adversarial examples by imposing not only perturbations but also spatial distortions on input images, including scaling, rotation, shear, and translation. As humans are less susceptible to small spatial distortions, the proposed approach can produce visually more realistic attacks with smaller perturbations, able to deceive classifiers without affecting human predictions. We learn our method by amortized techniques with neural networks and generate adversarial examples efficiently by a forward pass of the networks. Extensive experiments on attacking different types of non-robustified classifiers and robust classifiers with defence show that our method has state-of-the-art performance in comparison with advanced attack parallels.", "pdf": "/pdf/170cfc3bc6beae2308e16b2690cba26fcd11f984.pdf", "paperhash": "zhao|perturbations_are_not_enough_generating_adversarial_examples_with_spatial_distortions", "original_pdf": "/attachment/170cfc3bc6beae2308e16b2690cba26fcd11f984.pdf", "_bibtex": "@misc{\nzhao2020perturbations,\ntitle={Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions},\nauthor={He Zhao and Trung Le and Paul Montague and Olivier De Vel and Tamas Abraham and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJg3HyStwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJg3HyStwB", "replyto": "HJg3HyStwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1706/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1706/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575761859898, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1706/Reviewers"], "noninvitees": [], "tcdate": 1570237733464, "tmdate": 1575761859931, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1706/-/Official_Review"}}}, {"id": "SklVActsdS", "original": null, "number": 2, "cdate": 1570638539841, "ddate": null, "tcdate": 1570638539841, "tmdate": 1570638539841, "tddate": null, "forum": "HJg3HyStwB", "replyto": "HJg3HyStwB", "invitation": "ICLR.cc/2020/Conference/Paper1706/-/Public_Comment", "content": {"comment": "Great work addressing an important problem in deep learning !\n\nI would like to mention a closely related work [1] that uses Bayesian optimisation (BO) to sequentially suggest an attack to make the deep models failed. The attacks used in [1] has considered different ways of affine transformation including translation, rotation, shearing. Using BO in [1] will be sample-efficient in making the attacks. That is, the targetted deep model will be failed under less number of attacks. \n\nThe problem definition is quite similar. That is, the objective function defined in Eq1, used to select the attack $x_A$, in the current paper is similar to objective function defined in Eq1 in [1]. Having said that, the search spaces considered in two papers are different. Particularly, the Eq1 in [1] considered the parameter space of the attacks (such as the shearing parameters) while Eq1 in this paper considered the L2-ball in the raw feature space.\n\n[1] Gopakumar, Shivapratap, et al. \"Algorithmic assurance: an active approach to algorithmic testing using Bayesian optimisation.\" Advances in Neural Information Processing Systems. 2018.\n\nNB: The source code is also available. ", "title": "Related work in generating distortions under affine transformation"}, "signatures": ["~Vu_Nguyen1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Vu_Nguyen1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions", "authors": ["He Zhao", "Trung Le", "Paul Montague", "Olivier De Vel", "Tamas Abraham", "Dinh Phung"], "authorids": ["ethanhezhao@gmail.com", "trunglm@monash.edu", "paul.montague@dst.defence.gov.au", "olivier.devel@dst.defence.gov.au", "tamas.abraham@dst.defence.gov.au", "dinh.phung@monash.edu"], "keywords": [], "TL;DR": "A new adversarial attack for images with both perturbations and spatial distortions", "abstract": "Deep neural network image classifiers are reported to be susceptible to adversarial evasion attacks, which use carefully crafted images created to mislead a classifier. Recently, various kinds of adversarial attack methods have been proposed, most of which focus on adding small perturbations to input images. Despite the success of existing approaches, the way to generate realistic adversarial images with small perturbations remains a challenging problem. In this paper, we aim to address this problem by proposing a novel adversarial method, which generates adversarial examples by imposing not only perturbations but also spatial distortions on input images, including scaling, rotation, shear, and translation. As humans are less susceptible to small spatial distortions, the proposed approach can produce visually more realistic attacks with smaller perturbations, able to deceive classifiers without affecting human predictions. We learn our method by amortized techniques with neural networks and generate adversarial examples efficiently by a forward pass of the networks. Extensive experiments on attacking different types of non-robustified classifiers and robust classifiers with defence show that our method has state-of-the-art performance in comparison with advanced attack parallels.", "pdf": "/pdf/170cfc3bc6beae2308e16b2690cba26fcd11f984.pdf", "paperhash": "zhao|perturbations_are_not_enough_generating_adversarial_examples_with_spatial_distortions", "original_pdf": "/attachment/170cfc3bc6beae2308e16b2690cba26fcd11f984.pdf", "_bibtex": "@misc{\nzhao2020perturbations,\ntitle={Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions},\nauthor={He Zhao and Trung Le and Paul Montague and Olivier De Vel and Tamas Abraham and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJg3HyStwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJg3HyStwB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504190930, "tmdate": 1576860583002, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1706/Authors", "ICLR.cc/2020/Conference/Paper1706/Reviewers", "ICLR.cc/2020/Conference/Paper1706/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1706/-/Public_Comment"}}}, {"id": "B1gf_h28Or", "original": null, "number": 2, "cdate": 1570323562310, "ddate": null, "tcdate": 1570323562310, "tmdate": 1570323562310, "tddate": null, "forum": "HJg3HyStwB", "replyto": "H1er8xkSOS", "invitation": "ICLR.cc/2020/Conference/Paper1706/-/Official_Comment", "content": {"comment": "Dear Dimitris,\n\nThanks a lot for pointing out this interesting and related paper, which we weren't aware of at the time of working on our submission.\n\nAfter a quick look at this ICML19 paper, we find that in terms of attacks, there are several differences between ours and the ICML19 one. For example, the ICML19 one conducts rotation and translation to generate an adversarial example by doing optimisation given a test sample, while ours uses an amortized way, which learns a neural network to conduct rotation, translation, scaling, and shear. Moreover, ours is a joint amortized process (with two neural networks), that combines spatial distortions and perturbations, aiming to generate realistic adversarial examples with fewer perturbations. While the ICML19 one seems to use two separate steps for PGD and grid search for rotations and translations, respectively. We will discuss more on differences as well as connections between the two attacks in the updated version of our paper.\n\nWe also appreciate that the code of the ICML19 paper is released. Therefore, we are currently doing a comparison with the attack and defence methods introduced in the ICML19 paper and will provide a detailed discussion in the updated version of our paper.\n\nBesides, thanks for pointing out the BMVC paper as well, which will be added to our reference.\n\nThanks again,\nPaper 1706 Authors", "title": "Thanks for the reference"}, "signatures": ["ICLR.cc/2020/Conference/Paper1706/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1706/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions", "authors": ["He Zhao", "Trung Le", "Paul Montague", "Olivier De Vel", "Tamas Abraham", "Dinh Phung"], "authorids": ["ethanhezhao@gmail.com", "trunglm@monash.edu", "paul.montague@dst.defence.gov.au", "olivier.devel@dst.defence.gov.au", "tamas.abraham@dst.defence.gov.au", "dinh.phung@monash.edu"], "keywords": [], "TL;DR": "A new adversarial attack for images with both perturbations and spatial distortions", "abstract": "Deep neural network image classifiers are reported to be susceptible to adversarial evasion attacks, which use carefully crafted images created to mislead a classifier. Recently, various kinds of adversarial attack methods have been proposed, most of which focus on adding small perturbations to input images. Despite the success of existing approaches, the way to generate realistic adversarial images with small perturbations remains a challenging problem. In this paper, we aim to address this problem by proposing a novel adversarial method, which generates adversarial examples by imposing not only perturbations but also spatial distortions on input images, including scaling, rotation, shear, and translation. As humans are less susceptible to small spatial distortions, the proposed approach can produce visually more realistic attacks with smaller perturbations, able to deceive classifiers without affecting human predictions. We learn our method by amortized techniques with neural networks and generate adversarial examples efficiently by a forward pass of the networks. Extensive experiments on attacking different types of non-robustified classifiers and robust classifiers with defence show that our method has state-of-the-art performance in comparison with advanced attack parallels.", "pdf": "/pdf/170cfc3bc6beae2308e16b2690cba26fcd11f984.pdf", "paperhash": "zhao|perturbations_are_not_enough_generating_adversarial_examples_with_spatial_distortions", "original_pdf": "/attachment/170cfc3bc6beae2308e16b2690cba26fcd11f984.pdf", "_bibtex": "@misc{\nzhao2020perturbations,\ntitle={Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions},\nauthor={He Zhao and Trung Le and Paul Montague and Olivier De Vel and Tamas Abraham and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJg3HyStwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJg3HyStwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1706/Authors", "ICLR.cc/2020/Conference/Paper1706/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1706/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1706/Reviewers", "ICLR.cc/2020/Conference/Paper1706/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1706/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1706/Authors|ICLR.cc/2020/Conference/Paper1706/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152068, "tmdate": 1576860549724, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1706/Authors", "ICLR.cc/2020/Conference/Paper1706/Reviewers", "ICLR.cc/2020/Conference/Paper1706/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1706/-/Official_Comment"}}}, {"id": "H1er8xkSOS", "original": null, "number": 1, "cdate": 1570201677262, "ddate": null, "tcdate": 1570201677262, "tmdate": 1570201677262, "tddate": null, "forum": "HJg3HyStwB", "replyto": "HJg3HyStwB", "invitation": "ICLR.cc/2020/Conference/Paper1706/-/Public_Comment", "content": {"comment": "I wanted to bring to your attention our work studying spatial distortions---rotations and translations---and their combination with standard pixel-based perturbations: \"Exploring the landscape of spatial robustness\" (ICML'19, https://arxiv.org/abs/1712.02779). Specifically, we consider an adversary that tries all possible spatial transformations (through exhaustive grid search) and then applies a standard PGD attack on top.\n\nAlso, I believe it is worth mentioning \"Manitest: are classifiers really invariant?\" (BMVC'15, https://arxiv.org/abs/1507.06535) which---to the best of my knowledge---is the first work studying the robustness of deep networks to spatial transformations.", "title": "Prior work on combining spatial distortions with pixel-wise perturbations"}, "signatures": ["~Dimitris_Tsipras1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Dimitris_Tsipras1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions", "authors": ["He Zhao", "Trung Le", "Paul Montague", "Olivier De Vel", "Tamas Abraham", "Dinh Phung"], "authorids": ["ethanhezhao@gmail.com", "trunglm@monash.edu", "paul.montague@dst.defence.gov.au", "olivier.devel@dst.defence.gov.au", "tamas.abraham@dst.defence.gov.au", "dinh.phung@monash.edu"], "keywords": [], "TL;DR": "A new adversarial attack for images with both perturbations and spatial distortions", "abstract": "Deep neural network image classifiers are reported to be susceptible to adversarial evasion attacks, which use carefully crafted images created to mislead a classifier. Recently, various kinds of adversarial attack methods have been proposed, most of which focus on adding small perturbations to input images. Despite the success of existing approaches, the way to generate realistic adversarial images with small perturbations remains a challenging problem. In this paper, we aim to address this problem by proposing a novel adversarial method, which generates adversarial examples by imposing not only perturbations but also spatial distortions on input images, including scaling, rotation, shear, and translation. As humans are less susceptible to small spatial distortions, the proposed approach can produce visually more realistic attacks with smaller perturbations, able to deceive classifiers without affecting human predictions. We learn our method by amortized techniques with neural networks and generate adversarial examples efficiently by a forward pass of the networks. Extensive experiments on attacking different types of non-robustified classifiers and robust classifiers with defence show that our method has state-of-the-art performance in comparison with advanced attack parallels.", "pdf": "/pdf/170cfc3bc6beae2308e16b2690cba26fcd11f984.pdf", "paperhash": "zhao|perturbations_are_not_enough_generating_adversarial_examples_with_spatial_distortions", "original_pdf": "/attachment/170cfc3bc6beae2308e16b2690cba26fcd11f984.pdf", "_bibtex": "@misc{\nzhao2020perturbations,\ntitle={Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions},\nauthor={He Zhao and Trung Le and Paul Montague and Olivier De Vel and Tamas Abraham and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJg3HyStwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJg3HyStwB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504190930, "tmdate": 1576860583002, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1706/Authors", "ICLR.cc/2020/Conference/Paper1706/Reviewers", "ICLR.cc/2020/Conference/Paper1706/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1706/-/Public_Comment"}}}], "count": 11}