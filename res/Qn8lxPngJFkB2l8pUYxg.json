{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1463668964218, "tcdate": 1463668964218, "id": "wV61EMPqkcG0qV7mtLR1", "invitation": "ICLR.cc/2016/workshop/-/paper/108/comment", "forum": "Qn8lxPngJFkB2l8pUYxg", "replyto": "91vvzKgJlSkRlNvXUVGO", "signatures": ["~Rudolf_Kadlec1"], "readers": ["everyone"], "writers": ["~Rudolf_Kadlec1"], "content": {"title": "good point", "comment": "You are right. We will include this perspective in next version of our long paper on arxiv.  \nThank you\nRuda"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Neural Text Understanding with Attention Sum Reader", "abstract": "Two large-scale cloze-style context-question-answer datasets have been introduced recently: i) the CNN and Daily Mail news data and ii) the Children's Book Test.\nThanks to the size of these datasets, the associated task is well suited for deep-learning techniques that seem to outperform all alternative approaches.\nWe present a new, simple model that is tailor made for such question-answering problems.\nOur model directly sums attention over candidate answer words in the document instead of using it to compute weighted sum of word embeddings.\nOur model outperforms models previously proposed for these tasks by a large margin.", "pdf": "/pdf/Qn8lxPngJFkB2l8pUYxg.pdf", "paperhash": "kadlec|neural_text_understanding_with_attention_sum_reader", "conflicts": ["ibm.com"], "authors": ["Rudolf Kadlec", "Martin Schmid", "Ond\u0159ej Bajgar", "Jan Kleindienst"], "authorids": ["rudolf_kadlec@cz.ibm.com", "martin.schmid@cz.ibm.com", "obajgar@cz.ibm.com", "jankle@cz.ibm.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455816128008, "ddate": null, "super": null, "final": null, "tcdate": 1455816128008, "id": "ICLR.cc/2016/workshop/-/paper/108/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "Qn8lxPngJFkB2l8pUYxg", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/108/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1462742182306, "tcdate": 1462742182306, "id": "91vvzKgJlSkRlNvXUVGO", "invitation": "ICLR.cc/2016/workshop/-/paper/108/comment", "forum": "Qn8lxPngJFkB2l8pUYxg", "replyto": "E8V39o1PDf31v0m2iDp2", "signatures": ["~Felix_Hill1"], "readers": ["everyone"], "writers": ["~Felix_Hill1"], "content": {"title": "parallels between this approach and self-supervision", "comment": "This is great work, and a really nice effective modification of previous approaches for these tasks. \n\nMaybe the authors could also point out how elimination of the output layer has similarities with the self-supervision method we use in the Goldilocks Principle. While we compute prediction probabilities over all possible answers at test time, training with self-supervision also has the effect of bypassing the output layer, so in this sense the work generalises (and makes clearer) what we already proposed. It would be great  if you could add this detail. Of course, there are also similarities with Pointer Networks. \n\nThank you and good luck!\nFelix Hill\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Neural Text Understanding with Attention Sum Reader", "abstract": "Two large-scale cloze-style context-question-answer datasets have been introduced recently: i) the CNN and Daily Mail news data and ii) the Children's Book Test.\nThanks to the size of these datasets, the associated task is well suited for deep-learning techniques that seem to outperform all alternative approaches.\nWe present a new, simple model that is tailor made for such question-answering problems.\nOur model directly sums attention over candidate answer words in the document instead of using it to compute weighted sum of word embeddings.\nOur model outperforms models previously proposed for these tasks by a large margin.", "pdf": "/pdf/Qn8lxPngJFkB2l8pUYxg.pdf", "paperhash": "kadlec|neural_text_understanding_with_attention_sum_reader", "conflicts": ["ibm.com"], "authors": ["Rudolf Kadlec", "Martin Schmid", "Ond\u0159ej Bajgar", "Jan Kleindienst"], "authorids": ["rudolf_kadlec@cz.ibm.com", "martin.schmid@cz.ibm.com", "obajgar@cz.ibm.com", "jankle@cz.ibm.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455816128008, "ddate": null, "super": null, "final": null, "tcdate": 1455816128008, "id": "ICLR.cc/2016/workshop/-/paper/108/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "Qn8lxPngJFkB2l8pUYxg", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/108/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457048579226, "tcdate": 1457048579226, "id": "E8V39o1PDf31v0m2iDp2", "invitation": "ICLR.cc/2016/workshop/-/paper/108/review/11", "forum": "Qn8lxPngJFkB2l8pUYxg", "replyto": "Qn8lxPngJFkB2l8pUYxg", "signatures": ["ICLR.cc/2016/workshop/paper/108/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/108/reviewer/11"], "content": {"title": "nice paper with useful insights into recently proposed QA datasets", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper proposed a simple variation on attentional models designed for QA tasks that achieves state-of-the-art results on two different datasets. The task is, when given a document and a query, output the answer to the query where the answer is contained within the document. While previous work uses the attention scores to compute a weighted average of words in the document and then feeds that average to an output layer, the authors instead select the argmax of the attention scores as the answer (summing scores when a word is repeated multiple times in a document). The drawback of this approach is that it cannot work if an answer is not contained within the document, but this isn't an issue for the datasets in question.\n\nThe evaluation section could be stronger; the authors remark that \"single models can display considerable variation of results which can then prove difficult to reproduce\"; it would be best to report standard deviation so readers can quantify this variation. However, the results are significantly better than attentional LSTMs and memory networks despite the relative simplicity of the proposed model. The result is interesting and I think would be a valuable contribution as a workshop paper.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Neural Text Understanding with Attention Sum Reader", "abstract": "Two large-scale cloze-style context-question-answer datasets have been introduced recently: i) the CNN and Daily Mail news data and ii) the Children's Book Test.\nThanks to the size of these datasets, the associated task is well suited for deep-learning techniques that seem to outperform all alternative approaches.\nWe present a new, simple model that is tailor made for such question-answering problems.\nOur model directly sums attention over candidate answer words in the document instead of using it to compute weighted sum of word embeddings.\nOur model outperforms models previously proposed for these tasks by a large margin.", "pdf": "/pdf/Qn8lxPngJFkB2l8pUYxg.pdf", "paperhash": "kadlec|neural_text_understanding_with_attention_sum_reader", "conflicts": ["ibm.com"], "authors": ["Rudolf Kadlec", "Martin Schmid", "Ond\u0159ej Bajgar", "Jan Kleindienst"], "authorids": ["rudolf_kadlec@cz.ibm.com", "martin.schmid@cz.ibm.com", "obajgar@cz.ibm.com", "jankle@cz.ibm.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579951697, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579951697, "id": "ICLR.cc/2016/workshop/-/paper/108/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "Qn8lxPngJFkB2l8pUYxg", "replyto": "Qn8lxPngJFkB2l8pUYxg", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/108/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455816119913, "tcdate": 1455816119913, "id": "Qn8lxPngJFkB2l8pUYxg", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "Qn8lxPngJFkB2l8pUYxg", "signatures": ["~Rudolf_Kadlec1"], "readers": ["everyone"], "writers": ["~Rudolf_Kadlec1"], "content": {"CMT_id": "", "title": "Neural Text Understanding with Attention Sum Reader", "abstract": "Two large-scale cloze-style context-question-answer datasets have been introduced recently: i) the CNN and Daily Mail news data and ii) the Children's Book Test.\nThanks to the size of these datasets, the associated task is well suited for deep-learning techniques that seem to outperform all alternative approaches.\nWe present a new, simple model that is tailor made for such question-answering problems.\nOur model directly sums attention over candidate answer words in the document instead of using it to compute weighted sum of word embeddings.\nOur model outperforms models previously proposed for these tasks by a large margin.", "pdf": "/pdf/Qn8lxPngJFkB2l8pUYxg.pdf", "paperhash": "kadlec|neural_text_understanding_with_attention_sum_reader", "conflicts": ["ibm.com"], "authors": ["Rudolf Kadlec", "Martin Schmid", "Ond\u0159ej Bajgar", "Jan Kleindienst"], "authorids": ["rudolf_kadlec@cz.ibm.com", "martin.schmid@cz.ibm.com", "obajgar@cz.ibm.com", "jankle@cz.ibm.com"]}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 4}