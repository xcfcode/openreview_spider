{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396599629, "tcdate": 1486396599629, "number": 1, "id": "B1xa2zLul", "invitation": "ICLR.cc/2017/conference/-/paper466/acceptance", "forum": "SkuqA_cgx", "replyto": "SkuqA_cgx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The reviewers (and I) welcome new approaches to evaluation, and this work appears to be a sound contribution to that space. I suggest that, because the paper's incremental findings are of somewhat narrower interest, it is a good fit for a workshop, so that the ideas can be discussed at ICLR and further developed into a more mature publication.", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations", "abstract": "We propose a language-agnostic way of automatically generating sets of semantically similar clusters of entities along with sets of \"outlier\" elements, which may then be used to perform an intrinsic evaluation of word embeddings in the outlier detection task. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results show a correlation between performance on this dataset and performance on sentiment analysis.", "pdf": "/pdf/5bc457660e09eee1aad834c5b9bcf018f51f21d1.pdf", "TL;DR": "Applying simple heuristics to the Wikidata entity graph results in a high-quality semantic similarity dataset.", "paperhash": "blair|automated_generation_of_multilingual_clusters_for_the_evaluation_of_distributed_representations", "keywords": ["Natural language processing", "Applications"], "conflicts": ["basistech.com", "neu.edu"], "authors": ["Philip Blair", "Yuval Merhav", "Joel Barry"], "authorids": ["pblair@basistech.com", "yuval@basistech.com", "joelb@basistech.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396600154, "id": "ICLR.cc/2017/conference/-/paper466/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SkuqA_cgx", "replyto": "SkuqA_cgx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396600154}}}, {"tddate": null, "tmdate": 1482343887770, "tcdate": 1482343887770, "number": 5, "id": "SyORSSu4g", "invitation": "ICLR.cc/2017/conference/-/paper466/public/comment", "forum": "SkuqA_cgx", "replyto": "HkDOVF3ml", "signatures": ["~Philip_Blair1"], "readers": ["everyone"], "writers": ["~Philip_Blair1"], "content": {"title": "Scalability and Goodness-of-Fit", "comment": "Thank you for your comments.\n\nOur primary goal was to investigate a way to automatically generate outlier detection test cases without the\nrequired labor and domain-knowledge bias of human-created test sets. While our method may have only yielded 500 test groups (a total of 2,000-3,000 test cases for each language), we believe that the WikiSem500 dataset shows promise to this end. Especially, the strength of our approach is in the number of languages it can scale up to. We believe it can be applied for any language with decent Wikidata/Wikipedia coverage. Our first release of WikiSem500 includes five major languages, but we are planning a second release with significantly more languages. As far as we know, previous approaches don\u2019t scale to other languages without significant manual efforts. We\u2019ve updated the related work section with more details about the disadvantages of existing datasets compared to WikiSem500. \n\nIn the spirit of this, while we did not consider using Mechanical Turk for filtering out test cases, we would\nhave likely decided against it had we done so. Requiring human quality assurance as a step in the pipeline would necessitate having one or more language experts in each language one is attempting to construct test cases for. Instead, we opted to iteratively perform quality checks on our system's output and nail down the common characteristics of \"bad\" test groups. Once we had developed a fully fleshed-out system of heuristics for English, we found that very little work was required to extend them into an equally-fleshed-out system in other languages.\n\nRegarding your third point. Camacho-Collados & Navigli (2016) motivated the outlier-detection task by the high human performance compared to other tasks like word similarity, which we agree with. We also reproduced their 90+% human performance on a subset of our English dataset which the annotators had sufficient domain knowledge on. Our main contribution is that we created a larger and more diverse dataset, with both words and phrases, and in multiple languages. Their dataset was manually created like most existing datasets.\n\nWith respect to this paper being an appropriate fit for ICLR, we would like to note that prior to submitting we\nasked the conference coordinators this exact question:\n\u201cWill ICLR consider work about word embedding evaluation? In particular, a paper proposing a new evaluation dataset.\u201d\n \nTo which we received the following response:\n\"[T]hat is considered a relevant topic because it's about representing words. Better metrics and datasets are key to eventual modeling improvements. Thank you.\"\n\nFinally, thank you for bringing our attention to that citation which escaped our notice. We have updated the paper to include it.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations", "abstract": "We propose a language-agnostic way of automatically generating sets of semantically similar clusters of entities along with sets of \"outlier\" elements, which may then be used to perform an intrinsic evaluation of word embeddings in the outlier detection task. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results show a correlation between performance on this dataset and performance on sentiment analysis.", "pdf": "/pdf/5bc457660e09eee1aad834c5b9bcf018f51f21d1.pdf", "TL;DR": "Applying simple heuristics to the Wikidata entity graph results in a high-quality semantic similarity dataset.", "paperhash": "blair|automated_generation_of_multilingual_clusters_for_the_evaluation_of_distributed_representations", "keywords": ["Natural language processing", "Applications"], "conflicts": ["basistech.com", "neu.edu"], "authors": ["Philip Blair", "Yuval Merhav", "Joel Barry"], "authorids": ["pblair@basistech.com", "yuval@basistech.com", "joelb@basistech.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287565438, "id": "ICLR.cc/2017/conference/-/paper466/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkuqA_cgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper466/reviewers", "ICLR.cc/2017/conference/paper466/areachairs"], "cdate": 1485287565438}}}, {"tddate": null, "tmdate": 1482343839059, "tcdate": 1482343839059, "number": 4, "id": "BywsBS_El", "invitation": "ICLR.cc/2017/conference/-/paper466/public/comment", "forum": "SkuqA_cgx", "replyto": "r1rwrnWVg", "signatures": ["~Philip_Blair1"], "readers": ["everyone"], "writers": ["~Philip_Blair1"], "content": {"title": "Differences with the Analogy Dataset", "comment": "Thank you for your comments.\n\nAs far as we know, the majority of existing datasets, like the analogy dataset proposed by Mikolov et al., are in English and focus on single words only. Also, the Mikolov word analogies dataset contains 14 categories, and only about half of them are for semantic evaluation (e.g., \"US Cities\", \"Common Capitals\", \"All Capitals\"). In contrast, our dataset contains hundreds of categories, making it a more diverse and challenging dataset for general-purpose evaluation of word representations. The Mikolov dataset has the advantage of including syntactic categories, which we left for future work. In the related work section we focused mainly on problems with existing word similarity datasets. We've updated it to include word analogies as well.\n\nCould you please elaborate more on entity typing? Are you referring to fine-grained named entity recognition that can be used for building a similar dataset?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations", "abstract": "We propose a language-agnostic way of automatically generating sets of semantically similar clusters of entities along with sets of \"outlier\" elements, which may then be used to perform an intrinsic evaluation of word embeddings in the outlier detection task. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results show a correlation between performance on this dataset and performance on sentiment analysis.", "pdf": "/pdf/5bc457660e09eee1aad834c5b9bcf018f51f21d1.pdf", "TL;DR": "Applying simple heuristics to the Wikidata entity graph results in a high-quality semantic similarity dataset.", "paperhash": "blair|automated_generation_of_multilingual_clusters_for_the_evaluation_of_distributed_representations", "keywords": ["Natural language processing", "Applications"], "conflicts": ["basistech.com", "neu.edu"], "authors": ["Philip Blair", "Yuval Merhav", "Joel Barry"], "authorids": ["pblair@basistech.com", "yuval@basistech.com", "joelb@basistech.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287565438, "id": "ICLR.cc/2017/conference/-/paper466/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkuqA_cgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper466/reviewers", "ICLR.cc/2017/conference/paper466/areachairs"], "cdate": 1485287565438}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1482342769923, "tcdate": 1478295185281, "number": 466, "id": "SkuqA_cgx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SkuqA_cgx", "signatures": ["~Philip_Blair1"], "readers": ["everyone"], "content": {"title": "Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations", "abstract": "We propose a language-agnostic way of automatically generating sets of semantically similar clusters of entities along with sets of \"outlier\" elements, which may then be used to perform an intrinsic evaluation of word embeddings in the outlier detection task. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results show a correlation between performance on this dataset and performance on sentiment analysis.", "pdf": "/pdf/5bc457660e09eee1aad834c5b9bcf018f51f21d1.pdf", "TL;DR": "Applying simple heuristics to the Wikidata entity graph results in a high-quality semantic similarity dataset.", "paperhash": "blair|automated_generation_of_multilingual_clusters_for_the_evaluation_of_distributed_representations", "keywords": ["Natural language processing", "Applications"], "conflicts": ["basistech.com", "neu.edu"], "authors": ["Philip Blair", "Yuval Merhav", "Joel Barry"], "authorids": ["pblair@basistech.com", "yuval@basistech.com", "joelb@basistech.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 11, "writable": false, "overwriting": ["SyA0t9fFe"], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481913692731, "tcdate": 1481913692731, "number": 3, "id": "r1rwrnWVg", "invitation": "ICLR.cc/2017/conference/-/paper466/official/review", "forum": "SkuqA_cgx", "replyto": "SkuqA_cgx", "signatures": ["ICLR.cc/2017/conference/paper466/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper466/AnonReviewer1"], "content": {"title": "Borderline", "rating": "5: Marginally below acceptance threshold", "review": "This paper introduces a new dataset to evaluate word representations. The task considered in the paper, called outlier detection (also known as word intrusion), is to identify which word does not belong to a set of semantically related words. The task was proposed by Camacho-Collados & Navigli (2016) as an evaluation of word representations. The main contribution of this paper is to introduce a new dataset for this task, covering 5 languages. The dataset was generated automatically from the Wikidata hierarchy.\nEntities which are instances of the same category are considered as belonging to the same cluster, and outliers are sampled at various distances in the tree. Several heuristics are then proposed to exclude uninteresting clusters from the dataset.\n\nDeveloping good ressources to evaluate word representations is an important task. The new dataset introduced in this paper might be an interesting addition to the existing ones (however, it is hard to say by only reviewing the paper). I am a bit concerned by the lack of discussion and comparison with existing approaches (besides word similarity datasets). In particular, I believe it would be interesting to discuss the advantages of this evaluation/dataset, compared to existing ones such as word analogies. The proposed evaluation also seems highly related to entity typing, which is not discussed in the paper.\n\nOverall, I believe that introducing ressources for evaluating word representations is very important for the community. However, I am a bit ambivalent about this submission. I am not entirely convinced that the proposed dataset have clear advantages over existing ressources. It also seems that existing tasks, such as entity typing, already capture similar properties of word representations. Finally, it might be more relevant to submit this paper to LREC than to ICLR.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations", "abstract": "We propose a language-agnostic way of automatically generating sets of semantically similar clusters of entities along with sets of \"outlier\" elements, which may then be used to perform an intrinsic evaluation of word embeddings in the outlier detection task. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results show a correlation between performance on this dataset and performance on sentiment analysis.", "pdf": "/pdf/5bc457660e09eee1aad834c5b9bcf018f51f21d1.pdf", "TL;DR": "Applying simple heuristics to the Wikidata entity graph results in a high-quality semantic similarity dataset.", "paperhash": "blair|automated_generation_of_multilingual_clusters_for_the_evaluation_of_distributed_representations", "keywords": ["Natural language processing", "Applications"], "conflicts": ["basistech.com", "neu.edu"], "authors": ["Philip Blair", "Yuval Merhav", "Joel Barry"], "authorids": ["pblair@basistech.com", "yuval@basistech.com", "joelb@basistech.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512576553, "id": "ICLR.cc/2017/conference/-/paper466/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper466/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper466/AnonReviewer3", "ICLR.cc/2017/conference/paper466/AnonReviewer2", "ICLR.cc/2017/conference/paper466/AnonReviewer1"], "reply": {"forum": "SkuqA_cgx", "replyto": "SkuqA_cgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper466/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper466/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512576553}}}, {"tddate": null, "tmdate": 1481756498307, "tcdate": 1481756498303, "number": 2, "id": "r1qL1Uy4x", "invitation": "ICLR.cc/2017/conference/-/paper466/official/review", "forum": "SkuqA_cgx", "replyto": "SkuqA_cgx", "signatures": ["ICLR.cc/2017/conference/paper466/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper466/AnonReviewer2"], "content": {"title": "Accept", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper describes a new benchmark for word representations: spotting the \u201codd one out\u201d. The authors build upon an idea recently presented at the RepEval workshop, but are able to collect a significantly larger amount of examples by relying on existing ontologies.\n\nAlthough the innovation is relatively incremental, it is an important step in defining challenging benchmarks for general-purpose word representations. While humans are able to perform this task almost flawlessly (given adequate domain knowledge), the experiments in this paper show that current embeddings fall short. The technical contribution is thorough; the dataset construction appears logical, and the correlation analysis is convincing. I would like to see it accepted at ICLR.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations", "abstract": "We propose a language-agnostic way of automatically generating sets of semantically similar clusters of entities along with sets of \"outlier\" elements, which may then be used to perform an intrinsic evaluation of word embeddings in the outlier detection task. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results show a correlation between performance on this dataset and performance on sentiment analysis.", "pdf": "/pdf/5bc457660e09eee1aad834c5b9bcf018f51f21d1.pdf", "TL;DR": "Applying simple heuristics to the Wikidata entity graph results in a high-quality semantic similarity dataset.", "paperhash": "blair|automated_generation_of_multilingual_clusters_for_the_evaluation_of_distributed_representations", "keywords": ["Natural language processing", "Applications"], "conflicts": ["basistech.com", "neu.edu"], "authors": ["Philip Blair", "Yuval Merhav", "Joel Barry"], "authorids": ["pblair@basistech.com", "yuval@basistech.com", "joelb@basistech.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512576553, "id": "ICLR.cc/2017/conference/-/paper466/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper466/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper466/AnonReviewer3", "ICLR.cc/2017/conference/paper466/AnonReviewer2", "ICLR.cc/2017/conference/paper466/AnonReviewer1"], "reply": {"forum": "SkuqA_cgx", "replyto": "SkuqA_cgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper466/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper466/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512576553}}}, {"tddate": null, "tmdate": 1481573486658, "tcdate": 1481573486651, "number": 1, "id": "HkDOVF3ml", "invitation": "ICLR.cc/2017/conference/-/paper466/official/review", "forum": "SkuqA_cgx", "replyto": "SkuqA_cgx", "signatures": ["ICLR.cc/2017/conference/paper466/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper466/AnonReviewer3"], "content": {"title": "Data set seems interesting, but it's a pity that the automatic generation hasn't been scaled up.", "rating": "6: Marginally above acceptance threshold", "review": "First, let me praise the authors for generating and releasing an NLP data set: a socially useful task.\n\nThe authors use an algorithm to generate a 500-cluster-per-language data set in semantic similarity. This brings up a few points.\n\n1. If the point of using the algorithm is to be scalable, why release such a small data set? It's roughly the same order of magnitude as the data sets released in the SemEval tasks over the recent years. I would have expected something orders of magnitude larger.\n\n2. The authors hand checked a small subset of the clusters: they found one where it was ambiguous, and should probably have been removed. Mechanical Turk can scale pretty well -- why not post-facto filter all of the clusters using MT? This is (in effect) how ImageNet was created, and it has millions of items.\n\n3. Evaluating data set papers is an tricky issue. What makes a data set \"good\" or publishable? There are a number of medium-sized NLP data sets released every year (e.g., through SemEval). Those are designed to address tasks in NLP that people find interesting. I don't know of a data set that exactly addresses the task that the authors propose: the task is trying to address the idea of semantic similarity, which has had multiple data sets thrown at it since SemEval 2012. I wish that the paper had included comparisons to show that the particular task / data combination is better suited for analyzing semantic similarity than other existing data sets.\n\nTwo final notes:\n\nA. This paper doesn't seem very well-suited to ICLR.  New NLP data sets may be indirectly useful for evaluating word embeddings (and hence representations). But, I didn't learn much from the paper: GloVe is empirically less good for semantic similarity than other embeddings? If true, why? That would be interesting.\n\nB. The first proposal for the \"put a word into a cluster and see if it stands out\" task (in the context of human evaluation of topic models), is\n\nJonathan Chang, Jordan Boyd-Graber, Chong Wang, Sean Gerrish, and David M. Blei. Reading Tea Leaves: HowHumans Interpret Topic Models. Neural Information Processing Systems, 2009\n\nwhich deserves a citation, I think.\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations", "abstract": "We propose a language-agnostic way of automatically generating sets of semantically similar clusters of entities along with sets of \"outlier\" elements, which may then be used to perform an intrinsic evaluation of word embeddings in the outlier detection task. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results show a correlation between performance on this dataset and performance on sentiment analysis.", "pdf": "/pdf/5bc457660e09eee1aad834c5b9bcf018f51f21d1.pdf", "TL;DR": "Applying simple heuristics to the Wikidata entity graph results in a high-quality semantic similarity dataset.", "paperhash": "blair|automated_generation_of_multilingual_clusters_for_the_evaluation_of_distributed_representations", "keywords": ["Natural language processing", "Applications"], "conflicts": ["basistech.com", "neu.edu"], "authors": ["Philip Blair", "Yuval Merhav", "Joel Barry"], "authorids": ["pblair@basistech.com", "yuval@basistech.com", "joelb@basistech.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512576553, "id": "ICLR.cc/2017/conference/-/paper466/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper466/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper466/AnonReviewer3", "ICLR.cc/2017/conference/paper466/AnonReviewer2", "ICLR.cc/2017/conference/paper466/AnonReviewer1"], "reply": {"forum": "SkuqA_cgx", "replyto": "SkuqA_cgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper466/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper466/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512576553}}}, {"tddate": null, "tmdate": 1481311578781, "tcdate": 1481311578776, "number": 3, "id": "HkXPSKOXg", "invitation": "ICLR.cc/2017/conference/-/paper466/public/comment", "forum": "SkuqA_cgx", "replyto": "Sy7ie31mg", "signatures": ["~Philip_Blair1"], "readers": ["everyone"], "writers": ["~Philip_Blair1"], "content": {"title": "Results by Outlier Class and Comparison with the Analogy Task", "comment": "Thank you for the feedback! Indeed, differing degrees of difficulty are the intent of the three outlier classes, with O1 outliers being the most difficult to detect (as they are the most similar to the cluster entities) and O3 being the easiest. In order to test this, we have generated three datasets with only one of each outlier class. Figure 2 in the updated paper illustrates that there is in fact a strong correlation between outlier class and performance.  \n\nAdditionally, we have updated the paper to include results which compare our evaluations to performance on the analogy task. We find a strong correlation (Spearman's rho = 0.88) between performance on the two metrics."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations", "abstract": "We propose a language-agnostic way of automatically generating sets of semantically similar clusters of entities along with sets of \"outlier\" elements, which may then be used to perform an intrinsic evaluation of word embeddings in the outlier detection task. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results show a correlation between performance on this dataset and performance on sentiment analysis.", "pdf": "/pdf/5bc457660e09eee1aad834c5b9bcf018f51f21d1.pdf", "TL;DR": "Applying simple heuristics to the Wikidata entity graph results in a high-quality semantic similarity dataset.", "paperhash": "blair|automated_generation_of_multilingual_clusters_for_the_evaluation_of_distributed_representations", "keywords": ["Natural language processing", "Applications"], "conflicts": ["basistech.com", "neu.edu"], "authors": ["Philip Blair", "Yuval Merhav", "Joel Barry"], "authorids": ["pblair@basistech.com", "yuval@basistech.com", "joelb@basistech.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287565438, "id": "ICLR.cc/2017/conference/-/paper466/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkuqA_cgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper466/reviewers", "ICLR.cc/2017/conference/paper466/areachairs"], "cdate": 1485287565438}}}, {"tddate": null, "tmdate": 1481311288023, "tcdate": 1481311288016, "number": 2, "id": "ryxHVFdXg", "invitation": "ICLR.cc/2017/conference/-/paper466/public/comment", "forum": "SkuqA_cgx", "replyto": "S1Uhl7J7x", "signatures": ["~Philip_Blair1"], "readers": ["everyone"], "writers": ["~Philip_Blair1"], "content": {"title": "Second Round of Human Evaluations", "comment": "Thank you for the feedback! Yes, it is a concern that the reported human performance is quite low. We have taken your advice and performed a second set of human evaluations on a smaller dataset with more familiar topics. Performance (described in the updated paper) was drastically improved, with an overall human accuracy of 93% (each annotator missed exactly one of the 15 clusters) and 96.4% agreement among annotators. This significantly higher performance would seem to indicate that lack of domain knowledge was the primary cause of the low performance on the full dataset, which is further corroborated by inspecting the misses on the larger human evaluation. For example, on one of the most missed clusters, a large number of people falsely identified \"Paris green\" as an outlier among a set of insecticides (the true outlier was \"Bordeaux mixture,\" a fungicide). We hypothesize that many falsely associated the term \"Paris green\" with the color. On the other hand, if one had an intimate knowledge of pesticides, they would (in theory) be able to distinguish the true outlier. \n\nAnecdotally, we have found that most challenging test cases (such as the one above) often contain O1 outliers, for the distinction between them and the cluster entities can be quite subtle. We believe this is a strength of our approach, however, for embeddings are only able to do well on this dataset if they encode the domain-specific semantic meanings of words and phrases in addition to their usage in everyday speech."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations", "abstract": "We propose a language-agnostic way of automatically generating sets of semantically similar clusters of entities along with sets of \"outlier\" elements, which may then be used to perform an intrinsic evaluation of word embeddings in the outlier detection task. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results show a correlation between performance on this dataset and performance on sentiment analysis.", "pdf": "/pdf/5bc457660e09eee1aad834c5b9bcf018f51f21d1.pdf", "TL;DR": "Applying simple heuristics to the Wikidata entity graph results in a high-quality semantic similarity dataset.", "paperhash": "blair|automated_generation_of_multilingual_clusters_for_the_evaluation_of_distributed_representations", "keywords": ["Natural language processing", "Applications"], "conflicts": ["basistech.com", "neu.edu"], "authors": ["Philip Blair", "Yuval Merhav", "Joel Barry"], "authorids": ["pblair@basistech.com", "yuval@basistech.com", "joelb@basistech.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287565438, "id": "ICLR.cc/2017/conference/-/paper466/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkuqA_cgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper466/reviewers", "ICLR.cc/2017/conference/paper466/areachairs"], "cdate": 1485287565438}}}, {"tddate": null, "tmdate": 1480732826931, "tcdate": 1480732826925, "number": 2, "id": "Sy7ie31mg", "invitation": "ICLR.cc/2017/conference/-/paper466/pre-review/question", "forum": "SkuqA_cgx", "replyto": "SkuqA_cgx", "signatures": ["ICLR.cc/2017/conference/paper466/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper466/AnonReviewer1"], "content": {"title": "Reporting results on the different subsets of the data", "question": "This paper propose to evaluate word representations by detecting outliers from a set of words (or phrases). If I understand correctly, three different methods are proposed to generate outliers, based on the distance in the wikidata tree. These different ways to generate the data probably lead to outliers with different characteristics (in particular some might be easier to detect). Have the authors evaluated the word representations on the different subsets of the data?\n\nAs stated by the authors, another popular evaluation of word representations are analogy questions, introduced by Mikolov et al. Have the authors considered to compute the correlation between the proposed method and these analogy questions?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations", "abstract": "We propose a language-agnostic way of automatically generating sets of semantically similar clusters of entities along with sets of \"outlier\" elements, which may then be used to perform an intrinsic evaluation of word embeddings in the outlier detection task. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results show a correlation between performance on this dataset and performance on sentiment analysis.", "pdf": "/pdf/5bc457660e09eee1aad834c5b9bcf018f51f21d1.pdf", "TL;DR": "Applying simple heuristics to the Wikidata entity graph results in a high-quality semantic similarity dataset.", "paperhash": "blair|automated_generation_of_multilingual_clusters_for_the_evaluation_of_distributed_representations", "keywords": ["Natural language processing", "Applications"], "conflicts": ["basistech.com", "neu.edu"], "authors": ["Philip Blair", "Yuval Merhav", "Joel Barry"], "authorids": ["pblair@basistech.com", "yuval@basistech.com", "joelb@basistech.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959266564, "id": "ICLR.cc/2017/conference/-/paper466/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper466/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper466/AnonReviewer2", "ICLR.cc/2017/conference/paper466/AnonReviewer1"], "reply": {"forum": "SkuqA_cgx", "replyto": "SkuqA_cgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper466/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper466/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959266564}}}, {"tddate": null, "tmdate": 1480695982390, "tcdate": 1480695982385, "number": 1, "id": "S1Uhl7J7x", "invitation": "ICLR.cc/2017/conference/-/paper466/pre-review/question", "forum": "SkuqA_cgx", "replyto": "SkuqA_cgx", "signatures": ["ICLR.cc/2017/conference/paper466/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper466/AnonReviewer2"], "content": {"title": "Why is human performance so low?", "question": "I like the idea of evaluating via outlier detection, it seems like a very natural way to go. However, this intuition does not fit well with the human performance of 68.9%.\nIs this just an issue of broad domain? One way to test this would be to select a biased sample of clusters that tend to appear more in news corpora; e.g. a cluster of US presidents might be more familiar to the average annotator than a cluster of regions in Middle Earth.\nIf it is not a domain issue (after all, the annotators had access to Wikipedia), then this might indicate a problem in the dataset construction. Perhaps some of the outliers aren't different enough from the cluster? You might be able to check this via inter-annotator agreement per cluster (which is also interesting to report)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations", "abstract": "We propose a language-agnostic way of automatically generating sets of semantically similar clusters of entities along with sets of \"outlier\" elements, which may then be used to perform an intrinsic evaluation of word embeddings in the outlier detection task. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results show a correlation between performance on this dataset and performance on sentiment analysis.", "pdf": "/pdf/5bc457660e09eee1aad834c5b9bcf018f51f21d1.pdf", "TL;DR": "Applying simple heuristics to the Wikidata entity graph results in a high-quality semantic similarity dataset.", "paperhash": "blair|automated_generation_of_multilingual_clusters_for_the_evaluation_of_distributed_representations", "keywords": ["Natural language processing", "Applications"], "conflicts": ["basistech.com", "neu.edu"], "authors": ["Philip Blair", "Yuval Merhav", "Joel Barry"], "authorids": ["pblair@basistech.com", "yuval@basistech.com", "joelb@basistech.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959266564, "id": "ICLR.cc/2017/conference/-/paper466/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper466/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper466/AnonReviewer2", "ICLR.cc/2017/conference/paper466/AnonReviewer1"], "reply": {"forum": "SkuqA_cgx", "replyto": "SkuqA_cgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper466/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper466/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959266564}}}, {"tddate": null, "tmdate": 1478892948575, "tcdate": 1478892948571, "number": 1, "id": "S1ncpcX-g", "invitation": "ICLR.cc/2017/conference/-/paper466/public/comment", "forum": "SkuqA_cgx", "replyto": "SkuqA_cgx", "signatures": ["~Philip_Blair1"], "readers": ["everyone"], "writers": ["~Philip_Blair1"], "content": {"title": "Dataset Link", "comment": "It is mentioned in a footnote within the paper, but here is the link to the WikiSem500 dataset and a sample evaluation script: https://github.com/belph/wiki-sem-500"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations", "abstract": "We propose a language-agnostic way of automatically generating sets of semantically similar clusters of entities along with sets of \"outlier\" elements, which may then be used to perform an intrinsic evaluation of word embeddings in the outlier detection task. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results show a correlation between performance on this dataset and performance on sentiment analysis.", "pdf": "/pdf/5bc457660e09eee1aad834c5b9bcf018f51f21d1.pdf", "TL;DR": "Applying simple heuristics to the Wikidata entity graph results in a high-quality semantic similarity dataset.", "paperhash": "blair|automated_generation_of_multilingual_clusters_for_the_evaluation_of_distributed_representations", "keywords": ["Natural language processing", "Applications"], "conflicts": ["basistech.com", "neu.edu"], "authors": ["Philip Blair", "Yuval Merhav", "Joel Barry"], "authorids": ["pblair@basistech.com", "yuval@basistech.com", "joelb@basistech.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287565438, "id": "ICLR.cc/2017/conference/-/paper466/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkuqA_cgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper466/reviewers", "ICLR.cc/2017/conference/paper466/areachairs"], "cdate": 1485287565438}}}], "count": 12}