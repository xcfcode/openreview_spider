{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457651680745, "tcdate": 1457651680745, "id": "3QxzAmOmgCp7y9wltPQx", "invitation": "ICLR.cc/2016/workshop/-/paper/141/review/11", "forum": "E8VEozRYyi31v0m2iDwy", "replyto": "E8VEozRYyi31v0m2iDwy", "signatures": ["ICLR.cc/2016/workshop/paper/141/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/141/reviewer/11"], "content": {"title": "Interesting idea, explore further", "rating": "5: Marginally below acceptance threshold", "review": "This is a nice idea, but I am not sure if it is being used to do something really novel yet, in comparison with Siamese networks or convolutional autoencoders.\n\nThere seem to be two ways for the network to solve the high-res-patch/low-res-image matching problem.\nThe lazy way - downscale the patch, and compare it to small regions of the image.\nThe clever way - look at the patch, deduce what it is, i.e. dog fur, and then check if that matches the image, i.e. is\nit an image of a dog, or possibly a fur coat?\n\nIf the network is solving the problem the lazy way, then the results should be similar to what you would get using a\nconvolutional autoencoder. If it is solving the problem the clever way, then this a great new way to unsupervised\nlearning for images. The experimental evidence seems to be rather hastily put together; it is not clear yet that advantage is being taken of the difference in resolution patch-vs-image.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Deep Autoresolution Networks", "abstract": "Despite the success of very deep convolutional neural networks, they currently operate\nat very low resolutions relative to modern cameras. Visual attention mechanisms\naddress this by allowing models to access higher resolutions only when\nnecessary. However, in certain cases, this higher resolution isn\u2019t available. We\nshow that autoresolution networks, which learn correspondences between lowresolution\nand high-resolution images, learn representations that improve lowresolution\nclassification - without needing labeled high-resolution images.", "pdf": "/pdf/E8VEozRYyi31v0m2iDwy.pdf", "paperhash": "pereyra|deep_autoresolution_networks", "authorids": ["pereyra@google.com", "szegedy@google.com"], "conflicts": ["google.com", "usc.edu"], "authors": ["Gabriel Pereyra", "Christian Szegedy"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580034660, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580034660, "id": "ICLR.cc/2016/workshop/-/paper/141/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "E8VEozRYyi31v0m2iDwy", "replyto": "E8VEozRYyi31v0m2iDwy", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/141/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457594324243, "tcdate": 1457594324243, "id": "3Qxz3KoOWSp7y9wltPDr", "invitation": "ICLR.cc/2016/workshop/-/paper/141/review/10", "forum": "E8VEozRYyi31v0m2iDwy", "replyto": "E8VEozRYyi31v0m2iDwy", "signatures": ["ICLR.cc/2016/workshop/paper/141/reviewer/10"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/141/reviewer/10"], "content": {"title": "A reasonable but not fully-explored idea;  more comparisons would be helpful.", "rating": "6: Marginally above acceptance threshold", "review": "The paper proposes an unsupervised learning strategy that uses high-resolution and low-resolution image correspondence as a surrogate task to initialize networks for image classification.  Two towers are trained to produce a feature representation that allows a classifier to determine whether a cropped or blurred patch is a part of an original image.  Tests on CIFAR-10 show that the learned representation for the low-resolution representation is a useful initialization for classification.\n\nPros:\nOverall, this is an interesting unexplored criterion for pre-training a network (even though pre-training seems to have gone out of style).\nOn the experiments presented, the pre-trained solution is at least better than starting from random.\n\nCons:\n30% relative gain is a nice result, but this is against a non-pretrained-baseline.  How does this compare to stronger efforts that include some form of pretraining?  Considering the long list of methods that provide a gain for this benchmark, more comparisons seem important.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Deep Autoresolution Networks", "abstract": "Despite the success of very deep convolutional neural networks, they currently operate\nat very low resolutions relative to modern cameras. Visual attention mechanisms\naddress this by allowing models to access higher resolutions only when\nnecessary. However, in certain cases, this higher resolution isn\u2019t available. We\nshow that autoresolution networks, which learn correspondences between lowresolution\nand high-resolution images, learn representations that improve lowresolution\nclassification - without needing labeled high-resolution images.", "pdf": "/pdf/E8VEozRYyi31v0m2iDwy.pdf", "paperhash": "pereyra|deep_autoresolution_networks", "authorids": ["pereyra@google.com", "szegedy@google.com"], "conflicts": ["google.com", "usc.edu"], "authors": ["Gabriel Pereyra", "Christian Szegedy"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580034422, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580034422, "id": "ICLR.cc/2016/workshop/-/paper/141/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "E8VEozRYyi31v0m2iDwy", "replyto": "E8VEozRYyi31v0m2iDwy", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/141/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457046958528, "tcdate": 1457046958528, "id": "E8V39KGlYT31v0m2iDp0", "invitation": "ICLR.cc/2016/workshop/-/paper/141/comment", "forum": "E8VEozRYyi31v0m2iDwy", "replyto": "GvV45wgY5S1WDOmRiMpL", "signatures": ["~Gabriel_Pereyra1"], "readers": ["everyone"], "writers": ["~Gabriel_Pereyra1"], "content": {"title": "Model architecture and classification heatmaps", "comment": "Attached are links to the model architecture we used and qualitative examples of a trained model. \n\nFor the qualitative examples, the black and white square is a heat map, where white represents that the model predicted that a high-resolution patch from that location belonged to a low-resolution image (not shown). The examples with two pictures correspond to negative examples - the model should predict all black instead of all white. The results are after the model was trained on 20 million images on an internal Google dataset and the examples are all sampled from ImageNet's validation set.\n\nFor Cifar10, we are not aware of any similar results to compare with. Additionally, we would like to stress that the goal of our architecture is not to improve classification performance on Cifar10, this was just an easy way for us to demonstrate that our model learned useful representations.\n\nModel Architecture - https://www.dropbox.com/s/9c892ka2fjwoz0c/model_architecture.pdf?dl=0\n\nClassification Heatmaps - https://www.dropbox.com/s/yan30cs21n5cma4/classification_examples.html?dl=0"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Deep Autoresolution Networks", "abstract": "Despite the success of very deep convolutional neural networks, they currently operate\nat very low resolutions relative to modern cameras. Visual attention mechanisms\naddress this by allowing models to access higher resolutions only when\nnecessary. However, in certain cases, this higher resolution isn\u2019t available. We\nshow that autoresolution networks, which learn correspondences between lowresolution\nand high-resolution images, learn representations that improve lowresolution\nclassification - without needing labeled high-resolution images.", "pdf": "/pdf/E8VEozRYyi31v0m2iDwy.pdf", "paperhash": "pereyra|deep_autoresolution_networks", "authorids": ["pereyra@google.com", "szegedy@google.com"], "conflicts": ["google.com", "usc.edu"], "authors": ["Gabriel Pereyra", "Christian Szegedy"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455827361200, "ddate": null, "super": null, "final": null, "tcdate": 1455827361200, "id": "ICLR.cc/2016/workshop/-/paper/141/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "E8VEozRYyi31v0m2iDwy", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/141/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1456882042576, "tcdate": 1456882042576, "id": "GvV45wgY5S1WDOmRiMpL", "invitation": "ICLR.cc/2016/workshop/-/paper/141/review/12", "forum": "E8VEozRYyi31v0m2iDwy", "replyto": "E8VEozRYyi31v0m2iDwy", "signatures": ["ICLR.cc/2016/workshop/paper/141/reviewer/12"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/141/reviewer/12"], "content": {"title": "Good idea but missing some details", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposes deep auto-resolution networks using unsupervised context information. The model consists of two convolution towers, one with 17 convolution layers on high-resolution image patch and the other with 40 inception-style layers on low-resolution full image. The outputs of both towers are concatenated and fed into a classifier or a regressor. The classifier is used to predict whether the high-resolution image patch is in the image and the regressor is used to predict the location of the high-resolution image patch inside the image. No annotations are required to train this network and experimental results show that the 40 layer embedding tower outperforms a random initialized network on cifar10 dataset. \n\nThe strengths of this paper include:\n1) Learning from unsupervised context info is a very interesting topic. It enables us to train very deep networks to learn representations without any annotations. \n2) Experimental results show that the network can learn some representations and improve the classification task. \n\nI have the following concerns about this paper:\n1) There is no network architecture details in the paper. Without that information, it is hard for other people to reproduce the results. \n2) Also, no results reported in terms of the accuracy or error for the classifier and regressor net. It would be great to know how accurate the network can predict the context info and how much training data required to learn it.\n3) The experiments only compare random initialization and the pre-trained auto resolution network. It would be great to compare with other unsupervised network as pretrained networks.  \n\nIn general, I think the idea of this paper is great but the presentation and experiments are not satisfactory. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Deep Autoresolution Networks", "abstract": "Despite the success of very deep convolutional neural networks, they currently operate\nat very low resolutions relative to modern cameras. Visual attention mechanisms\naddress this by allowing models to access higher resolutions only when\nnecessary. However, in certain cases, this higher resolution isn\u2019t available. We\nshow that autoresolution networks, which learn correspondences between lowresolution\nand high-resolution images, learn representations that improve lowresolution\nclassification - without needing labeled high-resolution images.", "pdf": "/pdf/E8VEozRYyi31v0m2iDwy.pdf", "paperhash": "pereyra|deep_autoresolution_networks", "authorids": ["pereyra@google.com", "szegedy@google.com"], "conflicts": ["google.com", "usc.edu"], "authors": ["Gabriel Pereyra", "Christian Szegedy"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580033775, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580033775, "id": "ICLR.cc/2016/workshop/-/paper/141/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "E8VEozRYyi31v0m2iDwy", "replyto": "E8VEozRYyi31v0m2iDwy", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/141/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455827359470, "tcdate": 1455827359470, "id": "E8VEozRYyi31v0m2iDwy", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "E8VEozRYyi31v0m2iDwy", "signatures": ["~Gabriel_Pereyra1"], "readers": ["everyone"], "writers": ["~Gabriel_Pereyra1"], "content": {"CMT_id": "", "title": "Deep Autoresolution Networks", "abstract": "Despite the success of very deep convolutional neural networks, they currently operate\nat very low resolutions relative to modern cameras. Visual attention mechanisms\naddress this by allowing models to access higher resolutions only when\nnecessary. However, in certain cases, this higher resolution isn\u2019t available. We\nshow that autoresolution networks, which learn correspondences between lowresolution\nand high-resolution images, learn representations that improve lowresolution\nclassification - without needing labeled high-resolution images.", "pdf": "/pdf/E8VEozRYyi31v0m2iDwy.pdf", "paperhash": "pereyra|deep_autoresolution_networks", "authorids": ["pereyra@google.com", "szegedy@google.com"], "conflicts": ["google.com", "usc.edu"], "authors": ["Gabriel Pereyra", "Christian Szegedy"]}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 5}