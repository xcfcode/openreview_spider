{"notes": [{"id": "qZzy5urZw9", "original": "g8SBo9Gvz-07", "number": 2638, "cdate": 1601308292106, "ddate": null, "tcdate": 1601308292106, "tmdate": 1614207554410, "tddate": null, "forum": "qZzy5urZw9", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Robust Overfitting may be mitigated by properly learned smoothening", "authorids": ["~Tianlong_Chen1", "~Zhenyu_Zhang4", "~Sijia_Liu1", "~Shiyu_Chang2", "~Zhangyang_Wang1"], "authors": ["Tianlong Chen", "Zhenyu Zhang", "Sijia Liu", "Shiyu Chang", "Zhangyang Wang"], "keywords": ["Robust Overfitting", "Adversarial Training", "Adversarial Robustness"], "abstract": "A recent study (Rice et al.,  2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overfitting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overfitting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by $3.72\\%\\sim6.68\\%$ and robust accuracy by $0.22\\%\\sim2 .03\\%$, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types ($\\ell_{\\infty}$ and $\\ell_2$), and robustified methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models' robustness against transfer attacks. Codes are available at https://github.com/VITA-Group/Alleviate-Robust-Overfitting.", "one-sentence_summary": "Mitigate robust overfitting by properly learned smoothening, establishing the new state-of-the-art bar in adversarial training", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|robust_overfitting_may_be_mitigated_by_properly_learned_smoothening", "pdf": "/pdf/099a32f12e88483a4451fe099750daeb8a1a0128.pdf", "supplementary_material": "/attachment/a5c269fbfe37fcb546d8db23c18c46f1442eccbf.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021robust,\ntitle={Robust Overfitting may be mitigated by properly learned smoothening},\nauthor={Tianlong Chen and Zhenyu Zhang and Sijia Liu and Shiyu Chang and Zhangyang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=qZzy5urZw9}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "iV2_2Qg2rm9", "original": null, "number": 1, "cdate": 1610040430539, "ddate": null, "tcdate": 1610040430539, "tmdate": 1610474030463, "tddate": null, "forum": "qZzy5urZw9", "replyto": "qZzy5urZw9", "invitation": "ICLR.cc/2021/Conference/Paper2638/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper focuses on the problem of robust overfitting. The philosophy behind sounds quite interesting to me, namely, injecting \nmore learned smoothening during adversarial training. This philosophy leads to two simple yet effective methods: one leveraging knowledge distillation and self-training to smooth the logits, and the other performing stochastic weight averaging to smooth the weights.\n\nThe clarity and novelty are above the bar of ICLR. While the reviewers had some concerns on the significance, the authors did a particularly good job in their rebuttal. Thus, all of us have agreed to accept this paper for publication! Please carefully address all \ncomments in the final version."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Overfitting may be mitigated by properly learned smoothening", "authorids": ["~Tianlong_Chen1", "~Zhenyu_Zhang4", "~Sijia_Liu1", "~Shiyu_Chang2", "~Zhangyang_Wang1"], "authors": ["Tianlong Chen", "Zhenyu Zhang", "Sijia Liu", "Shiyu Chang", "Zhangyang Wang"], "keywords": ["Robust Overfitting", "Adversarial Training", "Adversarial Robustness"], "abstract": "A recent study (Rice et al.,  2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overfitting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overfitting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by $3.72\\%\\sim6.68\\%$ and robust accuracy by $0.22\\%\\sim2 .03\\%$, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types ($\\ell_{\\infty}$ and $\\ell_2$), and robustified methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models' robustness against transfer attacks. Codes are available at https://github.com/VITA-Group/Alleviate-Robust-Overfitting.", "one-sentence_summary": "Mitigate robust overfitting by properly learned smoothening, establishing the new state-of-the-art bar in adversarial training", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|robust_overfitting_may_be_mitigated_by_properly_learned_smoothening", "pdf": "/pdf/099a32f12e88483a4451fe099750daeb8a1a0128.pdf", "supplementary_material": "/attachment/a5c269fbfe37fcb546d8db23c18c46f1442eccbf.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021robust,\ntitle={Robust Overfitting may be mitigated by properly learned smoothening},\nauthor={Tianlong Chen and Zhenyu Zhang and Sijia Liu and Shiyu Chang and Zhangyang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=qZzy5urZw9}\n}"}, "tags": [], "invitation": {"reply": {"forum": "qZzy5urZw9", "replyto": "qZzy5urZw9", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040430525, "tmdate": 1610474030447, "id": "ICLR.cc/2021/Conference/Paper2638/-/Decision"}}}, {"id": "u_PP25tvI", "original": null, "number": 1, "cdate": 1603768058059, "ddate": null, "tcdate": 1603768058059, "tmdate": 1606756538732, "tddate": null, "forum": "qZzy5urZw9", "replyto": "qZzy5urZw9", "invitation": "ICLR.cc/2021/Conference/Paper2638/-/Official_Review", "content": {"title": "An interesting potential approach to prevent robust overfitting", "review": "The paper studies a method for mitigating robust overfitting. Rice et al., and others have observed that when training a neural network robustly on say CIFAR10, then the robust test error often overfits, i.e., it has a U-shaped curve as a function of training epochs. Rice et al. demonstrated that early stopping the robust training enables state-of-the-art robust performance. However, to realize this performance, it is necessary to find a good early stopping point, which can be difficult (but can be found with testing on a validation set). The paper proposes an alternative to early stopping: smoothing the logits and smoothing the weights, by using  two existing techniques, namely self-training and stochastic weight averaging. The paper finds that smoothing mitigates robust overfitting, and reports even a slight improvement over early stopping at the optimal point.\n\nStrength:\n- The paper proposes two existing techniques to prevent robust overfitting:  self-training and stochastic weight averaging, and shows that combining those two approaches is very effective in preventing robust overfitting for ResNet-18.\n- The paper is well written and easy to follow.\n\nWeaknesses:\n- All experiments are carried out with one network only: ResNet-10. To validate the claim that learned label smoothing can mitigate robust training, is important to test the label smoothing method on a variety of setups and models.\n- The improvement are tiny relativ to early stopping, and another submission to this workshop (https://openreview.net/pdf?id=Xb8xvrtB8Ce) has shown that the baseline the paper under review is comparing to (the setting of Rice et al.) is quite brittle relative to choices of hyperparameters such as slight differences in weight decay. Therefore, the gains obtained by the paper under review could very well be subsumed by slightly tuning early stopping.\n\nSummary: It is an important problem to study methods that mitigate robust overfitting, and the paper proposes a combination of two smoothing techniques and demonstrates its effectiveness through extensive experiments. I'm therefore leaning to recommend acceptance of this paper, however, as mentioned, the paper's results might not generalize to other models and the slight gains over early stopping might be void by slightly tuning ES better. Therefore, I would not be upset if this paper were rejected. In any case, the paper's results would be more convincing if it would contain results for different models (e.g., VGG and other deep nets but also simple baseline models such as random feature models), and if it would contain a simple theoretical statement to provide intuition why label smoothing should help. \n\n---- \nUPDATE: Thanks; I have read the response, kept my score, and responded below.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2638/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Overfitting may be mitigated by properly learned smoothening", "authorids": ["~Tianlong_Chen1", "~Zhenyu_Zhang4", "~Sijia_Liu1", "~Shiyu_Chang2", "~Zhangyang_Wang1"], "authors": ["Tianlong Chen", "Zhenyu Zhang", "Sijia Liu", "Shiyu Chang", "Zhangyang Wang"], "keywords": ["Robust Overfitting", "Adversarial Training", "Adversarial Robustness"], "abstract": "A recent study (Rice et al.,  2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overfitting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overfitting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by $3.72\\%\\sim6.68\\%$ and robust accuracy by $0.22\\%\\sim2 .03\\%$, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types ($\\ell_{\\infty}$ and $\\ell_2$), and robustified methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models' robustness against transfer attacks. Codes are available at https://github.com/VITA-Group/Alleviate-Robust-Overfitting.", "one-sentence_summary": "Mitigate robust overfitting by properly learned smoothening, establishing the new state-of-the-art bar in adversarial training", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|robust_overfitting_may_be_mitigated_by_properly_learned_smoothening", "pdf": "/pdf/099a32f12e88483a4451fe099750daeb8a1a0128.pdf", "supplementary_material": "/attachment/a5c269fbfe37fcb546d8db23c18c46f1442eccbf.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021robust,\ntitle={Robust Overfitting may be mitigated by properly learned smoothening},\nauthor={Tianlong Chen and Zhenyu Zhang and Sijia Liu and Shiyu Chang and Zhangyang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=qZzy5urZw9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "qZzy5urZw9", "replyto": "qZzy5urZw9", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2638/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538091830, "tmdate": 1606915781071, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2638/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2638/-/Official_Review"}}}, {"id": "Ha8AU0FU9oP", "original": null, "number": 3, "cdate": 1603847959725, "ddate": null, "tcdate": 1603847959725, "tmdate": 1606613837769, "tddate": null, "forum": "qZzy5urZw9", "replyto": "qZzy5urZw9", "invitation": "ICLR.cc/2021/Conference/Paper2638/-/Official_Review", "content": {"title": "Studies smoothing approaches from standard training to reduce robust overfitting", "review": "Summary\n=======\nThe paper leverages two methods for improving generalization in standard training, logit smoothing and stochastic weight averaging, and show that these results can mitigate robust overfitting and improve generalization for adversarial training methods. \n\nOverall, the paper was clear and easy to follow. There are a number of ablation studies showing the marginal effects of the two methods, as well as experiments demonstrating how the approaches vary with certain choices in methodology. My initial impression is positive, though there are certain changes described below which would help solidify the paper and its claims. \n\n\nComments for discussion\n=======================\nBy improving upon the results in Rice et al. 2020, the authors purport to have state of the art results. However, there's be a plethora of new work since then which have improved these numbers even further. Fortunately, since the submitted work handles the standard CIFAR10 setting, there are a number of public benchmarks that can be used here. It would be great if the authors could train a comparable model and evaluate it using one of these benchmarks (e.g. the autoattack framework at https://github.com/fra31/auto-attack). \n\nTo be clear, since a number of these approaches on the benchmark are quite recent, I am not requesting that the authors directly compare to these new methods in their work. However, the baseline that they do compare to (e.g. Rice et al. 2020) is evaluated in this framework (and had a not-insignifcant drop in robust accuracy), so it would be of significant utility to also evaluate the approach using the improved attack. Performing this evaluation would serve two purposes: \n\n1. This should alleviate most concerns on the validity of the result \n2. This makes the work easily comparable for future work \n\nNote that reaching the top of the benchmark is not a requirement for publication. As long as it is consistent with the claims of the paper, that the approach reduces robust overfitting for PGD training and improves upon the PGD baseline within this benchmark, then this is fine. If the authors can report how their approach performs under this improved evaluation or a comparable alternative, then I am happy to adjust my score accordingly. However, the authors probably shouldn't claim state-of-the-art performance without doing this evaluation first. \n\n\nMinor comments\n==============\n+ In section 3.2, it is mentioned that Table 2 supposedly shows differences when a robust self-teacher is pretrained, but this does not seem to be the case. \n\n\nUpdate\n======\nI have looked through the response and edited version. The updated evaluation looks solid and provides a potential solution to a robust overfitting problem. Although the work is primarily empirical in nature, it may inspire directions for future work to look into more theoretical explanations of robust overfitting. ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2638/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Overfitting may be mitigated by properly learned smoothening", "authorids": ["~Tianlong_Chen1", "~Zhenyu_Zhang4", "~Sijia_Liu1", "~Shiyu_Chang2", "~Zhangyang_Wang1"], "authors": ["Tianlong Chen", "Zhenyu Zhang", "Sijia Liu", "Shiyu Chang", "Zhangyang Wang"], "keywords": ["Robust Overfitting", "Adversarial Training", "Adversarial Robustness"], "abstract": "A recent study (Rice et al.,  2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overfitting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overfitting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by $3.72\\%\\sim6.68\\%$ and robust accuracy by $0.22\\%\\sim2 .03\\%$, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types ($\\ell_{\\infty}$ and $\\ell_2$), and robustified methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models' robustness against transfer attacks. Codes are available at https://github.com/VITA-Group/Alleviate-Robust-Overfitting.", "one-sentence_summary": "Mitigate robust overfitting by properly learned smoothening, establishing the new state-of-the-art bar in adversarial training", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|robust_overfitting_may_be_mitigated_by_properly_learned_smoothening", "pdf": "/pdf/099a32f12e88483a4451fe099750daeb8a1a0128.pdf", "supplementary_material": "/attachment/a5c269fbfe37fcb546d8db23c18c46f1442eccbf.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021robust,\ntitle={Robust Overfitting may be mitigated by properly learned smoothening},\nauthor={Tianlong Chen and Zhenyu Zhang and Sijia Liu and Shiyu Chang and Zhangyang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=qZzy5urZw9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "qZzy5urZw9", "replyto": "qZzy5urZw9", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2638/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538091830, "tmdate": 1606915781071, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2638/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2638/-/Official_Review"}}}, {"id": "fYSDz0hKd21", "original": null, "number": 14, "cdate": 1606159224103, "ddate": null, "tcdate": 1606159224103, "tmdate": 1606159224103, "tddate": null, "forum": "qZzy5urZw9", "replyto": "qZzy5urZw9", "invitation": "ICLR.cc/2021/Conference/Paper2638/-/Official_Comment", "content": {"title": "General Response", "comment": "We sincerely appreciate all reviewers\u2019 time and efforts in reviewing our paper. We truly thank reviewer #3 for the acknowledgment of our extra experiments and explanations, and for increasing the score of our paper. We genuinely appreciate the positive initial impression from reviewer #1 and #2. And we also thank all reviewers for their insightful and constructive suggestions, which help a lot in further improving our paper. \n\nIn addition to the pointwise responses below, here we summarize our updates.\n\n- **[Extra Experiments]** As mentioned by all three reviewers, we conduct new experiments to evaluate our approaches under improved attacks, e.g. Auto-Attack and CW attack. In the meantime, we apply our methods with VGG-16, Wide-ResNet-34-4, and Wide-ResNet-34-10. The results demonstrate that our approaches are effective under improved attacks and can be generalized to different models.\n\n- **[Modified Draft]** The modified nine pages draft is updated, including new experiment results and references. We will keep updating for better and clear readability.\n\n- **[Reproducibility]** Both training and evaluation codes for KD&SWA have been provided as additional supplementary material and the pre-trained models with ResNet18 on CIFAR10 can be found at https://www.dropbox.com/sh/1htrkwawfqh2hem/AAAOIz2A7ndA9VLyA6X_BXSta?dl=0.\n\nWe hope our pointwise responses below could clarify all reviewers\u2019 confusion and alleviate all concerns. We thank all reviewers\u2019 time again.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2638/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Overfitting may be mitigated by properly learned smoothening", "authorids": ["~Tianlong_Chen1", "~Zhenyu_Zhang4", "~Sijia_Liu1", "~Shiyu_Chang2", "~Zhangyang_Wang1"], "authors": ["Tianlong Chen", "Zhenyu Zhang", "Sijia Liu", "Shiyu Chang", "Zhangyang Wang"], "keywords": ["Robust Overfitting", "Adversarial Training", "Adversarial Robustness"], "abstract": "A recent study (Rice et al.,  2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overfitting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overfitting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by $3.72\\%\\sim6.68\\%$ and robust accuracy by $0.22\\%\\sim2 .03\\%$, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types ($\\ell_{\\infty}$ and $\\ell_2$), and robustified methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models' robustness against transfer attacks. Codes are available at https://github.com/VITA-Group/Alleviate-Robust-Overfitting.", "one-sentence_summary": "Mitigate robust overfitting by properly learned smoothening, establishing the new state-of-the-art bar in adversarial training", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|robust_overfitting_may_be_mitigated_by_properly_learned_smoothening", "pdf": "/pdf/099a32f12e88483a4451fe099750daeb8a1a0128.pdf", "supplementary_material": "/attachment/a5c269fbfe37fcb546d8db23c18c46f1442eccbf.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021robust,\ntitle={Robust Overfitting may be mitigated by properly learned smoothening},\nauthor={Tianlong Chen and Zhenyu Zhang and Sijia Liu and Shiyu Chang and Zhangyang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=qZzy5urZw9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "qZzy5urZw9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2638/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2638/Authors|ICLR.cc/2021/Conference/Paper2638/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846047, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2638/-/Official_Comment"}}}, {"id": "in8QAuRcMU", "original": null, "number": 2, "cdate": 1603782860991, "ddate": null, "tcdate": 1603782860991, "tmdate": 1606143890874, "tddate": null, "forum": "qZzy5urZw9", "replyto": "qZzy5urZw9", "invitation": "ICLR.cc/2021/Conference/Paper2638/-/Official_Review", "content": {"title": "This paper uses logit smoothing and weight averaging to enhance adversarial training. ", "review": "#########################################################################\n\nSummary:\nThis paper uses the existing tricks that can enhance the standard training, to show that combining some of those tricks (in this paper, labels/logits smoothing and weight averaging) can improve adversarial training. \n\n#########################################################################\n\nPros: \n1 Compared with existing weight manipulation AT methods, this paper first utilizes stochastic weight averaging (SWA) (averaging multiple checkpoints along the training trajectory) without incurring computational overhead. \n\n2 This paper conducted experiments across four different datasets.\n\n#########################################################################\n\nCons: \n1 The paper\u2019s novelty is marginal. Specifically, first, label/logit smoothing has been demonstrated effective in adversarial training due to the better separation of different classes. For example, to my knowledge, three papers got accepted with the shared philosophy but slightly different techniques/decorations [1, 2, 3] \nSecond, as the authors mentioned, manipulating model weights is also shown effective [4].\nTherefore, this paper's conceptual improvements are marginal.  \n\n[1] Metric Learning for Adversarial Robustness, NeurIPS 2019\\\n[2] Rethinking Softmax Cross-Entropy Loss for Adversarial Robustness, ICLR 2020\\\n[3] Boosting Adversarial Training with Hypersphere Embedding, NeurIPS 2020\\\n[4] Revisiting loss landscape for adversarial robustness, NeurIPS 2020\n\n2 This paper hypothesizes \u201cone source of robust overfitting might lie in that the model \u2018overfits\u2019 the attacks generated in the early stage of AT and fails to generalize or adapt to the attacks in the late stage.\u201d  It is not clear to me why this hypothesis is valid. Would you explain more justifications?\n\n3. In Figure 3, are there any experimental results for SWA SA and SWA RA? \nBesides, more robustness evaluations are needed, e.g., CW attack, AA attack, Guided Adversarial Margin Attack. \nMore adversarial training on different network structures are needed, e.g., Wide ResNet. \n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2638/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Overfitting may be mitigated by properly learned smoothening", "authorids": ["~Tianlong_Chen1", "~Zhenyu_Zhang4", "~Sijia_Liu1", "~Shiyu_Chang2", "~Zhangyang_Wang1"], "authors": ["Tianlong Chen", "Zhenyu Zhang", "Sijia Liu", "Shiyu Chang", "Zhangyang Wang"], "keywords": ["Robust Overfitting", "Adversarial Training", "Adversarial Robustness"], "abstract": "A recent study (Rice et al.,  2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overfitting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overfitting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by $3.72\\%\\sim6.68\\%$ and robust accuracy by $0.22\\%\\sim2 .03\\%$, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types ($\\ell_{\\infty}$ and $\\ell_2$), and robustified methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models' robustness against transfer attacks. Codes are available at https://github.com/VITA-Group/Alleviate-Robust-Overfitting.", "one-sentence_summary": "Mitigate robust overfitting by properly learned smoothening, establishing the new state-of-the-art bar in adversarial training", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|robust_overfitting_may_be_mitigated_by_properly_learned_smoothening", "pdf": "/pdf/099a32f12e88483a4451fe099750daeb8a1a0128.pdf", "supplementary_material": "/attachment/a5c269fbfe37fcb546d8db23c18c46f1442eccbf.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021robust,\ntitle={Robust Overfitting may be mitigated by properly learned smoothening},\nauthor={Tianlong Chen and Zhenyu Zhang and Sijia Liu and Shiyu Chang and Zhangyang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=qZzy5urZw9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "qZzy5urZw9", "replyto": "qZzy5urZw9", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2638/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538091830, "tmdate": 1606915781071, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2638/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2638/-/Official_Review"}}}, {"id": "nyAzrOQHKS", "original": null, "number": 13, "cdate": 1606143863649, "ddate": null, "tcdate": 1606143863649, "tmdate": 1606143863649, "tddate": null, "forum": "qZzy5urZw9", "replyto": "SFS22lWfXcF", "invitation": "ICLR.cc/2021/Conference/Paper2638/-/Official_Comment", "content": {"title": "The current explanations are good", "comment": "I am satisfied with the added explanations and experiments. \nI believe the authors' responses have further enhanced their work.\nTherefore, I vote for acceptance and increase my score further to 7. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2638/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Overfitting may be mitigated by properly learned smoothening", "authorids": ["~Tianlong_Chen1", "~Zhenyu_Zhang4", "~Sijia_Liu1", "~Shiyu_Chang2", "~Zhangyang_Wang1"], "authors": ["Tianlong Chen", "Zhenyu Zhang", "Sijia Liu", "Shiyu Chang", "Zhangyang Wang"], "keywords": ["Robust Overfitting", "Adversarial Training", "Adversarial Robustness"], "abstract": "A recent study (Rice et al.,  2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overfitting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overfitting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by $3.72\\%\\sim6.68\\%$ and robust accuracy by $0.22\\%\\sim2 .03\\%$, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types ($\\ell_{\\infty}$ and $\\ell_2$), and robustified methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models' robustness against transfer attacks. Codes are available at https://github.com/VITA-Group/Alleviate-Robust-Overfitting.", "one-sentence_summary": "Mitigate robust overfitting by properly learned smoothening, establishing the new state-of-the-art bar in adversarial training", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|robust_overfitting_may_be_mitigated_by_properly_learned_smoothening", "pdf": "/pdf/099a32f12e88483a4451fe099750daeb8a1a0128.pdf", "supplementary_material": "/attachment/a5c269fbfe37fcb546d8db23c18c46f1442eccbf.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021robust,\ntitle={Robust Overfitting may be mitigated by properly learned smoothening},\nauthor={Tianlong Chen and Zhenyu Zhang and Sijia Liu and Shiyu Chang and Zhangyang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=qZzy5urZw9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "qZzy5urZw9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2638/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2638/Authors|ICLR.cc/2021/Conference/Paper2638/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846047, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2638/-/Official_Comment"}}}, {"id": "qr7e3JyR0Tn", "original": null, "number": 3, "cdate": 1605679551408, "ddate": null, "tcdate": 1605679551408, "tmdate": 1606065287084, "tddate": null, "forum": "qZzy5urZw9", "replyto": "in8QAuRcMU", "invitation": "ICLR.cc/2021/Conference/Paper2638/-/Official_Comment", "content": {"title": "(Continued) Response to Reviewer #3 [Cons 3] ", "comment": "[Cons 3: More robustness evaluations and more network structures.] Thanks for the suggestion. We add the plots of SWA SA/RA in Figure 3 in the updated draft. SWA alone also helps mitigate robust overfitting but does not outperform KD&SWA. This indicates that both KD and SWA provide supplementary benefits to mitigate robust overfitting. Besides, we are evaluating our approach under the Auto-Attack and CW attack on ResNet-18 (in Table S2), and conducting more experiments with VGG16, Wide-ResNet-34-4, and Wide-ResNet-34-10 (in Table S1). Specifically, as shown in Table S2, our methods are effective under both Auto-Attack and CW Attack. Take the experiment on CIFAR-10 with $\\ell_\\infty$ adversary as an example, our approach shrinks the robust accuracy gap between the best checkpoint and the final model from 7.04% to -0.09% under Auto-Attack and 14.96% to 0.79% under CW Attack. And Table S1 shows our methods can be generalized across different architectures. All results have been added to this response and our draft.\n\n**Table S1.** Performance showing the occurrence of robust overfitting across different architectures and the effectiveness of our proposed remedies under $\\ell_\\infty$ PGD-20 adversary. We pick the checkpoint which has the best robust accuracy on the validation set.\n\n|Architecture|Dataset|Settings|Robust Accuray(RA)(Best->Final)| Standard Accuracy(SA)(Best->Final)|\n|:-:|:-:|:-:|:-:|:-:|\n|VGG-16|CIFAR-10|Baseline|(46.42\u219240.59)(\u21935.83)|(75.29\u219279.54)(\u21914.25)|\n|VGG-16|CIFAR-10|Our Methods|(48.99\u219248.93)(\u21930.06)|(79.00\u219279.69)(\u21910.69)|\n|VGG-16|CIFAR-100|Baseline|(21.64\u219217.43)(\u21934.21)|(39.26\u219245.84)(\u21916.58)|\n|VGG-16|CIFAR-100|Our Methods|(24.79\u219224.73)(\u21930.06)|(48.20\u219249.00)(\u21910.80)|\n|WideResNet-34-4|CIFAR-10|Baseline|(52.59\u219243.06)(\u21939.53)|(81.53\u219283.28)(\u21911.75)|\n|WideResNet-34-4|CIFAR-10|Our Methods|(54.28\u219253.90)(\u21930.38)|(85.17\u219285.50)(\u21910.33)|\n|WideResNet-34-4|CIFAR-100|Baseline|(28.02\u219220.61)(\u21937.41)|(53.19\u219253.63)(\u21910.44)|\n|WideResNet-34-4|CIFAR-100|Our Methods|(30.10\u219229.80)(\u21930.30)|(57.23\u219258.05)(\u21910.82)|\n|WideResNet-34-10|CIFAR-10|Baseline|(54.27\u219247.12)(\u21937.15)|(84.16\u219285.72)(\u21911.56)|\n|WideResNet-34-10|CIFAR-10|Our Methods|(55.50\u219255.34)(\u21930.16)|(86.81\u219287.06)(\u21910.25)|\n|WideResNet-34-10|CIFAR-100|Baseline|(29.95\u219224.02)(\u21935.93)|(56.56\u219256.42)(\u21930.14)|\n|WideResNet-34-10|CIFAR-100|Our Methods|(31.93\u219231.51)(\u21930.42)|(60.86\u219261.78)(\u21910.92)|\n\n\n**Table S2.** Robust accuracy under Auto-Attack and CW Attack on CIFAR-10/100 with ResNet18. The best checkpoint is picked with the best robust accuracy under PGD-20 on the validation set. We follow the same setting as [6] for CW Attack: 1 search step on C with an initial constant of 0.1, with 100 iterations for each search step and learning rate is 0.01.\n\n|Dataset|Norm|Settings|Auto-Attack(Best->Final)|CW Attack(Best->Final)|\n|:-:|:-:|:-:|:-:|:-:|\n|CIFAR-10|$\\ell_\\infty$|Baseline|(47.00\u219239.96)(\u21937.04)|(75.48\u219260.52)(\u219314.96)|\n|CIFAR-10|$\\ell_\\infty$|Our Methods|(49.35\u219249.44)(\u21910.09)|(77.83\u219277.04)(\u21930.79)|\n|CIFAR-10|$\\ell_2$|Baseline|(67.18\u219264.29)(\u21932.89)|(73.80\u219253.77)(\u219320.03)|\n|CIFAR-10|$\\ell_2$|Our Methods|(68.87\u219268.90)(\u21910.03)|(73.89\u219273.79)(\u21930.10)|\n|CIFAR-100|$\\ell_\\infty$|Baseline|(22.73\u219218.11)(\u21934.62)|(45.89\u219237.76)(\u21938.13)|\n|CIFAR-100|$\\ell_\\infty$|Our Methods|(25.42\u219225.35)(\u21930.07)|(49.46\u219249.07)(\u21930.39)|\n|CIFAR-100|$\\ell_2$|Baseline|(37.16\u219233.43)(\u21933.73)|(48.43\u219237.73)(\u219310.70)|\n|CIFAR-100|$\\ell_2$|Our Methods|(40.56\u219240.61)(\u21910.05)|(51.02\u219250.90)(\u21930.12)|\n\n[6] Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2638/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Overfitting may be mitigated by properly learned smoothening", "authorids": ["~Tianlong_Chen1", "~Zhenyu_Zhang4", "~Sijia_Liu1", "~Shiyu_Chang2", "~Zhangyang_Wang1"], "authors": ["Tianlong Chen", "Zhenyu Zhang", "Sijia Liu", "Shiyu Chang", "Zhangyang Wang"], "keywords": ["Robust Overfitting", "Adversarial Training", "Adversarial Robustness"], "abstract": "A recent study (Rice et al.,  2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overfitting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overfitting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by $3.72\\%\\sim6.68\\%$ and robust accuracy by $0.22\\%\\sim2 .03\\%$, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types ($\\ell_{\\infty}$ and $\\ell_2$), and robustified methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models' robustness against transfer attacks. Codes are available at https://github.com/VITA-Group/Alleviate-Robust-Overfitting.", "one-sentence_summary": "Mitigate robust overfitting by properly learned smoothening, establishing the new state-of-the-art bar in adversarial training", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|robust_overfitting_may_be_mitigated_by_properly_learned_smoothening", "pdf": "/pdf/099a32f12e88483a4451fe099750daeb8a1a0128.pdf", "supplementary_material": "/attachment/a5c269fbfe37fcb546d8db23c18c46f1442eccbf.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021robust,\ntitle={Robust Overfitting may be mitigated by properly learned smoothening},\nauthor={Tianlong Chen and Zhenyu Zhang and Sijia Liu and Shiyu Chang and Zhangyang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=qZzy5urZw9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "qZzy5urZw9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2638/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2638/Authors|ICLR.cc/2021/Conference/Paper2638/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846047, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2638/-/Official_Comment"}}}, {"id": "J20Fi7qHxXz", "original": null, "number": 12, "cdate": 1606060606681, "ddate": null, "tcdate": 1606060606681, "tmdate": 1606064915343, "tddate": null, "forum": "qZzy5urZw9", "replyto": "lEpEh1jOJGe", "invitation": "ICLR.cc/2021/Conference/Paper2638/-/Official_Comment", "content": {"title": "Response to Reviewer #2 [More Extra Experiments are in the Other Part of Our Response]", "comment": "Thanks for your prompt reply. In fact, we believe our other part of the response, \u201cResponse to Reviewer #2 [Cons 1]\u201d posted on 20 Nov 2020, had already addressed your concerns. \n\nWe have conducted new experiments using VGG16, Wide-ResNet-34-4, and Wide-ResNet-34-10 on the CIFAR-10/100 dataset and report the results in Table S1. For example, in VGG16, our methods reduce the gap between the RA best checkpoint and the final epochs from 5.83% to 0.06% on CIFAR-10 and 4.21% to 0.06% on CIFAR-100. Meanwhile, our methods gain an extra robustness improvement (2.57% on CIFAR-10 and 3.15% on CIFAR-100) compared with early stopping. Consistent improvements can also be observed with Wide-ResNets. In addition, we conduct more experiments to verify our approaches under Auto-Attack and CW attack, which is shown in Table S2. Evaluated under the more rigorous attack method, i.e. Auto-Attack, our methods can still be effective. \n\nWe invite you to take another look at that part and let us know if you have further questions. Thanks!"}, "signatures": ["ICLR.cc/2021/Conference/Paper2638/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Overfitting may be mitigated by properly learned smoothening", "authorids": ["~Tianlong_Chen1", "~Zhenyu_Zhang4", "~Sijia_Liu1", "~Shiyu_Chang2", "~Zhangyang_Wang1"], "authors": ["Tianlong Chen", "Zhenyu Zhang", "Sijia Liu", "Shiyu Chang", "Zhangyang Wang"], "keywords": ["Robust Overfitting", "Adversarial Training", "Adversarial Robustness"], "abstract": "A recent study (Rice et al.,  2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overfitting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overfitting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by $3.72\\%\\sim6.68\\%$ and robust accuracy by $0.22\\%\\sim2 .03\\%$, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types ($\\ell_{\\infty}$ and $\\ell_2$), and robustified methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models' robustness against transfer attacks. Codes are available at https://github.com/VITA-Group/Alleviate-Robust-Overfitting.", "one-sentence_summary": "Mitigate robust overfitting by properly learned smoothening, establishing the new state-of-the-art bar in adversarial training", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|robust_overfitting_may_be_mitigated_by_properly_learned_smoothening", "pdf": "/pdf/099a32f12e88483a4451fe099750daeb8a1a0128.pdf", "supplementary_material": "/attachment/a5c269fbfe37fcb546d8db23c18c46f1442eccbf.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021robust,\ntitle={Robust Overfitting may be mitigated by properly learned smoothening},\nauthor={Tianlong Chen and Zhenyu Zhang and Sijia Liu and Shiyu Chang and Zhangyang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=qZzy5urZw9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "qZzy5urZw9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2638/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2638/Authors|ICLR.cc/2021/Conference/Paper2638/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846047, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2638/-/Official_Comment"}}}, {"id": "UWOsN0mTba3", "original": null, "number": 8, "cdate": 1605894556284, "ddate": null, "tcdate": 1605894556284, "tmdate": 1606064891597, "tddate": null, "forum": "qZzy5urZw9", "replyto": "u_PP25tvI", "invitation": "ICLR.cc/2021/Conference/Paper2638/-/Official_Comment", "content": {"title": "Response to Reviewer #2 [Cons 1]", "comment": "[Cons1: More Setups and Models] Thanks for the helpful suggestions. We have conducted new experiments using VGG16, Wide-ResNet-34-4, and Wide-ResNet-34-10 on the CIFAR-10/100 dataset and report the results in Table S1. And if the reviewer could kindly provide more details or some references about the random feature models, we are very willing to conduct new experiments and report the results. As we can see, our approaches largely mitigate robust overfitting across multiple models. For example, in VGG16, our methods reduce the gap between the RA best checkpoint and the final epochs from 5.83% to 0.06% on CIFAR-10 and 4.21% to 0.06% on CIFAR-100. Meanwhile, our methods gain an extra robustness improvement (2.57% on CIFAR-10 and 3.15% on CIFAR-100) compared with early stopping. Consistent improvements can also be observed with Wide-ResNets. In addition, we conduct more experiments to verify our approaches under Auto-Attack and CW attack, which is shown in Table S2. Evaluated under the more rigorous attack method, i.e. Auto-Attack, our methods can still be effective. Compared with the results reported in the Auto-Attack leaderboard from [2], our approaches reach 1.19% and 6.47% improvement on CIFAR-10 and CIFAR-100 under ResNet18, respectively. Besides, there is no significant drop of robust accuracy (RA) between the RA best checkpoint and the final epoch under both Auto-Attack and CW attack.\n\n[1] Bag of Tricks for adversarial training\n\n[2] Overfitting in adversarially robust deep learning\n\n[3] Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses\n\n\nTable S1 Performance showing the occurrence of robust overfitting across different architectures and the effectiveness of our proposed remedies under $\\ell_\\infty$ PGD-20 adversary. We pick the checkpoint which has the best robust accuracy on the validation set.\n\n|Architecture|Dataset|Settings|Robust Accuray(RA)(Best->Final)| Standard Accuracy(SA)(Best->Final)|\n|:-:|:-:|:-:|:-:|:-:|\n|VGG-16|CIFAR-10|Baseline|(46.42\u219240.59)(\u21935.83)|(75.29\u219279.54)(\u21914.25)|\n|VGG-16|CIFAR-10|Our Methods|(48.99\u219248.93)(\u21930.06)|(79.00\u219279.69)(\u21910.69)|\n|VGG-16|CIFAR-100|Baseline|(21.64\u219217.43)(\u21934.21)|(39.26\u219245.84)(\u21916.58)|\n|VGG-16|CIFAR-100|Our Methods|(24.79\u219224.73)(\u21930.06)|(48.20\u219249.00)(\u21910.80)|\n|WideResNet-34-4|CIFAR-10|Baseline|(52.59\u219243.06)(\u21939.53)|(81.53\u219283.28)(\u21911.75)|\n|WideResNet-34-4|CIFAR-10|Our Methods|(54.28\u219253.90)(\u21930.38)|(85.17\u219285.50)(\u21910.33)|\n|WideResNet-34-4|CIFAR-100|Baseline|(28.02\u219220.61)(\u21937.41)|(53.19\u219253.63)(\u21910.44)|\n|WideResNet-34-4|CIFAR-100|Our Methods|(30.10\u219229.80)(\u21930.30)|(57.23\u219258.05)(\u21910.82)|\n|WideResNet-34-10|CIFAR-10|Baseline|(54.27\u219247.12)(\u21937.15)|(84.16\u219285.72)(\u21911.56)|\n|WideResNet-34-10|CIFAR-10|Our Methods|(55.50\u219255.34)(\u21930.16)|(86.81\u219287.06)(\u21910.25)|\n|WideResNet-34-10|CIFAR-100|Baseline|(29.95\u219224.02)(\u21935.93)|(56.56\u219256.42)(\u21930.14)|\n|WideResNet-34-10|CIFAR-100|Our Methods|(31.93\u219231.51)(\u21930.42)|(60.86\u219261.78)(\u21910.92)|\n\n\nTable S2 Robust accuracy under Auto-Attack and CW Attack on CIFAR-10/100 with ResNet18. The best checkpoint is picked with the best robust accuracy under PGD-20 on the validation set. We follow the same setting as [3] for CW Attack: 1 search step on C with an initial constant of 0.1, with 100 iterations for each search step and learning rate is 0.01.\n\n|Dataset|Norm|Settings|Auto-Attack(Best->Final)|CW Attack(Best->Final)|\n|:-:|:-:|:-:|:-:|:-:|\n|CIFAR-10|$\\ell_\\infty$|Baseline|(47.00\u219239.96)(\u21937.04)|(75.48\u219260.52)(\u219314.96)|\n|CIFAR-10|$\\ell_\\infty$|Our Methods|(49.35\u219249.44)(\u21910.09)|(77.83\u219277.04)(\u21930.79)|\n|CIFAR-10|$\\ell_2$|Baseline|(67.18\u219264.29)(\u21932.89)|(73.80\u219253.77)(\u219320.03)|\n|CIFAR-10|$\\ell_2$|Our Methods|(68.87\u219268.90)(\u21910.03)|(73.89\u219273.79)(\u21930.10)|\n|CIFAR-100|$\\ell_\\infty$|Baseline|(22.73\u219218.11)(\u21934.62)|(45.89\u219237.76)(\u21938.13)|\n|CIFAR-100|$\\ell_\\infty$|Our Methods|(25.42\u219225.35)(\u21930.07)|(49.46\u219249.07)(\u21930.39)|\n|CIFAR-100|$\\ell_2$|Baseline|(37.16\u219233.43)(\u21933.73)|(48.43\u219237.73)(\u219310.70)|\n|CIFAR-100|$\\ell_2$|Our Methods|(40.56\u219240.61)(\u21910.05)|(51.02\u219250.90)(\u21930.12)|\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2638/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Overfitting may be mitigated by properly learned smoothening", "authorids": ["~Tianlong_Chen1", "~Zhenyu_Zhang4", "~Sijia_Liu1", "~Shiyu_Chang2", "~Zhangyang_Wang1"], "authors": ["Tianlong Chen", "Zhenyu Zhang", "Sijia Liu", "Shiyu Chang", "Zhangyang Wang"], "keywords": ["Robust Overfitting", "Adversarial Training", "Adversarial Robustness"], "abstract": "A recent study (Rice et al.,  2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overfitting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overfitting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by $3.72\\%\\sim6.68\\%$ and robust accuracy by $0.22\\%\\sim2 .03\\%$, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types ($\\ell_{\\infty}$ and $\\ell_2$), and robustified methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models' robustness against transfer attacks. Codes are available at https://github.com/VITA-Group/Alleviate-Robust-Overfitting.", "one-sentence_summary": "Mitigate robust overfitting by properly learned smoothening, establishing the new state-of-the-art bar in adversarial training", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|robust_overfitting_may_be_mitigated_by_properly_learned_smoothening", "pdf": "/pdf/099a32f12e88483a4451fe099750daeb8a1a0128.pdf", "supplementary_material": "/attachment/a5c269fbfe37fcb546d8db23c18c46f1442eccbf.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021robust,\ntitle={Robust Overfitting may be mitigated by properly learned smoothening},\nauthor={Tianlong Chen and Zhenyu Zhang and Sijia Liu and Shiyu Chang and Zhangyang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=qZzy5urZw9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "qZzy5urZw9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2638/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2638/Authors|ICLR.cc/2021/Conference/Paper2638/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846047, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2638/-/Official_Comment"}}}, {"id": "lDpsc2Fd95x", "original": null, "number": 2, "cdate": 1605677528144, "ddate": null, "tcdate": 1605677528144, "tmdate": 1606064854834, "tddate": null, "forum": "qZzy5urZw9", "replyto": "in8QAuRcMU", "invitation": "ICLR.cc/2021/Conference/Paper2638/-/Official_Comment", "content": {"title": "Response to Reviewer #3 [Cons 1-2]", "comment": "[Cons 1: Marginal Contribution.] We disagree that the contribution is marginal. As recognized by Reviewer #2, our work for the first time provides principal and effective solutions to mitigate robust overfitting [5] beyond the early stopping. As shown in Table 3, KD or SWA alone helps alleviate robust overfitting, and the combination of KD and SWA  further improves the performance, indicating their supplementary benefits. Technically, we leverage knowledge distillation and self-training to smooth the logits rather than proposing metric learning regularizers [1,2,3]. For the model weights manipulation, we utilize a much simpler yet effective technique, stochastic weight averaging, while [4] resorts to adversarial weight perturbation via a complex min-max optimization. To the best of our knowledge, none of [1-4] was proposed for solving the problem of robust overfitting. Thus, we believe that our differences with [1-4] are significant. However, we also would like to thank the reviewer for pointing out [1-4] and we have included them in the updated paper. Thanks for the suggestions!  \n\n[Cons 2: Hypothesize Explanation.] Thanks for the question. Our explanation on \u201cone source of robust overfitting might lie in that the model \u2018overfits\u2019 the attacks generated in the early stage of AT and fails to generalize or adapt to the attacks in the late-stage\u201d can be verified by experiment results in Figure 4 (Left). To be more specific, we evaluate the transferability of attacks generated by models at different epoch checkpoints of PGD-AT Baseline, Baseline + KD, and Baseline + KD + SWA against an unseen victim model given by robustified ResNet-50 with PGD-10 on CIFAR-100. As we can see, the attacks generated by later checkpoints (> 50 epochs) of PGD-AT Baseline lack transferability to the unseen (test) model. This is an insightful result as attacks generated by later checkpoints (corresponding to more robust source models) are supposed to have better transferability (namely, generalization over unseen models). The violation of the above intuition suggests that the defender (outer minimizer in PGD-AT Baseline) starts to overfit the attacker (inner maximizer in PGD-AT Baseline) at later epochs and in turn, is unable to generate attacks with generalization-ability. In AT, the two-layer game makes the defender and the attacker co-evolved and becomes overfitting to each other at later epochs. Indeed, as we use the proposed Baseline + KD + SWA in  Figure 4 (Left), the overfitting issue, characterized by the degradation of attack transferability at later epochs, can largely be mitigated. \n\n[1] Metric Learning for Adversarial Robustness\n\n[2] Rethinking Softmax Cross-Entropy Loss for Adversarial Robustness\n\n[3] Boosting Adversarial Training with Hypersphere Embedding\n\n[4] Revisiting loss landscape for adversarial robustness\n\n[5] Overfitting in adversarially robust deep learning\n\n[6] Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses"}, "signatures": ["ICLR.cc/2021/Conference/Paper2638/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Overfitting may be mitigated by properly learned smoothening", "authorids": ["~Tianlong_Chen1", "~Zhenyu_Zhang4", "~Sijia_Liu1", "~Shiyu_Chang2", "~Zhangyang_Wang1"], "authors": ["Tianlong Chen", "Zhenyu Zhang", "Sijia Liu", "Shiyu Chang", "Zhangyang Wang"], "keywords": ["Robust Overfitting", "Adversarial Training", "Adversarial Robustness"], "abstract": "A recent study (Rice et al.,  2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overfitting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overfitting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by $3.72\\%\\sim6.68\\%$ and robust accuracy by $0.22\\%\\sim2 .03\\%$, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types ($\\ell_{\\infty}$ and $\\ell_2$), and robustified methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models' robustness against transfer attacks. Codes are available at https://github.com/VITA-Group/Alleviate-Robust-Overfitting.", "one-sentence_summary": "Mitigate robust overfitting by properly learned smoothening, establishing the new state-of-the-art bar in adversarial training", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|robust_overfitting_may_be_mitigated_by_properly_learned_smoothening", "pdf": "/pdf/099a32f12e88483a4451fe099750daeb8a1a0128.pdf", "supplementary_material": "/attachment/a5c269fbfe37fcb546d8db23c18c46f1442eccbf.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021robust,\ntitle={Robust Overfitting may be mitigated by properly learned smoothening},\nauthor={Tianlong Chen and Zhenyu Zhang and Sijia Liu and Shiyu Chang and Zhangyang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=qZzy5urZw9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "qZzy5urZw9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2638/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2638/Authors|ICLR.cc/2021/Conference/Paper2638/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846047, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2638/-/Official_Comment"}}}, {"id": "-gaKSULmuGN", "original": null, "number": 6, "cdate": 1605894016710, "ddate": null, "tcdate": 1605894016710, "tmdate": 1606064671237, "tddate": null, "forum": "qZzy5urZw9", "replyto": "Ha8AU0FU9oP", "invitation": "ICLR.cc/2021/Conference/Paper2638/-/Official_Comment", "content": {"title": "Response to Reviewer #1 [Cons 1-2]", "comment": "We\u2019re very glad you had a positive initial impression, and likewise, we found the set of perceptive questions you raised in your feedback very insightful, pushing us to think of a tighter experiment design. We provide pointwise responses below.\n\n[Cons1: Evaluation under the Improved Attack] Thanks for the helpful suggestion.  We understood your concerns on the evaluation of our approach under improved attacks, e.g., Auto-Attack and CW attack. We have included the reference and experiment results of the Auto-Attack and CW attack in our modified draft. The Auto-attack and CW attack implementations that we used follow (https://github.com/fra31/auto-attack) and the advertorch (https://github.com/BorealisAI/advertorch), respectively. As shown in Table S1, after applying the combination of KD and SWA, the overfitting problem is largely mitigated under Auto-Attack and CW attack. Take CIFAR-10 $\\ell_\\infty$ adversary as an example.  As we can see, our approach reduces the drop of robust accuracy from 7.04% to -0.09% under Auto-Attack, and 14.96% to 0.79% under CW attack, when comparing the best model to the eventually converged model.  Although the Auto-Attack Leaderboard is a rising benchmark, it is hard to conduct a fair comparison since there are diverse settings, including extra data, network architecture, training epochs, and other implementation configurations. We try our best to unify the setting and compare it with the ResNet-18 models from Rice et al. [1] in the Auto-Attack Leaderboard (https://github.com/fra31/auto-attack). We observe the ResNet-18 models from [1] achieve 18.95% robust accuracy with $\\ell_\\infty$ adversary auto-attacks on CIFAR-100 and 67.68% robust accuracy with $\\ell_2$ adversary on CIFAR-10, while our approaches reach 25.42% and 68.87% robust accuracy, respectively. Under the exact same settings, our proposal achieves substantial robustness improvement (6.47% and 1.19% robust accuracy) under Auto-Attacks. In addition, we have also provided additional experiment results on Wide-ResNet in the modified draft and submit all our models to the Auto-Attack leaderboard in the future.\n\n[Cons2: Typo] Thanks for the careful reading. Table 2 was a typo. It should be Table 4.\n\n[1] Overfitting in adversarially robust deep learning\n\n[2] Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses\n\nTable S1 Robust accuracy under Auto-Attack and CW Attack on CIFAR-10/100 with ResNet18. The best checkpoint is picked with the best robust accuracy under PGD-20 on the validation set. We follow the same setting as [2] for CW Attack: 1 search step on C with an initial constant of 0.1, with 100 iterations for each search step and learning rate is 0.01.\n\n|Dataset|Norm|Settings|Auto-Attack(Best->Final)|CW Attack(Best->Final)|\n|:-:|:-:|:-:|:-:|:-:|\n|CIFAR-10|$\\ell_\\infty$|Baseline|(47.00\u219239.96)(\u21937.04)|(75.48\u219260.52)(\u219314.96)|\n|CIFAR-10|$\\ell_\\infty$|Our Methods|(49.35\u219249.44)(\u21910.09)|(77.83\u219277.04)(\u21930.79)|\n|CIFAR-10|$\\ell_2$|Baseline|(67.18\u219264.29)(\u21932.89)|(73.80\u219253.77)(\u219320.03)|\n|CIFAR-10|$\\ell_2$|Our Methods|(68.87\u219268.90)(\u21910.03)|(73.89\u219273.79)(\u21930.10)|\n|CIFAR-100|$\\ell_\\infty$|Baseline|(22.73\u219218.11)(\u21934.62)|(45.89\u219237.76)(\u21938.13)|\n|CIFAR-100|$\\ell_\\infty$|Our Methods|(25.42\u219225.35)(\u21930.07)|(49.46\u219249.07)(\u21930.39)|\n|CIFAR-100|$\\ell_2$|Baseline|(37.16\u219233.43)(\u21933.73)|(48.43\u219237.73)(\u219310.70)|\n|CIFAR-100|$\\ell_2$|Our Methods|(40.56\u219240.61)(\u21910.05)|(51.02\u219250.90)(\u21930.12)|\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2638/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Overfitting may be mitigated by properly learned smoothening", "authorids": ["~Tianlong_Chen1", "~Zhenyu_Zhang4", "~Sijia_Liu1", "~Shiyu_Chang2", "~Zhangyang_Wang1"], "authors": ["Tianlong Chen", "Zhenyu Zhang", "Sijia Liu", "Shiyu Chang", "Zhangyang Wang"], "keywords": ["Robust Overfitting", "Adversarial Training", "Adversarial Robustness"], "abstract": "A recent study (Rice et al.,  2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overfitting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overfitting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by $3.72\\%\\sim6.68\\%$ and robust accuracy by $0.22\\%\\sim2 .03\\%$, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types ($\\ell_{\\infty}$ and $\\ell_2$), and robustified methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models' robustness against transfer attacks. Codes are available at https://github.com/VITA-Group/Alleviate-Robust-Overfitting.", "one-sentence_summary": "Mitigate robust overfitting by properly learned smoothening, establishing the new state-of-the-art bar in adversarial training", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|robust_overfitting_may_be_mitigated_by_properly_learned_smoothening", "pdf": "/pdf/099a32f12e88483a4451fe099750daeb8a1a0128.pdf", "supplementary_material": "/attachment/a5c269fbfe37fcb546d8db23c18c46f1442eccbf.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021robust,\ntitle={Robust Overfitting may be mitigated by properly learned smoothening},\nauthor={Tianlong Chen and Zhenyu Zhang and Sijia Liu and Shiyu Chang and Zhangyang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=qZzy5urZw9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "qZzy5urZw9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2638/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2638/Authors|ICLR.cc/2021/Conference/Paper2638/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846047, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2638/-/Official_Comment"}}}, {"id": "lEpEh1jOJGe", "original": null, "number": 11, "cdate": 1606030579342, "ddate": null, "tcdate": 1606030579342, "tmdate": 1606030579342, "tddate": null, "forum": "qZzy5urZw9", "replyto": "wPCyBVTvO_E", "invitation": "ICLR.cc/2021/Conference/Paper2638/-/Official_Comment", "content": {"title": "Comment on revision", "comment": "Thanks for the detailed response! It is encouraging to see that your proposed method performs on par or slightly better than the previous baseline. [1] is counted as a concurrent submission. I also understand that it might be difficult to derive theoretical results, even for a very simple setting. Nevertheless, the critique remains that the paper conducts relatively narrow experiments (only on ResNet-18, and there is little evidence (neither empirical nor theoretical) that the findings generalize beyond the studied ResNet-18 setting to for example VGG, other deep nets, or even simple baseline models such as random feature models. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2638/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Overfitting may be mitigated by properly learned smoothening", "authorids": ["~Tianlong_Chen1", "~Zhenyu_Zhang4", "~Sijia_Liu1", "~Shiyu_Chang2", "~Zhangyang_Wang1"], "authors": ["Tianlong Chen", "Zhenyu Zhang", "Sijia Liu", "Shiyu Chang", "Zhangyang Wang"], "keywords": ["Robust Overfitting", "Adversarial Training", "Adversarial Robustness"], "abstract": "A recent study (Rice et al.,  2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overfitting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overfitting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by $3.72\\%\\sim6.68\\%$ and robust accuracy by $0.22\\%\\sim2 .03\\%$, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types ($\\ell_{\\infty}$ and $\\ell_2$), and robustified methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models' robustness against transfer attacks. Codes are available at https://github.com/VITA-Group/Alleviate-Robust-Overfitting.", "one-sentence_summary": "Mitigate robust overfitting by properly learned smoothening, establishing the new state-of-the-art bar in adversarial training", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|robust_overfitting_may_be_mitigated_by_properly_learned_smoothening", "pdf": "/pdf/099a32f12e88483a4451fe099750daeb8a1a0128.pdf", "supplementary_material": "/attachment/a5c269fbfe37fcb546d8db23c18c46f1442eccbf.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021robust,\ntitle={Robust Overfitting may be mitigated by properly learned smoothening},\nauthor={Tianlong Chen and Zhenyu Zhang and Sijia Liu and Shiyu Chang and Zhangyang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=qZzy5urZw9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "qZzy5urZw9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2638/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2638/Authors|ICLR.cc/2021/Conference/Paper2638/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846047, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2638/-/Official_Comment"}}}, {"id": "wPCyBVTvO_E", "original": null, "number": 9, "cdate": 1605894667050, "ddate": null, "tcdate": 1605894667050, "tmdate": 1605894667050, "tddate": null, "forum": "qZzy5urZw9", "replyto": "u_PP25tvI", "invitation": "ICLR.cc/2021/Conference/Paper2638/-/Official_Comment", "content": {"title": "(Continued) Response to Reviewer #2 [Cons 2 & Open Question]", "comment": "[Cons2: Hyperparameters Investigations] Thanks for pointing this out. The tricks for adversarial training studied in the under-review paper [1] help a lot. As inspired by this paper, we conduct an ablation study on the choice of weight decay (WD), i.e., the key factor mentioned in [1]. Follow the same settings in [1], we compared the results of baseline PGD-AT and our methods in Table S3. The performance is sensitive to the choice of WD, which is aligned with the conclusion in [1]. With the same WD, our methods can significantly mitigate robust overfitting and meanwhile achieve an extra robust accuracy improvement (0.43%, 0.21%, and 1.42% for WD of 1e-4, 2e-4, 5e-5, respectively). When choosing 5e-4 of WD, both the baseline and our methods reach the best performance.  We do appreciate the work [1] and conducted new experiments to alleviate the reviewer's concerns. However, we count [1] as a concurrent submission and we hope that it should not be used to downgrade our contributions.\n\n[Open Question: Theoretical Intuition of Label Smoothing.] Thanks for this interesting open question. It is fair to note that our main goal is to provide systematic empirical investigations for alleviating the robust overfitting issue [2]. Besides the impressive experiment results, we also show extensive visualizations to justify how and why the robust overfitting happens and be mitigated. \n\nTo our best knowledge, most label smoothing works focus on empirical studies [4]. Recently, we noticed that handful of papers [3] start some initial theoretical analyses on minimization problems. [3,4] point out that label smoothing is related to loss-correction techniques [3], and encourages the representations of training examples from the same class to group in tight clusters [4] which is also observed in our paper (Figure 5). However, the theoretical analyses for min-max problems are still missing in the literature. We sincerely appreciate the suggestion and will further explore the theoretical foundations in the future.\n\n\n[1] Bag of Tricks for adversarial training\n\n[2] Overfitting in adversarially robust deep learning\n\n[3] Does Label Smoothing Mitigate Label Noise?\n\n[4] When Does Label Smoothing Help?\n\nTable S3 An ablation study on the choice of weight decay using ResNet-18, the robust accuracy(RA) is evaluated under PGD-20. We pick the checkpoint which has the best robust accuracy on the validation set.\n\n|Weight Decay|Settings|Robust Accuray(RA)(Best->Final)| Standard Accuracy(SA)(Best->Final)|\n|:-:|:-:|:-:|:-:|\n|1e-4|Baseline|(48.77\u219239.40)(\u21939.37)|(81.47\u219281.39)(\u21930.08)|\n|1e-4|Our Methods|(49.20\u219248.28)(\u21930.92)|(82.35\u219282.96)(\u21910.61)|\n|2e-4|Baseline|(50.45\u219240.22)(\u219310.23)|(81.33\u219281.51)(\u21910.18)|\n|2e-4|Our Methods|(50.66\u219250.18)(\u21930.48)|(82.89\u219283.73)(\u21910.84)|\n|5e-4|Baseline|(50.72\u219241.38)(\u21939.34)|(80.78\u219282.44)(\u21911.66)|\n|5e-4|Our Methods|(52.14\u219251.53)(\u21930.61)|(84.65\u219285.40)(\u21910.75)|\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2638/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Overfitting may be mitigated by properly learned smoothening", "authorids": ["~Tianlong_Chen1", "~Zhenyu_Zhang4", "~Sijia_Liu1", "~Shiyu_Chang2", "~Zhangyang_Wang1"], "authors": ["Tianlong Chen", "Zhenyu Zhang", "Sijia Liu", "Shiyu Chang", "Zhangyang Wang"], "keywords": ["Robust Overfitting", "Adversarial Training", "Adversarial Robustness"], "abstract": "A recent study (Rice et al.,  2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overfitting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overfitting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by $3.72\\%\\sim6.68\\%$ and robust accuracy by $0.22\\%\\sim2 .03\\%$, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types ($\\ell_{\\infty}$ and $\\ell_2$), and robustified methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models' robustness against transfer attacks. Codes are available at https://github.com/VITA-Group/Alleviate-Robust-Overfitting.", "one-sentence_summary": "Mitigate robust overfitting by properly learned smoothening, establishing the new state-of-the-art bar in adversarial training", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|robust_overfitting_may_be_mitigated_by_properly_learned_smoothening", "pdf": "/pdf/099a32f12e88483a4451fe099750daeb8a1a0128.pdf", "supplementary_material": "/attachment/a5c269fbfe37fcb546d8db23c18c46f1442eccbf.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021robust,\ntitle={Robust Overfitting may be mitigated by properly learned smoothening},\nauthor={Tianlong Chen and Zhenyu Zhang and Sijia Liu and Shiyu Chang and Zhangyang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=qZzy5urZw9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "qZzy5urZw9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2638/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2638/Authors|ICLR.cc/2021/Conference/Paper2638/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846047, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2638/-/Official_Comment"}}}, {"id": "SFS22lWfXcF", "original": null, "number": 7, "cdate": 1605894298148, "ddate": null, "tcdate": 1605894298148, "tmdate": 1605894298148, "tddate": null, "forum": "qZzy5urZw9", "replyto": "j2Dg7M0IZUd", "invitation": "ICLR.cc/2021/Conference/Paper2638/-/Official_Comment", "content": {"title": "Response to Reviewer #3 [More About Cons 1-2]", "comment": "[More about Cons 1: Marginal Contribution] Thanks for the comments and additional references. We agree that robust training does not stick to the min-max formulation, e.g., random smoothing, friendly adversarial training, and adversarial data free robust training (e.g., [15]), though minmax-based adversarial training (AT) remains trendy. However, it is still very interesting and promising to investigate the minmax-based AT since minmax-based AT normally achieves the strongest robustness. \n\nIt is fair to note that our focus is to address the challenge of robust overfitting defined by Rice et al. [5] in minmax-based AT, which also presents state-of-the-art robust performance (superior to [7,8] under the same settings) with the early stopping technique to avoid the curse of overfitting. Although [7,8], as different strategies to solve min-max formulations, are possibly effective in alleviating the robust overfitting, they achieve inferior robustness compared to Rice et al. [5]. This is why we focus on the specific problem in Rice et al. [5] since our findings join (Rice et al. [5]) in re-establishing the competitiveness and superiority of the most commonly-used AT baseline.\n\n[More about Cons 2: Hypothesis Explanation] As shown in [5], robust overfitting mainly occurs around the epochs of the first learning rate adjustment. Therefore, we conjecture this phenomenon is related to the \u201ctwo stages hypothesis\u201d in minimization problems [11-14]. These advanced works [11-14] point out that there are two stages in network training. Specifically, from the first to the second stage, the learning rate is decayed to a small magnitude and loss landscapes become much more complicated (e.g., from nearly convex to highly no-convex loss surfaces). However, this hypothesis is hardly explored under the min-max context. \n\nIn the recent work [9], they find that better local linearity (i.e., Equation 5 in [9]) mitigate the catastrophic forgetting in the fast adversarial training. Inspired by [9]\u2019s finding, we also measure the local linearity under the robust overfitting context, as shown in Figure A11. We observe that, during the first learning rate adjustment, the local linearity suddenly decreases. It potentially switches to the second stage of training with a much more complex loss landscape. Then, the properties of adversarial samples may be dramatically changed, due to the different gradients of a harder inner maximization problem. Our proposed methods prevent the sudden drop of local linearity measurements and alleviate the robust overfitting. \n\nThank you for updating the score, and your feedback really helped us on improving our work. We will further polish our paper according to your suggestions in the final version.\n\n[5] Overfitting in adversarially robust deep learning\n\n[7] Attacks which do not kill training make adversarial learning stronger.\n\n[8] Improving adversarial robustness through progressive hardening. \n\n[9] Understanding and Improving Fast Adversarial Training\n\n[10] Overfitting or Underfitting? Understand Robustness Drop-in Adversarial Training\n\n[11] The Two Regimes of Deep Network Training\n\n[12] Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks\n\n[13] Time Matters in Regularizing Deep Networks: Weight Decay and Data Augmentation Affect Early Learning Dynamics, Matter Little Near Convergence\n\n[14] Deep Neural Networks are Lazy: On the Inductive Bias of Deep Learning\n\n[15] Towards Achieving Adversarial Robustness by Enforcing Feature Consistency Across Bit Planes"}, "signatures": ["ICLR.cc/2021/Conference/Paper2638/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Overfitting may be mitigated by properly learned smoothening", "authorids": ["~Tianlong_Chen1", "~Zhenyu_Zhang4", "~Sijia_Liu1", "~Shiyu_Chang2", "~Zhangyang_Wang1"], "authors": ["Tianlong Chen", "Zhenyu Zhang", "Sijia Liu", "Shiyu Chang", "Zhangyang Wang"], "keywords": ["Robust Overfitting", "Adversarial Training", "Adversarial Robustness"], "abstract": "A recent study (Rice et al.,  2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overfitting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overfitting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by $3.72\\%\\sim6.68\\%$ and robust accuracy by $0.22\\%\\sim2 .03\\%$, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types ($\\ell_{\\infty}$ and $\\ell_2$), and robustified methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models' robustness against transfer attacks. Codes are available at https://github.com/VITA-Group/Alleviate-Robust-Overfitting.", "one-sentence_summary": "Mitigate robust overfitting by properly learned smoothening, establishing the new state-of-the-art bar in adversarial training", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|robust_overfitting_may_be_mitigated_by_properly_learned_smoothening", "pdf": "/pdf/099a32f12e88483a4451fe099750daeb8a1a0128.pdf", "supplementary_material": "/attachment/a5c269fbfe37fcb546d8db23c18c46f1442eccbf.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021robust,\ntitle={Robust Overfitting may be mitigated by properly learned smoothening},\nauthor={Tianlong Chen and Zhenyu Zhang and Sijia Liu and Shiyu Chang and Zhangyang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=qZzy5urZw9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "qZzy5urZw9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2638/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2638/Authors|ICLR.cc/2021/Conference/Paper2638/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846047, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2638/-/Official_Comment"}}}, {"id": "u5xpTF6LL39", "original": null, "number": 5, "cdate": 1605780043140, "ddate": null, "tcdate": 1605780043140, "tmdate": 1605780238368, "tddate": null, "forum": "qZzy5urZw9", "replyto": "qr7e3JyR0Tn", "invitation": "ICLR.cc/2021/Conference/Paper2638/-/Official_Comment", "content": {"title": "The added experiments look promising. ", "comment": "The added experiments across networks structures & attack evaluations look good. \\\nTherefore, I am willing to increase the score to 6. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2638/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Overfitting may be mitigated by properly learned smoothening", "authorids": ["~Tianlong_Chen1", "~Zhenyu_Zhang4", "~Sijia_Liu1", "~Shiyu_Chang2", "~Zhangyang_Wang1"], "authors": ["Tianlong Chen", "Zhenyu Zhang", "Sijia Liu", "Shiyu Chang", "Zhangyang Wang"], "keywords": ["Robust Overfitting", "Adversarial Training", "Adversarial Robustness"], "abstract": "A recent study (Rice et al.,  2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overfitting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overfitting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by $3.72\\%\\sim6.68\\%$ and robust accuracy by $0.22\\%\\sim2 .03\\%$, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types ($\\ell_{\\infty}$ and $\\ell_2$), and robustified methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models' robustness against transfer attacks. Codes are available at https://github.com/VITA-Group/Alleviate-Robust-Overfitting.", "one-sentence_summary": "Mitigate robust overfitting by properly learned smoothening, establishing the new state-of-the-art bar in adversarial training", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|robust_overfitting_may_be_mitigated_by_properly_learned_smoothening", "pdf": "/pdf/099a32f12e88483a4451fe099750daeb8a1a0128.pdf", "supplementary_material": "/attachment/a5c269fbfe37fcb546d8db23c18c46f1442eccbf.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021robust,\ntitle={Robust Overfitting may be mitigated by properly learned smoothening},\nauthor={Tianlong Chen and Zhenyu Zhang and Sijia Liu and Shiyu Chang and Zhangyang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=qZzy5urZw9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "qZzy5urZw9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2638/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2638/Authors|ICLR.cc/2021/Conference/Paper2638/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846047, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2638/-/Official_Comment"}}}, {"id": "j2Dg7M0IZUd", "original": null, "number": 4, "cdate": 1605691262909, "ddate": null, "tcdate": 1605691262909, "tmdate": 1605776247568, "tddate": null, "forum": "qZzy5urZw9", "replyto": "lDpsc2Fd95x", "invitation": "ICLR.cc/2021/Conference/Paper2638/-/Official_Comment", "content": {"title": "Reply to authors' first part of responses", "comment": "My response to authors\u2019 response [Cons 1: Marginal Contribution.]\n1 \u201cOur work for the first time provides principal and effective solutions to mitigate robust overfitting [5] beyond the early stopping.\u201d\nRecently, I came across some papers. Adversarial training may not stick to minimax formulation. Some curriculum-inspired adversarial training methods may not have the issue of robust overfitting (see [7] [8]). \n[7] Attacks which do not kill training make adversarial learning stronger. \n[8] Improving adversarial robustness through progressive hardening. \n\n2  I indeed appreciate the technical contributions on improving the adversarial training. \nThat I state \u201cmarginal contribution\u201d comes from the philosophy perspective.\n\nMy response to authors\u2019 response [Cons 2: Hypothesize Explanation.]\n1.I am trying to understand what happened & why robust overfitting occur. \nIn terms of relieving robust overfitting, I also came across the concurrent work of ICLR 2021 submission <https://openreview.net/forum?id=iAX0l6Cz8ub>. \nSince both are concurrently claim to relieve the robust overfitting, your claiming \u201csolving the problem of robust overfitting\u201d can be deemed as novel. \n\nI appreciate your explanations of Figure 4. \nYou are describing the phenomena that \u201cthe defender starts to overfit the attacker at later epochs and in turn, is unable to generate attacks with generalization-ability, i.e., co-evolution.\u201d \nBut I am trying to understand what exact things happened behind (e.g., minimax-based AT), which causes these phenomena. \n\n\nI am willing to increase the score if you could provide the convincing arguments. \nI will carefully look at your second part of responses later."}, "signatures": ["ICLR.cc/2021/Conference/Paper2638/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Overfitting may be mitigated by properly learned smoothening", "authorids": ["~Tianlong_Chen1", "~Zhenyu_Zhang4", "~Sijia_Liu1", "~Shiyu_Chang2", "~Zhangyang_Wang1"], "authors": ["Tianlong Chen", "Zhenyu Zhang", "Sijia Liu", "Shiyu Chang", "Zhangyang Wang"], "keywords": ["Robust Overfitting", "Adversarial Training", "Adversarial Robustness"], "abstract": "A recent study (Rice et al.,  2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overfitting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overfitting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by $3.72\\%\\sim6.68\\%$ and robust accuracy by $0.22\\%\\sim2 .03\\%$, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types ($\\ell_{\\infty}$ and $\\ell_2$), and robustified methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models' robustness against transfer attacks. Codes are available at https://github.com/VITA-Group/Alleviate-Robust-Overfitting.", "one-sentence_summary": "Mitigate robust overfitting by properly learned smoothening, establishing the new state-of-the-art bar in adversarial training", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|robust_overfitting_may_be_mitigated_by_properly_learned_smoothening", "pdf": "/pdf/099a32f12e88483a4451fe099750daeb8a1a0128.pdf", "supplementary_material": "/attachment/a5c269fbfe37fcb546d8db23c18c46f1442eccbf.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021robust,\ntitle={Robust Overfitting may be mitigated by properly learned smoothening},\nauthor={Tianlong Chen and Zhenyu Zhang and Sijia Liu and Shiyu Chang and Zhangyang Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=qZzy5urZw9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "qZzy5urZw9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2638/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2638/Authors|ICLR.cc/2021/Conference/Paper2638/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2638/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846047, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2638/-/Official_Comment"}}}], "count": 17}