{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392751560000, "tcdate": 1392751560000, "number": 1, "id": "kj-njtQse09Fg", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "vz8AumxkAfz5U", "replyto": "hTJQT_z-3SR-I", "signatures": ["Razvan Pascanu"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your detailed comments. We went through all of them and changed the paper accordingly. The new version of the paper (version 7) is online. Let us answer your comments and show how we addressed your concerns.\r\n\r\n* Regarding the notation on page 2, namely the use of R^P \to (R^N \to [0, inf)) and p_\theta(z) = F(\theta). We have explicitly spelled out the family F (instead of just providing its domain and codomain). The notation we provided is typical for higher order functions (for example when currying a function http://en.wikipedia.org/wiki/Currying). We agree that it is an uncommon notation (or an abuse of notation) and decided to remove it as suggested. The notation $p_\theta(z) = F(\theta)$ was fixed to the suggested form. Yes that is what we meant.\r\n\r\n* Regarding how a matrix-valued function of theta defines a metric on the space, we added a few sentences about it.  However, we want to emphasize that we only briefly mention the manifold interpretation of natural gradient to make the link with Amari's work.  We believe that the definition of the algorithm given in Eq. 2 (now Eq. 3) is actually more intuitive for the target audience of this work, and this is why we chose this way of explaining it.\r\n\r\n* Regarding our choice of using row vectors for gradients. There is no globally accepted convention with regard to the representation of gradient, either as column vectors or row vectors. A bit of search revealed that is not even a cultural distinction. http://www.cs.huji.ac.il/~csip/tirgul3_derivatives.pdf defines gradient as being a row vectors. In the computer graphics community the gradient is used as a row vector as well, e.g. http://chrishecker.com/Column_vs_row_vectors.  There are many other examples. http://en.wikipedia.org/wiki/Matrix_calculus also mentions this issue. I could not find any information stating what is the accepted convention for machine learning (or even statistics). If there is one, and it is in terms of column vectors rather then row vectors we're happy to change the notation. Due to my personal background, row vectors seems a more intuitive choice.\r\n\r\n* Regarding the use of the term natural gradient to denote the algorithm, we have fixed it replacing it everywhere with natural gradient descent. Similarly Hessian-Free was replaced with Hessian-Free Optimization. Thank you for pointing this out. \r\n\r\n* We replaced the phrase 'moving in the direction' with 'moving some given distance in the direction' as suggested.\r\n\r\n* Regarding the phrase 'the change induced in our model is constant'. Yes, we mean to say it is some given value. Specifically, we mean constant with respect to the constrained optimization that we solve for that given step, but it does not have to be globally constant (between iterations of natural gradient). We changed the text to the proposed formulation\r\n\r\nWe also agree that the sentence is not sufficiently strong. We reformulated to `such that the change induced in our model (in the KL sense)...`. We hope that with this formulation readers will become aware that we refer to the KL constraint in equation 2 (now 3). \r\n\r\n* Regarding the fact that eqn 2 (now eqn 3) is not an exact depiction of what the algorithm does. We understand your point of view and tried to incorporated in the current text. We wrote 'Specifically we look for $Delta \theta$ that minimizes a first order Taylor expansion of $mathcal{L}$ when the second order Taylor series of the KL-divergece between $p_{\theta}$ .. '. We do want to emphasize that viewing equation 2 (now 3) as what natural gradient descent is attempting to do is a valid justification of the algorithm, in the same sense that BFGS, truncated Newton, etc are trying to take a Newton step.\r\n \r\n* Regarding the term 'empirical Fisher'. You are right that this matrix used by Le Roux et al. is called empirical Fisher matrix. We believe this term to most likely create confusion and we purposely refrained from using it. We did this for the same reason we used the word TONGA to denote the algorithm introduced by Le Roux and others, while the original name of the algorithm was natural gradient.  We believe this algorithm to be distinct from the one proposed by Amari and hope that our paper makes this observation clear.\r\n\r\n* Page 6, eqn 16. You are right, and we change the text to say it approximates the heuristic used by Martens. We also spelled out the underlying assumption for these two heuristics to be the same (namely, as you pointed out, that CG converges). Thank you for making this point.\r\n\r\n* We have fixed the citation for 'Structural damping'. Thank you. \r\n\r\n* Regarding the fix coefficient of 1 for structural damping. We meant that by trying to put a constraint on the 2nd order Taylor approximation of the KL wrt the joint  $p(t,h |x)$,  one gets the structural damping term, with a fixed coefficient of 1. This does not refer to the original paper (and structural damping) but more to a shortcoming of trying to get this regularization in the natural gradient framework via using the KL wrt to the joint. In the next few lines of the text, we provide, however, another approach of constructing structural damping for natural gradient that lets you use any coefficient you want. Specifically we use multiple constraints in the constrained optimization that we solve at each iteration step. \r\n\r\n* Regarding warm restart (or initialization of CG with the previous solution). We can see your viewpoint. We tried to formulate the paragraph differently to not downplay the meaning of warm restart. Now it does mean something different in the case of KSD as compared to HF's warm restart. Specifically by using the previous direction as an initialization of CG we change the order and the conjugate directions that we visit, but CG solves the same problem (namely inverting the metric matrix). By adding the extra term to the Krylov subspace one can decompose what the algorithm is doing as shown in equation 19 (now 20), meaning that we do something similar to nonlinear conjugate gradient, taking directions locally conjugate to the **Hessian** at that point. The Hessian is different from the metric matrix. We believe that there is a difference between these approaches, which might  make KSD take advantage (to some degree) of the curvature of the error.\r\n\r\n* Regarding section 6 and what it tries to do. Very possibly an abuse of language, but when we refer to KSD we are referring to the particular algorithm (up to implementation details) introduced in  Vinyals and Povey. In this sense G is the Fisher information matrix. If we are to interpret the matrix that is being inverted as the Hessian, the section does not make as much sense anymore. What we are hinting to is that because KSD relies on the Fisher matrix, it does not actually rely on second order information of the error (but only on first order gradients). This is true, as the Gauss Newton is mostly an outer product of gradients (with the exception of the partial Hessian in the middle which is usually diagonal and does not carry information about the error). From this perspective one could expect to gain something by also looking at the Hessian of the error. This point was put forward for example by Le Roux et al. In this section we argue that we could see KSD as doing something of this nature, because it could be understood as nonlinear conjugate gradient (which looks at locally conjugate directions with respect to the Hessian), just that instead of using the gradients we use the natural gradients. \r\n\r\nWe have re-worked this section to make these things apparent. \r\n\r\n* Regarding page 7, where we talk about a bias introduced by using the same samples for the metric and gradient. We reformulated the text of this section, and avoided using the word bias. What we are hinting towards is that one can limit (to a certain extend) the amount of overfitting of the current minibatch that could happen at each step. This is the reformulation of the paragraph:\r\n\r\nOur intuition is that we are seeing the model overfitting, at each step, the current training minibatch. At each step we compute the gradient and the metric on the same example. There can be, within this  minibatch, some direction in which we could overfit some specific property of these examples. Because we only look at how the model changes at these points when computing the metric, all gradients will agree with this direction. Specifically, the model believes that moving in this direction will lead to steady change in the KL (the KL looks at the covariance between the gradients from the output layer to the parameters, which will be low in the picked direction) and the model will believe it can take a large step, learning this specific feature.  However it is not useful for generalization, nor is it useful for the other training examples (e.g. if the particular deformation is not actually correlated with the classes it needs to predict). This means that on subsequent training examples the model will under-perform, resulting in a worse overall error.\r\n\r\nOn the other hand, if we use a different minibatch for the gradient and for the metric, it is less likely that the same particular feature to be present in both the set of examples used for the gradient and those used for the metric. So either the gradient will not point in said direction (as the feature is not present in the gradient), or, if the feature is present in the gradient, it may not be present in the examples used for computing the metric. That would lead to a larger variance, and hence the model is less likely to take a large step in this direction.\r\n\r\n* Regarding section 12.1 of 'Training Deep and Recurrent Neural Networks with Hessian-Free Optimization'.The technical result in section 12.1 makes the underlying assumption that the minibatch we use to estimate these two quantities is large and matches perfectly the statistics of the data. In some sense this is a standard view when doing optimization. That is, optimizing $M(delta)$ to its minimum, based on the current minibatch is the optimum strategy.\r\n\r\nWe argue that in a stochastic regime (where we think our intuition applies) and specifically in a stochastic regime with moderate sized minibatches, this is far from true. The minimum of $M$ might lead to learning artifacts of the current minibatch that will hurt both generalization and even the global training error. \r\n\r\nIn particular, if we rephrase it in a similar wording, the gradient can be seen as the linear reward. It might be, that from the point of view of those examples, the quadratic penalty is small for a certain reward, because moving in the direction $d$ seems to be beneficial for minimizing the error on the specific set of examples. However, globally, because the Taylor series is not reliable (in particular because we have not considered sufficient amount of data), the direction $d$ should have a large penalty. Our intuition is that the likelihood of obtaining the same directions $d$ with large linear reward and low quadratic penalty when using different batches of data is lower due to the argument above. \r\n \r\nThe main argument brought against the different batches of data is that the metric might be singular. This would be the reason why we used to the word moderately sized minibatch. The likelihood of dealing with a singular value (given the maximal number of steps one takes during CG) decreases quite a bit with a moderately sized minibatch. Additionally, the fact that there might exist a direction d such that $g^Td < 0$ and $d^TBd = 0$ can be understood as stating that we do not know how the model changes (in the KL-sense) when we move along d. In practice we address this in two ways. The first one is damping, which is a prior on how much the density probability function changes (in the KL sense) if you move in any direction. The second approach we used is to rely on MinRes which is supposed to be able to handle singular matrices and return the solution with minimal norm that minimizes the system of linear equations. Overall we have not seen the algorithm suffer from this, and similar observations were reported by Vinyals and Povey.\r\n\r\n* Regarding the observation that unlabeled data can not help with overfitting for a convex problem, that is probably true. Generically speaking, the standard optimization paradigm might be misleading as this is an effect specific to learning (and not to optimization).\r\n\r\nWe believe that the best way of building intuition of what is going on is to think of the algorithm as trying to move in the parameter space while enforcing a certain amount of change to happen in the behavior of the model (a certain change in the KL divergence of the density probability function). As pointed out in the text, the constraint helps to not get stuck, e.g., near saddle points (i.e. some change has to happen in the behavior of the model), and also forces learning to temper itself. Now the ability of doing this highly depends on how well the model can measure how much it changes. We can use unlabeled data to get a better measure of how fast the density function changes everywhere. While not exactly correct, you could think as how fast the error increases or decreases at points that are different from those in the training set and hence be able to better temper itself.\r\n\r\nBecause the problem is non-convex, the direction of the gradient can change quite a bit, and not jumping over large parameter regions might have a big impact on the final performance. For example, we might be able (due to this tempering) to follow narrow valleys over which (without unlabeled data) we would otherwise not be able to proceed towards better solutions.\r\n\r\n* Regarding the question about the pseudocode on page 8. Here are some\r\nanswers:\r\n\r\nWhere are these 'newly sampled examples' coming from? \r\n    \r\nThe dataset is larger than the total number of examples we need for all the runs (the dataset is obtained by adding distortions and hence has theoretically an infinite amount of samples). So no sample is reused.\r\n\r\nWhat is the purpose of the first split into 'two large chunks'? \r\n    \r\nIf we would have considered the original data and split it directly into 10 we would have not had enough generated examples to carry out the experiment. Specifically, while the original dataset was obtained by adding distortions to the NIST dataset, the original dataset comes with a set of 800M sampled datapoints. We relied on these and did not attempt to generate more data. Because we wanted to look at our model after a sufficient amount of learning,  we decided to have the fix second chunk of data that we go through after we reordered the first half. We wanted to simulate online learning for this experiment and not revisit examples during training.\r\n\r\nAre the 'heldout examples' from the segment that was 'replaced'?\r\n\r\nNo. The heldout examples are never used for training in any of the runs, and the same heldout examples are used for all the different runs\r\n\r\nBy 'output of the trained model' you mean the output of the network or the objective functions scores for different cases?  \r\n\r\nThe output of the network. We believe using the score given by the network to the different classes would not result in a very different graph, though we have not run that experiment.\r\n\r\nNote that we've changed the text to make this observation more apparent.\r\n\r\n* Regarding the x-axis of Figure 2. The x-axis is the index of the segment that was replaced. The curve is monotonic because early examples always have a larger influence on how the model behaves compared to older examples. One could think of the early stages of learning as picking the basin of attraction of the local minima in which the model will end. As such, this part will cause the most variance in the output of the model. The same observation was done by Erhan et al. when running a very similar experiment (as noted in the paper, we used the same protocol, and just increased the dataset size).\r\n\r\n* Regarding running NGD in batch mode explaining Fig 2. No. Both NGD and SGD used a minibatch of the same size (512 examples). To account for the possibility of the metric being singular, we also used damping for NGD. Since both method processed the same amount of data per step,  we believe that the comparison is fair.\r\n\r\n* Regarding how to choose parameters for the experiment to be fair. We have done roughly what the reviewer is suggesting. By amount of time we used number of steps, and both methods reached roughly the same error of 49.8%  after the same amount of steps as indicated in the text and on the plot.\r\n\r\n* Overall we have rephrased this section of the paper. We state the hypothesis we want to test before explaining the experiment. We also emphasis that we talk about the overall variance, and moved over some of the details from the appendix (like minibatch size for both algorithms). \r\n\r\n* Regarding just simply running several trials with the same data and use this to measure overall variance. The question we are trying to answer is how does the order of the training examples affect training (not the initialization of the parameters). Specifically we used the same seed for any random number generator through the code, to make sure that we initialize the model in the same way. The only difference between the runs was using different data examples from the same distribution. The purpose of this experiment is to investigate an worrisome observation made earlier by Erhan et al. regarding the importance of the order of the training examples for SGD. Specifically we believe that, by apparently being more robust, NGD may suffer less when run on a large non perfectly homogeneous dataset, by making the question of which examples it needs to see first less important. SGD might be affected more, resulting in models with different behaviors depending to how the data is showed to the model\r\n\r\n\r\n* Regarding NCG being a 2nd order method or not. It is a matter of semantics and we agree with the reviewer that from this perspective nonlinear conjugate gradient might not be a 2nd-order method. However, the premise of nonlinear conjugate gradient is to move along directions that are locally conjugate with respect to the Hessian and in some sense to rely on the curvature information for speeding up convergence. We do rely on this to argue that our modification to NGD, natural conjugate gradient, takes into account, to some extend, the curvature information of the error by moving in directions that are locally conjugate the Hessian of the error. In this sense we do incorporate, to some degree, second order information alongside the KL curvature given by the natural gradient direction. We do not intend to claim that there might not be a more proper way of injecting second error information of the error in the algorithm.\r\n\r\n* Regarding the wording 'real conjugate direction'. We do a fair amount of language abuse in this section and rectified this in our edits. We mean that even locally the directions might not be conjugate to the curvature of the function if we are to adopt the manifold perspective. This is because, if we consider the Riemannian manifold structure, the previous natural gradient direction and the new one are, in all likelihood, not compatible. That is because the Fisher matrix at step $\theta_{t-1}$ is different from the Fisher at $\theta_t$ and so the two vector belong to different spaces. One would have to transport one of the vectors in the space of the other in order to be able to compare the two vectors (and compute the dot product between them).\r\n\r\n* Regarding gaining some kind of conjugacy to the immediately previous update direction w.t.y. to the current value of G.  We are not trying to be conjugate to the metric matrix, but rather to the curvature of the error (when the parameters are assumed to respect the underlying Riemannian structure). In some sense the intuition that you are putting forward is very close to what we are doing.\r\n\r\nTo make a bit more clear the difference between what we are trying to do and normal nonlinear conjugate gradient we put forward the following observations. Arguably, even if the error is quadratic, unless the metric is constant through out the underlying Riemannian space, the directions we will pick will still not be conjugate to each other globally. That is because we are not looking at the curvature of the error as a function of the parameter $\theta$, but the error as a function of the density probability function $p_\theta(t|x)$ and therefore this curvature will not be constant for different values of \theta. Just think that we identify $\theta$ with the density probability function that it represents, and now we contracting or extending the R^P space of theta to mimic how the KL between of this functions changes. This means that while the error is quadratic in terms of $\theta$, it might not be so in terms of the functions $p_\theta(t|x)$. Of course what we are stating here is a generic argument. Given the specific model we use, and specific form of $p_\theta(t|x)$, assuming the error is quadratic might imply the metric to be constant everywhere. We are just arguing that in general one could construct examples where this is not true.\r\n\r\n* About the proof done on page 9 being obvious. We do not think we prove something that is counter-intuitive and therefore in some sense not obvious. We do however think that going through the mechanics of this little sketch of a proof can be helpful. But we agree, a proof can be provided only in terms of these kind of high level arguments. \r\n\r\n* We fixed the phrase 'we can show' to 'As we will show below'\r\n\r\n* Fix the Typo 'Eucledian'\r\n\r\n* We are unsure in what sense the equations on top of page 10 are not consistent. \r\n\r\n* Fixed the typo 'batch model'\r\n\r\n* Regarding the hyper-parameters used. Most of the details are in the appendix, because we felt the paper was already much longer than the suggested length. We use a maximum of 250 iterations and stop CG (MinRes actually) when the error is below 5e-4. The damping coefficient was initially set to 3., and adapted based on the Levenberg-Marquardt heuristic (the natural gradient version proposed in eqn 16/17).\r\n\r\n* Regarding our timings. Indeed our code is slower than Martens' Hessian-Free code. There can be several issues of why this is so. We already pointed out we do not use preconditioning, but this is not the only difference between our code and the one provided by Martens. We rely on MinRes instead of linear CG and we have not altered the stopping criterion on MinRes (as Martens did for CG to not waste computations). We do not backtrack through the steps of Minres in the case when moving in a certain direction results in an increase of the error. Probably there are some other fine-grained differences (for e.g. our heuristic only matches the one used by HF when MinRes would converge) that could impact negatively our code. MinRes itself is, in some sense, more expensive than CG.  We would like to finally make a note, that to get 0.8 it actually takes 3.5h not 9h. Recovering these values directly from the graph might be harder due to the logscale. The NGD run was stopped after 5h when it had an error of 0.65. Being that all algorithms share the same pipeline (and MinRes) we believe that our benchmark is in some sense valid.\r\n\r\n* Regarding not using preconditioning. There is no reason for not using preconditioning. We do intend to run those experiments as well. We also do not exclude NGD to come on top once all these enhancements are added to all algorithms. However we do believe that technically, NCG, by moving on directions that are locally conjugate the Hessian of the error with respect to the functional manifold, has an advantage over NGD. This advantage might be lost by either the approximation that go into the implementation, or might not be helpful for the specific problem we are addressing. However we see our proposed algorithm at a first attempt at integrating some second order information in natural gradient. There are many other choices, and we are currently exploring some of these choices. Specifically, we are investigating what kind of information does such a Hessian matrix carry, information that might not be  present in the Gauss-Newton. The answer of such question could help designing the right experiment.\r\n\r\nWe hope to be able to provide such results soon, within the next couple of weeks. It is an interesting question that we do want to answer. Specifically, as future work, we are interested in understanding what kind of information is present in a Hessian that is not present in the Gauss-Newton (or Fisher) matrix and when is this information useful. We believe there might be certain kinds of plateau in the error surface that do not translate in plateau of the KL-divergence."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Natural Gradient for Deep Networks", "decision": "submitted, no decision", "abstract": "The aim of this paper is three-fold. First we show that Hessian-Free (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of natural gradient descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive natural gradient from basic principles, contrasting the difference between two versions of the algorithm found in the neural network literature, as well as highlighting a few differences between natural gradient and typical second order methods. Lastly we show empirically that natural gradient can be robust to overfitting and particularly it can be robust to the order in which the training data is presented to the model.", "pdf": "https://arxiv.org/abs/1301.3584", "paperhash": "pascanu|revisiting_natural_gradient_for_deep_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392751440000, "tcdate": 1392751440000, "number": 1, "id": "TfmU-xyHU5T9k", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "vz8AumxkAfz5U", "replyto": "eEvhY_ufacEkA", "signatures": ["Razvan Pascanu"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your comments. We have carefully looked at each one and merged them into the new version of the paper. The new version of the paper (version 7) is online. Allow us to answer your concerns.\r\n\r\nWith respect to the weak experimental section, we understand the reviewer's concern. Indeed we tried not to make big claims about the new proposed algorithm (NCG) or even to suggest that it is better then  a properly implemented and fine tuned Hessian Free optimization implementation. \r\n\r\nWe regard our work as a discussion on the theme of natural gradient. We believe that we provide a new interesting perspective on some optimization algorithms that were recently proposed for deep learning. We believe that understanding HF or KSD as natural gradient algorithms can be useful, as it offers new ways of extending or analysing these algorithms. Specifically, e.g., natural gradient is a first order method (wrt to the function that we try to minimize), meaning that we can now think of how to also incorporate second order information into the algorithm. This is something that has been done successfully for TONGA. Our proposed algorithm is in the paper as an example of how this can be done. Specifically, the role of section 9 is to justify that our theoretical analysis is not useless because, for example, we can do things such as this algorithm. We do not claim that our way of introducing second order information in natural gradient is optimal, or that there are no issues with the algorithm we proposed. We are actually intending to explore alternative ways of introducing second order information to natural gradient, and believe that our proposed algorithm might be quite inefficient at doing so. However, as it stands, it does provide proof that our theoretical analysis is not useless.\r\n\r\nWe believe that also disentangling TONGA from Amari's original natural gradient is also a useful contribution. While in general these two algorithms are not probably confused with each other, we have seen instances (within the deep learning literature) where they are. For this reason we opted, e.g., to use the name TONGA for the algorithm proposed by Le Roux and Bengio (even though originally it was called also natural gradient).\r\n\r\nLastly we provide some hypothesis of additional properties (beyond the list given in section 3) which we think make natural gradient a very promising learning algorithm in general. We have (as you pointed out) validated these hypothesis on a single dataset.  However, even only as hypothesis, we think such observations are interesting. There is also evidence in the literature pointing to the validity of some of our hypotheses. For example, in the KSD paper it is mentioned that using a different minibatch for computing the gradient from the one used for computing the metric or running BFGS is useful. The paper however does not uses the same explanation for this phenomena as the one we are providing.\r\n\r\nWe believe that, as it stands, given all the notational fixes and details proposed by the reviewers, our work can be useful to provide a few starting points for further work investigating such algorithms, by giving a new perspective and trying to connect many papers in the literature that do not seem particularly aware of each other.\r\n\r\nSome more detailed replies of your comments are as follows:\r\n\r\n\r\n* Regarding the paper by Mizotani and Demmel. Thank you for the citation, we added it to our reference list. We were, unfortunately, not aware of this work. \r\n\r\nIndeed our paper is deep learning centric. We do try to cite, as much as we are aware of, the work done on this subject in other fields. Thanks for helping us providing a better coverage of the literature.\r\n\r\nThis is one reason we opt to add Deep Learning in the title, such that the work is not confused with a generic review of natural gradient (or related algorithms). We are aware that these algorithms have a long history and they are being successfully applied in different subfields like, for example, natural gradient in reinforcement learning. We have tried to do a literature review as good as we could, and in our opinion the paper does a decent (though probably not perfect) job at citing work that is not specific to deep learning. \r\n\r\nThe paper we cite is of particular interest to us because of the way the Krylov subspace is altered. We try to explore this observation to provide an approach that incorporates second order information alongside the natural gradient (which relies on the manifold curvature).\r\n\r\n* Regarding the reason of why using the same examples for the metric (and gradient) could hurt learning (when the minibatch is relatively small) we haven extended our comments to better emphasis our understanding of this phenomena. \r\n\r\nOur intuition is that using the same training examples for both the Fisher matrix and gradient (when the batch is not large enough to perfectly match the statistics of the dataset) can result in overfitting the current minibatch.  Think, for example, of some particular deformation of most of the examples in a minibatch that appears correlated with the class that one has to predict (though on a global scale this specific deformation is not predictive of the output classes). The gradient on the minibatch will point towards also learning this feature. Normally SGD is not affected by such artifacts because one takes only a small step in the direction of the gradient and, in time, this step is cancelled by updates from other minibatches. However natural gradient can take large steps, because it rescales the gradient according to the metric. Specifically, if, for those examples that we have, the metric agrees that moving in this direction results in a steady change of the probability density function, it will take a large step. This means the model will waste capacity learning this particular feature that will cause it to make mistakes on subsequent training examples. \r\n\r\n* Regarding Figure 1. We are currently in the process of re-running these experiments. We agree that the choice of minibatch size seems arbitrarily and confusing. The reason was that, when using unlabeled data, we wanted to reduce the noise introduced by unlabeled examples (which were inherently noisier then the training examples) by increasing the minibatch size. We kept this larger minibatch size also when using different labeled examples.\r\n\r\n* We fixed the label of Figure 1 (top/bottom was replaced with left/right)\r\n\r\nRegarding Figure 2, we extended our explanation of the results and hypothesis we want to show. We also switched to log scale. \r\n\r\nThe goal of our experiment is not to show that natural gradient has lower relative variance of early examples compared to later examples. The goal of the experiment is to show overall lower variance. That means that, regardless of the order of inputs, natural gradient tends to behave more consistently, while one might get very different models when using SGD."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Natural Gradient for Deep Networks", "decision": "submitted, no decision", "abstract": "The aim of this paper is three-fold. First we show that Hessian-Free (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of natural gradient descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive natural gradient from basic principles, contrasting the difference between two versions of the algorithm found in the neural network literature, as well as highlighting a few differences between natural gradient and typical second order methods. Lastly we show empirically that natural gradient can be robust to overfitting and particularly it can be robust to the order in which the training data is presented to the model.", "pdf": "https://arxiv.org/abs/1301.3584", "paperhash": "pascanu|revisiting_natural_gradient_for_deep_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392751380000, "tcdate": 1392751380000, "number": 1, "id": "OZaGO59P2dOPM", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "vz8AumxkAfz5U", "replyto": "8-oWcSCRpb8Za", "signatures": ["Razvan Pascanu"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you again for all the comments. We have been through all of them and incorporated them into the new version of the article. The article is online (ver 7). We believe that the text is clearer now and meets ICLR quality standards. \r\n\r\nLet us go again through your points and address them. \r\n\r\n\r\n* We corrected the phrase 'density probability function'\r\n\r\n\r\n* Regarding p_\theta(z) and what it means for DNNs. The concept of natural gradient is not necessarily tied to DNNs. In section 2 we introduce the generic idea, both in the generative (unconditional) case and the conditional case (which includes DNNs). Compared to the standard approach of explaining natural gradient we try to avoid relying on the manifold description, but rather present the algorithm in a different light.  We use a constraint optimization at each step to find the right descent direction. While we believe this explanation is quite useful and a good contribution of the paper, it is not really novel. [1] briefly talks about it and [2] as well. We also described this idea in [3]. Section 2.1 relies specifically on DNNs.\r\n\r\n[1] Amari, S., 'Natural gradient works efficiently in learning', Neural Computations, 1998\r\n[2] Heskes, T. 'On natural learning and pruning in multilayered perceptrons', Neural Computation 2000\r\n[3] Desjardins, G., Pascanu, R., Courville, A. and Bengio, Y 'Metric-Free Natural Gradient for Joint-Training of Boltzmann Machines', ICLR 2013}\r\n\r\n\r\n* We have included the definition of the Fisher Information Matrix (equation 1 in the new version of the paper)\r\n\r\n\r\n* We have extended the first paragraph of section 2.1 into 2 paragraphs detailing what we mean by the probabilistic interpretation of a deep model. We have also pointed to chapters in Bishop were similar interpretations are introduced. \r\n\r\n\r\n* We corrected the phrase 'computed by model'\r\n\r\n\r\n* We have added a description of the Levenberg-Marquardt heuristic that we mention in Equation 15, now Equation 16. The heuristic comes from a trust region approach where we check how much we trust our approximation of the function and decide how much we need to damp the metric or Hessian. \r\n\r\n\r\n* Corrected the typos 'betweent', 'Eucladian', 'batch model', 'Riebiere'\r\n\r\n\r\n* Regarding how expensive it is to invert the metric (versus the other computations involved in a natural gradient descent step). We have not actually timed separately how long it takes to invert the matrix versus everything else. However, with no doubt, most of the time is spent in inverting the matrix. From empirical observations, for natural gradient descent, inverting the matrix can be ( substantially so) more than 90% of the time spent in each step. Regarding the number of steps, this varies from iteration to iteration, with a maximum of 250 steps. MinRes runs as long as the error we have at solving the linear system is larger then some value which is part of our hyper-parameters (set to 5e-4 in all our experiments). We clarified these points in the text.  \r\n\r\n\r\n* Corrected 'be obtain' as well as the notation sum_i^o and the phrase 'certain extend'\r\n\r\n\r\n* We made a note on the abuse of notation in Eq.27 (now eq. 28). Indeed \frac{1}{mathbf{y}} refers to applying the division element-wise. We also expressed the metric for the softmax activation in the same form as the other, namely J [something] J^T.  We edited equations 28 and 29. \r\n\r\n\r\n* As in the main paper, we have extended in the appendix as well what is the Levenberg-Marquardt heuristic that we try to mimic."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Natural Gradient for Deep Networks", "decision": "submitted, no decision", "abstract": "The aim of this paper is three-fold. First we show that Hessian-Free (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of natural gradient descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive natural gradient from basic principles, contrasting the difference between two versions of the algorithm found in the neural network literature, as well as highlighting a few differences between natural gradient and typical second order methods. Lastly we show empirically that natural gradient can be robust to overfitting and particularly it can be robust to the order in which the training data is presented to the model.", "pdf": "https://arxiv.org/abs/1301.3584", "paperhash": "pascanu|revisiting_natural_gradient_for_deep_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392357120000, "tcdate": 1392357120000, "number": 6, "id": "hTJQT_z-3SR-I", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "vz8AumxkAfz5U", "replyto": "vz8AumxkAfz5U", "signatures": ["anonymous reviewer a88c"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Revisiting Natural Gradient for Deep Networks", "review": "This paper looks at some connections between the natural gradient and updates based on the Gauss-Newton matrix which have appeared in various recent optimization algorithms for deep networks.  Some of these connections appear to be already mapped out, but the paper presents a unified picture and fills in additional details.  There are some interesting insights about how this interpretation can allow unlabelled data to be used in truncated Newton style methods like Hessian-free optimization (HF).  \r\n\r\nLater in the paper connections between Krylov Subspace Descent (KSD) and natural conjugate gradient algorithms, and a new algorithm is proposed which tries to combine the supposed advantages of several of these approaches.    This algorithm, which is a potentially interesting twist on HF that involves taking the previous update and the new proposed update (which will be an approximation of the natural gradient) and jointly optimizing over both of these.  However, I'm not convinced these is a better thing to do than just initialize CG from using previous iterations. While the experiments did demonstrate a distinct advantage to doing it this way in the setting involving no preconditioning, I remain skeptical.  \r\n\r\nOverall I think this is a solid paper, and should be accepted.  However, I really want the authors to address my various concerns before I can strongly recommend it.\r\n\r\n\r\nDetailed comments:\r\n\r\nPage 2: The notation in the second paragraph of section 2 is odd.  Although I understand what you mean, I don't think it is proper to use 'R^P->(R^N->[0,inf))'.   Also, 'p_theta(z) = F(theta)' is odd.  Shouldn't it be something like 'p_theta(z) = F(theta)(z)'?\r\n\r\nPage 2:  You should explain how a matrix-valued function of theta defines a metric on the space.  This is well known to mathematicians, but many in this audience won't be familiar with these concepts.\r\n\r\nPage 2:  Why use row vectors instead of columns vectors?  While technically valid, this will make reading your paper require more mental effort on part of the reader.\r\n\r\nPage 2:  'Natural gradient' is not an algorithm, it is a vector quantity.  You should say 'natural gradient descent' or something like that.   I know it is not uncommon to hear people say 'natural gradients' when implicitly referring to the algorithm, not the quantity, but I think this is a bad habit that should be avoided.\r\n\r\nPage 2:  'moving in the direction' -> 'moving some given distance in the direction'\r\n\r\nPage 2:  What do you mean by 'the change induced in our model is constant'?  Natural gradient descent type algorithms can use dynamically changing step sizes.  Did you mean to say, 'some given value'?\r\n\r\nMoreover, this statement doesn't distinguish natural gradient descent from gradient descent.  Gradient descent can also be thought of as (approximately) minimizing L w.r.t. some metric of change.  It may happen to be a parametrization-dependent metric, but the statement is still valid.\r\n\r\nMoreover, eqn. 2 isn't even what happens with algorithms that use the natural gradient.  They don't minimize L w.r.t. to some target change in KL, since this is typically intractable.  Instead, they use an approximation.  I know later on you talk about how this approximation is realized, but the point is that algorithms that use the natural gradient are not *defined* in terms of eqn. 2.  They are *defined* in terms of the iteration which can be interpreted as an approximation of this.\r\n\r\nPage 4:  This matrix used by Le Roux and others I think is called the 'empirical Fisher', perhaps due to its connection with the empirical distribution over t.  I think it was in a paper of Schraudolph that I remember hearing this term.\r\n\r\nPage 4: I would suggest you call it 'Hessian-free optimization'.  It sounds weird to call it just 'Hessian-free', despite the lack of O in the commonly used acronym.\r\n\r\nPage 6:   Eqn. 16 isn't exactly the same as the one from Martens (2010).  In particular, you seem to be implicitly using update'*G*update = grad'*inv(G)C*grad in the denominator.  This is only true if update = inv(G)*grad.  But this will only be true if the conjugate gradient method used converges, and in general it won't.\r\n\r\nPage 6: The citation here (and also likely the corresponding one in the intro) is wrong.  'Structural damping' is from: 'Learning Recurrent Neural Networks with Hessian-Free Optimization', not the paper cited.\r\n\r\nPage 6: Could you explain what you mean by 'fixed coefficient of 1 for the regularization term'?  As far as I understand, the structural damping term can have an arbitrary coefficient in front of it in the original paper.\r\n\r\nPage 6: 'just initializing the linear solver close to a solution'.  Actually, this is arguably more complex type of initialization than the one used in KSD.  It actually changes each basis vector of the Krylov subspace, instead of augmenting the space with just one extra direction. \r\n\r\nPage 6: I'm a bit confused about what is going on in section 6.  In particular, what is being said that is particular to the Fisher  information matrix interpretation of G?  Any relationship of KSD to kind of non-linear CG type algorithm seems to be a completely different issue (and one which is not really explored in this section anyway).\r\n\r\nPage 7:  You talk about using the same data to estimate the gradient and curvature/metric matrix introducing a bias.  \r\n\r\nHowever, this is going to be true for *any* choice of partial data for these computation.  In particular, your suggestion of taking independent samples for both will NOT make the estimate unbiased.  This is because the inverse of an unbiased estimator is not unbiased estimator of the inverse, and you need to take inverses of the matrix.  Thus this doesn't address the biasedness problem, it just gives a different biased result.\r\n\r\nMoreover, there are intuitive and technical reasons not to use different data for these estimates.  See the article 'Training Deep and Recurrent Neural Networks with Hessian-Free Optimization', section 12.1.\r\n\r\nDespite the potential issues for optimization, I think it is nonetheless an interesting observation that unlabelled data can be used to compute G in situations where there is a lot more of it than labelled data.  But keep in mind that, in the end, the optimization is driven by the gradient and not by G, and as long as the objective function is convex, this can't possibly help the overfitting problem.  Insofar as it seems to help for non-convex objectives, it is a bit mysterious.  It would be good to understand what is going on here, although I suspect that a theoretical account of this phenomenon would be nearly impossible to give due to the complexity of all of the pieces involved.\r\n\r\n\r\nPage 8:  It is hard to follow what is happenning in the pseudo code.  Some questions I had were:\r\n\r\nWhere are these 'newly sampled examples' coming from?\r\n\r\nWhat is the purpose of the first split into 'two large chunks'?\r\n\r\nAre the 'heldout examples' from the segment that was 'replaced'?\r\n\r\nBy 'output of the trained model' you mean the output of the network or the objective functions scores for different cases?\r\n\r\nFigure 2:  In figure 8, what is being plotted on the x-axis?  Is that the segment index?  Why should this quantity be monotonic in the index, which seems completely arbitrary due to the presumably random assignment of cases to segments?\r\n\r\nPage 8:  Were you running batch NGD here?   Wouldn't comparing a batch method to a stochastic method account for the 'variance' effects?\r\n\r\nAlso, how can you be sure that you are being fair when comparing the two methods in terms of learning rates.  It's probably not fair to compare them with the same learning rate, since both methods have completely different characteristics.  But then what values can you choose?   Maybe the only fair way would be to pick values that let both methods get to some destination error in roughly the same amount of time?  Even this seems not completely fair, but might be the best compromise.\r\n\r\n\r\nPage 8:  I'm confused about what this section is saying.  The hypothesis about the 'path that induces least variance' is never clearly articulated until after the results are presented.  Also, you should emphasize here that you are talking about variance of the final outputs.\r\n\r\n\r\nPage 8:  Why even run these variance experiments with these different chunks/segments at all?  Why not just take multiple runs with some fixed dataset and observe the variance?  What does this cross-validation style chunking actually add to the experiment?\r\n\r\n\r\nPage 9:  I don't know if it is correct to call NCG a 2nd-order method.  In some sense, it meets that exact definition of a first order method: the iterates are linear combinations of gradients from previous iterations.\r\n\r\nPage 9:  What do you mean by 'real conjugate direction'?  Conjugacy, as far as I know, is only strictly defined for quadratic functions.  Perhaps there is a natural way of talking about this in a more general setting, but I don't see how.\r\n\r\nMaybe you could achieve a kind of local conjugacy to the immediately previous update direction w.r.t. the current value of G, although is it obvious that if G were constant (e.g. the objective is a quadratic), that a new direction would be conjugate to *all* previous directions?   If this is true, it seems like it would be a well known interpretation of the conjugate gradient algorithm, and would be a much more general statement than what you showed (i.e. because you are replacing the function everywhere with its 2nd-order Taylor series with a fixed Hessian, so it might as well be quadratic).\r\n\r\nActually, isn't the proof of this fact obvious?  That is, one just has to look at the usual linear CG iterates and see that the new direction is a linear combination of the gradient and the old direction, and is optimally chosen from vectors within a subspace which contains the current gradient and the previous direction vector.\r\n\r\nPage 9:  'We can show' -> 'As we will show below'\r\n\r\nPage 9:  Typo: 'Eucladian'\r\n \r\nPage 10:  The notation for the natural gradient in the two equations at the top is not consistent.\r\n\r\nPage 10: Typo: 'batch model'\r\n\r\nPage 10: How many CG steps were used per update computation?  How did you handle the selection of the damping constant lambda here?\r\n\r\nPage 10:  If the baseline NGD is basically just HF, then it seems to be quite severely underperforming here.  In particular, an error of about 0.8 after about 10^4.5 s = 9 hours is far far too slow for that dataset given that you are using a GPU.  Even if measured in iterations, 316 is way too many.  On the other hand, the red curve is in the figures is consistent with what I've seen from the standard HF method.\r\n\r\nPerhaps this is due to lack of preconditioning.  However, why not use this in the experiments?  This is very easy to implement, and would make your results more directly comparable to previous ones.\r\n\r\nI suppose the fact that NCG-L is performing as well as it is without preconditioning is impressive, but something doesn't sit right with me about this.  I would be really interested to hear what happens if you incorporate preconditioning into your scheme."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Natural Gradient for Deep Networks", "decision": "submitted, no decision", "abstract": "The aim of this paper is three-fold. First we show that Hessian-Free (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of natural gradient descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive natural gradient from basic principles, contrasting the difference between two versions of the algorithm found in the neural network literature, as well as highlighting a few differences between natural gradient and typical second order methods. Lastly we show empirically that natural gradient can be robust to overfitting and particularly it can be robust to the order in which the training data is presented to the model.", "pdf": "https://arxiv.org/abs/1301.3584", "paperhash": "pascanu|revisiting_natural_gradient_for_deep_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392242400000, "tcdate": 1392242400000, "number": 5, "id": "eEvhY_ufacEkA", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "vz8AumxkAfz5U", "replyto": "vz8AumxkAfz5U", "signatures": ["anonymous reviewer 8445"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Revisiting Natural Gradient for Deep Networks", "review": "This paper explores the link between natural gradient and techniques such as Hessian-free or Krylov subspace descent. It then proposes a novel way of computing the new search direction and experimentally shows its value.\r\n\r\nI appreciate getting additional interpretations of these learning techniques and this is a definite value of the paper. The experiments, on the other hand, are weak and bring little value.\r\n\r\nAdditional comments:\r\n- To my knowledge, Krylov subspace descent is much older than 2012. It is for instance mentioned in Numerical Optimization (Nocedal) or 'Iterative scaled trust-region learning in Krylov subspaces via Peralmutter's implicit sparse Hessian-vector multiply' (Mizutani and Demmel). As it is, the literature looks very deep learning centric.\r\n\r\n- I appreciate the comment about using different examples for the computation of the Fisher information matrix and the gradient. However, it would be interesting to explore why using the same examples also hurts the training error. This also goes against the fact that using other examples from the training set yields a lower training error than using examples from the unlabeled set.\r\n\r\n- Figure 1: why do you use a minibatch of size 256 in the first case and a mainibatch of size 384 in the second? This introduces an unnecessary discrepancy.\r\n- Figure 1: 'top' and 'bottom' should be 'left' and 'right'.\r\n\r\n- Figure 2 is not very convincing as the red curve seems to drop really low, much lower than the blue one. Thus, you prove that NGD introduces less variance than MSGD, but you do not prove that the ratio of the importance of each example is different in both cases. Could you use a log scale instead?\r\n\r\n- Finally, the experimental section is weak. The only 'deep' part, which appears prominently in the title, consists of an experiment ran on one deep model. This is a bit short to provide a convincing argument about the proposed method.\r\n\r\nPros: nice interpretation of natural gradient and connections with existing algorithm\r\n\r\nCons:\r\nWeak experimental section\r\nThe paper could be better written\r\nIt feels a bit too 'deep' centric to me, whether in the references or the title despite its little influence on the actual paper."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Natural Gradient for Deep Networks", "decision": "submitted, no decision", "abstract": "The aim of this paper is three-fold. First we show that Hessian-Free (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of natural gradient descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive natural gradient from basic principles, contrasting the difference between two versions of the algorithm found in the neural network literature, as well as highlighting a few differences between natural gradient and typical second order methods. Lastly we show empirically that natural gradient can be robust to overfitting and particularly it can be robust to the order in which the training data is presented to the model.", "pdf": "https://arxiv.org/abs/1301.3584", "paperhash": "pascanu|revisiting_natural_gradient_for_deep_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390504860000, "tcdate": 1390504860000, "number": 4, "id": "3NcexfRXwZ3XL", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "vz8AumxkAfz5U", "replyto": "vz8AumxkAfz5U", "signatures": ["Razvan Pascanu"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Thank you for the review. All your points are well taken. The only thing I can say in my defense is that I was worried of the paper being already too long, but you are right. It is important to make the paper clear about what it tries to say. Needless to say I will fix all the typos pointed out (thank you for spotting them). \r\n\r\nI will also try to add  more on the effect of the minibatch size and number of iterations for inverting G. What I can say for now, based on sporadic observations that I've made is that the number of iterations is not that important as long as it is larger than some threshold. The threshold does depend on both the data and model that you use (basically on the error surface). \r\n\r\n\r\nRegarding the minibatch size, my claim is that as long as you have a learning rate in front of your gradient, and you are willing to damp your metric (some of these things might result in slower convergence) you can go to much smaller minibatches. For the NNCG algorithm, going to smaller minibatches seems to hurt. \r\n\r\nMy understanding of this behaviour is as follows. NNCG (or even NGD with a line search) tries to move at each step as much as it can to minimize the cost on that minibatch. If the minibatch is not sufficiently large, what happens is that you end up overfitting the current minibatch, resulting in overall worse training error and validation error. Doing the linesearch on a different minibatch than the one used to compute the gradient helps to some degree (makes this overfitting to happen less, as the gradient might not point exactly in the directions required to overfit the minibatch), it is important to have a sufficiently large minibatch to have the same statistics as the whole training set (if we only care about optimization). Some of these ideas are also presented in section 7, but I agree that they are in a very condense manner. I will try to expand on this idea, also providing more evidence (extra experiments). \r\n\r\nI will write back once I've updated the paper with a detail list of changes. It might take me a few days though (especially with the ICML deadline in the background)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Natural Gradient for Deep Networks", "decision": "submitted, no decision", "abstract": "The aim of this paper is three-fold. First we show that Hessian-Free (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of natural gradient descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive natural gradient from basic principles, contrasting the difference between two versions of the algorithm found in the neural network literature, as well as highlighting a few differences between natural gradient and typical second order methods. Lastly we show empirically that natural gradient can be robust to overfitting and particularly it can be robust to the order in which the training data is presented to the model.", "pdf": "https://arxiv.org/abs/1301.3584", "paperhash": "pascanu|revisiting_natural_gradient_for_deep_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390441380000, "tcdate": 1390441380000, "number": 3, "id": "8-oWcSCRpb8Za", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "vz8AumxkAfz5U", "replyto": "vz8AumxkAfz5U", "signatures": ["anonymous reviewer 98fe"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Revisiting Natural Gradient for Deep Networks", "review": "The paper contains some interesting analysis showing how certain previous methods\r\ncan be viewed as implementations of Natural Gradient Descent.\r\n\r\nThe paper is novel and is on an important topic, I believe. There are parts where\r\nI believe the paper could be clearer (see detailed comments below)- parts could\r\nbe expanded upon.  I believe the ICLR length guidelines are not strict.\r\n\r\nThere are also some interesting experiments that compare NG (and its variants)\r\nto SGD.  (it would be nice to have some more details on where the time is taken\r\nin NG, and what happens when you change the size of the subset used to compute\r\nthe metric, and the number of iterations used in approximate inversion of G).\r\n\r\nDetailed comments follow.\r\n\r\n\r\n-----------\r\ndensity probability function -> probability density function\r\n\r\nnot sure how p_{\theta}(z) relates to DNNs-- what is z?  The input features?  The class-label output?  Both?\r\n\r\nDefinition of Fisher information matrix?\r\n\r\nIn Sec. 2.1 when you talk about 'the probabilistic interpreation p_{\theta}(t | x)', this is very\r\nunclear.  I figured out what you meant only after looking at the Appendix.  In fact, the notation\r\nis inherently quite unclear because for me, the canonical example of deep learning is with softmax\r\noutput, and here you view t as a multinomial distribution over a single categorical variable,\r\nwhich would be a scalar not a vector, so the use of bold-face t is misleading, because it's more\r\nlike p_{\theta}(y | x) where y is the discrete label.  I think if\r\nyou want people to understand this paper it would make sense to devote a paragraph or two  here to\r\nexplaining what you mean.\r\n\r\ncomputed by model -> computed by the model\r\n\r\n\r\nAround Eq. (15), where you talk about the Levenburg-Marquardt heuristic, it was\r\nvery unclear to me.  I think you need to take a step back and explain what it is\r\nand how you are applying it.  Otherwise (and this is a risk with other parts of\r\nthe paper too) there is a danger that the only people who can understand it are\r\nthose who already understand the material so deeply that the paper's conclusions\r\nwould already be obvious.\r\n\r\nbetweent -> between\r\n\r\nEucladian -> Euclidean\r\n\r\nbatch model -> batch mode\r\n\r\nRiebiere -> Ribiere (twice)\r\n\r\nin Sec. 10-- I don't think you address how many iterations you use in the solver for\r\n approximately mutiplying by G^{-1}, and how much of the total time this takes.\r\n\r\n\r\n\r\n--\r\nAppendix:\r\n\r\nbe obtain -> be obtained\r\n\r\nshould sum_i^o be instead sum_{i=1}^o ?  (this appears more than once)\r\n\r\nIn the last line of Eq. 27, the notation doesn't really work, you are treating the vector y like a scalar,\r\ne.g. dividing by it.  This same notation appears in the main text.\r\nIt might be necessary to put, say, diag({\bf p}), and explain that p_i = 1/(y_i (1-y_i)).\r\nOr maybe you can point out somewhere in the text that these equations should be interpreted\r\nelementwise.\r\nIn eqs 28 and 29 you should have non-bold in y_i, and also there is an odd inconsistency of notation with\r\neq. 27-- you drop the J [something] J^T notation which I think is still applicable here.\r\n\r\ncertain extend -> certain extent\r\n\r\nThe text 'Following the functional manifold interpretation..' with eqs. 30 and 31 in the appendix,\r\nis quite unclear.  I think you are maybe assuming a deep familiarity with Martens' Hessian-Free\r\npaper and/or the Levenburg-Marquardt algorithm."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Natural Gradient for Deep Networks", "decision": "submitted, no decision", "abstract": "The aim of this paper is three-fold. First we show that Hessian-Free (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of natural gradient descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive natural gradient from basic principles, contrasting the difference between two versions of the algorithm found in the neural network literature, as well as highlighting a few differences between natural gradient and typical second order methods. Lastly we show empirically that natural gradient can be robust to overfitting and particularly it can be robust to the order in which the training data is presented to the model.", "pdf": "https://arxiv.org/abs/1301.3584", "paperhash": "pascanu|revisiting_natural_gradient_for_deep_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390330860000, "tcdate": 1390330860000, "number": 2, "id": "GoROGWWS59GLu", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "vz8AumxkAfz5U", "replyto": "vz8AumxkAfz5U", "signatures": ["Razvan Pascanu"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Thank you for pointing out the typos. I really liked your KSD paper :)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Natural Gradient for Deep Networks", "decision": "submitted, no decision", "abstract": "The aim of this paper is three-fold. First we show that Hessian-Free (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of natural gradient descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive natural gradient from basic principles, contrasting the difference between two versions of the algorithm found in the neural network literature, as well as highlighting a few differences between natural gradient and typical second order methods. Lastly we show empirically that natural gradient can be robust to overfitting and particularly it can be robust to the order in which the training data is presented to the model.", "pdf": "https://arxiv.org/abs/1301.3584", "paperhash": "pascanu|revisiting_natural_gradient_for_deep_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390039800000, "tcdate": 1390039800000, "number": 1, "id": "wKQ2KPGos6Xxb", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "vz8AumxkAfz5U", "replyto": "vz8AumxkAfz5U", "signatures": ["Daniel Povey"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Thanks for noticing our paper!\r\nnoticed a couple of typos: eucladian -> euclidean, model->mode"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Natural Gradient for Deep Networks", "decision": "submitted, no decision", "abstract": "The aim of this paper is three-fold. First we show that Hessian-Free (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of natural gradient descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive natural gradient from basic principles, contrasting the difference between two versions of the algorithm found in the neural network literature, as well as highlighting a few differences between natural gradient and typical second order methods. Lastly we show empirically that natural gradient can be robust to overfitting and particularly it can be robust to the order in which the training data is presented to the model.", "pdf": "https://arxiv.org/abs/1301.3584", "paperhash": "pascanu|revisiting_natural_gradient_for_deep_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387585980000, "tcdate": 1387585980000, "number": 21, "id": "vz8AumxkAfz5U", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "vz8AumxkAfz5U", "signatures": ["r.pascanu@gmail.com"], "readers": ["everyone"], "content": {"title": "Revisiting Natural Gradient for Deep Networks", "decision": "submitted, no decision", "abstract": "The aim of this paper is three-fold. First we show that Hessian-Free (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of natural gradient descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive natural gradient from basic principles, contrasting the difference between two versions of the algorithm found in the neural network literature, as well as highlighting a few differences between natural gradient and typical second order methods. Lastly we show empirically that natural gradient can be robust to overfitting and particularly it can be robust to the order in which the training data is presented to the model.", "pdf": "https://arxiv.org/abs/1301.3584", "paperhash": "pascanu|revisiting_natural_gradient_for_deep_networks", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "yoshua.bengio@gmail.com"]}, "writers": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 10}