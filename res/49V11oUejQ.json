{"notes": [{"id": "49V11oUejQ", "original": "Mxc9H_r2fwR", "number": 2457, "cdate": 1601308271379, "ddate": null, "tcdate": 1601308271379, "tmdate": 1614985703144, "tddate": null, "forum": "49V11oUejQ", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Efficient Robust Training via Backward Smoothing", "authorids": ["~Jinghui_Chen1", "~Yu_Cheng1", "~Zhe_Gan1", "~Quanquan_Gu1", "~Jingjing_Liu2"], "authors": ["Jinghui Chen", "Yu Cheng", "Zhe Gan", "Quanquan Gu", "Jingjing Liu"], "keywords": ["Efficient Robust Training", "Backward Smoothing", "Robustness"], "abstract": "Adversarial training is so far the most effective strategy in defending against adversarial examples. However, it suffers from high computational cost due to the iterative adversarial attacks in each training step. Recent studies show that it is possible to achieve Fast Adversarial Training by performing a single-step attack with random initialization. Yet, it remains a mystery why random initialization helps. Besides, such an approach still lags behind state-of-the-art adversarial training algorithms on both stability and model robustness. In this work, we develop a new understanding towards Fast Adversarial Training, by viewing random initialization as performing randomized smoothing for better optimization of the inner maximization problem. From this perspective, we show that the smoothing effect by random initialization is not sufficient under the adversarial perturbation constraint. A new initialization strategy, \\emph{backward smoothing}, is proposed to address this issue and significantly improves both stability and model robustness over single-step robust training methods. Experiments on multiple benchmarks demonstrate that our method achieves similar model robustness as the original TRADES method, while using much less training time (~3x improvement with the same training schedule). ", "one-sentence_summary": "We propose a new principle towards understanding Fast Adversarial Training, and a new initialization strategy that significantly improves both stability and model robustness over the single-step robust training methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|efficient_robust_training_via_backward_smoothing", "supplementary_material": "/attachment/f69958051f5c543363c667035bcf9f4de83ec9e2.zip", "pdf": "/pdf/ee969cde7ffd2ade652f069571e71705a0b27adb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aMIRgsXyhS", "_bibtex": "@misc{\nchen2021efficient,\ntitle={Efficient Robust Training via Backward Smoothing},\nauthor={Jinghui Chen and Yu Cheng and Zhe Gan and Quanquan Gu and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=49V11oUejQ}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 21, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "bTt2sFKuOGy", "original": null, "number": 1, "cdate": 1610040441457, "ddate": null, "tcdate": 1610040441457, "tmdate": 1610474042506, "tddate": null, "forum": "49V11oUejQ", "replyto": "49V11oUejQ", "invitation": "ICLR.cc/2021/Conference/Paper2457/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper studies efficient robust training. The key idea is to use backward smoothing as an advanced random initialization to improve a model's adversarial robustness. The approach is sound, well grounded, and quite logical. Results demonstrate the effectiveness.\n\nHowever, there exists some limitations:\n\n1) Andriushchenko & Flammarion, 2020 gives a better and more fundamental explanation on how to address fast adversarial training.\n\n2) Backward smoothing does not generalize to standard adversarial training. In other words, it only works for KL divergence loss rather than cross-entropy loss, and it seems that backward smoothing does not address the fundamental problem of fast adversarial training.\n\n3) If we compare the performance of Fast TRADES and Backward Smoothing since Backward Smoothing intends to improve Fast TRADES, there is always a tradeoff between clean accuracy and adversarial robustness, e.g., Table 4 and Table 8. \n\n4) Randomized smoothing is helpful for one-step adversarial training and randomized smoothing in general seems to be orthogonal to the proposed method. Moreover, 2-step PGD training can perform similarly well to backward smoothing while being much simpler conceptually.\n\nIn the end, I think that this paper may not be ready for publication at ICLR, but the next version must be a strong paper if above limitations can be well addressed."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Robust Training via Backward Smoothing", "authorids": ["~Jinghui_Chen1", "~Yu_Cheng1", "~Zhe_Gan1", "~Quanquan_Gu1", "~Jingjing_Liu2"], "authors": ["Jinghui Chen", "Yu Cheng", "Zhe Gan", "Quanquan Gu", "Jingjing Liu"], "keywords": ["Efficient Robust Training", "Backward Smoothing", "Robustness"], "abstract": "Adversarial training is so far the most effective strategy in defending against adversarial examples. However, it suffers from high computational cost due to the iterative adversarial attacks in each training step. Recent studies show that it is possible to achieve Fast Adversarial Training by performing a single-step attack with random initialization. Yet, it remains a mystery why random initialization helps. Besides, such an approach still lags behind state-of-the-art adversarial training algorithms on both stability and model robustness. In this work, we develop a new understanding towards Fast Adversarial Training, by viewing random initialization as performing randomized smoothing for better optimization of the inner maximization problem. From this perspective, we show that the smoothing effect by random initialization is not sufficient under the adversarial perturbation constraint. A new initialization strategy, \\emph{backward smoothing}, is proposed to address this issue and significantly improves both stability and model robustness over single-step robust training methods. Experiments on multiple benchmarks demonstrate that our method achieves similar model robustness as the original TRADES method, while using much less training time (~3x improvement with the same training schedule). ", "one-sentence_summary": "We propose a new principle towards understanding Fast Adversarial Training, and a new initialization strategy that significantly improves both stability and model robustness over the single-step robust training methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|efficient_robust_training_via_backward_smoothing", "supplementary_material": "/attachment/f69958051f5c543363c667035bcf9f4de83ec9e2.zip", "pdf": "/pdf/ee969cde7ffd2ade652f069571e71705a0b27adb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aMIRgsXyhS", "_bibtex": "@misc{\nchen2021efficient,\ntitle={Efficient Robust Training via Backward Smoothing},\nauthor={Jinghui Chen and Yu Cheng and Zhe Gan and Quanquan Gu and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=49V11oUejQ}\n}"}, "tags": [], "invitation": {"reply": {"forum": "49V11oUejQ", "replyto": "49V11oUejQ", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040441444, "tmdate": 1610474042491, "id": "ICLR.cc/2021/Conference/Paper2457/-/Decision"}}}, {"id": "4RM87Worw_B", "original": null, "number": 4, "cdate": 1603950586618, "ddate": null, "tcdate": 1603950586618, "tmdate": 1606765469396, "tddate": null, "forum": "49V11oUejQ", "replyto": "49V11oUejQ", "invitation": "ICLR.cc/2021/Conference/Paper2457/-/Official_Review", "content": {"title": "Insufficient motivations and the improvements seem to be a trade-off.", "review": "This work proposes backward smoothing as an advanced random initialization to improve a model's adversarial robustness. The paper is well-written and easy to follow. However, I have the following concerns:\n1) The paper argues that random initialization can help fast adversarial training because it helps improve the smoothness of the loss function. However, a recent work \"Understanding and Improving Fast Adversarial Training\" has a very nice explanation and solution on this problem, which is much more convincing to me.\n2) Even if I accept the condition that random initialization helps improve the smoothness of the loss function, the motivation showing in Figure 3 that the current smoothing effect by using random initialization is not sufficient can not convince me. It is definitely in the expectation that KL divergence between clean and adversarial examples would be much greater than random initialization. And I do not think that this can shed any light on the insufficient smoothness by the current random initialization.\n3) At the end of section 4, the authors mention that \"Note that the proposed Backward Smoothing seems also compatible with Adversarial Training. However, Adversarial Training does not contain terms using KL divergence loss, which may hinder its performance. We will show this empirically in Section 5.\" I tried to find experiments with backward smoothing applied to standard adversarial training but did not find these results. It would be good if the authors can point it to me if I missed them. Conditioned on  that we observe backward smoothing does not greatly help standard adversarial training, this failure case raises a great concern to me. Compared to random initialization, backward smoothing provides a better random initialization which can provide better smoothness. My question is: why a better random initialization can not generalize to standard adversarial training?\n4) For Table 5, it is necessary to include FAST TRADES as backward smoothing is mainly applied to TRADES.\n5) Does backward smoothing really help adversarial robustness or it just plays the tradeoff game between clean accuracy and adversarial robustness? For example in Table 8, comparing FAST Trades and backward smoothing, backward smoothing provides better adversarial robustness but sacrifices a big clean accuracy. \n\nIn all, I vote for a reject for the current version of this work.\n\n********After Discussion*************\nI thank the authors for answering my questions and performing additional experiments. Some of my concerned are addressed during the discussion stage. Therefore, I raise my score from 4 to 5. However,  I still hold my opinions that this work does not have a strong motivation, does not help too much for standard adversarial training and has a potential trade-off problem.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2457/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Robust Training via Backward Smoothing", "authorids": ["~Jinghui_Chen1", "~Yu_Cheng1", "~Zhe_Gan1", "~Quanquan_Gu1", "~Jingjing_Liu2"], "authors": ["Jinghui Chen", "Yu Cheng", "Zhe Gan", "Quanquan Gu", "Jingjing Liu"], "keywords": ["Efficient Robust Training", "Backward Smoothing", "Robustness"], "abstract": "Adversarial training is so far the most effective strategy in defending against adversarial examples. However, it suffers from high computational cost due to the iterative adversarial attacks in each training step. Recent studies show that it is possible to achieve Fast Adversarial Training by performing a single-step attack with random initialization. Yet, it remains a mystery why random initialization helps. Besides, such an approach still lags behind state-of-the-art adversarial training algorithms on both stability and model robustness. In this work, we develop a new understanding towards Fast Adversarial Training, by viewing random initialization as performing randomized smoothing for better optimization of the inner maximization problem. From this perspective, we show that the smoothing effect by random initialization is not sufficient under the adversarial perturbation constraint. A new initialization strategy, \\emph{backward smoothing}, is proposed to address this issue and significantly improves both stability and model robustness over single-step robust training methods. Experiments on multiple benchmarks demonstrate that our method achieves similar model robustness as the original TRADES method, while using much less training time (~3x improvement with the same training schedule). ", "one-sentence_summary": "We propose a new principle towards understanding Fast Adversarial Training, and a new initialization strategy that significantly improves both stability and model robustness over the single-step robust training methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|efficient_robust_training_via_backward_smoothing", "supplementary_material": "/attachment/f69958051f5c543363c667035bcf9f4de83ec9e2.zip", "pdf": "/pdf/ee969cde7ffd2ade652f069571e71705a0b27adb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aMIRgsXyhS", "_bibtex": "@misc{\nchen2021efficient,\ntitle={Efficient Robust Training via Backward Smoothing},\nauthor={Jinghui Chen and Yu Cheng and Zhe Gan and Quanquan Gu and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=49V11oUejQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "49V11oUejQ", "replyto": "49V11oUejQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2457/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538095953, "tmdate": 1606915784180, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2457/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2457/-/Official_Review"}}}, {"id": "4JwCGy-8Kp2", "original": null, "number": 2, "cdate": 1603814512812, "ddate": null, "tcdate": 1603814512812, "tmdate": 1606477316179, "tddate": null, "forum": "49V11oUejQ", "replyto": "49V11oUejQ", "invitation": "ICLR.cc/2021/Conference/Paper2457/-/Official_Review", "content": {"title": "Good empirical results, theoretical explanation to improve", "review": "The paper proposes Backward Smoothing to close the gap in terms of robustness between standard multi-step and fast (one or two steps) adversarial training (AT). In particular, at high level, given a network $f$, a point $x$ and its logits $f(x)$ it suggests to first sample a random perturbation $\\psi$ in the logits space around $f(x)$ and second computing, via one step of gradient descent, $\\xi^*$ so to minimize the KL-divergence between the softmax values of $f(x) + \\psi$ and $f(x + \\xi^*)$. Then $x + \\xi^*$ is used as starting point for one-step AT. The rationale behind the scheme is that the random step commonly added to $x$ before AT might be not sufficient to achieve a smoothing effect on the loss function and then an effective optimization.\n\nPros\n1. The proposed method is simple and easily integrated into the TRADES training scheme.\n2. The empirical results are strong, as Backward Smoothing  achieves better robustness than other fast AT versions and is more efficient that multi-step AT (including TRADES) with little loss in robustness. Also, the models are thoroughly tested with many white- and black-box attacks.\n\nCons\n1. The theoretical explanation of why random initialization helps and the connection to randomized smoothing (Sec. 3) is not very convincing in my opinion. Randomized smoothing aims at estimating better directions for the gradient step, but it's not clear whether this is the case when using a single random point (this would need to be experimentally validated). An alternative explanation of the role of random initialization is presented in (Andriushchenko & Flammarion, 2020), which would be worth discussing.\n2. The proposed method seems to share some similarities with ODI (Tashiro et al., 2020), which also proposes a scheme to find good starting points for PGD-based adversarial attacks optimizing some (randomly sampled) loss in the logits space. While in that work there's no focus on adversarial training, the overall idea looks similar, and I think the authors should comment on this.\n\nOther comments and questions\n1. In the comparison to other methods, I think for completeness it'd be good to show also Fast AT with 2 steps, since it has the same computational cost as Backward Smoothing and Fast AT performs better than Fast TRADES especially on CIFAR-100 and Tiny ImageNet. Additionally, including other techniques for fast adversarial training (Shafahi et al., 2019, Andriushchenko & Flammarion, 2020) might give a more complete picture.\n2. In Table 5, the gap between Backward Smoothing and TRADES is larger than what reported in Table 2. Is this because of the larger architecture or just the evaluation with stronger attacks (or something else)?\n3. How does the proposed technique behave for larger $\\epsilon$? Is it still able to achieve results similar to multi-step AT (usually for larger $\\epsilon$ the gap between single- and multi-step AT becomes even larger)?\n\nOverall, as mentioned above, the paper presents strong empirical results which support the proposed method. However, the justification for the method as presented is not particularly convincing and should be better validated. Also, a discussion of the similarities with prior work seems needed.\n\nTashiro et al., \"Diversity can be Transferred: Output Diversification for White- and Black-box Attacks\"\n\n---\nUpdate after rebuttal\n\nAfter reading the authors' response and the other reviews, I think the paper has quite clear pros and cons.\n\nThe experimental results (especially at $\\epsilon=8/255$) are strong and the underlying idea of finding a good starting point for single step adversarial training makes sense to me (see reply below).\n\nOn the other side, the initial (and partially current) explanation relying on randomized smoothing given by the authors is not convincing, in particular when discussing the role of the random step in the success of Fast AT. The new experiments provided in the revision which rather analyze the smoothness of the starting point found by Backward Smoothing look like a much better explanation of the success of the proposed method (note that although the overlap of terminology I don't see a direct interpretation of the smoothness of the loss function at some specifically crafted point in terms of randomized smoothing). This should be more thoroughly analyzed and commented, which would consist in a quite major update of the paper in my opinion.\n\nThe current version doesn't provide an exhaustive motivation and analysis of the proposed method (in any direction, randomized smoothing or others), although the revision improves in this sense. Then, although I appreciate the good empirical results and I'm still in favor of the proposed method, I have to lower the score. I encourage the authors to clarify the weaknesses of the paper, which might result in a better understanding of what's needed for a successful fast adversarial training.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2457/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Robust Training via Backward Smoothing", "authorids": ["~Jinghui_Chen1", "~Yu_Cheng1", "~Zhe_Gan1", "~Quanquan_Gu1", "~Jingjing_Liu2"], "authors": ["Jinghui Chen", "Yu Cheng", "Zhe Gan", "Quanquan Gu", "Jingjing Liu"], "keywords": ["Efficient Robust Training", "Backward Smoothing", "Robustness"], "abstract": "Adversarial training is so far the most effective strategy in defending against adversarial examples. However, it suffers from high computational cost due to the iterative adversarial attacks in each training step. Recent studies show that it is possible to achieve Fast Adversarial Training by performing a single-step attack with random initialization. Yet, it remains a mystery why random initialization helps. Besides, such an approach still lags behind state-of-the-art adversarial training algorithms on both stability and model robustness. In this work, we develop a new understanding towards Fast Adversarial Training, by viewing random initialization as performing randomized smoothing for better optimization of the inner maximization problem. From this perspective, we show that the smoothing effect by random initialization is not sufficient under the adversarial perturbation constraint. A new initialization strategy, \\emph{backward smoothing}, is proposed to address this issue and significantly improves both stability and model robustness over single-step robust training methods. Experiments on multiple benchmarks demonstrate that our method achieves similar model robustness as the original TRADES method, while using much less training time (~3x improvement with the same training schedule). ", "one-sentence_summary": "We propose a new principle towards understanding Fast Adversarial Training, and a new initialization strategy that significantly improves both stability and model robustness over the single-step robust training methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|efficient_robust_training_via_backward_smoothing", "supplementary_material": "/attachment/f69958051f5c543363c667035bcf9f4de83ec9e2.zip", "pdf": "/pdf/ee969cde7ffd2ade652f069571e71705a0b27adb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aMIRgsXyhS", "_bibtex": "@misc{\nchen2021efficient,\ntitle={Efficient Robust Training via Backward Smoothing},\nauthor={Jinghui Chen and Yu Cheng and Zhe Gan and Quanquan Gu and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=49V11oUejQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "49V11oUejQ", "replyto": "49V11oUejQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2457/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538095953, "tmdate": 1606915784180, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2457/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2457/-/Official_Review"}}}, {"id": "GrbC_6RQPyX", "original": null, "number": 3, "cdate": 1603824172009, "ddate": null, "tcdate": 1603824172009, "tmdate": 1606330474431, "tddate": null, "forum": "49V11oUejQ", "replyto": "49V11oUejQ", "invitation": "ICLR.cc/2021/Conference/Paper2457/-/Official_Review", "content": {"title": "Good experimental results but I don't agree with the motivation of the method (+ results on larger eps should be shown)", "review": "**Summary:**\nThe paper proposes a new algorithm for performing fast adversarial training. The proposed algorithm consists in solving the inner maximization problem in the following way: first, one takes a step of projected gradient descent (PGD) wrt an auxiliary loss (motivated by the idea of backward smoothing), and then one takes another step of PGD wrt the original loss function which is the KL divergence as suggested by the TRADES paper. The authors argue that taking two steps of PGD wrt different losses leads (perhaps, surprisingly) to higher robustness compared to taking two steps of PGD wrt the same original loss function.\n\n\n**Pros:**\n- Better robustness of models when computations are bounded with only 2 steps of PGD to solve the inner maximization problem.\n- Thorough robustness evaluation which includes the recent AutoAttack.\n- Ablation studies that help to better understand the influence of the two main hyperparameters of the proposed method.\n- The paper is clearly written although I would rather disagree with the justifications for the proposed method (see below).\n\n\n**Cons:**\n- The explanations from Section \u201cWhy random initialization helps?\u201d are not convincing because randomness is just one part of the solution proposed by Wong et al. (2020) to solve the inner maximization procedure. Another part is reducing the step size from 2\\*eps to 1.25\\*eps which effectively reduces the norm of the perturbation compared to standard FGSM as discussed in a recent related paper Andriushchenko & Flammarion (2020). Therefore, taking into account the crucial effect of the step size as shown in Fig. 3 in Wong et al. (2020), I don\u2019t think there is a reason to believe that randomness *alone* has any positive influence on the solution of the inner maximization problem. This is my main concern as the whole paper is built on the connection to randomized smoothing. I would be open to raise the score if you can show that randomization alone (with a single $\\xi$ or even multiple $\\xi$ to approximate the expectation in Eq. (3.2)) with the full step size (2\\*eps) alone can prevent catastrophic overfitting and help to achieve better robustness.\n- All the experiments are performed for eps=8/255 which is close to the eps for which even standard FGSM leads to high robustness (e.g., see Fig. 1 in Andriushchenko & Flammarion (2020)). Thus, it would be highly beneficial to show that the proposed approach works equally well compared to the competing approaches for a higher eps, e.g. eps=16/255 on CIFAR-10.\n- The proposed procedure of backward smoothing already appears in the literature in [Diversity can be Transferred: Output Diversification for White- and Black-box Attacks](https://arxiv.org/abs/2003.06878) where they propose a very similar algorithm but with the goal of obtaining a better adversarial attack (and thus they use multiple steps of PGD to solve it instead of just one). Although this paper is not yet published (accepted at NeurIPS\u201920), I think it would be important to at least acknowledge this connection.\n- I am still quite surprised that for the inner maximization problem, taking two steps of PGD wrt *different* losses leads to higher robustness compared to taking two steps of PGD wrt the same original loss function. In order to rule out potentially suboptimal choice of the step size for *Fast TRADES (2-step)*, I would be interested in seeing results of a grid search over it, i.e. similarly to Table 7 but for *Fast TRADES (2-step)*.\n\n\n**Minor suggestions:**\n- Fig. 2: the best step size is at the boundary of the grid {10/255, 8/255, 7/255}, thus it would make sense to extend the grid with even smaller values of the step size to make sure that Fast TRADES is not reported with suboptimal hyperparameters.\n- I'm also a bit surprised that *\"Backward smoothing ... even outperforms the state-of-the-art robust training methods\"* since the SOTA robust training methods include much more steps of PGD and thus they solve better the inner maximization problem. Why is their performance worse? Can it be, for example, because their hyperparameters were not sufficiently tuned (unlike for proposed method)?\n- Table 5: the listed \"AT\" baseline is quite weak (44.04%) from Madry et al. (2018). It would be better to use the results / models from Rice et al. (2020) which show results of AT+early stopping comparable to that of TRADES.\n- Table 5: it would be useful to add *Fast TRADES (2-step)* to the table as this is the most interesting baseline.\n\n\n**Score:**\nMy current score is 4/10 but I would be willing to raise it if my first two concerns (mentioned in **Cons**) are resolved.\n\n------\n\n**Update after the public discussion:**\n\nThanks a lot to the authors for clarifying many details and providing additional experimental data. In the updated version of the paper, the authors improve the results of the baseline \"Fast TRADES (2-step)\" and add additionally a stronger baseline of \"Fast AT (2-step)\" (except on CIFAR-10 with eps=16/255 where it's missing). However, at least for eps=8/255 on CIFAR-10, CIFAR-100, Tiny ImageNet, the authors show consistent improvements over the baselines with comparable computational cost (\"Fast TRADES (2-step)\" and \"Fast AT (2-step)\"). This is an interesting result, although it's not clear to me whether it's specific to eps=8/255 or it would generalize also to higher epsilons such as eps=12/255 or eps=16/255. \n\nOn the other hand, I still think that the current motivation of the method is very incomplete and it is still unclear why the proposed method should work in the first place. Perhaps, it's a good idea to further develop the additional experiments about the curvature of the loss surface at different points in the input space.\n\nThen I think there is additional work to be done in terms of understanding what the proposed method actually does (even if we don't take into account how it was motivated -- via randomized smoothing or not). In particular, it's still completely unclear to me why 2 steps of PGD with respect to the original KL divergence (i.e. Fast TRADES (2-step)) works worse than first 1 step with respect to one KL-divergence and then 1 step with respect to another KL-divergence (i.e. Backward Smoothing). This seems quite ad-hoc and requires further explanations, in my opinion. Moreover, I find it also quite puzzling that Backward Smoothing even outperforms 10-step TRADES / AT as shown in Tables 3 and 4 -- not sure about a justification behind this.\n\nTaking into account all of this, I update my score from 4/10 to 5/10. I think the paper can still be improved in various ways: both in terms of the motivation and experimental results.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2457/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Robust Training via Backward Smoothing", "authorids": ["~Jinghui_Chen1", "~Yu_Cheng1", "~Zhe_Gan1", "~Quanquan_Gu1", "~Jingjing_Liu2"], "authors": ["Jinghui Chen", "Yu Cheng", "Zhe Gan", "Quanquan Gu", "Jingjing Liu"], "keywords": ["Efficient Robust Training", "Backward Smoothing", "Robustness"], "abstract": "Adversarial training is so far the most effective strategy in defending against adversarial examples. However, it suffers from high computational cost due to the iterative adversarial attacks in each training step. Recent studies show that it is possible to achieve Fast Adversarial Training by performing a single-step attack with random initialization. Yet, it remains a mystery why random initialization helps. Besides, such an approach still lags behind state-of-the-art adversarial training algorithms on both stability and model robustness. In this work, we develop a new understanding towards Fast Adversarial Training, by viewing random initialization as performing randomized smoothing for better optimization of the inner maximization problem. From this perspective, we show that the smoothing effect by random initialization is not sufficient under the adversarial perturbation constraint. A new initialization strategy, \\emph{backward smoothing}, is proposed to address this issue and significantly improves both stability and model robustness over single-step robust training methods. Experiments on multiple benchmarks demonstrate that our method achieves similar model robustness as the original TRADES method, while using much less training time (~3x improvement with the same training schedule). ", "one-sentence_summary": "We propose a new principle towards understanding Fast Adversarial Training, and a new initialization strategy that significantly improves both stability and model robustness over the single-step robust training methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|efficient_robust_training_via_backward_smoothing", "supplementary_material": "/attachment/f69958051f5c543363c667035bcf9f4de83ec9e2.zip", "pdf": "/pdf/ee969cde7ffd2ade652f069571e71705a0b27adb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aMIRgsXyhS", "_bibtex": "@misc{\nchen2021efficient,\ntitle={Efficient Robust Training via Backward Smoothing},\nauthor={Jinghui Chen and Yu Cheng and Zhe Gan and Quanquan Gu and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=49V11oUejQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "49V11oUejQ", "replyto": "49V11oUejQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2457/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538095953, "tmdate": 1606915784180, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2457/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2457/-/Official_Review"}}}, {"id": "a1BjJizLPG", "original": null, "number": 21, "cdate": 1606295304039, "ddate": null, "tcdate": 1606295304039, "tmdate": 1606296790307, "tddate": null, "forum": "49V11oUejQ", "replyto": "JsV0Z4TTbHb", "invitation": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment", "content": {"title": "Thank you for your reply", "comment": "1. \u201cin my understanding, a starting point with smoother loss gives a more informative gradient (to maximize the loss in Eq. 4.5) and then a single step is sufficient. Is this the meaning of such experiment for the authors?\u201d\n- Yes, you are right about our new experiment. \n\n2. \u201cStill, I can't relate this to the principle behind randomized smoothing...can the authors comment on this?\u201d\n- Randomized smoothing is originally defined in the form of expectation over random perturbation. In practice, in order to approximate the exception, one can use sample average over multiple points. Nevertheless, in practice, even using a single random point to approximate the expectation often suffices to implement randomized smoothing. This is analogous to using stochastic gradient descent to solve a stochastic optimization problem defined in the form of expectation over random functions. As we all know, stochastic gradient descent with mini-batch size equal to 1 often works well enough to solve the stochastic optimization problem. The only caveat is that when the minibatch size is small, there could be chances that the stochastic gradient direction is very different from the true gradient direction. But in expectation (or with high probability), the stochastic gradient descent still converges. Actually, Figure 3 exactly demonstrates a similar phenomenon, that is, the loss for random perturbed points is in general smoother than the original points, while oscillations do happen sometimes due to the use of a single random point. We hope this clears your concern.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2457/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Robust Training via Backward Smoothing", "authorids": ["~Jinghui_Chen1", "~Yu_Cheng1", "~Zhe_Gan1", "~Quanquan_Gu1", "~Jingjing_Liu2"], "authors": ["Jinghui Chen", "Yu Cheng", "Zhe Gan", "Quanquan Gu", "Jingjing Liu"], "keywords": ["Efficient Robust Training", "Backward Smoothing", "Robustness"], "abstract": "Adversarial training is so far the most effective strategy in defending against adversarial examples. However, it suffers from high computational cost due to the iterative adversarial attacks in each training step. Recent studies show that it is possible to achieve Fast Adversarial Training by performing a single-step attack with random initialization. Yet, it remains a mystery why random initialization helps. Besides, such an approach still lags behind state-of-the-art adversarial training algorithms on both stability and model robustness. In this work, we develop a new understanding towards Fast Adversarial Training, by viewing random initialization as performing randomized smoothing for better optimization of the inner maximization problem. From this perspective, we show that the smoothing effect by random initialization is not sufficient under the adversarial perturbation constraint. A new initialization strategy, \\emph{backward smoothing}, is proposed to address this issue and significantly improves both stability and model robustness over single-step robust training methods. Experiments on multiple benchmarks demonstrate that our method achieves similar model robustness as the original TRADES method, while using much less training time (~3x improvement with the same training schedule). ", "one-sentence_summary": "We propose a new principle towards understanding Fast Adversarial Training, and a new initialization strategy that significantly improves both stability and model robustness over the single-step robust training methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|efficient_robust_training_via_backward_smoothing", "supplementary_material": "/attachment/f69958051f5c543363c667035bcf9f4de83ec9e2.zip", "pdf": "/pdf/ee969cde7ffd2ade652f069571e71705a0b27adb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aMIRgsXyhS", "_bibtex": "@misc{\nchen2021efficient,\ntitle={Efficient Robust Training via Backward Smoothing},\nauthor={Jinghui Chen and Yu Cheng and Zhe Gan and Quanquan Gu and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=49V11oUejQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "49V11oUejQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2457/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2457/Authors|ICLR.cc/2021/Conference/Paper2457/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848168, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment"}}}, {"id": "ZkLuvM9cI2r", "original": null, "number": 20, "cdate": 1606295245962, "ddate": null, "tcdate": 1606295245962, "tmdate": 1606295245962, "tddate": null, "forum": "49V11oUejQ", "replyto": "pt_ROtHJuW", "invitation": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment", "content": {"title": "Thank you for your reply", "comment": "1. \u201cThere exist single-step methods that work with step size 2 eps... that work fine with a full step size of 2eps\u2026 \u201d\n\n- This is a huge misunderstanding. We have carefully checked the hyperparameter configuration of GradAlign in (Andriushchenko & Flammarion, 2020), the authors only claimed that GradAlign works with *perturbation constraint* epsilon=8/255 as well as 16/255, rather than using a *step size* equal to 2eps. Their public codebase also suggests that they use a step size of 1.25eps, which is the same as Fast AT.  \n\n2. \u201crandomness is still not relevant to the success of fast adversarial training. \u201d\n- We respectfully disagree. The effect of randomness in Fast Adversarial Training is evident since without randomness, FGSM training with a step size of eps will fail as demonstrated in the Fast AT paper.  \n\n3. \u201cOne of my major concerns is that this approach may be just on par with 2-step PGD adversarial training when evaluated carefully and thus providing no extra benefits\u201d\n- The ~28% robust accuracy reported in (Andriushchenko & Flammarion, 2020) is based on PreActResnet-18, while our additional experiments are based on the Resnet-18. Therefore, our results and theirs are not directly comparable.  We will carry out an additional experiment of your suggested baselines for eps=16/255 setting on the Resnet-18, and add it to the final version. Due to the short notice, we are afraid that we are not able to finish these experiments by the author response deadline.\n\n\n4. \u201cI'm not convinced about this claim.\u201d\n- It is actually *not* our claim but Fast AT paper\u2019s claim (section 5.5 point 2 said, \u201cDefenders don\u2019t need strong adversaries during training\u201d). Also we did not claim that our method could achieve the same robustness as multi-step robust training, we are claiming that it could achieve similar performances, at least for our proposed method.\n\n5. \u201cfrom the Hessian point of view, there is no significant difference on whether to train on original or randomly perturbed points\u201d\n- As can be seen from Figure 3, the maximum Hessian eigenvalue of random perturbed points (orange curve) is significantly lower than that of the original points (blue curve). This suggests that randomized smoothing indeed makes the loss function smoother than the original loss. In addition, the maximum Hessian eigenvalue of perturbed points by Backward smoothing (green curve) is drastically lower than that of random perturbed points (orange curve), which strongly supports the improvement of Backward Smoothing over randomized smoothing.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2457/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Robust Training via Backward Smoothing", "authorids": ["~Jinghui_Chen1", "~Yu_Cheng1", "~Zhe_Gan1", "~Quanquan_Gu1", "~Jingjing_Liu2"], "authors": ["Jinghui Chen", "Yu Cheng", "Zhe Gan", "Quanquan Gu", "Jingjing Liu"], "keywords": ["Efficient Robust Training", "Backward Smoothing", "Robustness"], "abstract": "Adversarial training is so far the most effective strategy in defending against adversarial examples. However, it suffers from high computational cost due to the iterative adversarial attacks in each training step. Recent studies show that it is possible to achieve Fast Adversarial Training by performing a single-step attack with random initialization. Yet, it remains a mystery why random initialization helps. Besides, such an approach still lags behind state-of-the-art adversarial training algorithms on both stability and model robustness. In this work, we develop a new understanding towards Fast Adversarial Training, by viewing random initialization as performing randomized smoothing for better optimization of the inner maximization problem. From this perspective, we show that the smoothing effect by random initialization is not sufficient under the adversarial perturbation constraint. A new initialization strategy, \\emph{backward smoothing}, is proposed to address this issue and significantly improves both stability and model robustness over single-step robust training methods. Experiments on multiple benchmarks demonstrate that our method achieves similar model robustness as the original TRADES method, while using much less training time (~3x improvement with the same training schedule). ", "one-sentence_summary": "We propose a new principle towards understanding Fast Adversarial Training, and a new initialization strategy that significantly improves both stability and model robustness over the single-step robust training methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|efficient_robust_training_via_backward_smoothing", "supplementary_material": "/attachment/f69958051f5c543363c667035bcf9f4de83ec9e2.zip", "pdf": "/pdf/ee969cde7ffd2ade652f069571e71705a0b27adb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aMIRgsXyhS", "_bibtex": "@misc{\nchen2021efficient,\ntitle={Efficient Robust Training via Backward Smoothing},\nauthor={Jinghui Chen and Yu Cheng and Zhe Gan and Quanquan Gu and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=49V11oUejQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "49V11oUejQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2457/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2457/Authors|ICLR.cc/2021/Conference/Paper2457/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848168, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment"}}}, {"id": "PDc8KI6vEGp", "original": null, "number": 19, "cdate": 1606290759566, "ddate": null, "tcdate": 1606290759566, "tmdate": 1606290759566, "tddate": null, "forum": "49V11oUejQ", "replyto": "SGdR2o5JCLg", "invitation": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment", "content": {"title": "Thank you for your reply", "comment": "1. \u201cFirst, I still think Andriushchenko & Flammarion, 2020 gives a better and more fundamental explanation\u2026\u201d\n\n- We respectfully disagree with you. As we explained before, our explanation based on smoothness is more general and can cover theirs. Can you explain why theirs is more fundamental? \n\n2. \u201cit only works for KL divergence loss rather than cross-entropy loss\u2026 it seems that backward smoothing does not address the fundamental problem of fast adversarial training\u201d\n\n- Our method works for both, and it works better for KL divergence than for cross-entropy loss. Also, when it is applied to KL divergence, it gives the best-known performance and beats all previous methods. Could you clarify what is the \u201cfundamental problem of fast adversarial training\u201d you are referring to?\n\n3. \u201cI do not think the authors well address my concern about the trade-off problem\u201d\n\n- Both Fast TRADES and Backward Smoothing are proposed in this paper. While Fast TRADES achieves higher natural accuracy, Backward Smoothing achieves higher robust accuracy. The tradeoff you are concerned with is between two of our proposed methods. But given that one of these two methods, Backward Smoothing, achieves comparable performance as TRADES in both natural accuracy and robust accuracy, while being much faster than TRADES, we don\u2019t think the tradeoff you\u2019re talking about will downgrade our contribution. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2457/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Robust Training via Backward Smoothing", "authorids": ["~Jinghui_Chen1", "~Yu_Cheng1", "~Zhe_Gan1", "~Quanquan_Gu1", "~Jingjing_Liu2"], "authors": ["Jinghui Chen", "Yu Cheng", "Zhe Gan", "Quanquan Gu", "Jingjing Liu"], "keywords": ["Efficient Robust Training", "Backward Smoothing", "Robustness"], "abstract": "Adversarial training is so far the most effective strategy in defending against adversarial examples. However, it suffers from high computational cost due to the iterative adversarial attacks in each training step. Recent studies show that it is possible to achieve Fast Adversarial Training by performing a single-step attack with random initialization. Yet, it remains a mystery why random initialization helps. Besides, such an approach still lags behind state-of-the-art adversarial training algorithms on both stability and model robustness. In this work, we develop a new understanding towards Fast Adversarial Training, by viewing random initialization as performing randomized smoothing for better optimization of the inner maximization problem. From this perspective, we show that the smoothing effect by random initialization is not sufficient under the adversarial perturbation constraint. A new initialization strategy, \\emph{backward smoothing}, is proposed to address this issue and significantly improves both stability and model robustness over single-step robust training methods. Experiments on multiple benchmarks demonstrate that our method achieves similar model robustness as the original TRADES method, while using much less training time (~3x improvement with the same training schedule). ", "one-sentence_summary": "We propose a new principle towards understanding Fast Adversarial Training, and a new initialization strategy that significantly improves both stability and model robustness over the single-step robust training methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|efficient_robust_training_via_backward_smoothing", "supplementary_material": "/attachment/f69958051f5c543363c667035bcf9f4de83ec9e2.zip", "pdf": "/pdf/ee969cde7ffd2ade652f069571e71705a0b27adb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aMIRgsXyhS", "_bibtex": "@misc{\nchen2021efficient,\ntitle={Efficient Robust Training via Backward Smoothing},\nauthor={Jinghui Chen and Yu Cheng and Zhe Gan and Quanquan Gu and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=49V11oUejQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "49V11oUejQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2457/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2457/Authors|ICLR.cc/2021/Conference/Paper2457/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848168, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment"}}}, {"id": "SGdR2o5JCLg", "original": null, "number": 18, "cdate": 1606238421123, "ddate": null, "tcdate": 1606238421123, "tmdate": 1606248779376, "tddate": null, "forum": "49V11oUejQ", "replyto": "mHX3GZSjA8W", "invitation": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment", "content": {"title": "Still not convinced by the motivation and trade-off performance", "comment": "Thanks very much for the additional experiments and explanations.\n\nFirst, I still think Andriushchenko & Flammarion, 2020 gives a better and more fundamental explanation on how to address fast adversarial training after I looked through the authors' response.\n\nSecond, backward smoothing does not generalize to standard adversarial training. In other words, it only works for KL divergence loss rather than cross-entropy loss, this is still a big concern to me and it seems that backward smoothing does not address the fundamental problem of fast adversarial training.\n\nThird, I do not think the authors well address my concern about the trade-off problem. If we simply compare the performance of Fast TRADES and Backward Smoothing since Backward Smoothing intends to improve Fast TRADES, there is always a tradeoff between clean accuracy and adversarial robustness, e.g., Table 4 and Table 8. \n\nIn all, I would keep my rating unchanged."}, "signatures": ["ICLR.cc/2021/Conference/Paper2457/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Robust Training via Backward Smoothing", "authorids": ["~Jinghui_Chen1", "~Yu_Cheng1", "~Zhe_Gan1", "~Quanquan_Gu1", "~Jingjing_Liu2"], "authors": ["Jinghui Chen", "Yu Cheng", "Zhe Gan", "Quanquan Gu", "Jingjing Liu"], "keywords": ["Efficient Robust Training", "Backward Smoothing", "Robustness"], "abstract": "Adversarial training is so far the most effective strategy in defending against adversarial examples. However, it suffers from high computational cost due to the iterative adversarial attacks in each training step. Recent studies show that it is possible to achieve Fast Adversarial Training by performing a single-step attack with random initialization. Yet, it remains a mystery why random initialization helps. Besides, such an approach still lags behind state-of-the-art adversarial training algorithms on both stability and model robustness. In this work, we develop a new understanding towards Fast Adversarial Training, by viewing random initialization as performing randomized smoothing for better optimization of the inner maximization problem. From this perspective, we show that the smoothing effect by random initialization is not sufficient under the adversarial perturbation constraint. A new initialization strategy, \\emph{backward smoothing}, is proposed to address this issue and significantly improves both stability and model robustness over single-step robust training methods. Experiments on multiple benchmarks demonstrate that our method achieves similar model robustness as the original TRADES method, while using much less training time (~3x improvement with the same training schedule). ", "one-sentence_summary": "We propose a new principle towards understanding Fast Adversarial Training, and a new initialization strategy that significantly improves both stability and model robustness over the single-step robust training methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|efficient_robust_training_via_backward_smoothing", "supplementary_material": "/attachment/f69958051f5c543363c667035bcf9f4de83ec9e2.zip", "pdf": "/pdf/ee969cde7ffd2ade652f069571e71705a0b27adb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aMIRgsXyhS", "_bibtex": "@misc{\nchen2021efficient,\ntitle={Efficient Robust Training via Backward Smoothing},\nauthor={Jinghui Chen and Yu Cheng and Zhe Gan and Quanquan Gu and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=49V11oUejQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "49V11oUejQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2457/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2457/Authors|ICLR.cc/2021/Conference/Paper2457/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848168, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment"}}}, {"id": "pt_ROtHJuW", "original": null, "number": 17, "cdate": 1606231007949, "ddate": null, "tcdate": 1606231007949, "tmdate": 1606231058080, "tddate": null, "forum": "49V11oUejQ", "replyto": "MZYEo_4xLXH", "invitation": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment", "content": {"title": "Still not convinced about the role of randomness and the overall motivation of the method", "comment": "Thanks a lot for the additional experiments.\n\n*\"Second, using a step size of 2eps is simply too large. To the best of our knowledge, there does not exist a single-step adversarial training algorithm that works with step size 2eps.\"*\n\nI'm not sure what *\"simply too large\"* means. There exist single-step methods that work with step size 2 eps: e.g. see GradAlign+FGSM or CURE+FGSM reported in Andriushchenko & Flammarion (2020) that work fine with a full step size of 2eps (i.e. when the adversarial perturbations are in $\\\\{-\\varepsilon, \\varepsilon\\\\}^d$ if we ignore the box constraints of images $\\\\{0, 1\\\\}^d$) even for eps=16/255. \n\nSo my conclusion is that randomness is still not relevant to the success of fast adversarial training (or, at least, there is no evidence to claim that it is).\n\n-----\n\n*\"we have added experiments with eps=16/255 for CIFAR-10 dataset on the ResNet18 model.\"* and *\"we did not claim that backward smoothing constantly beats the state-of-the-art methods, we only achieve slightly better results for one or two settings\"*\n\nAndriushchenko & Flammarion (2020) report ~28% adversarial accuracy for adversarial training with PGD-2 on ResNet-18 for eps=16/255 (when early stopping is used, see Fig. 14 therein). One of my major concerns is that this approach may be just on par with 2-step PGD adversarial training when evaluated carefully and thus providing no extra benefits. I would be quite hesitant to recommend acceptance of a considerably more involved procedure if there exists a simpler and more established alternative of simple adversarial training with fewer steps.\n\n-----\n\n*\"Second, the success of Fast Adversarial Training has already suggested that for adversarial training, **stronger attacks for the inner problem are not required** as mentioned in their original paper.\"*\n\nI'm not convinced about this claim. The gap between the robustness of multi-step PGD training and one- / two-step PGD training is growing over the size of the $\\ell_\\infty$ epsilon. In fact, the difference is noticeable even for $\\varepsilon=8/255$ on CIFAR-10 as Wong et al. (2020) report, although they comment on this differently.\n\n-----\n\nRegarding the new experiment with the maximal eigenvalue of the Hessian: I think it's indeed an interesting experiment that shows that the curvature at a backward smoothed point is somehow smaller compared to the curvature at a random point or at the original input. At the same time, this experiment seems to invalidate the connection between the proposed backward smoothing and randomized smoothing as according to the new Figure 3, from the Hessian point of view, there is no significant difference on whether to train on original or randomly perturbed points.\n\n-----\n\nTo conclude, I would stay with my original opinion. I am still (1) not convinced that randomness has any important role and (2) the connection between randomized smoothing and backward smoothing is questionable. There is some empirical success of the proposed scheme, but in my opinion it's not sufficient. So I think the paper requires a revision to better motivate the method (particularly, the connection to randomized smoothing or the new perspective on the curvature) and more clearly show that it outperforms 2-step PGD adversarial training (not just 2-step TRADES) over multiple Linf epsilons."}, "signatures": ["ICLR.cc/2021/Conference/Paper2457/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Robust Training via Backward Smoothing", "authorids": ["~Jinghui_Chen1", "~Yu_Cheng1", "~Zhe_Gan1", "~Quanquan_Gu1", "~Jingjing_Liu2"], "authors": ["Jinghui Chen", "Yu Cheng", "Zhe Gan", "Quanquan Gu", "Jingjing Liu"], "keywords": ["Efficient Robust Training", "Backward Smoothing", "Robustness"], "abstract": "Adversarial training is so far the most effective strategy in defending against adversarial examples. However, it suffers from high computational cost due to the iterative adversarial attacks in each training step. Recent studies show that it is possible to achieve Fast Adversarial Training by performing a single-step attack with random initialization. Yet, it remains a mystery why random initialization helps. Besides, such an approach still lags behind state-of-the-art adversarial training algorithms on both stability and model robustness. In this work, we develop a new understanding towards Fast Adversarial Training, by viewing random initialization as performing randomized smoothing for better optimization of the inner maximization problem. From this perspective, we show that the smoothing effect by random initialization is not sufficient under the adversarial perturbation constraint. A new initialization strategy, \\emph{backward smoothing}, is proposed to address this issue and significantly improves both stability and model robustness over single-step robust training methods. Experiments on multiple benchmarks demonstrate that our method achieves similar model robustness as the original TRADES method, while using much less training time (~3x improvement with the same training schedule). ", "one-sentence_summary": "We propose a new principle towards understanding Fast Adversarial Training, and a new initialization strategy that significantly improves both stability and model robustness over the single-step robust training methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|efficient_robust_training_via_backward_smoothing", "supplementary_material": "/attachment/f69958051f5c543363c667035bcf9f4de83ec9e2.zip", "pdf": "/pdf/ee969cde7ffd2ade652f069571e71705a0b27adb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aMIRgsXyhS", "_bibtex": "@misc{\nchen2021efficient,\ntitle={Efficient Robust Training via Backward Smoothing},\nauthor={Jinghui Chen and Yu Cheng and Zhe Gan and Quanquan Gu and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=49V11oUejQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "49V11oUejQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2457/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2457/Authors|ICLR.cc/2021/Conference/Paper2457/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848168, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment"}}}, {"id": "JsV0Z4TTbHb", "original": null, "number": 16, "cdate": 1606218460869, "ddate": null, "tcdate": 1606218460869, "tmdate": 1606218460869, "tddate": null, "forum": "49V11oUejQ", "replyto": "UncqJRL-20-", "invitation": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment", "content": {"title": "Response to Authors", "comment": "Thanks for the response (and sorry for the late reply).\n\nAs mentioned in the initial review, I think the results and the evaluation are solid and further strengthened by the additional experiments in the revision.\n\nAlso the method seems reasonable to me, and the new analysis about the magnitude of the eigenvalues of the Hessian is actually quite insightful to me and a more convincing justification of the scheme than what argued initially and in the first part of the paper (Section 3). In fact, in my understanding, a starting point with smoother loss gives a more informative gradient (to maximize the loss in Eq. 4.5) and then a single step is sufficient. Is this the meaning of such experiment for the authors?\n\nStill, I can't relate this to the principle behind randomized smoothing, which is averaging over different samples. In A.1 the authors use multiple points but it seems that all are chosen with backward smoothing (right?). In Section 3 the claim seems to be that to better solving the problem in Eq. 3.1 a better estimation of the gradient is needed, which can be achieved via averaging the gradients at different random points, but in practice even a single random initial point already suffices. This can be summarized as the gradient at a random point in the Lp-ball is more informative than that at the original point.\nHowever, this is not validate and Fig. 3 in my opinion suggests that the difference (in smoothness) of the loss at the original and random point is not clear (especially given the large variance across epochs). Could the authors comment on this?\n\nOverall, I think the proposed methods is effective and better supported with the additional analysis in the revision."}, "signatures": ["ICLR.cc/2021/Conference/Paper2457/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Robust Training via Backward Smoothing", "authorids": ["~Jinghui_Chen1", "~Yu_Cheng1", "~Zhe_Gan1", "~Quanquan_Gu1", "~Jingjing_Liu2"], "authors": ["Jinghui Chen", "Yu Cheng", "Zhe Gan", "Quanquan Gu", "Jingjing Liu"], "keywords": ["Efficient Robust Training", "Backward Smoothing", "Robustness"], "abstract": "Adversarial training is so far the most effective strategy in defending against adversarial examples. However, it suffers from high computational cost due to the iterative adversarial attacks in each training step. Recent studies show that it is possible to achieve Fast Adversarial Training by performing a single-step attack with random initialization. Yet, it remains a mystery why random initialization helps. Besides, such an approach still lags behind state-of-the-art adversarial training algorithms on both stability and model robustness. In this work, we develop a new understanding towards Fast Adversarial Training, by viewing random initialization as performing randomized smoothing for better optimization of the inner maximization problem. From this perspective, we show that the smoothing effect by random initialization is not sufficient under the adversarial perturbation constraint. A new initialization strategy, \\emph{backward smoothing}, is proposed to address this issue and significantly improves both stability and model robustness over single-step robust training methods. Experiments on multiple benchmarks demonstrate that our method achieves similar model robustness as the original TRADES method, while using much less training time (~3x improvement with the same training schedule). ", "one-sentence_summary": "We propose a new principle towards understanding Fast Adversarial Training, and a new initialization strategy that significantly improves both stability and model robustness over the single-step robust training methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|efficient_robust_training_via_backward_smoothing", "supplementary_material": "/attachment/f69958051f5c543363c667035bcf9f4de83ec9e2.zip", "pdf": "/pdf/ee969cde7ffd2ade652f069571e71705a0b27adb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aMIRgsXyhS", "_bibtex": "@misc{\nchen2021efficient,\ntitle={Efficient Robust Training via Backward Smoothing},\nauthor={Jinghui Chen and Yu Cheng and Zhe Gan and Quanquan Gu and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=49V11oUejQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "49V11oUejQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2457/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2457/Authors|ICLR.cc/2021/Conference/Paper2457/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848168, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment"}}}, {"id": "N_bDgBk45CY", "original": null, "number": 14, "cdate": 1606123826374, "ddate": null, "tcdate": 1606123826374, "tmdate": 1606123939388, "tddate": null, "forum": "49V11oUejQ", "replyto": "_TtiPQH91TI", "invitation": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment", "content": {"title": "Thank you for your response", "comment": "We are glad to hear that most of your concerns have been addressed. We still want to argue that the motivation of our method is actually quite clear. We provided a new understanding of Fast Adversarial Training through the lens of randomized smoothing (a technique widely used for optimizing non-smooth functions in the optimization community). Indeed, Reviewers 1,2,4 mentioned an alternative explanation in (Andriushchenko & Flammarion, 2020) that reducing the magnitude of the perturbation helps with Fast Adversarial Training by making the loss more linear. Nevertheless, we would like to point out that our smoothing perspective is more essential and general than the linear-loss based interpretation in  (Andriushchenko & Flammarion, 2020), since linear loss is perhaps the simplest special case of smooth loss functions. In terms of the ablations for understanding smoothing\u2019s role, we have just added a new experiment replacing old Figure 3 in the revision. Specifically, we show the maximum eigenvalue of Hessian of the loss function at the original examples, randomly perturbed examples, and backward smoothed examples along the training trajectory. The result confirms that randomized smoothing indeed helps make the loss function smoother yet our proposed backward smoothing technique makes the loss much smoother compared to randomized smoothing. This confirms the motivation of our proposed method. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2457/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Robust Training via Backward Smoothing", "authorids": ["~Jinghui_Chen1", "~Yu_Cheng1", "~Zhe_Gan1", "~Quanquan_Gu1", "~Jingjing_Liu2"], "authors": ["Jinghui Chen", "Yu Cheng", "Zhe Gan", "Quanquan Gu", "Jingjing Liu"], "keywords": ["Efficient Robust Training", "Backward Smoothing", "Robustness"], "abstract": "Adversarial training is so far the most effective strategy in defending against adversarial examples. However, it suffers from high computational cost due to the iterative adversarial attacks in each training step. Recent studies show that it is possible to achieve Fast Adversarial Training by performing a single-step attack with random initialization. Yet, it remains a mystery why random initialization helps. Besides, such an approach still lags behind state-of-the-art adversarial training algorithms on both stability and model robustness. In this work, we develop a new understanding towards Fast Adversarial Training, by viewing random initialization as performing randomized smoothing for better optimization of the inner maximization problem. From this perspective, we show that the smoothing effect by random initialization is not sufficient under the adversarial perturbation constraint. A new initialization strategy, \\emph{backward smoothing}, is proposed to address this issue and significantly improves both stability and model robustness over single-step robust training methods. Experiments on multiple benchmarks demonstrate that our method achieves similar model robustness as the original TRADES method, while using much less training time (~3x improvement with the same training schedule). ", "one-sentence_summary": "We propose a new principle towards understanding Fast Adversarial Training, and a new initialization strategy that significantly improves both stability and model robustness over the single-step robust training methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|efficient_robust_training_via_backward_smoothing", "supplementary_material": "/attachment/f69958051f5c543363c667035bcf9f4de83ec9e2.zip", "pdf": "/pdf/ee969cde7ffd2ade652f069571e71705a0b27adb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aMIRgsXyhS", "_bibtex": "@misc{\nchen2021efficient,\ntitle={Efficient Robust Training via Backward Smoothing},\nauthor={Jinghui Chen and Yu Cheng and Zhe Gan and Quanquan Gu and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=49V11oUejQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "49V11oUejQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2457/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2457/Authors|ICLR.cc/2021/Conference/Paper2457/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848168, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment"}}}, {"id": "oq583wkn5S3", "original": null, "number": 13, "cdate": 1606123245067, "ddate": null, "tcdate": 1606123245067, "tmdate": 1606123245067, "tddate": null, "forum": "49V11oUejQ", "replyto": "49V11oUejQ", "invitation": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment", "content": {"title": "New Experiments (Figure 3) Added", "comment": "Reviewers 3 and 4 raised concerns about the role of smoothing in robust training, e.g., Figure 3 in the original submission is not informative. We have now added a new experiment replacing old Figure 3 in the revision to better show that smoothing indeed plays an important role in training robust models and the current smoothing effect is not sufficient. Specifically, we show the maximum eigenvalue of Hessian of the loss function at the original examples, randomly perturbed examples, and backward smoothed examples along the training trajectory in new Figure 3. The result suggests that randomized smoothing indeed helps make the loss function smoother yet it is not as effective as our proposed backward smoothing technique. Please refer to the updated pdf file for more details."}, "signatures": ["ICLR.cc/2021/Conference/Paper2457/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Robust Training via Backward Smoothing", "authorids": ["~Jinghui_Chen1", "~Yu_Cheng1", "~Zhe_Gan1", "~Quanquan_Gu1", "~Jingjing_Liu2"], "authors": ["Jinghui Chen", "Yu Cheng", "Zhe Gan", "Quanquan Gu", "Jingjing Liu"], "keywords": ["Efficient Robust Training", "Backward Smoothing", "Robustness"], "abstract": "Adversarial training is so far the most effective strategy in defending against adversarial examples. However, it suffers from high computational cost due to the iterative adversarial attacks in each training step. Recent studies show that it is possible to achieve Fast Adversarial Training by performing a single-step attack with random initialization. Yet, it remains a mystery why random initialization helps. Besides, such an approach still lags behind state-of-the-art adversarial training algorithms on both stability and model robustness. In this work, we develop a new understanding towards Fast Adversarial Training, by viewing random initialization as performing randomized smoothing for better optimization of the inner maximization problem. From this perspective, we show that the smoothing effect by random initialization is not sufficient under the adversarial perturbation constraint. A new initialization strategy, \\emph{backward smoothing}, is proposed to address this issue and significantly improves both stability and model robustness over single-step robust training methods. Experiments on multiple benchmarks demonstrate that our method achieves similar model robustness as the original TRADES method, while using much less training time (~3x improvement with the same training schedule). ", "one-sentence_summary": "We propose a new principle towards understanding Fast Adversarial Training, and a new initialization strategy that significantly improves both stability and model robustness over the single-step robust training methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|efficient_robust_training_via_backward_smoothing", "supplementary_material": "/attachment/f69958051f5c543363c667035bcf9f4de83ec9e2.zip", "pdf": "/pdf/ee969cde7ffd2ade652f069571e71705a0b27adb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aMIRgsXyhS", "_bibtex": "@misc{\nchen2021efficient,\ntitle={Efficient Robust Training via Backward Smoothing},\nauthor={Jinghui Chen and Yu Cheng and Zhe Gan and Quanquan Gu and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=49V11oUejQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "49V11oUejQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2457/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2457/Authors|ICLR.cc/2021/Conference/Paper2457/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848168, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment"}}}, {"id": "mHX3GZSjA8W", "original": null, "number": 3, "cdate": 1605597622621, "ddate": null, "tcdate": 1605597622621, "tmdate": 1606113641165, "tddate": null, "forum": "49V11oUejQ", "replyto": "4RM87Worw_B", "invitation": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment", "content": {"title": "Authors\u2019 Response to Reviewer 4", "comment": "Thank you for your insightful comments. Below, we provide detailed responses to your questions. \n\nQ1: (Andriushchenko & Flammarion, 2020) has a nice explanation and solution, which is more convincing to me.\n\nA1: See our comments to all reviewers.\n\nQ2: Even if I accept the condition..., the motivation showing in Figure 3 ... is not sufficient and can not convince me... current random initialization.\n\nA2: We have also added a new experiment in Figure 3 in the revision to better show that smoothing indeed plays an important role in training robust models and the current smoothing effect is not enough. Specifically, we show the maximum eigenvalue of Hessian of the loss function at the original examples, randomly perturbed examples, and backward smoothed examples along the training trajectory in Figure 3. The result suggests that randomized smoothing indeed helps make the loss function smoother yet it is not as effective as our proposed backward smoothing technique. We hope this new experiment can address your concern.\n\nQ3: At the end of section 4, the authors mention \u2026but did not find these results\u2026 why a better random initialization can not generalize to standard adversarial training?\n\nA3: In Section 5.4, we analyzed the compatibility of our algorithm with Adversarial Training. And as we explained in Section 5.4 (starting from line 6), we think the main reason for the deteriorated performance is the different choices of inner maximization loss for Adversarial Training and TRADES. Considering the random perturbation generated on the output space, the Cross-Entropy loss mainly focuses on the $y$-logit (limited smoothing effect) while KL divergence is closely related to all logits.  \n\nQ4: For Table 5, it is necessary to include FAST TRADES as backward smoothing is mainly applied to TRADES.\n\nA4: Thank you for your suggestion. We have added the FAST TRADES result in Table 5.\n\n| Method             | AutoAttack |\n|--------------------|------------|\n| AT                 | 44.04      |\n| Fast AT            | 43.21      |\n| TRADES             | 53.08      |\n| Fast TRADES        | 43.85      |\n| Backward Smoothing | 51.13      |\n\nQ5: Does backward smoothing really help adversarial robustness or it just plays the tradeoff game ...but sacrifices a big clean accuracy.\n\nA5: First, in all experiments and for all methods, we have tuned the models for their best robustness performances. Second, notice that our method is based on TRADES, and compared with TRADES, backward smoothing actually achieves very similar clean accuracy as well as robust accuracy, suggesting that we are not playing the tradeoff game. On the other hand, baselines such as Fast TRADES and Fast TRADES (2-step) are actually sacrificing robustness for better natural accuracy (compared to TRADES). This is why we need backward smoothing to fix this. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2457/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Robust Training via Backward Smoothing", "authorids": ["~Jinghui_Chen1", "~Yu_Cheng1", "~Zhe_Gan1", "~Quanquan_Gu1", "~Jingjing_Liu2"], "authors": ["Jinghui Chen", "Yu Cheng", "Zhe Gan", "Quanquan Gu", "Jingjing Liu"], "keywords": ["Efficient Robust Training", "Backward Smoothing", "Robustness"], "abstract": "Adversarial training is so far the most effective strategy in defending against adversarial examples. However, it suffers from high computational cost due to the iterative adversarial attacks in each training step. Recent studies show that it is possible to achieve Fast Adversarial Training by performing a single-step attack with random initialization. Yet, it remains a mystery why random initialization helps. Besides, such an approach still lags behind state-of-the-art adversarial training algorithms on both stability and model robustness. In this work, we develop a new understanding towards Fast Adversarial Training, by viewing random initialization as performing randomized smoothing for better optimization of the inner maximization problem. From this perspective, we show that the smoothing effect by random initialization is not sufficient under the adversarial perturbation constraint. A new initialization strategy, \\emph{backward smoothing}, is proposed to address this issue and significantly improves both stability and model robustness over single-step robust training methods. Experiments on multiple benchmarks demonstrate that our method achieves similar model robustness as the original TRADES method, while using much less training time (~3x improvement with the same training schedule). ", "one-sentence_summary": "We propose a new principle towards understanding Fast Adversarial Training, and a new initialization strategy that significantly improves both stability and model robustness over the single-step robust training methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|efficient_robust_training_via_backward_smoothing", "supplementary_material": "/attachment/f69958051f5c543363c667035bcf9f4de83ec9e2.zip", "pdf": "/pdf/ee969cde7ffd2ade652f069571e71705a0b27adb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aMIRgsXyhS", "_bibtex": "@misc{\nchen2021efficient,\ntitle={Efficient Robust Training via Backward Smoothing},\nauthor={Jinghui Chen and Yu Cheng and Zhe Gan and Quanquan Gu and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=49V11oUejQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "49V11oUejQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2457/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2457/Authors|ICLR.cc/2021/Conference/Paper2457/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848168, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment"}}}, {"id": "_TtiPQH91TI", "original": null, "number": 12, "cdate": 1606069896351, "ddate": null, "tcdate": 1606069896351, "tmdate": 1606069896351, "tddate": null, "forum": "49V11oUejQ", "replyto": "hTKlEfQzRfX", "invitation": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment", "content": {"title": "Response to authors", "comment": "Thanks to the authors for running the additional experiments. Many of my minor concerns are alleviated, and I believe there is strong empirical evidence for this procedure to allow for training TRADES quickly (replace Fast TRADES).\n\nHowever the overall picture remains that the procedure as given is not necessarily something very general (that works across many robust training methods) that you'd expect from a general framework such as randomized smoothing. As other reviewers have mentioned, the motivation in terms of randomized smoothing is a bit weak in its connection to the actual method and the experiments do not clearly show that the given explanations about smoothing are indeed the main contributor to the improvement.  Further investigation into this aspect would make elucidate this aspect. I will keep my score as I think the empirical evidence is pretty strong for faster TRADES, but I think the motivations for the method and ablations for understanding should be presented and conducted more precisely than its current form."}, "signatures": ["ICLR.cc/2021/Conference/Paper2457/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Robust Training via Backward Smoothing", "authorids": ["~Jinghui_Chen1", "~Yu_Cheng1", "~Zhe_Gan1", "~Quanquan_Gu1", "~Jingjing_Liu2"], "authors": ["Jinghui Chen", "Yu Cheng", "Zhe Gan", "Quanquan Gu", "Jingjing Liu"], "keywords": ["Efficient Robust Training", "Backward Smoothing", "Robustness"], "abstract": "Adversarial training is so far the most effective strategy in defending against adversarial examples. However, it suffers from high computational cost due to the iterative adversarial attacks in each training step. Recent studies show that it is possible to achieve Fast Adversarial Training by performing a single-step attack with random initialization. Yet, it remains a mystery why random initialization helps. Besides, such an approach still lags behind state-of-the-art adversarial training algorithms on both stability and model robustness. In this work, we develop a new understanding towards Fast Adversarial Training, by viewing random initialization as performing randomized smoothing for better optimization of the inner maximization problem. From this perspective, we show that the smoothing effect by random initialization is not sufficient under the adversarial perturbation constraint. A new initialization strategy, \\emph{backward smoothing}, is proposed to address this issue and significantly improves both stability and model robustness over single-step robust training methods. Experiments on multiple benchmarks demonstrate that our method achieves similar model robustness as the original TRADES method, while using much less training time (~3x improvement with the same training schedule). ", "one-sentence_summary": "We propose a new principle towards understanding Fast Adversarial Training, and a new initialization strategy that significantly improves both stability and model robustness over the single-step robust training methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|efficient_robust_training_via_backward_smoothing", "supplementary_material": "/attachment/f69958051f5c543363c667035bcf9f4de83ec9e2.zip", "pdf": "/pdf/ee969cde7ffd2ade652f069571e71705a0b27adb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aMIRgsXyhS", "_bibtex": "@misc{\nchen2021efficient,\ntitle={Efficient Robust Training via Backward Smoothing},\nauthor={Jinghui Chen and Yu Cheng and Zhe Gan and Quanquan Gu and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=49V11oUejQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "49V11oUejQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2457/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2457/Authors|ICLR.cc/2021/Conference/Paper2457/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848168, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment"}}}, {"id": "kn5ykq17GkJ", "original": null, "number": 7, "cdate": 1605599758099, "ddate": null, "tcdate": 1605599758099, "tmdate": 1605599758099, "tddate": null, "forum": "49V11oUejQ", "replyto": "Q9w7fXicZ5C", "invitation": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment", "content": {"title": "We further compared with Fast-AT (2-step)", "comment": "Please see our response to Reviewer 1, the answer to the third question."}, "signatures": ["ICLR.cc/2021/Conference/Paper2457/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Robust Training via Backward Smoothing", "authorids": ["~Jinghui_Chen1", "~Yu_Cheng1", "~Zhe_Gan1", "~Quanquan_Gu1", "~Jingjing_Liu2"], "authors": ["Jinghui Chen", "Yu Cheng", "Zhe Gan", "Quanquan Gu", "Jingjing Liu"], "keywords": ["Efficient Robust Training", "Backward Smoothing", "Robustness"], "abstract": "Adversarial training is so far the most effective strategy in defending against adversarial examples. However, it suffers from high computational cost due to the iterative adversarial attacks in each training step. Recent studies show that it is possible to achieve Fast Adversarial Training by performing a single-step attack with random initialization. Yet, it remains a mystery why random initialization helps. Besides, such an approach still lags behind state-of-the-art adversarial training algorithms on both stability and model robustness. In this work, we develop a new understanding towards Fast Adversarial Training, by viewing random initialization as performing randomized smoothing for better optimization of the inner maximization problem. From this perspective, we show that the smoothing effect by random initialization is not sufficient under the adversarial perturbation constraint. A new initialization strategy, \\emph{backward smoothing}, is proposed to address this issue and significantly improves both stability and model robustness over single-step robust training methods. Experiments on multiple benchmarks demonstrate that our method achieves similar model robustness as the original TRADES method, while using much less training time (~3x improvement with the same training schedule). ", "one-sentence_summary": "We propose a new principle towards understanding Fast Adversarial Training, and a new initialization strategy that significantly improves both stability and model robustness over the single-step robust training methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|efficient_robust_training_via_backward_smoothing", "supplementary_material": "/attachment/f69958051f5c543363c667035bcf9f4de83ec9e2.zip", "pdf": "/pdf/ee969cde7ffd2ade652f069571e71705a0b27adb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aMIRgsXyhS", "_bibtex": "@misc{\nchen2021efficient,\ntitle={Efficient Robust Training via Backward Smoothing},\nauthor={Jinghui Chen and Yu Cheng and Zhe Gan and Quanquan Gu and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=49V11oUejQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "49V11oUejQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2457/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2457/Authors|ICLR.cc/2021/Conference/Paper2457/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848168, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment"}}}, {"id": "hTKlEfQzRfX", "original": null, "number": 6, "cdate": 1605597902799, "ddate": null, "tcdate": 1605597902799, "tmdate": 1605598138058, "tddate": null, "forum": "49V11oUejQ", "replyto": "Qan3eHWT17Y", "invitation": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment", "content": {"title": "Authors\u2019 Response to Reviewer 3", "comment": "Thank you for your insightful comments. Below, we provide detailed responses to your questions.\n\n\nQ1: My main concern is the backward smoothing method seems... not clear that we should necessarily prefer a method based on TRADES.\n\nA1: In Table 1, we have shown that even with early stopping, TRADES still outperforms PGD-AT under the same training conditions (though early stopping does make the difference much smaller). We also added PGD-AT with early-stopping in Table 5 and it also suggests similar results.  Moreover, in fact, both adversarial training and TRADES are not state-of-the-art robust training methods (see AutoAttack leaderboard), while the current best method is still based on TRADES. Our method can be easily implemented based on those new methods and achieve better robustness. Therefore, there is actually a strong reason to prefer to use TRADES as our basis. Also we want to clarify that our method does not only work for TRADES. It can be applied to PGD AT and achieve marginal improvements with an improved stability as discussed in Section 5.4.\n\nQ2: The authors mention the natural way to ...if there were some experiments just using larger perturbations for random initialization.\n\nA2: Thank you for your suggestion, but we may not be able to perform your described experiment. First, we have to use the same norm constraints, otherwise, the results are not directly comparable under different threat models. If so, using larger perturbation does not make it better as we need to project it back to satisfy the norm constraints and this may cause the random perturbations all lying on the surface of the epsilon ball. To further address your concern, we conduct experiments according to your suggestion anyway by enlarging the random perturbation range to [0, 2*eps] and then continue with Fast AT. This leads to a robustness of 43.23% on CIFAR-10 dataset, which is even slightly worse than the original Fast AT algorithm. \n\nQ3: Fast TRADES seems to be at a different point of the tradeoff ...tuning the hyperparameter for Fast TRADES be enough to emulate the TRADES result?\n\nA3: Thank you for your suggestion, we have comprehensively tuned the result for Fast TRADES (step sizes have also been tuned). \n\n| Beta | Nat   | Rob   |\n|------|-------|-------|\n| 2    | 87.34 | 41.55 |\n| 4    | 85.98 | 42.02 |\n| 6    | 84.80 | 45.91 |\n| 8    | 83.39 | 46.98 |\n| 10   | 83.03 | 46.46 |\n\nIndeed, Fast TRADES could correspond to slightly shifted trade-off points as can be seen from the above table. However, the best results here are still far from the robustness achieved by our Backward Smoothing algorithm (52.50%) and there is still large gap compared to the standard TRADES method.\n\nQ4: Can the authors clarify how \u03be is initialized in eq 4.4?\n\nA4: We are sorry for the confusion. \u03be is initialized as zero vector in eq 4.4. \n\nQ5: Is there any insight to what differences between backward smoothing and TRADES are exposed with a stronger attack?\n\nA5: Note that aside from different attacks, we actually adopt larger models (WideResNet) in Table 5\u2019s results. And we believe that this is the major factor for the difference here rather than the different attacks. Intuitively speaking, larger architectures possess larger capacities and have more complicated decision boundaries. Therefore, may need stronger attacks to reach the dart spots in the area. On the other hand, we also test AutoAttack on smaller ResNet18 models used in Table 2, and the following results suggest that the difference by using different attacks are indeed marginal.\n\n| Methods            | PGD-100 | AutoAttack |\n|--------------------|---------|------------|\n| Backward Smoothing | 52.50   | 48.36      |\n| TRADES             | 52.74   | 48.86      |\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2457/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Robust Training via Backward Smoothing", "authorids": ["~Jinghui_Chen1", "~Yu_Cheng1", "~Zhe_Gan1", "~Quanquan_Gu1", "~Jingjing_Liu2"], "authors": ["Jinghui Chen", "Yu Cheng", "Zhe Gan", "Quanquan Gu", "Jingjing Liu"], "keywords": ["Efficient Robust Training", "Backward Smoothing", "Robustness"], "abstract": "Adversarial training is so far the most effective strategy in defending against adversarial examples. However, it suffers from high computational cost due to the iterative adversarial attacks in each training step. Recent studies show that it is possible to achieve Fast Adversarial Training by performing a single-step attack with random initialization. Yet, it remains a mystery why random initialization helps. Besides, such an approach still lags behind state-of-the-art adversarial training algorithms on both stability and model robustness. In this work, we develop a new understanding towards Fast Adversarial Training, by viewing random initialization as performing randomized smoothing for better optimization of the inner maximization problem. From this perspective, we show that the smoothing effect by random initialization is not sufficient under the adversarial perturbation constraint. A new initialization strategy, \\emph{backward smoothing}, is proposed to address this issue and significantly improves both stability and model robustness over single-step robust training methods. Experiments on multiple benchmarks demonstrate that our method achieves similar model robustness as the original TRADES method, while using much less training time (~3x improvement with the same training schedule). ", "one-sentence_summary": "We propose a new principle towards understanding Fast Adversarial Training, and a new initialization strategy that significantly improves both stability and model robustness over the single-step robust training methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|efficient_robust_training_via_backward_smoothing", "supplementary_material": "/attachment/f69958051f5c543363c667035bcf9f4de83ec9e2.zip", "pdf": "/pdf/ee969cde7ffd2ade652f069571e71705a0b27adb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aMIRgsXyhS", "_bibtex": "@misc{\nchen2021efficient,\ntitle={Efficient Robust Training via Backward Smoothing},\nauthor={Jinghui Chen and Yu Cheng and Zhe Gan and Quanquan Gu and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=49V11oUejQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "49V11oUejQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2457/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2457/Authors|ICLR.cc/2021/Conference/Paper2457/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848168, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment"}}}, {"id": "UncqJRL-20-", "original": null, "number": 5, "cdate": 1605597795886, "ddate": null, "tcdate": 1605597795886, "tmdate": 1605598118143, "tddate": null, "forum": "49V11oUejQ", "replyto": "4JwCGy-8Kp2", "invitation": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment", "content": {"title": "Authors\u2019 Response to Reviewer 1", "comment": "Thank you for your insightful comments. Below, we provide detailed responses to your questions.\n\nQ1: The theoretical explanation of why random initialization ...An alternative explanation of the role of random initialization is presented... which would be worth discussing.\n\nA1:  First, please refer to our response to all reviewers regarding the relationship with the pointed paper. Second, thank you for your suggestion, we have added extra experiments of using multiple random points. As can be seen from the following table, a single random point already leads to similar performance but saves more time. Note that our target is to improve the efficiency of adversarial training, therefore, we only use a single random point for randomized smoothing.\n\n| # random points | Robust Accuracy (PGD-100) | Time (min) |\n|-----------------|---------------------------|------------|\n| 1               | 52.50                     | 164        |\n| 2               | 52.67                     | 204        |\n| 5               | 52.70                     | 316        |\n| 10              | 52.73                     | 510        |\n\n\nQ2: The proposed method seems to share some similarities ... and I think the authors should comment on this.\n\nA2: Thank you for pointing it out. We will comment and discuss this paper in the revision.\n\nQ3: In the comparison to other methods, I think for completeness it'd be good to show also Fast AT with 2 steps, ...might give a more complete picture.\n\nA3: Thank you for your suggestion, we have added experiments for Fast AT with 2 steps. It achieves a robustness of 49.91% and 27.84% for CIFAR-10 and CIFAR-100 dataset on the ResNet-18 model, which still falls behind our Backward Smoothing method (52.50%/30.50%). We will update the tables in the revision. Compared with other baselines, Fast AT paper has already shown its advantage over (Shafahi et al., 2019) in their original paper. For (Andriushchenko & Flammarion, 2020), according to the policy, we are not required to compare with the recent work published less than 2 months from submission, but we will try to add a comparison later.\n\nQ4: In Table 5, the gap between Backward Smoothing... the larger architecture or just the evaluation with stronger attacks (or something else)?\n\nA4: We believe that the larger architecture is the main factor that influences the final robustness. Larger architectures intuitively have larger capacities and may need stronger attacks to reach some dark spot in the area. On the other hand, we also test AutoAttack on smaller ResNet18 models used in Table 2, and the following results suggest that the difference by using different attacks are indeed marginal.\n\n| Methods            | PGD-100 | AutoAttack |\n|--------------------|---------|------------|\n| Backward Smoothing | 52.50   | 48.36      |\n| TRADES             | 52.74   | 48.86      |\n\nQ5: How does the proposed technique behave...gap between single- and multi-step AT becomes even larger)?\n\nA5: Thank you for your suggestion, we have added experiments with eps=16/255 for CIFAR-10 data. \n\n| Method               | Nat (%) | Rob (%) | Time (min) |\n|----------------------|---------|---------|------------|\n| AT                   | 62.76   | 32.03   | 425        |\n| Fast AT              | 53.72   | 20.12   | 89         |\n| TRADES               | 62.09   | 28.63   | 470        |\n| Fast TRADES          | 56.55   | 17.47   | 137        |\n| Fast TRADES (2-step) | 57.22   | 19.28   | 167        |\n| Backward Smoothing   | 63.47   | 25.04   | 164        |\n\nWe can see that Backward Smoothing still achieves significant improvements over other single-step adversarial training algorithms although the gap between multi-step adversarial training methods is indeed larger.  \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2457/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Robust Training via Backward Smoothing", "authorids": ["~Jinghui_Chen1", "~Yu_Cheng1", "~Zhe_Gan1", "~Quanquan_Gu1", "~Jingjing_Liu2"], "authors": ["Jinghui Chen", "Yu Cheng", "Zhe Gan", "Quanquan Gu", "Jingjing Liu"], "keywords": ["Efficient Robust Training", "Backward Smoothing", "Robustness"], "abstract": "Adversarial training is so far the most effective strategy in defending against adversarial examples. However, it suffers from high computational cost due to the iterative adversarial attacks in each training step. Recent studies show that it is possible to achieve Fast Adversarial Training by performing a single-step attack with random initialization. Yet, it remains a mystery why random initialization helps. Besides, such an approach still lags behind state-of-the-art adversarial training algorithms on both stability and model robustness. In this work, we develop a new understanding towards Fast Adversarial Training, by viewing random initialization as performing randomized smoothing for better optimization of the inner maximization problem. From this perspective, we show that the smoothing effect by random initialization is not sufficient under the adversarial perturbation constraint. A new initialization strategy, \\emph{backward smoothing}, is proposed to address this issue and significantly improves both stability and model robustness over single-step robust training methods. Experiments on multiple benchmarks demonstrate that our method achieves similar model robustness as the original TRADES method, while using much less training time (~3x improvement with the same training schedule). ", "one-sentence_summary": "We propose a new principle towards understanding Fast Adversarial Training, and a new initialization strategy that significantly improves both stability and model robustness over the single-step robust training methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|efficient_robust_training_via_backward_smoothing", "supplementary_material": "/attachment/f69958051f5c543363c667035bcf9f4de83ec9e2.zip", "pdf": "/pdf/ee969cde7ffd2ade652f069571e71705a0b27adb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aMIRgsXyhS", "_bibtex": "@misc{\nchen2021efficient,\ntitle={Efficient Robust Training via Backward Smoothing},\nauthor={Jinghui Chen and Yu Cheng and Zhe Gan and Quanquan Gu and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=49V11oUejQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "49V11oUejQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2457/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2457/Authors|ICLR.cc/2021/Conference/Paper2457/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848168, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment"}}}, {"id": "MZYEo_4xLXH", "original": null, "number": 4, "cdate": 1605597723080, "ddate": null, "tcdate": 1605597723080, "tmdate": 1605598092111, "tddate": null, "forum": "49V11oUejQ", "replyto": "GrbC_6RQPyX", "invitation": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment", "content": {"title": "Authors\u2019 Response to Reviewer 2", "comment": "Thank you for your insightful comments. Below, we provide detailed responses to your questions.\n\nQ1: The explanations from Section \u201cWhy random initialization helps?\u201d are not convincing ...This is my main concern... achieve better robustness.\n\nA1: First, please refer to our response to all reviewers regarding the relationship with the pointed paper. Second, using a step size of 2*eps is simply too large. To the best of our knowledge, there does not exist a single-step adversarial training algorithm that works with step size 2*eps. Nevertheless, we can show that we indeed take the step size into account and the comparison with Fast AT on various step sizes shows the advantage of our proposed method.\n\n| Step Size | Fast AT | Backward Smoothing |\n|-----------|---------|--------------------|\n| 6/255     | 41.47   | 52.38              |\n| 7/255     | 43.00   | 52.40              |\n| 8/255     | 44.46   | 52.50              |\n| 9/255     | 45.39   | 52.16              |\n| 10/255    | 46.30   | 52.04              |\n| 11/255    | 39.66   | 42.45              |\n| 12/255    | 38.52   | 41.12              |\n \nQ2: All the experiments are performed for eps=8/255... compared to the competing approaches for a higher eps, e.g. eps=16/255 on CIFAR-10.\n\nA2: Thank you for your suggestion, we have added experiments with eps=16/255 for CIFAR-10 dataset on the ResNet18 model.\n\n| Method               | Nat (%) | Rob (%) | Time (min) |\n|----------------------|---------|---------|------------|\n| AT                   | 62.76   | 32.03   | 425        |\n| Fast AT              | 53.72   | 20.12   | 89         |\n| TRADES               | 62.09   | 28.63   | 470        |\n| Fast TRADES          | 56.55   | 17.47   | 137        |\n| Fast TRADES (2-step) | 57.22   | 19.28   | 167        |\n| Backward Smoothing   | 63.47   | 25.04   | 164        |\n\nWe can see that Backward Smoothing still achieves significant improvements over other single-step adversarial training algorithms. This verifies the effectiveness of the proposed method. \n\nQ3: The proposed procedure of backward smoothing... I think it would be important to at least acknowledge this connection.\n\nA3: Thank you for pointing it out. We will acknowledge the connection and discuss the paper in the revision.\n\nQ4: For the inner maximization problem, taking two steps of PGD ... results of a grid search over it, i.e. similarly to Table 7 but for Fast TRADES (2-step).\n\nA4: Thank you for your suggestion, we have added grid search results for Fast TRADES (2-step).  \n\n| Step Size | Fast TRADES |\n|-----------|-------------|\n| 4/255     | 48.54       |\n| 5/255     | 48.78       |\n| 6/255     | 48.33       |\n| 7/255     | 48.08       |\n| 8/255     | 48.01       |\n| 9/255     | 47.38       |\n| 10/255    | 45.87       |\n\nWe can observe that after careful tuning, we manage to improve the robustness of Fast TRADES (2-step) by 0.3%. We will update the result in the revision. \n\nQ5: Fig. 2: the best step size is at the boundary of the grid {10/255, 8/255, 7/255}, ... Fast TRADES is not reported with suboptimal hyperparameters.\n\nA5: Thank you for your suggestion, we also added extra tuning results for Fast TRADES.\n\n| Step Size | Fast TRADES |\n|-----------|-------------|\n| 5/255     | 44.01       |\n| 6/255     | 45.91       |\n| 7/255     | 45.21       |\n| 8/255     | 43.36       |\n| 10/255    | 38.83       |\n\nWe observe that a step size of 6/255 achieves slightly better robustness (0.7%) for Fast TRADES. We will update the result in the revision. \n\nQ6: I'm also a bit surprised that ...Can it be, for example, because their hyperparameters were not sufficiently tuned (unlike for proposed method)?\n\nA6: First, we did not claim that backward smoothing constantly beats the state-of-the-art methods, we only achieve slightly better results for one or two settings. And we indeed tuned the hyperparameters for all methods. Second, the success of Fast Adversarial Training has already suggested that for adversarial training, stronger attacks for the inner problem are not required as mentioned in their original paper.\n\nQ7: Table 5: the listed \"AT\" baseline is quite weak (44.04%) ... show results of AT+early stopping comparable to that of TRADES. \nTable 5: it would be useful to add Fast TRADES (2-step) to the table as this is the most interesting baseline.\n\nA7: Thank you for your suggestion, we have retrained an AT model with early stopping and also tested Fast TRADES (2-step) for Table 5. \n\n| Method               | AutoAttack |\n|----------------------|------------|\n| AT                   | 44.04      |\n| AT (early-stop)      | 49.10      |\n| Fast AT              | 43.21      |\n| TRADES               | 53.08      |\n| Fast TRADES (2-step) | 48.21      |\n| Backward Smoothing   | 51.13      |\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2457/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Robust Training via Backward Smoothing", "authorids": ["~Jinghui_Chen1", "~Yu_Cheng1", "~Zhe_Gan1", "~Quanquan_Gu1", "~Jingjing_Liu2"], "authors": ["Jinghui Chen", "Yu Cheng", "Zhe Gan", "Quanquan Gu", "Jingjing Liu"], "keywords": ["Efficient Robust Training", "Backward Smoothing", "Robustness"], "abstract": "Adversarial training is so far the most effective strategy in defending against adversarial examples. However, it suffers from high computational cost due to the iterative adversarial attacks in each training step. Recent studies show that it is possible to achieve Fast Adversarial Training by performing a single-step attack with random initialization. Yet, it remains a mystery why random initialization helps. Besides, such an approach still lags behind state-of-the-art adversarial training algorithms on both stability and model robustness. In this work, we develop a new understanding towards Fast Adversarial Training, by viewing random initialization as performing randomized smoothing for better optimization of the inner maximization problem. From this perspective, we show that the smoothing effect by random initialization is not sufficient under the adversarial perturbation constraint. A new initialization strategy, \\emph{backward smoothing}, is proposed to address this issue and significantly improves both stability and model robustness over single-step robust training methods. Experiments on multiple benchmarks demonstrate that our method achieves similar model robustness as the original TRADES method, while using much less training time (~3x improvement with the same training schedule). ", "one-sentence_summary": "We propose a new principle towards understanding Fast Adversarial Training, and a new initialization strategy that significantly improves both stability and model robustness over the single-step robust training methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|efficient_robust_training_via_backward_smoothing", "supplementary_material": "/attachment/f69958051f5c543363c667035bcf9f4de83ec9e2.zip", "pdf": "/pdf/ee969cde7ffd2ade652f069571e71705a0b27adb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aMIRgsXyhS", "_bibtex": "@misc{\nchen2021efficient,\ntitle={Efficient Robust Training via Backward Smoothing},\nauthor={Jinghui Chen and Yu Cheng and Zhe Gan and Quanquan Gu and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=49V11oUejQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "49V11oUejQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2457/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2457/Authors|ICLR.cc/2021/Conference/Paper2457/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848168, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment"}}}, {"id": "jR7GiNU6Xrn", "original": null, "number": 2, "cdate": 1605597498543, "ddate": null, "tcdate": 1605597498543, "tmdate": 1605597498543, "tddate": null, "forum": "49V11oUejQ", "replyto": "49V11oUejQ", "invitation": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment", "content": {"title": "General Response to All Reviewers", "comment": "Reviewers 1,2,4 all mentioned an alternative explanation is presented in (Andriushchenko & Flammarion, 2020), we also read and cited this work in our submission. They provided an explanation of why Fast Adversarial Training works: random initialization reduces the magnitude of the perturbation and thus the network becomes more linear and fits better toward FGSM attack. While we argue that the random initialization works as randomized smoothing for smoothing the inner maximization problem (making it easier to solve). In fact, our argument is more general and can cover theirs, because if the loss function is approximately linear, then it will be very smooth (i.e., the second-order term in the Taylor expansion is very small). Also note that our method indeed achieves very close performance with current-best methods on benchmark datasets and leaderboards. We will discuss the relationship of the two works in detail in our revision.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2457/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Robust Training via Backward Smoothing", "authorids": ["~Jinghui_Chen1", "~Yu_Cheng1", "~Zhe_Gan1", "~Quanquan_Gu1", "~Jingjing_Liu2"], "authors": ["Jinghui Chen", "Yu Cheng", "Zhe Gan", "Quanquan Gu", "Jingjing Liu"], "keywords": ["Efficient Robust Training", "Backward Smoothing", "Robustness"], "abstract": "Adversarial training is so far the most effective strategy in defending against adversarial examples. However, it suffers from high computational cost due to the iterative adversarial attacks in each training step. Recent studies show that it is possible to achieve Fast Adversarial Training by performing a single-step attack with random initialization. Yet, it remains a mystery why random initialization helps. Besides, such an approach still lags behind state-of-the-art adversarial training algorithms on both stability and model robustness. In this work, we develop a new understanding towards Fast Adversarial Training, by viewing random initialization as performing randomized smoothing for better optimization of the inner maximization problem. From this perspective, we show that the smoothing effect by random initialization is not sufficient under the adversarial perturbation constraint. A new initialization strategy, \\emph{backward smoothing}, is proposed to address this issue and significantly improves both stability and model robustness over single-step robust training methods. Experiments on multiple benchmarks demonstrate that our method achieves similar model robustness as the original TRADES method, while using much less training time (~3x improvement with the same training schedule). ", "one-sentence_summary": "We propose a new principle towards understanding Fast Adversarial Training, and a new initialization strategy that significantly improves both stability and model robustness over the single-step robust training methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|efficient_robust_training_via_backward_smoothing", "supplementary_material": "/attachment/f69958051f5c543363c667035bcf9f4de83ec9e2.zip", "pdf": "/pdf/ee969cde7ffd2ade652f069571e71705a0b27adb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aMIRgsXyhS", "_bibtex": "@misc{\nchen2021efficient,\ntitle={Efficient Robust Training via Backward Smoothing},\nauthor={Jinghui Chen and Yu Cheng and Zhe Gan and Quanquan Gu and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=49V11oUejQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "49V11oUejQ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2457/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2457/Authors|ICLR.cc/2021/Conference/Paper2457/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848168, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2457/-/Official_Comment"}}}, {"id": "Q9w7fXicZ5C", "original": null, "number": 1, "cdate": 1605551162745, "ddate": null, "tcdate": 1605551162745, "tmdate": 1605551247365, "tddate": null, "forum": "49V11oUejQ", "replyto": "Qan3eHWT17Y", "invitation": "ICLR.cc/2021/Conference/Paper2457/-/Public_Comment", "content": {"title": "Should be compared with PGD-2 (as uses only 2 inner gradient steps) ?", "comment": "Does it make sense to compare performance with PGD-2 (2 pgd steps) as both methods will have the same computation cost (assuming \"backward smoothing\" can be applied on top of FAST-AT) ?"}, "signatures": ["~Abhay_Yadav1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Abhay_Yadav1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Robust Training via Backward Smoothing", "authorids": ["~Jinghui_Chen1", "~Yu_Cheng1", "~Zhe_Gan1", "~Quanquan_Gu1", "~Jingjing_Liu2"], "authors": ["Jinghui Chen", "Yu Cheng", "Zhe Gan", "Quanquan Gu", "Jingjing Liu"], "keywords": ["Efficient Robust Training", "Backward Smoothing", "Robustness"], "abstract": "Adversarial training is so far the most effective strategy in defending against adversarial examples. However, it suffers from high computational cost due to the iterative adversarial attacks in each training step. Recent studies show that it is possible to achieve Fast Adversarial Training by performing a single-step attack with random initialization. Yet, it remains a mystery why random initialization helps. Besides, such an approach still lags behind state-of-the-art adversarial training algorithms on both stability and model robustness. In this work, we develop a new understanding towards Fast Adversarial Training, by viewing random initialization as performing randomized smoothing for better optimization of the inner maximization problem. From this perspective, we show that the smoothing effect by random initialization is not sufficient under the adversarial perturbation constraint. A new initialization strategy, \\emph{backward smoothing}, is proposed to address this issue and significantly improves both stability and model robustness over single-step robust training methods. Experiments on multiple benchmarks demonstrate that our method achieves similar model robustness as the original TRADES method, while using much less training time (~3x improvement with the same training schedule). ", "one-sentence_summary": "We propose a new principle towards understanding Fast Adversarial Training, and a new initialization strategy that significantly improves both stability and model robustness over the single-step robust training methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|efficient_robust_training_via_backward_smoothing", "supplementary_material": "/attachment/f69958051f5c543363c667035bcf9f4de83ec9e2.zip", "pdf": "/pdf/ee969cde7ffd2ade652f069571e71705a0b27adb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aMIRgsXyhS", "_bibtex": "@misc{\nchen2021efficient,\ntitle={Efficient Robust Training via Backward Smoothing},\nauthor={Jinghui Chen and Yu Cheng and Zhe Gan and Quanquan Gu and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=49V11oUejQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "49V11oUejQ", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/Authors", "ICLR.cc/2021/Conference/Paper2457/Reviewers", "ICLR.cc/2021/Conference/Paper2457/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024962630, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2457/-/Public_Comment"}}}, {"id": "Qan3eHWT17Y", "original": null, "number": 1, "cdate": 1602735762008, "ddate": null, "tcdate": 1602735762008, "tmdate": 1605024206748, "tddate": null, "forum": "49V11oUejQ", "replyto": "49V11oUejQ", "invitation": "ICLR.cc/2021/Conference/Paper2457/-/Official_Review", "content": {"title": "Review", "review": "Summary: This paper seeks to reduce the training time of TRADES adversarial training. It tries to understand fast adversarial training methods (single-step adversary methods) by viewing the random initialization of the adversarial perturbation in the PGD steps as randomized smoothing, making the inner maximization (the adversary) easier. They find empirically that this smoothing isn't enough for vanilla fast adversarial training with the TRADES objective (in the sense that it doesn't improve much beyond fast AT after switching out the objective), requiring large step sizes that bring instability to training. Then, they aim to improve fast TRADES by their proposed algorithm, \"backward smoothing\", which first perturbs the output of the model and then solves the inverse problem to find the corresponding input that would make this perturbation. The intuition is to make sure that the smoothing averages over a variety of output values to get a stronger smoothing effect. This seems to give similar results to TRADES while cutting down the training time significantly (only two inner PGD steps).\n\nStrengths:\n- The paper is pretty clearly written, the results are impressive in terms of cutting down the training time of TRADES about 4x, and there are a substantial number of experiments, including fair comparisons with 2-step fast TRADES and on strong attacks. \n- Sometimes, the robust accuracy is even better than TRADES, but the clean accuracy is almost always a bit worse.\n- They use many steps of PGD (100 steps) for robust accuracy evaluation.\n- The method is relatively simple (similar mechanics to PGD), and uses only 2 inner gradient steps. It seems to give more stable results than fast AT or fast TRADES, although most of the stability gain seems to come from switching to the TRADES objective (Fig 5).\n\nWeaknesses:\n- My main concern is the backward smoothing method seems general but only really works for TRADES (and not PGD AT). The intuitions that motivate the method don't seem special to TRADES, so it would be good to have some understanding of this phenomenon. Can the intuition given in the paper be verified somehow? Also, other papers have shown that tweaking PGD-AT (such as using early stopping) allows it to get just as good or better results than TRADES, so it's not clear that we should necessarily prefer a method based on TRADES. \n- The authors mention the natural way to increase smoothing, which would be to use larger random perturbations. They argue that because of the norm constraint, we cannot increase the perturbations. However, even with the norm constraint it's clear that larger perturbations would not be the same as small perturbations, and could plausibly have a different effect. This point would be more convincing if there were some experiments just using larger perturbations for random initialization.\n- Fast TRADES seems to be at a different point of the tradeoff curve between natural accuracy and robust accuracy, and perhaps it responds differently to the TRADES hyperparameter. From Table 2,3 etc. , we can see that Fast TRADES tends to improve natural accuracy and be worse than robust accuracy than TRADES. Could tuning the hyperparameter for Fast TRADES be enough to emulate the TRADES result?\n\nOther/clarifications:\n- Can the authors clarify how $\\xi$ is initialized in eq 4.4? \n- Is there any insight to what differences between backward smoothing and TRADES are exposed with a stronger attack?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2457/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2457/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Robust Training via Backward Smoothing", "authorids": ["~Jinghui_Chen1", "~Yu_Cheng1", "~Zhe_Gan1", "~Quanquan_Gu1", "~Jingjing_Liu2"], "authors": ["Jinghui Chen", "Yu Cheng", "Zhe Gan", "Quanquan Gu", "Jingjing Liu"], "keywords": ["Efficient Robust Training", "Backward Smoothing", "Robustness"], "abstract": "Adversarial training is so far the most effective strategy in defending against adversarial examples. However, it suffers from high computational cost due to the iterative adversarial attacks in each training step. Recent studies show that it is possible to achieve Fast Adversarial Training by performing a single-step attack with random initialization. Yet, it remains a mystery why random initialization helps. Besides, such an approach still lags behind state-of-the-art adversarial training algorithms on both stability and model robustness. In this work, we develop a new understanding towards Fast Adversarial Training, by viewing random initialization as performing randomized smoothing for better optimization of the inner maximization problem. From this perspective, we show that the smoothing effect by random initialization is not sufficient under the adversarial perturbation constraint. A new initialization strategy, \\emph{backward smoothing}, is proposed to address this issue and significantly improves both stability and model robustness over single-step robust training methods. Experiments on multiple benchmarks demonstrate that our method achieves similar model robustness as the original TRADES method, while using much less training time (~3x improvement with the same training schedule). ", "one-sentence_summary": "We propose a new principle towards understanding Fast Adversarial Training, and a new initialization strategy that significantly improves both stability and model robustness over the single-step robust training methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|efficient_robust_training_via_backward_smoothing", "supplementary_material": "/attachment/f69958051f5c543363c667035bcf9f4de83ec9e2.zip", "pdf": "/pdf/ee969cde7ffd2ade652f069571e71705a0b27adb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aMIRgsXyhS", "_bibtex": "@misc{\nchen2021efficient,\ntitle={Efficient Robust Training via Backward Smoothing},\nauthor={Jinghui Chen and Yu Cheng and Zhe Gan and Quanquan Gu and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=49V11oUejQ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "49V11oUejQ", "replyto": "49V11oUejQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2457/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538095953, "tmdate": 1606915784180, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2457/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2457/-/Official_Review"}}}], "count": 22}