{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486478588552, "tcdate": 1486396405609, "number": 1, "id": "Sy0lnzIdl", "invitation": "ICLR.cc/2017/conference/-/paper165/acceptance", "forum": "B1vRTeqxg", "replyto": "B1vRTeqxg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "As part of this meta-review, I read the paper and found some surprising claims, such as the somewhat poorly motivated claim that coercing the output of a sub-network be a unit vector my dividing it by its L2 norm is close to layer normalisation which is mathematically almost true, if the mean of of the activations is 0, and we accept a fixed offset in the calculation of the stddev, but conceptually a different form of normalisation. It is also curious that other methods of obtaining stable training in recursive networks, such as TreeLSTM (Zhu et al. 2015, Tai et al. 2015), were not compared to. None of these problems is particularly damning but it is slightly disappointing not to see these issues discussed in the review process.\n\nOverall, the reviews, which I found superficial in comparison to the other papers I am chairing, found the method proposed here sound, although some details lacked explanation. The consensus was that the general problem being addressed is interesting and timely, given the attention the topics of program induction and interpretation have been receiving in the community recently. There was also consensus that the setting the model was evaluated on was far too simple and unnatural, and that there is need for a more complex, task involving symbolic interpretation to validate the model. It is hard to tell, given all the design decisions made (l2-normalisation vs layer norm, softmax not working), whether the end product is tailored to the task at hand, and whether it will tell us something useful about how this approach generalises. I am inclined, on the basis of the reviewer's opinions of the setting and my own concerns outlined above, to recommend redirection to the workshop track.", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Continuous Semantic Representations of Symbolic Expressions", "abstract": "The question of how procedural knowledge is represented and inferred is a fundamental problem in machine learning and artificial intelligence. Recent work on program induction has proposed neural architectures, based on abstractions like stacks, Turing machines, and interpreters,  that operate on abstract computational machines or on execution traces. But the recursive abstraction that is central to procedural knowledge is perhaps most naturally represented by symbolic representations that have syntactic structure, such as logical expressions and source code. Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of mathematical and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation  on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.\n", "pdf": "/pdf/8cfedc863eec47c1c047f231b88712e77bd698ab.pdf", "TL;DR": "Assign continuous vectors to logical and algebraic symbolic expressions in such a way that semantically equivalent, but syntactically diverse expressions are assigned to identical (or highly similar) continuous vectors.", "paperhash": "allamanis|learning_continuous_semantic_representations_of_symbolic_expressions", "keywords": ["Deep learning"], "conflicts": ["ed.ac.uk", "microsoft.com"], "authors": ["Miltiadis Allamanis", "Pankajan Chanthirasegaran", "Pushmeet Kohli", "Charles Sutton"], "authorids": ["m.allamanis@ed.ac.uk", "pankajan.chanthirasegaran@ed.ac.uk", "pkohli@microsoft.com", "csutton@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396406960, "id": "ICLR.cc/2017/conference/-/paper165/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "B1vRTeqxg", "replyto": "B1vRTeqxg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396406960}}}, {"tddate": null, "tmdate": 1484860687356, "tcdate": 1481940882960, "number": 3, "id": "SyiqkXfNx", "invitation": "ICLR.cc/2017/conference/-/paper165/official/review", "forum": "B1vRTeqxg", "replyto": "B1vRTeqxg", "signatures": ["ICLR.cc/2017/conference/paper165/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper165/AnonReviewer2"], "content": {"title": "Symbolic Expression Representations.", "rating": "7: Good paper, accept", "review": "The authors propose a new model to learn symbolic expression representations. They do a reasonably extensive evaluation with similar approaches and motivate their approach well.\n\nAs expressed in the preliminary questions, I think the authors could improve the motivation for their subexpforce loss.\n\nAt the top of page 6 the authors mention that they compare to two-layer MLP w/o residual connections. I think having a direct comparison between a model with and w/o the subexpforce loss would be helpful too and should be included (i.e. keep the residual connections and normalization).\n\nMy main concern is the evaluation \"score\". It appears to be precision on a per query basis. I believe a more standard metric, precision-recall or roc would be more informative. In particular the chosen metric is expected to perform better when the equivalence classes are larger, since this isn't taken into account in the denominator, but the likelihood of a random expression matching the query increases.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Continuous Semantic Representations of Symbolic Expressions", "abstract": "The question of how procedural knowledge is represented and inferred is a fundamental problem in machine learning and artificial intelligence. Recent work on program induction has proposed neural architectures, based on abstractions like stacks, Turing machines, and interpreters,  that operate on abstract computational machines or on execution traces. But the recursive abstraction that is central to procedural knowledge is perhaps most naturally represented by symbolic representations that have syntactic structure, such as logical expressions and source code. Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of mathematical and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation  on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.\n", "pdf": "/pdf/8cfedc863eec47c1c047f231b88712e77bd698ab.pdf", "TL;DR": "Assign continuous vectors to logical and algebraic symbolic expressions in such a way that semantically equivalent, but syntactically diverse expressions are assigned to identical (or highly similar) continuous vectors.", "paperhash": "allamanis|learning_continuous_semantic_representations_of_symbolic_expressions", "keywords": ["Deep learning"], "conflicts": ["ed.ac.uk", "microsoft.com"], "authors": ["Miltiadis Allamanis", "Pankajan Chanthirasegaran", "Pushmeet Kohli", "Charles Sutton"], "authorids": ["m.allamanis@ed.ac.uk", "pankajan.chanthirasegaran@ed.ac.uk", "pkohli@microsoft.com", "csutton@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512677047, "id": "ICLR.cc/2017/conference/-/paper165/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper165/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper165/AnonReviewer3", "ICLR.cc/2017/conference/paper165/AnonReviewer1", "ICLR.cc/2017/conference/paper165/AnonReviewer2"], "reply": {"forum": "B1vRTeqxg", "replyto": "B1vRTeqxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper165/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper165/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512677047}}}, {"tddate": null, "tmdate": 1484846700460, "tcdate": 1484846700460, "number": 7, "id": "Hk4_LdCUx", "invitation": "ICLR.cc/2017/conference/-/paper165/public/comment", "forum": "B1vRTeqxg", "replyto": "S1jqpoGLe", "signatures": ["~Miltiadis_Allamanis1"], "readers": ["everyone"], "writers": ["~Miltiadis_Allamanis1"], "content": {"title": "ROC and PR curves", "comment": "Sorry for the long wait. I just updated the paper including ROC and PR curves. You can find them in page 13 as Figures 7 and 8. As you can see from the curves, EqNet still performs very well compared to the other methods."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Continuous Semantic Representations of Symbolic Expressions", "abstract": "The question of how procedural knowledge is represented and inferred is a fundamental problem in machine learning and artificial intelligence. Recent work on program induction has proposed neural architectures, based on abstractions like stacks, Turing machines, and interpreters,  that operate on abstract computational machines or on execution traces. But the recursive abstraction that is central to procedural knowledge is perhaps most naturally represented by symbolic representations that have syntactic structure, such as logical expressions and source code. Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of mathematical and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation  on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.\n", "pdf": "/pdf/8cfedc863eec47c1c047f231b88712e77bd698ab.pdf", "TL;DR": "Assign continuous vectors to logical and algebraic symbolic expressions in such a way that semantically equivalent, but syntactically diverse expressions are assigned to identical (or highly similar) continuous vectors.", "paperhash": "allamanis|learning_continuous_semantic_representations_of_symbolic_expressions", "keywords": ["Deep learning"], "conflicts": ["ed.ac.uk", "microsoft.com"], "authors": ["Miltiadis Allamanis", "Pankajan Chanthirasegaran", "Pushmeet Kohli", "Charles Sutton"], "authorids": ["m.allamanis@ed.ac.uk", "pankajan.chanthirasegaran@ed.ac.uk", "pkohli@microsoft.com", "csutton@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287704210, "id": "ICLR.cc/2017/conference/-/paper165/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1vRTeqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper165/reviewers", "ICLR.cc/2017/conference/paper165/areachairs"], "cdate": 1485287704210}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484846462995, "tcdate": 1478262222874, "number": 165, "id": "B1vRTeqxg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "B1vRTeqxg", "signatures": ["~Miltiadis_Allamanis1"], "readers": ["everyone"], "content": {"title": "Learning Continuous Semantic Representations of Symbolic Expressions", "abstract": "The question of how procedural knowledge is represented and inferred is a fundamental problem in machine learning and artificial intelligence. Recent work on program induction has proposed neural architectures, based on abstractions like stacks, Turing machines, and interpreters,  that operate on abstract computational machines or on execution traces. But the recursive abstraction that is central to procedural knowledge is perhaps most naturally represented by symbolic representations that have syntactic structure, such as logical expressions and source code. Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of mathematical and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation  on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.\n", "pdf": "/pdf/8cfedc863eec47c1c047f231b88712e77bd698ab.pdf", "TL;DR": "Assign continuous vectors to logical and algebraic symbolic expressions in such a way that semantically equivalent, but syntactically diverse expressions are assigned to identical (or highly similar) continuous vectors.", "paperhash": "allamanis|learning_continuous_semantic_representations_of_symbolic_expressions", "keywords": ["Deep learning"], "conflicts": ["ed.ac.uk", "microsoft.com"], "authors": ["Miltiadis Allamanis", "Pankajan Chanthirasegaran", "Pushmeet Kohli", "Charles Sutton"], "authorids": ["m.allamanis@ed.ac.uk", "pankajan.chanthirasegaran@ed.ac.uk", "pkohli@microsoft.com", "csutton@ed.ac.uk"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1484496809239, "tcdate": 1484496809239, "number": 6, "id": "SkZ3y7tLl", "invitation": "ICLR.cc/2017/conference/-/paper165/public/comment", "forum": "B1vRTeqxg", "replyto": "S1jqpoGLe", "signatures": ["~Miltiadis_Allamanis1"], "readers": ["everyone"], "writers": ["~Miltiadis_Allamanis1"], "content": {"title": "ROC and PR curve", "comment": "I now understand your point. Yes, of course, a ROC/PR curve can be computed as you suggest. I need some time to get this evaluation going, but I'll try to update the paper adding these new graphs within the next few days.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Continuous Semantic Representations of Symbolic Expressions", "abstract": "The question of how procedural knowledge is represented and inferred is a fundamental problem in machine learning and artificial intelligence. Recent work on program induction has proposed neural architectures, based on abstractions like stacks, Turing machines, and interpreters,  that operate on abstract computational machines or on execution traces. But the recursive abstraction that is central to procedural knowledge is perhaps most naturally represented by symbolic representations that have syntactic structure, such as logical expressions and source code. Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of mathematical and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation  on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.\n", "pdf": "/pdf/8cfedc863eec47c1c047f231b88712e77bd698ab.pdf", "TL;DR": "Assign continuous vectors to logical and algebraic symbolic expressions in such a way that semantically equivalent, but syntactically diverse expressions are assigned to identical (or highly similar) continuous vectors.", "paperhash": "allamanis|learning_continuous_semantic_representations_of_symbolic_expressions", "keywords": ["Deep learning"], "conflicts": ["ed.ac.uk", "microsoft.com"], "authors": ["Miltiadis Allamanis", "Pankajan Chanthirasegaran", "Pushmeet Kohli", "Charles Sutton"], "authorids": ["m.allamanis@ed.ac.uk", "pankajan.chanthirasegaran@ed.ac.uk", "pkohli@microsoft.com", "csutton@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287704210, "id": "ICLR.cc/2017/conference/-/paper165/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1vRTeqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper165/reviewers", "ICLR.cc/2017/conference/paper165/areachairs"], "cdate": 1485287704210}}}, {"tddate": null, "tmdate": 1484074386658, "tcdate": 1484074386658, "number": 3, "id": "S1jqpoGLe", "invitation": "ICLR.cc/2017/conference/-/paper165/official/comment", "forum": "B1vRTeqxg", "replyto": "H1p9EJarl", "signatures": ["ICLR.cc/2017/conference/paper165/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper165/AnonReviewer2"], "content": {"title": "score_k", "comment": "I don't quite follow why you cannot compute a traditional ROC. Why is your problem unsupervised? AFAICT you do have the labels/equivalence classes during test time, so you can generate all pairs for expressions and you know if they belong to the same class or not.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Continuous Semantic Representations of Symbolic Expressions", "abstract": "The question of how procedural knowledge is represented and inferred is a fundamental problem in machine learning and artificial intelligence. Recent work on program induction has proposed neural architectures, based on abstractions like stacks, Turing machines, and interpreters,  that operate on abstract computational machines or on execution traces. But the recursive abstraction that is central to procedural knowledge is perhaps most naturally represented by symbolic representations that have syntactic structure, such as logical expressions and source code. Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of mathematical and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation  on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.\n", "pdf": "/pdf/8cfedc863eec47c1c047f231b88712e77bd698ab.pdf", "TL;DR": "Assign continuous vectors to logical and algebraic symbolic expressions in such a way that semantically equivalent, but syntactically diverse expressions are assigned to identical (or highly similar) continuous vectors.", "paperhash": "allamanis|learning_continuous_semantic_representations_of_symbolic_expressions", "keywords": ["Deep learning"], "conflicts": ["ed.ac.uk", "microsoft.com"], "authors": ["Miltiadis Allamanis", "Pankajan Chanthirasegaran", "Pushmeet Kohli", "Charles Sutton"], "authorids": ["m.allamanis@ed.ac.uk", "pankajan.chanthirasegaran@ed.ac.uk", "pkohli@microsoft.com", "csutton@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287704075, "id": "ICLR.cc/2017/conference/-/paper165/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "B1vRTeqxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper165/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper165/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper165/reviewers", "ICLR.cc/2017/conference/paper165/areachairs"], "cdate": 1485287704075}}}, {"tddate": null, "tmdate": 1483695585809, "tcdate": 1483695570974, "number": 5, "id": "r1o0SkaSg", "invitation": "ICLR.cc/2017/conference/-/paper165/public/comment", "forum": "B1vRTeqxg", "replyto": "BkPLT2WVx", "signatures": ["~Miltiadis_Allamanis1"], "readers": ["everyone"], "writers": ["~Miltiadis_Allamanis1"], "content": {"title": "Response", "comment": "\n\n>> \"I do not believe that you can really do better than the truth table for boolean expr\"\n\nHumans deciding on the equivalence of two expressions do not always need to build a truth table. Some times it is very easy to apply a number of standard transformations (for instance, De Morgan's laws in the case of boolean expressions) to convert one expressions to another. For instance, two very long expressions might become equivalent by just one application of a rule - in this case no truth table computation would be required. The neural networks might be learning to compute such transformations implicitly. \n\nWe agree that this is a simple setting. We chose it because we felt that any representation learning method for that attempts to approximately learn expression semantics should be able to handle it. As the reviewer notes, the method could learn to compute the truth table of symbolic expressions. Therefore we found it very striking that no standard architecture could learn to do this! The idea behind this paper is that finding a representation learning method that *can* handle this simple setting is a necessary first step to a continuous representation learning method for program semantics.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Continuous Semantic Representations of Symbolic Expressions", "abstract": "The question of how procedural knowledge is represented and inferred is a fundamental problem in machine learning and artificial intelligence. Recent work on program induction has proposed neural architectures, based on abstractions like stacks, Turing machines, and interpreters,  that operate on abstract computational machines or on execution traces. But the recursive abstraction that is central to procedural knowledge is perhaps most naturally represented by symbolic representations that have syntactic structure, such as logical expressions and source code. Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of mathematical and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation  on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.\n", "pdf": "/pdf/8cfedc863eec47c1c047f231b88712e77bd698ab.pdf", "TL;DR": "Assign continuous vectors to logical and algebraic symbolic expressions in such a way that semantically equivalent, but syntactically diverse expressions are assigned to identical (or highly similar) continuous vectors.", "paperhash": "allamanis|learning_continuous_semantic_representations_of_symbolic_expressions", "keywords": ["Deep learning"], "conflicts": ["ed.ac.uk", "microsoft.com"], "authors": ["Miltiadis Allamanis", "Pankajan Chanthirasegaran", "Pushmeet Kohli", "Charles Sutton"], "authorids": ["m.allamanis@ed.ac.uk", "pankajan.chanthirasegaran@ed.ac.uk", "pkohli@microsoft.com", "csutton@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287704210, "id": "ICLR.cc/2017/conference/-/paper165/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1vRTeqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper165/reviewers", "ICLR.cc/2017/conference/paper165/areachairs"], "cdate": 1485287704210}}}, {"tddate": null, "tmdate": 1483695252788, "tcdate": 1483695252788, "number": 4, "id": "H1p9EJarl", "invitation": "ICLR.cc/2017/conference/-/paper165/public/comment", "forum": "B1vRTeqxg", "replyto": "SyiqkXfNx", "signatures": ["~Miltiadis_Allamanis1"], "readers": ["everyone"], "writers": ["~Miltiadis_Allamanis1"], "content": {"title": "Response", "comment": "Thank you for your comments. We will improve the motivation of subexpforce. We do compare between the model with and without the subexpforce loss in the text (page 7): \"When training the network with and without subexpression forcing, on average, the area under the curve (AUC) of the score k decreases by 16.8% on the SeenEqClass and 19.7% on the UnseenEqClass. This difference is smaller in the transfer setting of Figure 2b-i and Figure 2b-ii, where AUC decreases by 8.8% on average.\" Let us know if you'd be interested in seeing a lengthier comparison.\n\nRegarding our metric, \"score_k\" can be interpreted as recall at rank k. Since at test-time our problem has more of an unsupervised flavor, because unseen equivalence classes will be observed, it is hard to compute a ROC or a precision/recall metric. Can you clarify how would you suggest computing the precision/recall-ROC in this setting?\n\nWe understand your concern about imbalance of the equivalence classes. If we were to report both the macro-averaged score_k (first average per equivalence class and then average all equivalence classes) and the micro-averaged score_k (average the score_k of all points assigning equal weights to each expression), would that be sufficient to resolve your concern?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Continuous Semantic Representations of Symbolic Expressions", "abstract": "The question of how procedural knowledge is represented and inferred is a fundamental problem in machine learning and artificial intelligence. Recent work on program induction has proposed neural architectures, based on abstractions like stacks, Turing machines, and interpreters,  that operate on abstract computational machines or on execution traces. But the recursive abstraction that is central to procedural knowledge is perhaps most naturally represented by symbolic representations that have syntactic structure, such as logical expressions and source code. Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of mathematical and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation  on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.\n", "pdf": "/pdf/8cfedc863eec47c1c047f231b88712e77bd698ab.pdf", "TL;DR": "Assign continuous vectors to logical and algebraic symbolic expressions in such a way that semantically equivalent, but syntactically diverse expressions are assigned to identical (or highly similar) continuous vectors.", "paperhash": "allamanis|learning_continuous_semantic_representations_of_symbolic_expressions", "keywords": ["Deep learning"], "conflicts": ["ed.ac.uk", "microsoft.com"], "authors": ["Miltiadis Allamanis", "Pankajan Chanthirasegaran", "Pushmeet Kohli", "Charles Sutton"], "authorids": ["m.allamanis@ed.ac.uk", "pankajan.chanthirasegaran@ed.ac.uk", "pkohli@microsoft.com", "csutton@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287704210, "id": "ICLR.cc/2017/conference/-/paper165/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1vRTeqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper165/reviewers", "ICLR.cc/2017/conference/paper165/areachairs"], "cdate": 1485287704210}}}, {"tddate": null, "tmdate": 1483695084261, "tcdate": 1483695084261, "number": 3, "id": "BJ4l4kpre", "invitation": "ICLR.cc/2017/conference/-/paper165/public/comment", "forum": "B1vRTeqxg", "replyto": "Hkdc5tZVg", "signatures": ["~Miltiadis_Allamanis1"], "readers": ["everyone"], "writers": ["~Miltiadis_Allamanis1"], "content": {"title": "Response", "comment": "Thanks for pointing the typo in Table 3 and the issue with Fig4. We'll address them.\n\nWe understand that the setting where the equivalence classes of two expressions are already known is limited. The reason that we focused on such a setting is that we found that it was already quite hard, and seemed to be a necessary first step to more realistic settings such as expression similarity in real compute programs. Although in this work we are learning semantic vector representations using equivalence, there are multiple ways of retrieving (training) expressions that are approximately equivalent (e.g. identical outputs on a set of identical inputs). However, even in those cases, current neural network architectures would still not be able to capture semantics. Hence, in this work we make the first step and search for architectures that capture semantics in a setting where equivalence is known. And although we show that EqNets perform the best, it suggests that we need future research to first solve this problem, before moving to noisy data.\n\nFor arbitrary expressions (e.g. code) the problem of determining equivalence is indeed undecidable, but for boolean and polynomial expressions it is not. To detect equivalence of boolean expressions we convert them to the canonical conjunctive normal form which is unique. We get the same behavior for polynomials where we can symbolically expand them and simplify them into the unique form v_0*a + v_1*b + v_2*c + v_3*a^2 + ...\n\nOur network is only \"residual-like\" (and does _not_ use the standard residual architecture) because the size of the input layer is different from the size of the output layer (Fig. 1b). For example, for a node with two children, the input would have size 2*D, while the output is of size D. Therefore, using the identity is not exactly possible. There are many possible design choices to solve this, so our choice is just one sensible option. Finally, although the multiplication with W_{o0,\\tau_n} contributes to diminishing gradients, we retain the important element of residual networks by _not_ using a non-linearity.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Continuous Semantic Representations of Symbolic Expressions", "abstract": "The question of how procedural knowledge is represented and inferred is a fundamental problem in machine learning and artificial intelligence. Recent work on program induction has proposed neural architectures, based on abstractions like stacks, Turing machines, and interpreters,  that operate on abstract computational machines or on execution traces. But the recursive abstraction that is central to procedural knowledge is perhaps most naturally represented by symbolic representations that have syntactic structure, such as logical expressions and source code. Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of mathematical and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation  on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.\n", "pdf": "/pdf/8cfedc863eec47c1c047f231b88712e77bd698ab.pdf", "TL;DR": "Assign continuous vectors to logical and algebraic symbolic expressions in such a way that semantically equivalent, but syntactically diverse expressions are assigned to identical (or highly similar) continuous vectors.", "paperhash": "allamanis|learning_continuous_semantic_representations_of_symbolic_expressions", "keywords": ["Deep learning"], "conflicts": ["ed.ac.uk", "microsoft.com"], "authors": ["Miltiadis Allamanis", "Pankajan Chanthirasegaran", "Pushmeet Kohli", "Charles Sutton"], "authorids": ["m.allamanis@ed.ac.uk", "pankajan.chanthirasegaran@ed.ac.uk", "pkohli@microsoft.com", "csutton@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287704210, "id": "ICLR.cc/2017/conference/-/paper165/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1vRTeqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper165/reviewers", "ICLR.cc/2017/conference/paper165/areachairs"], "cdate": 1485287704210}}}, {"tddate": null, "tmdate": 1481939253394, "tcdate": 1481939253394, "number": 2, "id": "Syp4YGfVg", "invitation": "ICLR.cc/2017/conference/-/paper165/official/comment", "forum": "B1vRTeqxg", "replyto": "r1ouWVW4l", "signatures": ["ICLR.cc/2017/conference/paper165/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper165/AnonReviewer2"], "content": {"title": "noise vector", "comment": "Thanks for the clarification.\nAdd the end of 2.1 you mention that n is a binary noise vector. Does it operate on a whole r_X or on some dimensions of it. If the latter, isn't it very unlikely that r_{e_1} and r_{e_2} are completely eliminated from the \"input\" representation as you suggest above?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Continuous Semantic Representations of Symbolic Expressions", "abstract": "The question of how procedural knowledge is represented and inferred is a fundamental problem in machine learning and artificial intelligence. Recent work on program induction has proposed neural architectures, based on abstractions like stacks, Turing machines, and interpreters,  that operate on abstract computational machines or on execution traces. But the recursive abstraction that is central to procedural knowledge is perhaps most naturally represented by symbolic representations that have syntactic structure, such as logical expressions and source code. Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of mathematical and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation  on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.\n", "pdf": "/pdf/8cfedc863eec47c1c047f231b88712e77bd698ab.pdf", "TL;DR": "Assign continuous vectors to logical and algebraic symbolic expressions in such a way that semantically equivalent, but syntactically diverse expressions are assigned to identical (or highly similar) continuous vectors.", "paperhash": "allamanis|learning_continuous_semantic_representations_of_symbolic_expressions", "keywords": ["Deep learning"], "conflicts": ["ed.ac.uk", "microsoft.com"], "authors": ["Miltiadis Allamanis", "Pankajan Chanthirasegaran", "Pushmeet Kohli", "Charles Sutton"], "authorids": ["m.allamanis@ed.ac.uk", "pankajan.chanthirasegaran@ed.ac.uk", "pkohli@microsoft.com", "csutton@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287704075, "id": "ICLR.cc/2017/conference/-/paper165/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "B1vRTeqxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper165/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper165/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper165/reviewers", "ICLR.cc/2017/conference/paper165/areachairs"], "cdate": 1485287704075}}}, {"tddate": null, "tmdate": 1481915727541, "tcdate": 1481915727541, "number": 2, "id": "BkPLT2WVx", "invitation": "ICLR.cc/2017/conference/-/paper165/official/review", "forum": "B1vRTeqxg", "replyto": "B1vRTeqxg", "signatures": ["ICLR.cc/2017/conference/paper165/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper165/AnonReviewer1"], "content": {"title": "borderline: not convinced by the setting.", "rating": "5: Marginally below acceptance threshold", "review": "The goal of this paper is to learn vector representation of boolean and polynomial expressions, such that equivalent expressions have similar representations.\n\nThe model proposed in the paper is based on recursive neural network, as introduced by Socher et al. (2012). Given the syntactic parse tree of a formula (either boolean or polynomial), the representation for a node is obtained by applying a MLP on the representation of the children. This process is applied recursively to obtain the representation of the full expression. Contrary to Socher et al. (2012), this paper proposes to use more than one layer (this is especially important to capture XOR operation, which is not surprising at all). The paper also introduces a reconstruction error (called SubexpForce), so that the expression of children can be recovered from the expression of the parent (if I understood correctly). The model is trained using a classification loss, where the label of a given expression corresponds to its equivalence class. The method is then evaluated on randomly generated data, and compared to baselines such as tf-idf, GRU RNN or standard recursive neural network.\n\nWhile I do agree with the authors that learning good representation for symbolic expressions (and to capture compositionality) is an important task, I am not entirely convinced by the experimental setting proposed in this paper. Indeed, as stated in the paper, the task of deciding if two boolean expressions are equivalent is NP-hard, and I do not understand if the model can do better than implicitly computing the truth table of expressions. While sometimes a bit hard to follow, the paper is technically sound. In particular, the proposed model is well adapted to the problem and outperforms the baselines.\n\npros:\n - the model is relatively simple and sound.\n - using a classification loss over equivalence classes (should be compared with using similarity).\n\ncons:\n - not convinced by the setting: I do not believe that you can really do better than the truth table for boolean expr (or computing the value of the polynomial expression for randomly chosen points in [0, 1]^n).\n - some part of the paper are a bit hard to follow (e.g. justification of the SubexpForce, discussion of why softmax does not work, etc...).\n - comparison between classification loss and similarity loss is missing.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Continuous Semantic Representations of Symbolic Expressions", "abstract": "The question of how procedural knowledge is represented and inferred is a fundamental problem in machine learning and artificial intelligence. Recent work on program induction has proposed neural architectures, based on abstractions like stacks, Turing machines, and interpreters,  that operate on abstract computational machines or on execution traces. But the recursive abstraction that is central to procedural knowledge is perhaps most naturally represented by symbolic representations that have syntactic structure, such as logical expressions and source code. Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of mathematical and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation  on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.\n", "pdf": "/pdf/8cfedc863eec47c1c047f231b88712e77bd698ab.pdf", "TL;DR": "Assign continuous vectors to logical and algebraic symbolic expressions in such a way that semantically equivalent, but syntactically diverse expressions are assigned to identical (or highly similar) continuous vectors.", "paperhash": "allamanis|learning_continuous_semantic_representations_of_symbolic_expressions", "keywords": ["Deep learning"], "conflicts": ["ed.ac.uk", "microsoft.com"], "authors": ["Miltiadis Allamanis", "Pankajan Chanthirasegaran", "Pushmeet Kohli", "Charles Sutton"], "authorids": ["m.allamanis@ed.ac.uk", "pankajan.chanthirasegaran@ed.ac.uk", "pkohli@microsoft.com", "csutton@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512677047, "id": "ICLR.cc/2017/conference/-/paper165/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper165/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper165/AnonReviewer3", "ICLR.cc/2017/conference/paper165/AnonReviewer1", "ICLR.cc/2017/conference/paper165/AnonReviewer2"], "reply": {"forum": "B1vRTeqxg", "replyto": "B1vRTeqxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper165/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper165/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512677047}}}, {"tddate": null, "tmdate": 1481902736099, "tcdate": 1481902736099, "number": 1, "id": "Hkdc5tZVg", "invitation": "ICLR.cc/2017/conference/-/paper165/official/review", "forum": "B1vRTeqxg", "replyto": "B1vRTeqxg", "signatures": ["ICLR.cc/2017/conference/paper165/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper165/AnonReviewer3"], "content": {"title": "Intuitive and effective model for predicting semantic equivalence, but what the practical use of this approach is, is unclear.", "rating": "6: Marginally above acceptance threshold", "review": "\nThis work proposes to compute embeddings of symbolic expressions (e.g., boolean expressions, or polynomials) such that semantically equivalent expressions are near each other in the embedded space.  The proposed model uses recursive neural networks where the architecture is made to match that of the parse tree of a given symbolic expression.  To train the model parameters, the authors create a dataset of expressions where semantic equivalence relationships are known and minimize a loss function so that equivalent expressions are closer to each other than non-equivalent expressions via a max-margin loss function.  The authors also use a \u201csubexpression forcing\u201d mechanism which, if I understand it correctly, encourages the embeddings to respect some kind of compositionality.\n\nResults are shown on a few symbolic expression datasets created by the authors and the proposed method is demonstrated to outperform baselines pretty convincingly.  I especially like the PCA visualization where the action of negating an expression is shown to correspond roughly to negating the embedding in its vector space \u2014 it is a lot like the man - woman + queen = king type embeddings that we see in the word2vec and glove style papers.  \n\nThe weakest part of the paper is probably that the setting seems somewhat contrived \u2014 I can\u2019t really think of a real setting where it is easy to have a training set of known semantic equivalences, but still more worth it to use a neural network to do predictions.   The authors have also punted on dealing with variable names, assuming that distinct variables refer to different entities in the domain.  This is understandable, as variable names add a whole new layer of complexity on an already difficult problem, but also seems high limiting.  For example, the proposed methods would not be useable in an \u201cequation search engine\u201d unless we were able to accurately canonicalize variable names in some way.\n\nOther miscellaneous points:\n* Regarding problem hardness, I believe that the problem of determining if two expressions are equivalent is actually undecidable \u2014 see the \u201cword problem for Thue systems\u201d.  Related to this, I was not able to figure out how the authors determine ground truth equivalence in their training sets.  They say that expressions are simplified into a canonical form and grouped, but this seems to not be possible in general, so one question is \u2014 is it possible that equivalent expressions in the training data would have been mapped to different canonical forms?  Would it have been easier/possible to construct and compare truth tables?\n* The \u201cCOMBINE\u201d operation uses what the authors describe as a residual-like connection.  Looking at the equations, the reason why this is not actually a residual connection is because of the weight matrix that is multiplied by the lower level l_0 features.  A true residual connection would have passed the features through unchanged (identity connection) and would have also been better at fighting gradient explosion\u2026. so is there a reason why this was used rather than an identity connection?\n* In table 3, the first tf-idf entry: a + (c+a) * c seems equivalent to a + (c * (a+c))\n* Vertical spacing between Figure 4 caption and body of text is very small and looks like the caption continues into the body of the text.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Continuous Semantic Representations of Symbolic Expressions", "abstract": "The question of how procedural knowledge is represented and inferred is a fundamental problem in machine learning and artificial intelligence. Recent work on program induction has proposed neural architectures, based on abstractions like stacks, Turing machines, and interpreters,  that operate on abstract computational machines or on execution traces. But the recursive abstraction that is central to procedural knowledge is perhaps most naturally represented by symbolic representations that have syntactic structure, such as logical expressions and source code. Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of mathematical and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation  on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.\n", "pdf": "/pdf/8cfedc863eec47c1c047f231b88712e77bd698ab.pdf", "TL;DR": "Assign continuous vectors to logical and algebraic symbolic expressions in such a way that semantically equivalent, but syntactically diverse expressions are assigned to identical (or highly similar) continuous vectors.", "paperhash": "allamanis|learning_continuous_semantic_representations_of_symbolic_expressions", "keywords": ["Deep learning"], "conflicts": ["ed.ac.uk", "microsoft.com"], "authors": ["Miltiadis Allamanis", "Pankajan Chanthirasegaran", "Pushmeet Kohli", "Charles Sutton"], "authorids": ["m.allamanis@ed.ac.uk", "pankajan.chanthirasegaran@ed.ac.uk", "pkohli@microsoft.com", "csutton@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512677047, "id": "ICLR.cc/2017/conference/-/paper165/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper165/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper165/AnonReviewer3", "ICLR.cc/2017/conference/paper165/AnonReviewer1", "ICLR.cc/2017/conference/paper165/AnonReviewer2"], "reply": {"forum": "B1vRTeqxg", "replyto": "B1vRTeqxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper165/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper165/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512677047}}}, {"tddate": null, "tmdate": 1481880017455, "tcdate": 1481880017455, "number": 2, "id": "HJYCbEZEx", "invitation": "ICLR.cc/2017/conference/-/paper165/public/comment", "forum": "B1vRTeqxg", "replyto": "rkzN3OeQg", "signatures": ["~Miltiadis_Allamanis1"], "readers": ["everyone"], "writers": ["~Miltiadis_Allamanis1"], "content": {"title": "Objective and Inductive biases", "comment": "It is true that q_ei is unnormalized and with large norms the softmax can achieve high probabilities. In practice, we found that this leads to q_ei's that have norms ~10^6, which we believe that it causes overfitting explaining why the performance is a bit lower when we use the maximum likelihood objective, although the difference in performance was not huge.\n\nOur supervised objective is very similar to the classic classification objective on the seen equivalence classes, with the addition of having a margin.\n\nWe are not trying to capture inductive biases from data. Perhaps it would be more clear to say this. In large problems, especially those that are significantly larger from those we considered in the paper, it will be impossible to be exhaustive. So, the difference between learning-based methods for semantics and an exact (symbolic) solver\nfor the NP-hard SAT problem is that the learning-based method will be approximate in the sense that it will be more accurate for symbolic expressions that are more similar to those in the training data. Having said that, even for the small problems we considered in this paper, we found that it was very difficult to get good performance using standard neural architectures. This is why we had to implement new techniques that were better suited for semantic properties, i.e. that represent semantics compositionality, while still allowing high-curvature operations in the continuous semantic representation space (the resnet trick) and encouraging semantic representations to be approximately reversible (see previous answer on subexpforcing).\n\nIn principle, if our models were perfect, there would be a 1:1 correspondence between semantic locations and truth tables. In some cases, we can identify low-dimensional projections that correspond to single elements in the truth table (Fig 4a). However, note that we are able to generalize from training on small expressions to testing on deeper expressions (Figure 2b). So if there is a simple mapping between semantic representations and truth tables, then there is some evidence that we are learning a procedure for doing this, rather than a map that is specific to the examples in the training set.\n\nWe will further clarify these in the text."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Continuous Semantic Representations of Symbolic Expressions", "abstract": "The question of how procedural knowledge is represented and inferred is a fundamental problem in machine learning and artificial intelligence. Recent work on program induction has proposed neural architectures, based on abstractions like stacks, Turing machines, and interpreters,  that operate on abstract computational machines or on execution traces. But the recursive abstraction that is central to procedural knowledge is perhaps most naturally represented by symbolic representations that have syntactic structure, such as logical expressions and source code. Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of mathematical and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation  on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.\n", "pdf": "/pdf/8cfedc863eec47c1c047f231b88712e77bd698ab.pdf", "TL;DR": "Assign continuous vectors to logical and algebraic symbolic expressions in such a way that semantically equivalent, but syntactically diverse expressions are assigned to identical (or highly similar) continuous vectors.", "paperhash": "allamanis|learning_continuous_semantic_representations_of_symbolic_expressions", "keywords": ["Deep learning"], "conflicts": ["ed.ac.uk", "microsoft.com"], "authors": ["Miltiadis Allamanis", "Pankajan Chanthirasegaran", "Pushmeet Kohli", "Charles Sutton"], "authorids": ["m.allamanis@ed.ac.uk", "pankajan.chanthirasegaran@ed.ac.uk", "pkohli@microsoft.com", "csutton@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287704210, "id": "ICLR.cc/2017/conference/-/paper165/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1vRTeqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper165/reviewers", "ICLR.cc/2017/conference/paper165/areachairs"], "cdate": 1485287704210}}}, {"tddate": null, "tmdate": 1481879922947, "tcdate": 1481879922947, "number": 1, "id": "r1ouWVW4l", "invitation": "ICLR.cc/2017/conference/-/paper165/public/comment", "forum": "B1vRTeqxg", "replyto": "HyvImh17l", "signatures": ["~Miltiadis_Allamanis1"], "readers": ["everyone"], "writers": ["~Miltiadis_Allamanis1"], "content": {"title": "SubExpForce", "comment": "Essentially, you may think of subexpforcing as a constraint that tries to enforce reversibility, when possible, encouraging the semantic representation of the child nodes to be predictable from the parent. This helps to unify the representations of semantically identical but syntactically different representations. The added effect of the subexpforce with the denoising autoencoder is that it forces the expression representation to lie on a low-rank manifold inducing a clustering-like behavior of the triplets (r_{left} op r_{right})=r_{out}.\n\nWe will try to illustrate this with an example. Imagine three expressions e_1=\"a\", e_2=\"a \\land (b \\lor \\lnot b)\" and e_3=\"c\". Notice that \"e_1 \\land e_3\" is identical to \"e_2 \\land e_3\" and assume that both expressions have the exact same SemVec r_{out} but the SemVecs of e_1 and e_2 are far away even though e_1 and e_2 belong to the same equivalence\nclass. We observed this situation in practice with the simple TreeNNs. With subexpforce, we ask the optimization procedure to push r_{e_1} and r_{e_2} close to each other. This is achieved by asking the model to predict the representation of X (X \\land e_3=e_{out}) given the representation of e_3 (r_3) and e_out (r_{out}), ie. to make this reversible.\nThe network then predicts a single representation for X, r_X and pulls r_1 and r_2 towards the same location r_X.\n\nWe will make the subexpforce explanation more clear in the paper too."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Continuous Semantic Representations of Symbolic Expressions", "abstract": "The question of how procedural knowledge is represented and inferred is a fundamental problem in machine learning and artificial intelligence. Recent work on program induction has proposed neural architectures, based on abstractions like stacks, Turing machines, and interpreters,  that operate on abstract computational machines or on execution traces. But the recursive abstraction that is central to procedural knowledge is perhaps most naturally represented by symbolic representations that have syntactic structure, such as logical expressions and source code. Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of mathematical and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation  on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.\n", "pdf": "/pdf/8cfedc863eec47c1c047f231b88712e77bd698ab.pdf", "TL;DR": "Assign continuous vectors to logical and algebraic symbolic expressions in such a way that semantically equivalent, but syntactically diverse expressions are assigned to identical (or highly similar) continuous vectors.", "paperhash": "allamanis|learning_continuous_semantic_representations_of_symbolic_expressions", "keywords": ["Deep learning"], "conflicts": ["ed.ac.uk", "microsoft.com"], "authors": ["Miltiadis Allamanis", "Pankajan Chanthirasegaran", "Pushmeet Kohli", "Charles Sutton"], "authorids": ["m.allamanis@ed.ac.uk", "pankajan.chanthirasegaran@ed.ac.uk", "pkohli@microsoft.com", "csutton@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287704210, "id": "ICLR.cc/2017/conference/-/paper165/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1vRTeqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper165/reviewers", "ICLR.cc/2017/conference/paper165/areachairs"], "cdate": 1485287704210}}}, {"tddate": null, "tmdate": 1480784937833, "tcdate": 1480784937827, "number": 2, "id": "rkzN3OeQg", "invitation": "ICLR.cc/2017/conference/-/paper165/pre-review/question", "forum": "B1vRTeqxg", "replyto": "B1vRTeqxg", "signatures": ["ICLR.cc/2017/conference/paper165/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper165/AnonReviewer1"], "content": {"title": "Inductive biases", "question": "I am not sure to get why maximizing the likelihood of the data (Eq. 1) would lead to bad performance. The output of TreeNN is normalized, but not q_ei, is that correct? So couldn't high probabilities be achieved with large q_ei?\n\nI also do not get why the supervised objective is different from classification at train time (I understand that at test time, unseen equivalence classes are considered).\n\nFinally, in the introduction, the authors state that the goal of the paper is to develop models that will capture \"inductive biases\" from the data. What kind of biases are considered? It seems to me that the generated data lead to hard problems: it would probably be hard for a human to solve the boolean tasks, without computing the truth table of the expressions. Is this what the model is implicitly doing?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Continuous Semantic Representations of Symbolic Expressions", "abstract": "The question of how procedural knowledge is represented and inferred is a fundamental problem in machine learning and artificial intelligence. Recent work on program induction has proposed neural architectures, based on abstractions like stacks, Turing machines, and interpreters,  that operate on abstract computational machines or on execution traces. But the recursive abstraction that is central to procedural knowledge is perhaps most naturally represented by symbolic representations that have syntactic structure, such as logical expressions and source code. Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of mathematical and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation  on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.\n", "pdf": "/pdf/8cfedc863eec47c1c047f231b88712e77bd698ab.pdf", "TL;DR": "Assign continuous vectors to logical and algebraic symbolic expressions in such a way that semantically equivalent, but syntactically diverse expressions are assigned to identical (or highly similar) continuous vectors.", "paperhash": "allamanis|learning_continuous_semantic_representations_of_symbolic_expressions", "keywords": ["Deep learning"], "conflicts": ["ed.ac.uk", "microsoft.com"], "authors": ["Miltiadis Allamanis", "Pankajan Chanthirasegaran", "Pushmeet Kohli", "Charles Sutton"], "authorids": ["m.allamanis@ed.ac.uk", "pankajan.chanthirasegaran@ed.ac.uk", "pkohli@microsoft.com", "csutton@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959429331, "id": "ICLR.cc/2017/conference/-/paper165/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper165/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper165/AnonReviewer2", "ICLR.cc/2017/conference/paper165/AnonReviewer1"], "reply": {"forum": "B1vRTeqxg", "replyto": "B1vRTeqxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper165/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper165/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959429331}}}, {"tddate": null, "tmdate": 1480733518588, "tcdate": 1480733518584, "number": 1, "id": "HyvImh17l", "invitation": "ICLR.cc/2017/conference/-/paper165/pre-review/question", "forum": "B1vRTeqxg", "replyto": "B1vRTeqxg", "signatures": ["ICLR.cc/2017/conference/paper165/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper165/AnonReviewer2"], "content": {"title": "Clarification of SubExpForce", "question": "Hi,\ncould you please motivate the intuition behind subexpforce better. I am having trouble following the end of section 2.1."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Continuous Semantic Representations of Symbolic Expressions", "abstract": "The question of how procedural knowledge is represented and inferred is a fundamental problem in machine learning and artificial intelligence. Recent work on program induction has proposed neural architectures, based on abstractions like stacks, Turing machines, and interpreters,  that operate on abstract computational machines or on execution traces. But the recursive abstraction that is central to procedural knowledge is perhaps most naturally represented by symbolic representations that have syntactic structure, such as logical expressions and source code. Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of mathematical and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation  on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.\n", "pdf": "/pdf/8cfedc863eec47c1c047f231b88712e77bd698ab.pdf", "TL;DR": "Assign continuous vectors to logical and algebraic symbolic expressions in such a way that semantically equivalent, but syntactically diverse expressions are assigned to identical (or highly similar) continuous vectors.", "paperhash": "allamanis|learning_continuous_semantic_representations_of_symbolic_expressions", "keywords": ["Deep learning"], "conflicts": ["ed.ac.uk", "microsoft.com"], "authors": ["Miltiadis Allamanis", "Pankajan Chanthirasegaran", "Pushmeet Kohli", "Charles Sutton"], "authorids": ["m.allamanis@ed.ac.uk", "pankajan.chanthirasegaran@ed.ac.uk", "pkohli@microsoft.com", "csutton@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959429331, "id": "ICLR.cc/2017/conference/-/paper165/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper165/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper165/AnonReviewer2", "ICLR.cc/2017/conference/paper165/AnonReviewer1"], "reply": {"forum": "B1vRTeqxg", "replyto": "B1vRTeqxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper165/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper165/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959429331}}}], "count": 16}