{"notes": [{"id": "rkeO-lrYwr", "original": "S1xIcxlYvS", "number": 2141, "cdate": 1569439743817, "ddate": null, "tcdate": 1569439743817, "tmdate": 1577168254505, "tddate": null, "forum": "rkeO-lrYwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["jfrankle@csail.mit.edu", "karolina.dziugaite@gmail.com", "droy@utstat.toronto.edu", "mcarbin@csail.mit.edu"], "title": "Mode Connectivity and Sparse Neural Networks", "authors": ["Jonathan Frankle", "Gintare Karolina Dziugaite", "Daniel M. Roy", "Michael Carbin"], "pdf": "/pdf/415db8f7d8081842f3f179b64310700bc7e8d2cf.pdf", "TL;DR": "Whether or not a sparse subnetwork trains to the same accuracy of the full network depends on whether two runs of SGD on the subnetwork land in the same convex levelset.", "abstract": "We uncover a connection between two seemingly unrelated empirical phenomena: mode connectivity and sparsity. On the one hand, there is growing catalog of situations where, across multiple runs, SGD learns weights that fall into minima that are connected (mode connectivity). A striking example is described by Nagarajan & Kolter (2019). They observe that test error on MNIST does not change along the linear path connecting the end points of two independent SGD runs, starting from the same random initialization. On the other hand, there is the lottery ticket hypothesis of Frankle & Carbin (2019), where dense, randomly initialized networks have sparse subnetworks capable of training in isolation to full accuracy.\n\nHowever, neither phenomenon scales beyond small vision networks. We start by proposing a technique to find sparse subnetworks after initialization. We observe that these subnetworks match the accuracy of the full network only when two SGD runs for the same subnetwork are connected by linear paths with the no change in test error. Our findings connect the existence of sparse subnetworks that train to high accuracy with the dynamics of optimization via mode connectivity. In doing so, we identify analogues of the phenomena uncovered by Nagarajan & Kolter and Frankle & Carbin in ImageNet-scale architectures at state-of-the-art sparsity levels.", "keywords": ["sparsity", "mode connectivity", "lottery ticket", "optimization landscape"], "paperhash": "frankle|mode_connectivity_and_sparse_neural_networks", "original_pdf": "/attachment/cdce07e15a495d5ee3031f3f177780e2dac4ccdd.pdf", "_bibtex": "@misc{\nfrankle2020mode,\ntitle={Mode Connectivity and Sparse Neural Networks},\nauthor={Jonathan Frankle and Gintare Karolina Dziugaite and Daniel M. Roy and Michael Carbin},\nyear={2020},\nurl={https://openreview.net/forum?id=rkeO-lrYwr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "iFpAc-yVKO", "original": null, "number": 1, "cdate": 1576798741547, "ddate": null, "tcdate": 1576798741547, "tmdate": 1576800894694, "tddate": null, "forum": "rkeO-lrYwr", "replyto": "rkeO-lrYwr", "invitation": "ICLR.cc/2020/Conference/Paper2141/-/Decision", "content": {"decision": "Reject", "comment": "This paper investigates theories related to networks sparsification, related to mode connectivity and the so-called lottery ticket hypothesis.  The paper is interesting and has merit, but on balance I find the contributions not sufficiently clear to warrant acceptance.  The authors made substantial changes to the paper which are admirable and which bring it to borderline status. \n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jfrankle@csail.mit.edu", "karolina.dziugaite@gmail.com", "droy@utstat.toronto.edu", "mcarbin@csail.mit.edu"], "title": "Mode Connectivity and Sparse Neural Networks", "authors": ["Jonathan Frankle", "Gintare Karolina Dziugaite", "Daniel M. Roy", "Michael Carbin"], "pdf": "/pdf/415db8f7d8081842f3f179b64310700bc7e8d2cf.pdf", "TL;DR": "Whether or not a sparse subnetwork trains to the same accuracy of the full network depends on whether two runs of SGD on the subnetwork land in the same convex levelset.", "abstract": "We uncover a connection between two seemingly unrelated empirical phenomena: mode connectivity and sparsity. On the one hand, there is growing catalog of situations where, across multiple runs, SGD learns weights that fall into minima that are connected (mode connectivity). A striking example is described by Nagarajan & Kolter (2019). They observe that test error on MNIST does not change along the linear path connecting the end points of two independent SGD runs, starting from the same random initialization. On the other hand, there is the lottery ticket hypothesis of Frankle & Carbin (2019), where dense, randomly initialized networks have sparse subnetworks capable of training in isolation to full accuracy.\n\nHowever, neither phenomenon scales beyond small vision networks. We start by proposing a technique to find sparse subnetworks after initialization. We observe that these subnetworks match the accuracy of the full network only when two SGD runs for the same subnetwork are connected by linear paths with the no change in test error. Our findings connect the existence of sparse subnetworks that train to high accuracy with the dynamics of optimization via mode connectivity. In doing so, we identify analogues of the phenomena uncovered by Nagarajan & Kolter and Frankle & Carbin in ImageNet-scale architectures at state-of-the-art sparsity levels.", "keywords": ["sparsity", "mode connectivity", "lottery ticket", "optimization landscape"], "paperhash": "frankle|mode_connectivity_and_sparse_neural_networks", "original_pdf": "/attachment/cdce07e15a495d5ee3031f3f177780e2dac4ccdd.pdf", "_bibtex": "@misc{\nfrankle2020mode,\ntitle={Mode Connectivity and Sparse Neural Networks},\nauthor={Jonathan Frankle and Gintare Karolina Dziugaite and Daniel M. Roy and Michael Carbin},\nyear={2020},\nurl={https://openreview.net/forum?id=rkeO-lrYwr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rkeO-lrYwr", "replyto": "rkeO-lrYwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795704974, "tmdate": 1576800252657, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2141/-/Decision"}}}, {"id": "B1gQ7ILjsB", "original": null, "number": 6, "cdate": 1573770779173, "ddate": null, "tcdate": 1573770779173, "tmdate": 1573770816903, "tddate": null, "forum": "rkeO-lrYwr", "replyto": "rJlhQU77jr", "invitation": "ICLR.cc/2020/Conference/Paper2141/-/Official_Comment", "content": {"title": "Author Response to Reviewer 4 (Part 1)", "comment": "\nNOTE: We have posted an updated version of the paper that has been substantially restructured and rewritten to address your concerns. We highly recommend looking over the new paper.\n\nWe have summarized these changes in a general response (posted as a top-level comment). We ask that you read our general response before returning to this point-by-point response. We address many of your concerns there.\n\n--------------------\n\n> 1) The scope of the experiment is limited to a quite specific setting, \n> The experiments only show [the relationship between mode connectivity and sparsity] is true in a limited setting, focusing on specific pruning method and at a specific sparsity level.\n> Stability was tested only at one specific sparsity level\n> The paper also focused on cases where matching subnetworks were found by IMP, but matching subnetworks can also be found by other pruning methods. \n\nWe choose to focus specifically on IMP and the most extreme sparsities for which IMP can find a matching subnetworks for any rewinding iteration.\n\nWhy IMP? IMP produces particularly sparse matching subnetworks and is the algorithm behind current lottery ticket results, so we are interested in studying the networks that it produces for both scientific understanding of the lottery ticket hypothesis and potential practical lessons for training extremely sparse networks to high accuracy.\n\nWhy extreme sparsities? In general, sparse neural networks are more difficult to train from scratch. At extreme levels of sparsity, many classes of sparse networks (e.g., those produced by randomly reinitializing pruned networks and randomly pruning) train to lower accuracy than the full network [HPT+15, FC19, LSZ+19]. However, if it were possible to train sparse networks from scratch to the same accuracy as the full network, then it would present a new opportunity to improve the efficiency of neural network training. We are therefore interested in understanding the properties of special classes of sparse networks that are indeed matching (e.g., winning lottery tickets produced by IMP). Studying extremely sparse matching subnetworks from IMP provides the best contrast with (1) the full, overparametrized neural networks and (2) other classes of sparse networks that are not matching at these sparsities.\n\nAlthough we are interested in understanding this behavior at all levels of sparsity, computational limitations force us to focus on a single level of sparsity. IMP entails training a network at least a dozen times to reach high levels of sparsity, and instability analysis requires training each of these networks on three different data orders for three kinds of sparsity. For rigor, we replicate each of these experiments three times with different initializations.\n\n> 2) there are unsupported strong claims which need to be clarified.\n> In the abstract the paper claims that sparse subnetworks are matching subnetworks only when they are stable, but the results are shown in a limited setting only at a very high sparsity. \n\nAs noted in the top-level comment, we have revised our claims about sparse networks to focus specifically on IMP at the most extreme sparsities for which matching subnetworks are known to exist. As we argue, IMP subnetworks at these sparsities are particularly valuable for scientific study.\n\n> They tested stability on the highest sparsity level at which there was evidence that matching subnetworks existed, but how would the result generalize to other sparsity levels? With lower sparsity level (if weights are pruned less), is stability easier to achieve?\n>  it is not obvious it would be stable at all lower sparsity levels where IMP found matching subnetworks.\n\nIn short, we would not necessarily expect the results to generalize to lower sparsity levels. This is not a weakness. but just a matter of fact. As we explain in the top-level comment, the full networks are generally unstable at initialization but become stable later in training. However, they reach full accuracy regardless of whether they are stable. This means that stability and accuracy do not appear to be linked for the full network. We expect that particularly moderate sparsity levels will resemble the full network case, while higher sparsity levels will resemble the experiments in the paper.\n\n> I think the paper needs to show how the same relationship might generalize to different sparsity levels, or alternatively modify the claim (to what it actually shows)\n> Some of these stronger claims can be modified to describe what the experiments actually show.\n> The relationship found between stability and matching subnetworks in the high sparsity regime is a valuable insight that I believe should be conveyed correctly in this paper.\n\nAs we discuss in the top-level comment, we have narrowed the scope of our claim to only cover  the connection between stability and matching subnetworks found by IMP in this highly sparse regime."}, "signatures": ["ICLR.cc/2020/Conference/Paper2141/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2141/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jfrankle@csail.mit.edu", "karolina.dziugaite@gmail.com", "droy@utstat.toronto.edu", "mcarbin@csail.mit.edu"], "title": "Mode Connectivity and Sparse Neural Networks", "authors": ["Jonathan Frankle", "Gintare Karolina Dziugaite", "Daniel M. Roy", "Michael Carbin"], "pdf": "/pdf/415db8f7d8081842f3f179b64310700bc7e8d2cf.pdf", "TL;DR": "Whether or not a sparse subnetwork trains to the same accuracy of the full network depends on whether two runs of SGD on the subnetwork land in the same convex levelset.", "abstract": "We uncover a connection between two seemingly unrelated empirical phenomena: mode connectivity and sparsity. On the one hand, there is growing catalog of situations where, across multiple runs, SGD learns weights that fall into minima that are connected (mode connectivity). A striking example is described by Nagarajan & Kolter (2019). They observe that test error on MNIST does not change along the linear path connecting the end points of two independent SGD runs, starting from the same random initialization. On the other hand, there is the lottery ticket hypothesis of Frankle & Carbin (2019), where dense, randomly initialized networks have sparse subnetworks capable of training in isolation to full accuracy.\n\nHowever, neither phenomenon scales beyond small vision networks. We start by proposing a technique to find sparse subnetworks after initialization. We observe that these subnetworks match the accuracy of the full network only when two SGD runs for the same subnetwork are connected by linear paths with the no change in test error. Our findings connect the existence of sparse subnetworks that train to high accuracy with the dynamics of optimization via mode connectivity. In doing so, we identify analogues of the phenomena uncovered by Nagarajan & Kolter and Frankle & Carbin in ImageNet-scale architectures at state-of-the-art sparsity levels.", "keywords": ["sparsity", "mode connectivity", "lottery ticket", "optimization landscape"], "paperhash": "frankle|mode_connectivity_and_sparse_neural_networks", "original_pdf": "/attachment/cdce07e15a495d5ee3031f3f177780e2dac4ccdd.pdf", "_bibtex": "@misc{\nfrankle2020mode,\ntitle={Mode Connectivity and Sparse Neural Networks},\nauthor={Jonathan Frankle and Gintare Karolina Dziugaite and Daniel M. Roy and Michael Carbin},\nyear={2020},\nurl={https://openreview.net/forum?id=rkeO-lrYwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkeO-lrYwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2141/Authors", "ICLR.cc/2020/Conference/Paper2141/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2141/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2141/Reviewers", "ICLR.cc/2020/Conference/Paper2141/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2141/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2141/Authors|ICLR.cc/2020/Conference/Paper2141/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145733, "tmdate": 1576860545532, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2141/Authors", "ICLR.cc/2020/Conference/Paper2141/Reviewers", "ICLR.cc/2020/Conference/Paper2141/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2141/-/Official_Comment"}}}, {"id": "S1g7aB8isB", "original": null, "number": 5, "cdate": 1573770683505, "ddate": null, "tcdate": 1573770683505, "tmdate": 1573770800024, "tddate": null, "forum": "rkeO-lrYwr", "replyto": "rJlhQU77jr", "invitation": "ICLR.cc/2020/Conference/Paper2141/-/Official_Comment", "content": {"title": "Author Response to Reviewer 4 (Part 2)", "comment": "\n> highlight the significance of the connection between matching subnetworks and stability in this highly sparse subnetwork regime.\n\nWe have substantially restructured and rewritten our paper to make the significance of this connection clear. We ask that you take a look at our revised draft.\n\n> Furthermore, the statement is contradicted in Footnote 7: \u201cfor the sparsity levels we studied on VGG (low), the IMP subnetwork is stable but does not quite qualify as matching\u201c\n\nIn the submitted version of the paper, we tried to use the same sparsity level for all variants of VGG (i.e., standard, warmup, and low) and likewise for all variants of Resnet. However, our chosen sparsity level for VGG (low) was too sparse for IMP to produce a matching subnetwork at any rewinding iteration. In the updated version of the paper, we have chosen a separate sparsity level for each hyperparameter configuration based on the sparsest level for which IMP finds a matching subnetwork under any rewinding iteration we consider. We illustrate this process in Appendix A of the updated paper. The VGG (low) results now align with the other experiments.\n\n> Nagarajan & Kolter\u2019s observation about linear interpolation was on a completely different setup: using same duplicate network but training on disjoint subset of data, whereas in this paper it uses different subnetworks and trains it on full dataset with different data order. \n\nThat's correct. In the updated paper, we make sure this distinction is clear. We wanted to give Nagarajan and Kolter ample credit, since their experiment is the closest extant experiment to ours in the literature. We have emphasized this distinction in our revised paper, and we will implement any further feedback you have on clarifying this relationship to related work.\n\n> How was the sparsity level (30%) of Resnet-50 and Inception-v3 chosen in Table 1? (which was later used in Figure 5)\n\nThe sparsity level is actually 70% (that is, 30% of weights remaining). These were the sparsest IMP subnetworks of Resnet-50 and Inception-v3 for which IMP found matching subnetworks at any rewinding iteration under one-shot pruning. The new Appendix A clarifies how we chose our sparsity levels for every network in the paper.\n\n> In Figure 3 and 5, the y-axis \u201cStability(%)\u201d is unclear and not explained how this is computed. I first thought higher amount of stability(%) was good but it doesn't seem to be true.\n\nCalling the rise in error \"stability\" was a bad choice on our part. We fixed this and now call this rise in error \"instability\" and so lower instability is \"better\". Namely, when instability is 0, then the network is stable.\n\n> In some figures VGG-19 come first and then Resnet-20 while for others it was the other way around, which was confusing to read. (Also same for Resnet-50 and Inception-v3)\n\nThis order is now consistent in the updated draft of the paper.\n\n> There are same lines in multiple graphs, but the labeling is inconsistent, potentially confusing readers:\n\nLabeling is now consistent in the updated draft of the paper.\n\n[FC19] Frankle and Carbin. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. ICLR 2019.\n[HPT+15] Han et al. Learning both Weights and Connections for Efficient Neural Networks. NeurIPS 2015.\n[LSZ+19] Liu et al. Rethinking the Value of Network Pruning. ICLR 2019.\n[NK19] Nagarajan and Kolter. Uniform convergence may be unable to explain generalization in deep learning. Arxiv.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2141/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2141/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jfrankle@csail.mit.edu", "karolina.dziugaite@gmail.com", "droy@utstat.toronto.edu", "mcarbin@csail.mit.edu"], "title": "Mode Connectivity and Sparse Neural Networks", "authors": ["Jonathan Frankle", "Gintare Karolina Dziugaite", "Daniel M. Roy", "Michael Carbin"], "pdf": "/pdf/415db8f7d8081842f3f179b64310700bc7e8d2cf.pdf", "TL;DR": "Whether or not a sparse subnetwork trains to the same accuracy of the full network depends on whether two runs of SGD on the subnetwork land in the same convex levelset.", "abstract": "We uncover a connection between two seemingly unrelated empirical phenomena: mode connectivity and sparsity. On the one hand, there is growing catalog of situations where, across multiple runs, SGD learns weights that fall into minima that are connected (mode connectivity). A striking example is described by Nagarajan & Kolter (2019). They observe that test error on MNIST does not change along the linear path connecting the end points of two independent SGD runs, starting from the same random initialization. On the other hand, there is the lottery ticket hypothesis of Frankle & Carbin (2019), where dense, randomly initialized networks have sparse subnetworks capable of training in isolation to full accuracy.\n\nHowever, neither phenomenon scales beyond small vision networks. We start by proposing a technique to find sparse subnetworks after initialization. We observe that these subnetworks match the accuracy of the full network only when two SGD runs for the same subnetwork are connected by linear paths with the no change in test error. Our findings connect the existence of sparse subnetworks that train to high accuracy with the dynamics of optimization via mode connectivity. In doing so, we identify analogues of the phenomena uncovered by Nagarajan & Kolter and Frankle & Carbin in ImageNet-scale architectures at state-of-the-art sparsity levels.", "keywords": ["sparsity", "mode connectivity", "lottery ticket", "optimization landscape"], "paperhash": "frankle|mode_connectivity_and_sparse_neural_networks", "original_pdf": "/attachment/cdce07e15a495d5ee3031f3f177780e2dac4ccdd.pdf", "_bibtex": "@misc{\nfrankle2020mode,\ntitle={Mode Connectivity and Sparse Neural Networks},\nauthor={Jonathan Frankle and Gintare Karolina Dziugaite and Daniel M. Roy and Michael Carbin},\nyear={2020},\nurl={https://openreview.net/forum?id=rkeO-lrYwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkeO-lrYwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2141/Authors", "ICLR.cc/2020/Conference/Paper2141/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2141/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2141/Reviewers", "ICLR.cc/2020/Conference/Paper2141/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2141/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2141/Authors|ICLR.cc/2020/Conference/Paper2141/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145733, "tmdate": 1576860545532, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2141/Authors", "ICLR.cc/2020/Conference/Paper2141/Reviewers", "ICLR.cc/2020/Conference/Paper2141/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2141/-/Official_Comment"}}}, {"id": "S1eMPBLoiS", "original": null, "number": 4, "cdate": 1573770586297, "ddate": null, "tcdate": 1573770586297, "tmdate": 1573770586297, "tddate": null, "forum": "rkeO-lrYwr", "replyto": "HygFbYdatS", "invitation": "ICLR.cc/2020/Conference/Paper2141/-/Official_Comment", "content": {"title": "Author Response to Reviewer 2", "comment": "\nNOTE: We have posted an updated version of the paper that has been substantially restructured and rewritten to address your concerns. We highly recommend looking over the new paper.\n\nWe have summarized these changes in a general response (posted as a top-level comment). We ask that you read our general response before returning to this point-by-point response. We address many of your concerns there.\n\n--------------------\n\n> It is unclear from the paper what are the immediate / straightforward applications\n\nSee the PRACTICAL IMPLICATIONS section of the top-level comment.\n\n> First, this paper lacks a structured literature review.\n\nWe have integrated a review of relevant literature into the body of the revised paper. For the final version of the paper, we are working on a structured literature review that we will insert after the introduction.\n\n> Does the full network have the property of mode connectivity (when trained using different data orders), or this only occurs under sparsity. \n\nYes! In Section 3 of the new version, we show that full networks indeed have the property of mode connectivity.\n\n> Provide some metrics on how \u201cfar\u201d are the two final weights upon which mode connectivity (or stability) is explored.\n\nIn A.3 of the submission and Appendices D and E of our revised paper, we include the L2 distances between networks when we perform instability analysis. We include this data for all of our experiments (full networks and all three kinds of sparse networks). In the final version, we will also include bases for comparison (e.g., the distance between the initial and final weights, the distance between two networks trained with different initializations) to contextualize these L2 distance values.\n\nBriefly, we observe that L2 distances between the sparse networks seem to be at two different levels. When the networks are unstable (as in the case of unstable IMP subnetworks, randomly reinitialized subnetworks, and randomly pruned networks), L2 distance is at the higher level; that is, the networks are further apart. As the IMP subnetworks transition to stability, L2 distance decreases, reaching a lower (non-zero) level when they become stable. We do not observe any relationship between the stability of the unpruned networks and the L2 distances between them.\n\n> The introduction mentions connectivity was previously observed using \u201cdisjoint subsets\u201d of data...I wonder if this is a typo.\n\nIt is not a typo. The only prior work that looks at linear mode connectivity starting from the same initialization is [NK19]. That paper trains two copies of an MLP from the same initialization on disjoint subsets of MNIST. In our paper, we study different data orders rather than disjoint samples from the same distribution. We mentioned this work because wanted to give Nagarajan and Kolter ample credit since their experiment is the closest extant experiment to ours in the literature. We have emphasized this distinction in our revised paper.\n\n> Exploring if the findings still apply on disjoint data and/or varying amount of data, besides different data orders, is helpful.\n\nWe agree that there are a wide range of other behaviors of neural networks that we can explore with our instability analysis framework. We are particularly interested in studying instability when training with disjoint datasets (as you mention) and when varying batch size, learning rate, network width, optimizer, and learning rate schedule (e.g., cyclic learning rates [Smith17] and exponential learning rates [LA19]). Each of these investigations could be a paper in its own right and is beyond the scope of the current work.\n\n> The writing...can definitely use more work.\n\nWe have heavily revised the paper, and we believe the writing is substantially more polished. We are happy to accept further feedback that we will incorporate into the final version of the paper.\n\n[LA19] Li and Arora. An Exponential Learning Rate Schedule for Deep Learning. Arxiv.\n[NK19] Nagarajan and Kolter. Uniform convergence may be unable to explain generalization in deep learning. Arxiv.\n[Smith17] Leslie Smith. Cyclical Learning Rates for Training Neural Networks. WACV 2017.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2141/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2141/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jfrankle@csail.mit.edu", "karolina.dziugaite@gmail.com", "droy@utstat.toronto.edu", "mcarbin@csail.mit.edu"], "title": "Mode Connectivity and Sparse Neural Networks", "authors": ["Jonathan Frankle", "Gintare Karolina Dziugaite", "Daniel M. Roy", "Michael Carbin"], "pdf": "/pdf/415db8f7d8081842f3f179b64310700bc7e8d2cf.pdf", "TL;DR": "Whether or not a sparse subnetwork trains to the same accuracy of the full network depends on whether two runs of SGD on the subnetwork land in the same convex levelset.", "abstract": "We uncover a connection between two seemingly unrelated empirical phenomena: mode connectivity and sparsity. On the one hand, there is growing catalog of situations where, across multiple runs, SGD learns weights that fall into minima that are connected (mode connectivity). A striking example is described by Nagarajan & Kolter (2019). They observe that test error on MNIST does not change along the linear path connecting the end points of two independent SGD runs, starting from the same random initialization. On the other hand, there is the lottery ticket hypothesis of Frankle & Carbin (2019), where dense, randomly initialized networks have sparse subnetworks capable of training in isolation to full accuracy.\n\nHowever, neither phenomenon scales beyond small vision networks. We start by proposing a technique to find sparse subnetworks after initialization. We observe that these subnetworks match the accuracy of the full network only when two SGD runs for the same subnetwork are connected by linear paths with the no change in test error. Our findings connect the existence of sparse subnetworks that train to high accuracy with the dynamics of optimization via mode connectivity. In doing so, we identify analogues of the phenomena uncovered by Nagarajan & Kolter and Frankle & Carbin in ImageNet-scale architectures at state-of-the-art sparsity levels.", "keywords": ["sparsity", "mode connectivity", "lottery ticket", "optimization landscape"], "paperhash": "frankle|mode_connectivity_and_sparse_neural_networks", "original_pdf": "/attachment/cdce07e15a495d5ee3031f3f177780e2dac4ccdd.pdf", "_bibtex": "@misc{\nfrankle2020mode,\ntitle={Mode Connectivity and Sparse Neural Networks},\nauthor={Jonathan Frankle and Gintare Karolina Dziugaite and Daniel M. Roy and Michael Carbin},\nyear={2020},\nurl={https://openreview.net/forum?id=rkeO-lrYwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkeO-lrYwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2141/Authors", "ICLR.cc/2020/Conference/Paper2141/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2141/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2141/Reviewers", "ICLR.cc/2020/Conference/Paper2141/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2141/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2141/Authors|ICLR.cc/2020/Conference/Paper2141/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145733, "tmdate": 1576860545532, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2141/Authors", "ICLR.cc/2020/Conference/Paper2141/Reviewers", "ICLR.cc/2020/Conference/Paper2141/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2141/-/Official_Comment"}}}, {"id": "HkeWmS8iiS", "original": null, "number": 3, "cdate": 1573770521086, "ddate": null, "tcdate": 1573770521086, "tmdate": 1573770521086, "tddate": null, "forum": "rkeO-lrYwr", "replyto": "HyxwWTFAOS", "invitation": "ICLR.cc/2020/Conference/Paper2141/-/Official_Comment", "content": {"title": "Author Response to Reviewer 3", "comment": "\nNOTE: We have posted an updated version of the paper that has been substantially restructured and rewritten to address your concerns. We highly recommend looking over the new paper.\n\nWe have summarized these changes in a general response (posted as a top-level comment). We ask that you read our general response before returning to this point-by-point response. We address many of your concerns there.\n\n--------------------\n\n> The content are poorly presented for me fully appreciate the importance and practical implications of this work\n\nWe apologize that our original presentation did not clearly articulate the importance and practical implications of our work. We have taken your feedback to heart, and we have substantially restructured and rewritten the paper to ensure that these aspects are clear. We have summarized our clarified framing in the top-level comment.\n\nWe specifically address practical implications in the PRACTICAL IMPLICATIONS section of the top-level comment and in the discussion sections of our revised paper.\n\n> I don't understand why the connection between mode connectivity and lottery ticket hypothesis is an important one to reveal.\n\nIn the top-level comment, we present our clarified framing for the paper designed to emphasize why both linear mode connectivity on full networks and its connection to the lottery ticket hypothesis are important. We discuss concrete practical implications of our observations in the PRACTICAL IMPLICATIONS section of the top-level comment.\n\n> I can not extract useful intuitions/messages from the demonstration here on why this happens.\n\nAt the moment, the phenomena we observe are entirely empirical. We do not yet have a theoretical model to describe this behavior, although we are exploring various connections (e.g., it is consistent with the so-called neural tangent kernel regime where very wide neural networks behave like linear models). However, we contend that our experiments are sufficiently rigorous to convincingly establish the existence of these phenomena. We believe that recording these phenomena rigorously is a significant contribution that will inspire theoretical work to understand and explain these behaviors.\n\n> Minor comments for improving the paper.\n\nThank you for these detailed comments. We have addressed them in the new version of the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper2141/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2141/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jfrankle@csail.mit.edu", "karolina.dziugaite@gmail.com", "droy@utstat.toronto.edu", "mcarbin@csail.mit.edu"], "title": "Mode Connectivity and Sparse Neural Networks", "authors": ["Jonathan Frankle", "Gintare Karolina Dziugaite", "Daniel M. Roy", "Michael Carbin"], "pdf": "/pdf/415db8f7d8081842f3f179b64310700bc7e8d2cf.pdf", "TL;DR": "Whether or not a sparse subnetwork trains to the same accuracy of the full network depends on whether two runs of SGD on the subnetwork land in the same convex levelset.", "abstract": "We uncover a connection between two seemingly unrelated empirical phenomena: mode connectivity and sparsity. On the one hand, there is growing catalog of situations where, across multiple runs, SGD learns weights that fall into minima that are connected (mode connectivity). A striking example is described by Nagarajan & Kolter (2019). They observe that test error on MNIST does not change along the linear path connecting the end points of two independent SGD runs, starting from the same random initialization. On the other hand, there is the lottery ticket hypothesis of Frankle & Carbin (2019), where dense, randomly initialized networks have sparse subnetworks capable of training in isolation to full accuracy.\n\nHowever, neither phenomenon scales beyond small vision networks. We start by proposing a technique to find sparse subnetworks after initialization. We observe that these subnetworks match the accuracy of the full network only when two SGD runs for the same subnetwork are connected by linear paths with the no change in test error. Our findings connect the existence of sparse subnetworks that train to high accuracy with the dynamics of optimization via mode connectivity. In doing so, we identify analogues of the phenomena uncovered by Nagarajan & Kolter and Frankle & Carbin in ImageNet-scale architectures at state-of-the-art sparsity levels.", "keywords": ["sparsity", "mode connectivity", "lottery ticket", "optimization landscape"], "paperhash": "frankle|mode_connectivity_and_sparse_neural_networks", "original_pdf": "/attachment/cdce07e15a495d5ee3031f3f177780e2dac4ccdd.pdf", "_bibtex": "@misc{\nfrankle2020mode,\ntitle={Mode Connectivity and Sparse Neural Networks},\nauthor={Jonathan Frankle and Gintare Karolina Dziugaite and Daniel M. Roy and Michael Carbin},\nyear={2020},\nurl={https://openreview.net/forum?id=rkeO-lrYwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkeO-lrYwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2141/Authors", "ICLR.cc/2020/Conference/Paper2141/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2141/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2141/Reviewers", "ICLR.cc/2020/Conference/Paper2141/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2141/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2141/Authors|ICLR.cc/2020/Conference/Paper2141/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145733, "tmdate": 1576860545532, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2141/Authors", "ICLR.cc/2020/Conference/Paper2141/Reviewers", "ICLR.cc/2020/Conference/Paper2141/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2141/-/Official_Comment"}}}, {"id": "BJeG9EUsor", "original": null, "number": 1, "cdate": 1573770378511, "ddate": null, "tcdate": 1573770378511, "tmdate": 1573770471761, "tddate": null, "forum": "rkeO-lrYwr", "replyto": "rkeO-lrYwr", "invitation": "ICLR.cc/2020/Conference/Paper2141/-/Official_Comment", "content": {"title": "Author Response - Overall Comment (Part 2)", "comment": "\nPRACTICAL IMPLICATIONS\n\nOur paper is scientific in nature, with the goal of better understanding the relationship between SGD noise and the outcome of neural network optimization for both dense neural networks and sparse subnetworks found by IMP. Although our focus is not on immediate or straightforward applications, there are several ways that our results might lead to applications:\n\n* Others have already adopted our modified version of IMP with rewinding to build practical techniques. For example, the networks generated by rewinding transfer between datasets, making it possible to train sparser networks from the start [MYP+19]. In addition, replacing fine-tuning with rewinding when pruning a neural network makes it possible to maintain full accuracy at more extreme sparsities [Anon19b]. IMP with rewinding has also been adopted to study the lottery ticket hypothesis [Anon19c, Anon19d, Anon19e, Anon19f, Anon19g].\n\n* In larger-scale settings, we find that IMP subnetworks at extreme sparsities only become stable and matching after the full network has been trained for a small amount of time. Recent methods have explored pruning neural networks at initialization [LAT19, Anon19a], but our results suggest that the best time to prune may be slightly later in training. By that same token, most modern pruning methods only begin to sparsify networks late in training or after training [HTP+15,GEH19]. In these cases, our work suggests that there is potentially a substantial unexploited opportunity to prune neural networks much earlier in training.\n\n* Our observations on full networks implicitly divide training into two phases: an initial, unstable phase in which the final \u201clinearly connected\u201d mode is undetermined on the account of SGD noise, and a subsequent, stable phase in which the final linearly connected mode becomes determined. One possible way to exploit our this observation could be to explore changing aspects of the optimization process (e.g., learning rate schedule or optimizer) once the network enters the stable phase in order to improve the performance of training. Other techniques already follow this template; for example, Goyal et al. find that warmup is necessary early in training when using large batch sizes and high learning rates [GDG17+].  Instability analysis makes it possible to evaluate the consequences of these interventions. \n\nSUMMARY OF TECHNICAL CHANGES\n\n* We have renamed \u201cstability\u201d to \u201cinstability\u201d so that a network is \u201cstable\u201d when \u201cinstability\u201d is 0.\n\n* We have moved results on full networks from the appendices into the main body of the paper as Section 3.\n\n* In our analysis of full networks, we have examined instability with respect to train error in addition to test error.\n\n* We have updated our implementations of Resnet-20 and VGG-16 to reach higher, state-of-the-art accuracy. At this higher accuracy, the IMP subnetworks of these networks now become stable and matching slightly later in training than before, but they still do so 1-2% into training.\n\n* We compare the instability of IMP subnetworks to that of randomly reinitialized IMP subnetworks in addition to randomly pruned subnetworks.\n\n[Anon19a] Anonymous. Picking Winning Tickets Before Training by Preserving Gradient Flow. In submission to ICLR 2020.\n[Anon19b] Anonymous. Comparing Fine-Tuning and Rewinding in Neural Network Pruning. In submission to ICLR 2020.\n[Anon19c] Anonymous. Playing the Lottery with Rewards and Multiple Languages. In submission to ICLR 2020.\n[Anon19d] Anonymous. Finding Winning Tickets with Limited (or No) Supervision. In submission to ICLR 2020.\n[Anon19e] Anonymous. Winning the Lottery with Continuous Sparsification. In submission to ICLR 2020.\n[Anon19f] Anonymous. The Sooner the Better: Investigating the Structure of Early Winning Lottery Tickets. In submission to ICLR 2020.\n[Anon19g] Anonymous. The Early Phase of Neural Network Training. In submission to ICLR 2020.\n[GDG+17] Goyal et al. Accurate, Large Minbatch SGD: Training Imagenet in 1 Hour. CVPR 2018.\n[GEH19] Gale et al. The State of Sparsity in Deep Neural Networks. Arxiv.\n[HPT+15] Han et al. Learning both Weights and Connections for Efficient Neural Networks. NeurIPS 2015.\n[LAT19] Lee et al. SNIP: Single-Shot Network Pruning Based on Connection Sensitivity. ICLR 2019.\n[MYP+19] Morcos et al. One Ticket to Win them All: Generalizing Lottery Ticket Initializations Across Datasets and Optimizers. NeuIPS 2019.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2141/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2141/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jfrankle@csail.mit.edu", "karolina.dziugaite@gmail.com", "droy@utstat.toronto.edu", "mcarbin@csail.mit.edu"], "title": "Mode Connectivity and Sparse Neural Networks", "authors": ["Jonathan Frankle", "Gintare Karolina Dziugaite", "Daniel M. Roy", "Michael Carbin"], "pdf": "/pdf/415db8f7d8081842f3f179b64310700bc7e8d2cf.pdf", "TL;DR": "Whether or not a sparse subnetwork trains to the same accuracy of the full network depends on whether two runs of SGD on the subnetwork land in the same convex levelset.", "abstract": "We uncover a connection between two seemingly unrelated empirical phenomena: mode connectivity and sparsity. On the one hand, there is growing catalog of situations where, across multiple runs, SGD learns weights that fall into minima that are connected (mode connectivity). A striking example is described by Nagarajan & Kolter (2019). They observe that test error on MNIST does not change along the linear path connecting the end points of two independent SGD runs, starting from the same random initialization. On the other hand, there is the lottery ticket hypothesis of Frankle & Carbin (2019), where dense, randomly initialized networks have sparse subnetworks capable of training in isolation to full accuracy.\n\nHowever, neither phenomenon scales beyond small vision networks. We start by proposing a technique to find sparse subnetworks after initialization. We observe that these subnetworks match the accuracy of the full network only when two SGD runs for the same subnetwork are connected by linear paths with the no change in test error. Our findings connect the existence of sparse subnetworks that train to high accuracy with the dynamics of optimization via mode connectivity. In doing so, we identify analogues of the phenomena uncovered by Nagarajan & Kolter and Frankle & Carbin in ImageNet-scale architectures at state-of-the-art sparsity levels.", "keywords": ["sparsity", "mode connectivity", "lottery ticket", "optimization landscape"], "paperhash": "frankle|mode_connectivity_and_sparse_neural_networks", "original_pdf": "/attachment/cdce07e15a495d5ee3031f3f177780e2dac4ccdd.pdf", "_bibtex": "@misc{\nfrankle2020mode,\ntitle={Mode Connectivity and Sparse Neural Networks},\nauthor={Jonathan Frankle and Gintare Karolina Dziugaite and Daniel M. Roy and Michael Carbin},\nyear={2020},\nurl={https://openreview.net/forum?id=rkeO-lrYwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkeO-lrYwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2141/Authors", "ICLR.cc/2020/Conference/Paper2141/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2141/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2141/Reviewers", "ICLR.cc/2020/Conference/Paper2141/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2141/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2141/Authors|ICLR.cc/2020/Conference/Paper2141/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145733, "tmdate": 1576860545532, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2141/Authors", "ICLR.cc/2020/Conference/Paper2141/Reviewers", "ICLR.cc/2020/Conference/Paper2141/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2141/-/Official_Comment"}}}, {"id": "BkxR3EIjjH", "original": null, "number": 2, "cdate": 1573770421534, "ddate": null, "tcdate": 1573770421534, "tmdate": 1573770421534, "tddate": null, "forum": "rkeO-lrYwr", "replyto": "rkeO-lrYwr", "invitation": "ICLR.cc/2020/Conference/Paper2141/-/Official_Comment", "content": {"title": "Author Response - Overall Comment (Part 1)", "comment": "\nNOTE: We have posted an updated version of the paper that has been substantially restructured and rewritten to address your concerns. We highly recommend looking over the new paper.\n\n--------------------\n\nWe thank the reviewers for their feedback.\n\nUpon reading reviews 2 and 3, we recognize that we failed to adequately communicate the significance of our results. Upon reading review 4, we recognize that we failed to clarify the scope of our claims and justify the importance of our chosen methodology.\n\nBased on your feedback, we have substantially restructured and rewritten our paper to address these concerns. We believe that our \u201cstability analysis\u201d framework (which as per R4 we now call \u201cinstability analysis\u201d) and our new observations about IMP subnetworks are significant contributions, and we hope to convince you that this is the case with our updated version.\n\nHere, we summarize our revised framing. We have responded to specific concerns of individual reviewers in separate replies to their comments.\n\nSUMMARY OF REVISED FRAMING\n\nIn our original submission, we framed our contribution as a surprising connection between two empirical phenomena of recent interest: mode connectivity and sparse neural networks in the context of the lottery ticket hypothesis.\n\nIn the revised version, we instead emphasize that our \u201cinstability analysis\u201d framework provides a new lens through which to study the behavior of neural networks by way of linear mode connectivity.\n\nWe demonstrate the value of this framework in two ways. First, we study the instability of full, unpruned networks. We now recognize that this data, which was previously buried in the appendices, is a significant contribution in its own right and an important part of our story. The central finding of this experiment is that all networks become stable early in training. That is, early in training, the outcome of optimization is determined modulo linear mode connectivity.\n\nWe then use instability analysis to better understand \u201clottery ticket\u201d networks found by IMP. Our core finding is that, at extreme sparsities, an IMP subnetwork is matching (i.e., it can train in isolation to full accuracy) only when it is stable. This insight provides the first basis for understanding the mixed results on IMP in the literature. In addition, we modify IMP to \u201crewind\u201d subnetworks to their values at an iteration k > 0 rather than to initialization. For values of k that are early in training, IMP subnetworks become stable and matching in all cases that we consider, including large-scale settings where IMP fails to do so at initialization.  In response to R4\u2019s suggestions, we have modified the scope of our claims to focus exclusively on IMP subnetworks at the highest sparsity for which IMP at any rewinding iteration produces a matching subnetwork. \n\nCONTRIBUTIONS AND IMPLICATIONS\n\nOur revisions aim to clarify that our work makes significant contributions to both (1) our understanding of SGD on neural networks and (2) our understanding of sparse IMP subnetworks and the lottery ticket hypothesis. Please see the updated \u201cContributions\u201d paragraph in our new introduction.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2141/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2141/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jfrankle@csail.mit.edu", "karolina.dziugaite@gmail.com", "droy@utstat.toronto.edu", "mcarbin@csail.mit.edu"], "title": "Mode Connectivity and Sparse Neural Networks", "authors": ["Jonathan Frankle", "Gintare Karolina Dziugaite", "Daniel M. Roy", "Michael Carbin"], "pdf": "/pdf/415db8f7d8081842f3f179b64310700bc7e8d2cf.pdf", "TL;DR": "Whether or not a sparse subnetwork trains to the same accuracy of the full network depends on whether two runs of SGD on the subnetwork land in the same convex levelset.", "abstract": "We uncover a connection between two seemingly unrelated empirical phenomena: mode connectivity and sparsity. On the one hand, there is growing catalog of situations where, across multiple runs, SGD learns weights that fall into minima that are connected (mode connectivity). A striking example is described by Nagarajan & Kolter (2019). They observe that test error on MNIST does not change along the linear path connecting the end points of two independent SGD runs, starting from the same random initialization. On the other hand, there is the lottery ticket hypothesis of Frankle & Carbin (2019), where dense, randomly initialized networks have sparse subnetworks capable of training in isolation to full accuracy.\n\nHowever, neither phenomenon scales beyond small vision networks. We start by proposing a technique to find sparse subnetworks after initialization. We observe that these subnetworks match the accuracy of the full network only when two SGD runs for the same subnetwork are connected by linear paths with the no change in test error. Our findings connect the existence of sparse subnetworks that train to high accuracy with the dynamics of optimization via mode connectivity. In doing so, we identify analogues of the phenomena uncovered by Nagarajan & Kolter and Frankle & Carbin in ImageNet-scale architectures at state-of-the-art sparsity levels.", "keywords": ["sparsity", "mode connectivity", "lottery ticket", "optimization landscape"], "paperhash": "frankle|mode_connectivity_and_sparse_neural_networks", "original_pdf": "/attachment/cdce07e15a495d5ee3031f3f177780e2dac4ccdd.pdf", "_bibtex": "@misc{\nfrankle2020mode,\ntitle={Mode Connectivity and Sparse Neural Networks},\nauthor={Jonathan Frankle and Gintare Karolina Dziugaite and Daniel M. Roy and Michael Carbin},\nyear={2020},\nurl={https://openreview.net/forum?id=rkeO-lrYwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkeO-lrYwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2141/Authors", "ICLR.cc/2020/Conference/Paper2141/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2141/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2141/Reviewers", "ICLR.cc/2020/Conference/Paper2141/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2141/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2141/Authors|ICLR.cc/2020/Conference/Paper2141/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145733, "tmdate": 1576860545532, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2141/Authors", "ICLR.cc/2020/Conference/Paper2141/Reviewers", "ICLR.cc/2020/Conference/Paper2141/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2141/-/Official_Comment"}}}, {"id": "rJlhQU77jr", "original": null, "number": 3, "cdate": 1573234212130, "ddate": null, "tcdate": 1573234212130, "tmdate": 1573234212130, "tddate": null, "forum": "rkeO-lrYwr", "replyto": "rkeO-lrYwr", "invitation": "ICLR.cc/2020/Conference/Paper2141/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #4", "review": "\nThis paper empirically examines an interesting relationship between mode connectivity and matching sparse subnetworks (lottery ticket hypothesis). \n\nBy mode connectivity, the paper refers to a specific instance where the final trained SGD solutions are connected by a linear interpolation path without loss in test accuracy. When networks trained with SGD reliably find solutions which can be linearly interpolated without loss in test accuracy despite different data ordering,  the paper refers to these networks as \u2018stable.\u2019 \n\nMatching sparse subnetworks refer to subnetworks within a full dense network that matches the test accuracy of the full network when trained in isolation.  \n\nThe paper introduces a novel improvement on the existing iterative magnitude pruning (IMP) technique that is able to find matching subnetworks even after initialization by rewinding the weights. This allowed the authors to find matching subnetworks for deeper networks and in cases where it could not be done without some intervention in learning schedule. \n\nThe paper then finds a relationship that only when the subnetworks become stable, the subnetworks become matching subnetworks.\n\u2014\u2014\u2014\n\nAlthough finding a connection between two seemingly distinct phenomena is novel and interesting, I would recommend a weak reject for the following two reasons: \n1) The scope of the experiment is limited to a quite specific setting, \n2) there are unsupported strong claims which need to be clarified.\n\u2014\u2014\u2014\n\n1)\nIn the abstract the paper claims that sparse subnetworks are matching subnetworks only when they are stable, but the results are shown in a limited setting only at a very high sparsity. \nThey tested stability on the highest sparsity level at which there was evidence that matching subnetworks existed, but how would the result generalize to other sparsity levels?\nWith lower sparsity level (if weights are pruned less), is stability easier to achieve? \n\nThe paper also focused on cases where matching subnetworks were found by IMP, but matching subnetworks can also be found by other pruning methods. \nAs acknowledged in the limitations section, other relationships may exist between stability and matching subnetworks found by other pruning methods, or in different sparsity levels,\nwhich could be quite different from this paper\u2019s claim.\n\nIn order to address this concern, I think the paper needs to show how the same relationship might generalize to different sparsity levels, \nor alternatively modify the claim (to what it actually shows) and highlight the significance of the connection between matching subnetworks and stability in this highly sparse subnetwork regime.\n\n2) \nAs addressed above, in the Abstract and Introduction, the paper\u2019s claims are very general about mode connectivity and sparsity, claiming in the sparse regime, \u201ca subnetwork is matching if and only if it is stable.\u201d However, the experiments only show it is true in a limited setting, focusing on specific pruning method and at a specific sparsity level.\nFurthermore, the statement is contradicted in Footnote 7: \u201cfor the sparsity levels we studied on VGG (low), the IMP subnetwork is stable but does not quite qualify as matching\u201c\n\nThere are also a few other areas where there are unsupported claims.\n\n\u201cNamely, whenever IMP finds a matching subnetwork, test error does not increase when linearly interpolating between duplicates, meaning the subnetwork is stable.\u201d \n-> Stability was tested only at one specific sparsity level, and it is not obvious it would be stable at all lower sparsity levels where IMP found matching subnetworks.\n\n\u201cThis result extends Nagarajan & Kolter\u2019s observation about linear interpolation beyond MNIST to matching subnetworks found by IMP at initialization on our CIFAR10 networks\u201d \n-> Nagarajan & Kolter\u2019s observation about linear interpolation was on a completely different setup: using same duplicate network but training on disjoint subset of data, whereas in this paper it uses different subnetworks and trains it on full dataset with different data order. \n\nRelated to the first issue, I think some of these stronger claims can be modified to describe what the experiments actually show. \nThe relationship found between stability and matching subnetworks in the high sparsity regime is a valuable insight that I believe should be conveyed correctly in this paper.\n\n\u2014\u2014\u2014\n\nI also have some minor clarification question and suggestions for improvement. \n\nHow was the sparsity level (30%) of Resnet-50 and Inception-v3 chosen in Table 1? (which was later used in Figure 5)\n\n\u2014 In Figure 3 and 5, the y-axis \u201cStability(%)\u201d is unclear and not explained how this is computed. I first thought higher amount of stability(%) was good but it doesn't seem to be true.\n\n\u2014 The ordering of methods for plots could be more consistent. In some figures VGG-19 come first and then Resnet-20 while for others it was the other way around, which was confusing to read. (Also same for Resnet-50 and Inception-v3)\n\n\u2014 There are same lines in multiple graphs, but the labeling is inconsistent, potentially confusing readers:\nFigure 1: (Original Init, Standard) is the same as Figure 4: (Reset), \nand Figure 1: (Random Reinit, Standard) is the same as Figure 4: (Reset, Random Reinit)", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper2141/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2141/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jfrankle@csail.mit.edu", "karolina.dziugaite@gmail.com", "droy@utstat.toronto.edu", "mcarbin@csail.mit.edu"], "title": "Mode Connectivity and Sparse Neural Networks", "authors": ["Jonathan Frankle", "Gintare Karolina Dziugaite", "Daniel M. Roy", "Michael Carbin"], "pdf": "/pdf/415db8f7d8081842f3f179b64310700bc7e8d2cf.pdf", "TL;DR": "Whether or not a sparse subnetwork trains to the same accuracy of the full network depends on whether two runs of SGD on the subnetwork land in the same convex levelset.", "abstract": "We uncover a connection between two seemingly unrelated empirical phenomena: mode connectivity and sparsity. On the one hand, there is growing catalog of situations where, across multiple runs, SGD learns weights that fall into minima that are connected (mode connectivity). A striking example is described by Nagarajan & Kolter (2019). They observe that test error on MNIST does not change along the linear path connecting the end points of two independent SGD runs, starting from the same random initialization. On the other hand, there is the lottery ticket hypothesis of Frankle & Carbin (2019), where dense, randomly initialized networks have sparse subnetworks capable of training in isolation to full accuracy.\n\nHowever, neither phenomenon scales beyond small vision networks. We start by proposing a technique to find sparse subnetworks after initialization. We observe that these subnetworks match the accuracy of the full network only when two SGD runs for the same subnetwork are connected by linear paths with the no change in test error. Our findings connect the existence of sparse subnetworks that train to high accuracy with the dynamics of optimization via mode connectivity. In doing so, we identify analogues of the phenomena uncovered by Nagarajan & Kolter and Frankle & Carbin in ImageNet-scale architectures at state-of-the-art sparsity levels.", "keywords": ["sparsity", "mode connectivity", "lottery ticket", "optimization landscape"], "paperhash": "frankle|mode_connectivity_and_sparse_neural_networks", "original_pdf": "/attachment/cdce07e15a495d5ee3031f3f177780e2dac4ccdd.pdf", "_bibtex": "@misc{\nfrankle2020mode,\ntitle={Mode Connectivity and Sparse Neural Networks},\nauthor={Jonathan Frankle and Gintare Karolina Dziugaite and Daniel M. Roy and Michael Carbin},\nyear={2020},\nurl={https://openreview.net/forum?id=rkeO-lrYwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkeO-lrYwr", "replyto": "rkeO-lrYwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2141/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2141/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575917060561, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2141/Reviewers"], "noninvitees": [], "tcdate": 1570237727118, "tmdate": 1575917060573, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2141/-/Official_Review"}}}, {"id": "HyxwWTFAOS", "original": null, "number": 1, "cdate": 1570835710654, "ddate": null, "tcdate": 1570835710654, "tmdate": 1572972377773, "tddate": null, "forum": "rkeO-lrYwr", "replyto": "rkeO-lrYwr", "invitation": "ICLR.cc/2020/Conference/Paper2141/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper works on empirically demonstrating the connection between model connectivity and the lottery ticket hypothesis, which are individually explored in the literature. Here the model connectivity refers to the fact that SGD produces different solutions (from the randomness, such as data ordering) that are connected through model parameter transition paths of approximately equal loss/accuracy. The lottery ticket hypothesis tells that there exist sparse subnetworks of the corresponding full dense network which can attain as strong loss / accuracy as the full dense network. \n\nAs the primary contribution, the authors demonstrated that the following two observations often emerge together: 1) A sparse subnetwork can match the performance of the corresponding full dense network;  2) Running SGD on this sparse subnetwork produces solutions which are connected via a linear model parameter transition path of similar loss/accuracy; this is observed across both small tasks (using CIFAR10) and ImageNet-level tasks. Another contribution I can see besides the primary one is that the lottery ticket hypothesis still holds on large tasks, which is against the conventional wisdom demonstrated in recent papers (e.g. Gale et al., 2019); the authors show that it needs to rewind to weights after a short period of training instead of rewinding to the initialized weight in Iterative Magnitude Pruning to produce the \"lottery ticket\" in large tasks (such as CNN for ImageNet). \n\nI think the primary contribution on the connection between model connectivity and lottery ticket hypothesis is an interesting observation, but the content are poorly presented for me fully appreciate the importance and practical implications of this work. Thus I give weak reject. The major concerns and questions are as the following:\n\n1. From the paper, I don't understand why the connection between model connectivity and lottery ticket hypothesis is an important one to reveal. Is it important because it implies some practical approaches / heuristics to figure out performant sparse subnetworks? Is it intrinsically interesting because it validates some hypothesis in the training dynamics of SGD? These are not clear to me.\n\n2. I think the current presentation of the content is only limited to the empirical demonstration. And I can not extract useful intuitions/messages from the demonstration here on why this happens. These message should provide intuitions on why this connection exists. E.g. These message can be extracted from some SGD on some simple (toy) non-convex models with multiple local minimum regions.  \n\nMinor comments for improving the paper:\n\n1. At the end of line 1 in algorithm one, it is not clear what 1^|W0| means.\n\n2. The terms in figure legend needs to be properly defined to enable clear reading. Currently words such as \"reset\" is not mentioned in the text but appears in the legend of figure 4 and etc. \n\n\n\n\n\n\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2141/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2141/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jfrankle@csail.mit.edu", "karolina.dziugaite@gmail.com", "droy@utstat.toronto.edu", "mcarbin@csail.mit.edu"], "title": "Mode Connectivity and Sparse Neural Networks", "authors": ["Jonathan Frankle", "Gintare Karolina Dziugaite", "Daniel M. Roy", "Michael Carbin"], "pdf": "/pdf/415db8f7d8081842f3f179b64310700bc7e8d2cf.pdf", "TL;DR": "Whether or not a sparse subnetwork trains to the same accuracy of the full network depends on whether two runs of SGD on the subnetwork land in the same convex levelset.", "abstract": "We uncover a connection between two seemingly unrelated empirical phenomena: mode connectivity and sparsity. On the one hand, there is growing catalog of situations where, across multiple runs, SGD learns weights that fall into minima that are connected (mode connectivity). A striking example is described by Nagarajan & Kolter (2019). They observe that test error on MNIST does not change along the linear path connecting the end points of two independent SGD runs, starting from the same random initialization. On the other hand, there is the lottery ticket hypothesis of Frankle & Carbin (2019), where dense, randomly initialized networks have sparse subnetworks capable of training in isolation to full accuracy.\n\nHowever, neither phenomenon scales beyond small vision networks. We start by proposing a technique to find sparse subnetworks after initialization. We observe that these subnetworks match the accuracy of the full network only when two SGD runs for the same subnetwork are connected by linear paths with the no change in test error. Our findings connect the existence of sparse subnetworks that train to high accuracy with the dynamics of optimization via mode connectivity. In doing so, we identify analogues of the phenomena uncovered by Nagarajan & Kolter and Frankle & Carbin in ImageNet-scale architectures at state-of-the-art sparsity levels.", "keywords": ["sparsity", "mode connectivity", "lottery ticket", "optimization landscape"], "paperhash": "frankle|mode_connectivity_and_sparse_neural_networks", "original_pdf": "/attachment/cdce07e15a495d5ee3031f3f177780e2dac4ccdd.pdf", "_bibtex": "@misc{\nfrankle2020mode,\ntitle={Mode Connectivity and Sparse Neural Networks},\nauthor={Jonathan Frankle and Gintare Karolina Dziugaite and Daniel M. Roy and Michael Carbin},\nyear={2020},\nurl={https://openreview.net/forum?id=rkeO-lrYwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkeO-lrYwr", "replyto": "rkeO-lrYwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2141/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2141/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575917060561, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2141/Reviewers"], "noninvitees": [], "tcdate": 1570237727118, "tmdate": 1575917060573, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2141/-/Official_Review"}}}, {"id": "HygFbYdatS", "original": null, "number": 2, "cdate": 1571813633272, "ddate": null, "tcdate": 1571813633272, "tmdate": 1572972377737, "tddate": null, "forum": "rkeO-lrYwr", "replyto": "rkeO-lrYwr", "invitation": "ICLR.cc/2020/Conference/Paper2141/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper empirically presents a very interesting connection between two also very interesting phenomena (mode connectivity and lottery ticket hypothesis), while removing a previous limitation of the lottery ticket hypothesis on larger networks. through a good amount of experiments, the authors empirically showed these two phenomena co-occur together (i.e. matching networks are stable) and have positive correlation (i.e. the more \u201cmatching\u201d the network the more \u201cstable\u201d), under different network architectures and datasets.\n\nThough it is unclear from the paper what are the immediate / straightforward applications, the findings do present interesting contributions. Several things I found that can further improve this paper.\n\nFirst, this paper lacks a structured literature review. It is suggested that the findings may provide insights to our understanding of how SGD works in neural network. Laying some proper background knowledge on this area is needed in the literature review. \n\nThere are several experiments that I\u2019m curious to see. Though I must say the existing amount of experiments sufficiently validates the existence of the connection authors put forth and hence not required.\n\na) Provide some metrics on how \u201cfar\u201d are the two final weights upon which mode connectivity (or stability) is explored. For the sake of comparison, distance between the initial weights and final weights can be added. \n\nb) First off, the introduction mentions connectivity was previously observed using \u201cdisjoint subsets\u201d of data, whereas later in the paper only different orders of the same data are explored. I wonder if this is a typo. Regardless, exploring if the findings still apply on disjoint data and/or varying amount of data, besides different data orders, is helpful. \n\nc) Does the full network have the property of mode connectivity (when trained using different data orders), or this only occurs under sparsity.\n\nLastly, the writing of the paper doesn\u2019t interfere with understanding, but can definitely use more work. Abstract can be tightened. Several typos throughout the paper:\n- in the abstract, \"with the no change\" -> remove \"the\"\n- bottom of page 1, subt -> sub\n- second bullet point under \"contributions\": remove \":\"?\n- page 3, paragraph starting with \"stability\": \"the increase in worst-case increase\" -> \"the worst-case increase\"?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2141/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2141/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jfrankle@csail.mit.edu", "karolina.dziugaite@gmail.com", "droy@utstat.toronto.edu", "mcarbin@csail.mit.edu"], "title": "Mode Connectivity and Sparse Neural Networks", "authors": ["Jonathan Frankle", "Gintare Karolina Dziugaite", "Daniel M. Roy", "Michael Carbin"], "pdf": "/pdf/415db8f7d8081842f3f179b64310700bc7e8d2cf.pdf", "TL;DR": "Whether or not a sparse subnetwork trains to the same accuracy of the full network depends on whether two runs of SGD on the subnetwork land in the same convex levelset.", "abstract": "We uncover a connection between two seemingly unrelated empirical phenomena: mode connectivity and sparsity. On the one hand, there is growing catalog of situations where, across multiple runs, SGD learns weights that fall into minima that are connected (mode connectivity). A striking example is described by Nagarajan & Kolter (2019). They observe that test error on MNIST does not change along the linear path connecting the end points of two independent SGD runs, starting from the same random initialization. On the other hand, there is the lottery ticket hypothesis of Frankle & Carbin (2019), where dense, randomly initialized networks have sparse subnetworks capable of training in isolation to full accuracy.\n\nHowever, neither phenomenon scales beyond small vision networks. We start by proposing a technique to find sparse subnetworks after initialization. We observe that these subnetworks match the accuracy of the full network only when two SGD runs for the same subnetwork are connected by linear paths with the no change in test error. Our findings connect the existence of sparse subnetworks that train to high accuracy with the dynamics of optimization via mode connectivity. In doing so, we identify analogues of the phenomena uncovered by Nagarajan & Kolter and Frankle & Carbin in ImageNet-scale architectures at state-of-the-art sparsity levels.", "keywords": ["sparsity", "mode connectivity", "lottery ticket", "optimization landscape"], "paperhash": "frankle|mode_connectivity_and_sparse_neural_networks", "original_pdf": "/attachment/cdce07e15a495d5ee3031f3f177780e2dac4ccdd.pdf", "_bibtex": "@misc{\nfrankle2020mode,\ntitle={Mode Connectivity and Sparse Neural Networks},\nauthor={Jonathan Frankle and Gintare Karolina Dziugaite and Daniel M. Roy and Michael Carbin},\nyear={2020},\nurl={https://openreview.net/forum?id=rkeO-lrYwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkeO-lrYwr", "replyto": "rkeO-lrYwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2141/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2141/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575917060561, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2141/Reviewers"], "noninvitees": [], "tcdate": 1570237727118, "tmdate": 1575917060573, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2141/-/Official_Review"}}}], "count": 11}