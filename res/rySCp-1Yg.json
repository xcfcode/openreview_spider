{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028554194, "tcdate": 1490028554194, "number": 1, "id": "S1zz_Fasl", "invitation": "ICLR.cc/2017/workshop/-/paper18/acceptance", "forum": "rySCp-1Yg", "replyto": "rySCp-1Yg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters", "abstract": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results.", "pdf": "/pdf/25e566ab65ca8d00a2721336fcb529445087a1f5.pdf", "TL;DR": "Bloom embeddings allow a compact and accurate representation of high-dimensional binary inputs and/or outputs", "paperhash": "serr\u00e0|compact_embedding_of_binarycoded_inputs_and_outputs_using_bloom_filters", "conflicts": ["telefonica.com"], "keywords": [], "authors": ["Joan Serr\u00e0", "Alexandros Karatzoglou"], "authorids": ["joan.serra@telefonica.com", "alexandros.karatzoglou@telefonica.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028554739, "id": "ICLR.cc/2017/workshop/-/paper18/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rySCp-1Yg", "replyto": "rySCp-1Yg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028554739}}}, {"tddate": null, "tmdate": 1489138383106, "tcdate": 1489138383106, "number": 2, "id": "H1w0zeejg", "invitation": "ICLR.cc/2017/workshop/-/paper18/public/comment", "forum": "rySCp-1Yg", "replyto": "rkQWTQJsl", "signatures": ["~Joan_Serr\u00e01"], "readers": ["everyone"], "writers": ["~Joan_Serr\u00e01"], "content": {"title": "Answer to \"Neat idea\"", "comment": "Thank you very much for your comments and feedback. As mentioned to the other reviewer, in the paper we now clearly state that there is an extended version of the paper, with full experimental section, that was previously submitted to the main conference track.\nRegarding the false positives idea, it somehow overlaps with the co-occurrence-based Bloom embedding that we propose in the Appendix of the extended version. However, it may be not exactly the same. We can refine that in future experiments. Thanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters", "abstract": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results.", "pdf": "/pdf/25e566ab65ca8d00a2721336fcb529445087a1f5.pdf", "TL;DR": "Bloom embeddings allow a compact and accurate representation of high-dimensional binary inputs and/or outputs", "paperhash": "serr\u00e0|compact_embedding_of_binarycoded_inputs_and_outputs_using_bloom_filters", "conflicts": ["telefonica.com"], "keywords": [], "authors": ["Joan Serr\u00e0", "Alexandros Karatzoglou"], "authorids": ["joan.serra@telefonica.com", "alexandros.karatzoglou@telefonica.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1486982605819, "tcdate": 1486982605819, "id": "ICLR.cc/2017/workshop/-/paper18/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper18/reviewers"], "reply": {"forum": "rySCp-1Yg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1486982605819}}}, {"tddate": null, "tmdate": 1489138328707, "tcdate": 1489138328707, "number": 1, "id": "B1ZjGlxox", "invitation": "ICLR.cc/2017/workshop/-/paper18/public/comment", "forum": "rySCp-1Yg", "replyto": "HJFov2a9g", "signatures": ["~Joan_Serr\u00e01"], "readers": ["everyone"], "writers": ["~Joan_Serr\u00e01"], "content": {"title": "Answer to \"More details?\"", "comment": "Thank you for your comments. \n\nThe Workshop submission contains a subset of the experimental results we presented to the main conference track (https://openreview.net/forum?id=rkKCdAdgx). We updated the PDF of the workshop manuscript to mention and link the extended version explicitly.\n\n1) Training time: Performing a Bloom embedding is a fast operation of O(ck), where c is the number of ones in the one-hot encoded instance (c<<d, d being the original instance dimensionality) and k is the number of hash functions used (typically k<10, and in our experiments k<=5). Running a hash function is O(1) and very fast, as it only involves a couple of calls to a uniform random number generator and a couple multiplications/divisions (furthermore, hash function results can be pre-calculated, so that it is then basically a single read operation). Testing time: Recovering the original ranking of elements is an O(dk) operation. Thus, only a marginal overhead of k reads is introduced. Notice furthermore that the embedding operation needs to be performed only once (offline) for both training and testing. We now include part of this discussion at the end of the results section.\n\n2) Details regarding the baseline models can be found in the extended version of the paper. The 3-page format of the workshop is certainly limiting.\n\n3) We are not sure about the mentioned hierarchical representations. If they refer to the hierarchical softmax, it should be noted that it improves the speed of operations but not the space to store the layer and, therefore, the network. Space is the main focus of our work, and becomes critical when dealing with million-sized one-hot encodings in both inputs and outputs. Regarding previous attempts, we are only aware of one work using Bloom filters, the cited Cisse et al work. In that work, as far as we understand, the output of a Bloom filter is used to define m independent binary classification tasks, the results of which are then cleverly combined to produce a more robust classification. In our work, we directly train the neural network model with the full output of the Bloom filter (computing gradients with respect to the full representation), and not separately in a binary classification scheme (note that learning d binary classifiers when d is in the range of thousands or millions is not feasible). Working with the compact full representation allows to tackle the network size problem we want to address in the first place. The accuracy gains in some data sets are a nice by-product of using the full output of the Bloom filter to compute the gradients (k times more ones in the instance).\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters", "abstract": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results.", "pdf": "/pdf/25e566ab65ca8d00a2721336fcb529445087a1f5.pdf", "TL;DR": "Bloom embeddings allow a compact and accurate representation of high-dimensional binary inputs and/or outputs", "paperhash": "serr\u00e0|compact_embedding_of_binarycoded_inputs_and_outputs_using_bloom_filters", "conflicts": ["telefonica.com"], "keywords": [], "authors": ["Joan Serr\u00e0", "Alexandros Karatzoglou"], "authorids": ["joan.serra@telefonica.com", "alexandros.karatzoglou@telefonica.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1486982605819, "tcdate": 1486982605819, "id": "ICLR.cc/2017/workshop/-/paper18/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper18/reviewers"], "reply": {"forum": "rySCp-1Yg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1486982605819}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1489137486086, "tcdate": 1486982605067, "number": 18, "id": "rySCp-1Yg", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "rySCp-1Yg", "original": "rkKCdAdgx", "signatures": ["~Joan_Serr\u00e01"], "readers": ["everyone"], "content": {"title": "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters", "abstract": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results.", "pdf": "/pdf/25e566ab65ca8d00a2721336fcb529445087a1f5.pdf", "TL;DR": "Bloom embeddings allow a compact and accurate representation of high-dimensional binary inputs and/or outputs", "paperhash": "serr\u00e0|compact_embedding_of_binarycoded_inputs_and_outputs_using_bloom_filters", "conflicts": ["telefonica.com"], "keywords": [], "authors": ["Joan Serr\u00e0", "Alexandros Karatzoglou"], "authorids": ["joan.serra@telefonica.com", "alexandros.karatzoglou@telefonica.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "original": {"tddate": null, "replyto": null, "ddate": null, "active": true, "tmdate": 1484230577196, "tcdate": 1478187217575, "number": 70, "id": "rkKCdAdgx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rkKCdAdgx", "signatures": ["~Joan_Serr\u00e01"], "readers": ["everyone"], "content": {"title": "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters", "abstract": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results. We also discuss a number of further advantages of Bloom embeddings, such as 'on-the-fly' constant-time operation, zero or marginal space requirements, training time speedups, or the fact that they do not require any change to the core model architecture or training configuration.", "pdf": "/pdf/0b678b2fadd02759acb9cf2d3a9bf6e83538f4ae.pdf", "TL;DR": "Bloom embeddings compactly represent sparse high-dimensional binary-coded instances without compromising accuracy", "paperhash": "serr\u00e0|compact_embedding_of_binarycoded_inputs_and_outputs_using_bloom_filters", "conflicts": ["telefonica.com"], "keywords": ["Applications", "Deep learning", "Unsupervised Learning"], "authors": ["Joan Serr\u00e0", "Alexandros Karatzoglou"], "authorids": ["joan.serra@telefonica.com", "alexandros.karatzoglou@telefonica.com"]}, "writers": [], "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "tmdate": 1489087738577, "tcdate": 1489087738577, "number": 2, "id": "rkQWTQJsl", "invitation": "ICLR.cc/2017/workshop/-/paper18/official/review", "forum": "rySCp-1Yg", "replyto": "rySCp-1Yg", "signatures": ["ICLR.cc/2017/workshop/paper18/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper18/AnonReviewer1"], "content": {"title": "Neat idea", "rating": "7: Good paper, accept", "review": "I quite like the idea, and I think it has some merit, but as the other reviewer mentioned it is hard to judge by the experimental section.\n\nI think it would be good to attach a story to this idea. You are already mentioning recommender systems; in those systems being able to identify false positives is often less important than false negatives. One wouldn't want to mix up Star Wars and Finding Nemo, but a false positive on Star Wars vs Star Trek wouldn't be so bad. In this sense having non-random (but overall uniformly distributed) hashes could be a nice addition/future work.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters", "abstract": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results.", "pdf": "/pdf/25e566ab65ca8d00a2721336fcb529445087a1f5.pdf", "TL;DR": "Bloom embeddings allow a compact and accurate representation of high-dimensional binary inputs and/or outputs", "paperhash": "serr\u00e0|compact_embedding_of_binarycoded_inputs_and_outputs_using_bloom_filters", "conflicts": ["telefonica.com"], "keywords": [], "authors": ["Joan Serr\u00e0", "Alexandros Karatzoglou"], "authorids": ["joan.serra@telefonica.com", "alexandros.karatzoglou@telefonica.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489087739272, "id": "ICLR.cc/2017/workshop/-/paper18/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper18/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper18/AnonReviewer2", "ICLR.cc/2017/workshop/paper18/AnonReviewer1"], "reply": {"forum": "rySCp-1Yg", "replyto": "rySCp-1Yg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper18/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper18/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489087739272}}}, {"tddate": null, "tmdate": 1488992161185, "tcdate": 1488992161185, "number": 1, "id": "HJFov2a9g", "invitation": "ICLR.cc/2017/workshop/-/paper18/official/review", "forum": "rySCp-1Yg", "replyto": "rySCp-1Yg", "signatures": ["ICLR.cc/2017/workshop/paper18/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper18/AnonReviewer2"], "content": {"title": "more details?", "rating": "6: Marginally above acceptance threshold", "review": "The paper proposes the use of bloom filters to generate embeddings\nsuitable for representing very sparse data for either input or output.\nThe authors experiment on a set of tasks and show that they can reduce the\nrepresentation size by a factor of 5 without significant performance\nreduction, and still recover calibrated probabilities.\n\nI am not sure I understood how much time needs to be traded for space.\n(maybe none? that would be good!) both for training and testing.\n\nThe experiment section is very compact, so it is hard to verify if the\nbaseline models are reasonable and compare to the state-of-the-art on\nthe selected 7 tasks (a lot of details are omitted, and I understand this\nis because it has to fit in 3 pages but still).\n\nIt would have also been interesting to compare to other related approaches\n(like hierarchical representations). I'm also not sure I understood how it\nrelates to previous attempts at using Bloom filters (like Cisse et al).", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters", "abstract": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results.", "pdf": "/pdf/25e566ab65ca8d00a2721336fcb529445087a1f5.pdf", "TL;DR": "Bloom embeddings allow a compact and accurate representation of high-dimensional binary inputs and/or outputs", "paperhash": "serr\u00e0|compact_embedding_of_binarycoded_inputs_and_outputs_using_bloom_filters", "conflicts": ["telefonica.com"], "keywords": [], "authors": ["Joan Serr\u00e0", "Alexandros Karatzoglou"], "authorids": ["joan.serra@telefonica.com", "alexandros.karatzoglou@telefonica.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489087739272, "id": "ICLR.cc/2017/workshop/-/paper18/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper18/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper18/AnonReviewer2", "ICLR.cc/2017/workshop/paper18/AnonReviewer1"], "reply": {"forum": "rySCp-1Yg", "replyto": "rySCp-1Yg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper18/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper18/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489087739272}}}], "count": 6}