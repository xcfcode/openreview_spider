{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028556609, "tcdate": 1490028556609, "number": 1, "id": "r1HGdKajg", "invitation": "ICLR.cc/2017/workshop/-/paper23/acceptance", "forum": "rJvPIReKx", "replyto": "rJvPIReKx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalization to new compositions of known entities in image understanding", "abstract": "Recurrent neural networks can be trained to describe images with natural language, but it has been observed that they generalize poorly to new scenes at test time.\nHere we provide an experimental framework to quantify their generalization to unseen compositions. By describing images using short structured representations, we tease apart and evaluate separately two types of generalization: (1) generalization to new images of similar scenes, and (2) generalization to unseen compositions of known entities. We quantify these two types of generalization by a large-scale experiment on the MS-COCO dataset with a state-of-the-art recurrent network, and compare to a baseline structured prediction model on top of a deep network. We find that  a state-of-the-art image captioning approach is largely \"blind\" to new combinations of known entities (~2.3% precision@1), and achieves statistically similar precision@1 to that of a considerably simpler structured-prediction model with much smaller capacity. We therefore advocate using compositional generalization metrics to evaluate vision and language models, since generalizing to new combinations of known entities is key for understanding complex real data.", "pdf": "/pdf/98190a22f77de28801ee5cb153f7cdeb53396574.pdf", "TL;DR": "For image captioning, we propose an experimental framework to evaluate generalization to unseen compositions of known entities, showing that state-of-the-art captioning approach generalize very poorly to new compositions.", "paperhash": "atzmon|generalization_to_new_compositions_of_known_entities_in_image_understanding", "keywords": ["Computer vision", "Natural language processing", "Deep learning", "Supervised Learning", "Transfer Learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["google.com", "tau.ac.il", "biu.ac.il"], "authors": ["Yuval Atzmon", "Jonathan Berant", "Amir Globerson", "Vahid Kazemi", "Gal Chechik"], "authorids": ["yuval.atzmon@biu.ac.il", "yonatansito@gmail.com", "amir.globerson@gmail.com", "vahid@google.com", "gal.chechik@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028557205, "id": "ICLR.cc/2017/workshop/-/paper23/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rJvPIReKx", "replyto": "rJvPIReKx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028557205}}}, {"tddate": null, "tmdate": 1489437193312, "tcdate": 1489437193312, "number": 2, "id": "r1bMGYNjl", "invitation": "ICLR.cc/2017/workshop/-/paper23/official/review", "forum": "rJvPIReKx", "replyto": "rJvPIReKx", "signatures": ["ICLR.cc/2017/workshop/paper23/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper23/AnonReviewer1"], "content": {"title": "A good target for further study but limited execution", "rating": "5: Marginally below acceptance threshold", "review": "This paper takes the MSCOCO captioning dataset, heuristically parses the sentences into Subject Relation Object (SRO) triplets similar to Farhadi et al 2010, and then evaluates the Show Attend and Tell and some baselines on two splits of the data: 1) a split where the Test set has the same SRO distribution but new images (standard setting) and 2) a split where the Test set contains SRO triplets where the individual pieces were seen in the training data, but the particular composition wasn\u2019t. The paper shows that the generalization performance is very poor on (2), almost as bad as a relatively weak SSVM baseline.\n\nThe paper is fairly well written, modulo some odd quirks like undefined terms, though I have some larger recommendations. First, this is not a model paper, it\u2019s more of a dataset/evaluation paper. Therefore, the related work should focus specifically on this aspect instead of broadly discussing the modeling approaches used in image captioning. Second, the description of the main SA&T model is slightly too short and it\u2019s not clear how the SRO triplets are predicted. Is the LSTM emitting S,R,O in that order, pretending these are sentences of 3 words? The decoders are also using soft attention. I think I know what that means, but this could be clarified.\n\nIn terms of the results, at some level I\u2019m not too surprised about the results because the LSTM is trained to model the joint distribution over SRO triplets, so it might be reluctant to predict e.g. an O given S,R that never occurred in the training data. Of course, one would like the model to learn the true underlying function from images to labels without relying on the more shallow dependencies of the labels, and it appears that the model struggles here, for this many datapoints. This has been qualitatively noted in the literature before, but I\u2019m not aware of anyone who studied this more quantitatively and the related work doesn\u2019t cover this well. For example, VQA also struggles with this problem, which also partly motivated VQA 2.0. I\u2019m also aware of CLEVR, which proposed to study these problems in a synthetic setting, etc. Another interesting baseline here might be the same SA&T model, but feeding in all zero images, which would be a more controlled comparison, to see just how much the images are used, and to what extent it\u2019s just the joint distribution modeling.\n\nIn conclusions, this is one approach to measuring a problem that has been noted by the community. The SA&T model and a few very simple baselines are evaluated, but the conclusion isn\u2019t particularly striking and the analysis isn\u2019t particularly deep and therefore there is not too much to take away.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalization to new compositions of known entities in image understanding", "abstract": "Recurrent neural networks can be trained to describe images with natural language, but it has been observed that they generalize poorly to new scenes at test time.\nHere we provide an experimental framework to quantify their generalization to unseen compositions. By describing images using short structured representations, we tease apart and evaluate separately two types of generalization: (1) generalization to new images of similar scenes, and (2) generalization to unseen compositions of known entities. We quantify these two types of generalization by a large-scale experiment on the MS-COCO dataset with a state-of-the-art recurrent network, and compare to a baseline structured prediction model on top of a deep network. We find that  a state-of-the-art image captioning approach is largely \"blind\" to new combinations of known entities (~2.3% precision@1), and achieves statistically similar precision@1 to that of a considerably simpler structured-prediction model with much smaller capacity. We therefore advocate using compositional generalization metrics to evaluate vision and language models, since generalizing to new combinations of known entities is key for understanding complex real data.", "pdf": "/pdf/98190a22f77de28801ee5cb153f7cdeb53396574.pdf", "TL;DR": "For image captioning, we propose an experimental framework to evaluate generalization to unseen compositions of known entities, showing that state-of-the-art captioning approach generalize very poorly to new compositions.", "paperhash": "atzmon|generalization_to_new_compositions_of_known_entities_in_image_understanding", "keywords": ["Computer vision", "Natural language processing", "Deep learning", "Supervised Learning", "Transfer Learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["google.com", "tau.ac.il", "biu.ac.il"], "authors": ["Yuval Atzmon", "Jonathan Berant", "Amir Globerson", "Vahid Kazemi", "Gal Chechik"], "authorids": ["yuval.atzmon@biu.ac.il", "yonatansito@gmail.com", "amir.globerson@gmail.com", "vahid@google.com", "gal.chechik@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489437194027, "id": "ICLR.cc/2017/workshop/-/paper23/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper23/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper23/AnonReviewer2", "ICLR.cc/2017/workshop/paper23/AnonReviewer1"], "reply": {"forum": "rJvPIReKx", "replyto": "rJvPIReKx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper23/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper23/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489437194027}}}, {"tddate": null, "nonreaders": null, "tmdate": 1488079078981, "tcdate": 1488077879322, "number": 1, "id": "Bk1rV6J5e", "invitation": "ICLR.cc/2017/workshop/-/paper23/official/review", "forum": "rJvPIReKx", "replyto": "rJvPIReKx", "signatures": ["ICLR.cc/2017/workshop/paper23/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper23/AnonReviewer2"], "content": {"title": "Interesting direction, but limited novelty and conclusion", "rating": "5: Marginally below acceptance threshold", "review": "This paper studies how much compositionality approaches for image captioning have. The experiments compare two scenarios: on the standard COCO test split (which has similar compositions, but novel images), and on a new split which does not contain known components in a novel composition. Specifically, the paper parses coco captions into triplets of subject \u2013 relation \u2013 object and evaluates these rather than actual captions.\n\nStrength: \n-\tThe paper has a clear exposition and motivation.\n-\tThe paper studies an interesting scenario of how compositional approaches are for image captioning.\n\nMain Weaknesses:\n1.\tThe result suggest that the studied approaches/baselines cannot handle the scenario of novel triplets, i.e. have low compositionality. According to the authors expositions this was already known beforehand.\n2.\tThe specific experimental setup does not allow to study how this affects the actual generation of sentence, e.g. there might be some approaches which are very good at novel triplet prediction, but do not solve the problem of sentence generation.\n3.\tThe paper does not propose any novel approach and only studies *one* captioning approach (Show attend and tell), although the text sometimes suggests that there are multiple.\n4.\tFrom a learning perspective the studied model, show-attend-and-tell, does exactly what it is trained for: It predicts very low probability for tuples which have very low joint probability according to the training data. The LSTM learns exactly this aspect [similar the pair-wise terms in the SSVM model]. If one does not want to exploit the joint probabilities, one should just look at the unaries.\n5.\tThe paper misses to provide any insight how this know problem could be approached.\n6.\tPlease also provide precision@k for each of the components of, i.e. separately for S, R, and O to understand better where the problem of joint task originates from (in both settings: coco split and compositional split).\n\n\n\nFurther Weaknesses\n7.\tIt would be interesting and important to know, how well a model does which does not have pair-wise probabilities, e.g. only f_S, f_O, and f_R, but not f_SR and f_RO.\n8.\tPlease provide a better definition how precision@k is computed: does it mean if for @1 that it is on one if the highest ranked triplet matches *any* of the ground truth triplets?\n9.\tSpace can probably be saved by removing Figure 2, and only reporting @1, @5, @10 in a table. \n10.\tWould the precision@k evaluation not be better to be cumulative?\n11.\tWhat does the \u201cConv\u201d stand for in SSVM/Conv?\n12.\tLast sentence Sec. 3.3: Does this really apply only to SSVM or to all models?\n13.\tWhy has the MF 0% on the coco split? What percentage does it have on the training coco split? If it really is always very low, maybe this can be removed to save space, and only mention this at one point in the text.\n14.\tSection 1: what is \u201copen-IE\u201d?\n15. Please cite the actual publications not the arXives, whenever available.\n\n\n\nConclusion:\nWhile the approached problem is interesting and relevant, the paper does not propose any novel approach, but rather examines only a single captioning approach with a negative result and no conclusion where to go from here.\nCombined with many unclarities mentioned above, I lean towards rejecting this paper.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalization to new compositions of known entities in image understanding", "abstract": "Recurrent neural networks can be trained to describe images with natural language, but it has been observed that they generalize poorly to new scenes at test time.\nHere we provide an experimental framework to quantify their generalization to unseen compositions. By describing images using short structured representations, we tease apart and evaluate separately two types of generalization: (1) generalization to new images of similar scenes, and (2) generalization to unseen compositions of known entities. We quantify these two types of generalization by a large-scale experiment on the MS-COCO dataset with a state-of-the-art recurrent network, and compare to a baseline structured prediction model on top of a deep network. We find that  a state-of-the-art image captioning approach is largely \"blind\" to new combinations of known entities (~2.3% precision@1), and achieves statistically similar precision@1 to that of a considerably simpler structured-prediction model with much smaller capacity. We therefore advocate using compositional generalization metrics to evaluate vision and language models, since generalizing to new combinations of known entities is key for understanding complex real data.", "pdf": "/pdf/98190a22f77de28801ee5cb153f7cdeb53396574.pdf", "TL;DR": "For image captioning, we propose an experimental framework to evaluate generalization to unseen compositions of known entities, showing that state-of-the-art captioning approach generalize very poorly to new compositions.", "paperhash": "atzmon|generalization_to_new_compositions_of_known_entities_in_image_understanding", "keywords": ["Computer vision", "Natural language processing", "Deep learning", "Supervised Learning", "Transfer Learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["google.com", "tau.ac.il", "biu.ac.il"], "authors": ["Yuval Atzmon", "Jonathan Berant", "Amir Globerson", "Vahid Kazemi", "Gal Chechik"], "authorids": ["yuval.atzmon@biu.ac.il", "yonatansito@gmail.com", "amir.globerson@gmail.com", "vahid@google.com", "gal.chechik@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489437194027, "id": "ICLR.cc/2017/workshop/-/paper23/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper23/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper23/AnonReviewer2", "ICLR.cc/2017/workshop/paper23/AnonReviewer1"], "reply": {"forum": "rJvPIReKx", "replyto": "rJvPIReKx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper23/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper23/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489437194027}}}, {"tddate": null, "replyto": null, "nonreaders": null, "ddate": null, "tmdate": 1487147808564, "tcdate": 1487099486957, "number": 23, "id": "rJvPIReKx", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "rJvPIReKx", "signatures": ["~Yuval_Atzmon1"], "readers": ["everyone"], "content": {"title": "Generalization to new compositions of known entities in image understanding", "abstract": "Recurrent neural networks can be trained to describe images with natural language, but it has been observed that they generalize poorly to new scenes at test time.\nHere we provide an experimental framework to quantify their generalization to unseen compositions. By describing images using short structured representations, we tease apart and evaluate separately two types of generalization: (1) generalization to new images of similar scenes, and (2) generalization to unseen compositions of known entities. We quantify these two types of generalization by a large-scale experiment on the MS-COCO dataset with a state-of-the-art recurrent network, and compare to a baseline structured prediction model on top of a deep network. We find that  a state-of-the-art image captioning approach is largely \"blind\" to new combinations of known entities (~2.3% precision@1), and achieves statistically similar precision@1 to that of a considerably simpler structured-prediction model with much smaller capacity. We therefore advocate using compositional generalization metrics to evaluate vision and language models, since generalizing to new combinations of known entities is key for understanding complex real data.", "pdf": "/pdf/98190a22f77de28801ee5cb153f7cdeb53396574.pdf", "TL;DR": "For image captioning, we propose an experimental framework to evaluate generalization to unseen compositions of known entities, showing that state-of-the-art captioning approach generalize very poorly to new compositions.", "paperhash": "atzmon|generalization_to_new_compositions_of_known_entities_in_image_understanding", "keywords": ["Computer vision", "Natural language processing", "Deep learning", "Supervised Learning", "Transfer Learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["google.com", "tau.ac.il", "biu.ac.il"], "authors": ["Yuval Atzmon", "Jonathan Berant", "Amir Globerson", "Vahid Kazemi", "Gal Chechik"], "authorids": ["yuval.atzmon@biu.ac.il", "yonatansito@gmail.com", "amir.globerson@gmail.com", "vahid@google.com", "gal.chechik@gmail.com"]}, "writers": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 4}