{"notes": [{"id": "HyMRUiC9YX", "original": "B1egmPLKt7", "number": 221, "cdate": 1538087765961, "ddate": null, "tcdate": 1538087765961, "tmdate": 1545355433600, "tddate": null, "forum": "HyMRUiC9YX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Exploring and Enhancing the Transferability of Adversarial Examples", "abstract": " State-of-the-art deep neural networks are vulnerable to adversarial examples, formed by applying small but malicious perturbations to the original inputs. Moreover, the perturbations can \\textit{transfer across models}: adversarial examples generated for a specific model will often mislead other unseen models. Consequently the adversary can leverage it to attack deployed systems without any query, which severely hinders the application of deep learning, especially in the safety-critical areas. In this work, we empirically study how two classes of factors those might influence the transferability of adversarial examples.  One is about model-specific factors, including network architecture, model capacity and test accuracy. The other is the local smoothness of loss surface for constructing adversarial examples.  Inspired by  these understandings on the transferability of adversarial examples,  we then propose a simple but effective strategy  to enhance the transferability, whose effectiveness is confirmed by a variety of experiments on both  CIFAR-10 and ImageNet datasets.", "keywords": ["Deep learning", "Adversarial example", "Transferability", "Smoothed gradient"], "authorids": ["leiwu@pku.edu.cn", "zhanxing.zhu@pku.edu.cn", "chengtai@pku.edu.cn"], "authors": ["Lei Wu", "Zhanxing Zhu", "Cheng Tai"], "pdf": "/pdf/68bc7c478b44556282255089d05bb096362bb06c.pdf", "paperhash": "wu|exploring_and_enhancing_the_transferability_of_adversarial_examples", "_bibtex": "@misc{\nwu2019exploring,\ntitle={Exploring and Enhancing the Transferability of Adversarial Examples},\nauthor={Lei Wu and Zhanxing Zhu and Cheng Tai},\nyear={2019},\nurl={https://openreview.net/forum?id=HyMRUiC9YX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BkxTdp4SeE", "original": null, "number": 1, "cdate": 1545059701329, "ddate": null, "tcdate": 1545059701329, "tmdate": 1545354483036, "tddate": null, "forum": "HyMRUiC9YX", "replyto": "HyMRUiC9YX", "invitation": "ICLR.cc/2019/Conference/-/Paper221/Meta_Review", "content": {"metareview": "While the paper contains significant information, most insights have already been revealed in previous work as noted by R1. \nThe empirical novelty is therefore limited and the authors do not provide theoretical analysis to complement this.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Needs improvements."}, "signatures": ["ICLR.cc/2019/Conference/Paper221/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper221/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring and Enhancing the Transferability of Adversarial Examples", "abstract": " State-of-the-art deep neural networks are vulnerable to adversarial examples, formed by applying small but malicious perturbations to the original inputs. Moreover, the perturbations can \\textit{transfer across models}: adversarial examples generated for a specific model will often mislead other unseen models. Consequently the adversary can leverage it to attack deployed systems without any query, which severely hinders the application of deep learning, especially in the safety-critical areas. In this work, we empirically study how two classes of factors those might influence the transferability of adversarial examples.  One is about model-specific factors, including network architecture, model capacity and test accuracy. The other is the local smoothness of loss surface for constructing adversarial examples.  Inspired by  these understandings on the transferability of adversarial examples,  we then propose a simple but effective strategy  to enhance the transferability, whose effectiveness is confirmed by a variety of experiments on both  CIFAR-10 and ImageNet datasets.", "keywords": ["Deep learning", "Adversarial example", "Transferability", "Smoothed gradient"], "authorids": ["leiwu@pku.edu.cn", "zhanxing.zhu@pku.edu.cn", "chengtai@pku.edu.cn"], "authors": ["Lei Wu", "Zhanxing Zhu", "Cheng Tai"], "pdf": "/pdf/68bc7c478b44556282255089d05bb096362bb06c.pdf", "paperhash": "wu|exploring_and_enhancing_the_transferability_of_adversarial_examples", "_bibtex": "@misc{\nwu2019exploring,\ntitle={Exploring and Enhancing the Transferability of Adversarial Examples},\nauthor={Lei Wu and Zhanxing Zhu and Cheng Tai},\nyear={2019},\nurl={https://openreview.net/forum?id=HyMRUiC9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper221/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353293247, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyMRUiC9YX", "replyto": "HyMRUiC9YX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper221/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper221/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper221/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353293247}}}, {"id": "HyeuTWOE1E", "original": null, "number": 5, "cdate": 1543958975579, "ddate": null, "tcdate": 1543958975579, "tmdate": 1543958975579, "tddate": null, "forum": "HyMRUiC9YX", "replyto": "BJe1SGwW1E", "invitation": "ICLR.cc/2019/Conference/-/Paper221/Official_Comment", "content": {"title": "Moosavi-Dezfooli 2017 did not study the transferability of adversarial examples", "comment": "Thanks for the reference, and we will add discussions about universal adversarial perturbations. However, in a nutshell, Moosavi-Dezfooli 2017 did not study the \u201ctransferability of adversarial examples\u201d.\n\nMossavi-Dezfooli 2017 showed the existence of a \u201cuniversal\u201d (image-agnostic) perturbation that causes most of the nature images to be misclassified. This is very different from the \u201ctransferability\u201d addressed in our paper in two aspects. \n\n(1) The \u201ctransferability of adversarial examples\u201d actually refers to the cross-model generalization, i.e. perturbations generated for one specific model can also fool the other unseen models. The \u201cuniversal perturbations\u201d studied in Mossavi-Dezfooli  2017, however, refers to the cross-image generalization, which means one perturbation can lead to misclassify most of the natural image simultaneously. \n\n(2) For adversarial transferability, we don\u2019t need to generate adversarial examples that can fool a large number of models simultaneously. In practice, usually one model is enough to guarantee transferability.  Fooling a model ensemble can enhance the transferability, but it is not necessary.  In contrast, according to Figure 6 in Moosavi-Dezfooli 2017, at least thousands of training images  are required to guarantee the cross-image generalization, i.e. the universality talked in the paper. To some extent, the later can be understood in the framework of statistical learning theory, however the former can not be .  \n\nIn the experiments, Moosavi-Desfooli 2017 did demonstrate that the universal perturbations can transfer among different models. However they didn\u2019t study the issue."}, "signatures": ["ICLR.cc/2019/Conference/Paper221/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper221/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper221/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring and Enhancing the Transferability of Adversarial Examples", "abstract": " State-of-the-art deep neural networks are vulnerable to adversarial examples, formed by applying small but malicious perturbations to the original inputs. Moreover, the perturbations can \\textit{transfer across models}: adversarial examples generated for a specific model will often mislead other unseen models. Consequently the adversary can leverage it to attack deployed systems without any query, which severely hinders the application of deep learning, especially in the safety-critical areas. In this work, we empirically study how two classes of factors those might influence the transferability of adversarial examples.  One is about model-specific factors, including network architecture, model capacity and test accuracy. The other is the local smoothness of loss surface for constructing adversarial examples.  Inspired by  these understandings on the transferability of adversarial examples,  we then propose a simple but effective strategy  to enhance the transferability, whose effectiveness is confirmed by a variety of experiments on both  CIFAR-10 and ImageNet datasets.", "keywords": ["Deep learning", "Adversarial example", "Transferability", "Smoothed gradient"], "authorids": ["leiwu@pku.edu.cn", "zhanxing.zhu@pku.edu.cn", "chengtai@pku.edu.cn"], "authors": ["Lei Wu", "Zhanxing Zhu", "Cheng Tai"], "pdf": "/pdf/68bc7c478b44556282255089d05bb096362bb06c.pdf", "paperhash": "wu|exploring_and_enhancing_the_transferability_of_adversarial_examples", "_bibtex": "@misc{\nwu2019exploring,\ntitle={Exploring and Enhancing the Transferability of Adversarial Examples},\nauthor={Lei Wu and Zhanxing Zhu and Cheng Tai},\nyear={2019},\nurl={https://openreview.net/forum?id=HyMRUiC9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper221/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610692, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyMRUiC9YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper221/Authors", "ICLR.cc/2019/Conference/Paper221/Reviewers", "ICLR.cc/2019/Conference/Paper221/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper221/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper221/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper221/Authors|ICLR.cc/2019/Conference/Paper221/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper221/Reviewers", "ICLR.cc/2019/Conference/Paper221/Authors", "ICLR.cc/2019/Conference/Paper221/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610692}}}, {"id": "BJe1SGwW1E", "original": null, "number": 4, "cdate": 1543758391124, "ddate": null, "tcdate": 1543758391124, "tmdate": 1543758391124, "tddate": null, "forum": "HyMRUiC9YX", "replyto": "BJgtK89FCX", "invitation": "ICLR.cc/2019/Conference/-/Paper221/Official_Comment", "content": {"title": "Response on adversarial transferability", "comment": "Thanks for the answer.\nNote that  the paper \"Universal adversarial perturbations\" by Moosavi-Dezfooli et al. (Oral CVPR 2017)  \nhttps://arxiv.org/pdf/1610.08401.pdf\nThe paper is rather popular (got more than 240 citations) and addresses also the problem of transferability and generalization, other related references citing this paper can be found.\nThis paper deserves to be cited and by larger and more general study (experimental study provided by authors is actually large), I would expect more discussion and comparison about existing work."}, "signatures": ["ICLR.cc/2019/Conference/Paper221/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper221/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper221/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring and Enhancing the Transferability of Adversarial Examples", "abstract": " State-of-the-art deep neural networks are vulnerable to adversarial examples, formed by applying small but malicious perturbations to the original inputs. Moreover, the perturbations can \\textit{transfer across models}: adversarial examples generated for a specific model will often mislead other unseen models. Consequently the adversary can leverage it to attack deployed systems without any query, which severely hinders the application of deep learning, especially in the safety-critical areas. In this work, we empirically study how two classes of factors those might influence the transferability of adversarial examples.  One is about model-specific factors, including network architecture, model capacity and test accuracy. The other is the local smoothness of loss surface for constructing adversarial examples.  Inspired by  these understandings on the transferability of adversarial examples,  we then propose a simple but effective strategy  to enhance the transferability, whose effectiveness is confirmed by a variety of experiments on both  CIFAR-10 and ImageNet datasets.", "keywords": ["Deep learning", "Adversarial example", "Transferability", "Smoothed gradient"], "authorids": ["leiwu@pku.edu.cn", "zhanxing.zhu@pku.edu.cn", "chengtai@pku.edu.cn"], "authors": ["Lei Wu", "Zhanxing Zhu", "Cheng Tai"], "pdf": "/pdf/68bc7c478b44556282255089d05bb096362bb06c.pdf", "paperhash": "wu|exploring_and_enhancing_the_transferability_of_adversarial_examples", "_bibtex": "@misc{\nwu2019exploring,\ntitle={Exploring and Enhancing the Transferability of Adversarial Examples},\nauthor={Lei Wu and Zhanxing Zhu and Cheng Tai},\nyear={2019},\nurl={https://openreview.net/forum?id=HyMRUiC9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper221/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610692, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyMRUiC9YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper221/Authors", "ICLR.cc/2019/Conference/Paper221/Reviewers", "ICLR.cc/2019/Conference/Paper221/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper221/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper221/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper221/Authors|ICLR.cc/2019/Conference/Paper221/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper221/Reviewers", "ICLR.cc/2019/Conference/Paper221/Authors", "ICLR.cc/2019/Conference/Paper221/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610692}}}, {"id": "HkliEwcY07", "original": null, "number": 3, "cdate": 1543247666924, "ddate": null, "tcdate": 1543247666924, "tmdate": 1543247666924, "tddate": null, "forum": "HyMRUiC9YX", "replyto": "HkehbJYgnQ", "invitation": "ICLR.cc/2019/Conference/-/Paper221/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thank you for approving our contribution to understanding the transferability of adversarial examples.\n \nWe agree with that the smoothing gradient idea, especially the Gaussian smoothing technique, is not new, since the smoothing strategy could be used in many different scenarios. However, we motivate and derive the idea of smoothing the gradient based on our novel understanding on the transferability of adversarial examples between two models. To the best of knowledge, we are the first to derive and apply this technique to enhance the transferability of adversarial examples, whose significant improvement is also confirmed by our intensive experiments."}, "signatures": ["ICLR.cc/2019/Conference/Paper221/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper221/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper221/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring and Enhancing the Transferability of Adversarial Examples", "abstract": " State-of-the-art deep neural networks are vulnerable to adversarial examples, formed by applying small but malicious perturbations to the original inputs. Moreover, the perturbations can \\textit{transfer across models}: adversarial examples generated for a specific model will often mislead other unseen models. Consequently the adversary can leverage it to attack deployed systems without any query, which severely hinders the application of deep learning, especially in the safety-critical areas. In this work, we empirically study how two classes of factors those might influence the transferability of adversarial examples.  One is about model-specific factors, including network architecture, model capacity and test accuracy. The other is the local smoothness of loss surface for constructing adversarial examples.  Inspired by  these understandings on the transferability of adversarial examples,  we then propose a simple but effective strategy  to enhance the transferability, whose effectiveness is confirmed by a variety of experiments on both  CIFAR-10 and ImageNet datasets.", "keywords": ["Deep learning", "Adversarial example", "Transferability", "Smoothed gradient"], "authorids": ["leiwu@pku.edu.cn", "zhanxing.zhu@pku.edu.cn", "chengtai@pku.edu.cn"], "authors": ["Lei Wu", "Zhanxing Zhu", "Cheng Tai"], "pdf": "/pdf/68bc7c478b44556282255089d05bb096362bb06c.pdf", "paperhash": "wu|exploring_and_enhancing_the_transferability_of_adversarial_examples", "_bibtex": "@misc{\nwu2019exploring,\ntitle={Exploring and Enhancing the Transferability of Adversarial Examples},\nauthor={Lei Wu and Zhanxing Zhu and Cheng Tai},\nyear={2019},\nurl={https://openreview.net/forum?id=HyMRUiC9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper221/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610692, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyMRUiC9YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper221/Authors", "ICLR.cc/2019/Conference/Paper221/Reviewers", "ICLR.cc/2019/Conference/Paper221/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper221/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper221/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper221/Authors|ICLR.cc/2019/Conference/Paper221/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper221/Reviewers", "ICLR.cc/2019/Conference/Paper221/Authors", "ICLR.cc/2019/Conference/Paper221/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610692}}}, {"id": "r1ltev9YCQ", "original": null, "number": 2, "cdate": 1543247601212, "ddate": null, "tcdate": 1543247601212, "tmdate": 1543247601212, "tddate": null, "forum": "HyMRUiC9YX", "replyto": "SJg6llgw2X", "invitation": "ICLR.cc/2019/Conference/-/Paper221/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thank you for the appreciation on the novelty of our paper.\n\n- We have carefully proofread the manuscript and fixed the typos in the revised version. Could reviewer be more specific about the odd formulations, so that we can improve them?"}, "signatures": ["ICLR.cc/2019/Conference/Paper221/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper221/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper221/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring and Enhancing the Transferability of Adversarial Examples", "abstract": " State-of-the-art deep neural networks are vulnerable to adversarial examples, formed by applying small but malicious perturbations to the original inputs. Moreover, the perturbations can \\textit{transfer across models}: adversarial examples generated for a specific model will often mislead other unseen models. Consequently the adversary can leverage it to attack deployed systems without any query, which severely hinders the application of deep learning, especially in the safety-critical areas. In this work, we empirically study how two classes of factors those might influence the transferability of adversarial examples.  One is about model-specific factors, including network architecture, model capacity and test accuracy. The other is the local smoothness of loss surface for constructing adversarial examples.  Inspired by  these understandings on the transferability of adversarial examples,  we then propose a simple but effective strategy  to enhance the transferability, whose effectiveness is confirmed by a variety of experiments on both  CIFAR-10 and ImageNet datasets.", "keywords": ["Deep learning", "Adversarial example", "Transferability", "Smoothed gradient"], "authorids": ["leiwu@pku.edu.cn", "zhanxing.zhu@pku.edu.cn", "chengtai@pku.edu.cn"], "authors": ["Lei Wu", "Zhanxing Zhu", "Cheng Tai"], "pdf": "/pdf/68bc7c478b44556282255089d05bb096362bb06c.pdf", "paperhash": "wu|exploring_and_enhancing_the_transferability_of_adversarial_examples", "_bibtex": "@misc{\nwu2019exploring,\ntitle={Exploring and Enhancing the Transferability of Adversarial Examples},\nauthor={Lei Wu and Zhanxing Zhu and Cheng Tai},\nyear={2019},\nurl={https://openreview.net/forum?id=HyMRUiC9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper221/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610692, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyMRUiC9YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper221/Authors", "ICLR.cc/2019/Conference/Paper221/Reviewers", "ICLR.cc/2019/Conference/Paper221/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper221/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper221/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper221/Authors|ICLR.cc/2019/Conference/Paper221/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper221/Reviewers", "ICLR.cc/2019/Conference/Paper221/Authors", "ICLR.cc/2019/Conference/Paper221/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610692}}}, {"id": "BJgtK89FCX", "original": null, "number": 1, "cdate": 1543247489115, "ddate": null, "tcdate": 1543247489115, "tmdate": 1543247489115, "tddate": null, "forum": "HyMRUiC9YX", "replyto": "Bkl4MaqOnm", "invitation": "ICLR.cc/2019/Conference/-/Paper221/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Thank you for the comments. We provide the feedbacks below.\n\n- Indeed there are a huge number of papers on adversarial examples, but specifically only a small fraction  of them are about the  transferability of adversarial examples . Understanding why adversarial examples can transfer from one model to another model is a much harder problem, which is a rather unexplored area. Indeed we do not provide a perfect explanation in this submission, however the factors we have considered and the well-designed numerical investigations could be helpful for future studies on this topic. In addition, the works Fawzi'15 and Athalye'18 did not talk about the issue of adversarial transferability.\n\n- Could you be more specific on what do you expect for \"larger studies\" and \"general study\u201d?  This will be helpful for improving our work."}, "signatures": ["ICLR.cc/2019/Conference/Paper221/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper221/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper221/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring and Enhancing the Transferability of Adversarial Examples", "abstract": " State-of-the-art deep neural networks are vulnerable to adversarial examples, formed by applying small but malicious perturbations to the original inputs. Moreover, the perturbations can \\textit{transfer across models}: adversarial examples generated for a specific model will often mislead other unseen models. Consequently the adversary can leverage it to attack deployed systems without any query, which severely hinders the application of deep learning, especially in the safety-critical areas. In this work, we empirically study how two classes of factors those might influence the transferability of adversarial examples.  One is about model-specific factors, including network architecture, model capacity and test accuracy. The other is the local smoothness of loss surface for constructing adversarial examples.  Inspired by  these understandings on the transferability of adversarial examples,  we then propose a simple but effective strategy  to enhance the transferability, whose effectiveness is confirmed by a variety of experiments on both  CIFAR-10 and ImageNet datasets.", "keywords": ["Deep learning", "Adversarial example", "Transferability", "Smoothed gradient"], "authorids": ["leiwu@pku.edu.cn", "zhanxing.zhu@pku.edu.cn", "chengtai@pku.edu.cn"], "authors": ["Lei Wu", "Zhanxing Zhu", "Cheng Tai"], "pdf": "/pdf/68bc7c478b44556282255089d05bb096362bb06c.pdf", "paperhash": "wu|exploring_and_enhancing_the_transferability_of_adversarial_examples", "_bibtex": "@misc{\nwu2019exploring,\ntitle={Exploring and Enhancing the Transferability of Adversarial Examples},\nauthor={Lei Wu and Zhanxing Zhu and Cheng Tai},\nyear={2019},\nurl={https://openreview.net/forum?id=HyMRUiC9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper221/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610692, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyMRUiC9YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper221/Authors", "ICLR.cc/2019/Conference/Paper221/Reviewers", "ICLR.cc/2019/Conference/Paper221/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper221/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper221/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper221/Authors|ICLR.cc/2019/Conference/Paper221/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper221/Reviewers", "ICLR.cc/2019/Conference/Paper221/Authors", "ICLR.cc/2019/Conference/Paper221/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610692}}}, {"id": "Bkl4MaqOnm", "original": null, "number": 3, "cdate": 1541086476083, "ddate": null, "tcdate": 1541086476083, "tmdate": 1541534182965, "tddate": null, "forum": "HyMRUiC9YX", "replyto": "HyMRUiC9YX", "invitation": "ICLR.cc/2019/Conference/-/Paper221/Official_Review", "content": {"title": "Paper that presents an experimental study of adversarial examples transferability with a contribution based on loss smoothing. The study is interesting with good illustrations and potential improvements but seems a bit limited, the conclusions are rather expected or unsurprising.", "review": "This paper addresses the problem of adversarial transferability, i.e. the ability that an adversarial example generated by one model can successfully fool another model. There are numerous papers on this topic recently, such as Fawzi'15, Liu'17, Dong'18, Athalye'18...\nThe authors propose tot study two types of factors that might influence transferability: model-specific parameters and smoothness of loss surface for constructing adversarial examples. Two experimental studies are made for each influence factor from existing architectures. Another attack strategy aiming at smoothing the loss surface is proposed, an experimental evaluation shows the effectiveness of the proposed method.\n\nPros\n-the proposed experimental studies can be interesting to the community\n-many interesting illustrations are provided.\n\nCons\n-The conclusions of the study were suggested by previous papers or are rather expected: adversarial transfer is not symmetric: Deep models less transferable than shallow ones, averaging gradient is better\n-I find the experimental studies a bit limited and I would expect larger studies which would have improve the interest of the paper.\n-Only two influence factors are studied, again the paper would be more interesting with a more general study\n\nThe paper has an interesting potential but seems a bit limited in its present form.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper221/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring and Enhancing the Transferability of Adversarial Examples", "abstract": " State-of-the-art deep neural networks are vulnerable to adversarial examples, formed by applying small but malicious perturbations to the original inputs. Moreover, the perturbations can \\textit{transfer across models}: adversarial examples generated for a specific model will often mislead other unseen models. Consequently the adversary can leverage it to attack deployed systems without any query, which severely hinders the application of deep learning, especially in the safety-critical areas. In this work, we empirically study how two classes of factors those might influence the transferability of adversarial examples.  One is about model-specific factors, including network architecture, model capacity and test accuracy. The other is the local smoothness of loss surface for constructing adversarial examples.  Inspired by  these understandings on the transferability of adversarial examples,  we then propose a simple but effective strategy  to enhance the transferability, whose effectiveness is confirmed by a variety of experiments on both  CIFAR-10 and ImageNet datasets.", "keywords": ["Deep learning", "Adversarial example", "Transferability", "Smoothed gradient"], "authorids": ["leiwu@pku.edu.cn", "zhanxing.zhu@pku.edu.cn", "chengtai@pku.edu.cn"], "authors": ["Lei Wu", "Zhanxing Zhu", "Cheng Tai"], "pdf": "/pdf/68bc7c478b44556282255089d05bb096362bb06c.pdf", "paperhash": "wu|exploring_and_enhancing_the_transferability_of_adversarial_examples", "_bibtex": "@misc{\nwu2019exploring,\ntitle={Exploring and Enhancing the Transferability of Adversarial Examples},\nauthor={Lei Wu and Zhanxing Zhu and Cheng Tai},\nyear={2019},\nurl={https://openreview.net/forum?id=HyMRUiC9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper221/Official_Review", "cdate": 1542234511460, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyMRUiC9YX", "replyto": "HyMRUiC9YX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper221/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335675581, "tmdate": 1552335675581, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper221/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJg6llgw2X", "original": null, "number": 2, "cdate": 1540976629248, "ddate": null, "tcdate": 1540976629248, "tmdate": 1541534182760, "tddate": null, "forum": "HyMRUiC9YX", "replyto": "HyMRUiC9YX", "invitation": "ICLR.cc/2019/Conference/-/Paper221/Official_Review", "content": {"title": "Interesting insights and attack broadening our understanding of the scope of the problem of adversarial examples", "review": "The paper explores how the architecture, smoothness of the decision boundary and test accuracy of a model impacts the transferability of examples produced from it.  The paper provides a couple of novel insights, such as the asymmetry when transferring adversarial examples from one model to another. In addition, a novel method is proposed to enhance the transferability of adversarial examples from any model, through using smoothed gradients.\n\nThe experiments seem to show that the effect is rather large, and also makes the examples more robust to other transformations such as JPEG compression. Overall, these are interesting insights that could lead to further developments in making models more robust to adversarial examples. In particular, deriving adversarial examples that are both transferable and resilient to certain usual image transformations shows that the scope of the issue with adversarial examples may be even greater than what is understood today.\n\nThe paper is rather clear. Unfortunately, it is riddled with grammatical errors and should be proof-read carefully. A lot of singular/plurals are off, and some formulations are odd or downright unclear. Some examples (there are way too many to report them all):\n\n- \"Transfer-based attackS ... since they ...*\n- \"of adversarial exampleS ...\"\n- \"from model A can transfer to model B\"\n- \"less transferable than *those from* a shallow model\"?\n- \"investigations, We \": don't capitalize\n- \"the averaging *has* a smoothing effect\"\n- \"our motivation are\"\n- \"contributed it to\"\n- \"available *to the* adversary\"\n- \"crafting adversarial perturbationS\"\n- \"directly evaluation\"\n- \"be fixed 100\"\n\nPros:\n- Transferability and robustness of adversarial examples is a very important problem\n- Interesting insights, esp. the construction and evaluation of examples that are more resilient to certain image transformations\n- Experimental results are convincing\n\nCons:\n- Contribution overall may be a bit limited\n- Grammatical errors and odd formulations all over the place", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper221/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring and Enhancing the Transferability of Adversarial Examples", "abstract": " State-of-the-art deep neural networks are vulnerable to adversarial examples, formed by applying small but malicious perturbations to the original inputs. Moreover, the perturbations can \\textit{transfer across models}: adversarial examples generated for a specific model will often mislead other unseen models. Consequently the adversary can leverage it to attack deployed systems without any query, which severely hinders the application of deep learning, especially in the safety-critical areas. In this work, we empirically study how two classes of factors those might influence the transferability of adversarial examples.  One is about model-specific factors, including network architecture, model capacity and test accuracy. The other is the local smoothness of loss surface for constructing adversarial examples.  Inspired by  these understandings on the transferability of adversarial examples,  we then propose a simple but effective strategy  to enhance the transferability, whose effectiveness is confirmed by a variety of experiments on both  CIFAR-10 and ImageNet datasets.", "keywords": ["Deep learning", "Adversarial example", "Transferability", "Smoothed gradient"], "authorids": ["leiwu@pku.edu.cn", "zhanxing.zhu@pku.edu.cn", "chengtai@pku.edu.cn"], "authors": ["Lei Wu", "Zhanxing Zhu", "Cheng Tai"], "pdf": "/pdf/68bc7c478b44556282255089d05bb096362bb06c.pdf", "paperhash": "wu|exploring_and_enhancing_the_transferability_of_adversarial_examples", "_bibtex": "@misc{\nwu2019exploring,\ntitle={Exploring and Enhancing the Transferability of Adversarial Examples},\nauthor={Lei Wu and Zhanxing Zhu and Cheng Tai},\nyear={2019},\nurl={https://openreview.net/forum?id=HyMRUiC9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper221/Official_Review", "cdate": 1542234511460, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyMRUiC9YX", "replyto": "HyMRUiC9YX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper221/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335675581, "tmdate": 1552335675581, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper221/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkehbJYgnQ", "original": null, "number": 1, "cdate": 1540554500053, "ddate": null, "tcdate": 1540554500053, "tmdate": 1541534182555, "tddate": null, "forum": "HyMRUiC9YX", "replyto": "HyMRUiC9YX", "invitation": "ICLR.cc/2019/Conference/-/Paper221/Official_Review", "content": {"title": "Enhancing the Transferability of Adversarial Examples with Smoothed Gradient Attacks", "review": "Summary. The authors empirically investigate the influence of the architecture and the capacity of an NN-model on the transferability of adversarial examples. They also study the influence of the smoothness. From the obtained results, they propose the smoothed gradient attack showing improvements on the transferability of adversarial examples.\n\nPros.  \n* Robustness of neural nets is a challenging problem of interest for ICLR\n* The paper is well written\n* The experimental study is convincing\n* The experimental results for the smoothed gradient attacks are promising\n\nCons.\n* The results of the experimental study are somehow expected\n* the idea of smoothing gradients is not new\n\nEvaluation.\nThe experimental study of the transferability of adversarial examples is well designed. Experimental protocol is convincing. The smoothed gradient attacks improve many previously proposed attacks. Therefore, my opinion is rather positive. But, as a non expert in the field, I am not completely convinced by the novelty of the approach.\n\nSome details.\nTypos: That l8 abstract; systems l9 intro; and l2 related work; directly evaluation l2 Section4, must has l-10 p4; \n* the choice \\sigma = 15 in Section 6.2 should be justified by the following study\n* \\sigma is not given in Figure 3(a)", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper221/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring and Enhancing the Transferability of Adversarial Examples", "abstract": " State-of-the-art deep neural networks are vulnerable to adversarial examples, formed by applying small but malicious perturbations to the original inputs. Moreover, the perturbations can \\textit{transfer across models}: adversarial examples generated for a specific model will often mislead other unseen models. Consequently the adversary can leverage it to attack deployed systems without any query, which severely hinders the application of deep learning, especially in the safety-critical areas. In this work, we empirically study how two classes of factors those might influence the transferability of adversarial examples.  One is about model-specific factors, including network architecture, model capacity and test accuracy. The other is the local smoothness of loss surface for constructing adversarial examples.  Inspired by  these understandings on the transferability of adversarial examples,  we then propose a simple but effective strategy  to enhance the transferability, whose effectiveness is confirmed by a variety of experiments on both  CIFAR-10 and ImageNet datasets.", "keywords": ["Deep learning", "Adversarial example", "Transferability", "Smoothed gradient"], "authorids": ["leiwu@pku.edu.cn", "zhanxing.zhu@pku.edu.cn", "chengtai@pku.edu.cn"], "authors": ["Lei Wu", "Zhanxing Zhu", "Cheng Tai"], "pdf": "/pdf/68bc7c478b44556282255089d05bb096362bb06c.pdf", "paperhash": "wu|exploring_and_enhancing_the_transferability_of_adversarial_examples", "_bibtex": "@misc{\nwu2019exploring,\ntitle={Exploring and Enhancing the Transferability of Adversarial Examples},\nauthor={Lei Wu and Zhanxing Zhu and Cheng Tai},\nyear={2019},\nurl={https://openreview.net/forum?id=HyMRUiC9YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper221/Official_Review", "cdate": 1542234511460, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyMRUiC9YX", "replyto": "HyMRUiC9YX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper221/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335675581, "tmdate": 1552335675581, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper221/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}