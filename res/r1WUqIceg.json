{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396498473, "tcdate": 1486396498473, "number": 1, "id": "H1cUnzUOe", "invitation": "ICLR.cc/2017/conference/-/paper314/acceptance", "forum": "r1WUqIceg", "replyto": "r1WUqIceg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The authors propose a simple strategy that uses function values to improve the performance of Adam. There is no theoretical analysis of this variant, but there is an extensive empirical evaluation. A disadvantage of the proposed approach is that it has 3 parameters to tune, but the same parameters are used across experiments. Overall however, the PCs believe that this paper doesn't quite reach the level expected for ICLR and thus cannot be accepted."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Stochastic Gradient Descent with Feedback", "abstract": "In this paper we propose a simple and efficient method for improving stochastic gradient descent methods by using feedback from the objective function. The method tracks the relative changes in the objective function with a running average, and uses it to adaptively tune the learning rate in stochastic gradient descent. We specifically apply this idea to modify Adam, a popular algorithm for training deep neural networks. We conduct experiments to compare the resulting algorithm, which we call Eve, with state of the art methods used for training deep learning models. We train CNNs for image classification, and RNNs for language modeling and question answering. Our experiments show that Eve outperforms all other algorithms on these benchmark tasks. We also analyze the behavior of the feedback mechanism during the training process.\n", "pdf": "/pdf/f001292bdd0af4df12c44f6342a46ba381b3c5b0.pdf", "TL;DR": "We improve stochastic gradient descent by incorporating feedback from the objective function", "paperhash": "koushik|improving_stochastic_gradient_descent_with_feedback", "conflicts": ["cs.cmu.edu"], "authors": ["Jayanth Koushik", "Hiroaki Hayashi"], "authorids": ["jkoushik@cs.cmu.edu", "hiroakih@cs.cmu.edu"], "keywords": ["Deep learning", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396499044, "id": "ICLR.cc/2017/conference/-/paper314/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "r1WUqIceg", "replyto": "r1WUqIceg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396499044}}}, {"tddate": null, "tmdate": 1484907449938, "tcdate": 1484907449938, "number": 1, "id": "S1MpmP1vx", "invitation": "ICLR.cc/2017/conference/-/paper314/official/comment", "forum": "r1WUqIceg", "replyto": "HJF-smINg", "signatures": ["ICLR.cc/2017/conference/paper314/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper314/AnonReviewer1"], "content": {"title": "reply", "comment": "> - Could you please clarify what you mean by the momentum term? This point isn't very clear to us.\n\nGradient descent methods, including Adam, usually become more stable when adding a momentum therm that interpolates between the current update and the previous model parameters. Especially the Nesterov momentum gained popularity when combined with Adam.\n\n\n> Could you indicate if there is any specific additional analysis you wanted to see?\n\nNot necessarily an additional analysis but maybe acknowledge that the other algorithms are competitive in many cases"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Stochastic Gradient Descent with Feedback", "abstract": "In this paper we propose a simple and efficient method for improving stochastic gradient descent methods by using feedback from the objective function. The method tracks the relative changes in the objective function with a running average, and uses it to adaptively tune the learning rate in stochastic gradient descent. We specifically apply this idea to modify Adam, a popular algorithm for training deep neural networks. We conduct experiments to compare the resulting algorithm, which we call Eve, with state of the art methods used for training deep learning models. We train CNNs for image classification, and RNNs for language modeling and question answering. Our experiments show that Eve outperforms all other algorithms on these benchmark tasks. We also analyze the behavior of the feedback mechanism during the training process.\n", "pdf": "/pdf/f001292bdd0af4df12c44f6342a46ba381b3c5b0.pdf", "TL;DR": "We improve stochastic gradient descent by incorporating feedback from the objective function", "paperhash": "koushik|improving_stochastic_gradient_descent_with_feedback", "conflicts": ["cs.cmu.edu"], "authors": ["Jayanth Koushik", "Hiroaki Hayashi"], "authorids": ["jkoushik@cs.cmu.edu", "hiroakih@cs.cmu.edu"], "keywords": ["Deep learning", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287626053, "id": "ICLR.cc/2017/conference/-/paper314/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "r1WUqIceg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper314/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper314/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper314/reviewers", "ICLR.cc/2017/conference/paper314/areachairs"], "cdate": 1485287626053}}}, {"tddate": null, "tmdate": 1482248551284, "tcdate": 1482248551284, "number": 3, "id": "r1kdWC8Vl", "invitation": "ICLR.cc/2017/conference/-/paper314/official/review", "forum": "r1WUqIceg", "replyto": "r1WUqIceg", "signatures": ["ICLR.cc/2017/conference/paper314/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper314/AnonReviewer3"], "content": {"title": "Review", "rating": "5: Marginally below acceptance threshold", "review": "The paper introduced an extension of Adam optimizer that automatically adjust learning rate by comparing the subsequent values of the cost function during training. The authors empirically demonstrated the benefit of the Eve optimizer on CIFAR convnets, logistic regression and RNN problems.\n\nI have the following concerns about the paper\n\n- The proposed method is VARIANT to arbitrary shifts and scaling to the cost function.  \n\n- A more fair comparison with other baseline methods would be using additional exponential decay learning scheduling between the lower and upper threshold of d_t. I suspect 1/d_t just shrinks as an exponential decay from Figure 2.\n\n- Three additional hyper-parameters: k, K, \\beta_3.\n\nOverall, I think the method has its fundamental flew and the paper offers very limited novelty. There is no theoretical justification on the modification, and it would be good for the authors to discuss the potential failure mode of the proposed method. Furthermore, it is hard for me to follow Section 3.2. The writing quality and clarity of the method section can be further improved.   \n ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Stochastic Gradient Descent with Feedback", "abstract": "In this paper we propose a simple and efficient method for improving stochastic gradient descent methods by using feedback from the objective function. The method tracks the relative changes in the objective function with a running average, and uses it to adaptively tune the learning rate in stochastic gradient descent. We specifically apply this idea to modify Adam, a popular algorithm for training deep neural networks. We conduct experiments to compare the resulting algorithm, which we call Eve, with state of the art methods used for training deep learning models. We train CNNs for image classification, and RNNs for language modeling and question answering. Our experiments show that Eve outperforms all other algorithms on these benchmark tasks. We also analyze the behavior of the feedback mechanism during the training process.\n", "pdf": "/pdf/f001292bdd0af4df12c44f6342a46ba381b3c5b0.pdf", "TL;DR": "We improve stochastic gradient descent by incorporating feedback from the objective function", "paperhash": "koushik|improving_stochastic_gradient_descent_with_feedback", "conflicts": ["cs.cmu.edu"], "authors": ["Jayanth Koushik", "Hiroaki Hayashi"], "authorids": ["jkoushik@cs.cmu.edu", "hiroakih@cs.cmu.edu"], "keywords": ["Deep learning", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512626114, "id": "ICLR.cc/2017/conference/-/paper314/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper314/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper314/AnonReviewer2", "ICLR.cc/2017/conference/paper314/AnonReviewer1", "ICLR.cc/2017/conference/paper314/AnonReviewer3"], "reply": {"forum": "r1WUqIceg", "replyto": "r1WUqIceg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper314/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper314/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512626114}}}, {"tddate": null, "tmdate": 1482207039513, "tcdate": 1482207039513, "number": 7, "id": "HkvByELVg", "invitation": "ICLR.cc/2017/conference/-/paper314/public/comment", "forum": "r1WUqIceg", "replyto": "BJQ4HX-Ve", "signatures": ["~Jayanth_Koushik1"], "readers": ["everyone"], "writers": ["~Jayanth_Koushik1"], "content": {"title": "Response to review", "comment": "Thanks for reviewing! We want to note that the analysis around Figure 5 was only for the simplified case of batch gradient descent on a convex problem. What we mainly wanted to show is that in this case Eve is not equivalent to Adam with a larger learning rate. Figure 5 right does show *all* 10 runs of both Adam and Eve, and it is indeed the case that Adam does sometimes diverge whereas Eve does not. The reason for choosing 0.1 is that d_t converges to 0.1, so the learning rate becomes 10 times in the end - and Eve does become 0.1 of Adam eventually.\n\nRegarding the second point, we note that in the more general non-convex case, it is not sufficient to simply tune the learning rate. This is indicated by Figure 2 left which shows a behavior of d_t which is not easy to parametrize. And because of its non-monotone nature, it would not be the same to simply increase or decrease the learning rate. For the particular case of Figure 2, the learning rate first gets increased, and then decreased.\n\nWe hope that answers your concerns."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Stochastic Gradient Descent with Feedback", "abstract": "In this paper we propose a simple and efficient method for improving stochastic gradient descent methods by using feedback from the objective function. The method tracks the relative changes in the objective function with a running average, and uses it to adaptively tune the learning rate in stochastic gradient descent. We specifically apply this idea to modify Adam, a popular algorithm for training deep neural networks. We conduct experiments to compare the resulting algorithm, which we call Eve, with state of the art methods used for training deep learning models. We train CNNs for image classification, and RNNs for language modeling and question answering. Our experiments show that Eve outperforms all other algorithms on these benchmark tasks. We also analyze the behavior of the feedback mechanism during the training process.\n", "pdf": "/pdf/f001292bdd0af4df12c44f6342a46ba381b3c5b0.pdf", "TL;DR": "We improve stochastic gradient descent by incorporating feedback from the objective function", "paperhash": "koushik|improving_stochastic_gradient_descent_with_feedback", "conflicts": ["cs.cmu.edu"], "authors": ["Jayanth Koushik", "Hiroaki Hayashi"], "authorids": ["jkoushik@cs.cmu.edu", "hiroakih@cs.cmu.edu"], "keywords": ["Deep learning", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287626312, "id": "ICLR.cc/2017/conference/-/paper314/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1WUqIceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper314/reviewers", "ICLR.cc/2017/conference/paper314/areachairs"], "cdate": 1485287626312}}}, {"tddate": null, "tmdate": 1482205953334, "tcdate": 1482205953334, "number": 6, "id": "HJF-smINg", "invitation": "ICLR.cc/2017/conference/-/paper314/public/comment", "forum": "r1WUqIceg", "replyto": "ByZACLbEe", "signatures": ["~Jayanth_Koushik1"], "readers": ["everyone"], "writers": ["~Jayanth_Koushik1"], "content": {"title": "Response to review", "comment": "Thank you for the review. We want to address the 3 indicated cons:\n- Could you please clarify what you mean by the momentum term? This point isn't very clear to us.\n- Thank you for pointing this out. We will update the paper to fix the citation.\n- We conducted experiments across tasks (vision, language) on standard benchmarking datasets, and in all cases, the proposed method outperforms Adam. Could you indicate if there is any specific additional analysis you wanted to see?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Stochastic Gradient Descent with Feedback", "abstract": "In this paper we propose a simple and efficient method for improving stochastic gradient descent methods by using feedback from the objective function. The method tracks the relative changes in the objective function with a running average, and uses it to adaptively tune the learning rate in stochastic gradient descent. We specifically apply this idea to modify Adam, a popular algorithm for training deep neural networks. We conduct experiments to compare the resulting algorithm, which we call Eve, with state of the art methods used for training deep learning models. We train CNNs for image classification, and RNNs for language modeling and question answering. Our experiments show that Eve outperforms all other algorithms on these benchmark tasks. We also analyze the behavior of the feedback mechanism during the training process.\n", "pdf": "/pdf/f001292bdd0af4df12c44f6342a46ba381b3c5b0.pdf", "TL;DR": "We improve stochastic gradient descent by incorporating feedback from the objective function", "paperhash": "koushik|improving_stochastic_gradient_descent_with_feedback", "conflicts": ["cs.cmu.edu"], "authors": ["Jayanth Koushik", "Hiroaki Hayashi"], "authorids": ["jkoushik@cs.cmu.edu", "hiroakih@cs.cmu.edu"], "keywords": ["Deep learning", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287626312, "id": "ICLR.cc/2017/conference/-/paper314/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1WUqIceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper314/reviewers", "ICLR.cc/2017/conference/paper314/areachairs"], "cdate": 1485287626312}}}, {"tddate": null, "tmdate": 1481891645869, "tcdate": 1481891528851, "number": 2, "id": "ByZACLbEe", "invitation": "ICLR.cc/2017/conference/-/paper314/official/review", "forum": "r1WUqIceg", "replyto": "r1WUqIceg", "signatures": ["ICLR.cc/2017/conference/paper314/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper314/AnonReviewer1"], "content": {"title": "A learning rate tuning method for Adam", "rating": "6: Marginally above acceptance threshold", "review": "The paper demonstrates a semi-automatic learning rate schedule for the Adam optimizer, called Eve. Originality is somehow limited but the method appears to have a positive effect on neural network training. The paper is well written and illustrations are appropriate.\n\nPros:\n\n- probably a more sophisticated scheduling technique than a simple decay term\n- reasonable results on the CIFAR dataset (although with comparably small neural network)\n\nCons:\n\n- effect of momentum term would be of interest\n- the Adam reference doesn't point to the conference publications but only to arxiv\n- comparison to Adam not entirely conclusive", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Stochastic Gradient Descent with Feedback", "abstract": "In this paper we propose a simple and efficient method for improving stochastic gradient descent methods by using feedback from the objective function. The method tracks the relative changes in the objective function with a running average, and uses it to adaptively tune the learning rate in stochastic gradient descent. We specifically apply this idea to modify Adam, a popular algorithm for training deep neural networks. We conduct experiments to compare the resulting algorithm, which we call Eve, with state of the art methods used for training deep learning models. We train CNNs for image classification, and RNNs for language modeling and question answering. Our experiments show that Eve outperforms all other algorithms on these benchmark tasks. We also analyze the behavior of the feedback mechanism during the training process.\n", "pdf": "/pdf/f001292bdd0af4df12c44f6342a46ba381b3c5b0.pdf", "TL;DR": "We improve stochastic gradient descent by incorporating feedback from the objective function", "paperhash": "koushik|improving_stochastic_gradient_descent_with_feedback", "conflicts": ["cs.cmu.edu"], "authors": ["Jayanth Koushik", "Hiroaki Hayashi"], "authorids": ["jkoushik@cs.cmu.edu", "hiroakih@cs.cmu.edu"], "keywords": ["Deep learning", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512626114, "id": "ICLR.cc/2017/conference/-/paper314/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper314/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper314/AnonReviewer2", "ICLR.cc/2017/conference/paper314/AnonReviewer1", "ICLR.cc/2017/conference/paper314/AnonReviewer3"], "reply": {"forum": "r1WUqIceg", "replyto": "r1WUqIceg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper314/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper314/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512626114}}}, {"tddate": null, "tmdate": 1481876779335, "tcdate": 1481876779335, "number": 1, "id": "BJQ4HX-Ve", "invitation": "ICLR.cc/2017/conference/-/paper314/official/review", "forum": "r1WUqIceg", "replyto": "r1WUqIceg", "signatures": ["ICLR.cc/2017/conference/paper314/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper314/AnonReviewer2"], "content": {"title": "d_t", "rating": "5: Marginally below acceptance threshold", "review": "As you noted for Figure 5 Left, sometimes it seems sufficient to tune learning rates. I see your argument for Figure 6 Right, \nbut \n1) not for all good learning rates make Adam fail, I guess you selected the one where it did (note that Adam was several times faster than Eve in the beginning)\n2) I don't buy \"Eve always converges\" because you show it only for 0.1 and since Eve is not Adam, 0.1 of Adam is not 0.1 of Eve because of d_t. \n\nTo my understanding, you define d_t over time with 3 hyperparameters. Similarly, one can define d_t directly. The behaviour of d_t that you show is not extraordinary and can be parameterized. If Eve is better than Adam, then looking at d_t we can directly see whether we underestimated or overestimated learning rates. You could argue that Eve does it automatically but you do tune learning rates for each problem individually anyway. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Stochastic Gradient Descent with Feedback", "abstract": "In this paper we propose a simple and efficient method for improving stochastic gradient descent methods by using feedback from the objective function. The method tracks the relative changes in the objective function with a running average, and uses it to adaptively tune the learning rate in stochastic gradient descent. We specifically apply this idea to modify Adam, a popular algorithm for training deep neural networks. We conduct experiments to compare the resulting algorithm, which we call Eve, with state of the art methods used for training deep learning models. We train CNNs for image classification, and RNNs for language modeling and question answering. Our experiments show that Eve outperforms all other algorithms on these benchmark tasks. We also analyze the behavior of the feedback mechanism during the training process.\n", "pdf": "/pdf/f001292bdd0af4df12c44f6342a46ba381b3c5b0.pdf", "TL;DR": "We improve stochastic gradient descent by incorporating feedback from the objective function", "paperhash": "koushik|improving_stochastic_gradient_descent_with_feedback", "conflicts": ["cs.cmu.edu"], "authors": ["Jayanth Koushik", "Hiroaki Hayashi"], "authorids": ["jkoushik@cs.cmu.edu", "hiroakih@cs.cmu.edu"], "keywords": ["Deep learning", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512626114, "id": "ICLR.cc/2017/conference/-/paper314/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper314/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper314/AnonReviewer2", "ICLR.cc/2017/conference/paper314/AnonReviewer1", "ICLR.cc/2017/conference/paper314/AnonReviewer3"], "reply": {"forum": "r1WUqIceg", "replyto": "r1WUqIceg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper314/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper314/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512626114}}}, {"tddate": null, "tmdate": 1481654702871, "tcdate": 1481654702863, "number": 5, "id": "S1v3WT6Xe", "invitation": "ICLR.cc/2017/conference/-/paper314/public/comment", "forum": "r1WUqIceg", "replyto": "r1KJ2oxml", "signatures": ["~Jayanth_Koushik1"], "readers": ["everyone"], "writers": ["~Jayanth_Koushik1"], "content": {"title": "results of timing experiment", "comment": "We ran the CIFAR 10 experiment 10 times with Adam and Eve for 100 epochs each. Adam took 2233.11 seconds on an average for 100 epochs with a standard deviation of 9.24 seconds. For Eve, the mean was 2265.49 seconds, and the standard deviation was 19.03 seconds. Taking into account the number of samples and the batch size, we compute that per update, Eve takes on an average 0.83 milliseconds more than Adam.\n\nWe haven't added this to the paper, since the specific times are very dependent on many external factors, making conclusions hard to draw especially when the results are close. But we again note that all considered methods, including Eve, are just simple modifications of stochastic gradient descent, so their times are within constant factors of each other."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Stochastic Gradient Descent with Feedback", "abstract": "In this paper we propose a simple and efficient method for improving stochastic gradient descent methods by using feedback from the objective function. The method tracks the relative changes in the objective function with a running average, and uses it to adaptively tune the learning rate in stochastic gradient descent. We specifically apply this idea to modify Adam, a popular algorithm for training deep neural networks. We conduct experiments to compare the resulting algorithm, which we call Eve, with state of the art methods used for training deep learning models. We train CNNs for image classification, and RNNs for language modeling and question answering. Our experiments show that Eve outperforms all other algorithms on these benchmark tasks. We also analyze the behavior of the feedback mechanism during the training process.\n", "pdf": "/pdf/f001292bdd0af4df12c44f6342a46ba381b3c5b0.pdf", "TL;DR": "We improve stochastic gradient descent by incorporating feedback from the objective function", "paperhash": "koushik|improving_stochastic_gradient_descent_with_feedback", "conflicts": ["cs.cmu.edu"], "authors": ["Jayanth Koushik", "Hiroaki Hayashi"], "authorids": ["jkoushik@cs.cmu.edu", "hiroakih@cs.cmu.edu"], "keywords": ["Deep learning", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287626312, "id": "ICLR.cc/2017/conference/-/paper314/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1WUqIceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper314/reviewers", "ICLR.cc/2017/conference/paper314/areachairs"], "cdate": 1485287626312}}}, {"tddate": null, "tmdate": 1480797152843, "tcdate": 1480797152838, "number": 4, "id": "r1KJ2oxml", "invitation": "ICLR.cc/2017/conference/-/paper314/public/comment", "forum": "r1WUqIceg", "replyto": "B1KNecyQl", "signatures": ["~Jayanth_Koushik1"], "readers": ["everyone"], "writers": ["~Jayanth_Koushik1"], "content": {"title": "no significant overhead", "comment": "We will try to get these results soon, but we do note that since we only perform elementary operations, the running time shouldn\u2019t increase significantly."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Stochastic Gradient Descent with Feedback", "abstract": "In this paper we propose a simple and efficient method for improving stochastic gradient descent methods by using feedback from the objective function. The method tracks the relative changes in the objective function with a running average, and uses it to adaptively tune the learning rate in stochastic gradient descent. We specifically apply this idea to modify Adam, a popular algorithm for training deep neural networks. We conduct experiments to compare the resulting algorithm, which we call Eve, with state of the art methods used for training deep learning models. We train CNNs for image classification, and RNNs for language modeling and question answering. Our experiments show that Eve outperforms all other algorithms on these benchmark tasks. We also analyze the behavior of the feedback mechanism during the training process.\n", "pdf": "/pdf/f001292bdd0af4df12c44f6342a46ba381b3c5b0.pdf", "TL;DR": "We improve stochastic gradient descent by incorporating feedback from the objective function", "paperhash": "koushik|improving_stochastic_gradient_descent_with_feedback", "conflicts": ["cs.cmu.edu"], "authors": ["Jayanth Koushik", "Hiroaki Hayashi"], "authorids": ["jkoushik@cs.cmu.edu", "hiroakih@cs.cmu.edu"], "keywords": ["Deep learning", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287626312, "id": "ICLR.cc/2017/conference/-/paper314/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1WUqIceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper314/reviewers", "ICLR.cc/2017/conference/paper314/areachairs"], "cdate": 1485287626312}}}, {"tddate": null, "tmdate": 1480795868671, "tcdate": 1480795868666, "number": 3, "id": "H1rJvix7l", "invitation": "ICLR.cc/2017/conference/-/paper314/public/comment", "forum": "r1WUqIceg", "replyto": "rkuK6cJ7e", "signatures": ["~Jayanth_Koushik1"], "readers": ["everyone"], "writers": ["~Jayanth_Koushik1"], "content": {"title": "paper updated with assumption", "comment": "Hi. Thanks for the comment. This point has been brought up elsewhere; we are working on a proper fix for the issue, but for now we've updated the paper to add an assumption which prevents arbitrary shifts. The assumption holds for all experiments performed in the paper, and the results remain valid."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Stochastic Gradient Descent with Feedback", "abstract": "In this paper we propose a simple and efficient method for improving stochastic gradient descent methods by using feedback from the objective function. The method tracks the relative changes in the objective function with a running average, and uses it to adaptively tune the learning rate in stochastic gradient descent. We specifically apply this idea to modify Adam, a popular algorithm for training deep neural networks. We conduct experiments to compare the resulting algorithm, which we call Eve, with state of the art methods used for training deep learning models. We train CNNs for image classification, and RNNs for language modeling and question answering. Our experiments show that Eve outperforms all other algorithms on these benchmark tasks. We also analyze the behavior of the feedback mechanism during the training process.\n", "pdf": "/pdf/f001292bdd0af4df12c44f6342a46ba381b3c5b0.pdf", "TL;DR": "We improve stochastic gradient descent by incorporating feedback from the objective function", "paperhash": "koushik|improving_stochastic_gradient_descent_with_feedback", "conflicts": ["cs.cmu.edu"], "authors": ["Jayanth Koushik", "Hiroaki Hayashi"], "authorids": ["jkoushik@cs.cmu.edu", "hiroakih@cs.cmu.edu"], "keywords": ["Deep learning", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287626312, "id": "ICLR.cc/2017/conference/-/paper314/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1WUqIceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper314/reviewers", "ICLR.cc/2017/conference/paper314/areachairs"], "cdate": 1485287626312}}}, {"tddate": null, "tmdate": 1480727936281, "tcdate": 1480727936276, "number": 2, "id": "rkuK6cJ7e", "invitation": "ICLR.cc/2017/conference/-/paper314/pre-review/question", "forum": "r1WUqIceg", "replyto": "r1WUqIceg", "signatures": ["ICLR.cc/2017/conference/paper314/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper314/AnonReviewer3"], "content": {"title": "sensitive to cost function scaling?", "question": "The algorithm does not seem to be invariant to arbitrary shift in the objective function. If the objective function is shifted by a constant c , r_t would be be a different value from Algorithm 1 and thus the learning will take a different trajectory even though it is the exact same optimization problem. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Stochastic Gradient Descent with Feedback", "abstract": "In this paper we propose a simple and efficient method for improving stochastic gradient descent methods by using feedback from the objective function. The method tracks the relative changes in the objective function with a running average, and uses it to adaptively tune the learning rate in stochastic gradient descent. We specifically apply this idea to modify Adam, a popular algorithm for training deep neural networks. We conduct experiments to compare the resulting algorithm, which we call Eve, with state of the art methods used for training deep learning models. We train CNNs for image classification, and RNNs for language modeling and question answering. Our experiments show that Eve outperforms all other algorithms on these benchmark tasks. We also analyze the behavior of the feedback mechanism during the training process.\n", "pdf": "/pdf/f001292bdd0af4df12c44f6342a46ba381b3c5b0.pdf", "TL;DR": "We improve stochastic gradient descent by incorporating feedback from the objective function", "paperhash": "koushik|improving_stochastic_gradient_descent_with_feedback", "conflicts": ["cs.cmu.edu"], "authors": ["Jayanth Koushik", "Hiroaki Hayashi"], "authorids": ["jkoushik@cs.cmu.edu", "hiroakih@cs.cmu.edu"], "keywords": ["Deep learning", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959345299, "id": "ICLR.cc/2017/conference/-/paper314/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper314/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper314/AnonReviewer2", "ICLR.cc/2017/conference/paper314/AnonReviewer3"], "reply": {"forum": "r1WUqIceg", "replyto": "r1WUqIceg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper314/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper314/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959345299}}}, {"tddate": null, "tmdate": 1480724529304, "tcdate": 1480724529300, "number": 1, "id": "B1KNecyQl", "invitation": "ICLR.cc/2017/conference/-/paper314/pre-review/question", "forum": "r1WUqIceg", "replyto": "r1WUqIceg", "signatures": ["ICLR.cc/2017/conference/paper314/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper314/AnonReviewer2"], "content": {"title": "time complexity?", "question": "what about time complexity? could you add two more sub-figures of Figure 1 with wallclock time on x-axis? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Stochastic Gradient Descent with Feedback", "abstract": "In this paper we propose a simple and efficient method for improving stochastic gradient descent methods by using feedback from the objective function. The method tracks the relative changes in the objective function with a running average, and uses it to adaptively tune the learning rate in stochastic gradient descent. We specifically apply this idea to modify Adam, a popular algorithm for training deep neural networks. We conduct experiments to compare the resulting algorithm, which we call Eve, with state of the art methods used for training deep learning models. We train CNNs for image classification, and RNNs for language modeling and question answering. Our experiments show that Eve outperforms all other algorithms on these benchmark tasks. We also analyze the behavior of the feedback mechanism during the training process.\n", "pdf": "/pdf/f001292bdd0af4df12c44f6342a46ba381b3c5b0.pdf", "TL;DR": "We improve stochastic gradient descent by incorporating feedback from the objective function", "paperhash": "koushik|improving_stochastic_gradient_descent_with_feedback", "conflicts": ["cs.cmu.edu"], "authors": ["Jayanth Koushik", "Hiroaki Hayashi"], "authorids": ["jkoushik@cs.cmu.edu", "hiroakih@cs.cmu.edu"], "keywords": ["Deep learning", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959345299, "id": "ICLR.cc/2017/conference/-/paper314/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper314/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper314/AnonReviewer2", "ICLR.cc/2017/conference/paper314/AnonReviewer3"], "reply": {"forum": "r1WUqIceg", "replyto": "r1WUqIceg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper314/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper314/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959345299}}}, {"tddate": null, "tmdate": 1479889499711, "tcdate": 1479889499707, "number": 2, "id": "ryVPMAzMx", "invitation": "ICLR.cc/2017/conference/-/paper314/public/comment", "forum": "r1WUqIceg", "replyto": "ryfu95bMg", "signatures": ["~Jayanth_Koushik1"], "readers": ["everyone"], "writers": ["~Jayanth_Koushik1"], "content": {"title": "Typo fixed + updated the paper.", "comment": "Hello Timo,\n\nThank you very much for pointing out the typo. We updated the paper with the typo fix and the assumption section for more clarity. Please take a look.\nAlso, we are glad to know that this algorithm helped you in larger and more complex algorithms. Thank you for trying out.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Stochastic Gradient Descent with Feedback", "abstract": "In this paper we propose a simple and efficient method for improving stochastic gradient descent methods by using feedback from the objective function. The method tracks the relative changes in the objective function with a running average, and uses it to adaptively tune the learning rate in stochastic gradient descent. We specifically apply this idea to modify Adam, a popular algorithm for training deep neural networks. We conduct experiments to compare the resulting algorithm, which we call Eve, with state of the art methods used for training deep learning models. We train CNNs for image classification, and RNNs for language modeling and question answering. Our experiments show that Eve outperforms all other algorithms on these benchmark tasks. We also analyze the behavior of the feedback mechanism during the training process.\n", "pdf": "/pdf/f001292bdd0af4df12c44f6342a46ba381b3c5b0.pdf", "TL;DR": "We improve stochastic gradient descent by incorporating feedback from the objective function", "paperhash": "koushik|improving_stochastic_gradient_descent_with_feedback", "conflicts": ["cs.cmu.edu"], "authors": ["Jayanth Koushik", "Hiroaki Hayashi"], "authorids": ["jkoushik@cs.cmu.edu", "hiroakih@cs.cmu.edu"], "keywords": ["Deep learning", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287626312, "id": "ICLR.cc/2017/conference/-/paper314/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1WUqIceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper314/reviewers", "ICLR.cc/2017/conference/paper314/areachairs"], "cdate": 1485287626312}}}, {"tddate": null, "tmdate": 1479809642109, "tcdate": 1479809642104, "number": 1, "id": "ryfu95bMg", "invitation": "ICLR.cc/2017/conference/-/paper314/public/comment", "forum": "r1WUqIceg", "replyto": "r1WUqIceg", "signatures": ["~Timo_Aila1"], "readers": ["everyone"], "writers": ["~Timo_Aila1"], "content": {"title": "Error in pseudocode", "comment": "I think there is a bug in Algorithm 1. The comparison between f and \\hat{f} should be the other way around.\n\nWith that fix, I re-implemented the algorithm and indeed it was slightly faster than Adam in training a complex autoencoder :)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Stochastic Gradient Descent with Feedback", "abstract": "In this paper we propose a simple and efficient method for improving stochastic gradient descent methods by using feedback from the objective function. The method tracks the relative changes in the objective function with a running average, and uses it to adaptively tune the learning rate in stochastic gradient descent. We specifically apply this idea to modify Adam, a popular algorithm for training deep neural networks. We conduct experiments to compare the resulting algorithm, which we call Eve, with state of the art methods used for training deep learning models. We train CNNs for image classification, and RNNs for language modeling and question answering. Our experiments show that Eve outperforms all other algorithms on these benchmark tasks. We also analyze the behavior of the feedback mechanism during the training process.\n", "pdf": "/pdf/f001292bdd0af4df12c44f6342a46ba381b3c5b0.pdf", "TL;DR": "We improve stochastic gradient descent by incorporating feedback from the objective function", "paperhash": "koushik|improving_stochastic_gradient_descent_with_feedback", "conflicts": ["cs.cmu.edu"], "authors": ["Jayanth Koushik", "Hiroaki Hayashi"], "authorids": ["jkoushik@cs.cmu.edu", "hiroakih@cs.cmu.edu"], "keywords": ["Deep learning", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287626312, "id": "ICLR.cc/2017/conference/-/paper314/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1WUqIceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper314/reviewers", "ICLR.cc/2017/conference/paper314/areachairs"], "cdate": 1485287626312}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1479334032597, "tcdate": 1478285896723, "number": 314, "id": "r1WUqIceg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "r1WUqIceg", "signatures": ["~Jayanth_Koushik1"], "readers": ["everyone"], "content": {"title": "Improving Stochastic Gradient Descent with Feedback", "abstract": "In this paper we propose a simple and efficient method for improving stochastic gradient descent methods by using feedback from the objective function. The method tracks the relative changes in the objective function with a running average, and uses it to adaptively tune the learning rate in stochastic gradient descent. We specifically apply this idea to modify Adam, a popular algorithm for training deep neural networks. We conduct experiments to compare the resulting algorithm, which we call Eve, with state of the art methods used for training deep learning models. We train CNNs for image classification, and RNNs for language modeling and question answering. Our experiments show that Eve outperforms all other algorithms on these benchmark tasks. We also analyze the behavior of the feedback mechanism during the training process.\n", "pdf": "/pdf/f001292bdd0af4df12c44f6342a46ba381b3c5b0.pdf", "TL;DR": "We improve stochastic gradient descent by incorporating feedback from the objective function", "paperhash": "koushik|improving_stochastic_gradient_descent_with_feedback", "conflicts": ["cs.cmu.edu"], "authors": ["Jayanth Koushik", "Hiroaki Hayashi"], "authorids": ["jkoushik@cs.cmu.edu", "hiroakih@cs.cmu.edu"], "keywords": ["Deep learning", "Optimization"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 15}