{"notes": [{"id": "S1gyl6Vtvr", "original": "B1g6jK2Svr", "number": 323, "cdate": 1569438950987, "ddate": null, "tcdate": 1569438950987, "tmdate": 1577168271436, "tddate": null, "forum": "S1gyl6Vtvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["raden.m.muaz@gmail.com", "lin-j@i2r.a-star.edu.sg", "vijay@i2r.a-star.edu.sg", "shinoda@ks.cs.titech.ac.jp"], "title": "MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning", "authors": ["Raden Mu'az Mun'im", "Jie Lin", "Vijay Chandrasekhar", "Koichi Shinoda"], "pdf": "/pdf/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "abstract": "In this paper, we propose a framework, called MaskConvNet, for ConvNets filter pruning. MaskConvNet provides elegant support for training budget-aware pruned networks from scratch, by adding a simple mask module to a ConvNet architecture. MaskConvNet enjoys several advantages - (1) Flexible, the mask module can be integrated with any ConvNets in a plug-and-play manner. (2) Simple, the mask module is implemented by a hard Sigmoid function with a small number of trainable mask variables, adding negligible memory and computational overheads to the networks during training. (3) Effective, it is able to achieve competitive pruning rate while maintaining comparable accuracy with the baseline ConvNets without pruning, regardless of the datasets and ConvNet architectures used. (4) Fast, it is observed that the number of training epochs required by MaskConvNet is close to training a baseline without pruning. (5) Budget-aware, with a sparsity budget on target metric (e.g. model size and FLOP), MaskConvNet is able to train in a way that the optimizer can adaptively sparsify the network and automatically maintain sparsity level, till the pruned network produces good accuracy and fulfill the budget constraint simultaneously. Results on CIFAR-10 and ImageNet with several ConvNet architectures show that MaskConvNet works competitively well compared to previous pruning methods, with budget-constraint well respected. Code is available at https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0. We hope MaskConvNet, as a simple and general pruning framework, can address the gaps in existing literate and advance future studies to push the boundaries of neural network pruning.", "code": "https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0", "keywords": ["Structured Pruning", "Sparsity Regularization", "Budget-Aware"], "paperhash": "munim|maskconvnet_training_efficient_convnets_from_scratch_via_budgetconstrained_filter_pruning", "original_pdf": "/attachment/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "_bibtex": "@misc{\nmun'im2020maskconvnet,\ntitle={MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning},\nauthor={Raden Mu'az Mun'im and Jie Lin and Vijay Chandrasekhar and Koichi Shinoda},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gyl6Vtvr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "jNFRgGx3Vh", "original": null, "number": 1, "cdate": 1576798693297, "ddate": null, "tcdate": 1576798693297, "tmdate": 1576800942132, "tddate": null, "forum": "S1gyl6Vtvr", "replyto": "S1gyl6Vtvr", "invitation": "ICLR.cc/2020/Conference/Paper323/-/Decision", "content": {"decision": "Reject", "comment": "This paper presents a method to learn a pruned convolutional network during conventional training.  Pruning the network has advantages (in deployment) of reducing the final model size and reducing the required FLOPS for compute.  The method adds a pruning mask on each layer with an additional sparsity loss on the mask variables. The method avoids the cost of a train-prune-retrain optimization process that has been used in several earlier papers.  The method is evaluated on CIFAR-10 and ImageNet with three standard convolutional network architectures.  The results show comparable performance to the original networks with the learned sparse networks. \n\nThe reviewers made many substantial comments on the paper and most of these were addressed in the author response and subsequent discussion.  For example, Reviewer1 mentioned two other papers that promote sparsity implicitly during training (Q3), and the authors acknowledged the omission and described how those methods had less flexibility on a target metric (FLOPS) that is not parameter size.  Many of the author responses described changes to an updated paper that would clarify the claims and results (R1: Q2-7, R2:Q3).\n\nHowever, the reviewers raised many concerns for the original paper and they did not see an updated paper that contains the proposed revisions.  Given the numerous concerns with the original submission, the reviewers wanted to see the revised paper to assess whether their concerns had been addressed adequately. Additionally, the paper does not have a comparison experiment with state-of the art results, and the current results were not sufficiently convincing for the reviewers.  Reviewer1 and author response to questions 13--15 suggest that the experimental results with ResNet-34 are inadequate to show the benefits of the approach, but results for the proposed method with the larger ResNet-50 (which could show benefits) are not yet ready. \n\nThe current paper is not ready for publication.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["raden.m.muaz@gmail.com", "lin-j@i2r.a-star.edu.sg", "vijay@i2r.a-star.edu.sg", "shinoda@ks.cs.titech.ac.jp"], "title": "MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning", "authors": ["Raden Mu'az Mun'im", "Jie Lin", "Vijay Chandrasekhar", "Koichi Shinoda"], "pdf": "/pdf/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "abstract": "In this paper, we propose a framework, called MaskConvNet, for ConvNets filter pruning. MaskConvNet provides elegant support for training budget-aware pruned networks from scratch, by adding a simple mask module to a ConvNet architecture. MaskConvNet enjoys several advantages - (1) Flexible, the mask module can be integrated with any ConvNets in a plug-and-play manner. (2) Simple, the mask module is implemented by a hard Sigmoid function with a small number of trainable mask variables, adding negligible memory and computational overheads to the networks during training. (3) Effective, it is able to achieve competitive pruning rate while maintaining comparable accuracy with the baseline ConvNets without pruning, regardless of the datasets and ConvNet architectures used. (4) Fast, it is observed that the number of training epochs required by MaskConvNet is close to training a baseline without pruning. (5) Budget-aware, with a sparsity budget on target metric (e.g. model size and FLOP), MaskConvNet is able to train in a way that the optimizer can adaptively sparsify the network and automatically maintain sparsity level, till the pruned network produces good accuracy and fulfill the budget constraint simultaneously. Results on CIFAR-10 and ImageNet with several ConvNet architectures show that MaskConvNet works competitively well compared to previous pruning methods, with budget-constraint well respected. Code is available at https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0. We hope MaskConvNet, as a simple and general pruning framework, can address the gaps in existing literate and advance future studies to push the boundaries of neural network pruning.", "code": "https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0", "keywords": ["Structured Pruning", "Sparsity Regularization", "Budget-Aware"], "paperhash": "munim|maskconvnet_training_efficient_convnets_from_scratch_via_budgetconstrained_filter_pruning", "original_pdf": "/attachment/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "_bibtex": "@misc{\nmun'im2020maskconvnet,\ntitle={MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning},\nauthor={Raden Mu'az Mun'im and Jie Lin and Vijay Chandrasekhar and Koichi Shinoda},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gyl6Vtvr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1gyl6Vtvr", "replyto": "S1gyl6Vtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795729816, "tmdate": 1576800282485, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper323/-/Decision"}}}, {"id": "Byxx1z7noB", "original": null, "number": 8, "cdate": 1573822936065, "ddate": null, "tcdate": 1573822936065, "tmdate": 1573822936065, "tddate": null, "forum": "S1gyl6Vtvr", "replyto": "rkezZAJtsS", "invitation": "ICLR.cc/2020/Conference/Paper323/-/Official_Comment", "content": {"title": "Comment", "comment": "Thank you for your response!\n\nMinor remark on the paper: When you introduce unsparsification technique you use \"S\" which you never define in the text. I'm guessing you're referring to \"\\mathcal{L}_S\".\n\nIn my notes I asked a couple more questions and it would be nice if you could comment on those too.\n\nI still don't quite understand how you use use target metrics in Algorithm 1. What about multi-metrics? Do you have any experiments employing this concept? This bit is not very clear. After re-reading the section, I have a guess that you propose to update different lambdas independently. Still, I think the paper would benefit from making this point more explicit.\n\nIn general, it'd be much easier for me to make a decision based on the content of the revised paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper323/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper323/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["raden.m.muaz@gmail.com", "lin-j@i2r.a-star.edu.sg", "vijay@i2r.a-star.edu.sg", "shinoda@ks.cs.titech.ac.jp"], "title": "MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning", "authors": ["Raden Mu'az Mun'im", "Jie Lin", "Vijay Chandrasekhar", "Koichi Shinoda"], "pdf": "/pdf/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "abstract": "In this paper, we propose a framework, called MaskConvNet, for ConvNets filter pruning. MaskConvNet provides elegant support for training budget-aware pruned networks from scratch, by adding a simple mask module to a ConvNet architecture. MaskConvNet enjoys several advantages - (1) Flexible, the mask module can be integrated with any ConvNets in a plug-and-play manner. (2) Simple, the mask module is implemented by a hard Sigmoid function with a small number of trainable mask variables, adding negligible memory and computational overheads to the networks during training. (3) Effective, it is able to achieve competitive pruning rate while maintaining comparable accuracy with the baseline ConvNets without pruning, regardless of the datasets and ConvNet architectures used. (4) Fast, it is observed that the number of training epochs required by MaskConvNet is close to training a baseline without pruning. (5) Budget-aware, with a sparsity budget on target metric (e.g. model size and FLOP), MaskConvNet is able to train in a way that the optimizer can adaptively sparsify the network and automatically maintain sparsity level, till the pruned network produces good accuracy and fulfill the budget constraint simultaneously. Results on CIFAR-10 and ImageNet with several ConvNet architectures show that MaskConvNet works competitively well compared to previous pruning methods, with budget-constraint well respected. Code is available at https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0. We hope MaskConvNet, as a simple and general pruning framework, can address the gaps in existing literate and advance future studies to push the boundaries of neural network pruning.", "code": "https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0", "keywords": ["Structured Pruning", "Sparsity Regularization", "Budget-Aware"], "paperhash": "munim|maskconvnet_training_efficient_convnets_from_scratch_via_budgetconstrained_filter_pruning", "original_pdf": "/attachment/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "_bibtex": "@misc{\nmun'im2020maskconvnet,\ntitle={MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning},\nauthor={Raden Mu'az Mun'im and Jie Lin and Vijay Chandrasekhar and Koichi Shinoda},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gyl6Vtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1gyl6Vtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper323/Authors", "ICLR.cc/2020/Conference/Paper323/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper323/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper323/Reviewers", "ICLR.cc/2020/Conference/Paper323/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper323/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper323/Authors|ICLR.cc/2020/Conference/Paper323/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173127, "tmdate": 1576860539121, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper323/Authors", "ICLR.cc/2020/Conference/Paper323/Reviewers", "ICLR.cc/2020/Conference/Paper323/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper323/-/Official_Comment"}}}, {"id": "B1lUAIeFsB", "original": null, "number": 7, "cdate": 1573615310318, "ddate": null, "tcdate": 1573615310318, "tmdate": 1573615310318, "tddate": null, "forum": "S1gyl6Vtvr", "replyto": "B1gnLfV4qB", "invitation": "ICLR.cc/2020/Conference/Paper323/-/Official_Comment", "content": {"title": "Thanks for the careful review", "comment": "We will revise the citation format accordingly."}, "signatures": ["ICLR.cc/2020/Conference/Paper323/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper323/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["raden.m.muaz@gmail.com", "lin-j@i2r.a-star.edu.sg", "vijay@i2r.a-star.edu.sg", "shinoda@ks.cs.titech.ac.jp"], "title": "MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning", "authors": ["Raden Mu'az Mun'im", "Jie Lin", "Vijay Chandrasekhar", "Koichi Shinoda"], "pdf": "/pdf/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "abstract": "In this paper, we propose a framework, called MaskConvNet, for ConvNets filter pruning. MaskConvNet provides elegant support for training budget-aware pruned networks from scratch, by adding a simple mask module to a ConvNet architecture. MaskConvNet enjoys several advantages - (1) Flexible, the mask module can be integrated with any ConvNets in a plug-and-play manner. (2) Simple, the mask module is implemented by a hard Sigmoid function with a small number of trainable mask variables, adding negligible memory and computational overheads to the networks during training. (3) Effective, it is able to achieve competitive pruning rate while maintaining comparable accuracy with the baseline ConvNets without pruning, regardless of the datasets and ConvNet architectures used. (4) Fast, it is observed that the number of training epochs required by MaskConvNet is close to training a baseline without pruning. (5) Budget-aware, with a sparsity budget on target metric (e.g. model size and FLOP), MaskConvNet is able to train in a way that the optimizer can adaptively sparsify the network and automatically maintain sparsity level, till the pruned network produces good accuracy and fulfill the budget constraint simultaneously. Results on CIFAR-10 and ImageNet with several ConvNet architectures show that MaskConvNet works competitively well compared to previous pruning methods, with budget-constraint well respected. Code is available at https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0. We hope MaskConvNet, as a simple and general pruning framework, can address the gaps in existing literate and advance future studies to push the boundaries of neural network pruning.", "code": "https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0", "keywords": ["Structured Pruning", "Sparsity Regularization", "Budget-Aware"], "paperhash": "munim|maskconvnet_training_efficient_convnets_from_scratch_via_budgetconstrained_filter_pruning", "original_pdf": "/attachment/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "_bibtex": "@misc{\nmun'im2020maskconvnet,\ntitle={MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning},\nauthor={Raden Mu'az Mun'im and Jie Lin and Vijay Chandrasekhar and Koichi Shinoda},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gyl6Vtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1gyl6Vtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper323/Authors", "ICLR.cc/2020/Conference/Paper323/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper323/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper323/Reviewers", "ICLR.cc/2020/Conference/Paper323/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper323/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper323/Authors|ICLR.cc/2020/Conference/Paper323/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173127, "tmdate": 1576860539121, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper323/Authors", "ICLR.cc/2020/Conference/Paper323/Reviewers", "ICLR.cc/2020/Conference/Paper323/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper323/-/Official_Comment"}}}, {"id": "rkezZAJtsS", "original": null, "number": 6, "cdate": 1573613050308, "ddate": null, "tcdate": 1573613050308, "tmdate": 1573613050308, "tddate": null, "forum": "S1gyl6Vtvr", "replyto": "rJeplPkCtB", "invitation": "ICLR.cc/2020/Conference/Paper323/-/Official_Comment", "content": {"title": "Author response to Reviewer #3", "comment": "\nThank you for reviewing our work. We try to correct all the grammatical mistakes and typos. Below we address your main concern.\n\nQ1: The main thing that bothered me about the method was the usage of Hard Sigmoid. If a mask component ever gets into one of the flat regions it won\u2019t be able to escape. The authors propose a workaround which they call \u201cmask decay update\u201d. This approach looks quite hacky and I\u2019m not sure how easy it is to make it work in practice.\n\nUnlike heuristic pruning approaches such as magnitude-based pruning via thresholding, trainable pruning involves differentiating through 0 values during back-propagation, which is a long-standing difficult problem in the literature of network pruning.\n\nIf learning mask as discrete binary values {0,1},  one cannot get true gradients during back-propagation, but only estimates under Expectation assumptions. For example, the principled approaches like Variational Dropout [1] and L0-regularization [2] proposed continuous relaxation/re-parameterization to learn the activation probability of the mask. However, this method has \"sparse\" gradients issue as noted by L0-regularization. BAR [3] used the same re-parametrization trick to learn mask and reported difficulty in getting good accuracy with this technique unless using some tuned linearly decreasing regularization strength and knowledge distillation. For SSL [4], MorphNet [5] and Network Slimming [6], once a weight parameter is set to 0 during training, it is always 0 and can never be re-activated again.\n\nOther options include (1) using REINFORCE to learn the mask values, but this has high variance issue, or (2) using the Straight-Through Estimator to backprop through 0 (i.e., defining the backward pass \u201cgradient\u201d as other functions such as identity function), but this has biased gradients estimate issue, as referred to discussions in [2].\nInstead of learning mask with discrete values, we proposed the soft-binarization approach \u2013 MaskConvNet, for differentiating through mostly continuous values on the Hard Sigmoid function, where the mask values are in the interval [0,1] instead of discrete {0,1}.\n\nOur method does not rely on costly and noisy sampling procedures, and not requiring regularization strength scheduling as in [3]. With soft-binarization, the model learns mask outputs with mostly continuous values. When the model is reaching the budget sparsity, the final pruned structure is also found. More importantly, the reverse mask weight-decay is a method to recover dead neurons if the pruning rate exceeds the pre-defined budget. Reverse weight-decay requires non-zero parameters to work, and this is the reason why we use hard-sigmoid: the actual mask parameters stay non-zero while the mask outputs are zero (when mask parameters are smaller than -0.5). In the appendix section, we show that this method is robust to hyper-parameter selections. Mask decay value is set to 1e-5 or less and EMA alpha=0.99 regardless of the network architectures and the datasets used.\n\nWe agree that other methods have their appeal due to their theoretical grounding (less \"hacky\"). However, we can see lots of \"hacks\" in regularization literature such as Dropout, Label Smoothing, Confidence Penalty, Cutout, Mixup, Knowledge Distillation, BatchNorm, etc. Some took years to be improved and be theoretically sound, e.g. Dropout -> Variational Dropout, MC Dropout, etc. And some still have no 100% agreed consensus or analytically proved on why it works, e.g. Knowledge Distillation and BatchNorm. Likewise, we leave the investigation for the theoretical analysis of reverse weight-decay for future work. \n\n[1] Variational Dropout and the Local Reparameterization Trick https://arxiv.org/pdf/1506.02557.pdf\n[2] Learning Sparse Neural Networks through L0 Regularization  https://arxiv.org/abs/1712.01312\n[3] Structured Pruning of Neural Networks With Budget-Aware Regularization http://openaccess.thecvf.com/content_CVPR_2019/html/Lemaire_Structured_Pruning_of_Neural_Networks_With_Budget-Aware_Regularization_CVPR_2019_paper.html\n[4] Learning Structured Sparsity in Deep Neural Networks https://arxiv.org/abs/1608.03665\n[5] Learning Efficient Convolutional Networks through Network Slimming https://arxiv.org/abs/1708.06519\n[6] MorphNet: Fast & Simple Resource-Constrained Structure Learning of Deep Networks https://arxiv.org/abs/1711.06798\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper323/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper323/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["raden.m.muaz@gmail.com", "lin-j@i2r.a-star.edu.sg", "vijay@i2r.a-star.edu.sg", "shinoda@ks.cs.titech.ac.jp"], "title": "MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning", "authors": ["Raden Mu'az Mun'im", "Jie Lin", "Vijay Chandrasekhar", "Koichi Shinoda"], "pdf": "/pdf/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "abstract": "In this paper, we propose a framework, called MaskConvNet, for ConvNets filter pruning. MaskConvNet provides elegant support for training budget-aware pruned networks from scratch, by adding a simple mask module to a ConvNet architecture. MaskConvNet enjoys several advantages - (1) Flexible, the mask module can be integrated with any ConvNets in a plug-and-play manner. (2) Simple, the mask module is implemented by a hard Sigmoid function with a small number of trainable mask variables, adding negligible memory and computational overheads to the networks during training. (3) Effective, it is able to achieve competitive pruning rate while maintaining comparable accuracy with the baseline ConvNets without pruning, regardless of the datasets and ConvNet architectures used. (4) Fast, it is observed that the number of training epochs required by MaskConvNet is close to training a baseline without pruning. (5) Budget-aware, with a sparsity budget on target metric (e.g. model size and FLOP), MaskConvNet is able to train in a way that the optimizer can adaptively sparsify the network and automatically maintain sparsity level, till the pruned network produces good accuracy and fulfill the budget constraint simultaneously. Results on CIFAR-10 and ImageNet with several ConvNet architectures show that MaskConvNet works competitively well compared to previous pruning methods, with budget-constraint well respected. Code is available at https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0. We hope MaskConvNet, as a simple and general pruning framework, can address the gaps in existing literate and advance future studies to push the boundaries of neural network pruning.", "code": "https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0", "keywords": ["Structured Pruning", "Sparsity Regularization", "Budget-Aware"], "paperhash": "munim|maskconvnet_training_efficient_convnets_from_scratch_via_budgetconstrained_filter_pruning", "original_pdf": "/attachment/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "_bibtex": "@misc{\nmun'im2020maskconvnet,\ntitle={MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning},\nauthor={Raden Mu'az Mun'im and Jie Lin and Vijay Chandrasekhar and Koichi Shinoda},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gyl6Vtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1gyl6Vtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper323/Authors", "ICLR.cc/2020/Conference/Paper323/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper323/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper323/Reviewers", "ICLR.cc/2020/Conference/Paper323/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper323/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper323/Authors|ICLR.cc/2020/Conference/Paper323/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173127, "tmdate": 1576860539121, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper323/Authors", "ICLR.cc/2020/Conference/Paper323/Reviewers", "ICLR.cc/2020/Conference/Paper323/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper323/-/Official_Comment"}}}, {"id": "SJgiDp1KjB", "original": null, "number": 5, "cdate": 1573612899513, "ddate": null, "tcdate": 1573612899513, "tmdate": 1573612899513, "tddate": null, "forum": "S1gyl6Vtvr", "replyto": "BkljV61FoS", "invitation": "ICLR.cc/2020/Conference/Paper323/-/Official_Comment", "content": {"title": "part 2 ==References==", "comment": "\n[1] Rethinking the Value of Network Pruning. https://arxiv.org/abs/1810.05270\n[2] AutoPruner: An End-to-End Trainable Filter Pruning Method for Efficient Deep Model Inference. https://arxiv.org/abs/1805.08941\n[3] Learning Efficient Convolutional Networks through Network Slimming https://arxiv.org/abs/1708.06519\n[4] MorphNet: Fast & Simple Resource-Constrained Structure Learning of Deep Networks https://arxiv.org/abs/1711.06798\n[5] Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks. https://arxiv.org/abs/1808.06866\n[6] Pruning Filters for Efficient ConvNets. https://arxiv.org/abs/1608.08710\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper323/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper323/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["raden.m.muaz@gmail.com", "lin-j@i2r.a-star.edu.sg", "vijay@i2r.a-star.edu.sg", "shinoda@ks.cs.titech.ac.jp"], "title": "MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning", "authors": ["Raden Mu'az Mun'im", "Jie Lin", "Vijay Chandrasekhar", "Koichi Shinoda"], "pdf": "/pdf/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "abstract": "In this paper, we propose a framework, called MaskConvNet, for ConvNets filter pruning. MaskConvNet provides elegant support for training budget-aware pruned networks from scratch, by adding a simple mask module to a ConvNet architecture. MaskConvNet enjoys several advantages - (1) Flexible, the mask module can be integrated with any ConvNets in a plug-and-play manner. (2) Simple, the mask module is implemented by a hard Sigmoid function with a small number of trainable mask variables, adding negligible memory and computational overheads to the networks during training. (3) Effective, it is able to achieve competitive pruning rate while maintaining comparable accuracy with the baseline ConvNets without pruning, regardless of the datasets and ConvNet architectures used. (4) Fast, it is observed that the number of training epochs required by MaskConvNet is close to training a baseline without pruning. (5) Budget-aware, with a sparsity budget on target metric (e.g. model size and FLOP), MaskConvNet is able to train in a way that the optimizer can adaptively sparsify the network and automatically maintain sparsity level, till the pruned network produces good accuracy and fulfill the budget constraint simultaneously. Results on CIFAR-10 and ImageNet with several ConvNet architectures show that MaskConvNet works competitively well compared to previous pruning methods, with budget-constraint well respected. Code is available at https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0. We hope MaskConvNet, as a simple and general pruning framework, can address the gaps in existing literate and advance future studies to push the boundaries of neural network pruning.", "code": "https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0", "keywords": ["Structured Pruning", "Sparsity Regularization", "Budget-Aware"], "paperhash": "munim|maskconvnet_training_efficient_convnets_from_scratch_via_budgetconstrained_filter_pruning", "original_pdf": "/attachment/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "_bibtex": "@misc{\nmun'im2020maskconvnet,\ntitle={MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning},\nauthor={Raden Mu'az Mun'im and Jie Lin and Vijay Chandrasekhar and Koichi Shinoda},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gyl6Vtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1gyl6Vtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper323/Authors", "ICLR.cc/2020/Conference/Paper323/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper323/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper323/Reviewers", "ICLR.cc/2020/Conference/Paper323/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper323/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper323/Authors|ICLR.cc/2020/Conference/Paper323/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173127, "tmdate": 1576860539121, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper323/Authors", "ICLR.cc/2020/Conference/Paper323/Reviewers", "ICLR.cc/2020/Conference/Paper323/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper323/-/Official_Comment"}}}, {"id": "BkljV61FoS", "original": null, "number": 4, "cdate": 1573612851174, "ddate": null, "tcdate": 1573612851174, "tmdate": 1573612851174, "tddate": null, "forum": "S1gyl6Vtvr", "replyto": "ryel6kz0Kr", "invitation": "ICLR.cc/2020/Conference/Paper323/-/Official_Comment", "content": {"title": "Author response to Reviewer #2", "comment": "Thank you for reviewing our work.\n\nQ1: In the methods part, the authors said that \u201cPrevious pruning approaches often prune Conv filters with their successive Batch Normalization layer unchanged.\u201d Can the authors give some reference here as to which pruning approaches?\n\nMost of the train-prune-retrain pruning approaches suffer this issue. For example, Network Slimming [3] and MorphNet [4] proposed to sparsify BN scale/shift parameters. However, even though the scale is exactly 0, when they pruned the corresponding filter (i.e. cut down output depth for current Conv layer and input depth for the next Conv layer), the accuracy of the pruned model drops and must be fine-tuned. According to the appendix section of MorphNet, the BN statistics are disrupted after pruning and must be corrected with several thousand iterations with a tiny learning rate.\nOur method masked the filters of Covn/FC layer and the corresponding BN layer with the same mask vector to condition BN to ignore the masked filters during training, thus avoid the requirement for finetuning.\n\n\n\nQ2: Did the authors compare the proposed approach to training the pruned networks from scratch as done in [1]? Also can the authors analyze the sparsity patterns of the pruned networks as done in section G in the appendix of [1]?\n\nWe added experiments to compare our method with the train-from-scratch method named Soft Filter Pruning (SFP) [5] with ResNet56 v1 on CIFAR10, as also done in the paper \u201cRethinking the value of network pruning\u201d [1]. SFP pruned all layers in ResNet56 with a pre-defined uniform pruning rate across layers. For a fair comparison, we trained the masked ResNet56 by masking all the layers in each basic block (residual connection). Results show that our method and SFP (without fine-tuning) achieves 92.9% and 93.1% accuracy respectively, when both methods prune 30% of the weight parameters. (Note that our results on ResNet56 are a bit different from the numbers shown in Table 1 in our submission. For a fair comparison with L1-Pruning [6], we trained ResNet56 without masking the last Conv layer in each basic block, as what L1-Pruning did in the experiments.) \nWe also show the sparsity patterns of the pruned networks for VGG19 (pruned 90% of the weight parameters, with accuracy 93.5% on CIFAR10) and ResNet56 (pruned 30% of the parameters, with accuracy 92.9% on CIFAR10). One can see that the pattern emerged from our method is similar to the pattern observed in the paper \u201cRethinking the value of network pruning\u201d, e.g. for VGG19, late layers tend to have more redundancy than early layers, and for ResNet56, the pruning ratio tends to be uniform across stages.\n\n============================================================= ========\nLayer-wise sparsity pattern for VGG19 with 90% of weight parameters pruned.\nLayer#\tFilters ratio to be kept\nConv 1\t0.734\nConv 2\t0.984\nConv 3\t0.961\nConv 4\t0.992\nConv 5\t0.914\nConv 6\t0.758\nConv 7\t0.617\nConv 8\t0.520\nConv 9\t0.250\nConv 10\t0.172\nConv 11\t0.127\nConv 12\t0.115\nConv 13\t0.146\nConv 14\t0.145\nConv 15\t0.160\nConv 16\t0.361\n============================================================= ========\n============================================================= ========\nStage-wise sparsity pattern for ResNet56 with 30% of weight parameters pruned.\nLayer#\tParams ratio to be kept\nStage 1\t0.783\nStage 2\t0.681\nStage 3\t0.878\n============================================================= ========\n\n\nQ3: What is the difference of your approach to [2]? They seem to be very similar. I think it is necessary to add some discussion in the related work. Is there any experimental results for comparison with [2]?\n\nThanks for pointing us the AutoPruner paper, we will add it to our reference. Basically, AutoPruner proposed to prune weight by directly masking activation responses, rather than weight filters. Since activation responses are not learnable parameters, they add a AutoPruner layer to the network architecture. The AutoPrunner layer takes activation responses as input and the output is an approximate binary code in which zero value indicates the corresponding filter will be pruned. The AutoPrunner layer composes of average pooling,  max pooling, a fully-connected layer followed by a scaled sigmoid function, which introduces a large number of additional trainable parameters and computational complexity to the network.\nWe would like to run AutoPruner experiments for comparisons, unfortunately, we didn\u2019t find any source codes available on Github or the author\u2019s website. We will probably implement the method by ourselves and add comparisons in the near future.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper323/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper323/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["raden.m.muaz@gmail.com", "lin-j@i2r.a-star.edu.sg", "vijay@i2r.a-star.edu.sg", "shinoda@ks.cs.titech.ac.jp"], "title": "MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning", "authors": ["Raden Mu'az Mun'im", "Jie Lin", "Vijay Chandrasekhar", "Koichi Shinoda"], "pdf": "/pdf/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "abstract": "In this paper, we propose a framework, called MaskConvNet, for ConvNets filter pruning. MaskConvNet provides elegant support for training budget-aware pruned networks from scratch, by adding a simple mask module to a ConvNet architecture. MaskConvNet enjoys several advantages - (1) Flexible, the mask module can be integrated with any ConvNets in a plug-and-play manner. (2) Simple, the mask module is implemented by a hard Sigmoid function with a small number of trainable mask variables, adding negligible memory and computational overheads to the networks during training. (3) Effective, it is able to achieve competitive pruning rate while maintaining comparable accuracy with the baseline ConvNets without pruning, regardless of the datasets and ConvNet architectures used. (4) Fast, it is observed that the number of training epochs required by MaskConvNet is close to training a baseline without pruning. (5) Budget-aware, with a sparsity budget on target metric (e.g. model size and FLOP), MaskConvNet is able to train in a way that the optimizer can adaptively sparsify the network and automatically maintain sparsity level, till the pruned network produces good accuracy and fulfill the budget constraint simultaneously. Results on CIFAR-10 and ImageNet with several ConvNet architectures show that MaskConvNet works competitively well compared to previous pruning methods, with budget-constraint well respected. Code is available at https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0. We hope MaskConvNet, as a simple and general pruning framework, can address the gaps in existing literate and advance future studies to push the boundaries of neural network pruning.", "code": "https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0", "keywords": ["Structured Pruning", "Sparsity Regularization", "Budget-Aware"], "paperhash": "munim|maskconvnet_training_efficient_convnets_from_scratch_via_budgetconstrained_filter_pruning", "original_pdf": "/attachment/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "_bibtex": "@misc{\nmun'im2020maskconvnet,\ntitle={MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning},\nauthor={Raden Mu'az Mun'im and Jie Lin and Vijay Chandrasekhar and Koichi Shinoda},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gyl6Vtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1gyl6Vtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper323/Authors", "ICLR.cc/2020/Conference/Paper323/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper323/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper323/Reviewers", "ICLR.cc/2020/Conference/Paper323/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper323/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper323/Authors|ICLR.cc/2020/Conference/Paper323/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173127, "tmdate": 1576860539121, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper323/Authors", "ICLR.cc/2020/Conference/Paper323/Reviewers", "ICLR.cc/2020/Conference/Paper323/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper323/-/Official_Comment"}}}, {"id": "HylNL2G_iH", "original": null, "number": 3, "cdate": 1573559371673, "ddate": null, "tcdate": 1573559371673, "tmdate": 1573612593855, "tddate": null, "forum": "S1gyl6Vtvr", "replyto": "SkxyX2zdiB", "invitation": "ICLR.cc/2020/Conference/Paper323/-/Official_Comment", "content": {"title": "Part 3", "comment": "Q11: My understanding is that the regularization multiplier is affected by the learning rate, therefore their effect is lower as the training progresses. In this case, seems the opposite, right? (page 6 \u202abefore 3.4\u202c). \u202c\u202c\nYes, the regularization multiplier is indirectly affected by the learning rate. As shown in Algorithm 1, the regularization multiplier is updated by \\delta_s (\\delta_s = target_sparsity \u2013 current_sparsity), and \\delta_s is mainly driven by the mask values (output by Hard Sigmoid). The larger learning rate will drive the mask values to 0 faster. In our experiments, the learning rate is scheduled to be divided by 10 in certain epochs. Thus, the mask becomes sparse quickly in early epochs, and stabilizes later. Correspondingly, \\delta_s moves to 0 quickly in early epochs, and so does the regularization multiplier.\n\n\nQ12: The part with the sparsity budget is interesting. What guarantees are that the newly enabled neurons are actually useful? Could it be possible that the budget suggested is not the right for the task at hand and, therefore, the additional parameters are not really relevant / needed? \nOur method doesn\u2019t set any hand-crafted pruning criteria. Instead, with a pre-defined budget, the trainable mask layer enforces the back-propagation algorithm to automatically decide which filter is important and which is not, by updating the mask parameters. We observed that the pruned filters also follow the smaller-norm-less-informative rule, which is a common pruning criteria adopted by many prior works like Network Slimming.\nFor any tasks, there is a trade-off between accuracy and pruning rate. In principle our algorithm is able to find the pruned networks that can meet any budget, via balancing the cross-entropy loss and regularization loss. The question is how to maintain the accuracy even for very high pruning rate, which depends on the redundancy of the original network architecture, the difficulty of the task as well as the data scale, etc.\n\n\nQ13: There is loss with little sparsity when it comes to Imagenet (15 and 17% pruning) does not seem very promising even the training time is similar. \nFor a fair comparison with the L1-pruning [4] with ResNet34 on ImageNet, we didn\u2019t prune the last Conv layer in each basic block (as done in L1-pruning). This is one reason the pruning rate is limited. Another reason is we found that for ImageNet task shallower ResNet is harder to prune as it is less redundant.\n\n\nQ14: Seems like the experiments and comparisons to L1-pruning are not very surprising. It would be nice to have more comprehensive numbers. For imagenet, if using Resnet-50, would be easier to compare to other numbers. \nDue to limited computation resources, we are still working on training on ImageNet with Resnet-50. Currently, we found that shallower ResNets are quite parameter-efficient, and putting high sparsity budget will severely degrade accuracy.\n\n\nQ15: In the imagenet comparison, L1-pruning is the version-A of the paper. What about the others or why that particular model?\nWe could try all sparsity budgets but did not do, as training a ResNet model from scratch on ImageNet really takes a long time given that we only have limited computation resources. But we will definitely include more results once we have them.\n\n\nQ16: How are the actual groups made? \nDo you mean the groups in Table 1? We grouped by network architectures, i.e. VGG-16, VGG-19, ResNet56, ResNet-164, and WideResNet-28-10 as used in respective prior work on CIFAR10.\n\n\nQ17: I do believe the WideResNet-28-10 number of parameters for BAR is not correct.\nThanks. We read the supplementary materials of the BAR paper and derived the numbers from there. We checked again and cannot find why the number of parameters is not correct. Would appreciate it if you can share more details and we will look into this accordingly. \n\n\n[1] Learning Efficient Convolutional Networks through Network Slimming https://arxiv.org/abs/1708.06519\n[2] MorphNet: Fast & Simple Resource-Constrained Structure Learning of Deep Networks https://arxiv.org/abs/1711.06798\n[3] PREDICTABILITY MINIMIZATION http://people.idsia.ch/~juergen/edgedetect/node2.html\n[4] Pruning Filters for Efficient ConvNets. https://arxiv.org/abs/1608.08710"}, "signatures": ["ICLR.cc/2020/Conference/Paper323/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper323/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["raden.m.muaz@gmail.com", "lin-j@i2r.a-star.edu.sg", "vijay@i2r.a-star.edu.sg", "shinoda@ks.cs.titech.ac.jp"], "title": "MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning", "authors": ["Raden Mu'az Mun'im", "Jie Lin", "Vijay Chandrasekhar", "Koichi Shinoda"], "pdf": "/pdf/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "abstract": "In this paper, we propose a framework, called MaskConvNet, for ConvNets filter pruning. MaskConvNet provides elegant support for training budget-aware pruned networks from scratch, by adding a simple mask module to a ConvNet architecture. MaskConvNet enjoys several advantages - (1) Flexible, the mask module can be integrated with any ConvNets in a plug-and-play manner. (2) Simple, the mask module is implemented by a hard Sigmoid function with a small number of trainable mask variables, adding negligible memory and computational overheads to the networks during training. (3) Effective, it is able to achieve competitive pruning rate while maintaining comparable accuracy with the baseline ConvNets without pruning, regardless of the datasets and ConvNet architectures used. (4) Fast, it is observed that the number of training epochs required by MaskConvNet is close to training a baseline without pruning. (5) Budget-aware, with a sparsity budget on target metric (e.g. model size and FLOP), MaskConvNet is able to train in a way that the optimizer can adaptively sparsify the network and automatically maintain sparsity level, till the pruned network produces good accuracy and fulfill the budget constraint simultaneously. Results on CIFAR-10 and ImageNet with several ConvNet architectures show that MaskConvNet works competitively well compared to previous pruning methods, with budget-constraint well respected. Code is available at https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0. We hope MaskConvNet, as a simple and general pruning framework, can address the gaps in existing literate and advance future studies to push the boundaries of neural network pruning.", "code": "https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0", "keywords": ["Structured Pruning", "Sparsity Regularization", "Budget-Aware"], "paperhash": "munim|maskconvnet_training_efficient_convnets_from_scratch_via_budgetconstrained_filter_pruning", "original_pdf": "/attachment/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "_bibtex": "@misc{\nmun'im2020maskconvnet,\ntitle={MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning},\nauthor={Raden Mu'az Mun'im and Jie Lin and Vijay Chandrasekhar and Koichi Shinoda},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gyl6Vtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1gyl6Vtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper323/Authors", "ICLR.cc/2020/Conference/Paper323/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper323/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper323/Reviewers", "ICLR.cc/2020/Conference/Paper323/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper323/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper323/Authors|ICLR.cc/2020/Conference/Paper323/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173127, "tmdate": 1576860539121, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper323/Authors", "ICLR.cc/2020/Conference/Paper323/Reviewers", "ICLR.cc/2020/Conference/Paper323/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper323/-/Official_Comment"}}}, {"id": "SkxyX2zdiB", "original": null, "number": 2, "cdate": 1573559318788, "ddate": null, "tcdate": 1573559318788, "tmdate": 1573612465079, "tddate": null, "forum": "S1gyl6Vtvr", "replyto": "Bkl2aoGdjB", "invitation": "ICLR.cc/2020/Conference/Paper323/-/Official_Comment", "content": {"title": "Part 2", "comment": "\nQ7: I am not sure if the claim of pruning filters and not considering the bn layer is correct. I would tend to think that, if pruning a neuron, the corresponding BN module should be modified (that is, propagating the zero to subsequent layers).\nApologies for the ambiguity caused. We would like to clarify that the mismatch problem between Conv and BN layers exists for any filter pruning approaches that with threshold-based \"hard\" pruning. Given a Conv layer of a trained model, in the forward pass, filters with norm smaller than a pre-defined threshold are pruned away, the corresponding output activation maps with non-zero values are forced to be zero and fed to the successive BN layer. Similarly, in the backward pass, the gradients of output activation maps of this BN layer are also affected by its successive Conv layer with filters being pruned. Thus, the BN running mean/variance and trainable parameters (scale and shift) need to be adapted, due to the change of both the input and output activation maps of this BN layer, given that the BN statistics and parameters depend on all the input and output values respectively. This is the main reason that (iterative) fine-tuning is required after each \"hard\" pruning.\n\n\nQ8: I do like the extension to residual connections. It would be great to have more and clearer details on how is this done. Would this also apply to the UNET type of architecture?\nThe main difficulty for pruning residual connections is the input activation maps for layers pointed to the element-wise addition layer must be aligned after filter pruning. There are 3 options for handling residual connections. First, as done in L1-pruning [4], one can skip the pruning of the shortcut and the last Conv layer in each basic block. In other words, there are no mask parameters to be learned for these layers. Second, the mask can be enforced to be shared between the layers pointed to the element-wise addition layer. Task ResNet56 v1 as an example, 2 mask layers are designed for a basic block with a shortcut path and another path composed of 2 consecutive Conv layers. One mask is for the first Conv layer, and the other mask is shared by the shortcut and the last Conv in the block. Third, one can also take a step forward to not share the mask between the shortcut and the last Conv in each basic block. Instead, we let the algorithm automatically search the optimal mask for each layer and store the mask in the pruned model, the mask vector is with dimension equals the number of output depth, in which 0 indicates the corresponding filter is pruned. When executing the element-wise addition operations, we align (i.e. reshape) the output activation maps using the mask for each layer independently, then sum the well-aligned activation maps together.\n\nIn principle, mask sharing can be applied to UNET architecture as well. In this case a layer pair in U-residual connection must share the same mask to enforce the same output depth for residual addition operation to work. \n\n\n Q9: What is the intuition behind Eq 4 and how it is related to the relevance of a parameter? \nMinimizing mean is essentially minimizing L1-norm on mask vector, and maximizing variance is soft-binarization [3] which pushes mask values to either 0 or 1. In this way, important neurons will quickly gain mask value of 1. Empirically we found that normalizing variance by dividing with mean is more reliable than variance alone. Otherwise the masks will go to 0 and 1 too quickly and somehow cannot be reversed.\n\n\nQ10: The extension to multiple metrics is of interest, however, there is little detailed there. How is the automatic allocation done? This is repeated in section 3.3 but details are missing. \nA simple heuristic we proposed is to average the mean and variance-to-mean of the masks from different metrics. For example, to optimize 2 metrics (parameter size and FLOP) simultaneously, we first compute the mean and variance-to-mean of the shared mask, which are multiplied by the dynamic \\lambda parameters for each metric separately, then sum them together.\nAlgorithm 1 describes the main training steps for budget-aware MaskConvNet, in which \\delta_s is approaching to 0 once the budget is met. The corresponding mask parameters are the automatic allocation solution for filter pruning. Section 3.4 explains how the algorithm re-activates pruned filters when the pruning rate exceeds the budget.\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper323/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper323/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["raden.m.muaz@gmail.com", "lin-j@i2r.a-star.edu.sg", "vijay@i2r.a-star.edu.sg", "shinoda@ks.cs.titech.ac.jp"], "title": "MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning", "authors": ["Raden Mu'az Mun'im", "Jie Lin", "Vijay Chandrasekhar", "Koichi Shinoda"], "pdf": "/pdf/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "abstract": "In this paper, we propose a framework, called MaskConvNet, for ConvNets filter pruning. MaskConvNet provides elegant support for training budget-aware pruned networks from scratch, by adding a simple mask module to a ConvNet architecture. MaskConvNet enjoys several advantages - (1) Flexible, the mask module can be integrated with any ConvNets in a plug-and-play manner. (2) Simple, the mask module is implemented by a hard Sigmoid function with a small number of trainable mask variables, adding negligible memory and computational overheads to the networks during training. (3) Effective, it is able to achieve competitive pruning rate while maintaining comparable accuracy with the baseline ConvNets without pruning, regardless of the datasets and ConvNet architectures used. (4) Fast, it is observed that the number of training epochs required by MaskConvNet is close to training a baseline without pruning. (5) Budget-aware, with a sparsity budget on target metric (e.g. model size and FLOP), MaskConvNet is able to train in a way that the optimizer can adaptively sparsify the network and automatically maintain sparsity level, till the pruned network produces good accuracy and fulfill the budget constraint simultaneously. Results on CIFAR-10 and ImageNet with several ConvNet architectures show that MaskConvNet works competitively well compared to previous pruning methods, with budget-constraint well respected. Code is available at https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0. We hope MaskConvNet, as a simple and general pruning framework, can address the gaps in existing literate and advance future studies to push the boundaries of neural network pruning.", "code": "https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0", "keywords": ["Structured Pruning", "Sparsity Regularization", "Budget-Aware"], "paperhash": "munim|maskconvnet_training_efficient_convnets_from_scratch_via_budgetconstrained_filter_pruning", "original_pdf": "/attachment/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "_bibtex": "@misc{\nmun'im2020maskconvnet,\ntitle={MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning},\nauthor={Raden Mu'az Mun'im and Jie Lin and Vijay Chandrasekhar and Koichi Shinoda},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gyl6Vtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1gyl6Vtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper323/Authors", "ICLR.cc/2020/Conference/Paper323/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper323/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper323/Reviewers", "ICLR.cc/2020/Conference/Paper323/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper323/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper323/Authors|ICLR.cc/2020/Conference/Paper323/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173127, "tmdate": 1576860539121, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper323/Authors", "ICLR.cc/2020/Conference/Paper323/Reviewers", "ICLR.cc/2020/Conference/Paper323/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper323/-/Official_Comment"}}}, {"id": "Bkl2aoGdjB", "original": null, "number": 1, "cdate": 1573559235862, "ddate": null, "tcdate": 1573559235862, "tmdate": 1573559235862, "tddate": null, "forum": "S1gyl6Vtvr", "replyto": "S1eo1JwJ9S", "invitation": "ICLR.cc/2020/Conference/Paper323/-/Official_Comment", "content": {"title": "Author response to Reviewer #1", "comment": "Thank you for reviewing our work. We answer your questions below:\n\nQ1: How does this relate to methods using gating to prune in the presence of residual layers or BN?\nPrevious works prune weights/filters/layers via (1) L0/L1 regularization or variational dropout on weights/BN, or (2) simple binary thresholding defined over weight magnitudes / filters / BN norms. All these works can be treated as \u201chard\u201d pruning through a gating function, where a hand-crafted threshold is required to make decision whether or not to prune. Thus, (iterative) fine-tuning is introduced to amortize the accuracy loss due to hard pruning.\nOur work differs from the prior art in that we perform \u201csoft\u201d pruning where a soft filter-wise mask with continuous values in [0,1] is automatically learned together with filter weight parameters, and filters are reparametrized by element-wise multiplications between filter mask and filter weights. Filters with mask value 0 are pruned away from the network architecture. Thus, fine-tuning after training is not required anymore, and also threshold is not required thanks to the use of Hard Sigmoid as the activation function for the mask layer.\n\n\nQ2: Paper claims multiple times that related works need a train-prune-retrain process which is only valid for post-processing works.\nAgree. We will update our draft to make the point more clearly.\n\n\nQ3: I missed sparsity promoting works related to training from scratch where similar masks are implicitly used during training (even not explicitly stated) to maintain the zeros in the network. At least in [1,2] the training time is the same as the time used for training a network from scratch (same claim as in this paper).\nThanks for pointing us these 2 papers, we will add them to the reference. As mentioned in our draft, sparsity learning based pruning via L0/L1 regularization or variational dropout is similar to our work in the sense that they learn mask to prune the network. However, these works suffer from the problem that the sparsity is enforced for parameter size only, and not directly related to the target metric (e.g. FLOPs, or energy). While the mask defined in our work supports metrics beyond parameter size seamlessly.\n\n\nQ4: The mask is trained using a sigmoid function and claims this will be representative of the relevance of the 'neuron' within the layer. How is this really related?\nIn early training all mask parameters are initialized as 0 and fed into hard-sigmoid activation function, the output mask is 0.5 for all filters, meaning that all filters are with equal importance. Like filter weight parameters, these mask parameters will also be continuously updated with SGD optimizer during training, until the model converges and meets the sparsity budget. According to the hard-sigmoid function, the final output mask is in the range of [0,1], important neurons will have mask output of 1, and unimportant neurons will have mask output of 0.\n\n\nQ5: Why the initialization of the mask is to the mean? I think I am confused there. For initialization, I would argue all the neurons/parameters are relevant, right?\nTo clarify, the mask after Hard Sigmoid is all initialized as 0.5, indicating that all filters are with the equal importance; we think this is reasonable as there is no prior knowledge on which filters are more important than the others at the beginning of training. \nWe also investigated other initialization methods for the mask, such as all mask initialized as 1 or random Gaussian. Results are comparable to initialization with 0.5. Another reason we initialize the mask with constant is we don't want to disrupt He/Xavier initialization.\n\n\nQ6: The claim that this method is 'much simpler' is a bit subjective. I do not see why. Please elaborate. \nWe admit it is a bit subjective and will remove it accordingly. The message we would like to deliver is that the proposed mask module is light-weight and the additional computational cost introduced during training can be ignored. Moreover, by using shared BN mask, we did not need to retrain or fine-tune the pruned model, unlike previous work such as MorphNet[1] and Network Slimming[2]\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper323/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper323/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["raden.m.muaz@gmail.com", "lin-j@i2r.a-star.edu.sg", "vijay@i2r.a-star.edu.sg", "shinoda@ks.cs.titech.ac.jp"], "title": "MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning", "authors": ["Raden Mu'az Mun'im", "Jie Lin", "Vijay Chandrasekhar", "Koichi Shinoda"], "pdf": "/pdf/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "abstract": "In this paper, we propose a framework, called MaskConvNet, for ConvNets filter pruning. MaskConvNet provides elegant support for training budget-aware pruned networks from scratch, by adding a simple mask module to a ConvNet architecture. MaskConvNet enjoys several advantages - (1) Flexible, the mask module can be integrated with any ConvNets in a plug-and-play manner. (2) Simple, the mask module is implemented by a hard Sigmoid function with a small number of trainable mask variables, adding negligible memory and computational overheads to the networks during training. (3) Effective, it is able to achieve competitive pruning rate while maintaining comparable accuracy with the baseline ConvNets without pruning, regardless of the datasets and ConvNet architectures used. (4) Fast, it is observed that the number of training epochs required by MaskConvNet is close to training a baseline without pruning. (5) Budget-aware, with a sparsity budget on target metric (e.g. model size and FLOP), MaskConvNet is able to train in a way that the optimizer can adaptively sparsify the network and automatically maintain sparsity level, till the pruned network produces good accuracy and fulfill the budget constraint simultaneously. Results on CIFAR-10 and ImageNet with several ConvNet architectures show that MaskConvNet works competitively well compared to previous pruning methods, with budget-constraint well respected. Code is available at https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0. We hope MaskConvNet, as a simple and general pruning framework, can address the gaps in existing literate and advance future studies to push the boundaries of neural network pruning.", "code": "https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0", "keywords": ["Structured Pruning", "Sparsity Regularization", "Budget-Aware"], "paperhash": "munim|maskconvnet_training_efficient_convnets_from_scratch_via_budgetconstrained_filter_pruning", "original_pdf": "/attachment/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "_bibtex": "@misc{\nmun'im2020maskconvnet,\ntitle={MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning},\nauthor={Raden Mu'az Mun'im and Jie Lin and Vijay Chandrasekhar and Koichi Shinoda},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gyl6Vtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1gyl6Vtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper323/Authors", "ICLR.cc/2020/Conference/Paper323/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper323/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper323/Reviewers", "ICLR.cc/2020/Conference/Paper323/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper323/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper323/Authors|ICLR.cc/2020/Conference/Paper323/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173127, "tmdate": 1576860539121, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper323/Authors", "ICLR.cc/2020/Conference/Paper323/Reviewers", "ICLR.cc/2020/Conference/Paper323/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper323/-/Official_Comment"}}}, {"id": "rJeplPkCtB", "original": null, "number": 1, "cdate": 1571841781013, "ddate": null, "tcdate": 1571841781013, "tmdate": 1572972609856, "tddate": null, "forum": "S1gyl6Vtvr", "replyto": "S1gyl6Vtvr", "invitation": "ICLR.cc/2020/Conference/Paper323/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a framework for training time filter pruning for convolutional neural networks. The main idea is to use a trainable soft binary mask to zero out convolutional filters and corresponding batch norm parameters.\n\nPros:\n+ The proposed method seems relatively easy to implement.\n+ The quantitative results in the paper indicate that MaskConvNet achieves performance competitive with previously proposed pruning methods.\n\nCons:\n- Writing of the paper could be significantly improved. See some examples below.\n- The main thing that bothered me about the method was the usage of hard sigmoid. If a mask component ever gets into one of the flat regions it won\u2019t be able to escape. The authors propose a workaround which they call \u201cmask decay update\u201d. This approach looks quite hacky and I\u2019m not sure how easy it is to make it work in practice.\n\nNotes/questions:\n* Abstract: \u201celegant support\u201d -> \u201csupport\u201d\n* Everywhere in the text: Back-to-back citations should have the form (citation1; citation2; \u2026)\n* Section 1, paragraph 3: \u201csuffer one\u201d -> \u201csuffer from one\u201d\n* Section 1, paragraph 4: \u201cabove mentioned\u201d -> \u201cabove-mentioned\u201d\n* Figure 1: The figure would greatly benefit from a detailed description. What\u2019s IF, OF and OF? The reader shouldn\u2019t be guessing. \n* Section 2, paragraph 3: \u201ccorresponded\u201d -> \u201ccorresponding\u201d\n* Section 3.1, paragraph 2: \u201cW \\in R\u201d \u2013 W is probably not a scalar value therefore it\u2019s in R^n. The same goes for the mask.\n* Section 3.1, paragraph 2: \u201cIt\u2019s easy to know ...\u201d \u2013 this sentence needs to be rewritten, e.g., \u201cOne can see that \u2026\u201d\n* Section 3.1, paragraph 2: \u201csparser\u201d -> \u201cmore sparse\u201d\n* Section 3.2, \u201cExtension to Multi-metrics\u201d: \u201cFLOPs\u201d are never defined in the paper. How is this quantity computed exactly? I\u2019m also not entirely sure how useful it is to introduce multiple lambdas \u2013 it seems that this case corresponds to a single lambda which is a sum \\lambda_i.\n* Section 3.3, paragraph 1: \u201cundersparsed\u201d, \u201coversparsed\u201d \u2013 not sure if these words exist. Maybe rephrase instead of introducing new terms?\n* Section 3.3, paragraph 1: \u201cvery laborious\u201d -> \u201claborious\u201d\n* Figure 3: Why not show points all the way to 0 sparsity?\n* Section 4.2, CIFAR-10: The authors mention that (Lemaire et al., 19) achieve better FLOP sparsity due to usage of Knowledge Distillation. From this sentence alone it\u2019s not clear how exactly KD helps. Why can\u2019t KD be applied in the proposed framework? I\u2019d appreciate if the authors could elaborate on this.\n\nI must admit that I\u2019m not an expert in the field of NN pruning but I\u2019m surprised that training-time masking of filters has not been tried before. Even if it\u2019s really the case I\u2019m not entirely confident that the paper should be accepted: the \u201cunsparsification\u201d looks more like a hack than a principled approach and the overall quality of writing needs to be improved. I\u2019m giving a borderline score for now but I\u2019m willing to increase it provided that the rebuttal addresses my concerns."}, "signatures": ["ICLR.cc/2020/Conference/Paper323/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper323/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["raden.m.muaz@gmail.com", "lin-j@i2r.a-star.edu.sg", "vijay@i2r.a-star.edu.sg", "shinoda@ks.cs.titech.ac.jp"], "title": "MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning", "authors": ["Raden Mu'az Mun'im", "Jie Lin", "Vijay Chandrasekhar", "Koichi Shinoda"], "pdf": "/pdf/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "abstract": "In this paper, we propose a framework, called MaskConvNet, for ConvNets filter pruning. MaskConvNet provides elegant support for training budget-aware pruned networks from scratch, by adding a simple mask module to a ConvNet architecture. MaskConvNet enjoys several advantages - (1) Flexible, the mask module can be integrated with any ConvNets in a plug-and-play manner. (2) Simple, the mask module is implemented by a hard Sigmoid function with a small number of trainable mask variables, adding negligible memory and computational overheads to the networks during training. (3) Effective, it is able to achieve competitive pruning rate while maintaining comparable accuracy with the baseline ConvNets without pruning, regardless of the datasets and ConvNet architectures used. (4) Fast, it is observed that the number of training epochs required by MaskConvNet is close to training a baseline without pruning. (5) Budget-aware, with a sparsity budget on target metric (e.g. model size and FLOP), MaskConvNet is able to train in a way that the optimizer can adaptively sparsify the network and automatically maintain sparsity level, till the pruned network produces good accuracy and fulfill the budget constraint simultaneously. Results on CIFAR-10 and ImageNet with several ConvNet architectures show that MaskConvNet works competitively well compared to previous pruning methods, with budget-constraint well respected. Code is available at https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0. We hope MaskConvNet, as a simple and general pruning framework, can address the gaps in existing literate and advance future studies to push the boundaries of neural network pruning.", "code": "https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0", "keywords": ["Structured Pruning", "Sparsity Regularization", "Budget-Aware"], "paperhash": "munim|maskconvnet_training_efficient_convnets_from_scratch_via_budgetconstrained_filter_pruning", "original_pdf": "/attachment/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "_bibtex": "@misc{\nmun'im2020maskconvnet,\ntitle={MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning},\nauthor={Raden Mu'az Mun'im and Jie Lin and Vijay Chandrasekhar and Koichi Shinoda},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gyl6Vtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1gyl6Vtvr", "replyto": "S1gyl6Vtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper323/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper323/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575319630094, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper323/Reviewers"], "noninvitees": [], "tcdate": 1570237753797, "tmdate": 1575319630107, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper323/-/Official_Review"}}}, {"id": "ryel6kz0Kr", "original": null, "number": 2, "cdate": 1571852216045, "ddate": null, "tcdate": 1571852216045, "tmdate": 1572972609800, "tddate": null, "forum": "S1gyl6Vtvr", "replyto": "S1gyl6Vtvr", "invitation": "ICLR.cc/2020/Conference/Paper323/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this work, the authors propose a network pruning method to learn a pruned network during training. Specifically, they add a pruning mask for each layer and induce a sparisity loss on the mask variables during training. The pruned network is obtained by applying the learned mask to the networks.\n\nThe paper seems to be well contained. However, my assessment of this paper is weak reject. I am mainly concerned with the novelty of this method. Also i think some more evaluation is needed to fully understand the effectiveness of this method. My questions are summarized as follows:\n\nQ1: In the methods part, the authors said that \u201cPrevious pruning approaches often prune Conv filters with their successive Batch Normalization layer unchanged.\u201d Can the authors give some reference here as to which pruning approaches?\n\nQ2: Did the authors compare the proposed approach to training the pruned networks from scratch as done in [1]? Also can the authors analyze the sparsity patterns of the pruned networks as done in section G in the appendix of [1]?\n\nQ3: What is the difference of your approach to [2]? They seem to be very similar. I think it is necessary to add some discussion in the related work. Is there any experimental results for comparison with [2]?\n\n[1] Rethinking the Value of Network Pruning. Liu et al. ICLR 2019\n[2] AutoPruner: An End-to-End Trainable Filter Pruning Method for Efficient Deep Model Inference. Luo et al. Arxiv, 2018."}, "signatures": ["ICLR.cc/2020/Conference/Paper323/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper323/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["raden.m.muaz@gmail.com", "lin-j@i2r.a-star.edu.sg", "vijay@i2r.a-star.edu.sg", "shinoda@ks.cs.titech.ac.jp"], "title": "MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning", "authors": ["Raden Mu'az Mun'im", "Jie Lin", "Vijay Chandrasekhar", "Koichi Shinoda"], "pdf": "/pdf/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "abstract": "In this paper, we propose a framework, called MaskConvNet, for ConvNets filter pruning. MaskConvNet provides elegant support for training budget-aware pruned networks from scratch, by adding a simple mask module to a ConvNet architecture. MaskConvNet enjoys several advantages - (1) Flexible, the mask module can be integrated with any ConvNets in a plug-and-play manner. (2) Simple, the mask module is implemented by a hard Sigmoid function with a small number of trainable mask variables, adding negligible memory and computational overheads to the networks during training. (3) Effective, it is able to achieve competitive pruning rate while maintaining comparable accuracy with the baseline ConvNets without pruning, regardless of the datasets and ConvNet architectures used. (4) Fast, it is observed that the number of training epochs required by MaskConvNet is close to training a baseline without pruning. (5) Budget-aware, with a sparsity budget on target metric (e.g. model size and FLOP), MaskConvNet is able to train in a way that the optimizer can adaptively sparsify the network and automatically maintain sparsity level, till the pruned network produces good accuracy and fulfill the budget constraint simultaneously. Results on CIFAR-10 and ImageNet with several ConvNet architectures show that MaskConvNet works competitively well compared to previous pruning methods, with budget-constraint well respected. Code is available at https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0. We hope MaskConvNet, as a simple and general pruning framework, can address the gaps in existing literate and advance future studies to push the boundaries of neural network pruning.", "code": "https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0", "keywords": ["Structured Pruning", "Sparsity Regularization", "Budget-Aware"], "paperhash": "munim|maskconvnet_training_efficient_convnets_from_scratch_via_budgetconstrained_filter_pruning", "original_pdf": "/attachment/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "_bibtex": "@misc{\nmun'im2020maskconvnet,\ntitle={MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning},\nauthor={Raden Mu'az Mun'im and Jie Lin and Vijay Chandrasekhar and Koichi Shinoda},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gyl6Vtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1gyl6Vtvr", "replyto": "S1gyl6Vtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper323/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper323/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575319630094, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper323/Reviewers"], "noninvitees": [], "tcdate": 1570237753797, "tmdate": 1575319630107, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper323/-/Official_Review"}}}, {"id": "S1eo1JwJ9S", "original": null, "number": 3, "cdate": 1571938019404, "ddate": null, "tcdate": 1571938019404, "tmdate": 1572972609755, "tddate": null, "forum": "S1gyl6Vtvr", "replyto": "S1gyl6Vtvr", "invitation": "ICLR.cc/2020/Conference/Paper323/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a masking process to improve the pruning of DNN. In addition, the algorithm proposes to automatically allocate the pruning rates over layers. \n\nOn the positive side:\n- I do believe the main contribution is automatically allocating the pruning rates over the network. \n\n\n\n\n\nRelated works:\n- How does this relate to methods using gating to prune in the presence of residual layers or BN?\n\n- Paper claims multiple times that related works need a train-prune-retrain process which is only valid for post-processing works. \n\n- I missed sparsity promoting works related to training from scratch where similar masks are implicitly used during training (even not explicitly stated) to maintain the zeros in the network. At least in [1,2] the training time is the same as the time used for training a network from scratch (same claim as in this paper). \n\n\nMethod:\n- The mask is trained using a sigmoid function and claims this will be representative of the relevance of the 'neuron' within the layer. How is this really related?\n- Why the initialization of the mask is to the mean? I think I am confused there. For initialization, I would argue all the neurons/parameters are relevant, right?\n\n- The claim that this method is 'much simpler' is a bit subjective. I do not see why. Please elaborate. \n- I am not sure if the claim of pruning filters and not considering the bn layer is correct. I would tend to think that, if pruning a neuron, the corresponding BN module should be modified (that is, propagating the zero to subsequent layers). \n\n- I do like the extension to residual connections. It would be great to have more and clearer details on how is this done. Would this also apply to the UNET type of architecture?\n\n- What is the intuition behind Eq 4 and how it is related to the relevance of a parameter? \n\n- The extension to multiple metrics is of interest, however, there is little detailed there. How is the automatic allocation done? This is repeated in section 3.3 but details are missing. \n\n\n- My understanding is that the regularization multiplier is affected by the learning rate, therefore their effect is lower as the training progresses. In this case, seems the opposite, right? (page 6 before 3.4). \n\n\n- The part with the sparsity budget is interesting. What guarantees are that the newly enabled neurons are actually useful? Could it be possible that the budget suggested is not the right for the task at hand and, therefore, the additional parameters are not really relevant / needed? \n\n\n\n\n\nExperiments:\n\n- There is loss with little sparsity when it comes to Imagenet (15 and 17% pruning) does not seem very promising even the training time is similar. \n\n- Seems like the experiments and comparisons to L1-pruning are not very surprising. It would be nice to have more comprehensive numbers. For imagenet, if using Resnet-50, would be easier to compare to other numbers. \n- In the imagenet comparison, L1-pruning is the version-A of the paper. What about the others or why that particular model?\n- How are the actual groups made? \n\nMinor details:\n- Please improve figure 1. It is not easy to understand. Same with Figure 3. What is seen in Figure 4. \n- I do believe the WideResNet-28-10 number of parameters for BAR is not correct. \n- Section 4.2 is a bit overselling. I do not see 'much-higher' parameter sparsity. The claims are mostly valid for VGG type of networks in this particular setting. \n\n\n\n\n[1] Learning the Number of Neurons in Deep Networks, NeurIps 2016\n[2] Compression-aware training of DNN, NeurIps 2017\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper323/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper323/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["raden.m.muaz@gmail.com", "lin-j@i2r.a-star.edu.sg", "vijay@i2r.a-star.edu.sg", "shinoda@ks.cs.titech.ac.jp"], "title": "MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning", "authors": ["Raden Mu'az Mun'im", "Jie Lin", "Vijay Chandrasekhar", "Koichi Shinoda"], "pdf": "/pdf/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "abstract": "In this paper, we propose a framework, called MaskConvNet, for ConvNets filter pruning. MaskConvNet provides elegant support for training budget-aware pruned networks from scratch, by adding a simple mask module to a ConvNet architecture. MaskConvNet enjoys several advantages - (1) Flexible, the mask module can be integrated with any ConvNets in a plug-and-play manner. (2) Simple, the mask module is implemented by a hard Sigmoid function with a small number of trainable mask variables, adding negligible memory and computational overheads to the networks during training. (3) Effective, it is able to achieve competitive pruning rate while maintaining comparable accuracy with the baseline ConvNets without pruning, regardless of the datasets and ConvNet architectures used. (4) Fast, it is observed that the number of training epochs required by MaskConvNet is close to training a baseline without pruning. (5) Budget-aware, with a sparsity budget on target metric (e.g. model size and FLOP), MaskConvNet is able to train in a way that the optimizer can adaptively sparsify the network and automatically maintain sparsity level, till the pruned network produces good accuracy and fulfill the budget constraint simultaneously. Results on CIFAR-10 and ImageNet with several ConvNet architectures show that MaskConvNet works competitively well compared to previous pruning methods, with budget-constraint well respected. Code is available at https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0. We hope MaskConvNet, as a simple and general pruning framework, can address the gaps in existing literate and advance future studies to push the boundaries of neural network pruning.", "code": "https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0", "keywords": ["Structured Pruning", "Sparsity Regularization", "Budget-Aware"], "paperhash": "munim|maskconvnet_training_efficient_convnets_from_scratch_via_budgetconstrained_filter_pruning", "original_pdf": "/attachment/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "_bibtex": "@misc{\nmun'im2020maskconvnet,\ntitle={MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning},\nauthor={Raden Mu'az Mun'im and Jie Lin and Vijay Chandrasekhar and Koichi Shinoda},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gyl6Vtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1gyl6Vtvr", "replyto": "S1gyl6Vtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper323/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper323/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575319630094, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper323/Reviewers"], "noninvitees": [], "tcdate": 1570237753797, "tmdate": 1575319630107, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper323/-/Official_Review"}}}, {"id": "B1gnLfV4qB", "original": null, "number": 1, "cdate": 1572254292244, "ddate": null, "tcdate": 1572254292244, "tmdate": 1572254292244, "tddate": null, "forum": "S1gyl6Vtvr", "replyto": "S1gyl6Vtvr", "invitation": "ICLR.cc/2020/Conference/Paper323/-/Public_Comment", "content": {"title": "Formatting issue about the citations ", "comment": "Hi, some citations are lacking brackets (\\cite instead of \\citep).\n"}, "signatures": ["~Anthony_Wittmer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Anthony_Wittmer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["raden.m.muaz@gmail.com", "lin-j@i2r.a-star.edu.sg", "vijay@i2r.a-star.edu.sg", "shinoda@ks.cs.titech.ac.jp"], "title": "MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning", "authors": ["Raden Mu'az Mun'im", "Jie Lin", "Vijay Chandrasekhar", "Koichi Shinoda"], "pdf": "/pdf/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "abstract": "In this paper, we propose a framework, called MaskConvNet, for ConvNets filter pruning. MaskConvNet provides elegant support for training budget-aware pruned networks from scratch, by adding a simple mask module to a ConvNet architecture. MaskConvNet enjoys several advantages - (1) Flexible, the mask module can be integrated with any ConvNets in a plug-and-play manner. (2) Simple, the mask module is implemented by a hard Sigmoid function with a small number of trainable mask variables, adding negligible memory and computational overheads to the networks during training. (3) Effective, it is able to achieve competitive pruning rate while maintaining comparable accuracy with the baseline ConvNets without pruning, regardless of the datasets and ConvNet architectures used. (4) Fast, it is observed that the number of training epochs required by MaskConvNet is close to training a baseline without pruning. (5) Budget-aware, with a sparsity budget on target metric (e.g. model size and FLOP), MaskConvNet is able to train in a way that the optimizer can adaptively sparsify the network and automatically maintain sparsity level, till the pruned network produces good accuracy and fulfill the budget constraint simultaneously. Results on CIFAR-10 and ImageNet with several ConvNet architectures show that MaskConvNet works competitively well compared to previous pruning methods, with budget-constraint well respected. Code is available at https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0. We hope MaskConvNet, as a simple and general pruning framework, can address the gaps in existing literate and advance future studies to push the boundaries of neural network pruning.", "code": "https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0", "keywords": ["Structured Pruning", "Sparsity Regularization", "Budget-Aware"], "paperhash": "munim|maskconvnet_training_efficient_convnets_from_scratch_via_budgetconstrained_filter_pruning", "original_pdf": "/attachment/1b7d5c6c79d48382b56b021f20e112567dd9a86f.pdf", "_bibtex": "@misc{\nmun'im2020maskconvnet,\ntitle={MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning},\nauthor={Raden Mu'az Mun'im and Jie Lin and Vijay Chandrasekhar and Koichi Shinoda},\nyear={2020},\nurl={https://openreview.net/forum?id=S1gyl6Vtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1gyl6Vtvr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504210720, "tmdate": 1576860572746, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper323/Authors", "ICLR.cc/2020/Conference/Paper323/Reviewers", "ICLR.cc/2020/Conference/Paper323/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper323/-/Public_Comment"}}}], "count": 14}