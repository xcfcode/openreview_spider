{"notes": [{"tddate": null, "ddate": null, "tmdate": 1521306511343, "tcdate": 1521306511343, "number": 10, "cdate": 1521306511343, "id": "rJD5o6qtz", "invitation": "ICLR.cc/2018/Conference/-/Paper279/Official_Comment", "forum": "B1Gi6LeRZ", "replyto": "rkjVL6OYf", "signatures": ["ICLR.cc/2018/Conference/Paper279/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper279/Authors"], "content": {"title": "Re: Similar concept with \"mixup: Beyond Empirical Risk Minimization\"", "comment": "Thank you for pointing it out. Our BC learning is actually similar to mixup, but our work is different from mixup in two points:\n- We intuitively described why BC learning works well from a viewpoint of feature distributions.\n- We carefully designed how to mix two training examples. We showed that our method is better than the simplest method of r x1 + (1-r) x2.\nUnfortunately, we cannot mention the relationship with mixup in the conference paper because the camera-ready submission has already been closed."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Between-class Examples for Deep Sound Recognition", "abstract": "Deep learning methods have achieved high performance in sound recognition tasks. Deciding how to feed the training data is important for further performance improvement. We propose a novel learning method for deep sound recognition: Between-Class learning (BC learning). Our strategy is to learn a discriminative feature space by recognizing the between-class sounds as between-class sounds. We generate between-class sounds by mixing two sounds belonging to different classes with a random ratio. We then input the mixed sound to the model and train the model to output the mixing ratio. The advantages of BC learning are not limited only to the increase in variation of the training data; BC learning leads to an enlargement of Fisher\u2019s criterion in the feature space and a regularization of the positional relationship among the feature distributions of the classes. The experimental results show that BC learning improves the performance on various sound recognition networks, datasets, and data augmentation schemes, in which BC learning proves to be always beneficial. Furthermore, we construct a new deep sound recognition network (EnvNet-v2) and train it with BC learning. As a result, we achieved a performance surpasses the human level.", "pdf": "/pdf/635103060ef1a68ee4cf6f8139561a4cec5a360f.pdf", "TL;DR": "We propose an novel learning method for deep sound recognition named BC learning.", "paperhash": "tokozume|learning_from_betweenclass_examples_for_deep_sound_recognition", "_bibtex": "@inproceedings{\ntokozume2018learning,\ntitle={Learning from Between-class Examples for Deep Sound Recognition},\nauthor={Yuji Tokozume and Yoshitaka Ushiku and Tatsuya Harada},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=B1Gi6LeRZ},\n}", "keywords": ["sound recognition", "supervised learning", "feature learning"], "authors": ["Yuji Tokozume", "Yoshitaka Ushiku", "Tatsuya Harada"], "authorids": ["tokozume@mi.t.u-tokyo.ac.jp", "ushiku@mi.t.u-tokyo.ac.jp", "harada@mi.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825736357, "id": "ICLR.cc/2018/Conference/-/Paper279/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "B1Gi6LeRZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper279/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper279/Authors|ICLR.cc/2018/Conference/Paper279/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper279/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper279/Authors|ICLR.cc/2018/Conference/Paper279/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper279/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper279/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper279/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper279/Reviewers", "ICLR.cc/2018/Conference/Paper279/Authors", "ICLR.cc/2018/Conference/Paper279/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825736357}}}, {"tddate": null, "ddate": null, "tmdate": 1521174066948, "tcdate": 1521174066948, "number": 3, "cdate": 1521174066948, "id": "rkjVL6OYf", "invitation": "ICLR.cc/2018/Conference/-/Paper279/Public_Comment", "forum": "B1Gi6LeRZ", "replyto": "B1Gi6LeRZ", "signatures": ["~Daisuke_Niizumi1"], "readers": ["everyone"], "writers": ["~Daisuke_Niizumi1"], "content": {"title": "Similar concept with \"mixup: Beyond Empirical Risk Minimization\"", "comment": "Hi, for me your work seems to be similar to this paper:\nHongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz, mixup: Beyond Empirical Risk Minimization, 2017, https://arxiv.org/abs/1710.09412\nThis mixes two examples regardless of class from training set.\nI guess it would be better if you could also mention regarding relationship with this.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Between-class Examples for Deep Sound Recognition", "abstract": "Deep learning methods have achieved high performance in sound recognition tasks. Deciding how to feed the training data is important for further performance improvement. We propose a novel learning method for deep sound recognition: Between-Class learning (BC learning). Our strategy is to learn a discriminative feature space by recognizing the between-class sounds as between-class sounds. We generate between-class sounds by mixing two sounds belonging to different classes with a random ratio. We then input the mixed sound to the model and train the model to output the mixing ratio. The advantages of BC learning are not limited only to the increase in variation of the training data; BC learning leads to an enlargement of Fisher\u2019s criterion in the feature space and a regularization of the positional relationship among the feature distributions of the classes. The experimental results show that BC learning improves the performance on various sound recognition networks, datasets, and data augmentation schemes, in which BC learning proves to be always beneficial. Furthermore, we construct a new deep sound recognition network (EnvNet-v2) and train it with BC learning. As a result, we achieved a performance surpasses the human level.", "pdf": "/pdf/635103060ef1a68ee4cf6f8139561a4cec5a360f.pdf", "TL;DR": "We propose an novel learning method for deep sound recognition named BC learning.", "paperhash": "tokozume|learning_from_betweenclass_examples_for_deep_sound_recognition", "_bibtex": "@inproceedings{\ntokozume2018learning,\ntitle={Learning from Between-class Examples for Deep Sound Recognition},\nauthor={Yuji Tokozume and Yoshitaka Ushiku and Tatsuya Harada},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=B1Gi6LeRZ},\n}", "keywords": ["sound recognition", "supervised learning", "feature learning"], "authors": ["Yuji Tokozume", "Yoshitaka Ushiku", "Tatsuya Harada"], "authorids": ["tokozume@mi.t.u-tokyo.ac.jp", "ushiku@mi.t.u-tokyo.ac.jp", "harada@mi.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791689570, "id": "ICLR.cc/2018/Conference/-/Paper279/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "B1Gi6LeRZ", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper279/Authors", "ICLR.cc/2018/Conference/Paper279/Reviewers", "ICLR.cc/2018/Conference/Paper279/Area_Chair"], "cdate": 1512791689570}}}, {"tddate": null, "ddate": null, "tmdate": 1519364463997, "tcdate": 1509088666388, "number": 279, "cdate": 1518730183125, "id": "B1Gi6LeRZ", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "B1Gi6LeRZ", "original": "BJGoaLx0W", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Learning from Between-class Examples for Deep Sound Recognition", "abstract": "Deep learning methods have achieved high performance in sound recognition tasks. Deciding how to feed the training data is important for further performance improvement. We propose a novel learning method for deep sound recognition: Between-Class learning (BC learning). Our strategy is to learn a discriminative feature space by recognizing the between-class sounds as between-class sounds. We generate between-class sounds by mixing two sounds belonging to different classes with a random ratio. We then input the mixed sound to the model and train the model to output the mixing ratio. The advantages of BC learning are not limited only to the increase in variation of the training data; BC learning leads to an enlargement of Fisher\u2019s criterion in the feature space and a regularization of the positional relationship among the feature distributions of the classes. The experimental results show that BC learning improves the performance on various sound recognition networks, datasets, and data augmentation schemes, in which BC learning proves to be always beneficial. Furthermore, we construct a new deep sound recognition network (EnvNet-v2) and train it with BC learning. As a result, we achieved a performance surpasses the human level.", "pdf": "/pdf/635103060ef1a68ee4cf6f8139561a4cec5a360f.pdf", "TL;DR": "We propose an novel learning method for deep sound recognition named BC learning.", "paperhash": "tokozume|learning_from_betweenclass_examples_for_deep_sound_recognition", "_bibtex": "@inproceedings{\ntokozume2018learning,\ntitle={Learning from Between-class Examples for Deep Sound Recognition},\nauthor={Yuji Tokozume and Yoshitaka Ushiku and Tatsuya Harada},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=B1Gi6LeRZ},\n}", "keywords": ["sound recognition", "supervised learning", "feature learning"], "authors": ["Yuji Tokozume", "Yoshitaka Ushiku", "Tatsuya Harada"], "authorids": ["tokozume@mi.t.u-tokyo.ac.jp", "ushiku@mi.t.u-tokyo.ac.jp", "harada@mi.t.u-tokyo.ac.jp"]}, "nonreaders": [], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260098874, "tcdate": 1517249304306, "number": 98, "cdate": 1517249304291, "id": "H1xXX1pHM", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "B1Gi6LeRZ", "replyto": "B1Gi6LeRZ", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "meta score: 8\n\nThis is a good paper which augments the data by mixing sound classes, and then learns the  mixing ratio.  Experiments performed on a number of sound classification results\n\nPros\n - novel approach, clearly explained\n - very good set of experimentation with excellent results\n - good approach to mixing using perceptual criteria\n\nCons\n - discussion doesn't really generalise beyond sound recognition\n\n", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Between-class Examples for Deep Sound Recognition", "abstract": "Deep learning methods have achieved high performance in sound recognition tasks. Deciding how to feed the training data is important for further performance improvement. We propose a novel learning method for deep sound recognition: Between-Class learning (BC learning). Our strategy is to learn a discriminative feature space by recognizing the between-class sounds as between-class sounds. We generate between-class sounds by mixing two sounds belonging to different classes with a random ratio. We then input the mixed sound to the model and train the model to output the mixing ratio. The advantages of BC learning are not limited only to the increase in variation of the training data; BC learning leads to an enlargement of Fisher\u2019s criterion in the feature space and a regularization of the positional relationship among the feature distributions of the classes. The experimental results show that BC learning improves the performance on various sound recognition networks, datasets, and data augmentation schemes, in which BC learning proves to be always beneficial. Furthermore, we construct a new deep sound recognition network (EnvNet-v2) and train it with BC learning. As a result, we achieved a performance surpasses the human level.", "pdf": "/pdf/635103060ef1a68ee4cf6f8139561a4cec5a360f.pdf", "TL;DR": "We propose an novel learning method for deep sound recognition named BC learning.", "paperhash": "tokozume|learning_from_betweenclass_examples_for_deep_sound_recognition", "_bibtex": "@inproceedings{\ntokozume2018learning,\ntitle={Learning from Between-class Examples for Deep Sound Recognition},\nauthor={Yuji Tokozume and Yoshitaka Ushiku and Tatsuya Harada},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=B1Gi6LeRZ},\n}", "keywords": ["sound recognition", "supervised learning", "feature learning"], "authors": ["Yuji Tokozume", "Yoshitaka Ushiku", "Tatsuya Harada"], "authorids": ["tokozume@mi.t.u-tokyo.ac.jp", "ushiku@mi.t.u-tokyo.ac.jp", "harada@mi.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "tmdate": 1516646016886, "tcdate": 1516646016886, "number": 9, "cdate": 1516646016886, "id": "ryKFRjXBf", "invitation": "ICLR.cc/2018/Conference/-/Paper279/Official_Comment", "forum": "B1Gi6LeRZ", "replyto": "B1Gi6LeRZ", "signatures": ["ICLR.cc/2018/Conference/Paper279/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper279/Authors"], "content": {"title": "Revised paper uploaded", "comment": "We have revised the paper, considering the comments from AnonReviewer3.\nMajor changes:\n- The last paragraph of Section 1: modified the description about the novelty of our paper.\n- Section 3.3.2: added the description about the dimension of the feature space."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Between-class Examples for Deep Sound Recognition", "abstract": "Deep learning methods have achieved high performance in sound recognition tasks. Deciding how to feed the training data is important for further performance improvement. We propose a novel learning method for deep sound recognition: Between-Class learning (BC learning). Our strategy is to learn a discriminative feature space by recognizing the between-class sounds as between-class sounds. We generate between-class sounds by mixing two sounds belonging to different classes with a random ratio. We then input the mixed sound to the model and train the model to output the mixing ratio. The advantages of BC learning are not limited only to the increase in variation of the training data; BC learning leads to an enlargement of Fisher\u2019s criterion in the feature space and a regularization of the positional relationship among the feature distributions of the classes. The experimental results show that BC learning improves the performance on various sound recognition networks, datasets, and data augmentation schemes, in which BC learning proves to be always beneficial. Furthermore, we construct a new deep sound recognition network (EnvNet-v2) and train it with BC learning. As a result, we achieved a performance surpasses the human level.", "pdf": "/pdf/635103060ef1a68ee4cf6f8139561a4cec5a360f.pdf", "TL;DR": "We propose an novel learning method for deep sound recognition named BC learning.", "paperhash": "tokozume|learning_from_betweenclass_examples_for_deep_sound_recognition", "_bibtex": "@inproceedings{\ntokozume2018learning,\ntitle={Learning from Between-class Examples for Deep Sound Recognition},\nauthor={Yuji Tokozume and Yoshitaka Ushiku and Tatsuya Harada},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=B1Gi6LeRZ},\n}", "keywords": ["sound recognition", "supervised learning", "feature learning"], "authors": ["Yuji Tokozume", "Yoshitaka Ushiku", "Tatsuya Harada"], "authorids": ["tokozume@mi.t.u-tokyo.ac.jp", "ushiku@mi.t.u-tokyo.ac.jp", "harada@mi.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825736357, "id": "ICLR.cc/2018/Conference/-/Paper279/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "B1Gi6LeRZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper279/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper279/Authors|ICLR.cc/2018/Conference/Paper279/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper279/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper279/Authors|ICLR.cc/2018/Conference/Paper279/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper279/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper279/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper279/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper279/Reviewers", "ICLR.cc/2018/Conference/Paper279/Authors", "ICLR.cc/2018/Conference/Paper279/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825736357}}}, {"tddate": null, "ddate": null, "tmdate": 1516559236460, "tcdate": 1516559236460, "number": 8, "cdate": 1516559236460, "id": "BJ3tjUzSG", "invitation": "ICLR.cc/2018/Conference/-/Paper279/Official_Comment", "forum": "B1Gi6LeRZ", "replyto": "ByW22ClHf", "signatures": ["ICLR.cc/2018/Conference/Paper279/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper279/Authors"], "content": {"title": "Re: iclr 2018 reproducibility challenge", "comment": "Thanks for your questions.\n1. Each fold has 400 samples. We did not have test samples, and testing was done only on the validation fold. The model was trained with 4 folds (1600 samples) and tested with 1 fold (400 samples). We used the original fold settings defined by the proposer of ESC-50 (not random division). \n2. Epochs in Figure 5 represent single iteration over the 1600 training samples.\n3. We performed 5-fold cross-validation 5 times. Thus, the errors in Table 1 and Figure 5 represent the average of (5x5=) 25 errors.\n4. We did 10-crop testing every epoch."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Between-class Examples for Deep Sound Recognition", "abstract": "Deep learning methods have achieved high performance in sound recognition tasks. Deciding how to feed the training data is important for further performance improvement. We propose a novel learning method for deep sound recognition: Between-Class learning (BC learning). Our strategy is to learn a discriminative feature space by recognizing the between-class sounds as between-class sounds. We generate between-class sounds by mixing two sounds belonging to different classes with a random ratio. We then input the mixed sound to the model and train the model to output the mixing ratio. The advantages of BC learning are not limited only to the increase in variation of the training data; BC learning leads to an enlargement of Fisher\u2019s criterion in the feature space and a regularization of the positional relationship among the feature distributions of the classes. The experimental results show that BC learning improves the performance on various sound recognition networks, datasets, and data augmentation schemes, in which BC learning proves to be always beneficial. Furthermore, we construct a new deep sound recognition network (EnvNet-v2) and train it with BC learning. As a result, we achieved a performance surpasses the human level.", "pdf": "/pdf/635103060ef1a68ee4cf6f8139561a4cec5a360f.pdf", "TL;DR": "We propose an novel learning method for deep sound recognition named BC learning.", "paperhash": "tokozume|learning_from_betweenclass_examples_for_deep_sound_recognition", "_bibtex": "@inproceedings{\ntokozume2018learning,\ntitle={Learning from Between-class Examples for Deep Sound Recognition},\nauthor={Yuji Tokozume and Yoshitaka Ushiku and Tatsuya Harada},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=B1Gi6LeRZ},\n}", "keywords": ["sound recognition", "supervised learning", "feature learning"], "authors": ["Yuji Tokozume", "Yoshitaka Ushiku", "Tatsuya Harada"], "authorids": ["tokozume@mi.t.u-tokyo.ac.jp", "ushiku@mi.t.u-tokyo.ac.jp", "harada@mi.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825736357, "id": "ICLR.cc/2018/Conference/-/Paper279/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "B1Gi6LeRZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper279/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper279/Authors|ICLR.cc/2018/Conference/Paper279/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper279/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper279/Authors|ICLR.cc/2018/Conference/Paper279/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper279/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper279/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper279/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper279/Reviewers", "ICLR.cc/2018/Conference/Paper279/Authors", "ICLR.cc/2018/Conference/Paper279/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825736357}}}, {"tddate": null, "ddate": null, "tmdate": 1516461225106, "tcdate": 1516461225106, "number": 1, "cdate": 1516461225106, "id": "ByW22ClHf", "invitation": "ICLR.cc/2018/Conference/-/Paper279/Public_Comment", "forum": "B1Gi6LeRZ", "replyto": "B1Gi6LeRZ", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "iclr 2018 reproducibility challenge", "comment": "Hello,\nI am trying to reproduce your results, as part of the reproducibility challenge.\nThe database that I am using is ESC-50, with DSRNet.\nI have several questions regarding the training/results of the baseline network (no BC learning):\n1. Did you split the data to 5 cross validation sections, each one with 400 samples (2000/5)? Did you have test samples, besides the validation folds? or the testing was done only on the validation folds?\n2. Did the epochs in Figure 5 represent single iteration over the data base, or 5 iterations, because of the cross validation?\n3. How did you calculate the Error rate in table 1 and figure 5? Is it the error of the current fold? average between the errors of the 5 folds?\n4. When you write that during testing, you cropped 10 T-s sections, Did you do that on the validation fold every epoch, or on a different testing data?\nThanks\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Between-class Examples for Deep Sound Recognition", "abstract": "Deep learning methods have achieved high performance in sound recognition tasks. Deciding how to feed the training data is important for further performance improvement. We propose a novel learning method for deep sound recognition: Between-Class learning (BC learning). Our strategy is to learn a discriminative feature space by recognizing the between-class sounds as between-class sounds. We generate between-class sounds by mixing two sounds belonging to different classes with a random ratio. We then input the mixed sound to the model and train the model to output the mixing ratio. The advantages of BC learning are not limited only to the increase in variation of the training data; BC learning leads to an enlargement of Fisher\u2019s criterion in the feature space and a regularization of the positional relationship among the feature distributions of the classes. The experimental results show that BC learning improves the performance on various sound recognition networks, datasets, and data augmentation schemes, in which BC learning proves to be always beneficial. Furthermore, we construct a new deep sound recognition network (EnvNet-v2) and train it with BC learning. As a result, we achieved a performance surpasses the human level.", "pdf": "/pdf/635103060ef1a68ee4cf6f8139561a4cec5a360f.pdf", "TL;DR": "We propose an novel learning method for deep sound recognition named BC learning.", "paperhash": "tokozume|learning_from_betweenclass_examples_for_deep_sound_recognition", "_bibtex": "@inproceedings{\ntokozume2018learning,\ntitle={Learning from Between-class Examples for Deep Sound Recognition},\nauthor={Yuji Tokozume and Yoshitaka Ushiku and Tatsuya Harada},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=B1Gi6LeRZ},\n}", "keywords": ["sound recognition", "supervised learning", "feature learning"], "authors": ["Yuji Tokozume", "Yoshitaka Ushiku", "Tatsuya Harada"], "authorids": ["tokozume@mi.t.u-tokyo.ac.jp", "ushiku@mi.t.u-tokyo.ac.jp", "harada@mi.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791689570, "id": "ICLR.cc/2018/Conference/-/Paper279/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "B1Gi6LeRZ", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper279/Authors", "ICLR.cc/2018/Conference/Paper279/Reviewers", "ICLR.cc/2018/Conference/Paper279/Area_Chair"], "cdate": 1512791689570}}}, {"tddate": null, "ddate": null, "tmdate": 1516284043303, "tcdate": 1516284043303, "number": 7, "cdate": 1516284043303, "id": "BJ79_7RNM", "invitation": "ICLR.cc/2018/Conference/-/Paper279/Official_Comment", "forum": "B1Gi6LeRZ", "replyto": "S103m19Nf", "signatures": ["ICLR.cc/2018/Conference/Paper279/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper279/Authors"], "content": {"title": "Response", "comment": "Thanks for your positive comments and advice. We will modify the two points you have suggested (will be uploaded in a few days)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Between-class Examples for Deep Sound Recognition", "abstract": "Deep learning methods have achieved high performance in sound recognition tasks. Deciding how to feed the training data is important for further performance improvement. We propose a novel learning method for deep sound recognition: Between-Class learning (BC learning). Our strategy is to learn a discriminative feature space by recognizing the between-class sounds as between-class sounds. We generate between-class sounds by mixing two sounds belonging to different classes with a random ratio. We then input the mixed sound to the model and train the model to output the mixing ratio. The advantages of BC learning are not limited only to the increase in variation of the training data; BC learning leads to an enlargement of Fisher\u2019s criterion in the feature space and a regularization of the positional relationship among the feature distributions of the classes. The experimental results show that BC learning improves the performance on various sound recognition networks, datasets, and data augmentation schemes, in which BC learning proves to be always beneficial. Furthermore, we construct a new deep sound recognition network (EnvNet-v2) and train it with BC learning. As a result, we achieved a performance surpasses the human level.", "pdf": "/pdf/635103060ef1a68ee4cf6f8139561a4cec5a360f.pdf", "TL;DR": "We propose an novel learning method for deep sound recognition named BC learning.", "paperhash": "tokozume|learning_from_betweenclass_examples_for_deep_sound_recognition", "_bibtex": "@inproceedings{\ntokozume2018learning,\ntitle={Learning from Between-class Examples for Deep Sound Recognition},\nauthor={Yuji Tokozume and Yoshitaka Ushiku and Tatsuya Harada},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=B1Gi6LeRZ},\n}", "keywords": ["sound recognition", "supervised learning", "feature learning"], "authors": ["Yuji Tokozume", "Yoshitaka Ushiku", "Tatsuya Harada"], "authorids": ["tokozume@mi.t.u-tokyo.ac.jp", "ushiku@mi.t.u-tokyo.ac.jp", "harada@mi.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825736357, "id": "ICLR.cc/2018/Conference/-/Paper279/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "B1Gi6LeRZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper279/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper279/Authors|ICLR.cc/2018/Conference/Paper279/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper279/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper279/Authors|ICLR.cc/2018/Conference/Paper279/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper279/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper279/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper279/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper279/Reviewers", "ICLR.cc/2018/Conference/Paper279/Authors", "ICLR.cc/2018/Conference/Paper279/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825736357}}}, {"tddate": null, "ddate": null, "tmdate": 1516004278221, "tcdate": 1516004278221, "number": 6, "cdate": 1516004278221, "id": "S103m19Nf", "invitation": "ICLR.cc/2018/Conference/-/Paper279/Official_Comment", "forum": "B1Gi6LeRZ", "replyto": "SJp7SQEGz", "signatures": ["ICLR.cc/2018/Conference/Paper279/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper279/AnonReviewer3"], "content": {"title": "More clear now", "comment": "After reading all the reviews and the authors' responses I have a more clear view of the paper now. I changed my mind, I see now the interest of having this paper accepted.\n\nHaving said that, I strongly encourage the authors to modify the text so that two key aspects of the method are clearly explained.\nFirst, as you said in your comments: The novelty or key point of our method is not mixing multiple sounds, but rather learning method of training the model to output the mixing ratio.\nSecond, the limitation (which now is not a problem, but could be for some other applications/architectures): the dimension of the feature space d is generally designed to be larger than the number of classes c.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Between-class Examples for Deep Sound Recognition", "abstract": "Deep learning methods have achieved high performance in sound recognition tasks. Deciding how to feed the training data is important for further performance improvement. We propose a novel learning method for deep sound recognition: Between-Class learning (BC learning). Our strategy is to learn a discriminative feature space by recognizing the between-class sounds as between-class sounds. We generate between-class sounds by mixing two sounds belonging to different classes with a random ratio. We then input the mixed sound to the model and train the model to output the mixing ratio. The advantages of BC learning are not limited only to the increase in variation of the training data; BC learning leads to an enlargement of Fisher\u2019s criterion in the feature space and a regularization of the positional relationship among the feature distributions of the classes. The experimental results show that BC learning improves the performance on various sound recognition networks, datasets, and data augmentation schemes, in which BC learning proves to be always beneficial. Furthermore, we construct a new deep sound recognition network (EnvNet-v2) and train it with BC learning. As a result, we achieved a performance surpasses the human level.", "pdf": "/pdf/635103060ef1a68ee4cf6f8139561a4cec5a360f.pdf", "TL;DR": "We propose an novel learning method for deep sound recognition named BC learning.", "paperhash": "tokozume|learning_from_betweenclass_examples_for_deep_sound_recognition", "_bibtex": "@inproceedings{\ntokozume2018learning,\ntitle={Learning from Between-class Examples for Deep Sound Recognition},\nauthor={Yuji Tokozume and Yoshitaka Ushiku and Tatsuya Harada},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=B1Gi6LeRZ},\n}", "keywords": ["sound recognition", "supervised learning", "feature learning"], "authors": ["Yuji Tokozume", "Yoshitaka Ushiku", "Tatsuya Harada"], "authorids": ["tokozume@mi.t.u-tokyo.ac.jp", "ushiku@mi.t.u-tokyo.ac.jp", "harada@mi.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825736357, "id": "ICLR.cc/2018/Conference/-/Paper279/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "B1Gi6LeRZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper279/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper279/Authors|ICLR.cc/2018/Conference/Paper279/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper279/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper279/Authors|ICLR.cc/2018/Conference/Paper279/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper279/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper279/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper279/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper279/Reviewers", "ICLR.cc/2018/Conference/Paper279/Authors", "ICLR.cc/2018/Conference/Paper279/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825736357}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642424243, "tcdate": 1511576699170, "number": 1, "cdate": 1511576699170, "id": "HJmtVULeG", "invitation": "ICLR.cc/2018/Conference/-/Paper279/Official_Review", "forum": "B1Gi6LeRZ", "replyto": "B1Gi6LeRZ", "signatures": ["ICLR.cc/2018/Conference/Paper279/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Novel Approach to Using Limited Training Data By Changing Task for Better Class Discrimination", "rating": "9: Top 15% of accepted papers, strong accept", "review": "Overall: Authors defined a new learning task that requires a DNN to predict mixing ratio between sounds from two different classes. Previous approaches to training data mixing are (1) from random classes, or (2) from the same class. The presented approach mixes sounds from specific pairs of classes to increase discriminative power of the final learned network. Results look like significant improvements over standard learning setups.\n\nDetailed Evaluation: The approach presented is simple, clearly presented, and looks effective on benchmarks. In terms of originality, it is different from warping training example for the same task and it is a good extension of previously suggested example mixing procedures with a targeted benefit for improved discriminative power. The authors have also provided extensive analysis from the point of views (1) network architecture, (2) mixing method, (3) number of labels / classes in mix, (4) mixing layers -- really well done due-diligence across different model and task parameters.\n\nMinor Asks:\n(1) Clarification on how the error rates are defined. Especially since the standard learning task could be 0-1 loss and this new BC learning task could be based on distribution divergence (if we're not using argmax as class label).\n(2) #class_pairs targets as analysis - The number of epochs needed is naturally going to be higher since the BC-DNN has to train to predict mixing ratios between pairs of classes. Since pairs of classes could be huge if the total number of classes is large, it'll be nice to see how this scales. I.e. are we talking about a space of 10 total classes or 10000 total classes? How does num required epochs get impacted as we increase this class space?\n(3) Clarify how G_1/20 and G_2/20 is important / derived - I assume it's unit conversion from decibels.\n(4) Please explain why it is important to use the smoothed average of 10 softmax predictions in evaluation... what happens if you just randomly pick one of the 10 crops for prediction?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Between-class Examples for Deep Sound Recognition", "abstract": "Deep learning methods have achieved high performance in sound recognition tasks. Deciding how to feed the training data is important for further performance improvement. We propose a novel learning method for deep sound recognition: Between-Class learning (BC learning). Our strategy is to learn a discriminative feature space by recognizing the between-class sounds as between-class sounds. We generate between-class sounds by mixing two sounds belonging to different classes with a random ratio. We then input the mixed sound to the model and train the model to output the mixing ratio. The advantages of BC learning are not limited only to the increase in variation of the training data; BC learning leads to an enlargement of Fisher\u2019s criterion in the feature space and a regularization of the positional relationship among the feature distributions of the classes. The experimental results show that BC learning improves the performance on various sound recognition networks, datasets, and data augmentation schemes, in which BC learning proves to be always beneficial. Furthermore, we construct a new deep sound recognition network (EnvNet-v2) and train it with BC learning. As a result, we achieved a performance surpasses the human level.", "pdf": "/pdf/635103060ef1a68ee4cf6f8139561a4cec5a360f.pdf", "TL;DR": "We propose an novel learning method for deep sound recognition named BC learning.", "paperhash": "tokozume|learning_from_betweenclass_examples_for_deep_sound_recognition", "_bibtex": "@inproceedings{\ntokozume2018learning,\ntitle={Learning from Between-class Examples for Deep Sound Recognition},\nauthor={Yuji Tokozume and Yoshitaka Ushiku and Tatsuya Harada},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=B1Gi6LeRZ},\n}", "keywords": ["sound recognition", "supervised learning", "feature learning"], "authors": ["Yuji Tokozume", "Yoshitaka Ushiku", "Tatsuya Harada"], "authorids": ["tokozume@mi.t.u-tokyo.ac.jp", "ushiku@mi.t.u-tokyo.ac.jp", "harada@mi.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642424134, "id": "ICLR.cc/2018/Conference/-/Paper279/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper279/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper279/AnonReviewer1", "ICLR.cc/2018/Conference/Paper279/AnonReviewer3", "ICLR.cc/2018/Conference/Paper279/AnonReviewer2"], "reply": {"forum": "B1Gi6LeRZ", "replyto": "B1Gi6LeRZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper279/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642424134}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642424207, "tcdate": 1511803598272, "number": 2, "cdate": 1511803598272, "id": "HJ80q6KlG", "invitation": "ICLR.cc/2018/Conference/-/Paper279/Official_Review", "forum": "B1Gi6LeRZ", "replyto": "B1Gi6LeRZ", "signatures": ["ICLR.cc/2018/Conference/Paper279/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Interesting data augmentation technique, but lacks of deep insights on how and why does it work.", "rating": "4: Ok but not good enough - rejection", "review": "This manuscript proposes a method to improve the performance of a generic learning method by generating \"in between class\" (BC) training samples. The manuscript motivates the necessity of such technique and presents the basic intuition. The authors show how the so-called BC learning helps training different deep architectures for the sound recognition task.\n\nMy first remark regards the presentation of the technique. The authors argue that it is not a data augmentation technique, but rather a learning method. I strongly disagree with this statement, not only because the technique deals exactly with augmenting data, but also because it can be used in combination to any learning method (including non-deep learning methodologies). Naturally, the literature review deals with data augmentation technique, which supports my point of view.\n\nIn this regard, I would have expected comparison with other state-of-the-art data augmentation techniques. The usefulness of the BC technique is proven to a certain extent (see paragraph below) but there is not comparison with state-of-the-art. In other words, the authors do not compare the proposed method with other methods doing data augmentation. This is crucial to understand the advantages of the BC technique.\n\nThere is a more fundamental question for which I was not able to find an explicit answer in the manuscript. Intuitively, the diagram shown in Figure 4 works well for 3 classes in dimension 2. If we add another class, no matter how do we define the borders, there will be one pair of classes for which the transition from one to another will pass through the region of a third class. The situation worsens with more classes. However, this can be solved by adding one dimension, 4 classes and 3 dimensions seems something feasible. One can easily understand that if there is one more class than the number of dimensions, the assumption should be feasible, but beyond it starts to get problematic. This discussion does not appear at all in the manuscript and it would be an important limitation of the method, specially when dealing with large-scale data sets.\n\nOverall I believe the paper is not mature enough for publication.\n\nSome minor comments:\n- 2.1: We introduce --> We discussion\n- Pieczak 2015a did not propose the extraction of MFCC.\n- the x_i and t_i of section 3.2.2 should not be denoted with the same letters as in 3.2.1.\n- The correspondence with a semantic feature space is too pretentious, specially since no experiment in this direction is shown.\n- I understand that there is no mixing in the test phase, perhaps it would be useful to recall it.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Between-class Examples for Deep Sound Recognition", "abstract": "Deep learning methods have achieved high performance in sound recognition tasks. Deciding how to feed the training data is important for further performance improvement. We propose a novel learning method for deep sound recognition: Between-Class learning (BC learning). Our strategy is to learn a discriminative feature space by recognizing the between-class sounds as between-class sounds. We generate between-class sounds by mixing two sounds belonging to different classes with a random ratio. We then input the mixed sound to the model and train the model to output the mixing ratio. The advantages of BC learning are not limited only to the increase in variation of the training data; BC learning leads to an enlargement of Fisher\u2019s criterion in the feature space and a regularization of the positional relationship among the feature distributions of the classes. The experimental results show that BC learning improves the performance on various sound recognition networks, datasets, and data augmentation schemes, in which BC learning proves to be always beneficial. Furthermore, we construct a new deep sound recognition network (EnvNet-v2) and train it with BC learning. As a result, we achieved a performance surpasses the human level.", "pdf": "/pdf/635103060ef1a68ee4cf6f8139561a4cec5a360f.pdf", "TL;DR": "We propose an novel learning method for deep sound recognition named BC learning.", "paperhash": "tokozume|learning_from_betweenclass_examples_for_deep_sound_recognition", "_bibtex": "@inproceedings{\ntokozume2018learning,\ntitle={Learning from Between-class Examples for Deep Sound Recognition},\nauthor={Yuji Tokozume and Yoshitaka Ushiku and Tatsuya Harada},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=B1Gi6LeRZ},\n}", "keywords": ["sound recognition", "supervised learning", "feature learning"], "authors": ["Yuji Tokozume", "Yoshitaka Ushiku", "Tatsuya Harada"], "authorids": ["tokozume@mi.t.u-tokyo.ac.jp", "ushiku@mi.t.u-tokyo.ac.jp", "harada@mi.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642424134, "id": "ICLR.cc/2018/Conference/-/Paper279/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper279/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper279/AnonReviewer1", "ICLR.cc/2018/Conference/Paper279/AnonReviewer3", "ICLR.cc/2018/Conference/Paper279/AnonReviewer2"], "reply": {"forum": "B1Gi6LeRZ", "replyto": "B1Gi6LeRZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper279/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642424134}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642424154, "tcdate": 1511819934940, "number": 3, "cdate": 1511819934940, "id": "r1vicbqeG", "invitation": "ICLR.cc/2018/Conference/-/Paper279/Official_Review", "forum": "B1Gi6LeRZ", "replyto": "B1Gi6LeRZ", "signatures": ["ICLR.cc/2018/Conference/Paper279/AnonReviewer2"], "readers": ["everyone"], "content": {"title": " Learning from Between-class Examples increases the Fisher\u2019s criterion. BC learning regularizes the positional relationship of the classes in the feature space, by training the model not to misclassify the mixed sound as different classes. The presentation and discussion on the proposed method is good (even if the organisation of the paper could be improved). Some simplifications can be conducted (in the mixture). The main idea is good and novel, and relevant for ICLR.", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The propose data augmentation and BC learning is relevant, much robust than frequency jitter or simple data augmentation. \n\nIn equation 2, please check the measure of the mixture. Why not simply use a dB criteria ?\n\nThe comments about applying a CNN to local features or novel approach to increase sound recognition could be completed with some ICLR 2017 work towards injected priors using Chirplet Transform.\n\nThe authors might discuss more how to extend their model to image recognition, or at least of other modalities as suggested.\n\nSection 3.2.2 shall be placed later on, and clarified.\n\nDiscussion on mixing more than two sounds leads could be completed by associative properties, we think... ?\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Between-class Examples for Deep Sound Recognition", "abstract": "Deep learning methods have achieved high performance in sound recognition tasks. Deciding how to feed the training data is important for further performance improvement. We propose a novel learning method for deep sound recognition: Between-Class learning (BC learning). Our strategy is to learn a discriminative feature space by recognizing the between-class sounds as between-class sounds. We generate between-class sounds by mixing two sounds belonging to different classes with a random ratio. We then input the mixed sound to the model and train the model to output the mixing ratio. The advantages of BC learning are not limited only to the increase in variation of the training data; BC learning leads to an enlargement of Fisher\u2019s criterion in the feature space and a regularization of the positional relationship among the feature distributions of the classes. The experimental results show that BC learning improves the performance on various sound recognition networks, datasets, and data augmentation schemes, in which BC learning proves to be always beneficial. Furthermore, we construct a new deep sound recognition network (EnvNet-v2) and train it with BC learning. As a result, we achieved a performance surpasses the human level.", "pdf": "/pdf/635103060ef1a68ee4cf6f8139561a4cec5a360f.pdf", "TL;DR": "We propose an novel learning method for deep sound recognition named BC learning.", "paperhash": "tokozume|learning_from_betweenclass_examples_for_deep_sound_recognition", "_bibtex": "@inproceedings{\ntokozume2018learning,\ntitle={Learning from Between-class Examples for Deep Sound Recognition},\nauthor={Yuji Tokozume and Yoshitaka Ushiku and Tatsuya Harada},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=B1Gi6LeRZ},\n}", "keywords": ["sound recognition", "supervised learning", "feature learning"], "authors": ["Yuji Tokozume", "Yoshitaka Ushiku", "Tatsuya Harada"], "authorids": ["tokozume@mi.t.u-tokyo.ac.jp", "ushiku@mi.t.u-tokyo.ac.jp", "harada@mi.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642424134, "id": "ICLR.cc/2018/Conference/-/Paper279/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper279/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper279/AnonReviewer1", "ICLR.cc/2018/Conference/Paper279/AnonReviewer3", "ICLR.cc/2018/Conference/Paper279/AnonReviewer2"], "reply": {"forum": "B1Gi6LeRZ", "replyto": "B1Gi6LeRZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper279/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642424134}}}, {"tddate": null, "ddate": null, "tmdate": 1515188911651, "tcdate": 1515188911651, "number": 4, "cdate": 1515188911651, "id": "S1O3zOp7f", "invitation": "ICLR.cc/2018/Conference/-/Paper279/Official_Comment", "forum": "B1Gi6LeRZ", "replyto": "B1Gi6LeRZ", "signatures": ["ICLR.cc/2018/Conference/Paper279/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper279/Authors"], "content": {"title": "Revised paper uploaded", "comment": "We have uploaded a revised version of the paper. \n\nMajor changes:\n- Section 2.1: modified the description of Piczak's work.\n- Section 3.1: noted that there is no mixing in testing phase.\n- Section 3.2.1: clarified the deviation and meaning of equation 2.\n- Section 3.2.2: modified the indices of x and t.\n- Section 4.1.4: added experiments and discussion on # of training epochs vs. # of classes.\n- Total: 10 pages -> 9 pages (for main part)\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Between-class Examples for Deep Sound Recognition", "abstract": "Deep learning methods have achieved high performance in sound recognition tasks. Deciding how to feed the training data is important for further performance improvement. We propose a novel learning method for deep sound recognition: Between-Class learning (BC learning). Our strategy is to learn a discriminative feature space by recognizing the between-class sounds as between-class sounds. We generate between-class sounds by mixing two sounds belonging to different classes with a random ratio. We then input the mixed sound to the model and train the model to output the mixing ratio. The advantages of BC learning are not limited only to the increase in variation of the training data; BC learning leads to an enlargement of Fisher\u2019s criterion in the feature space and a regularization of the positional relationship among the feature distributions of the classes. The experimental results show that BC learning improves the performance on various sound recognition networks, datasets, and data augmentation schemes, in which BC learning proves to be always beneficial. Furthermore, we construct a new deep sound recognition network (EnvNet-v2) and train it with BC learning. As a result, we achieved a performance surpasses the human level.", "pdf": "/pdf/635103060ef1a68ee4cf6f8139561a4cec5a360f.pdf", "TL;DR": "We propose an novel learning method for deep sound recognition named BC learning.", "paperhash": "tokozume|learning_from_betweenclass_examples_for_deep_sound_recognition", "_bibtex": "@inproceedings{\ntokozume2018learning,\ntitle={Learning from Between-class Examples for Deep Sound Recognition},\nauthor={Yuji Tokozume and Yoshitaka Ushiku and Tatsuya Harada},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=B1Gi6LeRZ},\n}", "keywords": ["sound recognition", "supervised learning", "feature learning"], "authors": ["Yuji Tokozume", "Yoshitaka Ushiku", "Tatsuya Harada"], "authorids": ["tokozume@mi.t.u-tokyo.ac.jp", "ushiku@mi.t.u-tokyo.ac.jp", "harada@mi.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825736357, "id": "ICLR.cc/2018/Conference/-/Paper279/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "B1Gi6LeRZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper279/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper279/Authors|ICLR.cc/2018/Conference/Paper279/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper279/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper279/Authors|ICLR.cc/2018/Conference/Paper279/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper279/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper279/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper279/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper279/Reviewers", "ICLR.cc/2018/Conference/Paper279/Authors", "ICLR.cc/2018/Conference/Paper279/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825736357}}}, {"tddate": null, "ddate": null, "tmdate": 1513533561733, "tcdate": 1513530968477, "number": 3, "cdate": 1513530968477, "id": "BJlPLX4Mz", "invitation": "ICLR.cc/2018/Conference/-/Paper279/Official_Comment", "forum": "B1Gi6LeRZ", "replyto": "HJmtVULeG", "signatures": ["ICLR.cc/2018/Conference/Paper279/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper279/Authors"], "content": {"title": "Response to AnonReviewer1", "comment": "Thanks for your positive review. Our method is novel in that we train the model to output the mixing ratio between two different classes.\n\nAnswers for minor asks:\n(1) We do not define error rate of BC learning in training phase. In testing phase, the error rate definition of BC learning is the same as that of standard learning because we do not mix any sounds in testing phase.\n\n(2) Thanks for helpful advice. Although we could not try more than 50 classes, we have investigated the relationship between performance and the number of training epochs not only on ESC-50 (50 classes, Fig. 6) but also ESC-10 (10 classes). As a result, the sufficient number of training epochs for BC learning on ESC-10 was 900, which is smaller than that for BC learning on ESC-50 (1,200 epochs), whereas that for standard learning was 600 epochs on both ESC-50 and ESC-10. We assume that the number of training epochs needed would become large when there are many classes, as you have suggested. We will add this discussion to the final version.\n\n(3) Yes, G_1 and G_2 are derived from unit conversion from decibels to amplitudes. We will clarify it. Please see also the reply to AnonReviewer2.\n\n(4) We have tried random 1-crop testing and center 1-crop testing on EnvNet on ESC-50 (standard learning). The error rates of random 1-crop testing and center 1-crop testing were 41.3% and 39.2%, respectively, whereas that of 10-crop testing was 29.2% as in the paper. Averaging the predictions of multiple windows leads to a stable performance. We assume this is because we cannot know where the target sound exists in a testing sound, and the target sound sometimes has a long duration."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Between-class Examples for Deep Sound Recognition", "abstract": "Deep learning methods have achieved high performance in sound recognition tasks. Deciding how to feed the training data is important for further performance improvement. We propose a novel learning method for deep sound recognition: Between-Class learning (BC learning). Our strategy is to learn a discriminative feature space by recognizing the between-class sounds as between-class sounds. We generate between-class sounds by mixing two sounds belonging to different classes with a random ratio. We then input the mixed sound to the model and train the model to output the mixing ratio. The advantages of BC learning are not limited only to the increase in variation of the training data; BC learning leads to an enlargement of Fisher\u2019s criterion in the feature space and a regularization of the positional relationship among the feature distributions of the classes. The experimental results show that BC learning improves the performance on various sound recognition networks, datasets, and data augmentation schemes, in which BC learning proves to be always beneficial. Furthermore, we construct a new deep sound recognition network (EnvNet-v2) and train it with BC learning. As a result, we achieved a performance surpasses the human level.", "pdf": "/pdf/635103060ef1a68ee4cf6f8139561a4cec5a360f.pdf", "TL;DR": "We propose an novel learning method for deep sound recognition named BC learning.", "paperhash": "tokozume|learning_from_betweenclass_examples_for_deep_sound_recognition", "_bibtex": "@inproceedings{\ntokozume2018learning,\ntitle={Learning from Between-class Examples for Deep Sound Recognition},\nauthor={Yuji Tokozume and Yoshitaka Ushiku and Tatsuya Harada},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=B1Gi6LeRZ},\n}", "keywords": ["sound recognition", "supervised learning", "feature learning"], "authors": ["Yuji Tokozume", "Yoshitaka Ushiku", "Tatsuya Harada"], "authorids": ["tokozume@mi.t.u-tokyo.ac.jp", "ushiku@mi.t.u-tokyo.ac.jp", "harada@mi.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825736357, "id": "ICLR.cc/2018/Conference/-/Paper279/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "B1Gi6LeRZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper279/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper279/Authors|ICLR.cc/2018/Conference/Paper279/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper279/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper279/Authors|ICLR.cc/2018/Conference/Paper279/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper279/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper279/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper279/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper279/Reviewers", "ICLR.cc/2018/Conference/Paper279/Authors", "ICLR.cc/2018/Conference/Paper279/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825736357}}}, {"tddate": null, "ddate": null, "tmdate": 1513533541101, "tcdate": 1513530661044, "number": 2, "cdate": 1513530661044, "id": "SJp7SQEGz", "invitation": "ICLR.cc/2018/Conference/-/Paper279/Official_Comment", "forum": "B1Gi6LeRZ", "replyto": "HJ80q6KlG", "signatures": ["ICLR.cc/2018/Conference/Paper279/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper279/Authors"], "content": {"title": "Response to AnonReviewer3", "comment": "Thanks for your helpful review.\n\n- Regarding the presentation of BC learning:\nIt is true that BC learning is a data augmentation method as you have suggested, from a view point of using augmented data. However, our method is novel in that we change the objective of training by training the model to output the mixing ratio, which is fundamentally a different idea from previous data augmentation methods. The novelty or key point of our method is not mixing multiple sounds, but rather learning method of training the model to output the mixing ratio. That is why we represent our method as \"learning method.\" We intuitively describe why such a learning method is effective in Section 3.3 and demonstrate the effectiveness of BC learning through wide-ranging experiments.\n\n- Regarding comparison with other data augmentation methods:\nFirst, we compared BC learning with other data augmentation methods that mix multiple sounds in ablation analysis (see Table 2), and showed that our method of mixing just two classes with equation 2 and training the model to output the mixing ratio performs the best.\n\nSecond, our BC learning can be combined with any data augmentation methods that do not mix multiple sounds by mixing two augmented data. In Section 4.1.3, we demonstrated that BC learning is even \"compatible\" with a strong data augmentation, which we believe is more important than being \"stronger\" than that. This data augmentation method uses scale and amplitude augmentation similar to Salamon & Bello (2017) in addition to padding and cropping, and thus, it is close to the state-of-the-art level. As shown in Table 1, the error rates of DSRNet when using only BC learning (18.2%, 10.6%, and 23.4% on ESC-50, ESC-10, and UrbanSound8K, respectively) were lower than those when using the strong data augmentation (21.2%, 10.9%, and 24.9%). Furthermore, as a result of combination of BC learning and the strong data augmentation, we achieved a further higher performance (15.1%, 8.6%, and 21.7%). In this way, we demonstrated the strongness and compatibility of BC learning with other data augmentation techniques through various experiments.\n\nHere, we assume that the effect of BC learning is even strengthened when using a stronger data augmentation scheme. Because the potential within-class variance becomes large when using a strong data augmentation, the overlap between the feature distribution of each class and that of mixed sounds tends to become large and it becomes more difficult for model to output the mixing ratio (see also Fig. 2). Therefore, the effect of enlargement of Fisher\u2019s criterion would become stronger.\n\n- Regarding the limitation of BC learning:\nThanks for your advice. What you have pointed out is correct. However, the dimension of the feature space d is generally designed to be larger than the number of classes c (e.g., EnvNet/DSRNet: 4096; SoundNet: 256; M18: 512; and Logmel-CNN: 5000). If d < c-1, the features cannot sufficiently represent categorical information, and the model would not be able to achieve a good performance. We have tried to train an EnvNet whose dimension of fully connected layer was made less than 49 on ESC-50 with standard learning, but the loss did not begin to decrease. It is not a matter of BC learning. Furthermore, even if there is a network whose d is smaller than c-1, BC learning would enlarge Fisher's criterion and regularize the positional relationship as much as possible. Therefore, we do not think it is an important limitation of BC learning.\n\n\nThanks for other helpful comments. We will reflect them to the final version. Note than we showed the correspondence with a semantic feature space by visualizing the features of mixed sounds in Fig. 3."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Between-class Examples for Deep Sound Recognition", "abstract": "Deep learning methods have achieved high performance in sound recognition tasks. Deciding how to feed the training data is important for further performance improvement. We propose a novel learning method for deep sound recognition: Between-Class learning (BC learning). Our strategy is to learn a discriminative feature space by recognizing the between-class sounds as between-class sounds. We generate between-class sounds by mixing two sounds belonging to different classes with a random ratio. We then input the mixed sound to the model and train the model to output the mixing ratio. The advantages of BC learning are not limited only to the increase in variation of the training data; BC learning leads to an enlargement of Fisher\u2019s criterion in the feature space and a regularization of the positional relationship among the feature distributions of the classes. The experimental results show that BC learning improves the performance on various sound recognition networks, datasets, and data augmentation schemes, in which BC learning proves to be always beneficial. Furthermore, we construct a new deep sound recognition network (EnvNet-v2) and train it with BC learning. As a result, we achieved a performance surpasses the human level.", "pdf": "/pdf/635103060ef1a68ee4cf6f8139561a4cec5a360f.pdf", "TL;DR": "We propose an novel learning method for deep sound recognition named BC learning.", "paperhash": "tokozume|learning_from_betweenclass_examples_for_deep_sound_recognition", "_bibtex": "@inproceedings{\ntokozume2018learning,\ntitle={Learning from Between-class Examples for Deep Sound Recognition},\nauthor={Yuji Tokozume and Yoshitaka Ushiku and Tatsuya Harada},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=B1Gi6LeRZ},\n}", "keywords": ["sound recognition", "supervised learning", "feature learning"], "authors": ["Yuji Tokozume", "Yoshitaka Ushiku", "Tatsuya Harada"], "authorids": ["tokozume@mi.t.u-tokyo.ac.jp", "ushiku@mi.t.u-tokyo.ac.jp", "harada@mi.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825736357, "id": "ICLR.cc/2018/Conference/-/Paper279/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "B1Gi6LeRZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper279/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper279/Authors|ICLR.cc/2018/Conference/Paper279/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper279/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper279/Authors|ICLR.cc/2018/Conference/Paper279/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper279/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper279/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper279/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper279/Reviewers", "ICLR.cc/2018/Conference/Paper279/Authors", "ICLR.cc/2018/Conference/Paper279/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825736357}}}, {"tddate": null, "ddate": null, "tmdate": 1513533505835, "tcdate": 1513529949846, "number": 1, "cdate": 1513529949846, "id": "rJIDfQNfM", "invitation": "ICLR.cc/2018/Conference/-/Paper279/Official_Comment", "forum": "B1Gi6LeRZ", "replyto": "r1vicbqeG", "signatures": ["ICLR.cc/2018/Conference/Paper279/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper279/Authors"], "content": {"title": "Response to AnonReviewer2", "comment": "Thanks for your positive review. \n\n- Regarding equation 2:\nWe use 10^(G_1/20) and 10^(G_2/20) instead of simple G_1 and G_2 to convert decibels to amplitudes. We hypothesize that the ratio of auditory perception for the network is the same as the ratio of amplitude, and define p so that the auditory perception of the mixed sound becomes r: (1-r). This is because the main component functions of CNNs, such as conv/fc, relu, max pooling, and average pooling, satisfy homogeneity (i.e., f(ax) = af(x)) if we ignore the bias. We will clarify the derivation and meaning of equation 2.\n\n- Regarding how to extend BC learning to other modalities:\nWe assume that BC learning can also be applied to image classification. Image data can be treated as 2-D waveforms along x- and y- axis that contain various areas of frequency information in quite a similar manner to sound data. In addition, recent studies on speech/sound recognition have demonstrated that each filter of CNNs learns to respond to a particular frequency area (e.g., Sainath et al., 2015b). Considering them, we assume that CNNs have aspect of recognizing images by treating them as waveforms in a similar manner to how they recognize sounds, and what works on sounds must also work on images. A simple mixing method (r x_1 + (1-r) x_2) would work well, but we assume that a mixing method that treats the images as waveforms (similar to equation 2) leads to a further performance improvement.\n\n- Regarding mixing more than two classes:\nMixing more than two classes would have a similar effect to mixing just two classes. However, the number of class combinations dramatically increases, and it would become difficult to train. Mixing just two classes can directory impose a constraint on the feature distribution (as we describe in Section 3.3). Therefore, we assume that mixing just two classes is the most efficient. Experimental results also show that mixing two classes performs better than mixing three classes (see Table 2).\n\nThanks for other helpful comments. We will reflect them to the final version."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Between-class Examples for Deep Sound Recognition", "abstract": "Deep learning methods have achieved high performance in sound recognition tasks. Deciding how to feed the training data is important for further performance improvement. We propose a novel learning method for deep sound recognition: Between-Class learning (BC learning). Our strategy is to learn a discriminative feature space by recognizing the between-class sounds as between-class sounds. We generate between-class sounds by mixing two sounds belonging to different classes with a random ratio. We then input the mixed sound to the model and train the model to output the mixing ratio. The advantages of BC learning are not limited only to the increase in variation of the training data; BC learning leads to an enlargement of Fisher\u2019s criterion in the feature space and a regularization of the positional relationship among the feature distributions of the classes. The experimental results show that BC learning improves the performance on various sound recognition networks, datasets, and data augmentation schemes, in which BC learning proves to be always beneficial. Furthermore, we construct a new deep sound recognition network (EnvNet-v2) and train it with BC learning. As a result, we achieved a performance surpasses the human level.", "pdf": "/pdf/635103060ef1a68ee4cf6f8139561a4cec5a360f.pdf", "TL;DR": "We propose an novel learning method for deep sound recognition named BC learning.", "paperhash": "tokozume|learning_from_betweenclass_examples_for_deep_sound_recognition", "_bibtex": "@inproceedings{\ntokozume2018learning,\ntitle={Learning from Between-class Examples for Deep Sound Recognition},\nauthor={Yuji Tokozume and Yoshitaka Ushiku and Tatsuya Harada},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=B1Gi6LeRZ},\n}", "keywords": ["sound recognition", "supervised learning", "feature learning"], "authors": ["Yuji Tokozume", "Yoshitaka Ushiku", "Tatsuya Harada"], "authorids": ["tokozume@mi.t.u-tokyo.ac.jp", "ushiku@mi.t.u-tokyo.ac.jp", "harada@mi.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825736357, "id": "ICLR.cc/2018/Conference/-/Paper279/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "B1Gi6LeRZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper279/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper279/Authors|ICLR.cc/2018/Conference/Paper279/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper279/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper279/Authors|ICLR.cc/2018/Conference/Paper279/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper279/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper279/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper279/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper279/Reviewers", "ICLR.cc/2018/Conference/Paper279/Authors", "ICLR.cc/2018/Conference/Paper279/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825736357}}}], "count": 16}