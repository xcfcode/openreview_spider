{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1395068820000, "tcdate": 1395068820000, "number": 5, "id": "VtHoVZBCozVLx", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "8KokDTctkA8e4", "replyto": "8KokDTctkA8e4", "signatures": ["Nando de Freitas"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "If visual attention with deep nets interests you, see also:\r\n\r\nLearning where to attend with deep architectures for image tracking\r\nMisha Denil\u201a Loris Bazzani\u201a Hugo Larochelle and Nando de Freitas\r\nIn Neural Computation. Vol. 24. No. 8. Pages 2151\u20132184. 2012.\r\nDOI (10.1162/NECO_a_00312)"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning generative models with visual attention", "decision": "submitted, no decision", "abstract": "Attention has long been proposed by psychologists as important for effectively dealing with the enormous sensory stimulus available in the neocortex. Inspired by visual attention models in computational neuroscience and by the need for deep generative models to learn on object-centric data, we describe a framework for generative learning using attentional mechanisms. Attentional mechanism propagate signals from region-of-interest in a scene to higher layer areas of canonical representation, where generative modeling takes place. By ignoring background clutter, generative model can concentrate its resources to model objects of interest. Our model is a proper graphical model where the 2D similarity transformation from computer vision is part of the top-down process. A ConvNet is used to initialize good guesses during posterior inference, which is based on Hamiltonian Monte Carlo. Upon learning on face images, we demonstrate that our model can robustly attend to face regions of novel test subjects. Most importantly, our model can learn generative models of new faces from a novel dataset of large images where the location of the face is not known.", "pdf": "https://arxiv.org/abs/1312.6110", "paperhash": "tang|learning_generative_models_with_visual_attention", "keywords": [], "conflicts": [], "authors": ["Charlie Tang", "Nitish Srivastava", "Ruslan Salakhutdinov"], "authorids": ["clarecorp@gmail.com", "nitish.sri89@gmail.com", "rsalakhu@cs.toronto.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392797040000, "tcdate": 1392797040000, "number": 4, "id": "ufIDfM96DXGE3", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "8KokDTctkA8e4", "replyto": "8KokDTctkA8e4", "signatures": ["Charlie Tang"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We thank the reviewer for the thoughtful review, and we look forward to improving the manuscript in the future version.\r\n\r\nThe main motivation from this paper is not using ConvNet as a STOA object detector, but to allow for learning generative models of objects of interest in large unlabelled images. A good face detector would probably do just as well on localizing faces. However, our attentional inference process is a function of v, meaning that we can attend to whatever the top-down generative model 'has in mind'. Section 6.3 demonstrates that given the same input image but with different v, our model attends to the correct subject. \r\n\r\nWhile an off-the-shelf VJ face detector would do very well, it is unclear how a face detector would perform if it were only trained on the Caltech faces (450 faces). Our model performs slightly worse when clamping 'v' to be the mean face throughout the inference process. Note that we are already initializing v with the mean face.\r\n\r\nQualitatively, the samples in Fig. 6b do look more like the CMU multipie faces while samples in Fig. 6a look more like faces from the Caltech faces dataset. \r\n\r\nSection 6.3: It is not a trivial task because the ConvNet takes as its input v and the big image and then predict the next gaze. The ConvNet is learned to shift either left or right depending on what v is. This is not the same as a template matcher with v as the template. We agree that having the 'same-sex' targets would be an interesting experiment.\r\n\r\nThe threshold was chosen by hand, high enough to have a small number of false positives, and low enough to have reasonable recall (951/1418). This threshold was used to filter out falsely localized gazes on the novel dataset. \r\n\r\nIt is not a simple 3 layer DBM (at the top of page 4) because x is a function of u. It could be thought of as a 3 layer DBM with high-order multiplicative interactions between u, v, and x.\r\n\r\nPlease see the comment to Reviewer eb85 regarding the overloaded notation and the contradictory statements about HMC and local minima."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning generative models with visual attention", "decision": "submitted, no decision", "abstract": "Attention has long been proposed by psychologists as important for effectively dealing with the enormous sensory stimulus available in the neocortex. Inspired by visual attention models in computational neuroscience and by the need for deep generative models to learn on object-centric data, we describe a framework for generative learning using attentional mechanisms. Attentional mechanism propagate signals from region-of-interest in a scene to higher layer areas of canonical representation, where generative modeling takes place. By ignoring background clutter, generative model can concentrate its resources to model objects of interest. Our model is a proper graphical model where the 2D similarity transformation from computer vision is part of the top-down process. A ConvNet is used to initialize good guesses during posterior inference, which is based on Hamiltonian Monte Carlo. Upon learning on face images, we demonstrate that our model can robustly attend to face regions of novel test subjects. Most importantly, our model can learn generative models of new faces from a novel dataset of large images where the location of the face is not known.", "pdf": "https://arxiv.org/abs/1312.6110", "paperhash": "tang|learning_generative_models_with_visual_attention", "keywords": [], "conflicts": [], "authors": ["Charlie Tang", "Nitish Srivastava", "Ruslan Salakhutdinov"], "authorids": ["clarecorp@gmail.com", "nitish.sri89@gmail.com", "rsalakhu@cs.toronto.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392796800000, "tcdate": 1392796800000, "number": 1, "id": "Fe6EF0Bh0rb1D", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "8KokDTctkA8e4", "replyto": "fRoBOpqe7dfVP", "signatures": ["Charlie Tang"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "We thank the reviewer for the thoughtful review, and we look forward to improving\r\nthe manuscript in the future version.\r\n\r\nThe proposed model in this paper is a promising machine learning model because it allows for learning generative models of objects of interest in large images, which has not been done previously as far as we know. Specifically, without attention, it is simply computationally too expensive to learn good generative models based on large images.\r\n\r\nThe reviewer is correct in pointing out that we did not want 'gaze' to be taken literally. The model is only inspired by [18], while the main focus is to allow efficient learning of generative models of objects of interest given large unlabelled training images.\r\n\r\nIt is regrettable that p was used twice, we will correct this. The reviewer is also correct that we glossed over the description of the inference procedure in DBNs due to space limitations. We used approximate inference in the DBN by using only the first-layer RBM to infer h1, followed by using the 2nd layer RBM to infer h2. This is the same inference procedure used for the greedy layer-wise stacking of RBMs.\r\n\r\nWe used a standard ConvNet architecture with C layers followed by max-pooling S layers. First C layer had 16 5x5 filters looking at 72x72 image using ReLU hidden units followed by max-pooling and another round of a C layer and a max-pooling layer. This resulted in 16 filter maps with dimension of 20x20. A separate stream for the smaller canonical view used a C layer with 16 5x5 filters operating on the canonical 24x24 face patches, giving 16 filter maps with dimension of 20x20. The two sets of filter maps are combined by element-wise multiplication. Two fully connected layers followed with 1024 ReLU hidden units, followed by the final output of 4 predictions.\r\n\r\nIteratively predicting transformations is novel w.r.t. training generative models, especially for converging to the correct pose of the object of interest. \r\n\r\nHowever, there are methods in literature for shifting to areas of interest in the context of object detection, e.g. [R1].\r\n\r\nHMC is used for fine-tuning the window position and making our inference probabilistically correct. The bulk of the work is performed by the ConvNet. Using HMC alone for inference in this paper will get stuck in local optima. The sentence 'Resampling the momentum variable ... momentum to jump out of local minima' is regarding the general theory of why momentum variables are resampled in the HMC algorithm. We will clarify this.\r\n\r\nSection 6.1: yes the full model is run here: The DBN had two layers with 1024 hidden units for the first layer and 200 hidden units for the 2nd layer. We do not know the STOA performance for this dataset, but it is considered an easy dataset for face detection. We want to emphasize that our framework is not aimed at achieving STOA for object detection. Our aim is to allow for learning generative models of objects of interest in large unlabelled images. Depending on what the generative model has in mind in v, no new detector for v needs to be trained. This is demonstrated in the 'Ambiguity' experimental section. Note that u is initialized to be centered with a random jitter of 30 pixels and randomly scaled from 0.5 to 1.5 the size (see footnote 1). \r\n\r\nWhile images have faces roughly centered, this is not cheating as can be seen from the initial yellow box in figure 4, left panel.\r\n\r\nThe DBN, modeling the Caltech faces, is finetuned with the additional CMU multipie faces. Learning was performed with greedy layer-wise training using FPCD. Fig. 6 qualitatively shows that additional faces are learned from the CMU dataset.\r\n\r\nWhen combining two RBMs (in a specific way) you can either have a DBN or a DBM, depending on whether you want to interpret the resulting model as a directed or an undirected model.\r\n\r\nFigure 4: the artifact comes from mirroring the pixels at the borders to pad the image.\r\n\r\n[R1] Searching for objects driven by context. Alexe et al. NIPS 2012."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning generative models with visual attention", "decision": "submitted, no decision", "abstract": "Attention has long been proposed by psychologists as important for effectively dealing with the enormous sensory stimulus available in the neocortex. Inspired by visual attention models in computational neuroscience and by the need for deep generative models to learn on object-centric data, we describe a framework for generative learning using attentional mechanisms. Attentional mechanism propagate signals from region-of-interest in a scene to higher layer areas of canonical representation, where generative modeling takes place. By ignoring background clutter, generative model can concentrate its resources to model objects of interest. Our model is a proper graphical model where the 2D similarity transformation from computer vision is part of the top-down process. A ConvNet is used to initialize good guesses during posterior inference, which is based on Hamiltonian Monte Carlo. Upon learning on face images, we demonstrate that our model can robustly attend to face regions of novel test subjects. Most importantly, our model can learn generative models of new faces from a novel dataset of large images where the location of the face is not known.", "pdf": "https://arxiv.org/abs/1312.6110", "paperhash": "tang|learning_generative_models_with_visual_attention", "keywords": [], "conflicts": [], "authors": ["Charlie Tang", "Nitish Srivastava", "Ruslan Salakhutdinov"], "authorids": ["clarecorp@gmail.com", "nitish.sri89@gmail.com", "rsalakhu@cs.toronto.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392796620000, "tcdate": 1392796620000, "number": 1, "id": "e9ZBYBHFViYsc", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "8KokDTctkA8e4", "replyto": "waohvbXH4ZvO1", "signatures": ["Charlie Tang"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "We thank the reviewer for the thoughtful review, and we look forward to improving\r\nthe manuscript in the future version.\r\n\r\nDue to the limited space, some details were left out. We will try to address them in the next version. The training of ConvNet is standard and did not involve any special 'tricks'. We used SGD with minibatch size of 128 samples. We used a standard ConvNet architecture with C layers followed by max-pooling S layers. First C layer had 16 5x5 filters looking at the 72x72 image using ReLU hidden units followed by max-pooling and another round of a C layer and a max-pooling layer.  This resulted in 16 filter maps with dimension of 20x20. A separate stream for the smaller canonical view used a C layer with 16 5x5 filters operating on the canonical 24x24 face patches, giving 16 filter maps with dimension of 20x20. The two sets of filter maps are combined by element-wise multiplication. Two fully connected layers followed with 1024 ReLU hidden units, followed by the final output of 4 predictions. \r\n\r\nIn step 3 of fig. 3, one alternating Gibbs update is performed. \r\n\r\nWhile learning a Gaussian RBM model might be slow, step 3 in fig. 3 performs inference after learning. In fact, 50 Gibbs steps would actually adversely affect inference, since the Markov chain would drift to a different face as it samples from the distribution over all of the faces that the GRBM model has learned.\r\n\r\nPage 7: we used Fast PCD to train each layer of the DBN model in a greedy layer-wise fashion. No finetuning of the entire network was performed.\r\n\r\nThe threshold was chosen by hand, high enough for a small number of false positives, and low enough to have reasonable recall (951/1418). This threshold was used to filter out falsely localized gazes on the novel dataset. \r\n\r\nThe ConvNet part of the framework is more important than the generative model. Choosing GMMs instead of DBNs would perform equally well. The novelty of our framework is that it allows for learning generative models of objects of interest in large images.\r\n\r\nThe EM algorithm in section 5 is valid as long as the approximate posterior is unbiased. We can not guarantee that our localization procedure will always work. The role of the threshold is to not learn on false faces if we fail to localize.\r\n\r\nLast paragraph of page 7 states that $ log p(x(u)|u,v)$ (a Gaussian distribution) is thresholded, which is a function of $ v $. Essentially, we compare Euclidean distance between a generic face $ v $ and the localized image patch.\r\n\r\nAddressing your minor comments:\r\nBottom of page 2: when combining two RBMs you can either have a DBN or a DBM, depending on if you want to interpret the resulting model as a hybrid directed/undirected (DBN) or a fully undirected model (DBM).\r\n\r\nIn fig 3, the ConvNet takes two inputs: a small cropped face (which is above it) as well as a larger image (below it) and outputs a prediction that is [x, y, r, s].\r\n\r\nIn section 6.2, we perform approximate inference in the DBN by using the first-layer RBM to infer h1, followed by using the 2nd layer RBM to infer h2. This is the same inference procedure used for the greedy layer-wise stacking of RBMs."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning generative models with visual attention", "decision": "submitted, no decision", "abstract": "Attention has long been proposed by psychologists as important for effectively dealing with the enormous sensory stimulus available in the neocortex. Inspired by visual attention models in computational neuroscience and by the need for deep generative models to learn on object-centric data, we describe a framework for generative learning using attentional mechanisms. Attentional mechanism propagate signals from region-of-interest in a scene to higher layer areas of canonical representation, where generative modeling takes place. By ignoring background clutter, generative model can concentrate its resources to model objects of interest. Our model is a proper graphical model where the 2D similarity transformation from computer vision is part of the top-down process. A ConvNet is used to initialize good guesses during posterior inference, which is based on Hamiltonian Monte Carlo. Upon learning on face images, we demonstrate that our model can robustly attend to face regions of novel test subjects. Most importantly, our model can learn generative models of new faces from a novel dataset of large images where the location of the face is not known.", "pdf": "https://arxiv.org/abs/1312.6110", "paperhash": "tang|learning_generative_models_with_visual_attention", "keywords": [], "conflicts": [], "authors": ["Charlie Tang", "Nitish Srivastava", "Ruslan Salakhutdinov"], "authorids": ["clarecorp@gmail.com", "nitish.sri89@gmail.com", "rsalakhu@cs.toronto.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391872080000, "tcdate": 1391872080000, "number": 3, "id": "tiYHiyBGHPtuf", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "8KokDTctkA8e4", "replyto": "8KokDTctkA8e4", "signatures": ["anonymous reviewer 61e2"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning generative models with visual attention", "review": "The proposed solution is a conditional model (conditioned on a large input image I), which augments the typical RBM with two additional sets of random variables: transformations parameters $u$ and a 'steerable' visible layer $x$ (a patch of I, whose location is defined by $u$). An additional term in the energy function ensures that low-energy configurations are given to settings of $u$, for which $x$ is close to $v$ in an L2 sense. Inferring the transformation parameters in this model is thus analogous to the task of objection detection.  To shortcut this difficult optimization process the authors propose to use Hybrid Monte Carlo (HMC), but where the states $u$ are initialized via a convolutional neural network (CNN).\r\n\r\nI find the general idea behind the paper compelling: the problems of attention and scaling up of generative models are important ones which warrant further attention. While a solid step in this direction, the experimental section does not convincingly show evidence in favor of the model, in particular, it lacks proper baselines.\r\n\r\nSection 6.1: How do state of the art object detection / tracking algorithms do on this task ? Normalized cross-correlation and L2 template matching do not seem like appropriate baselines.\r\n\r\nSection 6.2 aims to show that the 'novelty of our model is that it can learn on new large images of faces without label information'. Unfortunately, the authors provide little evidence in favor of this. After training a DBN and the approximate inference module on Caltech, the authors report a successfull detection rate of 951/1418, unfortunately without any context or comparisons.  How does this compare against a standard Viola-Jones detector ? How would the model perform by simply clamping 'v' to the average face image ?  I would not expect the proposed generative model to outperform a dedicated face detection module, but the baseline seems necessary to provide context.  Furthermore, how are the samples in Fig. 6b evidence that the DBN learnt the Multie-PIE dataset ?  Are the samples 'more' qualitatively similar to Multi-PIE than Caltech ?  Without being familiar with Multi-Pie, this is not at all obvious. Samples in 6b appear similar (but worse) to 6a. A quantitative analysis seems necessary.\r\n\r\nThe experiments of Section 6.3 is interesting at a high-level, but its execution seems flawed. It appears that the authors clamped 'v' to the very patch (i.e.  cropped face image from the test set) which they intend to detect (?). This seems like an all too trivial task. To highlight the benefits of having a proper generative model, v could have been clamped to e.g. the face of a different individual but of the same sex as the 'target'. A more convincing application might be with a classification RBM whereby clamping label units results in 'attending' to the corresponding areas of the image.\r\n\r\nThat being said, I do look forward to the next revision of the paper.\r\n\r\n\r\nOther points:\r\n\r\n* A threshold parameter is used for detection in Section 6.2. How was this parameter chosen ? Precision/Recall seems like the only relevant measure here.\r\n\r\n* I found Section 3 to be particularly confusing, in large part due to the notation which obfuscates the relationship between $x$ and $u$. From the energy function, one could not be blamed for thinking that the model is a simple 3-layer DBM (with constant term f(u), to be rolled into the partition function).\r\n\r\n* I also found the description of the transformation parameters and warp w a bit confusing. For instance, 'used to rotate, scale [...] the canonical v' is a bit misleading, as v never actually undergoes any transformation. At a high-level, it seems simpler to think of u as selecting a patch x of I (where the probability of u is proportional to the L2 distance between x and v) ? One possible suggestion to help with clarity would be to delay the description of the warp transform to Section 4, and describe the model in terms of a generic transform T(I, u) ?\r\n\r\n* Overloaded notation for p (both pixel coordinate and momentum variable)\r\n\r\n* Contradictory statements about HMC and local minima.\r\n\r\n* lots of typos\r\n\r\n\r\nPROS:\r\n* novelty of model and potential for applications\r\n* approximate inference scheme (which continues the trend of using function approximation for approximate inference)\r\nCONS:\r\n* weak experimental evidence (lack of proper baselines)\r\n* clarity of presentation"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning generative models with visual attention", "decision": "submitted, no decision", "abstract": "Attention has long been proposed by psychologists as important for effectively dealing with the enormous sensory stimulus available in the neocortex. Inspired by visual attention models in computational neuroscience and by the need for deep generative models to learn on object-centric data, we describe a framework for generative learning using attentional mechanisms. Attentional mechanism propagate signals from region-of-interest in a scene to higher layer areas of canonical representation, where generative modeling takes place. By ignoring background clutter, generative model can concentrate its resources to model objects of interest. Our model is a proper graphical model where the 2D similarity transformation from computer vision is part of the top-down process. A ConvNet is used to initialize good guesses during posterior inference, which is based on Hamiltonian Monte Carlo. Upon learning on face images, we demonstrate that our model can robustly attend to face regions of novel test subjects. Most importantly, our model can learn generative models of new faces from a novel dataset of large images where the location of the face is not known.", "pdf": "https://arxiv.org/abs/1312.6110", "paperhash": "tang|learning_generative_models_with_visual_attention", "keywords": [], "conflicts": [], "authors": ["Charlie Tang", "Nitish Srivastava", "Ruslan Salakhutdinov"], "authorids": ["clarecorp@gmail.com", "nitish.sri89@gmail.com", "rsalakhu@cs.toronto.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391560620000, "tcdate": 1391560620000, "number": 1, "id": "fRoBOpqe7dfVP", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "8KokDTctkA8e4", "replyto": "8KokDTctkA8e4", "signatures": ["anonymous reviewer eb8f"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning generative models with visual attention", "review": "The authors present an attentional generative model, inspired by the routing circuits model of Olshausen et al. (ref. [14]). Similar to some earlier work, attentional aspects result from the model essentially being misspecified to only represent a single object, treating everything else in the image as noise implicitly (besides the cited [18], Chikkerur et al, 2009, is another relevant example). The model consists of a Deep Belief Net (DBN) that models individual objects (here: face patches) in a canonical reference frame, and a transformation operation that scales/rotates/translates the object to be positioned in a larger image. Inference iteratively updates both the internal representations and hypothesized positioning in the image, and is based on Hamiltonian Monte Carlo (HMC) as well as a convolutional net (ConvNet), which makes initial guesses.\r\n\r\nPros: I find the topic interesting, and the approach original and creative. \r\nCons: The paper suffers from quality and clarity issues. Moreover, this is not primarily a biological model, so the question is whether the proposed approach is promising for the machine learning application. As with other attention models, I find the evidence lacking that attention is really needed here to solve the task or that the approach works better than other, perhaps simpler alternatives.\r\n\r\n\r\nDetails:\r\n\r\nFirst of all, the paper has a number of typos and grammar issues that should be fixed (three examples right in the abstract: 'enormous sensory stimulus available in the neocortex' is an awkward formulation; 'we describe for [a?] generative learning framework', 'signals from [a] region of interest').\r\n\r\nI understand that biological attention this is not the primary focus of the paper, but I would have enjoyed a bit more discussion of the notion that attention corresponds to transforming input into an object centered frame, other than citing ref [14]. As far as I know, this is a speculative proposal and it would be nice to discuss whether there is recent evidence or theories supporting it. Similarly, the authors could have explained better how their framework is to be interpreted in terms of biological attention, if at all. For example, they appear to contrast their model to the 'covert' one of [18], and similarly the term 'gaze variables' suggests an interpretation in terms of overt eye movements. But note that the Olshausen model was supposed to model covert processes, i.e. internal routing of information with fixed retinal input. I also take it that we're not supposed to take the 'gaze' interpretation too literally, seeing as arbitrary scaling and rotations are involved...\r\n\r\nThe description of the model could be made a bit clearer. For example, the full graphical model could be displayed in Fig 1, rather than 'hiding' parts in the black box. {p} appears to be introduced twice (Section 3, paragraphs 3 and 5), and later p is used as momentum variable in HMC. I also get the impression that some parts of the model are not explained perhaps because they further complicate the model. In particular, the figure and detailed description only cover inference in a RBM. However, a DBN is what is actually used. Presumably the the hybrid directed/undirected DBN further complicates inference? \r\n\r\nSimilarly, the architecture and training parameters of the ConvNet are not described at all, even though the latter seems to be what does the main work when it comes to localizing the faces. Speaking of which, I found iteratively predicting the transformations with the ConvNet interesting--it would be good if the authors could comment on whether this is a novel contribution or whether there are related approaches. I'm less convinced by the performance of the HMC, which mostly seems to fine-tune the window position. Note that the authors motivate the HMC in Section 3 by writing that it helps 'with jumping out of local maximum' [sic], but then justify the ConvNet in Section 4 by writing that HMC tends to get stuck in local optima...\r\n\r\nAlso Section 6.1, is the full model run (including the DBN) here? If so, what are the model parameters? The detection performance is only compared to two, presumably baseline methods. What is state-of-the-art on this dataset? Also, footnote 1 says that u was initialized to be centered. Depending on the dataset, isn't that potentially cheating?\r\n\r\nSection 6.2: the authors state that the novelty of their approach is demonstrated here. The authors essentially first train their model with labels, and then use this first model as a face detector on a second, unlabeled dataset to localize the faces and train a second DBN. Here it was not clear to me whether they train the second DBN from scratch or merely fine-tune the first DBN pretrained with labels (they say they train the second DBN with FPCD. FPCD is for RBMs, so are they referring to layer-wise pretraining?). Either way, the main issue here is that the authors don't actually quantify the performance of the second model, so it is not clear if anything is gained with this approach, over just taking the first model that was only trained on the labeled data.\r\n\r\nLastly, I found Section 6.2, 'Inference with ambiguity', the most interesting bit, as it actually demonstrates the interaction between the canonical face model and the localization. Unfortunately, this part is very short. Perhaps the authors can expand this in future work.\r\n\r\nLeftovers:\r\n\r\n* Section 2: 'If a second level RBM is used to model the activities of the hidden units of the first layer GRBM, we can combine their energy functions to form a Deep Belief Net (DBN) [2]'. Doesn't simply combining the energies give a deep Boltzmann machine rather than a DBN?\r\n* Section 4.1: 'the spatial frequency of the natural image signals can form many local minima': this should be expressed better.\r\n* Section 4.1: '(e.g. 72\u00d772) and v (e.g. 24\u00d724)' here and elsewhere: why the 'e.g.'? Just say that this is what you are using.\r\n* Figure 3: the caption should clarify that this is not the full inference process, only the initial part.\r\n* Figure 4: there seem to be artifacts in the images?\r\n* Figure 5: the fonts are too small to be readable."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning generative models with visual attention", "decision": "submitted, no decision", "abstract": "Attention has long been proposed by psychologists as important for effectively dealing with the enormous sensory stimulus available in the neocortex. Inspired by visual attention models in computational neuroscience and by the need for deep generative models to learn on object-centric data, we describe a framework for generative learning using attentional mechanisms. Attentional mechanism propagate signals from region-of-interest in a scene to higher layer areas of canonical representation, where generative modeling takes place. By ignoring background clutter, generative model can concentrate its resources to model objects of interest. Our model is a proper graphical model where the 2D similarity transformation from computer vision is part of the top-down process. A ConvNet is used to initialize good guesses during posterior inference, which is based on Hamiltonian Monte Carlo. Upon learning on face images, we demonstrate that our model can robustly attend to face regions of novel test subjects. Most importantly, our model can learn generative models of new faces from a novel dataset of large images where the location of the face is not known.", "pdf": "https://arxiv.org/abs/1312.6110", "paperhash": "tang|learning_generative_models_with_visual_attention", "keywords": [], "conflicts": [], "authors": ["Charlie Tang", "Nitish Srivastava", "Ruslan Salakhutdinov"], "authorids": ["clarecorp@gmail.com", "nitish.sri89@gmail.com", "rsalakhu@cs.toronto.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391466600000, "tcdate": 1391466600000, "number": 2, "id": "waohvbXH4ZvO1", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "8KokDTctkA8e4", "replyto": "8KokDTctkA8e4", "signatures": ["anonymous reviewer de65"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning generative models with visual attention", "review": "Summary\r\nThis paper proposed a probabilistic framework for learning generative models where the 2D Similarity transformation is a part of the top-down process and a ConvNet is used to initialize the posterior inference. \r\nThough this paper is clearly interesting and important for both deep learning and computer vision community, it could potentially be improved. The main problem is that the current  submission does not include all of necessary details (please see the below), especially for the experimental section. \r\n\r\nPros\r\n-- well-written and organized\r\n-- an unified probabilistic graphical model framework for generation and detection \r\n-- interesting experimental results   \r\nCons\r\nThe current submission lacks of some important details:\r\n     The details of the ConvNet and its training process;\r\n     How many steps of Gibbs samples executed in step 3 in fig.3? For a Gaussian RBM trained on face image the mixing of its markov chain could be extremely slow; \r\n     What do you mean 'training DBN by FPCD' in page 7? Do you mean wake sleep algorithm?\r\n     How do you determine the threshold for logP(x|u,v)? I believe this threshold could be crucial for learning without gaze labels.\r\nThere are not any baseline for comparison in the section of experiments. Which part of the framework is more important? The DBN or the convNet? At least you could try to change DBN to a Gaussian mixture model (and/or change the ConvNet to a randomly guess). \r\nIs the Monte Carlo EM algorithm described in section 5 valid? Is there any guarantee? It can be seen in section 6.2 that the E-step could fail to localize faces. How could you prevent the DBN from learning those false faces.\r\nTo be honest, I am not convinced that a RBM (or DBN) trained on face images can be such a good generative model that always give higher free energy for non-face images. \r\n\r\nMinor comments\r\nLast paragraph , Page 2: Combining two energy functions of RBMs forms a DBM.\r\nFig. 3: What do you mean about the arrows above the ConvNet in step 2 and step 4?\r\nSection 6.2: please explain how you do inference in a DBN."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning generative models with visual attention", "decision": "submitted, no decision", "abstract": "Attention has long been proposed by psychologists as important for effectively dealing with the enormous sensory stimulus available in the neocortex. Inspired by visual attention models in computational neuroscience and by the need for deep generative models to learn on object-centric data, we describe a framework for generative learning using attentional mechanisms. Attentional mechanism propagate signals from region-of-interest in a scene to higher layer areas of canonical representation, where generative modeling takes place. By ignoring background clutter, generative model can concentrate its resources to model objects of interest. Our model is a proper graphical model where the 2D similarity transformation from computer vision is part of the top-down process. A ConvNet is used to initialize good guesses during posterior inference, which is based on Hamiltonian Monte Carlo. Upon learning on face images, we demonstrate that our model can robustly attend to face regions of novel test subjects. Most importantly, our model can learn generative models of new faces from a novel dataset of large images where the location of the face is not known.", "pdf": "https://arxiv.org/abs/1312.6110", "paperhash": "tang|learning_generative_models_with_visual_attention", "keywords": [], "conflicts": [], "authors": ["Charlie Tang", "Nitish Srivastava", "Ruslan Salakhutdinov"], "authorids": ["clarecorp@gmail.com", "nitish.sri89@gmail.com", "rsalakhu@cs.toronto.edu"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387874700000, "tcdate": 1387874700000, "number": 49, "id": "8KokDTctkA8e4", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "8KokDTctkA8e4", "signatures": ["clarecorp@gmail.com"], "readers": ["everyone"], "content": {"title": "Learning generative models with visual attention", "decision": "submitted, no decision", "abstract": "Attention has long been proposed by psychologists as important for effectively dealing with the enormous sensory stimulus available in the neocortex. Inspired by visual attention models in computational neuroscience and by the need for deep generative models to learn on object-centric data, we describe a framework for generative learning using attentional mechanisms. Attentional mechanism propagate signals from region-of-interest in a scene to higher layer areas of canonical representation, where generative modeling takes place. By ignoring background clutter, generative model can concentrate its resources to model objects of interest. Our model is a proper graphical model where the 2D similarity transformation from computer vision is part of the top-down process. A ConvNet is used to initialize good guesses during posterior inference, which is based on Hamiltonian Monte Carlo. Upon learning on face images, we demonstrate that our model can robustly attend to face regions of novel test subjects. Most importantly, our model can learn generative models of new faces from a novel dataset of large images where the location of the face is not known.", "pdf": "https://arxiv.org/abs/1312.6110", "paperhash": "tang|learning_generative_models_with_visual_attention", "keywords": [], "conflicts": [], "authors": ["Charlie Tang", "Nitish Srivastava", "Ruslan Salakhutdinov"], "authorids": ["clarecorp@gmail.com", "nitish.sri89@gmail.com", "rsalakhu@cs.toronto.edu"]}, "writers": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 8}