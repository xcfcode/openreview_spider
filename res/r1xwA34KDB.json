{"notes": [{"id": "r1xwA34KDB", "original": "HyeyznyBPB", "number": 268, "cdate": 1569438927224, "ddate": null, "tcdate": 1569438927224, "tmdate": 1577168289132, "tddate": null, "forum": "r1xwA34KDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Learning Invariants through Soft Unification", "authors": ["Nuri Cingillioglu", "Alessandra Russo"], "authorids": ["nuri.cingillioglu13@imperial.ac.uk", "a.russo@imperial.ac.uk"], "keywords": ["representation learning", "neural networks", "unification"], "TL;DR": "End-to-end learning of invariant representations with variables across examples such as if someone went somewhere then they are there.", "abstract": "Human reasoning involves recognising common underlying principles across many examples by utilising variables. The by-products of such reasoning are invariants that capture patterns across examples such as \"if someone went somewhere then they are there\" without mentioning specific people or places. Humans learn what variables are and how to use them at a young age, and the question this paper addresses is whether machines can also learn and use variables solely from examples without requiring human pre-engineering. We propose Unification Networks that incorporate soft unification into neural networks to learn variables and by doing so lift examples into invariants that can then be used to solve a given task. We evaluate our approach on four datasets to demonstrate that learning invariants captures patterns in the data and can improve performance over baselines.", "pdf": "/pdf/8a6cd032fff43e4e9182a63e4448629f4b1867cf.pdf", "code": "https://drive.google.com/file/d/1Ema_awqOoOn-Xd4aTkkwOQz1r26QqZ4A/view?usp=sharing", "paperhash": "cingillioglu|learning_invariants_through_soft_unification", "original_pdf": "/attachment/9d506633727694103537ffb5c1539c4891ad09a4.pdf", "_bibtex": "@misc{\ncingillioglu2020learning,\ntitle={Learning Invariants through Soft Unification},\nauthor={Nuri Cingillioglu and Alessandra Russo},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xwA34KDB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "oltx8YV7Bw", "original": null, "number": 1, "cdate": 1576798691898, "ddate": null, "tcdate": 1576798691898, "tmdate": 1576800943424, "tddate": null, "forum": "r1xwA34KDB", "replyto": "r1xwA34KDB", "invitation": "ICLR.cc/2020/Conference/Paper268/-/Decision", "content": {"decision": "Reject", "comment": "The main concern raised by reviewers is the limited experiments, which are on simple tasks and missing some baselines to state-of-the-art methods. While the overall approach is interesting, the reviewers found the empirical evidence to be fairly unconvincing. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Invariants through Soft Unification", "authors": ["Nuri Cingillioglu", "Alessandra Russo"], "authorids": ["nuri.cingillioglu13@imperial.ac.uk", "a.russo@imperial.ac.uk"], "keywords": ["representation learning", "neural networks", "unification"], "TL;DR": "End-to-end learning of invariant representations with variables across examples such as if someone went somewhere then they are there.", "abstract": "Human reasoning involves recognising common underlying principles across many examples by utilising variables. The by-products of such reasoning are invariants that capture patterns across examples such as \"if someone went somewhere then they are there\" without mentioning specific people or places. Humans learn what variables are and how to use them at a young age, and the question this paper addresses is whether machines can also learn and use variables solely from examples without requiring human pre-engineering. We propose Unification Networks that incorporate soft unification into neural networks to learn variables and by doing so lift examples into invariants that can then be used to solve a given task. We evaluate our approach on four datasets to demonstrate that learning invariants captures patterns in the data and can improve performance over baselines.", "pdf": "/pdf/8a6cd032fff43e4e9182a63e4448629f4b1867cf.pdf", "code": "https://drive.google.com/file/d/1Ema_awqOoOn-Xd4aTkkwOQz1r26QqZ4A/view?usp=sharing", "paperhash": "cingillioglu|learning_invariants_through_soft_unification", "original_pdf": "/attachment/9d506633727694103537ffb5c1539c4891ad09a4.pdf", "_bibtex": "@misc{\ncingillioglu2020learning,\ntitle={Learning Invariants through Soft Unification},\nauthor={Nuri Cingillioglu and Alessandra Russo},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xwA34KDB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1xwA34KDB", "replyto": "r1xwA34KDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795722197, "tmdate": 1576800273447, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper268/-/Decision"}}}, {"id": "SJx70JRSiB", "original": null, "number": 3, "cdate": 1573408715005, "ddate": null, "tcdate": 1573408715005, "tmdate": 1573408715005, "tddate": null, "forum": "r1xwA34KDB", "replyto": "BJxGVXNpKB", "invitation": "ICLR.cc/2020/Conference/Paper268/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "Dear reviewer, thank you for your feedback. We are excited that you find the idea interesting and important. To answer your questions and comments:\n\n\u201cBut the relationship between soft unification and attention isn't really spelled out -- what's the same, what's different?\u201d - The difference / similarity is mentioned multiple times in the paper, firstly in the introduction: \u201cwe consider unification a selection of the most appropriate value .., we can reframe it as a form of attention.\u201d ; secondly in Section 2: \u201cFor example, \u2026 $\\phi_V(X:s_d)$ would become a weighted sum of symbol embeddings as in conventional attention models\u201d, and finally in Definition 3 where soft unification is defined as a dot product attention, equation 3. Hence, soft unification is implemented as a form of dot product attention.\n\n\u201cUltimately it's not clear to me what the model is attending over during soft unification.\u201d - Following equation 3, soft unification attends over the symbols present in the example K. This K is another example from the dataset and could be a sequence, grid, a story or a logic program as setup in the datasets section and detailed for each architecture in Section 3.\n\n\u201cstrong vs. weak supervision\u201d - We mention the strongly supervised experiments in Section 5: \u201cin the strongly supervised cases, the negative log-likelihood for the context attentions are also added to the objective function.\u201d In the memory networks literature surrounding the bAbI dataset, this refers to using the supporting facts. We do not supervise the soft unification mechanism in any of the experiments.\n\n\u201ccomparison models DMN and IMA are not introduced at all, and include no references\u201d - We do not provide detailed previous work to reduce clutter, distinguish our work and adhere to space constraints. The reference for them are in the caption of Table 3: \u201cand DMN, IMA by Cingillioglu & Russo (2019).\u201d We ask the readers to refer to the cited paper for further details. Similarly N2N, GN2N, EntNet are not details and we ask the readers to refer to the citations.\n\n\u201cthe logical reasoning experiment is not clearly described\u201d - We use an existing data generation procedure as mentioned, \u201cusing the procedure by Cingillioglu & Russo (2019).\u201c and only give details of the specific settings we used to generate the data such as the arity and size of the data. Similar to the bAbI dataset, for the logical reasoning dataset we ask readers to refer to the original papers that introduce the individual tasks.\n\n\u201cthere is only a cursory conclusion\u201d - This is quite subjective as the conclusion of the paper clearly states a novel approach to incorporating variables to neural network architectures and learning invariants. We present the concrete output of this approach: the invariants learnt by analysing soft unification mechanism implemented as an attention.\n\n\u201cI am not sure the model has found a compelling use case.\u201d - This is interesting as the judgement seems to be made on the final accuracy based performance of the model in the bAbI and the logical reasoning dataset. The objective is \u201clearning invariants\u201d rather than to lower error rates further. We urge the readers to consider the qualitative novel output of our approach instead of a win or lose against other models in certain datasets. Our approach is flexible in the network architecture (UMLP, UCNN, UMN) as well as the tasks it can solve, in some cases  better or as good as other models.\n\n\u201cBut variables could very well be innate and simply early emerging.\u201d - This is also very interesting, it may very well be. We followed the line of work cited in the introduction and the related work to establish our argument showing evidence such as pretend play etc. as to why the notion of a variable could be learned. The discussion about whether it could be innate is more appropriate in the field of developmental psychology that is outside the scope and focus of this work. We would be happy to incorporate references showing evidence for the innateness of variables in human reasoning.\n\nPlease note that, Reviewer 4 supports the motivation of a cognitive background and Reviewer 1 points out the paper is well written, structured and clear. These seem to counter your two main concerns. We fail to find in the review any further scientific or technical grounds for disputing the validity or novelty of the work to hamper its publication. We believe we have addressed your questions and comments and highlighted the novelty and contribution this work brings."}, "signatures": ["ICLR.cc/2020/Conference/Paper268/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper268/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Invariants through Soft Unification", "authors": ["Nuri Cingillioglu", "Alessandra Russo"], "authorids": ["nuri.cingillioglu13@imperial.ac.uk", "a.russo@imperial.ac.uk"], "keywords": ["representation learning", "neural networks", "unification"], "TL;DR": "End-to-end learning of invariant representations with variables across examples such as if someone went somewhere then they are there.", "abstract": "Human reasoning involves recognising common underlying principles across many examples by utilising variables. The by-products of such reasoning are invariants that capture patterns across examples such as \"if someone went somewhere then they are there\" without mentioning specific people or places. Humans learn what variables are and how to use them at a young age, and the question this paper addresses is whether machines can also learn and use variables solely from examples without requiring human pre-engineering. We propose Unification Networks that incorporate soft unification into neural networks to learn variables and by doing so lift examples into invariants that can then be used to solve a given task. We evaluate our approach on four datasets to demonstrate that learning invariants captures patterns in the data and can improve performance over baselines.", "pdf": "/pdf/8a6cd032fff43e4e9182a63e4448629f4b1867cf.pdf", "code": "https://drive.google.com/file/d/1Ema_awqOoOn-Xd4aTkkwOQz1r26QqZ4A/view?usp=sharing", "paperhash": "cingillioglu|learning_invariants_through_soft_unification", "original_pdf": "/attachment/9d506633727694103537ffb5c1539c4891ad09a4.pdf", "_bibtex": "@misc{\ncingillioglu2020learning,\ntitle={Learning Invariants through Soft Unification},\nauthor={Nuri Cingillioglu and Alessandra Russo},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xwA34KDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xwA34KDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper268/Authors", "ICLR.cc/2020/Conference/Paper268/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper268/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper268/Reviewers", "ICLR.cc/2020/Conference/Paper268/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper268/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper268/Authors|ICLR.cc/2020/Conference/Paper268/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173915, "tmdate": 1576860531665, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper268/Authors", "ICLR.cc/2020/Conference/Paper268/Reviewers", "ICLR.cc/2020/Conference/Paper268/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper268/-/Official_Comment"}}}, {"id": "SJx0jk0HiS", "original": null, "number": 2, "cdate": 1573408677622, "ddate": null, "tcdate": 1573408677622, "tmdate": 1573408677622, "tddate": null, "forum": "r1xwA34KDB", "replyto": "HygP8HETtH", "invitation": "ICLR.cc/2020/Conference/Paper268/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "Dear reviewer, thank you for your feedback and we are glad you have found the paper well written and structured. To answer your questions and comments:\n\n\u201cthe paper does not provide thorough comparison with other state-of-the-art methods on reasoning related tasks\u201d - If we are referring to the bAbI and the logical reasoning tasks as reasoning related tasks, we provide a detailed comparison of each task with all of the state-of-the-art models in Appendix D Table 7 and 8. We present similar (related to our memory network architecture) memory based architectures in the main body of the paper and the rest of the state-of-the-art models in the appendix due to space limitations.\n\n\u201cIn section 6, what\u2019s the rationale of setting $t$ - threshold differently for bAbI solely?\u201d - This question is answered at the beginning of Section 6, \u201cThe magnitude of this threshold seems to depend on the amount of regularisation, equation 5, and the number of training steps along with batch size all controlling how much $\\psi$ is pushed towards 0.\u201d Thus, after training is complete there will be a lower bound on $\\psi$ depending on those aspects which is different for bAbI from the other datasets.\n\n\u201chow is the sparsity regularization parameter $\\tau$ chosen optimally for a particular task?\u201d - It is not chosen optimally, we used 0.1 as a reasonable coefficient in recognising that $\\tau$ is an L1 regularisation applied to $\\psi$. We haven\u2019t performed hyper-parameter tuning.\n\n\u201cthe caveat for lack of experiments and comparison with other state-of-the-art methods\u201d - We disagree with this statement as we present 4 datasets, 3 different architectures, different experimental setups (strong vs weak, 1k vs 50 training examples), analysis of invariants, analysis of soft unification as well as comparison to existing state-of-the-architectures in Sections 4, 5 and 6 respectively with detailed results and further analysis in Appendix D.\n\nWe hope we have answered your questions individually and highlighted the novelty, the results of the experimental setup and comparison to the state-of-the-art models presented in this work."}, "signatures": ["ICLR.cc/2020/Conference/Paper268/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper268/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Invariants through Soft Unification", "authors": ["Nuri Cingillioglu", "Alessandra Russo"], "authorids": ["nuri.cingillioglu13@imperial.ac.uk", "a.russo@imperial.ac.uk"], "keywords": ["representation learning", "neural networks", "unification"], "TL;DR": "End-to-end learning of invariant representations with variables across examples such as if someone went somewhere then they are there.", "abstract": "Human reasoning involves recognising common underlying principles across many examples by utilising variables. The by-products of such reasoning are invariants that capture patterns across examples such as \"if someone went somewhere then they are there\" without mentioning specific people or places. Humans learn what variables are and how to use them at a young age, and the question this paper addresses is whether machines can also learn and use variables solely from examples without requiring human pre-engineering. We propose Unification Networks that incorporate soft unification into neural networks to learn variables and by doing so lift examples into invariants that can then be used to solve a given task. We evaluate our approach on four datasets to demonstrate that learning invariants captures patterns in the data and can improve performance over baselines.", "pdf": "/pdf/8a6cd032fff43e4e9182a63e4448629f4b1867cf.pdf", "code": "https://drive.google.com/file/d/1Ema_awqOoOn-Xd4aTkkwOQz1r26QqZ4A/view?usp=sharing", "paperhash": "cingillioglu|learning_invariants_through_soft_unification", "original_pdf": "/attachment/9d506633727694103537ffb5c1539c4891ad09a4.pdf", "_bibtex": "@misc{\ncingillioglu2020learning,\ntitle={Learning Invariants through Soft Unification},\nauthor={Nuri Cingillioglu and Alessandra Russo},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xwA34KDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xwA34KDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper268/Authors", "ICLR.cc/2020/Conference/Paper268/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper268/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper268/Reviewers", "ICLR.cc/2020/Conference/Paper268/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper268/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper268/Authors|ICLR.cc/2020/Conference/Paper268/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173915, "tmdate": 1576860531665, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper268/Authors", "ICLR.cc/2020/Conference/Paper268/Reviewers", "ICLR.cc/2020/Conference/Paper268/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper268/-/Official_Comment"}}}, {"id": "HylCdJCBor", "original": null, "number": 1, "cdate": 1573408629770, "ddate": null, "tcdate": 1573408629770, "tmdate": 1573408629770, "tddate": null, "forum": "r1xwA34KDB", "replyto": "rkgXIqLJqH", "invitation": "ICLR.cc/2020/Conference/Paper268/-/Official_Comment", "content": {"title": "Response to Review #4", "comment": "Dear reviewer, thank you for your detailed constructive feedback. We are happy to answer your comments:\n\n\u201cthese are very simple toy tasks\u201d - We must first understand how these architectures work and analyse them in a controlled environment in order to mitigate the black box effect they create. In order to analyse the invariants, it is crucial to work in a fixed setting where the data generating distribution is known for comparison. This learning task has never been attempted before and the paper demonstrates the validity of the idea and its effectiveness, enabling work towards addressing more complex tasks.\n\n\u201cModelling variables over the sequence of symbols here is, in a sense, the wrong object of study\u201d - Our approach is indifferent to the underlying structure of the task, demonstrated by the different datasets. There is no right or wrong task with respect to learning variables since the model does not make assumptions about the data. We present 3 different structures still using the same definitions from Section 2. The soft unification function \u201cg\u201d potentially learns different features with different structures, as you have guessed in the next comment below.\n\n\u201cMy guess is that in practice the bi-GRU model that produces embedding features of the symbols in sequence is implicitly representing head/tail positioning.\u201d - Your guess is correct! We state this in Section 2 as the unifying properties $\\phi_U$ that can be learned. In the example you\u2019ve provided, 1 4 3 1, the unifying features of 1s will be different due to the bi-GRU. This also gives great capacity to the network and the ability to unify the head of a sequence with the tail of another sequence (Appendix D).\n\n\u201cIf logical reasoning is being used to solve this question, surely the symbol 'before' must also be represented as a variable\u201d - We must be careful in projecting our understanding of natural language and logical reasoning to dictate what should and shouldn\u2019t be a variable. If this was an alien language with unknown symbols, we wouldn\u2019t be able to say a symbol should be a variable nor assume a certain logical reasoning is involved. Hence, only variations in the data can tell, as is the case in our approach, whether a symbol should be a variable. Since the model can optimise to use 1 variable where we might expect 2, Figure 5b, it might not follow the data generating distribution exactly but still solve the task by exploiting these commonalities. We discuss this in \u201cInterpretability versus Ability\u201d, Section 6.\n\n\u201c.. formed counter-factuals to probe the way the models are answering the questions\u201d - This is an interesting point we also make in our relevant work in Section 7. Our objective of learning invariants to some extent uses counter-factuals as unification changes the facts of a story. Furthermore, your question is not a counter-factual as it can be answered with \u201cunknown\u201d looking at the story facts. The question \u201cwhere would X:bill have been before Y:school should he have gone to the garden yesterday\u201d is a counter-factual as it yields an answer of garden against the facts presented in the story.\n\n\u201cInterpretability is crucial here because to claim that unification and reasoning by logical induction is being used to solve tasks ...\u201d - We don\u2019t claim this is logical unification, reasoning or induction. In fact, we refrain from using those terms \u201csince neither the invariant structure needs to be rule-like nor the variables carry logical semantics\u201d. The reason is because the \u201cg\u201d and the \u201cf\u201d are learned end-to-end and could learn elements of logical unification, reasoning or not; hence, it is inappropriate to assume or claim that it is any sort of logical induction.\n\n\u201cDoes this mean there are no co-referent symbols in the invariate?\u201d Yes, each symbol is considered unique and a potential variable independently. In bAbI co-reference task,11, \u201cHe\u201d etc. are unique symbols and treated equally. Detailed results, including task 11, are in Appendix D Table 6.\n\n\u201cdoes a CNN result in more sensible variable assignments  than the mlp on a flattened representation of the grid problem?\u201d - It is a good question. It depends on what we mean by sensible. If we refer to them following the data generating distribution then the answer is similar to asking whether an MLP or a CNN solves the task better. This is because the variables are learned with respect to an upstream \u201cf\u201d and that network provides the gradients for which symbols should be variables. In either case, we observe occasional \u201cinsensible\u201d variables (Appendix D Figure 9) in which the invariants can still solve the task.\n\n\u201cWhat is the strongly supervised case?\u201d - These experiments use the supporting facts provided in the bAbI dataset as done in literature around memory networks. This is mentioned in Section 5: \u201cand, in the strongly supervised cases, the negative log-likelihood for the context attentions are also added to the objective function.\u201d We do not supervise soft unification nor label correct tokens."}, "signatures": ["ICLR.cc/2020/Conference/Paper268/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper268/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Invariants through Soft Unification", "authors": ["Nuri Cingillioglu", "Alessandra Russo"], "authorids": ["nuri.cingillioglu13@imperial.ac.uk", "a.russo@imperial.ac.uk"], "keywords": ["representation learning", "neural networks", "unification"], "TL;DR": "End-to-end learning of invariant representations with variables across examples such as if someone went somewhere then they are there.", "abstract": "Human reasoning involves recognising common underlying principles across many examples by utilising variables. The by-products of such reasoning are invariants that capture patterns across examples such as \"if someone went somewhere then they are there\" without mentioning specific people or places. Humans learn what variables are and how to use them at a young age, and the question this paper addresses is whether machines can also learn and use variables solely from examples without requiring human pre-engineering. We propose Unification Networks that incorporate soft unification into neural networks to learn variables and by doing so lift examples into invariants that can then be used to solve a given task. We evaluate our approach on four datasets to demonstrate that learning invariants captures patterns in the data and can improve performance over baselines.", "pdf": "/pdf/8a6cd032fff43e4e9182a63e4448629f4b1867cf.pdf", "code": "https://drive.google.com/file/d/1Ema_awqOoOn-Xd4aTkkwOQz1r26QqZ4A/view?usp=sharing", "paperhash": "cingillioglu|learning_invariants_through_soft_unification", "original_pdf": "/attachment/9d506633727694103537ffb5c1539c4891ad09a4.pdf", "_bibtex": "@misc{\ncingillioglu2020learning,\ntitle={Learning Invariants through Soft Unification},\nauthor={Nuri Cingillioglu and Alessandra Russo},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xwA34KDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xwA34KDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper268/Authors", "ICLR.cc/2020/Conference/Paper268/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper268/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper268/Reviewers", "ICLR.cc/2020/Conference/Paper268/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper268/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper268/Authors|ICLR.cc/2020/Conference/Paper268/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173915, "tmdate": 1576860531665, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper268/Authors", "ICLR.cc/2020/Conference/Paper268/Reviewers", "ICLR.cc/2020/Conference/Paper268/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper268/-/Official_Comment"}}}, {"id": "BJxGVXNpKB", "original": null, "number": 1, "cdate": 1571795754252, "ddate": null, "tcdate": 1571795754252, "tmdate": 1572972617352, "tddate": null, "forum": "r1xwA34KDB", "replyto": "r1xwA34KDB", "invitation": "ICLR.cc/2020/Conference/Paper268/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper explores a very interesting idea: can a model learn what variables are and how to use them? Unfortunately, the paper doesn't seem quite ready: the model description was very hard to follow and it's not clear the approach has found a compelling use case.\n\nI read the paper carefully three times, and try as I might, I simply can't get my head around the entire architecture. The modeling section jumps straight into a series of definitions, without trying to build intuition or provide a worked example. There is an example in Figure 2, but it isn't really explained and I didn't find it helpful. Unification seems to be implemented as a form of attention (or self-attention) where the model can control the degree to which a symbol acts as variable. But the relationship between soft unification and attention isn't really spelled out -- what's the same, what's different? Ultimately it's not clear to me what the model is attending over during soft unification.\n\nThere are various other aspects of the paper that aren't clear:\n- strong vs. weak supervision\n- comparison models DMN and IMA are not introduced at all, and include no references\n- the logical reasoning experiment is not clearly described\n- there is only a cursory conclusion\n\nI am not sure the model has found a compelling use case. On bAbi with weak supervision, the model is worse than the comparison models. It only slightly beats out memory networks with strong supervision. For logical reasoning, it's not clear what it is compared against or if the comparison is fair. The clearest win over standard networks is on the simple synthetic experiments.\n\nFinally, the authors mention the paper has a cognitive science motivation, in that \"Humans learn what variables are and how to use then at a young age\" or that \"symbolic thought with variables is learned...\", taking a strong \"nurture\" stance on the origin of variables. But variables could very well be innate and simply early emerging. Any discussion of the origin of variables in the mind requires more nuance.\n\nI am excited about this research direction, and it could ultimately be a very nice contribution as the work matures. I don't think the paper is ready in its current form.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper268/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper268/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Invariants through Soft Unification", "authors": ["Nuri Cingillioglu", "Alessandra Russo"], "authorids": ["nuri.cingillioglu13@imperial.ac.uk", "a.russo@imperial.ac.uk"], "keywords": ["representation learning", "neural networks", "unification"], "TL;DR": "End-to-end learning of invariant representations with variables across examples such as if someone went somewhere then they are there.", "abstract": "Human reasoning involves recognising common underlying principles across many examples by utilising variables. The by-products of such reasoning are invariants that capture patterns across examples such as \"if someone went somewhere then they are there\" without mentioning specific people or places. Humans learn what variables are and how to use them at a young age, and the question this paper addresses is whether machines can also learn and use variables solely from examples without requiring human pre-engineering. We propose Unification Networks that incorporate soft unification into neural networks to learn variables and by doing so lift examples into invariants that can then be used to solve a given task. We evaluate our approach on four datasets to demonstrate that learning invariants captures patterns in the data and can improve performance over baselines.", "pdf": "/pdf/8a6cd032fff43e4e9182a63e4448629f4b1867cf.pdf", "code": "https://drive.google.com/file/d/1Ema_awqOoOn-Xd4aTkkwOQz1r26QqZ4A/view?usp=sharing", "paperhash": "cingillioglu|learning_invariants_through_soft_unification", "original_pdf": "/attachment/9d506633727694103537ffb5c1539c4891ad09a4.pdf", "_bibtex": "@misc{\ncingillioglu2020learning,\ntitle={Learning Invariants through Soft Unification},\nauthor={Nuri Cingillioglu and Alessandra Russo},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xwA34KDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1xwA34KDB", "replyto": "r1xwA34KDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper268/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper268/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575910637228, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper268/Reviewers"], "noninvitees": [], "tcdate": 1570237754601, "tmdate": 1575910637242, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper268/-/Official_Review"}}}, {"id": "HygP8HETtH", "original": null, "number": 2, "cdate": 1571796302554, "ddate": null, "tcdate": 1571796302554, "tmdate": 1572972617307, "tddate": null, "forum": "r1xwA34KDB", "replyto": "r1xwA34KDB", "invitation": "ICLR.cc/2020/Conference/Paper268/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents a novel approach for learning invariants that can capture underlying patterns in the tasks through Unification Networks. This effectively allows the machine to learn the notion of `variable`, which is a symbol that can take on different values. \n\nPros:\nThe authors evaluated and presented empirical results on four common benchmark datasets, showing superiority over plain baseline without unification. \nThey further performed analysis on the learned invariants, and verified the sensibility. \nThe paper overall is well written and structured.\n\nCons:\nDespite its superiority over plain baseline, the paper does not provide thorough comparison with other state-of-the-art methods on reasoning related tasks.\n\nSome of the technical details regarding the choice of hyperparameters are missing. For example:\nIn section 6, what\u2019s the rationale of setting $t$ differently for bAbI solely?\nIn Equation 5, how is the sparsity regularization parameter $\\tau$ chosen optimally for a particular task? A bit more discussion on these choices would be helpful. \n\nOverall, this paper presents a seemingly promising architecture capable of learning and using variables, with the caveat for lack of experiments and comparison with other state-of-the-art methods. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper268/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper268/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Invariants through Soft Unification", "authors": ["Nuri Cingillioglu", "Alessandra Russo"], "authorids": ["nuri.cingillioglu13@imperial.ac.uk", "a.russo@imperial.ac.uk"], "keywords": ["representation learning", "neural networks", "unification"], "TL;DR": "End-to-end learning of invariant representations with variables across examples such as if someone went somewhere then they are there.", "abstract": "Human reasoning involves recognising common underlying principles across many examples by utilising variables. The by-products of such reasoning are invariants that capture patterns across examples such as \"if someone went somewhere then they are there\" without mentioning specific people or places. Humans learn what variables are and how to use them at a young age, and the question this paper addresses is whether machines can also learn and use variables solely from examples without requiring human pre-engineering. We propose Unification Networks that incorporate soft unification into neural networks to learn variables and by doing so lift examples into invariants that can then be used to solve a given task. We evaluate our approach on four datasets to demonstrate that learning invariants captures patterns in the data and can improve performance over baselines.", "pdf": "/pdf/8a6cd032fff43e4e9182a63e4448629f4b1867cf.pdf", "code": "https://drive.google.com/file/d/1Ema_awqOoOn-Xd4aTkkwOQz1r26QqZ4A/view?usp=sharing", "paperhash": "cingillioglu|learning_invariants_through_soft_unification", "original_pdf": "/attachment/9d506633727694103537ffb5c1539c4891ad09a4.pdf", "_bibtex": "@misc{\ncingillioglu2020learning,\ntitle={Learning Invariants through Soft Unification},\nauthor={Nuri Cingillioglu and Alessandra Russo},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xwA34KDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1xwA34KDB", "replyto": "r1xwA34KDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper268/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper268/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575910637228, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper268/Reviewers"], "noninvitees": [], "tcdate": 1570237754601, "tmdate": 1575910637242, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper268/-/Official_Review"}}}, {"id": "rkgXIqLJqH", "original": null, "number": 3, "cdate": 1571936842833, "ddate": null, "tcdate": 1571936842833, "tmdate": 1572972617260, "tddate": null, "forum": "r1xwA34KDB", "replyto": "r1xwA34KDB", "invitation": "ICLR.cc/2020/Conference/Paper268/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors propose a neural network approach to variable unification and\nreasoning by example as a way to mimic the human ability to identify invariant\npatterns in examples and then apply them more generally in practice.\nThis general idea of identifying invariates and mapping new instances to\nthem is well motivated by the authors, citing work in philosophy of mind,\ncognitive science, and developmental psychology.\n\nThe authors go on to propose MLP, CNN and Memory Network models\nof unification for sequence, grid, and story reasoning tasks respectively.\nExperiments on the sequence and grid datasets demonstrate the data efficiency\nof this approach. MLP and CNN models with unification achieve near perfect\nperformance in fewer iterations (an order of magnitude fewer in the MLP case!)\n than their non-unification enabled counter parts.\nUnification enabled models also demonstrate high performance in a reduced\ntraining set setting (using only 50 training examples).\nWhile this is encouraging, these are very simple toy tasks.\n\nI also am in doubt as to whether the representation of these problems\ncauses some issues. In the sequence task, one question the models are\ntrying to solve is what symbol is the head or tail of the sequence.\nModeling variables over the sequence of symbols here is, in a sense, the\nwrong object of study. The position of the symbols would need to be\nrepresented, e.g.\n\na b c d\n1 4 3 1\n\nwhere I've represented positions as a-d, and the learned invariant about\nhead questions would be:\n\nX:a b c d\nY:1 4 3 1\n\nAs is, by mapping symbols and not positions to variables, one cannot,\nat the variable level distinguish between the two 1s in the sequence above.\nMy guess is that in practice the bi-GRU model that produces embedding\nfeatures of the symbols in sequence is implicitly representing head/tail\npositioning.\n\nSimilar arguments could be made about the grid example.\n\nI don't find the experiments/analysis on the bAbI dataset very convincing.\nFor instance, in the example given in Figure 4b (reproduced below)\nis shown as an example of\ntemporal reasoning, where a symbol Z is mapped to the\nword morning (a symbol distinguishing a time), and the question asked is\nwhere was Bill before school.\nIf logical reasoning is being used to solve this question, surely the\nsymbol 'before' must also be represented as a variable. Its possible that\nthe model is instead learning a trick about mutual exclusivity, i.e. that\nY:school is the only location symbol not mentioned in question but this\ncould fail as a general strategy.\n\n\nthis Z:morning X:bill went to the Y:school\nyesterday X:bill journeyed to the A:park\nwhere was X:bill before the Y:school\nA:park\n\nFigure 4b\n\nIt would make for a much more interesting paper if the authors took\nexamples such as these and formed counter-factuals to probe the way\nthe models are answering the questions. E.g., transforming the question\nin 4b to \"where was X:bill today\" or \"where was X:bill after school.\"\n\nBecause the authors use soft unification, interpretability is difficult\nto assess. Interpretability is crucial here because to claim that unification and reasoning by logical induction\nis being used to solve tasks, it becomes important to show how the neural networks\nmake their decisions. Given the instances of extra variables and one to many\nmappings on the bAbI dataset it seems very likely that the models are not\nsolving many tasks\nusing unification as it would be possible to learn to use the symbols directly\nto learn to answer. As such, I think these issues are not addressed in the\npaper sufficiently to warrant acceptance.\n\n\nMinor Notes\n\n- In definition 1, the definition of Variable is a little confusing because there are two different senses of the word in use. I understand them to be (1) Variable (X) in the logical template that is intended to be learned and used in problem solving, and\n(2) variable (x) in the neural network model that is a soft asignment of\nthe Variable to a default symbol s. It would be nice if this distinction could\nbe noted or made clearer.\n\n- In the definition 2, in the phrase \"is the invariant example such as a tokenized story\" it might be worth stating that the tokens are the symbols in S.\n\n\n- My understanding is that each unique symbol in the invariate is a potential\nvariable. Does this mean there are no co-referent symbols in the invariate?\nWould be helpful to state whether babi contains co-referent expressions\nand how these might affect the model.\n\n\n- It might be interesting to see how model architecture affects variable\nlearning. For example, does a CNN result in more sensible variable\nassignments  than the mlp on a flattened representation of the grid problem?\n\n\n- What is the strongly supervised case? These are token level annotations I think (at least for babi) but it might be good to specify in more detail what\nthey  are.\n\n- The figure and explanation of the UMN are not very clear. From the figure\nis does not seem that the variables interact with the memory at all. More\nspace could be devoted to this section.\n\nPossibly Relevant Related Work\n\nBrenden Lake. Compositional generalization through metasequence-to-sequence learning. NeurIPS 2019.\n\n\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper268/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper268/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Invariants through Soft Unification", "authors": ["Nuri Cingillioglu", "Alessandra Russo"], "authorids": ["nuri.cingillioglu13@imperial.ac.uk", "a.russo@imperial.ac.uk"], "keywords": ["representation learning", "neural networks", "unification"], "TL;DR": "End-to-end learning of invariant representations with variables across examples such as if someone went somewhere then they are there.", "abstract": "Human reasoning involves recognising common underlying principles across many examples by utilising variables. The by-products of such reasoning are invariants that capture patterns across examples such as \"if someone went somewhere then they are there\" without mentioning specific people or places. Humans learn what variables are and how to use them at a young age, and the question this paper addresses is whether machines can also learn and use variables solely from examples without requiring human pre-engineering. We propose Unification Networks that incorporate soft unification into neural networks to learn variables and by doing so lift examples into invariants that can then be used to solve a given task. We evaluate our approach on four datasets to demonstrate that learning invariants captures patterns in the data and can improve performance over baselines.", "pdf": "/pdf/8a6cd032fff43e4e9182a63e4448629f4b1867cf.pdf", "code": "https://drive.google.com/file/d/1Ema_awqOoOn-Xd4aTkkwOQz1r26QqZ4A/view?usp=sharing", "paperhash": "cingillioglu|learning_invariants_through_soft_unification", "original_pdf": "/attachment/9d506633727694103537ffb5c1539c4891ad09a4.pdf", "_bibtex": "@misc{\ncingillioglu2020learning,\ntitle={Learning Invariants through Soft Unification},\nauthor={Nuri Cingillioglu and Alessandra Russo},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xwA34KDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1xwA34KDB", "replyto": "r1xwA34KDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper268/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper268/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575910637228, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper268/Reviewers"], "noninvitees": [], "tcdate": 1570237754601, "tmdate": 1575910637242, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper268/-/Official_Review"}}}], "count": 8}