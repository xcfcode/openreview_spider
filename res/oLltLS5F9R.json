{"notes": [{"id": "oLltLS5F9R", "original": "ryBtSJgEQle", "number": 150, "cdate": 1601308025459, "ddate": null, "tcdate": 1601308025459, "tmdate": 1614985651451, "tddate": null, "forum": "oLltLS5F9R", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Learning Graph Normalization for Graph Neural Networks", "authorids": ["~Yihao_Chen1", "tangxint@gmail.com", "~Xianbiao_Qi2", "~Chun-Guang_Li3", "~Rong_Xiao3"], "authors": ["Yihao Chen", "Xin Tang", "Xianbiao Qi", "Chun-Guang Li", "Rong Xiao"], "keywords": ["Graph Neural Network", "Normalization", "Graph Normalization"], "abstract": "Graph Neural Networks (GNNs) have emerged as a useful paradigm to process graph-structured data. Usually, GNNs are stacked to multiple layers and the node representations in each layer are computed through propagating and aggre- gating the neighboring node features with respect to the graph. To effectively train a GNN with multiple layers, some normalization techniques are necessary. Though the existing normalization techniques have been shown to accelerate train- ing GNNs, the structure information on the graph is ignored yet. In this paper, we propose two graph-aware normalization methods to effectively train GNNs. Then, by taking into account that normalization methods for GNNs are highly task-relevant and it is hard to know in advance which normalization method is the best, we propose to learn attentive graph normalization by optimizing a weighted combination of multiple graph normalization methods at different scales. By op- timizing the combination weights, we can automatically select the best or the best combination of multiple normalization methods for a specific task. We con- duct extensive experiments on benchmark datasets for different tasks and confirm that the graph-aware normalization methods lead to promising results and that the learned weights suggest the more appropriate normalization methods for specific task", "one-sentence_summary": "We propose two graph-aware normalization methods for training GNNs and also propose to learn graph normalization which optimizes a weighted combination of multiple normalization methods. ", "pdf": "/pdf/cfb42459daee6213f3645beebba59c05e0a994f5.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|learning_graph_normalization_for_graph_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XivziPUt41", "_bibtex": "@misc{\nchen2021learning,\ntitle={Learning Graph Normalization for Graph Neural Networks},\nauthor={Yihao Chen and Xin Tang and Xianbiao Qi and Chun-Guang Li and Rong Xiao},\nyear={2021},\nurl={https://openreview.net/forum?id=oLltLS5F9R}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "MmkqSEbt0Jb", "original": null, "number": 1, "cdate": 1610040511601, "ddate": null, "tcdate": 1610040511601, "tmdate": 1610474119474, "tddate": null, "forum": "oLltLS5F9R", "replyto": "oLltLS5F9R", "invitation": "ICLR.cc/2021/Conference/Paper150/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Four knowledgeable referees have indicated reject mainly because of limited motivation [R1,R3,R4], limited insights on the proposed approach [R1,R2,R3,R4], and inconclusive results [R1,R2,R3,R4]. The claims of the paper could have been strengthened by e.g. discussing currently missing experimental details [R1], performing statistical significance tests [R2,R4], and including comparisons to baselines/previously introduced normalization strategies [R2,R3]. Unfortunately, there was no rebuttal. The paper is therefore rejected."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Graph Normalization for Graph Neural Networks", "authorids": ["~Yihao_Chen1", "tangxint@gmail.com", "~Xianbiao_Qi2", "~Chun-Guang_Li3", "~Rong_Xiao3"], "authors": ["Yihao Chen", "Xin Tang", "Xianbiao Qi", "Chun-Guang Li", "Rong Xiao"], "keywords": ["Graph Neural Network", "Normalization", "Graph Normalization"], "abstract": "Graph Neural Networks (GNNs) have emerged as a useful paradigm to process graph-structured data. Usually, GNNs are stacked to multiple layers and the node representations in each layer are computed through propagating and aggre- gating the neighboring node features with respect to the graph. To effectively train a GNN with multiple layers, some normalization techniques are necessary. Though the existing normalization techniques have been shown to accelerate train- ing GNNs, the structure information on the graph is ignored yet. In this paper, we propose two graph-aware normalization methods to effectively train GNNs. Then, by taking into account that normalization methods for GNNs are highly task-relevant and it is hard to know in advance which normalization method is the best, we propose to learn attentive graph normalization by optimizing a weighted combination of multiple graph normalization methods at different scales. By op- timizing the combination weights, we can automatically select the best or the best combination of multiple normalization methods for a specific task. We con- duct extensive experiments on benchmark datasets for different tasks and confirm that the graph-aware normalization methods lead to promising results and that the learned weights suggest the more appropriate normalization methods for specific task", "one-sentence_summary": "We propose two graph-aware normalization methods for training GNNs and also propose to learn graph normalization which optimizes a weighted combination of multiple normalization methods. ", "pdf": "/pdf/cfb42459daee6213f3645beebba59c05e0a994f5.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|learning_graph_normalization_for_graph_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XivziPUt41", "_bibtex": "@misc{\nchen2021learning,\ntitle={Learning Graph Normalization for Graph Neural Networks},\nauthor={Yihao Chen and Xin Tang and Xianbiao Qi and Chun-Guang Li and Rong Xiao},\nyear={2021},\nurl={https://openreview.net/forum?id=oLltLS5F9R}\n}"}, "tags": [], "invitation": {"reply": {"forum": "oLltLS5F9R", "replyto": "oLltLS5F9R", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040511588, "tmdate": 1610474119459, "id": "ICLR.cc/2021/Conference/Paper150/-/Decision"}}}, {"id": "qFbntTcCme6", "original": null, "number": 2, "cdate": 1603529727492, "ddate": null, "tcdate": 1603529727492, "tmdate": 1605023932676, "tddate": null, "forum": "oLltLS5F9R", "replyto": "oLltLS5F9R", "invitation": "ICLR.cc/2021/Conference/Paper150/-/Official_Review", "content": {"title": "Not enough justification and explanation for the proposed normalization techniques", "review": "===== Summary =====\n\nThis paper proposes and evaluates different normalization techniques for graph neural networks. Also, the authors argue that the best normalization technique is task dependent, so they propose to use a weighted average of different normalizations that is learned during training, called AGN. In the paper they propose 4 different normalizations some of which are structure-dependent, and compare the performance of GCN, GAT and GatedGCN with and without these normalizations, and the learned combination of all of them.\n\n===== Pros =====\n* The paper explores normalization for Graph Neural Networks, which is a relevant topic.\n* In the experiments section the authors follow the benchmark proposed by Dwivedi et al. 2020, which uses standard tasks so that comparison with other methods is easier.\n\n===== Cons =====\n* The main concern about this paper is that the authors provide no justification about why the proposed normalizations are needed, or an explanation about why any of the proposed normalization techniques should perform better than the other techniques presented in this or other papers. Moreover, the authors only evaluate their proposed normalization methods without experimenting with other methods such as PairNorm or MsgNorm [2] which is also not cited.\n* Node-wise normalization is normalizing the feature vector of each node in each graph independently of all the other nodes in the same graph, therefore it is not equivalent to Layer Normalization as the authors say, because Layer Normalization would normalize all nodes in the same graph together.\n* Additionally, the Node-wise normalization has already been proposed as NodeNorm in [1]\n* The authors claim that adjacency-wise normalization and graph-wise normalization are both structure aware. While that is true for the former, since it normalizes nodes according to its neighbours, it isn\u2019t true for the graph-wise normalization. The reason is that as explained by the authors in the beginning of section 2 each sample in the batch is a single graph, so normalizing across the graph is the same as normalizing across the whole sample, which doesn\u2019t use the graph structure. It looks like this case is equivalent to Layer Normalization.\n\n==== Questions =====\n\n* I would like the authors to explain the motivation for their proposed normalization techniques beyond the empirical results. Why should they improve the vanilla GCN, GAT and GatedGCN models?\n* Specifically since it is a novel normalization technique presented in this paper, the authors should explain what is the motivation for the adjacency-wise normalization. Making an analogy to images, the proposed normalization would be the same as normalizing one unit in a convolutional layer by the value of its direct neighbours. Why is that useful for graphs?\n\n===== Minor comments =====\n\nSome sections of the paper are not very well written. For example the following sentence \u201cOn other hand, while multiple normalization methods are available for training GNNs and it is still hard to know in advance which normalization method is the most suitable to a specific task.\u201d is not very clear.\n\nIt isn\u2019t clear if AGN stands for \u201cAutomatic Graph Normalization\u201d or \u201cAttentive Graph Normalization\u201d as both are used across the paper.\n\n===== Reasons for score =====\n\nBecause of the lack of motivation and explanation of the proposed normalization methods I vote for rejection. In my opinion the paper lacks justification and explanations of the proposed methods, and it isn\u2019t clear why they are better than other normalization approaches.\n\n===== References =====\n\n[1] Zhou, K., Dong, Y., Lee, W. S., Hooi, B., Xu, H., & Feng, J. (2020). Effective training strategies for deep graph neural networks. arXiv preprint arXiv:2006.07107.\n[2] Li, G., Xiong, C., Thabet, A., & Ghanem, B. (2020). Deepergcn: All you need to train deeper gcns. arXiv preprint arXiv:2006.07739.\n\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper150/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper150/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Graph Normalization for Graph Neural Networks", "authorids": ["~Yihao_Chen1", "tangxint@gmail.com", "~Xianbiao_Qi2", "~Chun-Guang_Li3", "~Rong_Xiao3"], "authors": ["Yihao Chen", "Xin Tang", "Xianbiao Qi", "Chun-Guang Li", "Rong Xiao"], "keywords": ["Graph Neural Network", "Normalization", "Graph Normalization"], "abstract": "Graph Neural Networks (GNNs) have emerged as a useful paradigm to process graph-structured data. Usually, GNNs are stacked to multiple layers and the node representations in each layer are computed through propagating and aggre- gating the neighboring node features with respect to the graph. To effectively train a GNN with multiple layers, some normalization techniques are necessary. Though the existing normalization techniques have been shown to accelerate train- ing GNNs, the structure information on the graph is ignored yet. In this paper, we propose two graph-aware normalization methods to effectively train GNNs. Then, by taking into account that normalization methods for GNNs are highly task-relevant and it is hard to know in advance which normalization method is the best, we propose to learn attentive graph normalization by optimizing a weighted combination of multiple graph normalization methods at different scales. By op- timizing the combination weights, we can automatically select the best or the best combination of multiple normalization methods for a specific task. We con- duct extensive experiments on benchmark datasets for different tasks and confirm that the graph-aware normalization methods lead to promising results and that the learned weights suggest the more appropriate normalization methods for specific task", "one-sentence_summary": "We propose two graph-aware normalization methods for training GNNs and also propose to learn graph normalization which optimizes a weighted combination of multiple normalization methods. ", "pdf": "/pdf/cfb42459daee6213f3645beebba59c05e0a994f5.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|learning_graph_normalization_for_graph_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XivziPUt41", "_bibtex": "@misc{\nchen2021learning,\ntitle={Learning Graph Normalization for Graph Neural Networks},\nauthor={Yihao Chen and Xin Tang and Xianbiao Qi and Chun-Guang Li and Rong Xiao},\nyear={2021},\nurl={https://openreview.net/forum?id=oLltLS5F9R}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "oLltLS5F9R", "replyto": "oLltLS5F9R", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper150/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538069168, "tmdate": 1606915803395, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper150/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper150/-/Official_Review"}}}, {"id": "xdZmEKlllYc", "original": null, "number": 3, "cdate": 1603892461043, "ddate": null, "tcdate": 1603892461043, "tmdate": 1605023932580, "tddate": null, "forum": "oLltLS5F9R", "replyto": "oLltLS5F9R", "invitation": "ICLR.cc/2021/Conference/Paper150/-/Official_Review", "content": {"title": "Attention-based integration of different batch/layer normalization approaches in graph neural networks with an articulated empirical benchmarking", "review": "The paper provides a unified view over different layer/batch normalization mechanisms for graph neural networks and formalizes an attention-driven approach to adaptively integrate the different forms of normalizations. An extensive empirical analysis is provided, spanning graph classification, link prediction and node classification tasks.\n\nOn the positive side, the paper is very well written and organised. Extremely clear while remaining concise and focused on the point. The idea of integrating several normalization mechanisms through an attention layer makes perfect sense. Also, I appreciate the attempt to provide an articulated empirical analysis spanning graphs and tasks of different nature.\n\nOn the negative side, the first issue is that of limited novelty. The original contributions of the paper seem to be: (i) graph-wise normalization and (ii) the attention-based integration mechanism. The former contribution seems to provide some form of empirical advantage (more on this later) but it is quite derivative and does not seem to provide any substantial novel insight on the quality of the learned representations nor on the representational power. The second contribution (AGN) has some additional novelty, but still marginal, as it is a straightforward application of standard attention. More importantly, the claim of AGN being an effective method to integrate contributions from different normalization mechanism is not supported by the empirical results.\n\nWhich brings me to the second key issue in the paper: the empirical results do not seem to strongly support the claims in the paper. First, when considering the predictive performance, one can see that the radically different normalization mechanisms have different performances (on the same dataset) only for   GCN and GAT. When considering GatedGCN (for the same number of layers), the differences between the attention mechanisms are minor. Hence it seems to suggest that the neural architecture is far more impacting than the choice of the attention mechanism. In this respect, the choice of GatedGCN to show the attention weight results in Figure 2 seems somehow driven to confirm the hypothesised \u201cstability\u201d of weight distributions. I would have liked to see the same analysis performed on the rather less stable GCN and GAT.\n\nAdditional issues as concerns the empirical analysis are:\n-\tOnly training/test results are reported: where is the validation set? If no validation set is used (I hope not), how are model selection and training convergence choices taken in a robust way? Figure 4 and 5 show that different methods are stopped at different epochs: so how is this choice taken without looking into the test?\n-\tGiven the number of configurations and datasets considered it is difficult to gain a high-level insight into which approach is truly performing best. The paper should include some sort of summarised statistics, like the average rank of each approach across the different tasks. This way one can also run some tests of statistical significance to tell which ranks are statistically different. \n-\tIt is unclear to me why GN_a fails with out of memory on COLLAB while other methods don\u2019t. Where is the added memory complexity of GN_a that I am missing?\n\nI have one final concern as regards the definition of adjacency-wise normalization. Why is it using a feature-wise statistic (i.e. summing on the embedding features) instead of computing the actual first order (and second order) statistics w.r.t to the 1-hop neighbourhood of a node? While feature-wise statistics seems the only choice for node-wise normalization, I do not see it as a necessary case for the adjacency based one. There should be more discussion on this choice in the paper.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper150/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper150/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Graph Normalization for Graph Neural Networks", "authorids": ["~Yihao_Chen1", "tangxint@gmail.com", "~Xianbiao_Qi2", "~Chun-Guang_Li3", "~Rong_Xiao3"], "authors": ["Yihao Chen", "Xin Tang", "Xianbiao Qi", "Chun-Guang Li", "Rong Xiao"], "keywords": ["Graph Neural Network", "Normalization", "Graph Normalization"], "abstract": "Graph Neural Networks (GNNs) have emerged as a useful paradigm to process graph-structured data. Usually, GNNs are stacked to multiple layers and the node representations in each layer are computed through propagating and aggre- gating the neighboring node features with respect to the graph. To effectively train a GNN with multiple layers, some normalization techniques are necessary. Though the existing normalization techniques have been shown to accelerate train- ing GNNs, the structure information on the graph is ignored yet. In this paper, we propose two graph-aware normalization methods to effectively train GNNs. Then, by taking into account that normalization methods for GNNs are highly task-relevant and it is hard to know in advance which normalization method is the best, we propose to learn attentive graph normalization by optimizing a weighted combination of multiple graph normalization methods at different scales. By op- timizing the combination weights, we can automatically select the best or the best combination of multiple normalization methods for a specific task. We con- duct extensive experiments on benchmark datasets for different tasks and confirm that the graph-aware normalization methods lead to promising results and that the learned weights suggest the more appropriate normalization methods for specific task", "one-sentence_summary": "We propose two graph-aware normalization methods for training GNNs and also propose to learn graph normalization which optimizes a weighted combination of multiple normalization methods. ", "pdf": "/pdf/cfb42459daee6213f3645beebba59c05e0a994f5.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|learning_graph_normalization_for_graph_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XivziPUt41", "_bibtex": "@misc{\nchen2021learning,\ntitle={Learning Graph Normalization for Graph Neural Networks},\nauthor={Yihao Chen and Xin Tang and Xianbiao Qi and Chun-Guang Li and Rong Xiao},\nyear={2021},\nurl={https://openreview.net/forum?id=oLltLS5F9R}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "oLltLS5F9R", "replyto": "oLltLS5F9R", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper150/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538069168, "tmdate": 1606915803395, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper150/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper150/-/Official_Review"}}}, {"id": "kV5uTen_LWT", "original": null, "number": 4, "cdate": 1604010962187, "ddate": null, "tcdate": 1604010962187, "tmdate": 1605023932493, "tddate": null, "forum": "oLltLS5F9R", "replyto": "oLltLS5F9R", "invitation": "ICLR.cc/2021/Conference/Paper150/-/Official_Review", "content": {"title": "Good observation, extensive experiments but results are not conclusive ", "review": "\n\nSummary:\n\n\tThe authors propose two new graph-aware normalization techniques for training Graph Neural Networks, viz., (I) adjacency normalization (GN_a), and (ii) graph normalization (GN_g) and study them along with existing (iii) Node-wise (Layer Norm) and (iv) Batch normalization. Motivated by their experimental observation that different tasks work well with different normalization techniques, they propose an attention component that learns to weigh and combine the above four normalizations to best suit the end task. \n\n\u2014\u2014\nPros:\n\n        - Paper is well written\n        - Results reported on multiple datasets and tasks\n\t- The proposed Graph level normalization seems useful for node classification tasks over multiple graphs.     \n\u2014\u2014\nConcerns:\n\nMajor:\n\n\t- The paper lacks a clear model wise analysis of the usefulness of different normalization techniques\n\t- Though the paper mentions that the importance of normalization techniques varies with tasks, there seems to be no consistent model wise improvement under each task. A precise summary would help understand the pattern in the results.  \n\t- Need additional results on real-world datasets - Open Graph Benchmark, standard citation networks, etc.,.\n\t- Only the datasets in the Graph Classification/Regression task reported in Table 4 have all real-world datasets. Either of the existing Layer norm or Batch norm technique is the best or second-best normalization technique across all models on all three datasets. \n\t- It is not clear whether the difference with the proposed variants is statistically significant. Thus, a significance test on the same for all results would be helpful in understanding the usefulness of the proposed variants.\n\t- A GNN baseline with existing Layer norm and Batch norm combined is missing.\n\t- It not clear why the AGN model with GN_a didn't scale for the COLAB dataset \n\t- As the usefulness of GN_a and GN_g is not clear from the results, the need for the AGN model could not be appreciated. Albeit, the AGN model's importance is limited as there is no demanding need to learn the importance of a lot of components. Since AGN is not clearly the winner in most cases and as the results don't significantly improve with AGN, it might be better to test the different normalization techniques on a validation set and choose the one that works. Note that if the usefulness of GN_a and GN_g is clearly established, then the need for an attentive model is straightforward.\n\t\nDetailed concerns: \n\n\t(i) Node classification results are limited to datasets with multiple graphs. \n\tNo results on standard single graph benchmarks like Citation, Co-Purchase networks, Social networks, etc.. \n\tWould the proposed adjacency-based normalization do better the standard Layer norm and Batch norm?\n\t\n\t(ii) For node classification on multiple graphs, if the batch size is restricted to nodes from only one graph, how useful is adjacency normalization compared to layer norm and batch norm?\n\n\t(iii) Adjacency-Normalization (GN_a) performance is poor in the real dataset \n\t\t- From Table 2 on the SROIE dataset, the performance of GNN with GN_a drops ~8% points compared to the GNN with no Norm.\n\n\t(iv) Even on the artificial datasets, the performance gain with GN_a is not consistent \n\t\t- The GN_a + GCN model with 16 layers sees a drop of ~6% points \n\n\t(v) The baseline numbers are different than what is reported in Dwivedi et al. \n\t\t- 16 layers GCN+BN performance is lower than what is reported in Dwivedi et al. \n\t\t- While there is performance gain on \n\t", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper150/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper150/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Graph Normalization for Graph Neural Networks", "authorids": ["~Yihao_Chen1", "tangxint@gmail.com", "~Xianbiao_Qi2", "~Chun-Guang_Li3", "~Rong_Xiao3"], "authors": ["Yihao Chen", "Xin Tang", "Xianbiao Qi", "Chun-Guang Li", "Rong Xiao"], "keywords": ["Graph Neural Network", "Normalization", "Graph Normalization"], "abstract": "Graph Neural Networks (GNNs) have emerged as a useful paradigm to process graph-structured data. Usually, GNNs are stacked to multiple layers and the node representations in each layer are computed through propagating and aggre- gating the neighboring node features with respect to the graph. To effectively train a GNN with multiple layers, some normalization techniques are necessary. Though the existing normalization techniques have been shown to accelerate train- ing GNNs, the structure information on the graph is ignored yet. In this paper, we propose two graph-aware normalization methods to effectively train GNNs. Then, by taking into account that normalization methods for GNNs are highly task-relevant and it is hard to know in advance which normalization method is the best, we propose to learn attentive graph normalization by optimizing a weighted combination of multiple graph normalization methods at different scales. By op- timizing the combination weights, we can automatically select the best or the best combination of multiple normalization methods for a specific task. We con- duct extensive experiments on benchmark datasets for different tasks and confirm that the graph-aware normalization methods lead to promising results and that the learned weights suggest the more appropriate normalization methods for specific task", "one-sentence_summary": "We propose two graph-aware normalization methods for training GNNs and also propose to learn graph normalization which optimizes a weighted combination of multiple normalization methods. ", "pdf": "/pdf/cfb42459daee6213f3645beebba59c05e0a994f5.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|learning_graph_normalization_for_graph_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XivziPUt41", "_bibtex": "@misc{\nchen2021learning,\ntitle={Learning Graph Normalization for Graph Neural Networks},\nauthor={Yihao Chen and Xin Tang and Xianbiao Qi and Chun-Guang Li and Rong Xiao},\nyear={2021},\nurl={https://openreview.net/forum?id=oLltLS5F9R}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "oLltLS5F9R", "replyto": "oLltLS5F9R", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper150/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538069168, "tmdate": 1606915803395, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper150/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper150/-/Official_Review"}}}, {"id": "Lgf5rviiXoA", "original": null, "number": 1, "cdate": 1603068092886, "ddate": null, "tcdate": 1603068092886, "tmdate": 1605023932420, "tddate": null, "forum": "oLltLS5F9R", "replyto": "oLltLS5F9R", "invitation": "ICLR.cc/2021/Conference/Paper150/-/Official_Review", "content": {"title": "Lack of insights", "review": "Summary and Strength: The authors propose two graph-wise normalization ways for graph neural network based applications. Combined with other two normalization strategies, a combination framework is proposed for weight learning of four normalizations. The experimental results  demonstrate the performance improvements of the proposed framework.\n\nConcerns:\n1. The novelty of graph-wise normalization is too thin. More insights are needed. For example, the authors might want to deliver why the normalization can bring the performance improvement, and the advantages and disadvantages of different normalization ways.\n2. The notations seem a little messy. It might be better if the authors use superscripts in Section 2.\n3. Combining with four different normalizations in Eq. (11), higher performance is expected due to more parameters. Without insights, the engineering work of this paper hardly inspires the readers. \n4. Some time complexities should be discussed.\n5. Significant tests are suggested to verify the gained improvements.\n6. How to interpret Figure 2?\n\nThis paper proposed different normalization ways for graph neural networks, unfortunately fails to answer the key questions that which is the best or default one. Instead, the authors combine them together with a weighted strategy, which lowers the technical contribution. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper150/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper150/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Graph Normalization for Graph Neural Networks", "authorids": ["~Yihao_Chen1", "tangxint@gmail.com", "~Xianbiao_Qi2", "~Chun-Guang_Li3", "~Rong_Xiao3"], "authors": ["Yihao Chen", "Xin Tang", "Xianbiao Qi", "Chun-Guang Li", "Rong Xiao"], "keywords": ["Graph Neural Network", "Normalization", "Graph Normalization"], "abstract": "Graph Neural Networks (GNNs) have emerged as a useful paradigm to process graph-structured data. Usually, GNNs are stacked to multiple layers and the node representations in each layer are computed through propagating and aggre- gating the neighboring node features with respect to the graph. To effectively train a GNN with multiple layers, some normalization techniques are necessary. Though the existing normalization techniques have been shown to accelerate train- ing GNNs, the structure information on the graph is ignored yet. In this paper, we propose two graph-aware normalization methods to effectively train GNNs. Then, by taking into account that normalization methods for GNNs are highly task-relevant and it is hard to know in advance which normalization method is the best, we propose to learn attentive graph normalization by optimizing a weighted combination of multiple graph normalization methods at different scales. By op- timizing the combination weights, we can automatically select the best or the best combination of multiple normalization methods for a specific task. We con- duct extensive experiments on benchmark datasets for different tasks and confirm that the graph-aware normalization methods lead to promising results and that the learned weights suggest the more appropriate normalization methods for specific task", "one-sentence_summary": "We propose two graph-aware normalization methods for training GNNs and also propose to learn graph normalization which optimizes a weighted combination of multiple normalization methods. ", "pdf": "/pdf/cfb42459daee6213f3645beebba59c05e0a994f5.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|learning_graph_normalization_for_graph_neural_networks", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XivziPUt41", "_bibtex": "@misc{\nchen2021learning,\ntitle={Learning Graph Normalization for Graph Neural Networks},\nauthor={Yihao Chen and Xin Tang and Xianbiao Qi and Chun-Guang Li and Rong Xiao},\nyear={2021},\nurl={https://openreview.net/forum?id=oLltLS5F9R}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "oLltLS5F9R", "replyto": "oLltLS5F9R", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper150/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538069168, "tmdate": 1606915803395, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper150/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper150/-/Official_Review"}}}], "count": 6}