{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521573611874, "tcdate": 1521573611874, "number": 290, "cdate": 1521573611532, "id": "HyExJ1JqM", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "rkqJIc5UM", "replyto": "rkqJIc5UM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "This paper was invited to the workshop track based on reviews at the main conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Organize Knowledge with N-Gram Machines", "abstract": "Deep neural networks (DNNs) had great success on NLP tasks such as language modeling, machine translation and certain question answering (QA) tasks. However, the success is limited at more knowledge intensive tasks such as QA from a big corpus. Existing end-to-end deep QA models (Miller et al., 2016; Weston et al., 2014) need to read the entire text after observing the question, and therefore their complexity in responding a question is linear in the text size. This is prohibitive for practical tasks such as QA from Wikipedia, a novel, or the Web. We propose to solve this scalability issue by using symbolic meaning representations, which can be indexed and retrieved efficiently with complexity that is independent of the text size. More specifically, we use sequence-to-sequence models to encode knowledge symbolically and generate programs to answer questions from the encoded knowledge. We apply our approach, called the N-Gram Machine (NGM), to the bAbI tasks (Weston et al., 2015) and a special version of them (\u201clife-long bAbI\u201d) which has stories of up to 10 million sentences. Our experiments show that NGM can successfully solve both of these tasks accurately and efficiently. Unlike fully differentiable memory models, NGM\u2019s time complexity and answering quality are not affected by the story length. The whole system of NGM is trained end-to-end with REINFORCE (Williams, 1992). To avoid high variance in gradient estimation, which is typical in discrete latent variable models, we use beam search instead of sampling. To tackle the exponentially large search space, we use a stabilized auto-encoding objective and a structure tweak procedure to iteratively reduce and refine the search space.", "pdf": "/pdf/0e1a752c82e2b1c05cd75b96ed456d8ad95f83b9.pdf", "TL;DR": "We propose a framework that learns to encode knowledge symbolically and generate programs to reason about the encoded knowledge.", "paperhash": "yang|learning_to_organize_knowledge_with_ngram_machines", "_bibtex": "@misc{\nyang2018learning,\ntitle={{LEARNING} {TO} {ORGANIZE} {KNOWLEDGE} {WITH} N-{GRAM} {MACHINES}},\nauthor={Fan Yang and Jiazhong Nie and William W. Cohen and Ni Lao},\nyear={2018},\nurl={https://openreview.net/forum?id=By3v9k-RZ},\n}", "authors": ["Fan Yang", "Jiazhong Nie", "William W. Cohen", "Ni Lao"], "keywords": ["knowledge management", "information extraction", "structure auto-encoding"], "authorids": ["fanyang1@cs.cmu.edu", "niejiazhong@google.com", "wcohen@cs.cmu.edu", "ni.lao@saymosaic.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1518730174987, "tcdate": 1518147041977, "number": 38, "cdate": 1518147041977, "id": "rkqJIc5UM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "rkqJIc5UM", "original": "By3v9k-RZ", "signatures": ["~Fan_Yang1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Learning to Organize Knowledge with N-Gram Machines", "abstract": "Deep neural networks (DNNs) had great success on NLP tasks such as language modeling, machine translation and certain question answering (QA) tasks. However, the success is limited at more knowledge intensive tasks such as QA from a big corpus. Existing end-to-end deep QA models (Miller et al., 2016; Weston et al., 2014) need to read the entire text after observing the question, and therefore their complexity in responding a question is linear in the text size. This is prohibitive for practical tasks such as QA from Wikipedia, a novel, or the Web. We propose to solve this scalability issue by using symbolic meaning representations, which can be indexed and retrieved efficiently with complexity that is independent of the text size. More specifically, we use sequence-to-sequence models to encode knowledge symbolically and generate programs to answer questions from the encoded knowledge. We apply our approach, called the N-Gram Machine (NGM), to the bAbI tasks (Weston et al., 2015) and a special version of them (\u201clife-long bAbI\u201d) which has stories of up to 10 million sentences. Our experiments show that NGM can successfully solve both of these tasks accurately and efficiently. Unlike fully differentiable memory models, NGM\u2019s time complexity and answering quality are not affected by the story length. The whole system of NGM is trained end-to-end with REINFORCE (Williams, 1992). To avoid high variance in gradient estimation, which is typical in discrete latent variable models, we use beam search instead of sampling. To tackle the exponentially large search space, we use a stabilized auto-encoding objective and a structure tweak procedure to iteratively reduce and refine the search space.", "pdf": "/pdf/0e1a752c82e2b1c05cd75b96ed456d8ad95f83b9.pdf", "TL;DR": "We propose a framework that learns to encode knowledge symbolically and generate programs to reason about the encoded knowledge.", "paperhash": "yang|learning_to_organize_knowledge_with_ngram_machines", "_bibtex": "@misc{\nyang2018learning,\ntitle={{LEARNING} {TO} {ORGANIZE} {KNOWLEDGE} {WITH} N-{GRAM} {MACHINES}},\nauthor={Fan Yang and Jiazhong Nie and William W. Cohen and Ni Lao},\nyear={2018},\nurl={https://openreview.net/forum?id=By3v9k-RZ},\n}", "authors": ["Fan Yang", "Jiazhong Nie", "William W. Cohen", "Ni Lao"], "keywords": ["knowledge management", "information extraction", "structure auto-encoding"], "authorids": ["fanyang1@cs.cmu.edu", "niejiazhong@google.com", "wcohen@cs.cmu.edu", "ni.lao@saymosaic.com"]}, "nonreaders": [], "details": {"replyCount": 1, "writable": false, "overwriting": [], "revisions": false, "tags": [], "original": {"tddate": null, "ddate": null, "tmdate": 1518730174987, "tcdate": 1509124708264, "number": 510, "cdate": 1518730174976, "id": "By3v9k-RZ", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "By3v9k-RZ", "original": "B1nD9k-AZ", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "LEARNING TO ORGANIZE KNOWLEDGE WITH N-GRAM MACHINES", "abstract": "Deep neural networks (DNNs) had great success on NLP tasks such as language modeling, machine translation and certain question answering (QA) tasks. However, the success is limited at more knowledge intensive tasks such as QA from a big corpus. Existing end-to-end deep QA models (Miller et al., 2016; Weston et al., 2014) need to read the entire text after observing the question, and therefore their complexity in responding a question is linear in the text size. This is prohibitive for practical tasks such as QA from Wikipedia, a novel, or the Web. We propose to solve this scalability issue by using symbolic meaning representations, which can be indexed and retrieved efficiently with complexity that is independent of the text size. More specifically, we use sequence-to-sequence models to encode knowledge symbolically and generate programs to answer questions from the encoded knowledge. We apply our approach, called the N-Gram Machine (NGM), to the bAbI tasks (Weston et al., 2015) and a special version of them (\u201clife-long bAbI\u201d) which has stories of up to 10 million sentences. Our experiments show that NGM can successfully solve both of these tasks accurately and efficiently. Unlike fully differentiable memory models, NGM\u2019s time complexity and answering quality are not affected by the story length. The whole system of NGM is trained end-to-end with REINFORCE (Williams, 1992). To avoid high variance in gradient estimation, which is typical in discrete latent variable models, we use beam search instead of sampling. To tackle the exponentially large search space, we use a stabilized auto-encoding objective and a structure tweak procedure to iteratively reduce and refine the search space.\n", "pdf": "/pdf/7f9f452c0ff0c4689f378e2a767e89769fa02a0f.pdf", "TL;DR": "We propose a framework that learns to encode knowledge symbolically and generate programs to reason about the encoded knowledge.", "paperhash": "yang|learning_to_organize_knowledge_with_ngram_machines", "_bibtex": "@misc{\nyang2018learning,\ntitle={{LEARNING} {TO} {ORGANIZE} {KNOWLEDGE} {WITH} N-{GRAM} {MACHINES}},\nauthor={Fan Yang and Jiazhong Nie and William W. Cohen and Ni Lao},\nyear={2018},\nurl={https://openreview.net/forum?id=By3v9k-RZ},\n}", "authors": ["Fan Yang", "Jiazhong Nie", "William W. Cohen", "Ni Lao"], "keywords": ["neuro-symbolic reasoning", "information extraction", "learn to search"], "authorids": ["fanyang1@cs.cmu.edu", "niejiazhong@google.com", "wcohen@cs.cmu.edu", "nlao@google.com"]}, "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}], "count": 2}