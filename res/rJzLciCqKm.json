{"notes": [{"id": "rJzLciCqKm", "original": "HylVJKz5K7", "number": 538, "cdate": 1538087822323, "ddate": null, "tcdate": 1538087822323, "tmdate": 1551177129425, "tddate": null, "forum": "rJzLciCqKm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Learning from Positive and Unlabeled Data with a Selection Bias", "abstract": "We consider the problem of learning a binary classifier only from positive data and unlabeled data (PU learning). Recent methods of PU learning commonly assume that the labeled positive data are identically distributed as the unlabeled positive data. However, this assumption is unrealistic in many instances of PU learning because it fails to capture the existence of a selection bias in the labeling process. When the data has a selection bias, it is difficult to learn the Bayes optimal classifier by conventional methods of PU learning. In this paper, we propose a method to partially identify the classifier. The proposed algorithm learns a scoring function that preserves the order induced by the class posterior under mild assumptions, which can be used as a classifier by setting an appropriate threshold. Through experiments, we show that the method outperforms previous methods for PU learning on various real-world datasets.", "keywords": ["PU learning", "deep learning", "machine learning", "anomaly detection", "sampling bias"], "authorids": ["mkato@ms.k.u-tokyo.ac.jp", "teshima@ms.k.u-tokyo.ac.jp", "honda@edu.k.u-tokyo.ac.jp"], "authors": ["Masahiro Kato", "Takeshi Teshima", "Junya Honda"], "pdf": "/pdf/42bd77e12a44e424a2fd01fcfae01d48ed80b20f.pdf", "paperhash": "kato|learning_from_positive_and_unlabeled_data_with_a_selection_bias", "_bibtex": "@inproceedings{\nkato2018learning,\ntitle={Learning from Positive and Unlabeled Data with a Selection Bias},\nauthor={Masahiro Kato and Takeshi Teshima and Junya Honda},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJzLciCqKm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Bkx2y1UelE", "original": null, "number": 1, "cdate": 1544736484442, "ddate": null, "tcdate": 1544736484442, "tmdate": 1545354513779, "tddate": null, "forum": "rJzLciCqKm", "replyto": "rJzLciCqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper538/Meta_Review", "content": {"metareview": "This manuscript proposes a new algorithm for learning from positive and unlabeled data. The motivation for this work includes cases of selection bias, where the positive label is correlated with observation. The resulting procedure is shown to learn a scoring function that preserves the class-posterior ordering, and can thus be thresholded to obtain a classifier.\n\nThe problem addressed is interesting, and the approach sounds reasonable. The writing seems to be well done, particularly after the rebuttal when the work was better placed in context.\n\nThe reviewers and AC note issues with the evaluation of the proposed method. In particular, the authors do not provide a sufficiently convincing empirical evaluation on real data. ", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Metareview"}, "signatures": ["ICLR.cc/2019/Conference/Paper538/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper538/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Positive and Unlabeled Data with a Selection Bias", "abstract": "We consider the problem of learning a binary classifier only from positive data and unlabeled data (PU learning). Recent methods of PU learning commonly assume that the labeled positive data are identically distributed as the unlabeled positive data. However, this assumption is unrealistic in many instances of PU learning because it fails to capture the existence of a selection bias in the labeling process. When the data has a selection bias, it is difficult to learn the Bayes optimal classifier by conventional methods of PU learning. In this paper, we propose a method to partially identify the classifier. The proposed algorithm learns a scoring function that preserves the order induced by the class posterior under mild assumptions, which can be used as a classifier by setting an appropriate threshold. Through experiments, we show that the method outperforms previous methods for PU learning on various real-world datasets.", "keywords": ["PU learning", "deep learning", "machine learning", "anomaly detection", "sampling bias"], "authorids": ["mkato@ms.k.u-tokyo.ac.jp", "teshima@ms.k.u-tokyo.ac.jp", "honda@edu.k.u-tokyo.ac.jp"], "authors": ["Masahiro Kato", "Takeshi Teshima", "Junya Honda"], "pdf": "/pdf/42bd77e12a44e424a2fd01fcfae01d48ed80b20f.pdf", "paperhash": "kato|learning_from_positive_and_unlabeled_data_with_a_selection_bias", "_bibtex": "@inproceedings{\nkato2018learning,\ntitle={Learning from Positive and Unlabeled Data with a Selection Bias},\nauthor={Masahiro Kato and Takeshi Teshima and Junya Honda},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJzLciCqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper538/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353181401, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJzLciCqKm", "replyto": "rJzLciCqKm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper538/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper538/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper538/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353181401}}}, {"id": "HkeyUe_96Q", "original": null, "number": 3, "cdate": 1542254663371, "ddate": null, "tcdate": 1542254663371, "tmdate": 1542254663371, "tddate": null, "forum": "rJzLciCqKm", "replyto": "H1g4cld937", "invitation": "ICLR.cc/2019/Conference/-/Paper538/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "Thank you for your insightful comments. The references you suggested are very helpful and we have included them in the newest manuscript. Our replies are listed below.\n\nQ1. It seems the problem could be cast as an (interesting) special case of learning from instance dependent label noise. The assumption of the selection probability preserving the ordering of the true class probability has a fair amount of precedent in these work. Some discussion on the connection seems prudent.\nA1. Thank you for pointing out the important related work. We have clarified the novelty of our problem in the newest manuscript. As pointed out, our problem setting can be interpreted as the case-control PU learning with instance-dependent selection bias. To the best of our knowledge, this problem has not been tackled until now either in the PU learning or the learning from label noise. In order to apply methods of learning from noisy label to PU learning, we need to assume the censoring scenario and these methods are not applicable to our problem setting based on the case-control scenario. The censoring scenario is a special case of learning from noisy label where only negative data is contaminated. Thus, in that scenario, unlabeled data can be regarded as negative-labeled data contaminated by positive data. On the other hand, in the case-control scenario, unlabeled data is from p(x), i.e., we cannot observe p(x|o = 0). Therefore, our problem is different from the work of learning from noisy label. In addition, our method is also applicable to the censoring scenario when the invariance of order holds, because the unlabeled data of the case-control scenario can be made from positive and unlabeled data of the censoring scenario.\n  \nQ2. The lack of unbiasedness implies that minimizing over a restricted function may result in different solutions. I did feel the point could be made a little more explicit.\nA2. In the newest manuscript, we made the fact that we cannot obtain the unbiased estimator of the risk more explicit. Still, we believe that the most interesting point of our paper is also in this fact. It is because our main idea is to estimate the proxy for p(y=+1|x). Although our algorithm lacks unbiasedness, we can train a justifiable classier as discussed in Theorem 1. In our paper, we showed a method that can partially identify what we want to estimate instead of estimating the Bayesian optimal classifier and p(y=+1|x).\n\nQ3. There isn't a clear way of estimating P(y = 1). It somewhat restricts the universality of the approach.\nA3. We also agree with this point. There is no justified prior work for estimating the class prior in our setting. In order to answer this question as much as possible, we added two experimental results. In the first experiment, we measured the sensitivity of our algorithm to the misspecified class prior. In the second experiment, we empirically tested our algorithm with an estimated class prior. As Reviewer 2 pointed out, we can still use the existing methods in the class prior estimation as a heuristic. We used Ramaswamy et al. (2016) to estimate the class prior, which is considered to be the state-of-the-art method in the case-control PU learning. All results are reported in Section 5.4.\n\nQ4. While the logistic or LSIF losses are valid choices, one could use other similar loss.\nA4. 1) For the density ratio estimation, we will add experiment of KLIEP. 2) The reason why we did not discuss other loss except for logistic loss is that the risk minimizer matches the desired density ratio for the log loss function. Other losses may not be guaranteed to enjoy this property. \n\nQ5. Elkan & Noto paper operates in the censoring rather than case-controlled setting.\nA5. We cited their paper because they mentioned the difference between the case-control and censoring scenario. To avoid confusion, we have removed the mentioning in the newest manuscript.\n\nQ6. One achieves the BEP with the choice of threshold. It seems one cast the problem of estimating p(y = 1) as the problem of choosing a good threshold?\nA6. We agree with your opinion. As we mentioned in A3, we add new experiments to answer the question as much as possible. The experiments show that our method still works well under the estimated class prior.\n\nQ7. Restricting attention to scorers with output in [eps, 1 - eps] is a little strange.\nA7. Considering the extended real numbers may be an option, but, even in that case, a potential \\infty - \\infty cannot be avoided. Thus, a treatment like introducing the epsilon would be required. Furthermore, the value of epsilon is arbitrary in (0, 0.5) and it does not limit the applicability of our theoretical analysis.\n\nQ8. In the proof of Thm 3, I don't see the need to go through an infinite dimensional Lagrangian route.\nA8. Thank you for your suggestion. As you suggested, we have simplified the proof in the newest manuscript by considering a point-wise minimization of the objective functional."}, "signatures": ["ICLR.cc/2019/Conference/Paper538/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper538/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper538/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Positive and Unlabeled Data with a Selection Bias", "abstract": "We consider the problem of learning a binary classifier only from positive data and unlabeled data (PU learning). Recent methods of PU learning commonly assume that the labeled positive data are identically distributed as the unlabeled positive data. However, this assumption is unrealistic in many instances of PU learning because it fails to capture the existence of a selection bias in the labeling process. When the data has a selection bias, it is difficult to learn the Bayes optimal classifier by conventional methods of PU learning. In this paper, we propose a method to partially identify the classifier. The proposed algorithm learns a scoring function that preserves the order induced by the class posterior under mild assumptions, which can be used as a classifier by setting an appropriate threshold. Through experiments, we show that the method outperforms previous methods for PU learning on various real-world datasets.", "keywords": ["PU learning", "deep learning", "machine learning", "anomaly detection", "sampling bias"], "authorids": ["mkato@ms.k.u-tokyo.ac.jp", "teshima@ms.k.u-tokyo.ac.jp", "honda@edu.k.u-tokyo.ac.jp"], "authors": ["Masahiro Kato", "Takeshi Teshima", "Junya Honda"], "pdf": "/pdf/42bd77e12a44e424a2fd01fcfae01d48ed80b20f.pdf", "paperhash": "kato|learning_from_positive_and_unlabeled_data_with_a_selection_bias", "_bibtex": "@inproceedings{\nkato2018learning,\ntitle={Learning from Positive and Unlabeled Data with a Selection Bias},\nauthor={Masahiro Kato and Takeshi Teshima and Junya Honda},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJzLciCqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper538/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619818, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJzLciCqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper538/Authors", "ICLR.cc/2019/Conference/Paper538/Reviewers", "ICLR.cc/2019/Conference/Paper538/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper538/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper538/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper538/Authors|ICLR.cc/2019/Conference/Paper538/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper538/Reviewers", "ICLR.cc/2019/Conference/Paper538/Authors", "ICLR.cc/2019/Conference/Paper538/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619818}}}, {"id": "BJxy8aDqaX", "original": null, "number": 2, "cdate": 1542253895156, "ddate": null, "tcdate": 1542253895156, "tmdate": 1542253895156, "tddate": null, "forum": "rJzLciCqKm", "replyto": "rJgSzZK33Q", "invitation": "ICLR.cc/2019/Conference/-/Paper538/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Thank you for your insightful comments. \nWe have updated the manuscript with more explanation of our examples and a clarified statement of Assumption 1. Our replies to the questions are listed below.\n\nQ1. In example 1, be specific about what p(y|...) and p(o|...) are.\nA1. We have added more explanation in the newest manuscript. In example 1, p(y=+1|x) is the probability that a given input x is an anomaly, while p(o=+1|x) is the probability that a given input x gets labeled as an anomaly in the dataset.\n\nQ2. In example 2, I wasn't sure what p(o|...) exactly would be.\nA2. We have added more explanation in the newest manuscript. In example 2, a positively labeled data is a picture x that is known to belong to a user, while p(o=+1|x) is the probability that the user provides the picture x as a training datum.\n\nQ3. Assumption 1, the first sentence I understand. The \"if and only if\" part I don't see. Can you clarify?\nA3. The first and second sentences were together supposed to mean that p(y=+1|x_i) \\leq p(y=+1|x_j) is equivalent to p(o=+1|x_i) \\leq p(o=+1|x_j). We are sorry for the unclarity and the typo. We have updated the manuscript to make the statement clear.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper538/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper538/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper538/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Positive and Unlabeled Data with a Selection Bias", "abstract": "We consider the problem of learning a binary classifier only from positive data and unlabeled data (PU learning). Recent methods of PU learning commonly assume that the labeled positive data are identically distributed as the unlabeled positive data. However, this assumption is unrealistic in many instances of PU learning because it fails to capture the existence of a selection bias in the labeling process. When the data has a selection bias, it is difficult to learn the Bayes optimal classifier by conventional methods of PU learning. In this paper, we propose a method to partially identify the classifier. The proposed algorithm learns a scoring function that preserves the order induced by the class posterior under mild assumptions, which can be used as a classifier by setting an appropriate threshold. Through experiments, we show that the method outperforms previous methods for PU learning on various real-world datasets.", "keywords": ["PU learning", "deep learning", "machine learning", "anomaly detection", "sampling bias"], "authorids": ["mkato@ms.k.u-tokyo.ac.jp", "teshima@ms.k.u-tokyo.ac.jp", "honda@edu.k.u-tokyo.ac.jp"], "authors": ["Masahiro Kato", "Takeshi Teshima", "Junya Honda"], "pdf": "/pdf/42bd77e12a44e424a2fd01fcfae01d48ed80b20f.pdf", "paperhash": "kato|learning_from_positive_and_unlabeled_data_with_a_selection_bias", "_bibtex": "@inproceedings{\nkato2018learning,\ntitle={Learning from Positive and Unlabeled Data with a Selection Bias},\nauthor={Masahiro Kato and Takeshi Teshima and Junya Honda},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJzLciCqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper538/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619818, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJzLciCqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper538/Authors", "ICLR.cc/2019/Conference/Paper538/Reviewers", "ICLR.cc/2019/Conference/Paper538/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper538/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper538/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper538/Authors|ICLR.cc/2019/Conference/Paper538/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper538/Reviewers", "ICLR.cc/2019/Conference/Paper538/Authors", "ICLR.cc/2019/Conference/Paper538/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619818}}}, {"id": "ryxnYnvqp7", "original": null, "number": 1, "cdate": 1542253699686, "ddate": null, "tcdate": 1542253699686, "tmdate": 1542253842564, "tddate": null, "forum": "rJzLciCqKm", "replyto": "SJxpvEPt3m", "invitation": "ICLR.cc/2019/Conference/-/Paper538/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Thank you for your constructive comments. We revised our manuscript based on your advice and continue to reflect your advice on our manuscript. Our replies are listed below.\n\nQ1. He et al. 2018 use a very similar assumption and no comparison with that work is provided.\nA1.  He et al. (2018) does not seem to be a completed work and we could not discuss it. There are three reasons why a comparison with He et al. (2018) is not included in the paper. \n1. Almost all theorems in that paper do not hold. Please see the last paragraph of this answer for examples of the incorrect points. \n2. Although the assumption mentioned in the abstract of their paper sounds similar to ours, they added assumptions in p.7 and implicitly changed them in the proofs. These assumptions are not familiar in the context of PU learning. For example, they assume that, if p(y=+1|x) < 0.5 for a positive example, then x is not labeled. Besides, just after the declaration of the assumption, they start to use a different assumption. The actual assumption, which implicitly appears in (24), means that all unlabeled data is negative data.  Moreover, their problem setting is not the case-control scenario unlike ours.\n3. As for experimental comparisons, it was difficult to re-implement their proposed method only from their paper. For example, they have a function beta(x) that needs to be estimated. The estimation of beta(x) requires a constrained optimization whose result depends on the optimization method. However, they did not describe what optimization method they used.  We requested the authors to send the source code before the initial submission of this paper but we did not receive a response.\n\nFor these reasons, it is difficult to regard this paper as a valid reference. In the first manuscript, we mentioned this paper in the introduction to refer to the setting of their experiment. However, we deleted the paper from the related works in Introduction of the newest manuscript because we have fully confirmed that the paper is problematic as described above.\n\nHere, we describe examples of the incorrect points in that paper.  They use \\rho_+ = P(\\tilde{Y} = -1|X, Y=+1) in (8) and \\rho_- = P(\\tilde{Y} = +1|X, Y=+1) in (9) (these definitions are those from p.6, and different definitions are used in p.1). Then, they assume that \\rho_+ > 0 if p(y=+1|x) > 0.5 in (16); \\rho_-  = 0 if p(y=+1|x) < 0.5 in (17). As explained above, (17) is not familiar to PU learning. Furthermore, from (24) in p.9 they started to assume \\rho_+  = 0  (not \\rho_-=0) if p(y=+1|x) < 0.5 and proved their theorems. Combined the other assumption in (18), we must assume that all unlabeled data is negative data in order for main theorems to hold. Besides, there are also mistakes in the declaration of (16) and (17).\n\nQ2.  Have you tried using the estimate for the class prior...? Consider estimating Pr(y=1) using existing methods....\nA2.   In the initial submission we did not estimate the class prior since the prior work is not justified under our problem setting. In the newest manuscript, we added an experimental result where Ramaswamy et al. (2016) is naively used for estimation of the class prior, which is considered to be the state-of-the-art method in the \u201ccase-control\u201d scenario with the selected completely at random assumption. In addition, we added another experiment which measures the sensitivity of our algorithm to a misspecified class prior. All results are reported in Section 5.4.\n\nQ3. Related work discussion is completely missing.\nA3. Because this paper deals with a novel problem setting, there is no directly comparable work. As explained in A1, the content of He et al. (2018) is too problematic to discuss and they discussed a different problem (not the case-control scenario). Based on the references suggested by Reviewer 1, we added a discussion regarding learning from instance-dependent label noise.\n\nQ4. The acronym SCR is not very conventional; I would suggest IID.\nA4. The term \u201cselected completely at random\u201d has been conventionally used since Elkan and Noto (2008). Bekker and Davis (2018) used \u201cSCAR\u201d to represent it, so we follow them to change the acronym to SCAR in the newest manuscript. We cannot use \u201cIID\u201d because our positive data is also sampled from p(x|y=+1, o=+1) in an i.i.d. manner.\n\nQ5. Why was the log-loss used?\nA5. The reason why we did not discuss other loss is that, only for the log loss function, we proved that the risk minimizer matches the density ratio that we want to estimate. Although we might use some other loss functions to approximate the conditional probability, the stationary points for these losses do not have this property in general.\n\nQ6. Theorem 3: add some intuition and explain tradeoff on \\epsilon.\nA6. There is no tradeoff on \\epsilon. Its value can be arbitrarily specified in the range of (0, 1/2). It is an artifact for the theoretical analysis to avoid a potential ill-definedness due to (\\infty - \\infty) in the problem formulation."}, "signatures": ["ICLR.cc/2019/Conference/Paper538/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper538/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper538/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Positive and Unlabeled Data with a Selection Bias", "abstract": "We consider the problem of learning a binary classifier only from positive data and unlabeled data (PU learning). Recent methods of PU learning commonly assume that the labeled positive data are identically distributed as the unlabeled positive data. However, this assumption is unrealistic in many instances of PU learning because it fails to capture the existence of a selection bias in the labeling process. When the data has a selection bias, it is difficult to learn the Bayes optimal classifier by conventional methods of PU learning. In this paper, we propose a method to partially identify the classifier. The proposed algorithm learns a scoring function that preserves the order induced by the class posterior under mild assumptions, which can be used as a classifier by setting an appropriate threshold. Through experiments, we show that the method outperforms previous methods for PU learning on various real-world datasets.", "keywords": ["PU learning", "deep learning", "machine learning", "anomaly detection", "sampling bias"], "authorids": ["mkato@ms.k.u-tokyo.ac.jp", "teshima@ms.k.u-tokyo.ac.jp", "honda@edu.k.u-tokyo.ac.jp"], "authors": ["Masahiro Kato", "Takeshi Teshima", "Junya Honda"], "pdf": "/pdf/42bd77e12a44e424a2fd01fcfae01d48ed80b20f.pdf", "paperhash": "kato|learning_from_positive_and_unlabeled_data_with_a_selection_bias", "_bibtex": "@inproceedings{\nkato2018learning,\ntitle={Learning from Positive and Unlabeled Data with a Selection Bias},\nauthor={Masahiro Kato and Takeshi Teshima and Junya Honda},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJzLciCqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper538/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619818, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJzLciCqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper538/Authors", "ICLR.cc/2019/Conference/Paper538/Reviewers", "ICLR.cc/2019/Conference/Paper538/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper538/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper538/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper538/Authors|ICLR.cc/2019/Conference/Paper538/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper538/Reviewers", "ICLR.cc/2019/Conference/Paper538/Authors", "ICLR.cc/2019/Conference/Paper538/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619818}}}, {"id": "rJgSzZK33Q", "original": null, "number": 3, "cdate": 1541341453094, "ddate": null, "tcdate": 1541341453094, "tmdate": 1541533908566, "tddate": null, "forum": "rJzLciCqKm", "replyto": "rJzLciCqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper538/Official_Review", "content": {"title": "New technique for positive-unlabeled learning focussing on addressing selection bias.", "review": "In this paper, the authors present a new technique to learn from positive and unlabeled data. Specifically they are addressing the issues that arise when the positive and unlabeled data do not come from the same distribution. The way to achieve this is to learn a scoring function which preserves -the order- of the label posteriors. In other words, the authors are not making assumptions and then learning the exact posterior of p(y|...) but rather just a function r(x) with the property that if p(y_i) < p(y_j) then r(x_i) < r(x_j).\n\nI am not super familiar in the area but I didn't see any fundamental flaws. The approach makes sense and although I cannot judge the novelty of this paper, it is a useful tool in the PU learning toolbox addressing an arguably important problem (selection bias). Except for section 5.3, the experiments are not that interesting as they are made up artificially by the authors.\n\nThoughts:\n- In example 1, be specific about what p(y|...) and p(o|...) are.\n- In example 2, I wasn't sure what p(o|...) exactly would be.\n- Assumption 1, the first sentence I understand. The \"if and only if\" part I don't see. Can you clarify?\n", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper538/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Positive and Unlabeled Data with a Selection Bias", "abstract": "We consider the problem of learning a binary classifier only from positive data and unlabeled data (PU learning). Recent methods of PU learning commonly assume that the labeled positive data are identically distributed as the unlabeled positive data. However, this assumption is unrealistic in many instances of PU learning because it fails to capture the existence of a selection bias in the labeling process. When the data has a selection bias, it is difficult to learn the Bayes optimal classifier by conventional methods of PU learning. In this paper, we propose a method to partially identify the classifier. The proposed algorithm learns a scoring function that preserves the order induced by the class posterior under mild assumptions, which can be used as a classifier by setting an appropriate threshold. Through experiments, we show that the method outperforms previous methods for PU learning on various real-world datasets.", "keywords": ["PU learning", "deep learning", "machine learning", "anomaly detection", "sampling bias"], "authorids": ["mkato@ms.k.u-tokyo.ac.jp", "teshima@ms.k.u-tokyo.ac.jp", "honda@edu.k.u-tokyo.ac.jp"], "authors": ["Masahiro Kato", "Takeshi Teshima", "Junya Honda"], "pdf": "/pdf/42bd77e12a44e424a2fd01fcfae01d48ed80b20f.pdf", "paperhash": "kato|learning_from_positive_and_unlabeled_data_with_a_selection_bias", "_bibtex": "@inproceedings{\nkato2018learning,\ntitle={Learning from Positive and Unlabeled Data with a Selection Bias},\nauthor={Masahiro Kato and Takeshi Teshima and Junya Honda},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJzLciCqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper538/Official_Review", "cdate": 1542234438455, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJzLciCqKm", "replyto": "rJzLciCqKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper538/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335747081, "tmdate": 1552335747081, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper538/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1g4cld937", "original": null, "number": 2, "cdate": 1541206156469, "ddate": null, "tcdate": 1541206156469, "tmdate": 1541533908360, "tddate": null, "forum": "rJzLciCqKm", "replyto": "rJzLciCqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper538/Official_Review", "content": {"title": "Reasonable but somewhat unsurprising approach to an interesting problem", "review": "The paper proposes an approach to learning from positive and unlabelled data with a sample selection bias. Specifically, it is assumed that the observed positive instances are not necessarily drawn iid from the true positive distribution: rather, there is some bias as to which positive examples are selected. Under an assumption on the selection probability being proportional to the true probability, it is established that one may equally rank instances based on their probability of being labelled. Two algorithms are proposed for this task.\n\nLearning under sample selection bias is an important and interesting problem. It is also arguably more realistic than the classic PU learning setting. The paper proposes a reasonable solution, which builds on some recent advances in the literature on PU learning.\n\nMy only critique is that the results are somewhat unsurprising in light of existing work on this topic (the idea of constructing unbiased risk estimators), and also on the topic of learning from label loans. Further, I believe some clarifications would better position the contributions of the paper, both in terms of strengths and limitations. More specifically:\n\n- it seems the problem could be cast as a (interesting) special case of learning from instance dependent label noise. The assumption of the selection (i.e., label flip) probability preserving the ordering of the true class probability has a fair amount of precedent in these works; see, e.g.,\n\nBylander '97, Learning probabilistically consistent linear threshold functions\nDu and Cai '15, Modelling class noise with symmetric and asymmetric distributions\nBootkrajang '16, A generalised label noise model for classification in the presence of annotation errors\n\nIt is in light of these works that I do not find Theorem 1 surprising. I note that the sample-selection bias setting could be seen as an interesting special case, but some discussion on the connection seems prudent.\n\n- like in the above works, the proposed approach does not construct an unbiased estimator to the underlying risk. Instead, what is shown in e.g. Theorem 3 is that the Bayes-optimal solution to the risk is sensible. This is of course a minimal desiderata for any learning method, but unlike approaches for the classic PU learning setting, the lack of unbiasedness implies that minimizing over a restricted function class F may result in quite different solutions than if we had access to the true labels. Again, this isn't a limitation unique to this particular work, but I did feel the point could be made a little more explicit.\n\n- also like the above work, there isn't a clear way of estimating P(y = 1). As this is crucial for the final risk estimate, it somewhat restricts the universality of the approach.\n\n- with regards to the two algorithms proposed, both go about estimating the underlying \"noisy\" class probability (i.e., the probability of an instance being selected for labeling), just with different losses. While the logistic or \"LSIF\" loss are certainly valid choices, one could use any number of other similar loss (e.g., the exponential loss from class-probability estimation, or the \"KLIEP\" loss from density ratio estimation). Of course the specific choice of LSIF e.g. can be motivated since it has a closed-form solution, but the basic point is that the two approaches really boil down to changing the underlying loss function. This point could also be clarified.\n\n\nOther comments:\n\n- I believe the Elkan & Noto paper operates in the censoring rather than case-controlled setting.\n\n- there are a few grammatical issues: e.g.., \"Several recent researches\", \"is to find anomaly data\"\n\n- I don't follow how the case-controlled setting is \"more general\" than the censoring setting, as claimed in Sec 2; do you mean it is more practically realistic?\n\n- it is correct to say in 2.1 that one cannot estimate p(y = 1 | x) from only PU data without assumptions. The next sentence states that a typical assumption that is thus made is SCR. However, this also does not guarantee that we can estimate the probability, since estimating p(y = 1) is also not possible without even further assumptions (see e.g. the mutually contaminated distribution work of Scott et al., 2013).\n\n- in Defn 1, it would be clearer to explicate the dependence of all quantities on r.\n\n- it is interesting that one achieves the BEP with the choice of threshold given by (2). But given that p(y = 1) is in general hard to estimate, it seems one could equally cast the problem of estimating p(y = 1) as the problem of choosing a good threshold? (This of course ignores the fact that we ostensibly need p(y = 1) when constructing the risk estimate.)\n\n- restricting attention to scorers with output in [eps, 1 - eps] is a little strange. I assume this is in order to avoid solutions at +- infinity, which is a well-known problem with the logistic loss. It may be more natural to simply state that you operate with the extended real numbers.\n\n- in the proof of Thm 3, I don't see the need to go through an infinite dimensional Lagrangian route. Since one is optimizing over all possible measurable functions, can one not (under suitable regularity conditions on the distribution & loss) simply compute the minimizer point wise for each x? This optimization would be a one-dimensional problem over predictions the domain [eps, 1 - eps]. The \"inner risk\" to be optimized (in the sense of Steinwart '06, \"How to compare different loss functions and their risks\") would I believe be a convex function, admitting exactly the minimizer claimed in the statement of the theorem.\n\n- it is a bit confusing to move from F to \\hat{F} as the function class.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper538/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Positive and Unlabeled Data with a Selection Bias", "abstract": "We consider the problem of learning a binary classifier only from positive data and unlabeled data (PU learning). Recent methods of PU learning commonly assume that the labeled positive data are identically distributed as the unlabeled positive data. However, this assumption is unrealistic in many instances of PU learning because it fails to capture the existence of a selection bias in the labeling process. When the data has a selection bias, it is difficult to learn the Bayes optimal classifier by conventional methods of PU learning. In this paper, we propose a method to partially identify the classifier. The proposed algorithm learns a scoring function that preserves the order induced by the class posterior under mild assumptions, which can be used as a classifier by setting an appropriate threshold. Through experiments, we show that the method outperforms previous methods for PU learning on various real-world datasets.", "keywords": ["PU learning", "deep learning", "machine learning", "anomaly detection", "sampling bias"], "authorids": ["mkato@ms.k.u-tokyo.ac.jp", "teshima@ms.k.u-tokyo.ac.jp", "honda@edu.k.u-tokyo.ac.jp"], "authors": ["Masahiro Kato", "Takeshi Teshima", "Junya Honda"], "pdf": "/pdf/42bd77e12a44e424a2fd01fcfae01d48ed80b20f.pdf", "paperhash": "kato|learning_from_positive_and_unlabeled_data_with_a_selection_bias", "_bibtex": "@inproceedings{\nkato2018learning,\ntitle={Learning from Positive and Unlabeled Data with a Selection Bias},\nauthor={Masahiro Kato and Takeshi Teshima and Junya Honda},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJzLciCqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper538/Official_Review", "cdate": 1542234438455, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJzLciCqKm", "replyto": "rJzLciCqKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper538/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335747081, "tmdate": 1552335747081, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper538/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJxpvEPt3m", "original": null, "number": 1, "cdate": 1541137509446, "ddate": null, "tcdate": 1541137509446, "tmdate": 1541533908115, "tddate": null, "forum": "rJzLciCqKm", "replyto": "rJzLciCqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper538/Official_Review", "content": {"title": "Little novelty, experiments do not offer comparison with related work", "review": "The authors consider the problem of learning from positive and unlabeled data in which only a subset of the true positives is labeled. While the common assumption (eg Elkan & Noto, du Plessis et al.) prescribes that the labeled set is picked independently at random from the positive set, this paper assumes that a (positive) example x is more likely to be labeled the more it exhibits positive features: formally, the higher Pr(y=1 | x), the higher Pr(o=1 | x). For instance, in the case of anomaly detection, the more likely an example is anomalous, the more likely it would get manually flagged (labeled) as positive. The authors refer to this assumption as Invariance of Order.\n\nThe proposed method requires the knowledge of the positive class prior Pr(y=1), and can be summarized in the following three steps: (i) estimate r(x)=Pr(x | y=1, o=1`)/Pr(x); (ii) find the threshold \\theta such that the number of datapoints x with r(x) > \\theta is a fraction Pr(y=1); (iii) train a classifier on sign(r(x) - \\theta). Conceptually, the Invariance of Order assumption allows to use the order on r(x) as a proxy for an order on Pr(y=1|x), so then the knowledge of Pr(y=1) is enough to find \\theta, and to port the original problem to a vanilla binary classification problem.\n\nConcerns:\n- He et al. 2018 use a very similar assumption and no comparison with that work is provided. The authors briefly mention that work in the introduction but don't perform due diligence in assessing differences/novelties with respect to that work, neither as a discussion or in the experiments.\n- The requirement of knowing the fraction of positive examples is hard to justify in practice. Have you tried using the estimate obtained by Elkan et al, or other related work?\n- Experiments are confusing and not convincing: apart from the very last experiment, all datasets are synthetic. No comparison with previous work is presented, except for \"unbiased PU learning (PU)\", which I assume is Elkan et al ? If that is indeed the case, which one of their methods are you comparing against? Even more troublesome is the fact that in all experiments you're providing your algorithm with the correct class-prior Pr(y=1), but it's not clear if this is provided to PU as well. You may want to consider estimating Pr(y=1) using methods from related work to see how it affects the accuracy.\n- Related work discussion is completely missing apart from one paragraph in the introduction.\n\nMinor:\n- The acronym SCR is not very conventional; I would suggest IID which is often used as shorthand for independently identically distributed.\n- Invariance of Order: when introducing it, you may want to add a sentence providing the intuition behind the assumption.\n- Example 2 (Face recognition) is not very convincing and not very clear. Please rephrase.\n- Pseudo-classification risk: why was the log-loss used? Can other losses be used as well?\n- Theorem 3: add some intuition and explain tradeoff on \\epsilon\n- Experiments section: help the reader by adding a reminder on equations, as it's difficult to flip back and forth to their definitions. Eg, \"we trained a classifier minimizing (4) and (7) with the model (10)\" is difficult to digest and follow.\n- Experiments: confusing commas in {800,1,600,3,200} => {800, 1600, 3200}\n- Too many acronyms and abbreviations.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper538/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Positive and Unlabeled Data with a Selection Bias", "abstract": "We consider the problem of learning a binary classifier only from positive data and unlabeled data (PU learning). Recent methods of PU learning commonly assume that the labeled positive data are identically distributed as the unlabeled positive data. However, this assumption is unrealistic in many instances of PU learning because it fails to capture the existence of a selection bias in the labeling process. When the data has a selection bias, it is difficult to learn the Bayes optimal classifier by conventional methods of PU learning. In this paper, we propose a method to partially identify the classifier. The proposed algorithm learns a scoring function that preserves the order induced by the class posterior under mild assumptions, which can be used as a classifier by setting an appropriate threshold. Through experiments, we show that the method outperforms previous methods for PU learning on various real-world datasets.", "keywords": ["PU learning", "deep learning", "machine learning", "anomaly detection", "sampling bias"], "authorids": ["mkato@ms.k.u-tokyo.ac.jp", "teshima@ms.k.u-tokyo.ac.jp", "honda@edu.k.u-tokyo.ac.jp"], "authors": ["Masahiro Kato", "Takeshi Teshima", "Junya Honda"], "pdf": "/pdf/42bd77e12a44e424a2fd01fcfae01d48ed80b20f.pdf", "paperhash": "kato|learning_from_positive_and_unlabeled_data_with_a_selection_bias", "_bibtex": "@inproceedings{\nkato2018learning,\ntitle={Learning from Positive and Unlabeled Data with a Selection Bias},\nauthor={Masahiro Kato and Takeshi Teshima and Junya Honda},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJzLciCqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper538/Official_Review", "cdate": 1542234438455, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJzLciCqKm", "replyto": "rJzLciCqKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper538/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335747081, "tmdate": 1552335747081, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper538/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}