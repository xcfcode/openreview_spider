{"notes": [{"id": "H1lK5kBKvr", "original": "BJeqWoRdPr", "number": 1885, "cdate": 1569439632729, "ddate": null, "tcdate": 1569439632729, "tmdate": 1577168292856, "tddate": null, "forum": "H1lK5kBKvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["gaozhongpai@sjtu.edu.cn", "juyong@ustc.edu.cn", "gyd2011@mail.ustc.edu.cn", "chaoma@sjtu.edu.cn", "zhaiguangtao@sjtu.edu.cn", "xkyang@sjtu.edu.cn"], "title": "Semi-supervised 3D Face Reconstruction with Nonlinear Disentangled Representations", "authors": ["Zhongpai Gao", "Juyong Zhang", "Yudong Guo", "Chao Ma", "Guangtao Zhai", "Xiaokang Yang"], "pdf": "/pdf/a713decd4db11e07db15c481dd038a119d6141d9.pdf", "TL;DR": "We train our face reconstruction model with adversarial loss in semi-supervised manner on hybrid batches of unlabeled and labeled face images to exploit the value of large amounts of unlabeled face images from unconstrained photo collections.", "abstract": "Recovering 3D geometry shape, albedo and lighting from a single image has wide applications in many areas, which is also a typical ill-posed problem. In order to eliminate the ambiguity, face prior knowledge like linear 3D morphable models (3DMM) learned from limited scan data are often adopted to the reconstruction process. However, methods based on linear parametric models cannot generalize well for facial images in the wild with various ages, ethnicity, expressions, poses, and lightings. Recent methods aim to learn a nonlinear parametric model using convolutional neural networks (CNN) to regress the face shape and texture directly. However, the models were only trained on a dataset that is generated from a linear 3DMM. Moreover, the identity and expression representations are entangled in these models, which hurdles many facial editing applications. In this paper, we train our model with adversarial loss in a semi-supervised manner on hybrid batches of unlabeled and labeled face images to exploit the value of large amounts of unlabeled face images from unconstrained photo collections. A novel center loss is introduced to make sure that different facial images from the same person have the same identity shape and albedo. Besides, our proposed model disentangles identity, expression, pose, and lighting representations, which improves the overall reconstruction performance and facilitates facial editing applications, e.g., expression transfer. Comprehensive experiments demonstrate that our model produces high-quality reconstruction compared to state-of-the-art methods and is robust to various expression, pose, and lighting conditions.\n", "keywords": ["3D face reconstruction", "semi-supervised learning", "disentangled representation", "inverse rendering", "graph convolutional networks"], "paperhash": "gao|semisupervised_3d_face_reconstruction_with_nonlinear_disentangled_representations", "original_pdf": "/attachment/a713decd4db11e07db15c481dd038a119d6141d9.pdf", "_bibtex": "@misc{\ngao2020semisupervised,\ntitle={Semi-supervised 3D Face Reconstruction with Nonlinear Disentangled Representations},\nauthor={Zhongpai Gao and Juyong Zhang and Yudong Guo and Chao Ma and Guangtao Zhai and Xiaokang Yang},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lK5kBKvr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "8jlAURBDwx", "original": null, "number": 1, "cdate": 1576798735051, "ddate": null, "tcdate": 1576798735051, "tmdate": 1576800901323, "tddate": null, "forum": "H1lK5kBKvr", "replyto": "H1lK5kBKvr", "invitation": "ICLR.cc/2020/Conference/Paper1885/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a semi-supervised method for reconstructing 3D faces from images via a disentangled representation. The method builds on previous work by Tran et al (2018, 2019). While some results presented in the paper show that this method works well, all reviewers agree that the authors should have provided more experimental evidence to convincingly demonstrate the benefits of their method. The reviewers are also unconvinced by how computationally expensive this method is or by the contributions of the unlabelled data to the performance of the proposed model. Given that the authors did not address the reviewers\u2019 concerns, and for the reasons stated above, I recommend rejecting this paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gaozhongpai@sjtu.edu.cn", "juyong@ustc.edu.cn", "gyd2011@mail.ustc.edu.cn", "chaoma@sjtu.edu.cn", "zhaiguangtao@sjtu.edu.cn", "xkyang@sjtu.edu.cn"], "title": "Semi-supervised 3D Face Reconstruction with Nonlinear Disentangled Representations", "authors": ["Zhongpai Gao", "Juyong Zhang", "Yudong Guo", "Chao Ma", "Guangtao Zhai", "Xiaokang Yang"], "pdf": "/pdf/a713decd4db11e07db15c481dd038a119d6141d9.pdf", "TL;DR": "We train our face reconstruction model with adversarial loss in semi-supervised manner on hybrid batches of unlabeled and labeled face images to exploit the value of large amounts of unlabeled face images from unconstrained photo collections.", "abstract": "Recovering 3D geometry shape, albedo and lighting from a single image has wide applications in many areas, which is also a typical ill-posed problem. In order to eliminate the ambiguity, face prior knowledge like linear 3D morphable models (3DMM) learned from limited scan data are often adopted to the reconstruction process. However, methods based on linear parametric models cannot generalize well for facial images in the wild with various ages, ethnicity, expressions, poses, and lightings. Recent methods aim to learn a nonlinear parametric model using convolutional neural networks (CNN) to regress the face shape and texture directly. However, the models were only trained on a dataset that is generated from a linear 3DMM. Moreover, the identity and expression representations are entangled in these models, which hurdles many facial editing applications. In this paper, we train our model with adversarial loss in a semi-supervised manner on hybrid batches of unlabeled and labeled face images to exploit the value of large amounts of unlabeled face images from unconstrained photo collections. A novel center loss is introduced to make sure that different facial images from the same person have the same identity shape and albedo. Besides, our proposed model disentangles identity, expression, pose, and lighting representations, which improves the overall reconstruction performance and facilitates facial editing applications, e.g., expression transfer. Comprehensive experiments demonstrate that our model produces high-quality reconstruction compared to state-of-the-art methods and is robust to various expression, pose, and lighting conditions.\n", "keywords": ["3D face reconstruction", "semi-supervised learning", "disentangled representation", "inverse rendering", "graph convolutional networks"], "paperhash": "gao|semisupervised_3d_face_reconstruction_with_nonlinear_disentangled_representations", "original_pdf": "/attachment/a713decd4db11e07db15c481dd038a119d6141d9.pdf", "_bibtex": "@misc{\ngao2020semisupervised,\ntitle={Semi-supervised 3D Face Reconstruction with Nonlinear Disentangled Representations},\nauthor={Zhongpai Gao and Juyong Zhang and Yudong Guo and Chao Ma and Guangtao Zhai and Xiaokang Yang},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lK5kBKvr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1lK5kBKvr", "replyto": "H1lK5kBKvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795726030, "tmdate": 1576800278067, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1885/-/Decision"}}}, {"id": "rkx9eFq6tH", "original": null, "number": 3, "cdate": 1571821810375, "ddate": null, "tcdate": 1571821810375, "tmdate": 1574243567971, "tddate": null, "forum": "H1lK5kBKvr", "replyto": "H1lK5kBKvr", "invitation": "ICLR.cc/2020/Conference/Paper1885/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "Overview:\nThis paper introduces a model for image-based facial 3D reconstruction. The proposed model is an encoder-decoder architecture that is trained in semi-supervised way to map images to sets of vectors representing identity (which encodes albedo and geometry), pose, expression and lighting. The encoder is a standard image CNN, whereas the decoders for geometry and albedo rely on spectral graph CNNs (similar to e.g. COMA, Ranjan\u201918). \nThe main contribution of the work with respect to the existing methods is the use of additional loss terms that enable semi-supervised training and learning somewhat more disentangled representations. Authors report quantitative results on MICC Florence, with marginal improvements over the baselines (the choice of the baselines is reasonable).\n\nDecision:\nThe overall architecture is very similar to existing works such as COMA (Ranjan\u201918) and (Tran\u201919), including the specific architecture for geometry decoders, and thus the contributions are primarily in the newly added loss terms. \nI also find the promise of \u201cdisentangled\u201d representation a bit over-stated, as the albedo and base geometry still seem to be encoded in the same \u201cidentity\u201d vector (see related question below).\nThe numerical improvements seem fairly modest with respect to (Tran\u201919). In addition, there is no numerical ablation study that would demonstrate the actual utility of the main contributions (such as adversarial loss): there are qualitative results but they are not very convincing. \nThus, the final rating \u201cweak reject\u201d.\n\nAdditional comments / typos:\n\n* I am not fully following the argument about sharing identity for albedo and shape on p2: \u201calbedo and face shape are decoded ...\u201d. Would it not be more beneficial to have a fully decoupled representation between the albedo and the facial geometry? I do not see how albedo information would be useful for encoding face geometry and vise-versa. \n* Authors claim that one of the main drawbacks e.g. of (Train\u201919) is the fact that they train on data generated from linear 3DMM. This is indeed the case, but it does not seem like here the authors fully overcome this issue: they do have additional weakly-supervised data, but they still strongly rely on linear 3DMM supervision (p6, \u201cpairwise shape loss\u201d, \u201cadversarial loss\u201d), and do not seem to provide experimental evidence that the model will work without it.\n* In particular, the \u201cadversarial training\u201d actually corresponds to learning the distribution of the linear 3DMM. Would it not mean that ultimately the model will be limited to learning only linear ? Could you please elaborate on this?\n* p3: \u201callows ene-to-end \u2026 training\u201d\n* p3: \u201cframework to exact \u2026 representations\u201c.\n* p8: \u201cevaluation matric\u201d\n\nUpdate:\nAuthors did not provide any response, thus I keep my rating.\n\n\n\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1885/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1885/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gaozhongpai@sjtu.edu.cn", "juyong@ustc.edu.cn", "gyd2011@mail.ustc.edu.cn", "chaoma@sjtu.edu.cn", "zhaiguangtao@sjtu.edu.cn", "xkyang@sjtu.edu.cn"], "title": "Semi-supervised 3D Face Reconstruction with Nonlinear Disentangled Representations", "authors": ["Zhongpai Gao", "Juyong Zhang", "Yudong Guo", "Chao Ma", "Guangtao Zhai", "Xiaokang Yang"], "pdf": "/pdf/a713decd4db11e07db15c481dd038a119d6141d9.pdf", "TL;DR": "We train our face reconstruction model with adversarial loss in semi-supervised manner on hybrid batches of unlabeled and labeled face images to exploit the value of large amounts of unlabeled face images from unconstrained photo collections.", "abstract": "Recovering 3D geometry shape, albedo and lighting from a single image has wide applications in many areas, which is also a typical ill-posed problem. In order to eliminate the ambiguity, face prior knowledge like linear 3D morphable models (3DMM) learned from limited scan data are often adopted to the reconstruction process. However, methods based on linear parametric models cannot generalize well for facial images in the wild with various ages, ethnicity, expressions, poses, and lightings. Recent methods aim to learn a nonlinear parametric model using convolutional neural networks (CNN) to regress the face shape and texture directly. However, the models were only trained on a dataset that is generated from a linear 3DMM. Moreover, the identity and expression representations are entangled in these models, which hurdles many facial editing applications. In this paper, we train our model with adversarial loss in a semi-supervised manner on hybrid batches of unlabeled and labeled face images to exploit the value of large amounts of unlabeled face images from unconstrained photo collections. A novel center loss is introduced to make sure that different facial images from the same person have the same identity shape and albedo. Besides, our proposed model disentangles identity, expression, pose, and lighting representations, which improves the overall reconstruction performance and facilitates facial editing applications, e.g., expression transfer. Comprehensive experiments demonstrate that our model produces high-quality reconstruction compared to state-of-the-art methods and is robust to various expression, pose, and lighting conditions.\n", "keywords": ["3D face reconstruction", "semi-supervised learning", "disentangled representation", "inverse rendering", "graph convolutional networks"], "paperhash": "gao|semisupervised_3d_face_reconstruction_with_nonlinear_disentangled_representations", "original_pdf": "/attachment/a713decd4db11e07db15c481dd038a119d6141d9.pdf", "_bibtex": "@misc{\ngao2020semisupervised,\ntitle={Semi-supervised 3D Face Reconstruction with Nonlinear Disentangled Representations},\nauthor={Zhongpai Gao and Juyong Zhang and Yudong Guo and Chao Ma and Guangtao Zhai and Xiaokang Yang},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lK5kBKvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1lK5kBKvr", "replyto": "H1lK5kBKvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1885/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1885/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574795918005, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1885/Reviewers"], "noninvitees": [], "tcdate": 1570237730876, "tmdate": 1574795918019, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1885/-/Official_Review"}}}, {"id": "HklBjUQHtB", "original": null, "number": 1, "cdate": 1571268252685, "ddate": null, "tcdate": 1571268252685, "tmdate": 1572972410915, "tddate": null, "forum": "H1lK5kBKvr", "replyto": "H1lK5kBKvr", "invitation": "ICLR.cc/2020/Conference/Paper1885/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a semi-supervised and adversarial training process to exploit the value of unlabeled faces and overcome the limitation of a linear 3DMM and the nonlinear models proposed early (Tran & Liu (2018), Tran et al (2019)). This approach designs a framework to exact nonlinear disentangled representations from a face image with the help of loss functions including face recognition loss, shape pairwise loss and adversarial loss. This framework's contribution is demonstrated with experiments which show this model achieves state-of-the-art performance in face reconstruction.\n\nThis paper should be rejected because:\n(1) the experiments are not representative enough and the results are controversial,\n(2) this paper does not clearly demonstrate how they exploit the value of the unlabeled training images,\n(3) the creative progress of this paper is not typical compared to the early nonlinear model (Tran & Liu (2018), Tran et al (2019)).\n\n\nMain argument: \n\nThe experiments do not provide convincing evidence of the correctness of the proposed approach or its utility compared to existing approaches. The results are not representative enough and missing many details:\n1) What is the ratio of unlabeled training images and labeled training images?\n2) Why only show the results of Cooperative and Indoor situation? \n3) Why the standard deviation of the result in Cooperative situation is higher than the early models?\n4) Why the mean value of the result in Indoor situation is higher?\n5) Why so few situations and datasets your experiments run on?\n6) How did you initialize the parameters and the weights?\n7) How about the time-consuming and memory-consuming of your model?\n\nThe paper does not demonstrate the difference and progress between its model and the early nonlinear model clearly (Tran & Liu (2018), Tran et al (2019)). This paper points out that they fully exploit the value of unlabeled face data, but there are few evidences in this paper to support that. And it also points out the time-consuming problem of early models, but there are no experiment results show how efficient its model is.\n\nThe loss functions are also not convincing enough:\n1) How to choose or initialize the value of lambda center in the Face recognition loss?\n2) Have you demonstrated the solution you used in Shape smooth loss which aims to solve the vertices do not satisfy the Laplacian equation?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1885/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1885/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gaozhongpai@sjtu.edu.cn", "juyong@ustc.edu.cn", "gyd2011@mail.ustc.edu.cn", "chaoma@sjtu.edu.cn", "zhaiguangtao@sjtu.edu.cn", "xkyang@sjtu.edu.cn"], "title": "Semi-supervised 3D Face Reconstruction with Nonlinear Disentangled Representations", "authors": ["Zhongpai Gao", "Juyong Zhang", "Yudong Guo", "Chao Ma", "Guangtao Zhai", "Xiaokang Yang"], "pdf": "/pdf/a713decd4db11e07db15c481dd038a119d6141d9.pdf", "TL;DR": "We train our face reconstruction model with adversarial loss in semi-supervised manner on hybrid batches of unlabeled and labeled face images to exploit the value of large amounts of unlabeled face images from unconstrained photo collections.", "abstract": "Recovering 3D geometry shape, albedo and lighting from a single image has wide applications in many areas, which is also a typical ill-posed problem. In order to eliminate the ambiguity, face prior knowledge like linear 3D morphable models (3DMM) learned from limited scan data are often adopted to the reconstruction process. However, methods based on linear parametric models cannot generalize well for facial images in the wild with various ages, ethnicity, expressions, poses, and lightings. Recent methods aim to learn a nonlinear parametric model using convolutional neural networks (CNN) to regress the face shape and texture directly. However, the models were only trained on a dataset that is generated from a linear 3DMM. Moreover, the identity and expression representations are entangled in these models, which hurdles many facial editing applications. In this paper, we train our model with adversarial loss in a semi-supervised manner on hybrid batches of unlabeled and labeled face images to exploit the value of large amounts of unlabeled face images from unconstrained photo collections. A novel center loss is introduced to make sure that different facial images from the same person have the same identity shape and albedo. Besides, our proposed model disentangles identity, expression, pose, and lighting representations, which improves the overall reconstruction performance and facilitates facial editing applications, e.g., expression transfer. Comprehensive experiments demonstrate that our model produces high-quality reconstruction compared to state-of-the-art methods and is robust to various expression, pose, and lighting conditions.\n", "keywords": ["3D face reconstruction", "semi-supervised learning", "disentangled representation", "inverse rendering", "graph convolutional networks"], "paperhash": "gao|semisupervised_3d_face_reconstruction_with_nonlinear_disentangled_representations", "original_pdf": "/attachment/a713decd4db11e07db15c481dd038a119d6141d9.pdf", "_bibtex": "@misc{\ngao2020semisupervised,\ntitle={Semi-supervised 3D Face Reconstruction with Nonlinear Disentangled Representations},\nauthor={Zhongpai Gao and Juyong Zhang and Yudong Guo and Chao Ma and Guangtao Zhai and Xiaokang Yang},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lK5kBKvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1lK5kBKvr", "replyto": "H1lK5kBKvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1885/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1885/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574795918005, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1885/Reviewers"], "noninvitees": [], "tcdate": 1570237730876, "tmdate": 1574795918019, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1885/-/Official_Review"}}}, {"id": "Skx902J2FS", "original": null, "number": 2, "cdate": 1571712209936, "ddate": null, "tcdate": 1571712209936, "tmdate": 1572972410882, "tddate": null, "forum": "H1lK5kBKvr", "replyto": "H1lK5kBKvr", "invitation": "ICLR.cc/2020/Conference/Paper1885/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper presents an encoder-decoder architecture to reconstruct 3D face from a single image with disentangled representations: identity, expression, pose, and lighting. The authors develop a semi-supervised training scheme to fully exploit the value of large amount of unlabeled face images from unconstrained photo collections. Experimental results on MICC Florence and AFLW2000-3D verify the efficacy of the proposed method.\n\nThe presentation and writing are clear. The problem solved in this paper aligns with real applications.\n\nMy concerns regarding this paper are as below.\n1) What are the training computational complexity and testing time cost of the proposed method? Since speed is very important for real applications.\n2) The datasets used for evaluation are quite old. More experiments on more recent challenging benchmarks are needed to verify the superiority of the proposed method, e.g., IJB-B/C, etc.\n3) Some related works are missing and need to be discussed, e.g., Joint 3D Face Reconstruction and Dense Face Alignment from A Single Image with 2D-Assisted Self-Supervised Learning [Tu et al., 2019], 3D-Aided Dual-Agent GANs for Unconstrained Face Recognition [Zhao et al., T-PAMI 2018], 3D-Aided Deep Pose-Invariant Face Recognition [Zhao et al., IJCAI 2018], etc.\n4) Format of references should be consistent.\n\nBased on my above comments, I give the rate of WR. If the authors could solve my concerns in rebuttal, I would like to further adjust my rate."}, "signatures": ["ICLR.cc/2020/Conference/Paper1885/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1885/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["gaozhongpai@sjtu.edu.cn", "juyong@ustc.edu.cn", "gyd2011@mail.ustc.edu.cn", "chaoma@sjtu.edu.cn", "zhaiguangtao@sjtu.edu.cn", "xkyang@sjtu.edu.cn"], "title": "Semi-supervised 3D Face Reconstruction with Nonlinear Disentangled Representations", "authors": ["Zhongpai Gao", "Juyong Zhang", "Yudong Guo", "Chao Ma", "Guangtao Zhai", "Xiaokang Yang"], "pdf": "/pdf/a713decd4db11e07db15c481dd038a119d6141d9.pdf", "TL;DR": "We train our face reconstruction model with adversarial loss in semi-supervised manner on hybrid batches of unlabeled and labeled face images to exploit the value of large amounts of unlabeled face images from unconstrained photo collections.", "abstract": "Recovering 3D geometry shape, albedo and lighting from a single image has wide applications in many areas, which is also a typical ill-posed problem. In order to eliminate the ambiguity, face prior knowledge like linear 3D morphable models (3DMM) learned from limited scan data are often adopted to the reconstruction process. However, methods based on linear parametric models cannot generalize well for facial images in the wild with various ages, ethnicity, expressions, poses, and lightings. Recent methods aim to learn a nonlinear parametric model using convolutional neural networks (CNN) to regress the face shape and texture directly. However, the models were only trained on a dataset that is generated from a linear 3DMM. Moreover, the identity and expression representations are entangled in these models, which hurdles many facial editing applications. In this paper, we train our model with adversarial loss in a semi-supervised manner on hybrid batches of unlabeled and labeled face images to exploit the value of large amounts of unlabeled face images from unconstrained photo collections. A novel center loss is introduced to make sure that different facial images from the same person have the same identity shape and albedo. Besides, our proposed model disentangles identity, expression, pose, and lighting representations, which improves the overall reconstruction performance and facilitates facial editing applications, e.g., expression transfer. Comprehensive experiments demonstrate that our model produces high-quality reconstruction compared to state-of-the-art methods and is robust to various expression, pose, and lighting conditions.\n", "keywords": ["3D face reconstruction", "semi-supervised learning", "disentangled representation", "inverse rendering", "graph convolutional networks"], "paperhash": "gao|semisupervised_3d_face_reconstruction_with_nonlinear_disentangled_representations", "original_pdf": "/attachment/a713decd4db11e07db15c481dd038a119d6141d9.pdf", "_bibtex": "@misc{\ngao2020semisupervised,\ntitle={Semi-supervised 3D Face Reconstruction with Nonlinear Disentangled Representations},\nauthor={Zhongpai Gao and Juyong Zhang and Yudong Guo and Chao Ma and Guangtao Zhai and Xiaokang Yang},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lK5kBKvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1lK5kBKvr", "replyto": "H1lK5kBKvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1885/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1885/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574795918005, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1885/Reviewers"], "noninvitees": [], "tcdate": 1570237730876, "tmdate": 1574795918019, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1885/-/Official_Review"}}}], "count": 5}