{"notes": [{"id": "eo6U4CAwVmg", "original": "A0mturSAr1J", "number": 2902, "cdate": 1601308321860, "ddate": null, "tcdate": 1601308321860, "tmdate": 1615974370700, "tddate": null, "forum": "eo6U4CAwVmg", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Training GANs with Stronger Augmentations via Contrastive Discriminator", "authorids": ["~Jongheon_Jeong1", "~Jinwoo_Shin1"], "authors": ["Jongheon Jeong", "Jinwoo Shin"], "keywords": ["generative adversarial networks", "contrastive learning", "data augmentation", "visual representation learning", "unsupervised learning"], "abstract": "Recent works in Generative Adversarial Networks (GANs) are actively revisiting various data augmentation techniques as an effective way to prevent discriminator overfitting. It is still unclear, however, that which augmentations could actually improve GANs, and in particular, how to apply a wider range of augmentations in training. In this paper, we propose a novel way to address these questions by incorporating a recent contrastive representation learning scheme into the GAN discriminator, coined ContraD. This \"fusion\" enables the discriminators to work with much stronger augmentations without increasing their training instability, thereby preventing the discriminator overfitting issue in GANs more effectively. Even better, we observe that the contrastive learning itself also benefits from our GAN training, i.e., by maintaining discriminative features between real and fake samples, suggesting a strong coherence between the two worlds: good contrastive representations are also good for GAN discriminators, and vice versa. Our experimental results show that GANs with ContraD consistently improve FID and IS compared to other recent techniques incorporating data augmentations, still maintaining highly discriminative features in the discriminator in terms of the linear evaluation. Finally, as a byproduct, we also show that our GANs trained in an unsupervised manner (without labels) can induce many conditional generative models via a simple latent sampling, leveraging the learned features of ContraD. Code is available at https://github.com/jh-jeong/ContraD.", "one-sentence_summary": "We propose a novel discriminator of GAN showing that contrastive representation learning, e.g., SimCLR, and GAN can benefit each other when they are jointly trained. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jeong|training_gans_with_stronger_augmentations_via_contrastive_discriminator", "pdf": "/pdf/2d308c93802630f8c000471788307eb87a9027fd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\njeong2021training,\ntitle={Training {\\{}GAN{\\}}s with Stronger Augmentations via Contrastive Discriminator},\nauthor={Jongheon Jeong and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=eo6U4CAwVmg}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "QgEGG74f3NH", "original": null, "number": 1, "cdate": 1610040464115, "ddate": null, "tcdate": 1610040464115, "tmdate": 1610474067222, "tddate": null, "forum": "eo6U4CAwVmg", "replyto": "eo6U4CAwVmg", "invitation": "ICLR.cc/2021/Conference/Paper2902/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper aims to improve the training of generative adversarial networks (GANs) by incorporating the principle of contrastive learning into the training of discriminators in GANs. Unlike in an ordinary GAN which seeks to minimize the GAN loss directly, the proposed GAN variant with a contrastive discriminator (ContraD) uses the discriminator network to first learn a contrastive representation from a given set of data augmentations and real/generated examples and then train a discriminator based on the learned contrastive representation. It is noticed that a side effect of such blending is the improvement in contrastive learning as a result of GAN training. The resulting GAN model with a contrastive discriminator is shown to outperform other techniques using data augmentation.\n\n**Strengths:**\n  * It proposes a new way of training the discriminators of GANs based on the principle of contrastive learning.\n  * The paper is generally well written to articulate the main points that the authors want to convey.\n  * The experimental evaluation is well designed and comprehensive.\n\n**Weaknesses:**\n  * Even though the proposed learning scheme is novel, the building blocks are based on existing techniques in GAN and contrastive learning.\n  * The claim that GAN helps contrastive learning is not fully substantiated.\n  * It is claimed in the paper that the proposed contrastive discriminator can lead to much stronger augmentations *without catastrophic forgetting*. However, this \u201ccatastrophic forgetting\u201d aspect is not really empirically validated in the experiments.\n  * The writing has room for improvement.\n\nDespite its weaknesses, this paper explores a novel direction of training GANs that would be of interest to the research community.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training GANs with Stronger Augmentations via Contrastive Discriminator", "authorids": ["~Jongheon_Jeong1", "~Jinwoo_Shin1"], "authors": ["Jongheon Jeong", "Jinwoo Shin"], "keywords": ["generative adversarial networks", "contrastive learning", "data augmentation", "visual representation learning", "unsupervised learning"], "abstract": "Recent works in Generative Adversarial Networks (GANs) are actively revisiting various data augmentation techniques as an effective way to prevent discriminator overfitting. It is still unclear, however, that which augmentations could actually improve GANs, and in particular, how to apply a wider range of augmentations in training. In this paper, we propose a novel way to address these questions by incorporating a recent contrastive representation learning scheme into the GAN discriminator, coined ContraD. This \"fusion\" enables the discriminators to work with much stronger augmentations without increasing their training instability, thereby preventing the discriminator overfitting issue in GANs more effectively. Even better, we observe that the contrastive learning itself also benefits from our GAN training, i.e., by maintaining discriminative features between real and fake samples, suggesting a strong coherence between the two worlds: good contrastive representations are also good for GAN discriminators, and vice versa. Our experimental results show that GANs with ContraD consistently improve FID and IS compared to other recent techniques incorporating data augmentations, still maintaining highly discriminative features in the discriminator in terms of the linear evaluation. Finally, as a byproduct, we also show that our GANs trained in an unsupervised manner (without labels) can induce many conditional generative models via a simple latent sampling, leveraging the learned features of ContraD. Code is available at https://github.com/jh-jeong/ContraD.", "one-sentence_summary": "We propose a novel discriminator of GAN showing that contrastive representation learning, e.g., SimCLR, and GAN can benefit each other when they are jointly trained. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jeong|training_gans_with_stronger_augmentations_via_contrastive_discriminator", "pdf": "/pdf/2d308c93802630f8c000471788307eb87a9027fd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\njeong2021training,\ntitle={Training {\\{}GAN{\\}}s with Stronger Augmentations via Contrastive Discriminator},\nauthor={Jongheon Jeong and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=eo6U4CAwVmg}\n}"}, "tags": [], "invitation": {"reply": {"forum": "eo6U4CAwVmg", "replyto": "eo6U4CAwVmg", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040464101, "tmdate": 1610474067204, "id": "ICLR.cc/2021/Conference/Paper2902/-/Decision"}}}, {"id": "sYACn1zjun8", "original": null, "number": 3, "cdate": 1603967406124, "ddate": null, "tcdate": 1603967406124, "tmdate": 1606245089594, "tddate": null, "forum": "eo6U4CAwVmg", "replyto": "eo6U4CAwVmg", "invitation": "ICLR.cc/2021/Conference/Paper2902/-/Official_Review", "content": {"title": "Good work on synthesizing self-supervised training and GANs; but some claims are perhaps too strong", "review": "**Summary**\nThe manuscript proposes ContraD - a method that incorporates the recent SimCLR self-supervised learning method for images into the GAN training framework. Experimental results show that the proposed method consistently improves on strong baselines in terms of FID scores.\n\n**Score justification**\nThe paper is well-written, the introduction carefully synthesizes a lot of recent work on self-supervised representation learning and GANs; results are convincing (albeit mostly obtained on smaller-scale datasets) and ablations support paper claims.\n\n\n**Major comments**\n* Ablation of $L_{con}^{+}$ in Table 6 tells a mixed story - it breaks down for the SNResNet discriminator, but seems to work well for the SNDCGAN discriminator. It's hard to conclude something definitive from the failed SNResNet experiment (could have worked with more hyper-parameter tuning?), but the successful SNResNet experiment suggests that the $L_{con}^{+}$ may not be necessary at all, which is at odds with the authors' narrative. Would be great to see more ablations showing that this is not the case, including a combined ablation of $L_{con}^{+}$ , $L_{con}^{-}$ and stop gradients.\n\n* It would be interesting to see linear separation and transfer to other datasets (as in the SimCLR paper) for the ImageNet models. Strong results there would support the authors' narrative of the synergy GANs and SimCLR training.\n* I am not fully convinced by the strong claims of strong coherence between GANs and contrastive representation learning (e.g. \"[...] these two representations greatly complement each other [...]\", \"[...] a good contrastive representation is good for GANs and vise versa\"). In particular it seems to be that GAN training benefits from contrastive learning (it seems to be a good auxiliary loss in presence of strong augmentation), but the claims of benefit to SimCLR don't seem fully substantiated.\n\n**Minor comments**\n* Multiple citations do not have correct capitalization (e.g. \"gan\")\n* Results presented in Table 3 (linear evaluation on CIFAR-10) can be a bit misleading - at first glance they seem to suggest that * ContraD improves on SimCLR results from the original paper (model pre-trained on ImageNet and transferred to CIFAR-10). Making it clear that this is not the case (e.g. by including the original SimCLR results and/or supervised performance) would be helpful to the readers.\n* Similarly, Table 4 would be easier to take in in the context of FID scores that are achieved by class-conditional GANs.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2902/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training GANs with Stronger Augmentations via Contrastive Discriminator", "authorids": ["~Jongheon_Jeong1", "~Jinwoo_Shin1"], "authors": ["Jongheon Jeong", "Jinwoo Shin"], "keywords": ["generative adversarial networks", "contrastive learning", "data augmentation", "visual representation learning", "unsupervised learning"], "abstract": "Recent works in Generative Adversarial Networks (GANs) are actively revisiting various data augmentation techniques as an effective way to prevent discriminator overfitting. It is still unclear, however, that which augmentations could actually improve GANs, and in particular, how to apply a wider range of augmentations in training. In this paper, we propose a novel way to address these questions by incorporating a recent contrastive representation learning scheme into the GAN discriminator, coined ContraD. This \"fusion\" enables the discriminators to work with much stronger augmentations without increasing their training instability, thereby preventing the discriminator overfitting issue in GANs more effectively. Even better, we observe that the contrastive learning itself also benefits from our GAN training, i.e., by maintaining discriminative features between real and fake samples, suggesting a strong coherence between the two worlds: good contrastive representations are also good for GAN discriminators, and vice versa. Our experimental results show that GANs with ContraD consistently improve FID and IS compared to other recent techniques incorporating data augmentations, still maintaining highly discriminative features in the discriminator in terms of the linear evaluation. Finally, as a byproduct, we also show that our GANs trained in an unsupervised manner (without labels) can induce many conditional generative models via a simple latent sampling, leveraging the learned features of ContraD. Code is available at https://github.com/jh-jeong/ContraD.", "one-sentence_summary": "We propose a novel discriminator of GAN showing that contrastive representation learning, e.g., SimCLR, and GAN can benefit each other when they are jointly trained. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jeong|training_gans_with_stronger_augmentations_via_contrastive_discriminator", "pdf": "/pdf/2d308c93802630f8c000471788307eb87a9027fd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\njeong2021training,\ntitle={Training {\\{}GAN{\\}}s with Stronger Augmentations via Contrastive Discriminator},\nauthor={Jongheon Jeong and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=eo6U4CAwVmg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "eo6U4CAwVmg", "replyto": "eo6U4CAwVmg", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2902/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086324, "tmdate": 1606915790470, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2902/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2902/-/Official_Review"}}}, {"id": "9EI99yALVt", "original": null, "number": 31, "cdate": 1606222465306, "ddate": null, "tcdate": 1606222465306, "tmdate": 1606222465306, "tddate": null, "forum": "eo6U4CAwVmg", "replyto": "_SmulYOzFmr", "invitation": "ICLR.cc/2021/Conference/Paper2902/-/Official_Comment", "content": {"title": "Response to R2 (3)", "comment": "Thanks for your thoughtful suggestion. We will include a more detailed discussion or experiments on ADA compared to our method in the final draft. We also express our gratitude for reconsidering your score on our manuscript! "}, "signatures": ["ICLR.cc/2021/Conference/Paper2902/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training GANs with Stronger Augmentations via Contrastive Discriminator", "authorids": ["~Jongheon_Jeong1", "~Jinwoo_Shin1"], "authors": ["Jongheon Jeong", "Jinwoo Shin"], "keywords": ["generative adversarial networks", "contrastive learning", "data augmentation", "visual representation learning", "unsupervised learning"], "abstract": "Recent works in Generative Adversarial Networks (GANs) are actively revisiting various data augmentation techniques as an effective way to prevent discriminator overfitting. It is still unclear, however, that which augmentations could actually improve GANs, and in particular, how to apply a wider range of augmentations in training. In this paper, we propose a novel way to address these questions by incorporating a recent contrastive representation learning scheme into the GAN discriminator, coined ContraD. This \"fusion\" enables the discriminators to work with much stronger augmentations without increasing their training instability, thereby preventing the discriminator overfitting issue in GANs more effectively. Even better, we observe that the contrastive learning itself also benefits from our GAN training, i.e., by maintaining discriminative features between real and fake samples, suggesting a strong coherence between the two worlds: good contrastive representations are also good for GAN discriminators, and vice versa. Our experimental results show that GANs with ContraD consistently improve FID and IS compared to other recent techniques incorporating data augmentations, still maintaining highly discriminative features in the discriminator in terms of the linear evaluation. Finally, as a byproduct, we also show that our GANs trained in an unsupervised manner (without labels) can induce many conditional generative models via a simple latent sampling, leveraging the learned features of ContraD. Code is available at https://github.com/jh-jeong/ContraD.", "one-sentence_summary": "We propose a novel discriminator of GAN showing that contrastive representation learning, e.g., SimCLR, and GAN can benefit each other when they are jointly trained. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jeong|training_gans_with_stronger_augmentations_via_contrastive_discriminator", "pdf": "/pdf/2d308c93802630f8c000471788307eb87a9027fd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\njeong2021training,\ntitle={Training {\\{}GAN{\\}}s with Stronger Augmentations via Contrastive Discriminator},\nauthor={Jongheon Jeong and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=eo6U4CAwVmg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "eo6U4CAwVmg", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2902/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2902/Authors|ICLR.cc/2021/Conference/Paper2902/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843290, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2902/-/Official_Comment"}}}, {"id": "gbroPcaasof", "original": null, "number": 1, "cdate": 1603893495919, "ddate": null, "tcdate": 1603893495919, "tmdate": 1606216354144, "tddate": null, "forum": "eo6U4CAwVmg", "replyto": "eo6U4CAwVmg", "invitation": "ICLR.cc/2021/Conference/Paper2902/-/Official_Review", "content": {"title": "A combination of GAN and contrastive learning, but with unaddressed concerns", "review": "The authors propose to improve GAN training by incorporating augmentations from contrastive learning. Specifically, a new contrastive discriminator, named ContraD, is proposed for GANs; with ContraD, the encoder part of the discriminator is trained with (two) contrastive learning losses, while the left discriminator head and the GAN generator are trained as usual. The authors argue that this specific fusion of GAN and contrastive learning signi\ufb01cantly stabilizes GAN training and, moreover, the fused two research fields could benefit each other.\n\nThe paper is well written overall. The idea is interesting, and the technical details may be sound. However, I am not fully convinced because of the concerns listed below.\n\u00a0\nAlthough the approach is proposed to stabilize GAN training, this aspect was not highlighted in the experiments. \u00a0\n\u00a0\nIn Section 3.1, the discriminator encoder is trained with two specifically-chosen self-supervised-learning losses, and the experiments show that such a specific combination is essential for a good performance; otherwise, \u201cthe GAN loss could completely negate the effectiveness of ContraD. \u201d So why would that happen? How to choose those losses in practice? Detailed discussions are necessary here.\n\u00a0\nThe first sentence of Section 3.2 is contradictory with Eq (12) and Algorithm 1.\n\u00a0\nIn Eq (14), how to choose the v for a specific class? Was the label information used to get the v?\n\u00a0\nIn Table 1, it seems the performance of StyleGAN2 is much lower than that reported in the original paper. Please elaborate on that.\n\u00a0\nIn the experiments, larger batch sizes are used for the proposed method. Will that affect the fairness? Discussions are necessary.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2902/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training GANs with Stronger Augmentations via Contrastive Discriminator", "authorids": ["~Jongheon_Jeong1", "~Jinwoo_Shin1"], "authors": ["Jongheon Jeong", "Jinwoo Shin"], "keywords": ["generative adversarial networks", "contrastive learning", "data augmentation", "visual representation learning", "unsupervised learning"], "abstract": "Recent works in Generative Adversarial Networks (GANs) are actively revisiting various data augmentation techniques as an effective way to prevent discriminator overfitting. It is still unclear, however, that which augmentations could actually improve GANs, and in particular, how to apply a wider range of augmentations in training. In this paper, we propose a novel way to address these questions by incorporating a recent contrastive representation learning scheme into the GAN discriminator, coined ContraD. This \"fusion\" enables the discriminators to work with much stronger augmentations without increasing their training instability, thereby preventing the discriminator overfitting issue in GANs more effectively. Even better, we observe that the contrastive learning itself also benefits from our GAN training, i.e., by maintaining discriminative features between real and fake samples, suggesting a strong coherence between the two worlds: good contrastive representations are also good for GAN discriminators, and vice versa. Our experimental results show that GANs with ContraD consistently improve FID and IS compared to other recent techniques incorporating data augmentations, still maintaining highly discriminative features in the discriminator in terms of the linear evaluation. Finally, as a byproduct, we also show that our GANs trained in an unsupervised manner (without labels) can induce many conditional generative models via a simple latent sampling, leveraging the learned features of ContraD. Code is available at https://github.com/jh-jeong/ContraD.", "one-sentence_summary": "We propose a novel discriminator of GAN showing that contrastive representation learning, e.g., SimCLR, and GAN can benefit each other when they are jointly trained. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jeong|training_gans_with_stronger_augmentations_via_contrastive_discriminator", "pdf": "/pdf/2d308c93802630f8c000471788307eb87a9027fd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\njeong2021training,\ntitle={Training {\\{}GAN{\\}}s with Stronger Augmentations via Contrastive Discriminator},\nauthor={Jongheon Jeong and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=eo6U4CAwVmg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "eo6U4CAwVmg", "replyto": "eo6U4CAwVmg", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2902/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086324, "tmdate": 1606915790470, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2902/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2902/-/Official_Review"}}}, {"id": "_SmulYOzFmr", "original": null, "number": 30, "cdate": 1606216307377, "ddate": null, "tcdate": 1606216307377, "tmdate": 1606216307377, "tddate": null, "forum": "eo6U4CAwVmg", "replyto": "ksybXJH7sR_", "invitation": "ICLR.cc/2021/Conference/Paper2902/-/Official_Comment", "content": {"title": "Thanks for the response", "comment": "Thanks for the detailed response. It will be great value if you can take the experiments under the ADA\u2019s setup into consideration, since ADA is also claimed to apply a wide range of augmentations. At least, a discussion (advantages and disadvantages) on comparing these two methods will be helpful for audience."}, "signatures": ["ICLR.cc/2021/Conference/Paper2902/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training GANs with Stronger Augmentations via Contrastive Discriminator", "authorids": ["~Jongheon_Jeong1", "~Jinwoo_Shin1"], "authors": ["Jongheon Jeong", "Jinwoo Shin"], "keywords": ["generative adversarial networks", "contrastive learning", "data augmentation", "visual representation learning", "unsupervised learning"], "abstract": "Recent works in Generative Adversarial Networks (GANs) are actively revisiting various data augmentation techniques as an effective way to prevent discriminator overfitting. It is still unclear, however, that which augmentations could actually improve GANs, and in particular, how to apply a wider range of augmentations in training. In this paper, we propose a novel way to address these questions by incorporating a recent contrastive representation learning scheme into the GAN discriminator, coined ContraD. This \"fusion\" enables the discriminators to work with much stronger augmentations without increasing their training instability, thereby preventing the discriminator overfitting issue in GANs more effectively. Even better, we observe that the contrastive learning itself also benefits from our GAN training, i.e., by maintaining discriminative features between real and fake samples, suggesting a strong coherence between the two worlds: good contrastive representations are also good for GAN discriminators, and vice versa. Our experimental results show that GANs with ContraD consistently improve FID and IS compared to other recent techniques incorporating data augmentations, still maintaining highly discriminative features in the discriminator in terms of the linear evaluation. Finally, as a byproduct, we also show that our GANs trained in an unsupervised manner (without labels) can induce many conditional generative models via a simple latent sampling, leveraging the learned features of ContraD. Code is available at https://github.com/jh-jeong/ContraD.", "one-sentence_summary": "We propose a novel discriminator of GAN showing that contrastive representation learning, e.g., SimCLR, and GAN can benefit each other when they are jointly trained. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jeong|training_gans_with_stronger_augmentations_via_contrastive_discriminator", "pdf": "/pdf/2d308c93802630f8c000471788307eb87a9027fd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\njeong2021training,\ntitle={Training {\\{}GAN{\\}}s with Stronger Augmentations via Contrastive Discriminator},\nauthor={Jongheon Jeong and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=eo6U4CAwVmg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "eo6U4CAwVmg", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2902/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2902/Authors|ICLR.cc/2021/Conference/Paper2902/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843290, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2902/-/Official_Comment"}}}, {"id": "CHppbK8-kOS", "original": null, "number": 28, "cdate": 1606185879827, "ddate": null, "tcdate": 1606185879827, "tmdate": 1606185879827, "tddate": null, "forum": "eo6U4CAwVmg", "replyto": "cZYbo5eZadr", "invitation": "ICLR.cc/2021/Conference/Paper2902/-/Official_Comment", "content": {"title": "Response to R4 (2)", "comment": "Thank you for your positive response before the discussion phase ends, and we are happy to hear that our response could help to address your questions! \n\nIndeed, we are currently working on higher resolution datasets as you suggested, and believe these additional results would greatly strengthen our paper. We hope for your understanding that we could not update the results in this phase due to limited time and resources, and we are willing to add them in the final draft. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2902/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training GANs with Stronger Augmentations via Contrastive Discriminator", "authorids": ["~Jongheon_Jeong1", "~Jinwoo_Shin1"], "authors": ["Jongheon Jeong", "Jinwoo Shin"], "keywords": ["generative adversarial networks", "contrastive learning", "data augmentation", "visual representation learning", "unsupervised learning"], "abstract": "Recent works in Generative Adversarial Networks (GANs) are actively revisiting various data augmentation techniques as an effective way to prevent discriminator overfitting. It is still unclear, however, that which augmentations could actually improve GANs, and in particular, how to apply a wider range of augmentations in training. In this paper, we propose a novel way to address these questions by incorporating a recent contrastive representation learning scheme into the GAN discriminator, coined ContraD. This \"fusion\" enables the discriminators to work with much stronger augmentations without increasing their training instability, thereby preventing the discriminator overfitting issue in GANs more effectively. Even better, we observe that the contrastive learning itself also benefits from our GAN training, i.e., by maintaining discriminative features between real and fake samples, suggesting a strong coherence between the two worlds: good contrastive representations are also good for GAN discriminators, and vice versa. Our experimental results show that GANs with ContraD consistently improve FID and IS compared to other recent techniques incorporating data augmentations, still maintaining highly discriminative features in the discriminator in terms of the linear evaluation. Finally, as a byproduct, we also show that our GANs trained in an unsupervised manner (without labels) can induce many conditional generative models via a simple latent sampling, leveraging the learned features of ContraD. Code is available at https://github.com/jh-jeong/ContraD.", "one-sentence_summary": "We propose a novel discriminator of GAN showing that contrastive representation learning, e.g., SimCLR, and GAN can benefit each other when they are jointly trained. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jeong|training_gans_with_stronger_augmentations_via_contrastive_discriminator", "pdf": "/pdf/2d308c93802630f8c000471788307eb87a9027fd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\njeong2021training,\ntitle={Training {\\{}GAN{\\}}s with Stronger Augmentations via Contrastive Discriminator},\nauthor={Jongheon Jeong and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=eo6U4CAwVmg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "eo6U4CAwVmg", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2902/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2902/Authors|ICLR.cc/2021/Conference/Paper2902/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843290, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2902/-/Official_Comment"}}}, {"id": "ksybXJH7sR_", "original": null, "number": 25, "cdate": 1606137945185, "ddate": null, "tcdate": 1606137945185, "tmdate": 1606182877203, "tddate": null, "forum": "eo6U4CAwVmg", "replyto": "thlGma-9lFm", "invitation": "ICLR.cc/2021/Conference/Paper2902/-/Official_Comment", "content": {"title": "Response to R2 (2)", "comment": "Many thanks for clarifying your question before the discussion phase ends. \n\nAs mentioned in the previous response, we follow the StyleGAN2 experiment setup of DiffAug [Zhao et al., 2020] that is a highly concurrent work of ADA [Karras et al., 2020] that you are referring to. \n\nOur results in Table 1 (and DiffAug) can be lower than those reported in ADA [Karras et al., 2020], as the detailed experimental setups are largely different in many aspects: \n\n- (a) - ADA uses 4x longer training than ours (and DiffAug) with many different hyperparameters, e.g., larger learning rate. The actual architectures are slightly different as well: ADA uses a larger one.  \n- (b) - ADA computes FIDs differently to ours (and DiffAug). More specifically, we compute FID of 10K generated samples vs. 10K \"test\" samples of CIFAR, while ADA does of 50k generated samples vs. all available real samples, i.e., 60K \"train+test\" samples: FID tends to be smaller on larger sample sizes, as done in ADA. Nevertheless, we believe our criteria of using only test samples is a more standard and reasonable way of computing FIDs, as also argued in prior works [Lucic et al., 2018; Kurach et al., 2019].\n- (c) - The two concurrent methods, ADA and DiffAug, are very similar, except for ADA\u2019s additional \"dynamic\" augmentation scheme driven from validation samples: i.e., the setup would assume an extra split for the validation. As also mentioned (as a footnote) in p.6, this is another reason why we compare DiffAug instead of ADA for a clearer evaluation: our goal is to verify which method better incorporates a given (fixed) augmentation.\n\nHence, especially due to (b) and (c), we have decided to follow DiffAug rather than ADA for designing our experimental setups, and think our current evaluation does verify the effectiveness of our method on StyleGAN2. However, if you think experiments under the ADA\u2019s setup are valuable to add, we will incorporate the results in the final draft (please understand that it will not be possible to update them until the end of the discussion period, i.e., in 2 days, considering that these experiments would roughly take ~1000 GPU*hrs for each training configuration). \n\n---\n- [Lucic et al., 2018] Are GANs Created Equal? A Large-Scale Study, NeurIPS 2018.\n- [Kurach et al., 2019] A Large-Scale Study on Regularization and Normalization in GANs, ICML 2019.\n- [Zhao et al., 2020] Differentiable Augmentation for Data-Efficient GAN Training, NeurIPS 2020.\n- [Karras et al., 2020] Training Generative Adversarial Networks with Limited Data, NeurIPS 2020.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2902/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training GANs with Stronger Augmentations via Contrastive Discriminator", "authorids": ["~Jongheon_Jeong1", "~Jinwoo_Shin1"], "authors": ["Jongheon Jeong", "Jinwoo Shin"], "keywords": ["generative adversarial networks", "contrastive learning", "data augmentation", "visual representation learning", "unsupervised learning"], "abstract": "Recent works in Generative Adversarial Networks (GANs) are actively revisiting various data augmentation techniques as an effective way to prevent discriminator overfitting. It is still unclear, however, that which augmentations could actually improve GANs, and in particular, how to apply a wider range of augmentations in training. In this paper, we propose a novel way to address these questions by incorporating a recent contrastive representation learning scheme into the GAN discriminator, coined ContraD. This \"fusion\" enables the discriminators to work with much stronger augmentations without increasing their training instability, thereby preventing the discriminator overfitting issue in GANs more effectively. Even better, we observe that the contrastive learning itself also benefits from our GAN training, i.e., by maintaining discriminative features between real and fake samples, suggesting a strong coherence between the two worlds: good contrastive representations are also good for GAN discriminators, and vice versa. Our experimental results show that GANs with ContraD consistently improve FID and IS compared to other recent techniques incorporating data augmentations, still maintaining highly discriminative features in the discriminator in terms of the linear evaluation. Finally, as a byproduct, we also show that our GANs trained in an unsupervised manner (without labels) can induce many conditional generative models via a simple latent sampling, leveraging the learned features of ContraD. Code is available at https://github.com/jh-jeong/ContraD.", "one-sentence_summary": "We propose a novel discriminator of GAN showing that contrastive representation learning, e.g., SimCLR, and GAN can benefit each other when they are jointly trained. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jeong|training_gans_with_stronger_augmentations_via_contrastive_discriminator", "pdf": "/pdf/2d308c93802630f8c000471788307eb87a9027fd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\njeong2021training,\ntitle={Training {\\{}GAN{\\}}s with Stronger Augmentations via Contrastive Discriminator},\nauthor={Jongheon Jeong and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=eo6U4CAwVmg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "eo6U4CAwVmg", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2902/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2902/Authors|ICLR.cc/2021/Conference/Paper2902/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843290, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2902/-/Official_Comment"}}}, {"id": "cZYbo5eZadr", "original": null, "number": 27, "cdate": 1606172279769, "ddate": null, "tcdate": 1606172279769, "tmdate": 1606172279769, "tddate": null, "forum": "eo6U4CAwVmg", "replyto": "CBVk3av3VB7", "invitation": "ICLR.cc/2021/Conference/Paper2902/-/Official_Comment", "content": {"title": "  ", "comment": "I appreciate the detailed responses that the authors have provided to the questions and concerns I raised. I am particularly satisfied with the new experiments conducted to address questions 3 and 5, providing further evidence for some of the claims made by the authors.\nI hope further experiments on higher resolution datasets will obtain similar conclusions.\n\nRegarding the raised concern about the methodological novelties, I agree with the reviewers that the proposed combination is novel, but I still think this is not sufficient to consider strong methodological novelties as a major strength for this paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper2902/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training GANs with Stronger Augmentations via Contrastive Discriminator", "authorids": ["~Jongheon_Jeong1", "~Jinwoo_Shin1"], "authors": ["Jongheon Jeong", "Jinwoo Shin"], "keywords": ["generative adversarial networks", "contrastive learning", "data augmentation", "visual representation learning", "unsupervised learning"], "abstract": "Recent works in Generative Adversarial Networks (GANs) are actively revisiting various data augmentation techniques as an effective way to prevent discriminator overfitting. It is still unclear, however, that which augmentations could actually improve GANs, and in particular, how to apply a wider range of augmentations in training. In this paper, we propose a novel way to address these questions by incorporating a recent contrastive representation learning scheme into the GAN discriminator, coined ContraD. This \"fusion\" enables the discriminators to work with much stronger augmentations without increasing their training instability, thereby preventing the discriminator overfitting issue in GANs more effectively. Even better, we observe that the contrastive learning itself also benefits from our GAN training, i.e., by maintaining discriminative features between real and fake samples, suggesting a strong coherence between the two worlds: good contrastive representations are also good for GAN discriminators, and vice versa. Our experimental results show that GANs with ContraD consistently improve FID and IS compared to other recent techniques incorporating data augmentations, still maintaining highly discriminative features in the discriminator in terms of the linear evaluation. Finally, as a byproduct, we also show that our GANs trained in an unsupervised manner (without labels) can induce many conditional generative models via a simple latent sampling, leveraging the learned features of ContraD. Code is available at https://github.com/jh-jeong/ContraD.", "one-sentence_summary": "We propose a novel discriminator of GAN showing that contrastive representation learning, e.g., SimCLR, and GAN can benefit each other when they are jointly trained. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jeong|training_gans_with_stronger_augmentations_via_contrastive_discriminator", "pdf": "/pdf/2d308c93802630f8c000471788307eb87a9027fd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\njeong2021training,\ntitle={Training {\\{}GAN{\\}}s with Stronger Augmentations via Contrastive Discriminator},\nauthor={Jongheon Jeong and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=eo6U4CAwVmg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "eo6U4CAwVmg", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2902/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2902/Authors|ICLR.cc/2021/Conference/Paper2902/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843290, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2902/-/Official_Comment"}}}, {"id": "thlGma-9lFm", "original": null, "number": 23, "cdate": 1606106232881, "ddate": null, "tcdate": 1606106232881, "tmdate": 1606106434495, "tddate": null, "forum": "eo6U4CAwVmg", "replyto": "udrw3p_xDjk", "invitation": "ICLR.cc/2021/Conference/Paper2902/-/Official_Comment", "content": {"title": "On Q5, the performance over the work based on StyleGAN2  [karras2020training]", "comment": "I have made a mistake about the reference paper, sorry. Actually, I mean the work based on StyleGAN2  [karras2020training] has shown strong performance on CIFAR10 with FID=2.67 and IS=10.06, which is much better than the performance show in table 1, please elaborate on that.\n\n[karras2020training] Training Generative Adversarial Networks with Limited Data"}, "signatures": ["ICLR.cc/2021/Conference/Paper2902/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training GANs with Stronger Augmentations via Contrastive Discriminator", "authorids": ["~Jongheon_Jeong1", "~Jinwoo_Shin1"], "authors": ["Jongheon Jeong", "Jinwoo Shin"], "keywords": ["generative adversarial networks", "contrastive learning", "data augmentation", "visual representation learning", "unsupervised learning"], "abstract": "Recent works in Generative Adversarial Networks (GANs) are actively revisiting various data augmentation techniques as an effective way to prevent discriminator overfitting. It is still unclear, however, that which augmentations could actually improve GANs, and in particular, how to apply a wider range of augmentations in training. In this paper, we propose a novel way to address these questions by incorporating a recent contrastive representation learning scheme into the GAN discriminator, coined ContraD. This \"fusion\" enables the discriminators to work with much stronger augmentations without increasing their training instability, thereby preventing the discriminator overfitting issue in GANs more effectively. Even better, we observe that the contrastive learning itself also benefits from our GAN training, i.e., by maintaining discriminative features between real and fake samples, suggesting a strong coherence between the two worlds: good contrastive representations are also good for GAN discriminators, and vice versa. Our experimental results show that GANs with ContraD consistently improve FID and IS compared to other recent techniques incorporating data augmentations, still maintaining highly discriminative features in the discriminator in terms of the linear evaluation. Finally, as a byproduct, we also show that our GANs trained in an unsupervised manner (without labels) can induce many conditional generative models via a simple latent sampling, leveraging the learned features of ContraD. Code is available at https://github.com/jh-jeong/ContraD.", "one-sentence_summary": "We propose a novel discriminator of GAN showing that contrastive representation learning, e.g., SimCLR, and GAN can benefit each other when they are jointly trained. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jeong|training_gans_with_stronger_augmentations_via_contrastive_discriminator", "pdf": "/pdf/2d308c93802630f8c000471788307eb87a9027fd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\njeong2021training,\ntitle={Training {\\{}GAN{\\}}s with Stronger Augmentations via Contrastive Discriminator},\nauthor={Jongheon Jeong and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=eo6U4CAwVmg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "eo6U4CAwVmg", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2902/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2902/Authors|ICLR.cc/2021/Conference/Paper2902/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843290, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2902/-/Official_Comment"}}}, {"id": "cCY1tXOW5Y9", "original": null, "number": 17, "cdate": 1605883115760, "ddate": null, "tcdate": 1605883115760, "tmdate": 1605941192358, "tddate": null, "forum": "eo6U4CAwVmg", "replyto": "eo6U4CAwVmg", "invitation": "ICLR.cc/2021/Conference/Paper2902/-/Official_Comment", "content": {"title": "Summary of Revisions", "comment": "Dear reviewers, \n\nMany thanks again for your constructive feedback to improve our manuscript. We have carefully incorporated your comments into this revision, as summarized in what follows:\n\n- Linear evaluation and transfer learning on ImageNet models (Appendix B; R3)\n- Additional ablations (Section 4.2): ContraD + \"HFlip, Trans\" (R1, R4) / \"$h_{\\tt r}=h_{\\tt f}$\" (R4)\n- Discussion on the choice of $L_{\\tt con}^{-}$ (Section 3.1; R1, R2)\n- Improved clarity: Table 3 (R1, R3) / Section 3.2 (R2) / Figure 1\n- Improved clarity in mathematical notations, including a typo in Eq. 7 (R4)\n- Comments on technical contributions (Section 5; R1, R4)\n- Comments on \"ContraD is beneficial to SimCLR\" (Section 5; R3) \n- Minor fixes and comments: \"... stabilize GAN \u2026\" $\\rightarrow$ \"... improve GAN ...\" (R2) / \"gan\" $\\rightarrow$ \"GAN\" in the citations (R3) / a typo (R4)\n\nThese updates are temporarily highlighted in \"red\" for your convenience.\n\nIf you have time, please check this revised manuscript and our previous response, and let us know if there are any other concerns to be clarified. We will be happy to respond to your further comments during the remainder of the author discussion period.\n\nBest regards,\n\nAuthors"}, "signatures": ["ICLR.cc/2021/Conference/Paper2902/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training GANs with Stronger Augmentations via Contrastive Discriminator", "authorids": ["~Jongheon_Jeong1", "~Jinwoo_Shin1"], "authors": ["Jongheon Jeong", "Jinwoo Shin"], "keywords": ["generative adversarial networks", "contrastive learning", "data augmentation", "visual representation learning", "unsupervised learning"], "abstract": "Recent works in Generative Adversarial Networks (GANs) are actively revisiting various data augmentation techniques as an effective way to prevent discriminator overfitting. It is still unclear, however, that which augmentations could actually improve GANs, and in particular, how to apply a wider range of augmentations in training. In this paper, we propose a novel way to address these questions by incorporating a recent contrastive representation learning scheme into the GAN discriminator, coined ContraD. This \"fusion\" enables the discriminators to work with much stronger augmentations without increasing their training instability, thereby preventing the discriminator overfitting issue in GANs more effectively. Even better, we observe that the contrastive learning itself also benefits from our GAN training, i.e., by maintaining discriminative features between real and fake samples, suggesting a strong coherence between the two worlds: good contrastive representations are also good for GAN discriminators, and vice versa. Our experimental results show that GANs with ContraD consistently improve FID and IS compared to other recent techniques incorporating data augmentations, still maintaining highly discriminative features in the discriminator in terms of the linear evaluation. Finally, as a byproduct, we also show that our GANs trained in an unsupervised manner (without labels) can induce many conditional generative models via a simple latent sampling, leveraging the learned features of ContraD. Code is available at https://github.com/jh-jeong/ContraD.", "one-sentence_summary": "We propose a novel discriminator of GAN showing that contrastive representation learning, e.g., SimCLR, and GAN can benefit each other when they are jointly trained. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jeong|training_gans_with_stronger_augmentations_via_contrastive_discriminator", "pdf": "/pdf/2d308c93802630f8c000471788307eb87a9027fd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\njeong2021training,\ntitle={Training {\\{}GAN{\\}}s with Stronger Augmentations via Contrastive Discriminator},\nauthor={Jongheon Jeong and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=eo6U4CAwVmg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "eo6U4CAwVmg", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2902/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2902/Authors|ICLR.cc/2021/Conference/Paper2902/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843290, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2902/-/Official_Comment"}}}, {"id": "wBJI5oXyTL4", "original": null, "number": 14, "cdate": 1605517869446, "ddate": null, "tcdate": 1605517869446, "tmdate": 1605607224691, "tddate": null, "forum": "eo6U4CAwVmg", "replyto": "wWtYkekwup", "invitation": "ICLR.cc/2021/Conference/Paper2902/-/Official_Comment", "content": {"title": "Response to R4 (1/2)", "comment": "We sincerely appreciate your thoughtful comments, efforts, and time. We respond to each of your questions and concerns one-by-one in what follows: possibly, some questions that need additional expensive experiments to address may not be included in the current response, but they will be supplemented after a revised manuscript is updated. We hope to deliver the revision soon, where we will make sure to reflect all comments. \n\n---\n**Q1. Methodological contributions**\n\nWe first note that GAN and SimCLR have been two independently-developed methods, and combining them as a new working method can be highly non-trivial: even, there is a challenge in engineering them, as the popular experimental setups (e.g., the choice of neural architectures or batch sizes) for GAN and SimCLR are different and their performance is highly sensitive to them.\n\nOur key ideas have enabled the successful combination of GAN and SimCLR (as supported in our ablation study of Section 4.2), as well as they give many insights to both techniques: for example, our new design of introducing a small header to minimize the GAN loss upon other (e.g., contrastive) representation is a promising yet unexplored way of designing a new GAN architecture. For the SimCLR side, on the other hand, we suggest a new idea of incorporating \"fake\" samples for contrastive learning, also an interesting direction along with other recent attempts to improve the efficiency of negative sampling in contrastive learning, e.g., via hard negative mining [Kalantidis et al., 2020; Robinson et al., 2020]. Overall, we do believe that our work sheds a new angle to both worlds, GAN and SimCLR.\n\nWe will add respective comments to the revision.\n\n---\n**Q2. Results on more challenging datasets?**\n\nFor your information, we note here that our experiments have also covered ImageNet with BigGAN architecture [Brock et al., 2019] in Appendix B. Nevertheless, following your suggestion, we will continue to work on additional higher resolution datasets and deliver their results in the revision (if time permits) or the final draft.\n\n---\n**Q3. ContraD without SimCLR augmentation?**\n\nFollowing the suggestion made by you (and R1), we will update Table 5 in the revision to include an additional ablation of ContraD with a weaker augmentation of \"HFlip, Trans\": as summarized below, we observe that ContraD is still as good as (or better in terms of IS) a strong baseline of \"balanced consistency regularization\" (bCR) [Zhao et al., 2020]. The performance degradation compared to \"ContraD + SimCLR\" is due to that SimCLR requires strong augmentations to learn a good representation [Chen et al., 2020], i.e., ContraD with weaker augmentations learns worse representation in terms of its linear evaluation performance as shown in the following table.\n\n\n| Method  |   Augment.   | FID $\\downarrow$ |  IS $\\uparrow$ | Lin. Eval. (%) |\n|:--------|:------------:|:----:|:----:|:--------------:|\n| -       | -            | 26.6 | 7.38 |       -        |\n| bCR\t    | HFlip, Trans | 14.0 | 8.35 |       -        |\n| bCR + Aug.  | SimCLR       | 20.6 | 7.44 |       -        |\n| ContraD | SimCLR       | 10.9 | 8.78 |  77.5 $\\pm$ 0.20  |\n| ContraD - Aug.  | HFlip, Trans | 13.7 | 8.54 |  72.9 $\\pm$ 0.04  |\n\n---\n**Q4. Comparison with ADA [Karras et al., 2020]?**\n\nAs mentioned (as a footnote) in p.6, we do not compare with ADA [Karras et al., 2020] but do compare with its concurrent work, namely DiffAug [Zhao et al., 2020]. It is mainly for a clearer comparison of which method better incorporates a given (fixed) augmentation: ADA offers a similar method to DiffAug, but with an additional \"dynamic\" augmentation scheme using a validation dataset, which can be viewed as an orthogonal idea to our method. \n\n---\n**Q5. What if $h_f = h_r$?**\n\nWe have empirically observed that having separate projection headers for $h_f$ and $h_r$ is empirically more stable to train than sharing them: e.g., under $h_f = h_r$, we could not obtain a reasonable performance with ContraD at \"G: SNDCGAN / D: SNResNet-18\" on CIFAR-10 (FID ($\\downarrow$) / IS ($\\uparrow$) = 194.6 / 3.02). Intuitively, an optimal embedding (after projection) from the supervised contrastive loss ($L_{\\tt con}^{-}$) can be harmful to those from SimCLR, as the former would encourage the embeddings of real samples to shrink into a single point, which is, in some sense, the opposite direction of SimCLR. We will add the respective discussion and results in the revision.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2902/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training GANs with Stronger Augmentations via Contrastive Discriminator", "authorids": ["~Jongheon_Jeong1", "~Jinwoo_Shin1"], "authors": ["Jongheon Jeong", "Jinwoo Shin"], "keywords": ["generative adversarial networks", "contrastive learning", "data augmentation", "visual representation learning", "unsupervised learning"], "abstract": "Recent works in Generative Adversarial Networks (GANs) are actively revisiting various data augmentation techniques as an effective way to prevent discriminator overfitting. It is still unclear, however, that which augmentations could actually improve GANs, and in particular, how to apply a wider range of augmentations in training. In this paper, we propose a novel way to address these questions by incorporating a recent contrastive representation learning scheme into the GAN discriminator, coined ContraD. This \"fusion\" enables the discriminators to work with much stronger augmentations without increasing their training instability, thereby preventing the discriminator overfitting issue in GANs more effectively. Even better, we observe that the contrastive learning itself also benefits from our GAN training, i.e., by maintaining discriminative features between real and fake samples, suggesting a strong coherence between the two worlds: good contrastive representations are also good for GAN discriminators, and vice versa. Our experimental results show that GANs with ContraD consistently improve FID and IS compared to other recent techniques incorporating data augmentations, still maintaining highly discriminative features in the discriminator in terms of the linear evaluation. Finally, as a byproduct, we also show that our GANs trained in an unsupervised manner (without labels) can induce many conditional generative models via a simple latent sampling, leveraging the learned features of ContraD. Code is available at https://github.com/jh-jeong/ContraD.", "one-sentence_summary": "We propose a novel discriminator of GAN showing that contrastive representation learning, e.g., SimCLR, and GAN can benefit each other when they are jointly trained. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jeong|training_gans_with_stronger_augmentations_via_contrastive_discriminator", "pdf": "/pdf/2d308c93802630f8c000471788307eb87a9027fd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\njeong2021training,\ntitle={Training {\\{}GAN{\\}}s with Stronger Augmentations via Contrastive Discriminator},\nauthor={Jongheon Jeong and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=eo6U4CAwVmg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "eo6U4CAwVmg", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2902/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2902/Authors|ICLR.cc/2021/Conference/Paper2902/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843290, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2902/-/Official_Comment"}}}, {"id": "SjEC9UWf-Vg", "original": null, "number": 12, "cdate": 1605517444114, "ddate": null, "tcdate": 1605517444114, "tmdate": 1605607205182, "tddate": null, "forum": "eo6U4CAwVmg", "replyto": "fqyvlqreIbH", "invitation": "ICLR.cc/2021/Conference/Paper2902/-/Official_Comment", "content": {"title": "Response to R1", "comment": "We sincerely appreciate your thoughtful comments, efforts, and time. We respond to each of your questions and concerns one-by-one in what follows: possibly, some questions that need additional expensive experiments to address may not be included in the current response, but they will be supplemented after a revised manuscript is updated. We hope to deliver the revision soon, where we will make sure to reflect all comments.\n\n---\n**Q1. Limited technical contributions**\n\nWe first note that GAN and SimCLR have been two independently-developed methods, and combining them as a new working method can be highly non-trivial: even, there is a challenge in engineering them, as the popular experimental setups (e.g., the choice of neural architectures or batch sizes) for GAN and SimCLR are different and their performance is highly sensitive to them.\n\nOur key ideas have enabled the successful combination of GAN and SimCLR (as supported in our ablation study of Section 4.2), as well as they give many insights to both techniques: for example, our new design of introducing a small header to minimize the GAN loss upon other (e.g., contrastive) representation is a promising yet unexplored way of designing a new GAN architecture. For the SimCLR side, on the other hand, we suggest a new idea of incorporating \"fake\" samples for contrastive learning, also an interesting direction along with other recent attempts to improve the efficiency of negative sampling in contrastive learning, e.g., via hard negative mining [Kalantidis et al., 2020; Robinson et al., 2020]. Overall, we do believe that our work sheds a new angle to both worlds, GAN and SimCLR.\n\nWe will add respective comments to the revision.\n\n---\n**Q2. What if $L_{\\tt con}^{-}$ uses another view of fake samples, in a similar manner to $L_{\\tt con}^{+}$?**\n\nMany thanks for an insightful comment. We have actually tried several possible variants of $L_{\\tt con}^{-}$ including \u201cmulti-view\u201d losses as you suggested, but could not find a significant advantage compared to the current form in terms of the final FIDs. Intuitively, the supervised contrastive loss used for $L_{\\tt con}^{-}$ already encourages different augmentations of a fake sample (along with other samples as well) to be similar in the normalized embedding space implicitly, so that an explicit loss for this may be redundant.\n\nHence, the current design is minimal and more beneficial in terms of computational efficiency, as using more and different views in the loss would increase memory consumption and forward computation in training. \n\nWe will add respective comments to the revision.\n\n---\n**Q3. ContraD with \"HFlip, Trans\"?**\n\nFollowing the suggestion made by you (and R4), we will update Table 5 in the revision to include an additional ablation of ContraD with a weaker augmentation of \"HFlip, Trans\": as summarized below, we observe that ContraD is still as good as (or better in terms of IS) a strong baseline of \"balanced consistency regularization\" (bCR) [Zhao et al., 2020]. The performance degradation compared to \"ContraD + SimCLR\" is due to that SimCLR requires strong augmentations to learn a good representation [Chen et al., 2020], i.e., ContraD with weaker augmentations learns worse representation in terms of its linear evaluation performance as reported in the following table.\n\n| Method  |   Augment.   | FID $\\downarrow$ |  IS $\\uparrow$ | Lin. Eval. (%) |\n|:--------|:------------:|:----:|:----:|:--------------:|\n| -       | -            | 26.6 | 7.38 |       -        |\n| bCR\t    | HFlip, Trans | 14.0 | 8.35 |       -        |\n| bCR + Aug.  | SimCLR       | 20.6 | 7.44 |       -        |\n| ContraD | SimCLR       | 10.9 | 8.78 |  77.5 $\\pm$ 0.20  |\n| ContraD - Aug.  | HFlip, Trans | 13.7 | 8.54 |  72.9 $\\pm$ 0.04  |\n\n---\n**Q4. Discrepancy between \"SimCLR\" in Table 3 and the original paper [Chen et al., 2020]?**\n\nThe discrepancy is because we use different training setups: e.g., we use three popular GAN discriminator architectures, namely SNDCGAN, SNResNet-18, and StyleGAN2 for training (from scratch), while the original SimCLR paper uses ResNet-50 [Chen et al., 2020]. As we put more quantitative efforts in our experiments to improve generation quality of GANs rather than representation quality of SimCLR, we think these prior GAN setups are more valuable to test.\n\nMore specifically, as mentioned in Section 4.1, we denote \"SimCLR\" in Table 3 to indicate an ablation of ContraD when $\\lambda_{\\tt con} = \\lambda_{\\tt dis} = 0$, i.e., D is only optimized via $L_{\\tt con}^{+}$ (Eq. 6), the SimCLR loss, without using fake samples. We will clarify this in the revision. \n\n---\n- [Chen et al., 2020] A Simple Framework for Contrastive Learning of Visual Representations, ICML 2020.\n- [Kalantidis et al., 2020] Hard Negative Mixing for Contrastive Learning, NeurIPS 2020.\n- [Robinson et al., 2020] Contrastive Learning with Hard Negative Samples, 2020.\n- [Zhao et al., 2020] Improved Consistency Regularization for GANs, 2020.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2902/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training GANs with Stronger Augmentations via Contrastive Discriminator", "authorids": ["~Jongheon_Jeong1", "~Jinwoo_Shin1"], "authors": ["Jongheon Jeong", "Jinwoo Shin"], "keywords": ["generative adversarial networks", "contrastive learning", "data augmentation", "visual representation learning", "unsupervised learning"], "abstract": "Recent works in Generative Adversarial Networks (GANs) are actively revisiting various data augmentation techniques as an effective way to prevent discriminator overfitting. It is still unclear, however, that which augmentations could actually improve GANs, and in particular, how to apply a wider range of augmentations in training. In this paper, we propose a novel way to address these questions by incorporating a recent contrastive representation learning scheme into the GAN discriminator, coined ContraD. This \"fusion\" enables the discriminators to work with much stronger augmentations without increasing their training instability, thereby preventing the discriminator overfitting issue in GANs more effectively. Even better, we observe that the contrastive learning itself also benefits from our GAN training, i.e., by maintaining discriminative features between real and fake samples, suggesting a strong coherence between the two worlds: good contrastive representations are also good for GAN discriminators, and vice versa. Our experimental results show that GANs with ContraD consistently improve FID and IS compared to other recent techniques incorporating data augmentations, still maintaining highly discriminative features in the discriminator in terms of the linear evaluation. Finally, as a byproduct, we also show that our GANs trained in an unsupervised manner (without labels) can induce many conditional generative models via a simple latent sampling, leveraging the learned features of ContraD. Code is available at https://github.com/jh-jeong/ContraD.", "one-sentence_summary": "We propose a novel discriminator of GAN showing that contrastive representation learning, e.g., SimCLR, and GAN can benefit each other when they are jointly trained. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jeong|training_gans_with_stronger_augmentations_via_contrastive_discriminator", "pdf": "/pdf/2d308c93802630f8c000471788307eb87a9027fd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\njeong2021training,\ntitle={Training {\\{}GAN{\\}}s with Stronger Augmentations via Contrastive Discriminator},\nauthor={Jongheon Jeong and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=eo6U4CAwVmg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "eo6U4CAwVmg", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2902/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2902/Authors|ICLR.cc/2021/Conference/Paper2902/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843290, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2902/-/Official_Comment"}}}, {"id": "udrw3p_xDjk", "original": null, "number": 16, "cdate": 1605518131053, "ddate": null, "tcdate": 1605518131053, "tmdate": 1605518131053, "tddate": null, "forum": "eo6U4CAwVmg", "replyto": "gbroPcaasof", "invitation": "ICLR.cc/2021/Conference/Paper2902/-/Official_Comment", "content": {"title": "Response to R2", "comment": "We sincerely appreciate your thoughtful comments, efforts, and time. We respond to each of your questions and concerns one-by-one in what follows: possibly, some questions that need additional expensive experiments to address may not be included in the current response, but they will be supplemented after a revised manuscript is updated. We hope to deliver the revision soon, where we will make sure to reflect all comments. \n\n---\n**Q1. \"Although the approach is proposed to stabilize GAN training, this aspect was not highlighted in the experiments.\"**\n\nWe use the term \"stability\" in a broad sense, e.g., our experiments view this in terms of the final performance of GAN after the training: the high-fidelity (e.g. high IS), yet diverse (e.g. low FID) generation from a GAN cannot be achieved with an instability (e.g., mode collapse) in training. Nevertheless, we agree that the term of stability in GAN training might be confusing for some readers, and we will clarify this in the revision.\n\n---\n**Q2. \"... the GAN loss could completely negate the effectiveness of ContraD. ...\": why does it happen?**\n\nRecall that the encoder $D$ of ContraD minimizes two losses: namely, $L_{\\tt con}^{+}$ and $L_{\\tt con}^{-}$ (but not $L_{\\tt dis}$), and $L_{\\tt con}^{+}$ is simply the SimCLR loss [Chen et al., 2020] (as mentioned in Section 3.1). Here, the referred line is to emphasize the importance of choosing $L_{\\tt con}^{-}$: the role of $L_{\\tt con}^{-}$ is (a) forcing $D$ to be able to discriminate real vs. fake samples, while (b) not compromising the representation from $L_{\\tt con}^{+}$. One of our findings is that our proposed supervised contrastive form (Eq. 8) of loss does satisfy both of (a) and (b): the pure GAN loss as $L_{\\tt con}^{-}$ may satisfy (a), but not (b). \n\n---\n**Q3. \"The first sentence of Section 3.2 is contradictory with Eq. 12 and Algorithm 1.\"**\n\nThank you for pointing out this detail. We note that the generator loss of ContraD (Eq. 12) is indeed equivalent to the original non-saturating loss if we regard $\\sigma(h_{\\tt d}(D(\\cdot)))$ as a single discriminator. The difference in formula occurs since we re-define $D$ to be vector-valued (Section 3.1) to explain ContraD. We will clarify this in the revision.\n\n---\n**Q4. How to choose the $v$ in Eq. 14 for a specific class? Was the label information used to get the $v$?**\n\nIn our experiments, we do use the label information to find a class-wise $v$: namely, we simply take the linear weights learned from a fine-tuning upon the ContraD representation  (e.g. linear evaluation protocol), as also mentioned in p.7. Nevertheless, in general, the notion of $v$ is independent to labels, e.g., one can apply an unsupervised clustering method on ContraD to obtain another $v$. An important point here is that such a label information is completely unused when training the base ContraD.\n\n---\n**Q5. \"The performance of StyleGAN2 in Table 1 is much lower than that reported in the original paper [Karras et al., 2020]?\"**\n\nTo the best of our knowledge, the original StyleGAN2 paper [Karras et al., 2020] does not perform CIFAR-10/100 experiments. Hence, the reported CIFAR-10/100 experimental results (and detailed setups) for StyleGAN2 are instead from DiffAug [Zhao et al., 2020] (Appendix E for the details) for a fair comparison in Table 1, where we have confirmed the reproducibility of them. \n\n---\n**Q6. Larger batch sizes for ContraD - will that affect the fairness?**\n\nWe actually investigated the detailed effect of batch sizes on the standard, bCR, and ContraD training in Appendix D, confirming that the benefit from larger batch sizes is not common in GAN training: e.g., we observe the standard GAN training works best at the smallest batch size of 64, which is also consistent with the popular practices in GAN [Lucic et al., 2018; Kurach et al., 2019]. As mentioned in Section 4, the choice of larger batch for ContraD rather comes from integrating SimCLR into GAN, which is our unique perspective. \n\n---\n- [Lucic et al., 2018] Are GANs Created Equal? A Large-Scale Study, NeurIPS 2018.\n- [Kurach et al., 2019] A Large-Scale Study on Regularization and Normalization in GANs, ICML 2019.\n- [Chen et al., 2020] A Simple Framework for Contrastive Learning of Visual Representations, ICML 2020.\n- [Karras et al., 2020] Analyzing and Improving the Image Quality of StyleGAN, CVPR 2020.\n- [Zhao et al., 2020] Differentiable Augmentation for Data-Efficient GAN Training, NeurIPS 2020.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2902/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training GANs with Stronger Augmentations via Contrastive Discriminator", "authorids": ["~Jongheon_Jeong1", "~Jinwoo_Shin1"], "authors": ["Jongheon Jeong", "Jinwoo Shin"], "keywords": ["generative adversarial networks", "contrastive learning", "data augmentation", "visual representation learning", "unsupervised learning"], "abstract": "Recent works in Generative Adversarial Networks (GANs) are actively revisiting various data augmentation techniques as an effective way to prevent discriminator overfitting. It is still unclear, however, that which augmentations could actually improve GANs, and in particular, how to apply a wider range of augmentations in training. In this paper, we propose a novel way to address these questions by incorporating a recent contrastive representation learning scheme into the GAN discriminator, coined ContraD. This \"fusion\" enables the discriminators to work with much stronger augmentations without increasing their training instability, thereby preventing the discriminator overfitting issue in GANs more effectively. Even better, we observe that the contrastive learning itself also benefits from our GAN training, i.e., by maintaining discriminative features between real and fake samples, suggesting a strong coherence between the two worlds: good contrastive representations are also good for GAN discriminators, and vice versa. Our experimental results show that GANs with ContraD consistently improve FID and IS compared to other recent techniques incorporating data augmentations, still maintaining highly discriminative features in the discriminator in terms of the linear evaluation. Finally, as a byproduct, we also show that our GANs trained in an unsupervised manner (without labels) can induce many conditional generative models via a simple latent sampling, leveraging the learned features of ContraD. Code is available at https://github.com/jh-jeong/ContraD.", "one-sentence_summary": "We propose a novel discriminator of GAN showing that contrastive representation learning, e.g., SimCLR, and GAN can benefit each other when they are jointly trained. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jeong|training_gans_with_stronger_augmentations_via_contrastive_discriminator", "pdf": "/pdf/2d308c93802630f8c000471788307eb87a9027fd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\njeong2021training,\ntitle={Training {\\{}GAN{\\}}s with Stronger Augmentations via Contrastive Discriminator},\nauthor={Jongheon Jeong and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=eo6U4CAwVmg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "eo6U4CAwVmg", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2902/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2902/Authors|ICLR.cc/2021/Conference/Paper2902/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843290, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2902/-/Official_Comment"}}}, {"id": "CBVk3av3VB7", "original": null, "number": 15, "cdate": 1605517910345, "ddate": null, "tcdate": 1605517910345, "tmdate": 1605517910345, "tddate": null, "forum": "eo6U4CAwVmg", "replyto": "wBJI5oXyTL4", "invitation": "ICLR.cc/2021/Conference/Paper2902/-/Official_Comment", "content": {"title": "Response to R4 (2/2)", "comment": "---\n**Q6. Editorial comments**\n\nMany thanks for the careful reading, especially for pointing out an important typo in Eq. 7: as suggested, the order of log and sum must be reversed. Your editorial comments are now all fixed in the revised manuscript.\n\n---\n- [Brock et al., 2019] Large Scale GAN Training for High Fidelity Natural Image Synthesis, ICLR 2019.\n- [Chen et al., 2020] A Simple Framework for Contrastive Learning of Visual Representations, ICML 2020.\n- [Kalantidis et al., 2020] Hard Negative Mixing for Contrastive Learning, NeurIPS 2020.\n- [Karras et al., 2020] Training Generative Adversarial Networks with Limited Data, NeurIPS 2020.\n- [Robinson et al., 2020] Contrastive Learning with Hard Negative Samples, 2020.\n- [Zhao et al., 2020] Differentiable Augmentation for Data-Efficient GAN Training, NeurIPS 2020."}, "signatures": ["ICLR.cc/2021/Conference/Paper2902/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training GANs with Stronger Augmentations via Contrastive Discriminator", "authorids": ["~Jongheon_Jeong1", "~Jinwoo_Shin1"], "authors": ["Jongheon Jeong", "Jinwoo Shin"], "keywords": ["generative adversarial networks", "contrastive learning", "data augmentation", "visual representation learning", "unsupervised learning"], "abstract": "Recent works in Generative Adversarial Networks (GANs) are actively revisiting various data augmentation techniques as an effective way to prevent discriminator overfitting. It is still unclear, however, that which augmentations could actually improve GANs, and in particular, how to apply a wider range of augmentations in training. In this paper, we propose a novel way to address these questions by incorporating a recent contrastive representation learning scheme into the GAN discriminator, coined ContraD. This \"fusion\" enables the discriminators to work with much stronger augmentations without increasing their training instability, thereby preventing the discriminator overfitting issue in GANs more effectively. Even better, we observe that the contrastive learning itself also benefits from our GAN training, i.e., by maintaining discriminative features between real and fake samples, suggesting a strong coherence between the two worlds: good contrastive representations are also good for GAN discriminators, and vice versa. Our experimental results show that GANs with ContraD consistently improve FID and IS compared to other recent techniques incorporating data augmentations, still maintaining highly discriminative features in the discriminator in terms of the linear evaluation. Finally, as a byproduct, we also show that our GANs trained in an unsupervised manner (without labels) can induce many conditional generative models via a simple latent sampling, leveraging the learned features of ContraD. Code is available at https://github.com/jh-jeong/ContraD.", "one-sentence_summary": "We propose a novel discriminator of GAN showing that contrastive representation learning, e.g., SimCLR, and GAN can benefit each other when they are jointly trained. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jeong|training_gans_with_stronger_augmentations_via_contrastive_discriminator", "pdf": "/pdf/2d308c93802630f8c000471788307eb87a9027fd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\njeong2021training,\ntitle={Training {\\{}GAN{\\}}s with Stronger Augmentations via Contrastive Discriminator},\nauthor={Jongheon Jeong and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=eo6U4CAwVmg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "eo6U4CAwVmg", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2902/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2902/Authors|ICLR.cc/2021/Conference/Paper2902/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843290, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2902/-/Official_Comment"}}}, {"id": "bQdXheOIF0n", "original": null, "number": 13, "cdate": 1605517600692, "ddate": null, "tcdate": 1605517600692, "tmdate": 1605517600692, "tddate": null, "forum": "eo6U4CAwVmg", "replyto": "sYACn1zjun8", "invitation": "ICLR.cc/2021/Conference/Paper2902/-/Official_Comment", "content": {"title": "Response to R3", "comment": "We sincerely appreciate your thoughtful comments, efforts, and time. We respond to each of your questions and concerns one-by-one in what follows: possibly, some questions that need additional expensive experiments to address may not be included in the current response, but they will be supplemented after a revised manuscript is updated. We hope to deliver the revision soon, where we will make sure to reflect all comments. \n\n---\n**Q1. $L_{\\tt con}^{+}$ may not be necessary, if there exists a better hyperparameter that works at (\"No $L_{\\tt con}^{+}$\", SNResNet-18) in Table 6?**\n\nMany thanks for an insightful comment. Our focus is to improve both GAN and SimCLR in a unified angle, where all our experiments are designed to support this. $L_{\\tt con}^{+}$ is the essential component of SimCLR and without this, it would not be possible to perform linear evaluation (Table 3) or conditional generation (Table 4) upon the ContraD representation in our experiments. \n\nEven though SNDCGAN could be trained without $L_{\\tt con}^{+}$ as you pointed out, using $L_{\\tt con}^{+}$ clearly improves its performance further as shown in Table 6: i.e., even if one finds other good hyperparameters for SNResNet, $L_{\\tt con}^{+}$ is still useful. We also remark the hyperparameter used in our experiments is a popular one searched from a large-scale study [Lucic et al., 2018], which is known to widely work in practice: SNResNet without $L_{\\tt con}^{+}$ has failed in this setup, but somewhat interestingly, $L_{\\tt con}^{+}$ remedies the issue. \n\n---\n**Q2. Linear evaluation and transfer learning on the ImageNet models**\n\nFollowing your suggestion, we will report the results in the revision.\n\n---\n**Q3. Not fully convinced to the claim \"ContraD is beneficial to SimCLR\"**\n\nWe agree that we put more efforts in evaluation and justification on why SimCLR is beneficial to GAN. As a byproduct, we also observe the opposite direction is possible from the empirical results in Table 3: although they are conducted in smaller scale than the original setup of SimCLR [Chen et al., 2020] (which is based on ImageNet with ResNet-50), our results clearly confirm that $L_{\\tt con}^{-}$ with another network $G$ (which is trained via $L_{\\tt dis}$ and $L_G$) is an effective auxiliary loss for SimCLR training. This effectiveness of \"fake\" samples in contrastive learning has been unexplored, but we think it is promising along with the recent line of research to improve the efficiency of negative sampling in contrastive learning, e.g., via hard negative mining [Kalantidis et al., 2020; Robinson et al., 2020]. In this respect, we believe scaling up ContraD to compete with other state-of-the-art self-supervised learning benchmarks is an important future direction. We will provide such a more detailed discussion in the revision.\n\n---\n**Q4. Other minor comments**\n\nThank you for making constructive suggestions to improve the clarity of our manuscript. We will incorporate them in the revision.\n\n---\n- [Lucic et al., 2018] Are GANs Created Equal? A Large-Scale Study, NeurIPS 2018.\n- [Chen et al., 2020] A Simple Framework for Contrastive Learning of Visual Representations, ICML 2020.\n- [Kalantidis et al., 2020] Hard Negative Mixing for Contrastive Learning, NeurIPS 2020.\n- [Robinson et al., 2020] Contrastive Learning with Hard Negative Samples, 2020.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2902/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training GANs with Stronger Augmentations via Contrastive Discriminator", "authorids": ["~Jongheon_Jeong1", "~Jinwoo_Shin1"], "authors": ["Jongheon Jeong", "Jinwoo Shin"], "keywords": ["generative adversarial networks", "contrastive learning", "data augmentation", "visual representation learning", "unsupervised learning"], "abstract": "Recent works in Generative Adversarial Networks (GANs) are actively revisiting various data augmentation techniques as an effective way to prevent discriminator overfitting. It is still unclear, however, that which augmentations could actually improve GANs, and in particular, how to apply a wider range of augmentations in training. In this paper, we propose a novel way to address these questions by incorporating a recent contrastive representation learning scheme into the GAN discriminator, coined ContraD. This \"fusion\" enables the discriminators to work with much stronger augmentations without increasing their training instability, thereby preventing the discriminator overfitting issue in GANs more effectively. Even better, we observe that the contrastive learning itself also benefits from our GAN training, i.e., by maintaining discriminative features between real and fake samples, suggesting a strong coherence between the two worlds: good contrastive representations are also good for GAN discriminators, and vice versa. Our experimental results show that GANs with ContraD consistently improve FID and IS compared to other recent techniques incorporating data augmentations, still maintaining highly discriminative features in the discriminator in terms of the linear evaluation. Finally, as a byproduct, we also show that our GANs trained in an unsupervised manner (without labels) can induce many conditional generative models via a simple latent sampling, leveraging the learned features of ContraD. Code is available at https://github.com/jh-jeong/ContraD.", "one-sentence_summary": "We propose a novel discriminator of GAN showing that contrastive representation learning, e.g., SimCLR, and GAN can benefit each other when they are jointly trained. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jeong|training_gans_with_stronger_augmentations_via_contrastive_discriminator", "pdf": "/pdf/2d308c93802630f8c000471788307eb87a9027fd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\njeong2021training,\ntitle={Training {\\{}GAN{\\}}s with Stronger Augmentations via Contrastive Discriminator},\nauthor={Jongheon Jeong and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=eo6U4CAwVmg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "eo6U4CAwVmg", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2902/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2902/Authors|ICLR.cc/2021/Conference/Paper2902/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843290, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2902/-/Official_Comment"}}}, {"id": "wWtYkekwup", "original": null, "number": 2, "cdate": 1603929903082, "ddate": null, "tcdate": 1603929903082, "tmdate": 1605024107428, "tddate": null, "forum": "eo6U4CAwVmg", "replyto": "eo6U4CAwVmg", "invitation": "ICLR.cc/2021/Conference/Paper2902/-/Official_Review", "content": {"title": " ", "review": "In this paper, the authors suggest using the contrastive loss to improve the training of the discriminator and further stabilize the GAN training process. More specifically, the proposed method incorporates the self-supervised simCLR contrastive loss on a pair of transformed real images and supervised contrastive loss on the fake ones. The proposed method is evaluated on the image synthesis task on CIFAR10/100 and CelebA-HQ-128 images and over several different GAN models.\n\nStrengths:\n* The idea of using self-supervised learning for improving the training dynamics of the discriminator makes sense and is an interesting exploration area.\n* Empirical evaluations show a consistent and significant advantage for the proposed methods and ablation studies verify the contributions of the different proposed components.\n\nweaknesses:\n* The proposed method is rather a careful ensembling of existing components, e.g. simCLR self-supervised or supervised contrastive loss in the right context, rather than a radically novel methodological contribution.\n* The proposed method is only tested on relatively low-resolution datasets, namely CIFAR10/100 and CelebA-HQ-128. It would have been interesting to also demonstrate the contributions on the more challenging higher resolution datasets.\n\nDetailed comments:\n* Equation 7: If I am not mistaken, it doesn't seem quite the same as in the referred supervised contrastive loss from Khosla et al.; more specifically, I think the order between log and \\sum_{v_{i+}^{(2)} \\in V_{i+}^{(2)}} need to be reversed. Please clarify.\n* An ablation study I was missing was having the proposed method without the extensive simCLR augmentations and see how it compares to the other methods.\n* It would have been interesting to also compare with some other recent and relevant work, e.g. ADA (Karras et al.).\n* \"Remark that we use an independent projection header h_f instead of h_r\" => Is this making a significant difference? Are there ablation studies showing this?\n* Typo: \"approaches that handles\"\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2902/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training GANs with Stronger Augmentations via Contrastive Discriminator", "authorids": ["~Jongheon_Jeong1", "~Jinwoo_Shin1"], "authors": ["Jongheon Jeong", "Jinwoo Shin"], "keywords": ["generative adversarial networks", "contrastive learning", "data augmentation", "visual representation learning", "unsupervised learning"], "abstract": "Recent works in Generative Adversarial Networks (GANs) are actively revisiting various data augmentation techniques as an effective way to prevent discriminator overfitting. It is still unclear, however, that which augmentations could actually improve GANs, and in particular, how to apply a wider range of augmentations in training. In this paper, we propose a novel way to address these questions by incorporating a recent contrastive representation learning scheme into the GAN discriminator, coined ContraD. This \"fusion\" enables the discriminators to work with much stronger augmentations without increasing their training instability, thereby preventing the discriminator overfitting issue in GANs more effectively. Even better, we observe that the contrastive learning itself also benefits from our GAN training, i.e., by maintaining discriminative features between real and fake samples, suggesting a strong coherence between the two worlds: good contrastive representations are also good for GAN discriminators, and vice versa. Our experimental results show that GANs with ContraD consistently improve FID and IS compared to other recent techniques incorporating data augmentations, still maintaining highly discriminative features in the discriminator in terms of the linear evaluation. Finally, as a byproduct, we also show that our GANs trained in an unsupervised manner (without labels) can induce many conditional generative models via a simple latent sampling, leveraging the learned features of ContraD. Code is available at https://github.com/jh-jeong/ContraD.", "one-sentence_summary": "We propose a novel discriminator of GAN showing that contrastive representation learning, e.g., SimCLR, and GAN can benefit each other when they are jointly trained. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jeong|training_gans_with_stronger_augmentations_via_contrastive_discriminator", "pdf": "/pdf/2d308c93802630f8c000471788307eb87a9027fd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\njeong2021training,\ntitle={Training {\\{}GAN{\\}}s with Stronger Augmentations via Contrastive Discriminator},\nauthor={Jongheon Jeong and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=eo6U4CAwVmg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "eo6U4CAwVmg", "replyto": "eo6U4CAwVmg", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2902/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086324, "tmdate": 1606915790470, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2902/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2902/-/Official_Review"}}}, {"id": "fqyvlqreIbH", "original": null, "number": 4, "cdate": 1604381324477, "ddate": null, "tcdate": 1604381324477, "tmdate": 1605024107288, "tddate": null, "forum": "eo6U4CAwVmg", "replyto": "eo6U4CAwVmg", "invitation": "ICLR.cc/2021/Conference/Paper2902/-/Official_Review", "content": {"title": "a well-executed empirical paper with strong performance", "review": "==== Summary ====\n\nThis paper improves upon state-of-the-art GANs by incorporating recent advances of contrastive representation learning into the training of discriminator. In particular, the discriminator loss function consists of three terms: (1) the original SimCLR loss on the multi-view real data pairs; (2) the supervised contrastive loss (Khosla et al, 2020) that assigns high scores among the fake sample pairs and giving lower score among the real data pairs; (3) the usual discriminator loss in GAN training. While each of these terms alone is not entirely new, the author proposes several tricks to make the training of GANs together with the contrastive loss works. Empirically, the proposed method outperforms other GAN methods trained with auxiliary data augmentation techniques, and demonstrates good representations under the linear classifier probing setup. \n\nPros:\n\n(1) Writing is clear and easy to follow\n\n(2) Strong empirical performance in both image generation and classifier probing \nSolid experiment designs with thorough ablation studies\n\nCons:\n\n(1) Rather limited novelty in terms of technical contributions\n\n==== Technical Questions ====\n\nQ1: Regarding the supervised contrastive loss for fake samples (i.e., L_{con}^{-} in Figure 1).\nI am wondering if it is beneficial to also consider data augmentation on the fake sample produced by generators. In that case, the loss function matrix in Figure 1 will look closer to the L_{con}^{+} part, which encourages different views of the same fake sample to have similar representations.\n\nQ2: I feel like there\u2019s one ablation setting missing in Table 1: using \u201cHFlip, Trans\u201d data augmentation for the ContraD. This way, we can see the true benefit of combining those three losses.\n\nQ3: For the linear evaluation results in Table 3, SimCLR results seem not consistent with their original paper? What causes the difference?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2902/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2902/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training GANs with Stronger Augmentations via Contrastive Discriminator", "authorids": ["~Jongheon_Jeong1", "~Jinwoo_Shin1"], "authors": ["Jongheon Jeong", "Jinwoo Shin"], "keywords": ["generative adversarial networks", "contrastive learning", "data augmentation", "visual representation learning", "unsupervised learning"], "abstract": "Recent works in Generative Adversarial Networks (GANs) are actively revisiting various data augmentation techniques as an effective way to prevent discriminator overfitting. It is still unclear, however, that which augmentations could actually improve GANs, and in particular, how to apply a wider range of augmentations in training. In this paper, we propose a novel way to address these questions by incorporating a recent contrastive representation learning scheme into the GAN discriminator, coined ContraD. This \"fusion\" enables the discriminators to work with much stronger augmentations without increasing their training instability, thereby preventing the discriminator overfitting issue in GANs more effectively. Even better, we observe that the contrastive learning itself also benefits from our GAN training, i.e., by maintaining discriminative features between real and fake samples, suggesting a strong coherence between the two worlds: good contrastive representations are also good for GAN discriminators, and vice versa. Our experimental results show that GANs with ContraD consistently improve FID and IS compared to other recent techniques incorporating data augmentations, still maintaining highly discriminative features in the discriminator in terms of the linear evaluation. Finally, as a byproduct, we also show that our GANs trained in an unsupervised manner (without labels) can induce many conditional generative models via a simple latent sampling, leveraging the learned features of ContraD. Code is available at https://github.com/jh-jeong/ContraD.", "one-sentence_summary": "We propose a novel discriminator of GAN showing that contrastive representation learning, e.g., SimCLR, and GAN can benefit each other when they are jointly trained. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jeong|training_gans_with_stronger_augmentations_via_contrastive_discriminator", "pdf": "/pdf/2d308c93802630f8c000471788307eb87a9027fd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\njeong2021training,\ntitle={Training {\\{}GAN{\\}}s with Stronger Augmentations via Contrastive Discriminator},\nauthor={Jongheon Jeong and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=eo6U4CAwVmg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "eo6U4CAwVmg", "replyto": "eo6U4CAwVmg", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2902/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086324, "tmdate": 1606915790470, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2902/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2902/-/Official_Review"}}}], "count": 18}