{"notes": [{"id": "ByeMHULt_N", "original": "HkeAMH0udN", "number": 47, "cdate": 1553716794323, "ddate": null, "tcdate": 1553716794323, "tmdate": 1562083049221, "tddate": null, "forum": "ByeMHULt_N", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Blind_Submission", "content": {"title": "Learning Deep Latent-variable MRFs with Amortized Bethe Free Energy Minimization", "authors": ["Sam Wiseman"], "authorids": ["swiseman@ttic.edu"], "keywords": ["MRF", "latent variable", "Bethe", "UGM", "approximate inference", "deep generative model"], "TL;DR": "Learning deep latent variable MRFs with a saddle-point objective derived from the Bethe partition function approximation.", "abstract": "While much recent work has targeted learning deep discrete latent variable models with variational inference, this setting remains challenging, and it is often necessary to make use of potentially high-variance gradient estimators in optimizing the ELBO. As an alternative, we propose to optimize a non-ELBO objective derived from the Bethe free energy approximation to an MRF's partition function. This objective gives rise to a saddle-point learning problem, which we train inference networks to approximately optimize. The derived objective requires no sampling, and can be efficiently computed for many MRFs of interest. We evaluate the proposed approach in learning high-order neural HMMs on text, and find that it often outperforms other approximate inference schemes in terms of true held-out log likelihood. At the same time, we find that all the approximate inference-based approaches to learning high-order neural HMMs we consider underperform learning with exact inference by a significant margin.", "pdf": "/pdf/85794a3b4f6b82273c6be31fa9a0bdd81298de26.pdf", "paperhash": "wiseman|learning_deep_latentvariable_mrfs_with_amortized_bethe_free_energy_minimization"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Blind_Submission", "cdate": 1547567085825, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": [".*"]}, "writers": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1547567085825, "tmdate": 1555704438520, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}}, "tauthor": "OpenReview.net"}, {"id": "HkxXugn-cV", "original": null, "number": 2, "cdate": 1555312746624, "ddate": null, "tcdate": 1555312746624, "tmdate": 1556906151872, "tddate": null, "forum": "ByeMHULt_N", "replyto": "ByeMHULt_N", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper47/Official_Review", "content": {"title": "Interesting idea", "review": "This paper presents an objective for learning latent variable MRFs based on Bethe free energy and amortized inference. It is different from optimizing the standard ELBO in that it does not require sampling (which has large variance) nor it is a lower/upper bound of the log-likelihood for general structured data. On some benchmark with neural HMMs, it is shown that the proposed approach achieves better held-out likelihood than other variational inference based approaches. \n\nThis paper presents an interesting idea which blends both the deep generative models research as well as the traditional Bethe free energy formulation. The prelmimarny results seems promising. I wonder how much difficult the saddle-point optimization will become on more complex models comparing with ELBO optimization. \n\nMinor comment:\n\nThe last equation in Section 2.2: the second summation should be over z_3'. ", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper47/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper47/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Deep Latent-variable MRFs with Amortized Bethe Free Energy Minimization", "authors": ["Sam Wiseman"], "authorids": ["swiseman@ttic.edu"], "keywords": ["MRF", "latent variable", "Bethe", "UGM", "approximate inference", "deep generative model"], "TL;DR": "Learning deep latent variable MRFs with a saddle-point objective derived from the Bethe partition function approximation.", "abstract": "While much recent work has targeted learning deep discrete latent variable models with variational inference, this setting remains challenging, and it is often necessary to make use of potentially high-variance gradient estimators in optimizing the ELBO. As an alternative, we propose to optimize a non-ELBO objective derived from the Bethe free energy approximation to an MRF's partition function. This objective gives rise to a saddle-point learning problem, which we train inference networks to approximately optimize. The derived objective requires no sampling, and can be efficiently computed for many MRFs of interest. We evaluate the proposed approach in learning high-order neural HMMs on text, and find that it often outperforms other approximate inference schemes in terms of true held-out log likelihood. At the same time, we find that all the approximate inference-based approaches to learning high-order neural HMMs we consider underperform learning with exact inference by a significant margin.", "pdf": "/pdf/85794a3b4f6b82273c6be31fa9a0bdd81298de26.pdf", "paperhash": "wiseman|learning_deep_latentvariable_mrfs_with_amortized_bethe_free_energy_minimization"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper47/Official_Review", "cdate": 1554234170681, "reply": {"forum": "ByeMHULt_N", "replyto": "ByeMHULt_N", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper47/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper47/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1554234170681, "tmdate": 1556906093572, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper47/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}, {"id": "SyxgecVYFN", "original": null, "number": 1, "cdate": 1554758120262, "ddate": null, "tcdate": 1554758120262, "tmdate": 1556906151649, "tddate": null, "forum": "ByeMHULt_N", "replyto": "ByeMHULt_N", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper47/Official_Review", "content": {"title": "Clear method for learning deep latent-variable MRFs using Bethe free energy optimization", "review": "This paper proposed a method for learning deep latent-variable MRF with an optimization objective that utilizes the Bethe free energy. To solve the underlying constraints of Bethe free energy optimizations, the authors proposed to represent the \\tau vector using the basis of the subspace of the equality constraints and put the positivity constraints to be part of the objective. By applying these techniques, we obtain a saddle-point optimization objective with trainable inference networks and hence we can train the latent-variable MRF. The authors did some experiments on 2nd and 3rd order HMMs for empirical studies.\n\nPros:\n\n1. The paper is well-written and easy to follow.\n\n2. The original optimization for Bethe free energy is with constraints. However, the proposed objective function is without constraints, which is easier to train. The authors used the Moore-Penrose pseudoinverse of the constraint matrix V to represent the subspace of \\tau, which makes the optimization process easier.\n\nCons and questions:\n\n1. From the experiment results, it seems that the proposed method is not behaving well compared to the exact methods, if we do not use \"exact marginals\". I doubt if the performance improvements of \"L_F + exact marginals\" are due to the \"exact marginals\", not the proposed method.\n\n2. For experiment results of the baseline methods in Table 1 and the proposed method in Table 2, the authors try to compare the PPL performances between them. Are the two experiment settings (one for directed HMMs and one for undirected HMMs) comparable? If they are, then why not putting the two tables together and compared all experiment results between them? If not, is the comparison fair?\n\n3. It will be great if the authors can also work on some models where we can not tractably compute log marginal likelihoods, instead of only HMMs.", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper47/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper47/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Deep Latent-variable MRFs with Amortized Bethe Free Energy Minimization", "authors": ["Sam Wiseman"], "authorids": ["swiseman@ttic.edu"], "keywords": ["MRF", "latent variable", "Bethe", "UGM", "approximate inference", "deep generative model"], "TL;DR": "Learning deep latent variable MRFs with a saddle-point objective derived from the Bethe partition function approximation.", "abstract": "While much recent work has targeted learning deep discrete latent variable models with variational inference, this setting remains challenging, and it is often necessary to make use of potentially high-variance gradient estimators in optimizing the ELBO. As an alternative, we propose to optimize a non-ELBO objective derived from the Bethe free energy approximation to an MRF's partition function. This objective gives rise to a saddle-point learning problem, which we train inference networks to approximately optimize. The derived objective requires no sampling, and can be efficiently computed for many MRFs of interest. We evaluate the proposed approach in learning high-order neural HMMs on text, and find that it often outperforms other approximate inference schemes in terms of true held-out log likelihood. At the same time, we find that all the approximate inference-based approaches to learning high-order neural HMMs we consider underperform learning with exact inference by a significant margin.", "pdf": "/pdf/85794a3b4f6b82273c6be31fa9a0bdd81298de26.pdf", "paperhash": "wiseman|learning_deep_latentvariable_mrfs_with_amortized_bethe_free_energy_minimization"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper47/Official_Review", "cdate": 1554234170681, "reply": {"forum": "ByeMHULt_N", "replyto": "ByeMHULt_N", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper47/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper47/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1554234170681, "tmdate": 1556906093572, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper47/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}, {"id": "r1lfOmODqE", "original": null, "number": 1, "cdate": 1555690345587, "ddate": null, "tcdate": 1555690345587, "tmdate": 1556906151425, "tddate": null, "forum": "ByeMHULt_N", "replyto": "ByeMHULt_N", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper47/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Deep Latent-variable MRFs with Amortized Bethe Free Energy Minimization", "authors": ["Sam Wiseman"], "authorids": ["swiseman@ttic.edu"], "keywords": ["MRF", "latent variable", "Bethe", "UGM", "approximate inference", "deep generative model"], "TL;DR": "Learning deep latent variable MRFs with a saddle-point objective derived from the Bethe partition function approximation.", "abstract": "While much recent work has targeted learning deep discrete latent variable models with variational inference, this setting remains challenging, and it is often necessary to make use of potentially high-variance gradient estimators in optimizing the ELBO. As an alternative, we propose to optimize a non-ELBO objective derived from the Bethe free energy approximation to an MRF's partition function. This objective gives rise to a saddle-point learning problem, which we train inference networks to approximately optimize. The derived objective requires no sampling, and can be efficiently computed for many MRFs of interest. We evaluate the proposed approach in learning high-order neural HMMs on text, and find that it often outperforms other approximate inference schemes in terms of true held-out log likelihood. At the same time, we find that all the approximate inference-based approaches to learning high-order neural HMMs we consider underperform learning with exact inference by a significant margin.", "pdf": "/pdf/85794a3b4f6b82273c6be31fa9a0bdd81298de26.pdf", "paperhash": "wiseman|learning_deep_latentvariable_mrfs_with_amortized_bethe_free_energy_minimization"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper47/Decision", "cdate": 1554814601844, "reply": {"forum": "ByeMHULt_N", "replyto": "ByeMHULt_N", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554814601844, "tmdate": 1556906103049, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}], "count": 4}