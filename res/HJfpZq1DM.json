{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124434217, "tcdate": 1518473658523, "number": 358, "cdate": 1518473658523, "id": "HJfpZq1DM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "HJfpZq1DM", "signatures": ["~Ange_Tato1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "IMPROVING ADAM OPTIMIZER", "abstract": "We present a modified version of the Adam (Adaptive moment estimation) opti-mization algorithm, able to improve the speed of convergence and finds a betterminimum for the loss function com-pared to the original algorithm. The proposedsolution borrows some ideas from the momentum based optimizer and the exponen-tial decay technique. The current step size made by Adam to update the parametersis modify in such a way that the new step takes in consideration the directionof the gradient and the previous steps update.  We conducted several tests withdeep Convolutional Neural Networks in the MNIST data. The results showed thatAAdam(Accelerated Adam) outperforms Adam and NAdam (Nesterov ac-celeratedAdam). The preliminary evidence suggests that making such a change improvesthe speed of convergence and the quality of the learned models.", "paperhash": "tato|improving_adam_optimizer", "_bibtex": "@misc{\n  tato2018improving,\n  title={IMPROVING ADAM OPTIMIZER},\n  author={Ange Tato and Roger Nkambou},\n  year={2018},\n  url={https://openreview.net/forum?id=HJfpZq1DM}\n}", "authorids": ["nyamen_tato.ange_adrienne@uqam.ca", "nkambou.roger@uqam.ca"], "authors": ["Ange Tato", "Roger Nkambou"], "keywords": ["Neural Networks", "Gradient Descent", "Deep Learning", "Machine Learn-ing", "Optimization algorithms"], "pdf": "/pdf/846bf096cb2e4f65ad1424d856a780893ddb0293.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582931363, "tcdate": 1520327464055, "number": 1, "cdate": 1520327464055, "id": "B1gEi0sOM", "invitation": "ICLR.cc/2018/Workshop/-/Paper358/Official_Review", "forum": "HJfpZq1DM", "replyto": "HJfpZq1DM", "signatures": ["ICLR.cc/2018/Workshop/Paper358/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper358/AnonReviewer3"], "content": {"title": "Insufficient experiments and no theory", "rating": "4: Ok but not good enough - rejection", "review": "Summary:\nThis paper suggest ,AAdam, an alternation of the Adam optimizer that incorporates an additional term which encourages progress in directions where \nthe gradients consistently point in the same direction. Then they show empirical results comparing their performance to Adam and another accelerated version of Adam.\n\nThe Algorithm:\nThe authors suggest a simple modification. However, this modification uses the sign of the gradient estimates which may result a very bad behaviour even for simple convex problems. The authors do not discuss this issue in their paper.\nAlso, the authors do not provide any theoretical guarantees.\n\nExperiments:\nThe authors only make comparisons for the MNIST dataset, which is insufficient.\nIllustrating the benefits of AAdam requires a much more elaborate experimentation.\n\nI therefore recommend to reject the paper.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "IMPROVING ADAM OPTIMIZER", "abstract": "We present a modified version of the Adam (Adaptive moment estimation) opti-mization algorithm, able to improve the speed of convergence and finds a betterminimum for the loss function com-pared to the original algorithm. The proposedsolution borrows some ideas from the momentum based optimizer and the exponen-tial decay technique. The current step size made by Adam to update the parametersis modify in such a way that the new step takes in consideration the directionof the gradient and the previous steps update.  We conducted several tests withdeep Convolutional Neural Networks in the MNIST data. The results showed thatAAdam(Accelerated Adam) outperforms Adam and NAdam (Nesterov ac-celeratedAdam). The preliminary evidence suggests that making such a change improvesthe speed of convergence and the quality of the learned models.", "paperhash": "tato|improving_adam_optimizer", "_bibtex": "@misc{\n  tato2018improving,\n  title={IMPROVING ADAM OPTIMIZER},\n  author={Ange Tato and Roger Nkambou},\n  year={2018},\n  url={https://openreview.net/forum?id=HJfpZq1DM}\n}", "authorids": ["nyamen_tato.ange_adrienne@uqam.ca", "nkambou.roger@uqam.ca"], "authors": ["Ange Tato", "Roger Nkambou"], "keywords": ["Neural Networks", "Gradient Descent", "Deep Learning", "Machine Learn-ing", "Optimization algorithms"], "pdf": "/pdf/846bf096cb2e4f65ad1424d856a780893ddb0293.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582931173, "id": "ICLR.cc/2018/Workshop/-/Paper358/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper358/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper358/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper358/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper358/AnonReviewer2"], "reply": {"forum": "HJfpZq1DM", "replyto": "HJfpZq1DM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper358/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper358/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582931173}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582885288, "tcdate": 1520531949069, "number": 2, "cdate": 1520531949069, "id": "rJHe5gkFM", "invitation": "ICLR.cc/2018/Workshop/-/Paper358/Official_Review", "forum": "HJfpZq1DM", "replyto": "HJfpZq1DM", "signatures": ["ICLR.cc/2018/Workshop/Paper358/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper358/AnonReviewer1"], "content": {"title": "Interesting idea; too little content", "rating": "3: Clear rejection", "review": "The paper presents an interesting modification of Adam. The submission is low on content however, even for a workshop paper. Some basic intuition for the idea is provided, but no more than that. Performance is evaluated on a *SINGLE* experiment on MNIST, where the AAdam slightly outperforms Adam and Nadam. \n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "IMPROVING ADAM OPTIMIZER", "abstract": "We present a modified version of the Adam (Adaptive moment estimation) opti-mization algorithm, able to improve the speed of convergence and finds a betterminimum for the loss function com-pared to the original algorithm. The proposedsolution borrows some ideas from the momentum based optimizer and the exponen-tial decay technique. The current step size made by Adam to update the parametersis modify in such a way that the new step takes in consideration the directionof the gradient and the previous steps update.  We conducted several tests withdeep Convolutional Neural Networks in the MNIST data. The results showed thatAAdam(Accelerated Adam) outperforms Adam and NAdam (Nesterov ac-celeratedAdam). The preliminary evidence suggests that making such a change improvesthe speed of convergence and the quality of the learned models.", "paperhash": "tato|improving_adam_optimizer", "_bibtex": "@misc{\n  tato2018improving,\n  title={IMPROVING ADAM OPTIMIZER},\n  author={Ange Tato and Roger Nkambou},\n  year={2018},\n  url={https://openreview.net/forum?id=HJfpZq1DM}\n}", "authorids": ["nyamen_tato.ange_adrienne@uqam.ca", "nkambou.roger@uqam.ca"], "authors": ["Ange Tato", "Roger Nkambou"], "keywords": ["Neural Networks", "Gradient Descent", "Deep Learning", "Machine Learn-ing", "Optimization algorithms"], "pdf": "/pdf/846bf096cb2e4f65ad1424d856a780893ddb0293.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582931173, "id": "ICLR.cc/2018/Workshop/-/Paper358/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper358/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper358/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper358/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper358/AnonReviewer2"], "reply": {"forum": "HJfpZq1DM", "replyto": "HJfpZq1DM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper358/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper358/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582931173}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582858757, "tcdate": 1520564913852, "number": 3, "cdate": 1520564913852, "id": "rJq2qd1Yf", "invitation": "ICLR.cc/2018/Workshop/-/Paper358/Official_Review", "forum": "HJfpZq1DM", "replyto": "HJfpZq1DM", "signatures": ["ICLR.cc/2018/Workshop/Paper358/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper358/AnonReviewer2"], "content": {"title": "Hyperparameters study is missing", "rating": "3: Clear rejection", "review": "The empirical results shown in the paper are not convincing. The tiny difference shown on MNIST could be due to a particular choice of hyperparameters or even random seed (see the bumps). For instance, the difference of AAdam and Adam might be roughly  approximated by a different choice of momentum coefficients and/or baseline learning rate in the original Adam. ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "IMPROVING ADAM OPTIMIZER", "abstract": "We present a modified version of the Adam (Adaptive moment estimation) opti-mization algorithm, able to improve the speed of convergence and finds a betterminimum for the loss function com-pared to the original algorithm. The proposedsolution borrows some ideas from the momentum based optimizer and the exponen-tial decay technique. The current step size made by Adam to update the parametersis modify in such a way that the new step takes in consideration the directionof the gradient and the previous steps update.  We conducted several tests withdeep Convolutional Neural Networks in the MNIST data. The results showed thatAAdam(Accelerated Adam) outperforms Adam and NAdam (Nesterov ac-celeratedAdam). The preliminary evidence suggests that making such a change improvesthe speed of convergence and the quality of the learned models.", "paperhash": "tato|improving_adam_optimizer", "_bibtex": "@misc{\n  tato2018improving,\n  title={IMPROVING ADAM OPTIMIZER},\n  author={Ange Tato and Roger Nkambou},\n  year={2018},\n  url={https://openreview.net/forum?id=HJfpZq1DM}\n}", "authorids": ["nyamen_tato.ange_adrienne@uqam.ca", "nkambou.roger@uqam.ca"], "authors": ["Ange Tato", "Roger Nkambou"], "keywords": ["Neural Networks", "Gradient Descent", "Deep Learning", "Machine Learn-ing", "Optimization algorithms"], "pdf": "/pdf/846bf096cb2e4f65ad1424d856a780893ddb0293.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582931173, "id": "ICLR.cc/2018/Workshop/-/Paper358/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper358/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper358/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper358/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper358/AnonReviewer2"], "reply": {"forum": "HJfpZq1DM", "replyto": "HJfpZq1DM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper358/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper358/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582931173}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573606501, "tcdate": 1521573606501, "number": 267, "cdate": 1521573606164, "id": "By0Jkkycz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "HJfpZq1DM", "replyto": "HJfpZq1DM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "IMPROVING ADAM OPTIMIZER", "abstract": "We present a modified version of the Adam (Adaptive moment estimation) opti-mization algorithm, able to improve the speed of convergence and finds a betterminimum for the loss function com-pared to the original algorithm. The proposedsolution borrows some ideas from the momentum based optimizer and the exponen-tial decay technique. The current step size made by Adam to update the parametersis modify in such a way that the new step takes in consideration the directionof the gradient and the previous steps update.  We conducted several tests withdeep Convolutional Neural Networks in the MNIST data. The results showed thatAAdam(Accelerated Adam) outperforms Adam and NAdam (Nesterov ac-celeratedAdam). The preliminary evidence suggests that making such a change improvesthe speed of convergence and the quality of the learned models.", "paperhash": "tato|improving_adam_optimizer", "_bibtex": "@misc{\n  tato2018improving,\n  title={IMPROVING ADAM OPTIMIZER},\n  author={Ange Tato and Roger Nkambou},\n  year={2018},\n  url={https://openreview.net/forum?id=HJfpZq1DM}\n}", "authorids": ["nyamen_tato.ange_adrienne@uqam.ca", "nkambou.roger@uqam.ca"], "authors": ["Ange Tato", "Roger Nkambou"], "keywords": ["Neural Networks", "Gradient Descent", "Deep Learning", "Machine Learn-ing", "Optimization algorithms"], "pdf": "/pdf/846bf096cb2e4f65ad1424d856a780893ddb0293.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}