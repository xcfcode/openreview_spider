{"notes": [{"id": "BJlRKgkDwN", "original": "BJxTFlkPDE", "number": 12, "cdate": 1552507013927, "ddate": null, "tcdate": 1552507013927, "tmdate": 1562082108745, "tddate": null, "forum": "BJlRKgkDwN", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "content": {"title": "WEAKLY SEMI-SUPERVISED NEURAL TOPIC MODELS", "authors": ["Ian Gemp", "Ramesh Nallapati", "Ran Ding", "Feng Nan", "Bing Xiang"], "authorids": ["imgemp@gmail.com", "rnallapa@amazon.com", "dingran.nju@gmail.com", "nanfen@amazon.com", "bxiang@amazon.com"], "keywords": ["topic models", "variational inference", "variational autoencoder", "semi-supervised", "deep learning"], "TL;DR": "We propose supervising VAE-style topic models by intelligently adjusting the prior on a per document basis. We find a logit-normal posterior provides the best performance.", "abstract": "We consider the problem of topic modeling in a weakly semi-supervised setting. In this scenario, we assume that the user knows a priori a subset of the topics she wants the model to learn and is able to provide a few exemplar documents for those topics. In addition, while each document may typically consist of multiple topics, we do not assume that the user will identify all its topics exhaustively.\n      \nRecent state-of-the-art topic models such as NVDM, referred to herein as Neural Topic Models (NTMs), fall under the variational autoencoder framework. We extend NTMs to the weakly semi-supervised setting by using informative priors in the training objective. After analyzing the effect of informative priors, we propose a simple modification of the NVDM model using a logit-normal posterior that we show achieves better alignment to user-desired topics versus other NTM models.", "pdf": "/pdf/6b0958caca64260ae5c0a0a25aa87bb13d529399.pdf", "paperhash": "gemp|weakly_semisupervised_neural_topic_models"}, "signatures": ["ICLR.cc/2019/Workshop/LLD"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "cdate": 1548689671889, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "content": {"authors": {"values-regex": ".*"}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1548689671889, "tmdate": 1557933709646, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/LLD"], "details": {"writable": true}}}, "tauthor": "ICLR.cc/2019/Workshop/LLD"}, {"id": "Bkem3KVBYE", "original": null, "number": 1, "cdate": 1554495914813, "ddate": null, "tcdate": 1554495914813, "tmdate": 1555512025701, "tddate": null, "forum": "BJlRKgkDwN", "replyto": "BJlRKgkDwN", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper12/Official_Review", "content": {"title": "A reasonable approach to a practical problem", "review": "Title:\nRating: 3\nConfidence: 1\n\nSummary:\nThe authors present a method of weakly supervising a topic model to guide it toward topic alignments that better align with user intuition for what topics should be. Their approach allows the user to label a subset of documents with a subset of their topics, making it flexible.\n\nI am not in a great position to confirm the soundness of the math, but the problem being solved and the suggested approach seem reasonable and relevant, with practical application.\n\nPoints:\n- I appreciate the flexibility of the design; allowing any number of documents to be labeled with any number of topics makes it much more likely to be used in practice, I believe.\n- The design decision to incorporate this information in the prior is nice and largely decoupled from the VAE in a good way.\n- I found the explanation of how ground truth was determined to be a little opaque. If this method in used in other works, please cite them. If it is not, more justification should be given here (possibly with a figure or equation for clarification).\n- Figure 1 is very helpful in justifying why you chose the model you did.\n- Table 2 does not seem to add much. It is not obvious from looking at the table that any of those methods is better than the others. \n- What are the confidence intervals on the results in Table 3? Some of this differences are quite small.\n\nNits:\n- Separate Table 1 from the text more\n- You mention in Section 5 that Delicious has 20 topics, but then say that \"half\" the number of labels is 11, and fully-labeled is up to 28? Please clarify what is being referred to or fix those numbers.", "rating": "3: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper12/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper12/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "WEAKLY SEMI-SUPERVISED NEURAL TOPIC MODELS", "authors": ["Ian Gemp", "Ramesh Nallapati", "Ran Ding", "Feng Nan", "Bing Xiang"], "authorids": ["imgemp@gmail.com", "rnallapa@amazon.com", "dingran.nju@gmail.com", "nanfen@amazon.com", "bxiang@amazon.com"], "keywords": ["topic models", "variational inference", "variational autoencoder", "semi-supervised", "deep learning"], "TL;DR": "We propose supervising VAE-style topic models by intelligently adjusting the prior on a per document basis. We find a logit-normal posterior provides the best performance.", "abstract": "We consider the problem of topic modeling in a weakly semi-supervised setting. In this scenario, we assume that the user knows a priori a subset of the topics she wants the model to learn and is able to provide a few exemplar documents for those topics. In addition, while each document may typically consist of multiple topics, we do not assume that the user will identify all its topics exhaustively.\n      \nRecent state-of-the-art topic models such as NVDM, referred to herein as Neural Topic Models (NTMs), fall under the variational autoencoder framework. We extend NTMs to the weakly semi-supervised setting by using informative priors in the training objective. After analyzing the effect of informative priors, we propose a simple modification of the NVDM model using a logit-normal posterior that we show achieves better alignment to user-desired topics versus other NTM models.", "pdf": "/pdf/6b0958caca64260ae5c0a0a25aa87bb13d529399.pdf", "paperhash": "gemp|weakly_semisupervised_neural_topic_models"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper12/Official_Review", "cdate": 1553713420475, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "BJlRKgkDwN", "replyto": "BJlRKgkDwN", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper12/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper12/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713420475, "tmdate": 1555511818292, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper12/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "HkxY9HPIKN", "original": null, "number": 2, "cdate": 1554572689453, "ddate": null, "tcdate": 1554572689453, "tmdate": 1555512023072, "tddate": null, "forum": "BJlRKgkDwN", "replyto": "BJlRKgkDwN", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper12/Official_Review", "content": {"title": "unclear", "review": "This short paper investigates weak supervision for neural-based topic modelling (NTM) by modifying both the priors and the posteriors, in a VAE setting.\n\nThe priors are modified to using the logits of the probability for each topic to appear in a document. The use of the logits is meant to strengthen \"ground truth\" information given by a user (supervision), while not penalizing the probability for another topics to appear in a document which would not have been labelled by the user (weak supervision).\nOn the one hand, his part of the work is quite clear, and the authors justify this as a way to make a NMT better align with the semantic of the user.\nOn the other hand, the explanation about the generative process is unclear, and would benefit of a longer explanation.\nI understood that, in addition to the normal setting of a VAE, two other posteriors have been tested: the Concrete approximation to Bernouilli, and the use of a logit-normal distribution.\nThe all section about the comparison of the two is really confusing and it becomes hard to clearly follow authors' reasoning.\n\nIn their experiments, the authors have used three sources of multi-labelled dataset. Topic modelling is not my domain of expertise, but could the authors have not used other datasets to evaluate their approach against (e.g. 20NewsGroups and RCV1-v2 datasets, similarly to original NVDM (Miao et al., 2016))? What's the motivation for using BibTeX, Deliicious and AAPD?\nBased on table 3, both the NVDM-o (with logit-normal posterior), and Bernouilli (columns 2 and 3) are quite similar for at least two of the metrics, but this point is not discussed by the authors, at all.\n\n* other comments/questions about the paper *\n- latent Dirichlet allocation (LDA) => Latent [...];\n- Often times => Oftentimes;\n- The integration of table 1 could be better;\n- Why is Wikipedia mentioned?\n- what's the difference between P-NPMI and NPMI? It's not clear;\n- due the => due to the;\n- W_{K} => W_{k}  (Bernouilli decoding computing the word distribution), compared to \"W_{k} refers to\";", "rating": "2: Marginally below acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper12/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper12/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "WEAKLY SEMI-SUPERVISED NEURAL TOPIC MODELS", "authors": ["Ian Gemp", "Ramesh Nallapati", "Ran Ding", "Feng Nan", "Bing Xiang"], "authorids": ["imgemp@gmail.com", "rnallapa@amazon.com", "dingran.nju@gmail.com", "nanfen@amazon.com", "bxiang@amazon.com"], "keywords": ["topic models", "variational inference", "variational autoencoder", "semi-supervised", "deep learning"], "TL;DR": "We propose supervising VAE-style topic models by intelligently adjusting the prior on a per document basis. We find a logit-normal posterior provides the best performance.", "abstract": "We consider the problem of topic modeling in a weakly semi-supervised setting. In this scenario, we assume that the user knows a priori a subset of the topics she wants the model to learn and is able to provide a few exemplar documents for those topics. In addition, while each document may typically consist of multiple topics, we do not assume that the user will identify all its topics exhaustively.\n      \nRecent state-of-the-art topic models such as NVDM, referred to herein as Neural Topic Models (NTMs), fall under the variational autoencoder framework. We extend NTMs to the weakly semi-supervised setting by using informative priors in the training objective. After analyzing the effect of informative priors, we propose a simple modification of the NVDM model using a logit-normal posterior that we show achieves better alignment to user-desired topics versus other NTM models.", "pdf": "/pdf/6b0958caca64260ae5c0a0a25aa87bb13d529399.pdf", "paperhash": "gemp|weakly_semisupervised_neural_topic_models"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper12/Official_Review", "cdate": 1553713420475, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "BJlRKgkDwN", "replyto": "BJlRKgkDwN", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper12/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper12/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713420475, "tmdate": 1555511818292, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper12/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "rJlJ8vnzc4", "original": null, "number": 1, "cdate": 1555380038986, "ddate": null, "tcdate": 1555380038986, "tmdate": 1555510981037, "tddate": null, "forum": "BJlRKgkDwN", "replyto": "BJlRKgkDwN", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper12/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "WEAKLY SEMI-SUPERVISED NEURAL TOPIC MODELS", "authors": ["Ian Gemp", "Ramesh Nallapati", "Ran Ding", "Feng Nan", "Bing Xiang"], "authorids": ["imgemp@gmail.com", "rnallapa@amazon.com", "dingran.nju@gmail.com", "nanfen@amazon.com", "bxiang@amazon.com"], "keywords": ["topic models", "variational inference", "variational autoencoder", "semi-supervised", "deep learning"], "TL;DR": "We propose supervising VAE-style topic models by intelligently adjusting the prior on a per document basis. We find a logit-normal posterior provides the best performance.", "abstract": "We consider the problem of topic modeling in a weakly semi-supervised setting. In this scenario, we assume that the user knows a priori a subset of the topics she wants the model to learn and is able to provide a few exemplar documents for those topics. In addition, while each document may typically consist of multiple topics, we do not assume that the user will identify all its topics exhaustively.\n      \nRecent state-of-the-art topic models such as NVDM, referred to herein as Neural Topic Models (NTMs), fall under the variational autoencoder framework. We extend NTMs to the weakly semi-supervised setting by using informative priors in the training objective. After analyzing the effect of informative priors, we propose a simple modification of the NVDM model using a logit-normal posterior that we show achieves better alignment to user-desired topics versus other NTM models.", "pdf": "/pdf/6b0958caca64260ae5c0a0a25aa87bb13d529399.pdf", "paperhash": "gemp|weakly_semisupervised_neural_topic_models"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper12/Decision", "cdate": 1554736069424, "reply": {"forum": "BJlRKgkDwN", "replyto": "BJlRKgkDwN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554736069424, "tmdate": 1555510969272, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}], "count": 4}