{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124480427, "tcdate": 1518432718452, "number": 110, "cdate": 1518432718452, "id": "B1LRWg1wz", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "B1LRWg1wz", "signatures": ["~Jack_Lindsey1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Semiparametric Reinforcement Learning", "abstract": "We introduce a semiparametric approach to deep reinforcement learning inspired by complementary learning systems theory in cognitive neuroscience.  Our approach allows a neural network to integrate nonparametric, episodic memory-based computations with parametric statistical learning in an end-to-end fashion. We give a deep Q network access to intermediate and final results of a differentiable approximation to k-nearest-neighbors performed on a dictionary of historic state-action embeddings.  Our method displays the early-learning advantage associated with episodic memory-based algorithms while mitigating the asymptotic performance disadvantage suffered by such approaches.  In several cases we find that our model learns even more quickly from few examples than pure kNN-based approaches.  Analysis shows that our semiparametric algorithm relies heavily on the kNN output early on and less so as training progresses, which is consistent with complementary learning systems theory.", "paperhash": "jain|semiparametric_reinforcement_learning", "keywords": ["deep learning", "nonparametric", "episodic learning", "nearest neighbors", "complementary learning systems", "reinforcement learning"], "_bibtex": "@misc{\n  jain2018semiparametric,\n  title={Semiparametric Reinforcement Learning},\n  author={Mika Sarkin Jain and Jack Lindsey},\n  year={2018},\n  url={https://openreview.net/forum?id=B1LRWg1wz}\n}", "authorids": ["mikasarkinjain@gmail.com", "jacklindsey@stanford.edu"], "authors": ["Mika Sarkin Jain", "Jack Lindsey"], "TL;DR": "Combining parametric and nonparametric methods in RL problems yields fast learning while maintaining good final performance.", "pdf": "/pdf/1fe77f06b94d77659e0aa3ce98f3da72deb2bb48.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582988110, "tcdate": 1519599682514, "number": 1, "cdate": 1519599682514, "id": "BkqrgTedM", "invitation": "ICLR.cc/2018/Workshop/-/Paper110/Official_Review", "forum": "B1LRWg1wz", "replyto": "B1LRWg1wz", "signatures": ["ICLR.cc/2018/Workshop/Paper110/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper110/AnonReviewer2"], "content": {"title": "Straightforward extension to NEC to include a parametric correct", "rating": "7: Good paper, accept", "review": "NEC (Pritzel, 2017) demonstrated a non-parametric approach to scalable, model-free RL, by using a learned, neural network for embedding states in a KNN and querying this to estimate state-action values. NEC was significantly more data efficient than most parametric approaches, but in many environments performance plateaued below that of parametric model-free  \n\nThis work extends NEC by, in addition to estimating state-action values as in NEC, learning a parametric corrector network which estimates a correction based on the query state embedding, all retrieved past-state embeddings and values. This is demonstrated in several environments to outperform NEC.\n\nPros:\n- Fairly well-communicated result.\n- Topic of significant interest.\n- Good baseline comparisons.\n\nCons:\n- A relatively straightforward extension to NEC, \n- Relatively modest empirical improvements (compared to prior work).\n\nMinor:\nTau (appears the Q-value estimates) is never explained. Is the normalization here done differently than in NEC?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semiparametric Reinforcement Learning", "abstract": "We introduce a semiparametric approach to deep reinforcement learning inspired by complementary learning systems theory in cognitive neuroscience.  Our approach allows a neural network to integrate nonparametric, episodic memory-based computations with parametric statistical learning in an end-to-end fashion. We give a deep Q network access to intermediate and final results of a differentiable approximation to k-nearest-neighbors performed on a dictionary of historic state-action embeddings.  Our method displays the early-learning advantage associated with episodic memory-based algorithms while mitigating the asymptotic performance disadvantage suffered by such approaches.  In several cases we find that our model learns even more quickly from few examples than pure kNN-based approaches.  Analysis shows that our semiparametric algorithm relies heavily on the kNN output early on and less so as training progresses, which is consistent with complementary learning systems theory.", "paperhash": "jain|semiparametric_reinforcement_learning", "keywords": ["deep learning", "nonparametric", "episodic learning", "nearest neighbors", "complementary learning systems", "reinforcement learning"], "_bibtex": "@misc{\n  jain2018semiparametric,\n  title={Semiparametric Reinforcement Learning},\n  author={Mika Sarkin Jain and Jack Lindsey},\n  year={2018},\n  url={https://openreview.net/forum?id=B1LRWg1wz}\n}", "authorids": ["mikasarkinjain@gmail.com", "jacklindsey@stanford.edu"], "authors": ["Mika Sarkin Jain", "Jack Lindsey"], "TL;DR": "Combining parametric and nonparametric methods in RL problems yields fast learning while maintaining good final performance.", "pdf": "/pdf/1fe77f06b94d77659e0aa3ce98f3da72deb2bb48.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582987905, "id": "ICLR.cc/2018/Workshop/-/Paper110/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper110/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper110/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper110/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper110/AnonReviewer3"], "reply": {"forum": "B1LRWg1wz", "replyto": "B1LRWg1wz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper110/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper110/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582987905}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582802142, "tcdate": 1520622581937, "number": 2, "cdate": 1520622581937, "id": "SJ0lhUxtz", "invitation": "ICLR.cc/2018/Workshop/-/Paper110/Official_Review", "forum": "B1LRWg1wz", "replyto": "B1LRWg1wz", "signatures": ["ICLR.cc/2018/Workshop/Paper110/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper110/AnonReviewer1"], "content": {"title": "a good paper", "rating": "7: Good paper, accept", "review": "This paper proposed a new learning scheme --- semiparametric reinforcement learning. This new approach lets a neural network to integrate nonparametric, episodic memory-based computations with parametric statistical learning in a seamless fashion. The method also has several advantages --- early-learning (associated with episodic memory-based algorithms) and reduced asymptotic performance disadvantage.\n\nOverall the paper is well-written. The methodology within the paper appears to be reasonable and very interesting to me. There is enough empirical results as well as technical details for a workshop paper. Because this is not my research area, I cannot judge its technical contribution. ", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semiparametric Reinforcement Learning", "abstract": "We introduce a semiparametric approach to deep reinforcement learning inspired by complementary learning systems theory in cognitive neuroscience.  Our approach allows a neural network to integrate nonparametric, episodic memory-based computations with parametric statistical learning in an end-to-end fashion. We give a deep Q network access to intermediate and final results of a differentiable approximation to k-nearest-neighbors performed on a dictionary of historic state-action embeddings.  Our method displays the early-learning advantage associated with episodic memory-based algorithms while mitigating the asymptotic performance disadvantage suffered by such approaches.  In several cases we find that our model learns even more quickly from few examples than pure kNN-based approaches.  Analysis shows that our semiparametric algorithm relies heavily on the kNN output early on and less so as training progresses, which is consistent with complementary learning systems theory.", "paperhash": "jain|semiparametric_reinforcement_learning", "keywords": ["deep learning", "nonparametric", "episodic learning", "nearest neighbors", "complementary learning systems", "reinforcement learning"], "_bibtex": "@misc{\n  jain2018semiparametric,\n  title={Semiparametric Reinforcement Learning},\n  author={Mika Sarkin Jain and Jack Lindsey},\n  year={2018},\n  url={https://openreview.net/forum?id=B1LRWg1wz}\n}", "authorids": ["mikasarkinjain@gmail.com", "jacklindsey@stanford.edu"], "authors": ["Mika Sarkin Jain", "Jack Lindsey"], "TL;DR": "Combining parametric and nonparametric methods in RL problems yields fast learning while maintaining good final performance.", "pdf": "/pdf/1fe77f06b94d77659e0aa3ce98f3da72deb2bb48.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582987905, "id": "ICLR.cc/2018/Workshop/-/Paper110/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper110/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper110/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper110/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper110/AnonReviewer3"], "reply": {"forum": "B1LRWg1wz", "replyto": "B1LRWg1wz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper110/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper110/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582987905}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582653914, "tcdate": 1520784970036, "number": 3, "cdate": 1520784970036, "id": "B1MLL0Mtf", "invitation": "ICLR.cc/2018/Workshop/-/Paper110/Official_Review", "forum": "B1LRWg1wz", "replyto": "B1LRWg1wz", "signatures": ["ICLR.cc/2018/Workshop/Paper110/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper110/AnonReviewer3"], "content": {"title": "Insufficient theoretical and experimental evidence for validity", "rating": "3: Clear rejection", "review": "Summary:\nThe Authors propose to extend the NEC algorithm by adding an additive correction term to its output. The correction term is computed by a Neural Network that takes as input the concatenation of the current state-action embedding as well as the embeddings and stored values of all 50 nearest neighbours that are used in the NEC algorithm.\n\nNovelty (6/10):\nWhile building on an existing algorithm, it proposes a non-trivial extension to it.\n\nClarity (4/10):\nPoints subtracted because I couldn't find any explanation or citation for the \"Roll-a-ball\" game.\nAlso, several important hyperparameters/explanations are missing, in particular:\n- What do \"Training Iterations\" mean in the performance plots?\n- If it does mean gradient updates, how many frames are seen between each gradient update?\nThis information is important as one goal of the algorithm is to improve learning speed compared to standard parametric methods.\n\nSignificance (8/10):\nI believe the question of how to combine parametric and nonparametric solutions is highly relevant for Deep RL as it has the potential of overcoming the problem of large required sample sizes before acceptable results are achieved, a major problem in many (real world) domains.\n\nQuality (3/10):\nI don't believe the suggested idea in it's current state is far enough developed.\nI neither find the experimental evidence nor the idea by itself convincing enough, as explained below.\n\nRegarding the idea:\nI don't find the concatenation of all 50 nearest neighbours and their values to compute the correction term (which is the main contribution of the paper) an obviously reasonable solution. The experimental evidence is too weak to convince me otherwise.\n- It creates an extremely large fully connected layer to combine all that information: About 1.7M parameters just in one layer for Atari despite only a 128 sized hidden layer (256 for each state embedding). Admittedly that is in the same order of magnitude than the entire DQN-network so it's not terrible, but it's certainly not an inventive or elegant solution.\n- The fully connected layer does not take into account the permutation invariance of the set of those 50 nearest neighbours. They don't mention any ordering of the set (e.g. by distance), so I assume the order is random and thereby permutation invariant.\n\nI'm also not sure whether the fundamental idea of the correction architecture makes sense: \nSince the idea of having a tabular representation in NEC is that this will adapt faster to new information than a NN, the target for the NN (the correction term) is changing faster than the NN is likely to be able to adapt.\nIn later stages of training when the parametric model starts to outperform the nonparametric one, the target for the correction-NN is potentially more difficult to fit to than in the standard parametric case, thereby hurting performance: Now, instead of targeting the state-action value directly, it targets the deviation of the kNN algorithm (potentially still fast changing) from the state action value. \nI might be missing something, but a discussion of this issue should be included in the paper as I don't believe it to be obvious of why it is supposed to work.\n\nConsequently, their claim that their architecture improves performances over NEC in early phases of training is therefore interesting but not backed up enough. \n\nExperimental evidence:\n- More and longer runs are needed. A single run is not representative in Atari as the variance can be quite large.\n- The information is not given in the paper, but if \"Training Iterations\" (x-axis on plots) means gradient updates and they also collect 16 frames in between updates (like in Pritzel et al), then they do not convincingly outperform NEC, as 2.5M updates correspond to 40M frames.\n  - Alien: ~1100 (Semip.) vs ~4500 (NEC)\n  - HERO: ~18000 (Semip., after 80M frames) and ~15000 (Semip. after 40M frames) vs ~17000 (NEC, 40M frames)\n  - Bowling: ~60 (Semip.) vs ~80 (ENC)\n  - Enduro: ~20 (Semip., after ~32M frames) vs ~0 (NEC)\n(All results are estimated by looking at the performance plots, the value at around 40M frames is reported unless stated otherwise)\n\n\nIn conclusion:\nPros:\n- Very relevant and significant topic\n\nCons:\n- Key information missing from paper (Roll-a-ball & how many frames are seen)\n- Insufficient experimental results (Not convincingly outperforming any baseline)\n- Insufficient theoretical analysis/discussion of their method\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semiparametric Reinforcement Learning", "abstract": "We introduce a semiparametric approach to deep reinforcement learning inspired by complementary learning systems theory in cognitive neuroscience.  Our approach allows a neural network to integrate nonparametric, episodic memory-based computations with parametric statistical learning in an end-to-end fashion. We give a deep Q network access to intermediate and final results of a differentiable approximation to k-nearest-neighbors performed on a dictionary of historic state-action embeddings.  Our method displays the early-learning advantage associated with episodic memory-based algorithms while mitigating the asymptotic performance disadvantage suffered by such approaches.  In several cases we find that our model learns even more quickly from few examples than pure kNN-based approaches.  Analysis shows that our semiparametric algorithm relies heavily on the kNN output early on and less so as training progresses, which is consistent with complementary learning systems theory.", "paperhash": "jain|semiparametric_reinforcement_learning", "keywords": ["deep learning", "nonparametric", "episodic learning", "nearest neighbors", "complementary learning systems", "reinforcement learning"], "_bibtex": "@misc{\n  jain2018semiparametric,\n  title={Semiparametric Reinforcement Learning},\n  author={Mika Sarkin Jain and Jack Lindsey},\n  year={2018},\n  url={https://openreview.net/forum?id=B1LRWg1wz}\n}", "authorids": ["mikasarkinjain@gmail.com", "jacklindsey@stanford.edu"], "authors": ["Mika Sarkin Jain", "Jack Lindsey"], "TL;DR": "Combining parametric and nonparametric methods in RL problems yields fast learning while maintaining good final performance.", "pdf": "/pdf/1fe77f06b94d77659e0aa3ce98f3da72deb2bb48.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582987905, "id": "ICLR.cc/2018/Workshop/-/Paper110/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper110/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper110/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper110/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper110/AnonReviewer3"], "reply": {"forum": "B1LRWg1wz", "replyto": "B1LRWg1wz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper110/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper110/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582987905}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573559821, "tcdate": 1521573559821, "number": 74, "cdate": 1521573559475, "id": "S1xaRA0KG", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "B1LRWg1wz", "replyto": "B1LRWg1wz", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semiparametric Reinforcement Learning", "abstract": "We introduce a semiparametric approach to deep reinforcement learning inspired by complementary learning systems theory in cognitive neuroscience.  Our approach allows a neural network to integrate nonparametric, episodic memory-based computations with parametric statistical learning in an end-to-end fashion. We give a deep Q network access to intermediate and final results of a differentiable approximation to k-nearest-neighbors performed on a dictionary of historic state-action embeddings.  Our method displays the early-learning advantage associated with episodic memory-based algorithms while mitigating the asymptotic performance disadvantage suffered by such approaches.  In several cases we find that our model learns even more quickly from few examples than pure kNN-based approaches.  Analysis shows that our semiparametric algorithm relies heavily on the kNN output early on and less so as training progresses, which is consistent with complementary learning systems theory.", "paperhash": "jain|semiparametric_reinforcement_learning", "keywords": ["deep learning", "nonparametric", "episodic learning", "nearest neighbors", "complementary learning systems", "reinforcement learning"], "_bibtex": "@misc{\n  jain2018semiparametric,\n  title={Semiparametric Reinforcement Learning},\n  author={Mika Sarkin Jain and Jack Lindsey},\n  year={2018},\n  url={https://openreview.net/forum?id=B1LRWg1wz}\n}", "authorids": ["mikasarkinjain@gmail.com", "jacklindsey@stanford.edu"], "authors": ["Mika Sarkin Jain", "Jack Lindsey"], "TL;DR": "Combining parametric and nonparametric methods in RL problems yields fast learning while maintaining good final performance.", "pdf": "/pdf/1fe77f06b94d77659e0aa3ce98f3da72deb2bb48.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}