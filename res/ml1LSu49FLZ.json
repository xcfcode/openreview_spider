{"notes": [{"id": "ml1LSu49FLZ", "original": "Uueu_JOPxy0", "number": 2755, "cdate": 1601308305522, "ddate": null, "tcdate": 1601308305522, "tmdate": 1614985642264, "tddate": null, "forum": "ml1LSu49FLZ", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Topic-aware Contextualized Transformers", "authorids": ["~Ruiying_Lu1", "~Bo_Chen1", "~Dan_dan_Guo1", "wds_dana@163.com", "~Mingyuan_Zhou1"], "authors": ["Ruiying Lu", "Bo Chen", "Dan dan Guo", "Dongsheng Wang", "Mingyuan Zhou"], "keywords": [], "abstract": "Training on disjoint fixed-length segments, Transformers successfully transform static word embeddings into contextualized word representations. However, they often restrict the context of a token to the segment it resides in and hence neglect the flow of contextual information across segments, failing to capture longer-term dependencies beyond the predefined segment length. This paper uses a probabilistic deep topic model to provide contextualized embeddings at both the token and segment levels. It also introduces topic self-attention and a contextual next-word embedding guided topic select-attention, injecting contextualized topic information into Transformer-based architectures. Moving beyond conventional Transformers that ignore longer-range word dependencies and contextualize their word representations at the segment level, the proposed method not only captures global semantic coherence of all segments and global word concurrence patterns, but also enriches the representation of each token by adapting it to its local context, which is not limited to the segment it resides in and can be flexibly defined according to the task. Experiments on various corpora show that adding only a few extra parameters, the proposed topic-aware contextualized transformers consistently outperform their conventional counterparts, and can be used to generate coherent sentences and paragraphs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lu|topicaware_contextualized_transformers", "supplementary_material": "/attachment/d0ffd545c8283f5cb15f4cd16b152fb33f047966.zip", "pdf": "/pdf/47fd4a366cda7228969cecfa9fb0984a6f2b4fd0.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Zb8hrRkSvQ", "_bibtex": "@misc{\nlu2021topicaware,\ntitle={Topic-aware Contextualized Transformers},\nauthor={Ruiying Lu and Bo Chen and Dan dan Guo and Dongsheng Wang and Mingyuan Zhou},\nyear={2021},\nurl={https://openreview.net/forum?id=ml1LSu49FLZ}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "jeQSJYwzGvX", "original": null, "number": 1, "cdate": 1610040519421, "ddate": null, "tcdate": 1610040519421, "tmdate": 1610474127965, "tddate": null, "forum": "ml1LSu49FLZ", "replyto": "ml1LSu49FLZ", "invitation": "ICLR.cc/2021/Conference/Paper2755/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper proposes enhancing contextualized word embeddings learned by Transformers by modeling long-range dependencies via a deep topic model, using a Poisson Gamma Belief Network (PGBN). The experimental results show incorporating topic information can further improve the performance of Transformers. While this is an interesting idea, reviewers pointed out some weaknesses: \n- GLUE evaluation is not a test of long-term dependencies, it remains unclear whether providing topic information of preceding segments is enough to allow the model to draw information from these segments that is useful for a task.\n- The improvement over the baseline does not seem to be significant.\n- The ablation study could be improved and more experiments could be done to understand the effect of hyperparameters choices from the topic model, such as the number of layers of PGBN as well as the topic number of each layer.\n- A comparison of the model performance for different lengths of input sequences would be helpful.\n- There are many recent methods for long.range transformer transformer variants, it would be interesting to compare them against the proposed latent topic-based method.\n\nUnfortunately, no answers are provided by the authors to the questions asked by the reviewers, which makes me recommend rejection."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Topic-aware Contextualized Transformers", "authorids": ["~Ruiying_Lu1", "~Bo_Chen1", "~Dan_dan_Guo1", "wds_dana@163.com", "~Mingyuan_Zhou1"], "authors": ["Ruiying Lu", "Bo Chen", "Dan dan Guo", "Dongsheng Wang", "Mingyuan Zhou"], "keywords": [], "abstract": "Training on disjoint fixed-length segments, Transformers successfully transform static word embeddings into contextualized word representations. However, they often restrict the context of a token to the segment it resides in and hence neglect the flow of contextual information across segments, failing to capture longer-term dependencies beyond the predefined segment length. This paper uses a probabilistic deep topic model to provide contextualized embeddings at both the token and segment levels. It also introduces topic self-attention and a contextual next-word embedding guided topic select-attention, injecting contextualized topic information into Transformer-based architectures. Moving beyond conventional Transformers that ignore longer-range word dependencies and contextualize their word representations at the segment level, the proposed method not only captures global semantic coherence of all segments and global word concurrence patterns, but also enriches the representation of each token by adapting it to its local context, which is not limited to the segment it resides in and can be flexibly defined according to the task. Experiments on various corpora show that adding only a few extra parameters, the proposed topic-aware contextualized transformers consistently outperform their conventional counterparts, and can be used to generate coherent sentences and paragraphs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lu|topicaware_contextualized_transformers", "supplementary_material": "/attachment/d0ffd545c8283f5cb15f4cd16b152fb33f047966.zip", "pdf": "/pdf/47fd4a366cda7228969cecfa9fb0984a6f2b4fd0.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Zb8hrRkSvQ", "_bibtex": "@misc{\nlu2021topicaware,\ntitle={Topic-aware Contextualized Transformers},\nauthor={Ruiying Lu and Bo Chen and Dan dan Guo and Dongsheng Wang and Mingyuan Zhou},\nyear={2021},\nurl={https://openreview.net/forum?id=ml1LSu49FLZ}\n}"}, "tags": [], "invitation": {"reply": {"forum": "ml1LSu49FLZ", "replyto": "ml1LSu49FLZ", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040519408, "tmdate": 1610474127949, "id": "ICLR.cc/2021/Conference/Paper2755/-/Decision"}}}, {"id": "OIqI3GXNBSZ", "original": null, "number": 2, "cdate": 1603674489058, "ddate": null, "tcdate": 1603674489058, "tmdate": 1605024139420, "tddate": null, "forum": "ml1LSu49FLZ", "replyto": "ml1LSu49FLZ", "invitation": "ICLR.cc/2021/Conference/Paper2755/-/Official_Review", "content": {"title": "Not significant improvement over baseline", "review": "The paper introduces a novel LM architecture that combines a Transformer with a PGBN topic model, enabling the transformer model to make use of addtional context topic information. The PGBN extracts topic information from the input, which is then used to enrich the information that is available to the transformer.\nThey propose three different methods of incorporating this context information from the topic model into the transformer: Topic embedding vectors to add to each token, segment embeddings to summarize preceding segments, and topic attention.\nThe topic model intervention can be applied to available pretrained transformers without the need to retrain the models from scratch. Experiments using pretrained GPT-2 and BERT show that incorporating topic information outperforms the respective baseline transformers both in language modeling perplexity and on the GLUE benchmark.\n\nTo my knowledge this is a novel model design. The information extracted by the topic model is provided to the transformer in three different ways, ensuring the utilization of all levels of topic information. It allows for topic information to be drawn from both the previous and the current segments of the input data.\nMoreover, the topic model augmentation can be applied to pretrained transformers. This makes it very versatile and greatly improves its usefulness in practice.\n\nHowever,\n\nThe presented work is motivated by the fact that transformers are limited in the length of the input they can condition on, and that topic information from previous segments could alleviate this problem. The model is then evaluated on GLUE, which is not a test of long-term dependencies. \nIt remains unclear whether providing topic information of preceding segments is enough to allow the model to draw information from these segments that is useful for a task, beyond mimicking their style.\n\nThe authors propose to use the learnt topics to perform conditioned topic-specific text generation. Given the multi-layer topic model architecture and the fact that GPT-2 uses BPE tokenization, splitting words into subwords, this raises the question whether most topics are interpretable enough to use them in a meaningful way for conditioning manually..\n\nMoreover, I do not find the results convincing that this methods works. I do not think that proper emphasis has been put on the baseline as it is diverging from its initial solution.\n\nIn general, I find this paper hard to read and keep track of what is being proposed. I find that it convolutes simple concepts and it could be a lot easier to understand the proposed method if written in a different way.\n\nI think the 54 citations in the introduction is unnecessary, please keep your paper more concise on what exactly you are building on top of and what you're proposing.\n\nI don't fully understand the TE embeddings. Is it N-gram distributions that you make a topic model over?\n\nIs WE a \"normal\" word embedding?\n\nPlease elaborate on E_A and E_B", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2755/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2755/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Topic-aware Contextualized Transformers", "authorids": ["~Ruiying_Lu1", "~Bo_Chen1", "~Dan_dan_Guo1", "wds_dana@163.com", "~Mingyuan_Zhou1"], "authors": ["Ruiying Lu", "Bo Chen", "Dan dan Guo", "Dongsheng Wang", "Mingyuan Zhou"], "keywords": [], "abstract": "Training on disjoint fixed-length segments, Transformers successfully transform static word embeddings into contextualized word representations. However, they often restrict the context of a token to the segment it resides in and hence neglect the flow of contextual information across segments, failing to capture longer-term dependencies beyond the predefined segment length. This paper uses a probabilistic deep topic model to provide contextualized embeddings at both the token and segment levels. It also introduces topic self-attention and a contextual next-word embedding guided topic select-attention, injecting contextualized topic information into Transformer-based architectures. Moving beyond conventional Transformers that ignore longer-range word dependencies and contextualize their word representations at the segment level, the proposed method not only captures global semantic coherence of all segments and global word concurrence patterns, but also enriches the representation of each token by adapting it to its local context, which is not limited to the segment it resides in and can be flexibly defined according to the task. Experiments on various corpora show that adding only a few extra parameters, the proposed topic-aware contextualized transformers consistently outperform their conventional counterparts, and can be used to generate coherent sentences and paragraphs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lu|topicaware_contextualized_transformers", "supplementary_material": "/attachment/d0ffd545c8283f5cb15f4cd16b152fb33f047966.zip", "pdf": "/pdf/47fd4a366cda7228969cecfa9fb0984a6f2b4fd0.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Zb8hrRkSvQ", "_bibtex": "@misc{\nlu2021topicaware,\ntitle={Topic-aware Contextualized Transformers},\nauthor={Ruiying Lu and Bo Chen and Dan dan Guo and Dongsheng Wang and Mingyuan Zhou},\nyear={2021},\nurl={https://openreview.net/forum?id=ml1LSu49FLZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ml1LSu49FLZ", "replyto": "ml1LSu49FLZ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2755/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538089336, "tmdate": 1606915805567, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2755/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2755/-/Official_Review"}}}, {"id": "FYReshArlB", "original": null, "number": 3, "cdate": 1603936742601, "ddate": null, "tcdate": 1603936742601, "tmdate": 1605024139301, "tddate": null, "forum": "ml1LSu49FLZ", "replyto": "ml1LSu49FLZ", "invitation": "ICLR.cc/2021/Conference/Paper2755/-/Official_Review", "content": {"title": "This paper introduces an interesting idea of enhancing the contextualised word embedding learned by Transformers with long range semantic dependencies via topic learned by Poisson Gamma Belief Network (PGBN), a deep topic model.", "review": "This paper introduces an interesting idea of enhancing the contextualised word embedding learned by Transformers with long-range semantic dependencies via topic learned by Poisson Gamma Belief Network (PGBN), a deep topic model. To leverage the topic information to guide the learning process of transformers, the authors proposed two types of topic-ware embeddings and one topic attention mechanism. The experimental results show incorporating topic information can further improve the performance of Transformers.\n\nOverall, it is an interesting paper. I would lean towards accepting it. I like the idea of injecting topic information into neural language models, like transformer. The ablation study shows that the performance of base models increases with the topic information.\n\nPros\n1.  Capturing longer-range term dependency in Transformers is an important problem. Although there are existing works on increasing the size of the input, such as Hierarchical transformers, the authors propose to used deep probabilistic topic model to leverage the semantic information via latent topics.\n2. The paper provides both quantitative results on both language generation and GLUE tasks and qualitative analysis, both show the necessity of considering topic information in transformers. The ablation study on the three components used to inject topic information is good\n3. The paper is written pretty well, and easy to read.\n\nCons:\n1. Although the ablation study is good already, I would suggest the authors conduct the following ablation studies wonder 1) how the regularisation terms would affect the performance? 2)  will the interleaving discussed on page 5 have an impact on performance?\n2. Figure 4 visualises the topic attention mechanism. What are those frequently attended topics?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2755/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2755/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Topic-aware Contextualized Transformers", "authorids": ["~Ruiying_Lu1", "~Bo_Chen1", "~Dan_dan_Guo1", "wds_dana@163.com", "~Mingyuan_Zhou1"], "authors": ["Ruiying Lu", "Bo Chen", "Dan dan Guo", "Dongsheng Wang", "Mingyuan Zhou"], "keywords": [], "abstract": "Training on disjoint fixed-length segments, Transformers successfully transform static word embeddings into contextualized word representations. However, they often restrict the context of a token to the segment it resides in and hence neglect the flow of contextual information across segments, failing to capture longer-term dependencies beyond the predefined segment length. This paper uses a probabilistic deep topic model to provide contextualized embeddings at both the token and segment levels. It also introduces topic self-attention and a contextual next-word embedding guided topic select-attention, injecting contextualized topic information into Transformer-based architectures. Moving beyond conventional Transformers that ignore longer-range word dependencies and contextualize their word representations at the segment level, the proposed method not only captures global semantic coherence of all segments and global word concurrence patterns, but also enriches the representation of each token by adapting it to its local context, which is not limited to the segment it resides in and can be flexibly defined according to the task. Experiments on various corpora show that adding only a few extra parameters, the proposed topic-aware contextualized transformers consistently outperform their conventional counterparts, and can be used to generate coherent sentences and paragraphs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lu|topicaware_contextualized_transformers", "supplementary_material": "/attachment/d0ffd545c8283f5cb15f4cd16b152fb33f047966.zip", "pdf": "/pdf/47fd4a366cda7228969cecfa9fb0984a6f2b4fd0.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Zb8hrRkSvQ", "_bibtex": "@misc{\nlu2021topicaware,\ntitle={Topic-aware Contextualized Transformers},\nauthor={Ruiying Lu and Bo Chen and Dan dan Guo and Dongsheng Wang and Mingyuan Zhou},\nyear={2021},\nurl={https://openreview.net/forum?id=ml1LSu49FLZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ml1LSu49FLZ", "replyto": "ml1LSu49FLZ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2755/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538089336, "tmdate": 1606915805567, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2755/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2755/-/Official_Review"}}}, {"id": "b_LgHFqdXwD", "original": null, "number": 1, "cdate": 1603631420791, "ddate": null, "tcdate": 1603631420791, "tmdate": 1605024139229, "tddate": null, "forum": "ml1LSu49FLZ", "replyto": "ml1LSu49FLZ", "invitation": "ICLR.cc/2021/Conference/Paper2755/-/Official_Review", "content": {"title": "Official Blind Review #2", "review": "Summary:\n\nThis paper introduces a global topic model into the Transformer to enrich longer-term dependencies beyond the fixed training segment, including contextual token/segment embedding (TE/SE) and Topic Attention (TA). However, some components seem to be unnecessary: The function of \u201c+TA+SE\u201d is very trivial by comparing \u201c+Topic attention (TA)\u201d with \u201c+TE + SE + TA\u201d in Table 1. In addition, the experiments of GLUE in Table 2 are only conducted on \u201cBert-base\u201d with marginal improvements.\n\n---------------------------------------\nStrength:\n\n1. This work is the first to integrate the contextualized topic information via a deep probabilistic topic model into the Transformer architecture.\n\n2. Three different types of contextual topic information are provided to capture long-range semantic dependencies into the Transformer models. \n\n3. Experiments on various corpora show the effectiveness of the proposed methods, with only a few extra parameters.\n\n---------------------------------------\nWeakness:\n\n1. The ablation study is weak. TA and SE seem redundant in Table 1 compared with TA. The results in Table2 are marginal, given that the BERT-base baseline is relatively sensitive to hyperparameters. More language models (maybe larger) should be tested. Moreover, in view of (a), TE may be redundant together with TA. Since the results in Table are based on TE+TA, there should be a TE+TA comparison in Table 1. \n\n2. \u201cTopic attention between words and topics\u201d in the analysis seems farfetched because the attended topics of \u201cmarket\u201d are more than 34-th and 24-th. There are more similar but less relevant topics like 82-th, 50-th, etc. According to the examples, the topic information could be related to the semantic similarity of words. There might be a risk that the topic information would have already been entailed in the pre-trained language models.\n\n3. No experiments show the effect of hyperparameters choices from the topic model, such as the number of layers of PGBN as well as the topic number of each layer.\n\n4. Since this work is motivated to capture longer-range dependencies, it is not clear how the topic information helps. A comparison of the model performance for different lengths of input sequences would be helpful.\n\n5. There is no comparison with other methods about long-range transformer variants, which focus on the transformer model\u2019s self-attention mechanism mentioned in Line 3, Page2.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2755/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2755/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Topic-aware Contextualized Transformers", "authorids": ["~Ruiying_Lu1", "~Bo_Chen1", "~Dan_dan_Guo1", "wds_dana@163.com", "~Mingyuan_Zhou1"], "authors": ["Ruiying Lu", "Bo Chen", "Dan dan Guo", "Dongsheng Wang", "Mingyuan Zhou"], "keywords": [], "abstract": "Training on disjoint fixed-length segments, Transformers successfully transform static word embeddings into contextualized word representations. However, they often restrict the context of a token to the segment it resides in and hence neglect the flow of contextual information across segments, failing to capture longer-term dependencies beyond the predefined segment length. This paper uses a probabilistic deep topic model to provide contextualized embeddings at both the token and segment levels. It also introduces topic self-attention and a contextual next-word embedding guided topic select-attention, injecting contextualized topic information into Transformer-based architectures. Moving beyond conventional Transformers that ignore longer-range word dependencies and contextualize their word representations at the segment level, the proposed method not only captures global semantic coherence of all segments and global word concurrence patterns, but also enriches the representation of each token by adapting it to its local context, which is not limited to the segment it resides in and can be flexibly defined according to the task. Experiments on various corpora show that adding only a few extra parameters, the proposed topic-aware contextualized transformers consistently outperform their conventional counterparts, and can be used to generate coherent sentences and paragraphs.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lu|topicaware_contextualized_transformers", "supplementary_material": "/attachment/d0ffd545c8283f5cb15f4cd16b152fb33f047966.zip", "pdf": "/pdf/47fd4a366cda7228969cecfa9fb0984a6f2b4fd0.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Zb8hrRkSvQ", "_bibtex": "@misc{\nlu2021topicaware,\ntitle={Topic-aware Contextualized Transformers},\nauthor={Ruiying Lu and Bo Chen and Dan dan Guo and Dongsheng Wang and Mingyuan Zhou},\nyear={2021},\nurl={https://openreview.net/forum?id=ml1LSu49FLZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ml1LSu49FLZ", "replyto": "ml1LSu49FLZ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2755/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538089336, "tmdate": 1606915805567, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2755/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2755/-/Official_Review"}}}], "count": 5}