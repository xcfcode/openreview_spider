{"notes": [{"id": "e3bhF_p0T7c", "original": "_LPcQBSf6-U", "number": 2581, "cdate": 1601308285709, "ddate": null, "tcdate": 1601308285709, "tmdate": 1614985640851, "tddate": null, "forum": "e3bhF_p0T7c", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Analysis of Alignment Phenomenon in Simple Teacher-student Networks with Finite Width", "authorids": ["~Hanlin_Zhu2", "~Chengyang_Ying2", "~Song_Zuo1"], "authors": ["Hanlin Zhu", "Chengyang Ying", "Song Zuo"], "keywords": ["alignment", "finite width network", "teacher student model", "angular distance function"], "abstract": "Recent theoretical analysis suggests that ultra-wide neural networks always converge to global minima near the initialization under first order methods. However, the convergence property of neural networks with finite width could be very different. The simplest experiment with two-layer teacher-student networks shows that the input weights of student neurons eventually align with one of the teacher neurons. This suggests a distinct convergence nature for ``not-too-wide'' neural networks that there might not be any local minima near the initialization. As the theoretical justification, we prove that under the most basic settings, all student neurons must align with the teacher neuron at any local minima. The methodology is extendable to more general cases, where the proof can be reduced to analyzing the properties of a special class of functions that we call {\\em Angular Distance (AD) function}. Finally, we demonstrate that these properties can be easily verified numerically.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|analysis_of_alignment_phenomenon_in_simple_teacherstudent_networks_with_finite_width", "one-sentence_summary": "We prove the uniqueness of local minima in a simple student-teacher framework, and conjecture the same conclusion for more general settings. ", "supplementary_material": "/attachment/bd5947b09954f58c89a69054ca6148bdf55f7c2d.zip", "pdf": "/pdf/8bf37a95497c98e0d8908a84113320c4d997d314.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JamhVIew04", "_bibtex": "@misc{\nzhu2021analysis,\ntitle={Analysis of Alignment Phenomenon in Simple Teacher-student Networks with Finite Width},\nauthor={Hanlin Zhu and Chengyang Ying and Song Zuo},\nyear={2021},\nurl={https://openreview.net/forum?id=e3bhF_p0T7c}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "eOnsyf3S496", "original": null, "number": 1, "cdate": 1610040520784, "ddate": null, "tcdate": 1610040520784, "tmdate": 1610474129468, "tddate": null, "forum": "e3bhF_p0T7c", "replyto": "e3bhF_p0T7c", "invitation": "ICLR.cc/2021/Conference/Paper2581/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The reviewer seems to reach a consensus that the paper is not ready for publication at ICLR. One of the major issues seems to be that the paper only analyzes the case of $d=2$. (In the AC's opinion,  $d>2$ might be fundamentally more difficult to analyze than $d=2$). \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analysis of Alignment Phenomenon in Simple Teacher-student Networks with Finite Width", "authorids": ["~Hanlin_Zhu2", "~Chengyang_Ying2", "~Song_Zuo1"], "authors": ["Hanlin Zhu", "Chengyang Ying", "Song Zuo"], "keywords": ["alignment", "finite width network", "teacher student model", "angular distance function"], "abstract": "Recent theoretical analysis suggests that ultra-wide neural networks always converge to global minima near the initialization under first order methods. However, the convergence property of neural networks with finite width could be very different. The simplest experiment with two-layer teacher-student networks shows that the input weights of student neurons eventually align with one of the teacher neurons. This suggests a distinct convergence nature for ``not-too-wide'' neural networks that there might not be any local minima near the initialization. As the theoretical justification, we prove that under the most basic settings, all student neurons must align with the teacher neuron at any local minima. The methodology is extendable to more general cases, where the proof can be reduced to analyzing the properties of a special class of functions that we call {\\em Angular Distance (AD) function}. Finally, we demonstrate that these properties can be easily verified numerically.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|analysis_of_alignment_phenomenon_in_simple_teacherstudent_networks_with_finite_width", "one-sentence_summary": "We prove the uniqueness of local minima in a simple student-teacher framework, and conjecture the same conclusion for more general settings. ", "supplementary_material": "/attachment/bd5947b09954f58c89a69054ca6148bdf55f7c2d.zip", "pdf": "/pdf/8bf37a95497c98e0d8908a84113320c4d997d314.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JamhVIew04", "_bibtex": "@misc{\nzhu2021analysis,\ntitle={Analysis of Alignment Phenomenon in Simple Teacher-student Networks with Finite Width},\nauthor={Hanlin Zhu and Chengyang Ying and Song Zuo},\nyear={2021},\nurl={https://openreview.net/forum?id=e3bhF_p0T7c}\n}"}, "tags": [], "invitation": {"reply": {"forum": "e3bhF_p0T7c", "replyto": "e3bhF_p0T7c", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040520771, "tmdate": 1610474129454, "id": "ICLR.cc/2021/Conference/Paper2581/-/Decision"}}}, {"id": "d0bYdDc2ekk", "original": null, "number": 1, "cdate": 1603702803974, "ddate": null, "tcdate": 1603702803974, "tmdate": 1606807140832, "tddate": null, "forum": "e3bhF_p0T7c", "replyto": "e3bhF_p0T7c", "invitation": "ICLR.cc/2021/Conference/Paper2581/-/Official_Review", "content": {"title": "Review", "review": "[Summary]\n\nThis paper studies the optimization landscape of one-hidden-layer neural networks in the teacher-student setting, where the ground truth teacher network is one relu and the learner network is m>=2 relus. The paper proves that (1) when m=2, any stationary point (of the student network) has to be aligned with the teacher; (2) when m>=3, stationary points satisfying certain additional conditions have to be aligned with the teacher. Above, alignment means that the student network is perfectly equal to the teacher. \n\n[Pros]\n\nThe one-hidden-layer teacher-student setting studied in this paper is an important and difficult theoretical problem. The reason why GD on an over-parameterized student could perfectly recover the teacher has been a somewhat long-standing open problem. This paper makes progress on this problem by proving the alignment property at m=2, and showing that the \u201cdeterminant condition\u201d implies alignment at m>=3.\n\n\n[Cons]\n\nI feel like the direction of studying alignment in this specific one-neuron teacher model may be a bit problematic, and may not be that significant or generalizable.\n\n---First, results in this paper seem to be about the *stationary points*, not the *local minima* (which at least additionally requires the Hessian to be PSD). This can be seen from Eq (1) and (2) which is only the stationarity condition (gradient equals zero) and does not consider the Hessian. This means that all the subsequent claims are about stationary points and must also contain the saddle points. It seems like the authors may be unaware of this. (The paper talks about \u201clocal minima\u201d where it really means gradient equals zero.)\n\n---Second, the fact that the theory contains saddle points makes it somewhat confusing --- If a saddle point does exist, then it has to be non-aligned (because a saddle point must have non-zero loss). So the present result seems to imply that there is no saddle point at m=2 and (likely) no saddle point at m>=3, when the teacher is one relu. (Could the authors confirm or comment on this?)\n\n---Further, even if we restrict attention to local minima, (Safran and Shamir 2018) already showed that when the teacher contains multiple neurons, the student network could have spurious local minima with non-zero risk (i.e. non-aligned minima). Therefore we could not hope for alignment for all local minima when the teacher has multiple neurons, which implies that the studying the alignment in the one-neuron teacher model may not be generalizable.\n\n------\nThank the authors for the responses. While I agree with some of the points made by the authors (e.g. understanding the base case is interesting), I am still concerned about the significance of the result. Therefore I would like to keep my original evaluation.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2581/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2581/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analysis of Alignment Phenomenon in Simple Teacher-student Networks with Finite Width", "authorids": ["~Hanlin_Zhu2", "~Chengyang_Ying2", "~Song_Zuo1"], "authors": ["Hanlin Zhu", "Chengyang Ying", "Song Zuo"], "keywords": ["alignment", "finite width network", "teacher student model", "angular distance function"], "abstract": "Recent theoretical analysis suggests that ultra-wide neural networks always converge to global minima near the initialization under first order methods. However, the convergence property of neural networks with finite width could be very different. The simplest experiment with two-layer teacher-student networks shows that the input weights of student neurons eventually align with one of the teacher neurons. This suggests a distinct convergence nature for ``not-too-wide'' neural networks that there might not be any local minima near the initialization. As the theoretical justification, we prove that under the most basic settings, all student neurons must align with the teacher neuron at any local minima. The methodology is extendable to more general cases, where the proof can be reduced to analyzing the properties of a special class of functions that we call {\\em Angular Distance (AD) function}. Finally, we demonstrate that these properties can be easily verified numerically.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|analysis_of_alignment_phenomenon_in_simple_teacherstudent_networks_with_finite_width", "one-sentence_summary": "We prove the uniqueness of local minima in a simple student-teacher framework, and conjecture the same conclusion for more general settings. ", "supplementary_material": "/attachment/bd5947b09954f58c89a69054ca6148bdf55f7c2d.zip", "pdf": "/pdf/8bf37a95497c98e0d8908a84113320c4d997d314.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JamhVIew04", "_bibtex": "@misc{\nzhu2021analysis,\ntitle={Analysis of Alignment Phenomenon in Simple Teacher-student Networks with Finite Width},\nauthor={Hanlin Zhu and Chengyang Ying and Song Zuo},\nyear={2021},\nurl={https://openreview.net/forum?id=e3bhF_p0T7c}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "e3bhF_p0T7c", "replyto": "e3bhF_p0T7c", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2581/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538093127, "tmdate": 1606915805959, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2581/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2581/-/Official_Review"}}}, {"id": "tG8knaiH18K", "original": null, "number": 6, "cdate": 1606037229891, "ddate": null, "tcdate": 1606037229891, "tmdate": 1606037229891, "tddate": null, "forum": "e3bhF_p0T7c", "replyto": "GaF-16SbAv5", "invitation": "ICLR.cc/2021/Conference/Paper2581/-/Official_Comment", "content": {"title": "Thanks for the clarifications", "comment": "I read the other reviews and responses and did not change the score."}, "signatures": ["ICLR.cc/2021/Conference/Paper2581/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2581/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analysis of Alignment Phenomenon in Simple Teacher-student Networks with Finite Width", "authorids": ["~Hanlin_Zhu2", "~Chengyang_Ying2", "~Song_Zuo1"], "authors": ["Hanlin Zhu", "Chengyang Ying", "Song Zuo"], "keywords": ["alignment", "finite width network", "teacher student model", "angular distance function"], "abstract": "Recent theoretical analysis suggests that ultra-wide neural networks always converge to global minima near the initialization under first order methods. However, the convergence property of neural networks with finite width could be very different. The simplest experiment with two-layer teacher-student networks shows that the input weights of student neurons eventually align with one of the teacher neurons. This suggests a distinct convergence nature for ``not-too-wide'' neural networks that there might not be any local minima near the initialization. As the theoretical justification, we prove that under the most basic settings, all student neurons must align with the teacher neuron at any local minima. The methodology is extendable to more general cases, where the proof can be reduced to analyzing the properties of a special class of functions that we call {\\em Angular Distance (AD) function}. Finally, we demonstrate that these properties can be easily verified numerically.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|analysis_of_alignment_phenomenon_in_simple_teacherstudent_networks_with_finite_width", "one-sentence_summary": "We prove the uniqueness of local minima in a simple student-teacher framework, and conjecture the same conclusion for more general settings. ", "supplementary_material": "/attachment/bd5947b09954f58c89a69054ca6148bdf55f7c2d.zip", "pdf": "/pdf/8bf37a95497c98e0d8908a84113320c4d997d314.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JamhVIew04", "_bibtex": "@misc{\nzhu2021analysis,\ntitle={Analysis of Alignment Phenomenon in Simple Teacher-student Networks with Finite Width},\nauthor={Hanlin Zhu and Chengyang Ying and Song Zuo},\nyear={2021},\nurl={https://openreview.net/forum?id=e3bhF_p0T7c}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "e3bhF_p0T7c", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2581/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2581/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2581/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2581/Authors|ICLR.cc/2021/Conference/Paper2581/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2581/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846707, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2581/-/Official_Comment"}}}, {"id": "8BCE0cliWzQ", "original": null, "number": 5, "cdate": 1606032344361, "ddate": null, "tcdate": 1606032344361, "tmdate": 1606032344361, "tddate": null, "forum": "e3bhF_p0T7c", "replyto": "d0bYdDc2ekk", "invitation": "ICLR.cc/2021/Conference/Paper2581/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thank the reviewer for the insightful and helpful comments. \n\n\u201cResults in this paper seem to be about the stationary points, not the local minima.\u201d\nWe study stationary points and our conclusions apply to local minima. We established the central equation system (eq (2)) based on the first order conditions (eq (1)), as the main focus of this paper is about converged states rather than local minima. In principle, just as the reviewer said, the solutions to eq (2) may also contain saddle points. However, as we proved in the paper (or conjectured for the general cases), all solutions to eq (2) must be trivial, namely *all stationary points should be global minima*. Therefore, the saddle point issue that the reviewer worried about should never happen. \n\n\u201cSo the present result seems to imply that there is no saddle point at m=2 and (likely) no saddle point at m>=3, when the teacher is one relu.\u201d\nYes, our theorems (conjectures) imply that there is (likely) no saddle point.\n\n\u201cWe could not hope for alignment for all local minima when the teacher has multiple neurons, which implies that studying the alignment in the one-neuron teacher model may not be generalizable.\u201d\nWe didn\u2019t hope for the alignment to generalize to all neural networks. While combining our results and Safran and Shamir (2018), we know that the alignment guarantee will break for networks with more than X teacher neurals, where X no more than 6 and likely no less than 2.  \nIn particular, having a clear understanding of the alignment in the basic case will help us understand why it does not generalize for more complicated cases.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2581/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2581/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analysis of Alignment Phenomenon in Simple Teacher-student Networks with Finite Width", "authorids": ["~Hanlin_Zhu2", "~Chengyang_Ying2", "~Song_Zuo1"], "authors": ["Hanlin Zhu", "Chengyang Ying", "Song Zuo"], "keywords": ["alignment", "finite width network", "teacher student model", "angular distance function"], "abstract": "Recent theoretical analysis suggests that ultra-wide neural networks always converge to global minima near the initialization under first order methods. However, the convergence property of neural networks with finite width could be very different. The simplest experiment with two-layer teacher-student networks shows that the input weights of student neurons eventually align with one of the teacher neurons. This suggests a distinct convergence nature for ``not-too-wide'' neural networks that there might not be any local minima near the initialization. As the theoretical justification, we prove that under the most basic settings, all student neurons must align with the teacher neuron at any local minima. The methodology is extendable to more general cases, where the proof can be reduced to analyzing the properties of a special class of functions that we call {\\em Angular Distance (AD) function}. Finally, we demonstrate that these properties can be easily verified numerically.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|analysis_of_alignment_phenomenon_in_simple_teacherstudent_networks_with_finite_width", "one-sentence_summary": "We prove the uniqueness of local minima in a simple student-teacher framework, and conjecture the same conclusion for more general settings. ", "supplementary_material": "/attachment/bd5947b09954f58c89a69054ca6148bdf55f7c2d.zip", "pdf": "/pdf/8bf37a95497c98e0d8908a84113320c4d997d314.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JamhVIew04", "_bibtex": "@misc{\nzhu2021analysis,\ntitle={Analysis of Alignment Phenomenon in Simple Teacher-student Networks with Finite Width},\nauthor={Hanlin Zhu and Chengyang Ying and Song Zuo},\nyear={2021},\nurl={https://openreview.net/forum?id=e3bhF_p0T7c}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "e3bhF_p0T7c", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2581/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2581/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2581/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2581/Authors|ICLR.cc/2021/Conference/Paper2581/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2581/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846707, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2581/-/Official_Comment"}}}, {"id": "o-aoGlZP2-3", "original": null, "number": 4, "cdate": 1606032254956, "ddate": null, "tcdate": 1606032254956, "tmdate": 1606032254956, "tddate": null, "forum": "e3bhF_p0T7c", "replyto": "VfaW484GAuR", "invitation": "ICLR.cc/2021/Conference/Paper2581/-/Official_Comment", "content": {"title": "Respond to Reviewer 4", "comment": "We thank the reviewer for the insightful and helpful comments. \n\n\u201cWhy the results only consider the case when d=2?\u201d\nThe problem is already very challenging even for the simplest d = 2 and m = 3 case. We would love to generalize the results for d > 2 in the future.\n\n\u201cThe presentation of the current paper needs to be improved.\u201d\nThanks for your suggestion, we will improve our paper accordingly.\n\nThanks for the suggested references. We will cite them.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2581/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2581/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analysis of Alignment Phenomenon in Simple Teacher-student Networks with Finite Width", "authorids": ["~Hanlin_Zhu2", "~Chengyang_Ying2", "~Song_Zuo1"], "authors": ["Hanlin Zhu", "Chengyang Ying", "Song Zuo"], "keywords": ["alignment", "finite width network", "teacher student model", "angular distance function"], "abstract": "Recent theoretical analysis suggests that ultra-wide neural networks always converge to global minima near the initialization under first order methods. However, the convergence property of neural networks with finite width could be very different. The simplest experiment with two-layer teacher-student networks shows that the input weights of student neurons eventually align with one of the teacher neurons. This suggests a distinct convergence nature for ``not-too-wide'' neural networks that there might not be any local minima near the initialization. As the theoretical justification, we prove that under the most basic settings, all student neurons must align with the teacher neuron at any local minima. The methodology is extendable to more general cases, where the proof can be reduced to analyzing the properties of a special class of functions that we call {\\em Angular Distance (AD) function}. Finally, we demonstrate that these properties can be easily verified numerically.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|analysis_of_alignment_phenomenon_in_simple_teacherstudent_networks_with_finite_width", "one-sentence_summary": "We prove the uniqueness of local minima in a simple student-teacher framework, and conjecture the same conclusion for more general settings. ", "supplementary_material": "/attachment/bd5947b09954f58c89a69054ca6148bdf55f7c2d.zip", "pdf": "/pdf/8bf37a95497c98e0d8908a84113320c4d997d314.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JamhVIew04", "_bibtex": "@misc{\nzhu2021analysis,\ntitle={Analysis of Alignment Phenomenon in Simple Teacher-student Networks with Finite Width},\nauthor={Hanlin Zhu and Chengyang Ying and Song Zuo},\nyear={2021},\nurl={https://openreview.net/forum?id=e3bhF_p0T7c}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "e3bhF_p0T7c", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2581/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2581/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2581/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2581/Authors|ICLR.cc/2021/Conference/Paper2581/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2581/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846707, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2581/-/Official_Comment"}}}, {"id": "GaF-16SbAv5", "original": null, "number": 3, "cdate": 1606032156519, "ddate": null, "tcdate": 1606032156519, "tmdate": 1606032156519, "tddate": null, "forum": "e3bhF_p0T7c", "replyto": "WcNiNJjNCSd", "invitation": "ICLR.cc/2021/Conference/Paper2581/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank the reviewer for the insightful and helpful comments. \n\n\u201cThe authors mention d=2 only once in the paper. This should be clearly stated in the Abstract, Introduction and Conclusion sections.\u201d \nWe will emphasize that and consider adding more experiments for general settings in the future.\n\n\u201cComparison with Tian (2020)\u201d \nTian (2020) required a quite strong assumption that the gradient is zero for all possible inputs. Instead, our result is built on the most standard condition that gradient descent is stable (expected gradient is zero). We will add the discussion to the paper.\n\n\u201cEmpirically justify Conjecture 5.5?\u201d \nThanks for your suggestion. We will perform more experiments to verify the conjecture.\n\n\u201cPerform experiments to verify whether it will converge\u201d \nWe agree that this is good to have in the paper, yet this is not the main focus. We will try to incorporate more experiments in the future.\n\nWe will add the missing citations, and thank you for the references.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2581/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2581/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analysis of Alignment Phenomenon in Simple Teacher-student Networks with Finite Width", "authorids": ["~Hanlin_Zhu2", "~Chengyang_Ying2", "~Song_Zuo1"], "authors": ["Hanlin Zhu", "Chengyang Ying", "Song Zuo"], "keywords": ["alignment", "finite width network", "teacher student model", "angular distance function"], "abstract": "Recent theoretical analysis suggests that ultra-wide neural networks always converge to global minima near the initialization under first order methods. However, the convergence property of neural networks with finite width could be very different. The simplest experiment with two-layer teacher-student networks shows that the input weights of student neurons eventually align with one of the teacher neurons. This suggests a distinct convergence nature for ``not-too-wide'' neural networks that there might not be any local minima near the initialization. As the theoretical justification, we prove that under the most basic settings, all student neurons must align with the teacher neuron at any local minima. The methodology is extendable to more general cases, where the proof can be reduced to analyzing the properties of a special class of functions that we call {\\em Angular Distance (AD) function}. Finally, we demonstrate that these properties can be easily verified numerically.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|analysis_of_alignment_phenomenon_in_simple_teacherstudent_networks_with_finite_width", "one-sentence_summary": "We prove the uniqueness of local minima in a simple student-teacher framework, and conjecture the same conclusion for more general settings. ", "supplementary_material": "/attachment/bd5947b09954f58c89a69054ca6148bdf55f7c2d.zip", "pdf": "/pdf/8bf37a95497c98e0d8908a84113320c4d997d314.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JamhVIew04", "_bibtex": "@misc{\nzhu2021analysis,\ntitle={Analysis of Alignment Phenomenon in Simple Teacher-student Networks with Finite Width},\nauthor={Hanlin Zhu and Chengyang Ying and Song Zuo},\nyear={2021},\nurl={https://openreview.net/forum?id=e3bhF_p0T7c}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "e3bhF_p0T7c", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2581/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2581/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2581/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2581/Authors|ICLR.cc/2021/Conference/Paper2581/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2581/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846707, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2581/-/Official_Comment"}}}, {"id": "1hLxxx2-4GL", "original": null, "number": 2, "cdate": 1606031942275, "ddate": null, "tcdate": 1606031942275, "tmdate": 1606031989487, "tddate": null, "forum": "e3bhF_p0T7c", "replyto": "6-0mu0kqvvT", "invitation": "ICLR.cc/2021/Conference/Paper2581/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We thank the reviewer for the insightful and helpful comments. \n\n\u201cWhat if the critical point condition only holds approximately up to a small error of \u03f5? \u2026 What about the proof of Lemma 4.5 and Theorem 5.1? Would the analysis extend to allow for sampling errors or not?\u201d \nThis is a very good question for future work. We are also interested in answering these questions, yet these might not be the main focus of this paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2581/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2581/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analysis of Alignment Phenomenon in Simple Teacher-student Networks with Finite Width", "authorids": ["~Hanlin_Zhu2", "~Chengyang_Ying2", "~Song_Zuo1"], "authors": ["Hanlin Zhu", "Chengyang Ying", "Song Zuo"], "keywords": ["alignment", "finite width network", "teacher student model", "angular distance function"], "abstract": "Recent theoretical analysis suggests that ultra-wide neural networks always converge to global minima near the initialization under first order methods. However, the convergence property of neural networks with finite width could be very different. The simplest experiment with two-layer teacher-student networks shows that the input weights of student neurons eventually align with one of the teacher neurons. This suggests a distinct convergence nature for ``not-too-wide'' neural networks that there might not be any local minima near the initialization. As the theoretical justification, we prove that under the most basic settings, all student neurons must align with the teacher neuron at any local minima. The methodology is extendable to more general cases, where the proof can be reduced to analyzing the properties of a special class of functions that we call {\\em Angular Distance (AD) function}. Finally, we demonstrate that these properties can be easily verified numerically.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|analysis_of_alignment_phenomenon_in_simple_teacherstudent_networks_with_finite_width", "one-sentence_summary": "We prove the uniqueness of local minima in a simple student-teacher framework, and conjecture the same conclusion for more general settings. ", "supplementary_material": "/attachment/bd5947b09954f58c89a69054ca6148bdf55f7c2d.zip", "pdf": "/pdf/8bf37a95497c98e0d8908a84113320c4d997d314.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JamhVIew04", "_bibtex": "@misc{\nzhu2021analysis,\ntitle={Analysis of Alignment Phenomenon in Simple Teacher-student Networks with Finite Width},\nauthor={Hanlin Zhu and Chengyang Ying and Song Zuo},\nyear={2021},\nurl={https://openreview.net/forum?id=e3bhF_p0T7c}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "e3bhF_p0T7c", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2581/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2581/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2581/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2581/Authors|ICLR.cc/2021/Conference/Paper2581/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2581/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846707, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2581/-/Official_Comment"}}}, {"id": "VfaW484GAuR", "original": null, "number": 2, "cdate": 1603865511800, "ddate": null, "tcdate": 1603865511800, "tmdate": 1605024177147, "tddate": null, "forum": "e3bhF_p0T7c", "replyto": "e3bhF_p0T7c", "invitation": "ICLR.cc/2021/Conference/Paper2581/-/Official_Review", "content": {"title": "Reviews", "review": "This paper considers the problem of learning neural networks with finite width. More specifically, the authors consider the simple teach-student setting and show that the input weights of student neurons will align with the teacher neuron. The problem considered in this paper is interesting. However, the results derived in this paper only consider some simple setting which may limit its impact. Here are some concerns I have for the current paper:\n1. The authors only consider the setting that the teach network only has one neuron. Thus it may not be reasonable to claim that the student neurons will align with one of the teacher\u2019s neurons. If the authors can provide results for the teacher network with multiple neurons, the contribution of the current paper would be stronger.\n2. Why the results only consider the case when d=2?\n3. The presentation of the current paper needs to be improved. For example, for the proof sketches of Theorems 4.1, 4.4, it is unclear the roles of Lemmas 4.3, 4.5.\n4. There is a recent work [1] also studying the learning of a single neuron model, the authors may want to compare with their results.\n5. The introduction and related work sections are clear and thorough. However, there are some relevant papers [2,3] also study the teacher network setting, the authors may want to discuss them in these sections.  \n\nReferences:\n[1] Frei, Spencer, Yuan Cao, and Quanquan Gu. \"Agnostic Learning of a Single Neuron with Gradient Descent.\" Advances in Neural Information Processing Systems (NeurIPS), 2020\n\n[2] Fu, Haoyu, Yuejie Chi, and Yingbin Liang. \"Guaranteed recovery of one-hidden-layer neural networks via cross entropy.\" IEEE Transactions on Signal Processing 68 (2020): 3225-3235.\n\n[3] Zhang, Xiao, et al. \"Learning one-hidden-layer relu networks via gradient descent.\" The 22nd International Conference on Artificial Intelligence and Statistics. PMLR, 2019.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2581/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2581/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analysis of Alignment Phenomenon in Simple Teacher-student Networks with Finite Width", "authorids": ["~Hanlin_Zhu2", "~Chengyang_Ying2", "~Song_Zuo1"], "authors": ["Hanlin Zhu", "Chengyang Ying", "Song Zuo"], "keywords": ["alignment", "finite width network", "teacher student model", "angular distance function"], "abstract": "Recent theoretical analysis suggests that ultra-wide neural networks always converge to global minima near the initialization under first order methods. However, the convergence property of neural networks with finite width could be very different. The simplest experiment with two-layer teacher-student networks shows that the input weights of student neurons eventually align with one of the teacher neurons. This suggests a distinct convergence nature for ``not-too-wide'' neural networks that there might not be any local minima near the initialization. As the theoretical justification, we prove that under the most basic settings, all student neurons must align with the teacher neuron at any local minima. The methodology is extendable to more general cases, where the proof can be reduced to analyzing the properties of a special class of functions that we call {\\em Angular Distance (AD) function}. Finally, we demonstrate that these properties can be easily verified numerically.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|analysis_of_alignment_phenomenon_in_simple_teacherstudent_networks_with_finite_width", "one-sentence_summary": "We prove the uniqueness of local minima in a simple student-teacher framework, and conjecture the same conclusion for more general settings. ", "supplementary_material": "/attachment/bd5947b09954f58c89a69054ca6148bdf55f7c2d.zip", "pdf": "/pdf/8bf37a95497c98e0d8908a84113320c4d997d314.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JamhVIew04", "_bibtex": "@misc{\nzhu2021analysis,\ntitle={Analysis of Alignment Phenomenon in Simple Teacher-student Networks with Finite Width},\nauthor={Hanlin Zhu and Chengyang Ying and Song Zuo},\nyear={2021},\nurl={https://openreview.net/forum?id=e3bhF_p0T7c}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "e3bhF_p0T7c", "replyto": "e3bhF_p0T7c", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2581/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538093127, "tmdate": 1606915805959, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2581/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2581/-/Official_Review"}}}, {"id": "WcNiNJjNCSd", "original": null, "number": 3, "cdate": 1603880024063, "ddate": null, "tcdate": 1603880024063, "tmdate": 1605024177074, "tddate": null, "forum": "e3bhF_p0T7c", "replyto": "e3bhF_p0T7c", "invitation": "ICLR.cc/2021/Conference/Paper2581/-/Official_Review", "content": {"title": "Novel results on alignment in neural networks, but the current scope of the work is too narrow.", "review": "### Summary\n\nThis work considers learning one-hidden layer convex neural networks where the teacher is a single ReLU and the data follows a two-dimensional Gaussian distribution. The authors study the loss landscape of the population loss and provide conditions on critical points which imply that the neurons are aligned with the teacher network (I will call this an aligned solution). They show that for m<=2, any critical point is necessarily an aligned solution. For m > 2, they show that a critical point is an aligned solution in certain special cases (e.g., when all polar angles are rational multiples of pi). They further show that in the general case, under certain conjectures, a critical point is an aligned solution. For the latter result they introduce a novel concept of Angular Distance (AD) functions.\n\n### Reason for score\n\nAnalyzing the non-convex loss landscape of neural networks is extremely challenging (even in seemingly \"simple\" cases) and I appreciate the effort of the authors to tackle it. While the theoretical results are novel and may be useful for analyzing more advanced settings, I think that the scope of the current manuscript is too narrow and lacks several important empirical results and discussions. Therefore, I don't think that the current version is ready for publication and I highly encourage the authors to improve the current paper (see my suggestions for improvement below).\n\n### Pros\n1.\tThe authors tackle a very challenging problem and provide novel theoretical results.\n2.\tThe concept of AD functions is interesting and might be useful in other analyses of neural networks.\n\n### Cons\n1.\tIn this paper the authors only show results for d=2. This is highly restrictive. I understand that for d>2 the analysis might be intractable. Therefore, I think that the authors should add extensive experiments which show that alignment occurs in higher dimensions. To further strengthen the results, it would be interesting to see if some of the theoretical insights of d=2 can be also applied to higher dimensions (e.g., the analysis of AD functions). The authors mention d=2 only once in the paper. This should be clearly stated in the Abstract, Introduction and Conclusion sections.\n2.\tThe authors cite the work of Tian (2020) but do not provide a clear comparison to this work. Specifically, it is not exactly clear what is the novelty of this work compared to Tian (2020) since the latter also shows that alignment occurs for any dimension.\n3.\tIn the experiments section the authors do not empirically verify Conjecture 5.5. Therefore, it is not clear whether this is a realistic conjecture.\n4.\tThe authors show two special cases where any critical point is an aligned solution. The key question is whether gradient descent converges to solutions which satisfy the conditions of these cases. This is not discussed and I encourage the authors to perform experiments to check if this holds.\n5.\tThe authors should cite more works on student specialization, e.g., \"Dynamics of stochastic gradient descent for two-layer neural networks in the teacher student setup\" of Goldt et al. (2019) and \"Dynamics of on-line gradient descent learning for multilayer neural networks\" of Saad et al. (1996).\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2581/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2581/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analysis of Alignment Phenomenon in Simple Teacher-student Networks with Finite Width", "authorids": ["~Hanlin_Zhu2", "~Chengyang_Ying2", "~Song_Zuo1"], "authors": ["Hanlin Zhu", "Chengyang Ying", "Song Zuo"], "keywords": ["alignment", "finite width network", "teacher student model", "angular distance function"], "abstract": "Recent theoretical analysis suggests that ultra-wide neural networks always converge to global minima near the initialization under first order methods. However, the convergence property of neural networks with finite width could be very different. The simplest experiment with two-layer teacher-student networks shows that the input weights of student neurons eventually align with one of the teacher neurons. This suggests a distinct convergence nature for ``not-too-wide'' neural networks that there might not be any local minima near the initialization. As the theoretical justification, we prove that under the most basic settings, all student neurons must align with the teacher neuron at any local minima. The methodology is extendable to more general cases, where the proof can be reduced to analyzing the properties of a special class of functions that we call {\\em Angular Distance (AD) function}. Finally, we demonstrate that these properties can be easily verified numerically.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|analysis_of_alignment_phenomenon_in_simple_teacherstudent_networks_with_finite_width", "one-sentence_summary": "We prove the uniqueness of local minima in a simple student-teacher framework, and conjecture the same conclusion for more general settings. ", "supplementary_material": "/attachment/bd5947b09954f58c89a69054ca6148bdf55f7c2d.zip", "pdf": "/pdf/8bf37a95497c98e0d8908a84113320c4d997d314.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JamhVIew04", "_bibtex": "@misc{\nzhu2021analysis,\ntitle={Analysis of Alignment Phenomenon in Simple Teacher-student Networks with Finite Width},\nauthor={Hanlin Zhu and Chengyang Ying and Song Zuo},\nyear={2021},\nurl={https://openreview.net/forum?id=e3bhF_p0T7c}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "e3bhF_p0T7c", "replyto": "e3bhF_p0T7c", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2581/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538093127, "tmdate": 1606915805959, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2581/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2581/-/Official_Review"}}}, {"id": "6-0mu0kqvvT", "original": null, "number": 4, "cdate": 1604276917380, "ddate": null, "tcdate": 1604276917380, "tmdate": 1605024177010, "tddate": null, "forum": "e3bhF_p0T7c", "replyto": "e3bhF_p0T7c", "invitation": "ICLR.cc/2021/Conference/Paper2581/-/Official_Review", "content": {"title": "Interesting ideas for characterizing the critical points of over-parametrized student network for a teacher network with one neuron", "review": "Summary of review:\n\nThis paper studies the optimization landscape of over-parametrized two-layer ReLU neural networks under an (isotropic) Gaussian input distribution. The main goal is to characterize the critical points of the over-parametrized student network, and show that these critical points \"align\" with the neurons of the teacher network. This is a well-motivated study since the alignment phenomenon is intriguing (Tian, 2020) but lacks a theoretical analysis.\n\nSetting:\n\nThis paper focuses on a teacher network with a single two-dimensional neuron followed by a ReLU activation function. The student network contains multiple (two-dimensional) neurons followed by a ReLU activation function.  \n\nResults:\n\nThis paper presents a suite of results for the above setting, including:\n\n(i) When the student network has two neurons, this paper shows that both neurons \"align\" perfectly with the neuron of the teacher network. Furthermore, the sum of the length of the two neurons is equal to the length of the neuron of the teacher network.\n\n(ii) When the student network has more than two neurons, this paper shows that the above \"alignment\" result holds for two special cases:\n\n-- (ii-a) when all the neurons of the student network are in the upper half-plane;\n\n-- (ii-b) when all the polar angles are rational multiples of $\\pi$. Furthermore, the polar angles satisfy a certain positive-definiteness condition in this special case.\n\n-- For the general case, this paper discovers an interesting determinant condition that guarantees the condition of (ii-b), hence the \"alignment\" result. In general, it is not clear whether the determinant condition always holds, and this paper provides several conjectures/ideas that provide some preliminary evidence for why this condition should hold.\n\nLimitation:\n\nWhile the questions are well-motivated and the settings are quite standard, I am concerned that the techniques of this paper are too restrictive to the assumptions. In particular, these include:\n\n(a) The dimension of the neurons being only two. This looks like a strong assumption, but is not mentioned or even discussed anywhere in the abstract and the introduction.\n\n(b) The results for the student network having more than two neurons are still rather preliminary. While this paper does provide several interesting ideas that might help in ultimately resolving the question, as the results stand currently, it is quite unclear to the reviewer whether the conjecture(s) will hold or not.\n\nQuestions:\n\n(a) The proof of Theorem 3.1 relies on that the critical point condition holds exactly. What if the critical point condition only holds approximately up to a small error of $\\epsilon$? Would your analysis still extend there? This setting is more interesting because in general, it is difficult for gradient descent to converge to an exact critical point. Furthermore, this setting might only extend to allow for sampling errors in an empirical loss.\n\n(b) What about the proof of Lemma 4.5 and Theorem 5.1? Would the analysis extend to allow for sampling errors or not?\n\nOther comments:\n\n- At the bottom of page 5, it will help to move the $\\rho_0$ and $\\rho_j$ outside of $\\cos$. Right now, this equation looks a bit confusing..", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2581/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2581/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analysis of Alignment Phenomenon in Simple Teacher-student Networks with Finite Width", "authorids": ["~Hanlin_Zhu2", "~Chengyang_Ying2", "~Song_Zuo1"], "authors": ["Hanlin Zhu", "Chengyang Ying", "Song Zuo"], "keywords": ["alignment", "finite width network", "teacher student model", "angular distance function"], "abstract": "Recent theoretical analysis suggests that ultra-wide neural networks always converge to global minima near the initialization under first order methods. However, the convergence property of neural networks with finite width could be very different. The simplest experiment with two-layer teacher-student networks shows that the input weights of student neurons eventually align with one of the teacher neurons. This suggests a distinct convergence nature for ``not-too-wide'' neural networks that there might not be any local minima near the initialization. As the theoretical justification, we prove that under the most basic settings, all student neurons must align with the teacher neuron at any local minima. The methodology is extendable to more general cases, where the proof can be reduced to analyzing the properties of a special class of functions that we call {\\em Angular Distance (AD) function}. Finally, we demonstrate that these properties can be easily verified numerically.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|analysis_of_alignment_phenomenon_in_simple_teacherstudent_networks_with_finite_width", "one-sentence_summary": "We prove the uniqueness of local minima in a simple student-teacher framework, and conjecture the same conclusion for more general settings. ", "supplementary_material": "/attachment/bd5947b09954f58c89a69054ca6148bdf55f7c2d.zip", "pdf": "/pdf/8bf37a95497c98e0d8908a84113320c4d997d314.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JamhVIew04", "_bibtex": "@misc{\nzhu2021analysis,\ntitle={Analysis of Alignment Phenomenon in Simple Teacher-student Networks with Finite Width},\nauthor={Hanlin Zhu and Chengyang Ying and Song Zuo},\nyear={2021},\nurl={https://openreview.net/forum?id=e3bhF_p0T7c}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "e3bhF_p0T7c", "replyto": "e3bhF_p0T7c", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2581/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538093127, "tmdate": 1606915805959, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2581/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2581/-/Official_Review"}}}], "count": 11}