{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124431081, "tcdate": 1518452796972, "number": 149, "cdate": 1518452796972, "id": "r1SBxSkvM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "r1SBxSkvM", "signatures": ["~Sujoy_Roy1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Replacing Loss Functions And Target Representations For Adversarial Defense", "abstract": "Recent works have shown that neural networks are susceptible to adversarial data, despite demonstrating high performance across various tasks. Hence, there is a growing need to develop techniques that make neural networks more robust against attacks given their increasingly frequent applications in real-life use cases. In this work, we propose simple techniques for adversarial defense, namely: (1) changing the loss function from cross entropy to mean-squared error, (2) representing targets as codewords generated from random codebooks, and (3) using an autoencoder to filter noisy logits before the final activation layer. Our experiments on CIFAR-10 using the DenseNet model have shown that these techniques can help prevent targeted attacks as well as improve classification accuracy on adversarial data generated in a white-box or black-box setting.", "paperhash": "saito|replacing_loss_functions_and_target_representations_for_adversarial_defense", "keywords": ["adversarial attacks", "target representation", "loss function"], "_bibtex": "@misc{\n  saito2018replacing,\n  title={Replacing Loss Functions And Target Representations For Adversarial Defense},\n  author={Sean Saito and Sujoy Roy},\n  year={2018},\n  url={https://openreview.net/forum?id=r1SBxSkvM}\n}", "authorids": ["sean.saito@sap.com", "sujoy.roy@sap.com"], "authors": ["Sean Saito", "Sujoy Roy"], "TL;DR": "Changing the loss function and target representation along with adding an autoencoder layer can significantly improve resistance to adversarial attacks", "pdf": "/pdf/e4430a087946ce51a1f20461a7ba1bc2d4b1b07d.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582995773, "tcdate": 1519344045964, "number": 1, "cdate": 1519344045964, "id": "BkU3tC2Pz", "invitation": "ICLR.cc/2018/Workshop/-/Paper149/Official_Review", "forum": "r1SBxSkvM", "replyto": "r1SBxSkvM", "signatures": ["ICLR.cc/2018/Workshop/Paper149/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper149/AnonReviewer3"], "content": {"title": "Official review of Adversarial Defense paper", "rating": "5: Marginally below acceptance threshold", "review": "The authors propose 3 new methods for making deep networks robust to adversarial attacks. In particular, the authors propose (1) training with MSE error, (2) employing a random codebook and (3) leveraging an autoencoder to regularize the logits. All 3 methods led to improved robustness in the classifier to adversarial perturbations.\n\nThe authors show gains in adversarial robustness by using the prescribed techniques.\n\nPros:\n- Each of the methods led to notable increased robustness to FGSM and BIM adversarial attack.\n\nCons:\n- The authors do not report the accuracy of the new models on a 'vanilla' test dataset, so we do not know if there altered training procedures lead to poorer cross-validated models.\n- Many details of the auto-encoder and codebook method not included in the paper. Additionally, the motivation for each approach is not entirely clear.\n- The authors do not compare the fidelity of this method to other baseline approaches for adversarial defense.\n- The paper lacks a clear motivation in terms of the suggested techniques. \n\n\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Replacing Loss Functions And Target Representations For Adversarial Defense", "abstract": "Recent works have shown that neural networks are susceptible to adversarial data, despite demonstrating high performance across various tasks. Hence, there is a growing need to develop techniques that make neural networks more robust against attacks given their increasingly frequent applications in real-life use cases. In this work, we propose simple techniques for adversarial defense, namely: (1) changing the loss function from cross entropy to mean-squared error, (2) representing targets as codewords generated from random codebooks, and (3) using an autoencoder to filter noisy logits before the final activation layer. Our experiments on CIFAR-10 using the DenseNet model have shown that these techniques can help prevent targeted attacks as well as improve classification accuracy on adversarial data generated in a white-box or black-box setting.", "paperhash": "saito|replacing_loss_functions_and_target_representations_for_adversarial_defense", "keywords": ["adversarial attacks", "target representation", "loss function"], "_bibtex": "@misc{\n  saito2018replacing,\n  title={Replacing Loss Functions And Target Representations For Adversarial Defense},\n  author={Sean Saito and Sujoy Roy},\n  year={2018},\n  url={https://openreview.net/forum?id=r1SBxSkvM}\n}", "authorids": ["sean.saito@sap.com", "sujoy.roy@sap.com"], "authors": ["Sean Saito", "Sujoy Roy"], "TL;DR": "Changing the loss function and target representation along with adding an autoencoder layer can significantly improve resistance to adversarial attacks", "pdf": "/pdf/e4430a087946ce51a1f20461a7ba1bc2d4b1b07d.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582995580, "id": "ICLR.cc/2018/Workshop/-/Paper149/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper149/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper149/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper149/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper149/AnonReviewer2"], "reply": {"forum": "r1SBxSkvM", "replyto": "r1SBxSkvM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper149/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper149/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582995580}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582933102, "tcdate": 1520314120436, "number": 2, "cdate": 1520314120436, "id": "rklMviodG", "invitation": "ICLR.cc/2018/Workshop/-/Paper149/Official_Review", "forum": "r1SBxSkvM", "replyto": "r1SBxSkvM", "signatures": ["ICLR.cc/2018/Workshop/Paper149/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper149/AnonReviewer1"], "content": {"title": "review", "rating": "4: Ok but not good enough - rejection", "review": "The paper presents three approaches to better defend an image model against adversarial attacks.\nThe first one is to replace the loss from cross entropy (CE) to mean squared error (MSE).\nResults on adversarial attacks show better defense with this loss. Sadly, no results are shown to see the potential weakening of the model on normal images. It is known that MSE is usually worse than CE in these cases.\nThe second approach consider using a different code than one-hot for classification, and supposes this remains a secret and makes it hard to attack. Using output codes is a well-known robustness technique, called ECOC, and should be referred to. Once again, no result are provided on clean images.\nThe third proposal has been suggested in many other papers (for instance MagNet).\n\nOverall, I found the paper not good enough for acceptance: it proposes 3 ideas, none of which are properly compared nor justified, and there's a lack of context from existing literature.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Replacing Loss Functions And Target Representations For Adversarial Defense", "abstract": "Recent works have shown that neural networks are susceptible to adversarial data, despite demonstrating high performance across various tasks. Hence, there is a growing need to develop techniques that make neural networks more robust against attacks given their increasingly frequent applications in real-life use cases. In this work, we propose simple techniques for adversarial defense, namely: (1) changing the loss function from cross entropy to mean-squared error, (2) representing targets as codewords generated from random codebooks, and (3) using an autoencoder to filter noisy logits before the final activation layer. Our experiments on CIFAR-10 using the DenseNet model have shown that these techniques can help prevent targeted attacks as well as improve classification accuracy on adversarial data generated in a white-box or black-box setting.", "paperhash": "saito|replacing_loss_functions_and_target_representations_for_adversarial_defense", "keywords": ["adversarial attacks", "target representation", "loss function"], "_bibtex": "@misc{\n  saito2018replacing,\n  title={Replacing Loss Functions And Target Representations For Adversarial Defense},\n  author={Sean Saito and Sujoy Roy},\n  year={2018},\n  url={https://openreview.net/forum?id=r1SBxSkvM}\n}", "authorids": ["sean.saito@sap.com", "sujoy.roy@sap.com"], "authors": ["Sean Saito", "Sujoy Roy"], "TL;DR": "Changing the loss function and target representation along with adding an autoencoder layer can significantly improve resistance to adversarial attacks", "pdf": "/pdf/e4430a087946ce51a1f20461a7ba1bc2d4b1b07d.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582995580, "id": "ICLR.cc/2018/Workshop/-/Paper149/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper149/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper149/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper149/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper149/AnonReviewer2"], "reply": {"forum": "r1SBxSkvM", "replyto": "r1SBxSkvM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper149/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper149/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582995580}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582724137, "tcdate": 1520678220030, "number": 3, "cdate": 1520678220030, "id": "SkVUBNWtz", "invitation": "ICLR.cc/2018/Workshop/-/Paper149/Official_Review", "forum": "r1SBxSkvM", "replyto": "r1SBxSkvM", "signatures": ["ICLR.cc/2018/Workshop/Paper149/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper149/AnonReviewer2"], "content": {"title": "Some new ideas", "rating": "6: Marginally above acceptance threshold", "review": "This paper presents three new techniques for defending neural network based image classifiers against adversarial images:\n\t- training with MSE loss rather than cross entropy loss\n\t- replacing one-hot label representation with randomly chosen binary codes for each label\n\t- add an auto-encoder layer before the final softmax to filter noise at this layer that the adversarial images may generate\n\t\nPros:\n\t- The defense techniques are novel as far as I know\n\t- All three defenses improve performance against the basic FGSM attack\n\nCons:\n\t- The techniques do improve the performance, but the gains are not as significant as existing defenses\n\t- They didn't compare to any other baseline defenses making it difficult to see how their model compares to existing work\n\t- They don't compare to adversarial samples generated from a model using their defense, except in one case.  Notably they don't report results on BIM for adversarial samples generated from any of their defenses\n\t- Surprisingly, when the adversarial images are generated using the MSE model, the MSE defense performs even better than when the adversarial images are generated using the CE model.  This result is surprising, and a bit suspect, so I would have liked to see an explanation of why this is happening (or at least an admission that they don't understand what's happening here).  This is particularly important given that this is the most important result in the paper\n\nSo overall, I think the paper has some novel ideas which have not yet been properly validated.  Given the goals for the workshop, to present new but not fully polished work, I think this puts it above the bar for acceptance.\n", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Replacing Loss Functions And Target Representations For Adversarial Defense", "abstract": "Recent works have shown that neural networks are susceptible to adversarial data, despite demonstrating high performance across various tasks. Hence, there is a growing need to develop techniques that make neural networks more robust against attacks given their increasingly frequent applications in real-life use cases. In this work, we propose simple techniques for adversarial defense, namely: (1) changing the loss function from cross entropy to mean-squared error, (2) representing targets as codewords generated from random codebooks, and (3) using an autoencoder to filter noisy logits before the final activation layer. Our experiments on CIFAR-10 using the DenseNet model have shown that these techniques can help prevent targeted attacks as well as improve classification accuracy on adversarial data generated in a white-box or black-box setting.", "paperhash": "saito|replacing_loss_functions_and_target_representations_for_adversarial_defense", "keywords": ["adversarial attacks", "target representation", "loss function"], "_bibtex": "@misc{\n  saito2018replacing,\n  title={Replacing Loss Functions And Target Representations For Adversarial Defense},\n  author={Sean Saito and Sujoy Roy},\n  year={2018},\n  url={https://openreview.net/forum?id=r1SBxSkvM}\n}", "authorids": ["sean.saito@sap.com", "sujoy.roy@sap.com"], "authors": ["Sean Saito", "Sujoy Roy"], "TL;DR": "Changing the loss function and target representation along with adding an autoencoder layer can significantly improve resistance to adversarial attacks", "pdf": "/pdf/e4430a087946ce51a1f20461a7ba1bc2d4b1b07d.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582995580, "id": "ICLR.cc/2018/Workshop/-/Paper149/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper149/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper149/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper149/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper149/AnonReviewer2"], "reply": {"forum": "r1SBxSkvM", "replyto": "r1SBxSkvM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper149/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper149/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582995580}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573585387, "tcdate": 1521573585387, "number": 181, "cdate": 1521573585052, "id": "S1t0RRAKz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "r1SBxSkvM", "replyto": "r1SBxSkvM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Replacing Loss Functions And Target Representations For Adversarial Defense", "abstract": "Recent works have shown that neural networks are susceptible to adversarial data, despite demonstrating high performance across various tasks. Hence, there is a growing need to develop techniques that make neural networks more robust against attacks given their increasingly frequent applications in real-life use cases. In this work, we propose simple techniques for adversarial defense, namely: (1) changing the loss function from cross entropy to mean-squared error, (2) representing targets as codewords generated from random codebooks, and (3) using an autoencoder to filter noisy logits before the final activation layer. Our experiments on CIFAR-10 using the DenseNet model have shown that these techniques can help prevent targeted attacks as well as improve classification accuracy on adversarial data generated in a white-box or black-box setting.", "paperhash": "saito|replacing_loss_functions_and_target_representations_for_adversarial_defense", "keywords": ["adversarial attacks", "target representation", "loss function"], "_bibtex": "@misc{\n  saito2018replacing,\n  title={Replacing Loss Functions And Target Representations For Adversarial Defense},\n  author={Sean Saito and Sujoy Roy},\n  year={2018},\n  url={https://openreview.net/forum?id=r1SBxSkvM}\n}", "authorids": ["sean.saito@sap.com", "sujoy.roy@sap.com"], "authors": ["Sean Saito", "Sujoy Roy"], "TL;DR": "Changing the loss function and target representation along with adding an autoencoder layer can significantly improve resistance to adversarial attacks", "pdf": "/pdf/e4430a087946ce51a1f20461a7ba1bc2d4b1b07d.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}