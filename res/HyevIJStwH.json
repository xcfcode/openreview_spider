{"notes": [{"id": "HyevIJStwH", "original": "HklbWvpdwB", "number": 1732, "cdate": 1569439566913, "ddate": null, "tcdate": 1569439566913, "tmdate": 1583912022137, "tddate": null, "forum": "HyevIJStwH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["ljlwykqh@126.com", "yunzhi.bai@outlook.fr", "jianggq@pku.edu.cn", "roushi0322@sina.cn", "wanghuayan@kuaishou.com"], "title": "Understanding Why Neural Networks Generalize Well Through GSNR of Parameters", "authors": ["Jinlong Liu", "Yunzhi Bai", "Guoqing Jiang", "Ting Chen", "Huayan Wang"], "pdf": "/pdf/d80fd2ee1d630210ee8cf81fc5c106d3015aae05.pdf", "abstract": "As deep neural networks (DNNs) achieve tremendous success across many application domains, researchers tried to explore in many aspects on why they generalize well. In this paper, we provide a novel perspective on these issues using the gradient signal to noise ratio (GSNR) of parameters during training process of DNNs. The GSNR of a parameter is simply defined as the ratio between its gradient's squared mean and variance, over the data distribution. Based on several approximations, we establish a quantitative relationship between model parameters' GSNR and the generalization gap. This relationship indicates that larger GSNR during training process leads to better generalization performance. Futher, we show that, different from that of shallow models (e.g. logistic regression, support vector machines), the gradient descent optimization dynamics of DNNs naturally produces large GSNR during training, which is probably the key to DNNs\u2019 remarkable generalization ability.", "keywords": ["DNN", "generalization", "GSNR", "gradient descent"], "paperhash": "liu|understanding_why_neural_networks_generalize_well_through_gsnr_of_parameters", "_bibtex": "@inproceedings{\nLiu2020Understanding,\ntitle={Understanding Why Neural Networks Generalize Well Through GSNR of Parameters},\nauthor={Jinlong Liu and Yunzhi Bai and Guoqing Jiang and Ting Chen and Huayan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyevIJStwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/92f284434ac3b907829affb4a3bfc33ce8908ec7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "DfU1kBE-ej", "original": null, "number": 1, "cdate": 1576798731052, "ddate": null, "tcdate": 1576798731052, "tmdate": 1576800905425, "tddate": null, "forum": "HyevIJStwH", "replyto": "HyevIJStwH", "invitation": "ICLR.cc/2020/Conference/Paper1732/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "Quoting a reviewer for a very nice summary:\n\n\"In this work, the authors suggest a new point of view on generalization through the lens of the distribution of the per-sample gradients. The authors consider the variance and mean of the per-sample gradients for each parameter of the model and define for each parameter the Gradient Signal to Noise ratio (GSNR). The GSNR of a parameter is the ratio between the mean squared of the gradient per parameter per sample (computed over the samples) and the variance of the gradient per parameter per sample (also computed over the samples). The GSNR is promising as a measure of generalization and the authors provide a nice leading order derivation of the GSNR as a proxy for the measure of the generalization gap in the model.\"\n\nThe majority of the reviewers vote to accept this paper. We can view the 3 as a weak signal as that reviewer stated in his review that he struggled to rate the paper because it contained a lot of math.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ljlwykqh@126.com", "yunzhi.bai@outlook.fr", "jianggq@pku.edu.cn", "roushi0322@sina.cn", "wanghuayan@kuaishou.com"], "title": "Understanding Why Neural Networks Generalize Well Through GSNR of Parameters", "authors": ["Jinlong Liu", "Yunzhi Bai", "Guoqing Jiang", "Ting Chen", "Huayan Wang"], "pdf": "/pdf/d80fd2ee1d630210ee8cf81fc5c106d3015aae05.pdf", "abstract": "As deep neural networks (DNNs) achieve tremendous success across many application domains, researchers tried to explore in many aspects on why they generalize well. In this paper, we provide a novel perspective on these issues using the gradient signal to noise ratio (GSNR) of parameters during training process of DNNs. The GSNR of a parameter is simply defined as the ratio between its gradient's squared mean and variance, over the data distribution. Based on several approximations, we establish a quantitative relationship between model parameters' GSNR and the generalization gap. This relationship indicates that larger GSNR during training process leads to better generalization performance. Futher, we show that, different from that of shallow models (e.g. logistic regression, support vector machines), the gradient descent optimization dynamics of DNNs naturally produces large GSNR during training, which is probably the key to DNNs\u2019 remarkable generalization ability.", "keywords": ["DNN", "generalization", "GSNR", "gradient descent"], "paperhash": "liu|understanding_why_neural_networks_generalize_well_through_gsnr_of_parameters", "_bibtex": "@inproceedings{\nLiu2020Understanding,\ntitle={Understanding Why Neural Networks Generalize Well Through GSNR of Parameters},\nauthor={Jinlong Liu and Yunzhi Bai and Guoqing Jiang and Ting Chen and Huayan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyevIJStwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/92f284434ac3b907829affb4a3bfc33ce8908ec7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HyevIJStwH", "replyto": "HyevIJStwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795708877, "tmdate": 1576800257422, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1732/-/Decision"}}}, {"id": "HkxvaNRhFB", "original": null, "number": 1, "cdate": 1571771582980, "ddate": null, "tcdate": 1571771582980, "tmdate": 1574790517376, "tddate": null, "forum": "HyevIJStwH", "replyto": "HyevIJStwH", "invitation": "ICLR.cc/2020/Conference/Paper1732/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "In this work, the authors suggest a new point of view on generalization through the lens of the distribution of the per-sample gradients. The authors consider the variance and mean of the per-sample gradients for each parameter of the model and define for each parameter the Gradient Signal to Noise ratio (GSNR). The GSNR of a parameter is the ratio between the mean squared of the gradient per parameter per sample (computed over the samples) and the variance of the gradient per parameter per sample (also computed over the samples). The GSNR is promising as a measure of generalization and the authors provide a nice leading order derivation of the GSNR as a proxy for the measure of the generalization gap in the model. After the derivation, experimental results on MNIST are presented and suggest that empirically there is a relation between the generalization gap of neural network trained by gradient descent and the GSNR quantity given in the paper. Next the author analyze the GNSR of DNNs as opposed with shallow models or other learning techniques and observe that the GSNR differs when using random labels (lower GSNR) as compared with true labels and exhibits different behavior along training for DNNs and gradient descent.\n\n Assumption 2.3.1 is not necessarily the most realistic in the current training paradigm since multi-epoch training indeed separates the training and testing per-sample gradient distributions significantly. \n\nOverall the paper gives a fresh (as far as I know) and nice idea on generalization of neural networks. The derivation requires quite a few stringent assumption (the leading order analysis, on the step size, assumption 2.3.1 on the test and train gradient distributions) the experiments do suggest that the theory is valid to an extent, especially during the early parts of training. In contrast, the presentation of the paper distracts from the work and needs additional cleaning up. Also, the experimental section 2.4 would benefit additional empirical analysis on other datasets  and additional experiments, as well as more thorough explanation of the experiments. The writing of the paper needs additional proofreading as currently it is easy to spot typos and grammar errors along the paper. I currently vote weak reject, for solid content and not-quite-ready presentation.\n\nAdditional experiments and careful proofreading should definitely enhance the paper and get it to the level of publications, so I am willing to change my decision if the authors improve the writing and the overall presentation. I like the idea presented in the paper and encourage the authors to resubmit a more tidy draft.\n\nsmaller presentation issues and typos/grammar issues:\n\nFigure 1: name X and Y labels with more meaningful names instead of RHS,LHS\nFigure 3(c): add legend\nline 64: consists >>> which consists\nline 73: we refer R(z) >>> we refer to R(z)\nline 79 futher >>> further overall sentence needs more work\nline 92 distribution becomes >>> distributions become \nline 100: using common notation summing over n samples, the sum should possibly start from i=1 to n instead of i = 0 to n\nline 132 include >>> includes (also possibly rephrase sentence)\nline 136 vefity >>> verify \nline 137: M number >>> M  \nline 157 the data points closely distributed >>> the data points are closely distributed\nline 162 thorough analytically >>> thorough analytical \n\n\n\n\n-------- Update After Rebuttal --------\n\nI have read the comments the authors made and reviewed the paper's revision. \nThe presentation has improved a lot (Even though there is still room to go). \nNonetheless at this time I choose to update my score to a weak accept, as I think the authors bring a fresh (as far as I know) and interesting idea with regards to empirical generalization.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1732/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1732/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ljlwykqh@126.com", "yunzhi.bai@outlook.fr", "jianggq@pku.edu.cn", "roushi0322@sina.cn", "wanghuayan@kuaishou.com"], "title": "Understanding Why Neural Networks Generalize Well Through GSNR of Parameters", "authors": ["Jinlong Liu", "Yunzhi Bai", "Guoqing Jiang", "Ting Chen", "Huayan Wang"], "pdf": "/pdf/d80fd2ee1d630210ee8cf81fc5c106d3015aae05.pdf", "abstract": "As deep neural networks (DNNs) achieve tremendous success across many application domains, researchers tried to explore in many aspects on why they generalize well. In this paper, we provide a novel perspective on these issues using the gradient signal to noise ratio (GSNR) of parameters during training process of DNNs. The GSNR of a parameter is simply defined as the ratio between its gradient's squared mean and variance, over the data distribution. Based on several approximations, we establish a quantitative relationship between model parameters' GSNR and the generalization gap. This relationship indicates that larger GSNR during training process leads to better generalization performance. Futher, we show that, different from that of shallow models (e.g. logistic regression, support vector machines), the gradient descent optimization dynamics of DNNs naturally produces large GSNR during training, which is probably the key to DNNs\u2019 remarkable generalization ability.", "keywords": ["DNN", "generalization", "GSNR", "gradient descent"], "paperhash": "liu|understanding_why_neural_networks_generalize_well_through_gsnr_of_parameters", "_bibtex": "@inproceedings{\nLiu2020Understanding,\ntitle={Understanding Why Neural Networks Generalize Well Through GSNR of Parameters},\nauthor={Jinlong Liu and Yunzhi Bai and Guoqing Jiang and Ting Chen and Huayan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyevIJStwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/92f284434ac3b907829affb4a3bfc33ce8908ec7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyevIJStwH", "replyto": "HyevIJStwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1732/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1732/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575902452018, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1732/Reviewers"], "noninvitees": [], "tcdate": 1570237733106, "tmdate": 1575902452037, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1732/-/Official_Review"}}}, {"id": "rye6IOi42B", "original": null, "number": 3, "cdate": 1574381653041, "ddate": null, "tcdate": 1574381653041, "tmdate": 1574381653041, "tddate": null, "forum": "HyevIJStwH", "replyto": "HyevIJStwH", "invitation": "ICLR.cc/2020/Conference/Paper1732/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "Paper Summary\n\nThis paper introduces a quantity termed the \"one-step generalization ratio\" . They derive approximate relations between OSGR and GSNR then show experimental results demonstrating the validity of these approximations, thus linking GSNR and a quantity related to generalization. They investigate the empirical value of GSNR during training of a neural network on Cifar10 with real labels vs. random labels. The final section derives a relation which attempts to explain the correlation between the size of the expected gradient and the learning of features.\n\nReview\n\nFirst, I should note that I am not well-versed in this specific line of work (GSNR ,as other papers were cited with this concept), and thus I cannot readily assess whether the contributions are \"well-placed in the literature\".\n\nFor the derivations and theory, I checked as much of the derivations as possible, although I'm not familiar with all the assumptions used throughout, but they seem relatively reasonable and supported by the experiments. I.e. 2.3.1 seems reasonable at the start of training. But I'm not sure how to evaluate some of the limiting arguments, like taking the step size to zero. \n\nThe paper addresses an important topic and is interesting. The arguments seem well reasoned and logical. The experiments addressed the veracity of of the approximate relations derived in the first section as well as some interesting trends related to the GSNR quantity during training of some simple models. \n\nSmall Concerns \n\nOn line 188 it is mentioned that GSNR is the \"reason\" for a particular phenomenon or explains a phenomenon. It certainly seems reasonable that it is correlated or an indicator of the underlying phenomenon, but it doesn't seem (at least intuitively) that GSNR is the cause or reason for the generalization concepts being examined.  \n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1732/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1732/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ljlwykqh@126.com", "yunzhi.bai@outlook.fr", "jianggq@pku.edu.cn", "roushi0322@sina.cn", "wanghuayan@kuaishou.com"], "title": "Understanding Why Neural Networks Generalize Well Through GSNR of Parameters", "authors": ["Jinlong Liu", "Yunzhi Bai", "Guoqing Jiang", "Ting Chen", "Huayan Wang"], "pdf": "/pdf/d80fd2ee1d630210ee8cf81fc5c106d3015aae05.pdf", "abstract": "As deep neural networks (DNNs) achieve tremendous success across many application domains, researchers tried to explore in many aspects on why they generalize well. In this paper, we provide a novel perspective on these issues using the gradient signal to noise ratio (GSNR) of parameters during training process of DNNs. The GSNR of a parameter is simply defined as the ratio between its gradient's squared mean and variance, over the data distribution. Based on several approximations, we establish a quantitative relationship between model parameters' GSNR and the generalization gap. This relationship indicates that larger GSNR during training process leads to better generalization performance. Futher, we show that, different from that of shallow models (e.g. logistic regression, support vector machines), the gradient descent optimization dynamics of DNNs naturally produces large GSNR during training, which is probably the key to DNNs\u2019 remarkable generalization ability.", "keywords": ["DNN", "generalization", "GSNR", "gradient descent"], "paperhash": "liu|understanding_why_neural_networks_generalize_well_through_gsnr_of_parameters", "_bibtex": "@inproceedings{\nLiu2020Understanding,\ntitle={Understanding Why Neural Networks Generalize Well Through GSNR of Parameters},\nauthor={Jinlong Liu and Yunzhi Bai and Guoqing Jiang and Ting Chen and Huayan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyevIJStwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/92f284434ac3b907829affb4a3bfc33ce8908ec7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyevIJStwH", "replyto": "HyevIJStwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1732/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1732/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575902452018, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1732/Reviewers"], "noninvitees": [], "tcdate": 1570237733106, "tmdate": 1575902452037, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1732/-/Official_Review"}}}, {"id": "HJgI26Q3iH", "original": null, "number": 4, "cdate": 1573825966156, "ddate": null, "tcdate": 1573825966156, "tmdate": 1573825966156, "tddate": null, "forum": "HyevIJStwH", "replyto": "HyevIJStwH", "invitation": "ICLR.cc/2020/Conference/Paper1732/-/Official_Comment", "content": {"title": "We have submitted a final revision of the Discussion Stage 2.", "comment": "Hi, we have submitted the final revision. Changes in this revision are mainly about spelling errors.\nThanks."}, "signatures": ["ICLR.cc/2020/Conference/Paper1732/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1732/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ljlwykqh@126.com", "yunzhi.bai@outlook.fr", "jianggq@pku.edu.cn", "roushi0322@sina.cn", "wanghuayan@kuaishou.com"], "title": "Understanding Why Neural Networks Generalize Well Through GSNR of Parameters", "authors": ["Jinlong Liu", "Yunzhi Bai", "Guoqing Jiang", "Ting Chen", "Huayan Wang"], "pdf": "/pdf/d80fd2ee1d630210ee8cf81fc5c106d3015aae05.pdf", "abstract": "As deep neural networks (DNNs) achieve tremendous success across many application domains, researchers tried to explore in many aspects on why they generalize well. In this paper, we provide a novel perspective on these issues using the gradient signal to noise ratio (GSNR) of parameters during training process of DNNs. The GSNR of a parameter is simply defined as the ratio between its gradient's squared mean and variance, over the data distribution. Based on several approximations, we establish a quantitative relationship between model parameters' GSNR and the generalization gap. This relationship indicates that larger GSNR during training process leads to better generalization performance. Futher, we show that, different from that of shallow models (e.g. logistic regression, support vector machines), the gradient descent optimization dynamics of DNNs naturally produces large GSNR during training, which is probably the key to DNNs\u2019 remarkable generalization ability.", "keywords": ["DNN", "generalization", "GSNR", "gradient descent"], "paperhash": "liu|understanding_why_neural_networks_generalize_well_through_gsnr_of_parameters", "_bibtex": "@inproceedings{\nLiu2020Understanding,\ntitle={Understanding Why Neural Networks Generalize Well Through GSNR of Parameters},\nauthor={Jinlong Liu and Yunzhi Bai and Guoqing Jiang and Ting Chen and Huayan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyevIJStwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/92f284434ac3b907829affb4a3bfc33ce8908ec7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyevIJStwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1732/Authors", "ICLR.cc/2020/Conference/Paper1732/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1732/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1732/Reviewers", "ICLR.cc/2020/Conference/Paper1732/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1732/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1732/Authors|ICLR.cc/2020/Conference/Paper1732/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151693, "tmdate": 1576860558130, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1732/Authors", "ICLR.cc/2020/Conference/Paper1732/Reviewers", "ICLR.cc/2020/Conference/Paper1732/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1732/-/Official_Comment"}}}, {"id": "S1xhXUI9jB", "original": null, "number": 3, "cdate": 1573705252200, "ddate": null, "tcdate": 1573705252200, "tmdate": 1573705252200, "tddate": null, "forum": "HyevIJStwH", "replyto": "HyevIJStwH", "invitation": "ICLR.cc/2020/Conference/Paper1732/-/Official_Comment", "content": {"title": "We have submitted a second revision.", "comment": "Hi, we have submitted a second revision where \n(1) we changed \"eq. 17\" to \"eq. 19\" in the plot labels of  Figure 1, Figure 5 and Figure 6;\n(2) we removed the definitions of expectation of g_D and g_D' from  the beginning of section 2.3, to improve the readability."}, "signatures": ["ICLR.cc/2020/Conference/Paper1732/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1732/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ljlwykqh@126.com", "yunzhi.bai@outlook.fr", "jianggq@pku.edu.cn", "roushi0322@sina.cn", "wanghuayan@kuaishou.com"], "title": "Understanding Why Neural Networks Generalize Well Through GSNR of Parameters", "authors": ["Jinlong Liu", "Yunzhi Bai", "Guoqing Jiang", "Ting Chen", "Huayan Wang"], "pdf": "/pdf/d80fd2ee1d630210ee8cf81fc5c106d3015aae05.pdf", "abstract": "As deep neural networks (DNNs) achieve tremendous success across many application domains, researchers tried to explore in many aspects on why they generalize well. In this paper, we provide a novel perspective on these issues using the gradient signal to noise ratio (GSNR) of parameters during training process of DNNs. The GSNR of a parameter is simply defined as the ratio between its gradient's squared mean and variance, over the data distribution. Based on several approximations, we establish a quantitative relationship between model parameters' GSNR and the generalization gap. This relationship indicates that larger GSNR during training process leads to better generalization performance. Futher, we show that, different from that of shallow models (e.g. logistic regression, support vector machines), the gradient descent optimization dynamics of DNNs naturally produces large GSNR during training, which is probably the key to DNNs\u2019 remarkable generalization ability.", "keywords": ["DNN", "generalization", "GSNR", "gradient descent"], "paperhash": "liu|understanding_why_neural_networks_generalize_well_through_gsnr_of_parameters", "_bibtex": "@inproceedings{\nLiu2020Understanding,\ntitle={Understanding Why Neural Networks Generalize Well Through GSNR of Parameters},\nauthor={Jinlong Liu and Yunzhi Bai and Guoqing Jiang and Ting Chen and Huayan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyevIJStwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/92f284434ac3b907829affb4a3bfc33ce8908ec7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyevIJStwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1732/Authors", "ICLR.cc/2020/Conference/Paper1732/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1732/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1732/Reviewers", "ICLR.cc/2020/Conference/Paper1732/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1732/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1732/Authors|ICLR.cc/2020/Conference/Paper1732/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151693, "tmdate": 1576860558130, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1732/Authors", "ICLR.cc/2020/Conference/Paper1732/Reviewers", "ICLR.cc/2020/Conference/Paper1732/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1732/-/Official_Comment"}}}, {"id": "SJxVhP5FoS", "original": null, "number": 2, "cdate": 1573656491634, "ddate": null, "tcdate": 1573656491634, "tmdate": 1573699584915, "tddate": null, "forum": "HyevIJStwH", "replyto": "HkxvaNRhFB", "invitation": "ICLR.cc/2020/Conference/Paper1732/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "Thank you for your responsible review and comments.\n\nWe have resubmitted a revision and keep improving the overall presentation, where\n(1) the paper has been largely revamped to improve its quality of presentation; \n(2) notations are changed to improve readability and consistency, and a notation table is added in Appendix C; \n(3) all figures are re-generated to make them more readable. \n(4) additional experiments on CIFAR10 and toy dataset are included.\n\nWe conducted the experiments on CIFAR10 and toy dataset, in Appendix A.2 and A.3 respectively. Briefly, for CIFAR10 we used a deeper CNN with batch normalization layer, and for the toy dataset which is constructed by a very simple way, we used a 2-layers MLP. The experiment settings on both dataset are similar to that on MNIST, and we got similar observations.\n\nAssumption 2.3.1 usually holds true in the early training stage. Multi-epoch training undoubtedly separates the training and testing per-sample gradient distributions significantly, which will break the stringent Assumption 2.3.1 in the late training stage. However, the experiments on Toy-DNN, MNIST and CIFAR10 together indicate that, for common network structures (MLP or CNN), with the commonly used step size of 0.001, the strongly positive correlation of LHS and RHS of equation (19)(in the revision) remains until training converges even when the Assumption 2.3.1 no longer holds. Therefore, the relation that \u201cthe larger GSNR during training leads to better generalization ability\u201d is valid during the whole training process with or without Assumption 2.3.1. We have also improved presentation of Assumption 2.3.1 to make it more intuitive.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1732/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper1732/Reviewers", "ICLR.cc/2020/Conference/Paper1732/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1732/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ljlwykqh@126.com", "yunzhi.bai@outlook.fr", "jianggq@pku.edu.cn", "roushi0322@sina.cn", "wanghuayan@kuaishou.com"], "title": "Understanding Why Neural Networks Generalize Well Through GSNR of Parameters", "authors": ["Jinlong Liu", "Yunzhi Bai", "Guoqing Jiang", "Ting Chen", "Huayan Wang"], "pdf": "/pdf/d80fd2ee1d630210ee8cf81fc5c106d3015aae05.pdf", "abstract": "As deep neural networks (DNNs) achieve tremendous success across many application domains, researchers tried to explore in many aspects on why they generalize well. In this paper, we provide a novel perspective on these issues using the gradient signal to noise ratio (GSNR) of parameters during training process of DNNs. The GSNR of a parameter is simply defined as the ratio between its gradient's squared mean and variance, over the data distribution. Based on several approximations, we establish a quantitative relationship between model parameters' GSNR and the generalization gap. This relationship indicates that larger GSNR during training process leads to better generalization performance. Futher, we show that, different from that of shallow models (e.g. logistic regression, support vector machines), the gradient descent optimization dynamics of DNNs naturally produces large GSNR during training, which is probably the key to DNNs\u2019 remarkable generalization ability.", "keywords": ["DNN", "generalization", "GSNR", "gradient descent"], "paperhash": "liu|understanding_why_neural_networks_generalize_well_through_gsnr_of_parameters", "_bibtex": "@inproceedings{\nLiu2020Understanding,\ntitle={Understanding Why Neural Networks Generalize Well Through GSNR of Parameters},\nauthor={Jinlong Liu and Yunzhi Bai and Guoqing Jiang and Ting Chen and Huayan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyevIJStwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/92f284434ac3b907829affb4a3bfc33ce8908ec7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyevIJStwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1732/Authors", "ICLR.cc/2020/Conference/Paper1732/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1732/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1732/Reviewers", "ICLR.cc/2020/Conference/Paper1732/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1732/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1732/Authors|ICLR.cc/2020/Conference/Paper1732/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151693, "tmdate": 1576860558130, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1732/Authors", "ICLR.cc/2020/Conference/Paper1732/Reviewers", "ICLR.cc/2020/Conference/Paper1732/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1732/-/Official_Comment"}}}, {"id": "B1ehzS5For", "original": null, "number": 1, "cdate": 1573655827967, "ddate": null, "tcdate": 1573655827967, "tmdate": 1573656305661, "tddate": null, "forum": "HyevIJStwH", "replyto": "B1lyj_jm5B", "invitation": "ICLR.cc/2020/Conference/Paper1732/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "Thank you for your responsible review and comments.\n\nWe have resubmitted a revision and keep improving the overall presentation, where\n(1) the paper has been largely revamped to improve its quality of presentation; \n(2) notations are changed to improve readability and consistency and a notation table is added in Appendix C;\n(3) all figures are re-generated to make them more readable.\n(4) additional experiments on CIFAR10 and toy dataset are included.\n\nYour main concerns are addressed below.\n\nThe definition of SNR on Wikipedia is the ratio between squared mean and variance, which ensures it is positive.\n\nAssumption 2.3.1 is needed for the derivation. It usually holds true in the early training stage. Multi-epoch training undoubtedly separates the training and testing per-sample gradient distributions significantly, which will break the stringent Assumption 2.3.1 in the late training stage. However, the experiments on Toy-DNN, Mnist and CIFAR10 together indicate that with the commonly used step size of 0.001, the strongly positive correlation of LHS and RHS of equation(19) (in the revision) remains until training converges even when the Assumption 2.3.1 no longer holds. Therefore, the relation that \u201cthe larger GSNR during training leads to better generalization ability\u201d is valid during the whole training process with or without Assumption 2.3.1. We have also improved presentation of Assumption 2.3.1 to make it more intuitive.\n\nWe have \\sigma represents \u201cthe standard deviation of gradient mean over a training dataset of size n\u201d whereas \\rho represents \u201cthe standard deviation of parameters\u2019 gradient of a single sample\u201d. Under Assumption 2.3.1, it can be proved that they differ by a factor of n, which is the dataset size. \\mu is the same, please refer to equation 7 in the first version or equation 6 in the revision. We give a proof here.\n$$E_{D\\sim \\mathcal Z^n}[\\mathbf g_D(\\mathbf\\theta)]=E_{(x_0,y_0)\\sim \\mathcal Z,(x_1,y_1)\\sim \\mathcal Z,...,(x_n,y_n)\\sim \\mathcal Z}(\\frac1n\\sum_{i=1}^n\\mathbf g_i(\\mathbf \\theta))=\\frac1n\\sum_{i=1}^nE_{(x_i,y_i)\\sim \\mathcal Z}(\\mathbf g_i(\\mathbf \\theta))=E_{(x,y)\\sim \\mathcal Z}(\\mathbf g(x,y,\\mathbf \\theta))\\equiv\\tilde{\\mathbf g}(\\mathbf\\theta)$$\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1732/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1732/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ljlwykqh@126.com", "yunzhi.bai@outlook.fr", "jianggq@pku.edu.cn", "roushi0322@sina.cn", "wanghuayan@kuaishou.com"], "title": "Understanding Why Neural Networks Generalize Well Through GSNR of Parameters", "authors": ["Jinlong Liu", "Yunzhi Bai", "Guoqing Jiang", "Ting Chen", "Huayan Wang"], "pdf": "/pdf/d80fd2ee1d630210ee8cf81fc5c106d3015aae05.pdf", "abstract": "As deep neural networks (DNNs) achieve tremendous success across many application domains, researchers tried to explore in many aspects on why they generalize well. In this paper, we provide a novel perspective on these issues using the gradient signal to noise ratio (GSNR) of parameters during training process of DNNs. The GSNR of a parameter is simply defined as the ratio between its gradient's squared mean and variance, over the data distribution. Based on several approximations, we establish a quantitative relationship between model parameters' GSNR and the generalization gap. This relationship indicates that larger GSNR during training process leads to better generalization performance. Futher, we show that, different from that of shallow models (e.g. logistic regression, support vector machines), the gradient descent optimization dynamics of DNNs naturally produces large GSNR during training, which is probably the key to DNNs\u2019 remarkable generalization ability.", "keywords": ["DNN", "generalization", "GSNR", "gradient descent"], "paperhash": "liu|understanding_why_neural_networks_generalize_well_through_gsnr_of_parameters", "_bibtex": "@inproceedings{\nLiu2020Understanding,\ntitle={Understanding Why Neural Networks Generalize Well Through GSNR of Parameters},\nauthor={Jinlong Liu and Yunzhi Bai and Guoqing Jiang and Ting Chen and Huayan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyevIJStwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/92f284434ac3b907829affb4a3bfc33ce8908ec7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyevIJStwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1732/Authors", "ICLR.cc/2020/Conference/Paper1732/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1732/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1732/Reviewers", "ICLR.cc/2020/Conference/Paper1732/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1732/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1732/Authors|ICLR.cc/2020/Conference/Paper1732/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504151693, "tmdate": 1576860558130, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1732/Authors", "ICLR.cc/2020/Conference/Paper1732/Reviewers", "ICLR.cc/2020/Conference/Paper1732/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1732/-/Official_Comment"}}}, {"id": "B1lyj_jm5B", "original": null, "number": 2, "cdate": 1572219030731, "ddate": null, "tcdate": 1572219030731, "tmdate": 1572972430629, "tddate": null, "forum": "HyevIJStwH", "replyto": "HyevIJStwH", "invitation": "ICLR.cc/2020/Conference/Paper1732/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper defines the quantity of \"gradient SNR\" (GSNR), shows that larger GSNR leads to better generalization, and shows that SGD training of deep networks has large GSNR. It tells a great story on why SGD-trained DNNs have good generalization.\n\nThis topic is highly relevant to this conference.\n\nHowever, I struggle to rate this paper, since I feel swamped with math. It is hard work to read this paper, and I can honestly say that I could semi-confidently follow until about Eq. (8). To even get there, I had to scroll back and forth to remember the definitions of the various symbols. The math may be very well correct, but it is infeasible to verify (or follow) it fully. It does not make it easier that one cannot really search a PDF for greek symbols with indices etc. Someone who reads theoretical papers all day long might do better here.\n\nThis is the reason I rate the paper Weak Reject.\n\nSome feedback points:\n\nSection 2.1:\n\nEq. (1): It seems the common definition of SNR is the ratio of mean standard deviation. Your SNR is its square. This should be explained.\n\nI think it would help the reader a lot to give some intuitive meaning to the GSNR value. Can you, in Section 2.1, explain with examples what typical (or extreme) values would be?\n\nAssumption 2.3.1:\n\nThis is dropped on the reader without any motivation. It is also confusing: \"we will make our derivation under the non-overfitting limit approximation\" conflicts with \"In the early training stage,...\" So is this whole derivation only true in the early stages?\n\nAssumption 2.3.1 seems to address a thought I had when reading this: At the end of the training, I would expect mu_q(theta) to be zero (the definition of convergence). At the start, it is arbitrary as it entirely depends on the initial values. So this paper must look at some part between the two extremes to make sense. Is it? Is this assumption related?\n\nWhat is the difference between \\sigma and \\rho? Seems one is on the data distribution and one on a sampled set. But then why is \\mu the same in both cases (Eq. (1) vs. Eq. (5))?\n\nAll plots:\n\nThe plot labels are far too small to be readable."}, "signatures": ["ICLR.cc/2020/Conference/Paper1732/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1732/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ljlwykqh@126.com", "yunzhi.bai@outlook.fr", "jianggq@pku.edu.cn", "roushi0322@sina.cn", "wanghuayan@kuaishou.com"], "title": "Understanding Why Neural Networks Generalize Well Through GSNR of Parameters", "authors": ["Jinlong Liu", "Yunzhi Bai", "Guoqing Jiang", "Ting Chen", "Huayan Wang"], "pdf": "/pdf/d80fd2ee1d630210ee8cf81fc5c106d3015aae05.pdf", "abstract": "As deep neural networks (DNNs) achieve tremendous success across many application domains, researchers tried to explore in many aspects on why they generalize well. In this paper, we provide a novel perspective on these issues using the gradient signal to noise ratio (GSNR) of parameters during training process of DNNs. The GSNR of a parameter is simply defined as the ratio between its gradient's squared mean and variance, over the data distribution. Based on several approximations, we establish a quantitative relationship between model parameters' GSNR and the generalization gap. This relationship indicates that larger GSNR during training process leads to better generalization performance. Futher, we show that, different from that of shallow models (e.g. logistic regression, support vector machines), the gradient descent optimization dynamics of DNNs naturally produces large GSNR during training, which is probably the key to DNNs\u2019 remarkable generalization ability.", "keywords": ["DNN", "generalization", "GSNR", "gradient descent"], "paperhash": "liu|understanding_why_neural_networks_generalize_well_through_gsnr_of_parameters", "_bibtex": "@inproceedings{\nLiu2020Understanding,\ntitle={Understanding Why Neural Networks Generalize Well Through GSNR of Parameters},\nauthor={Jinlong Liu and Yunzhi Bai and Guoqing Jiang and Ting Chen and Huayan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyevIJStwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/92f284434ac3b907829affb4a3bfc33ce8908ec7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyevIJStwH", "replyto": "HyevIJStwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1732/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1732/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575902452018, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1732/Reviewers"], "noninvitees": [], "tcdate": 1570237733106, "tmdate": 1575902452037, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1732/-/Official_Review"}}}], "count": 9}