{"notes": [{"id": "B1eiJyrtDB", "original": "r1xAOJo_Pr", "number": 1481, "cdate": 1569439459015, "ddate": null, "tcdate": 1569439459015, "tmdate": 1577168254624, "tddate": null, "forum": "B1eiJyrtDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Improved Generalization Bound of Permutation Invariant Deep Neural Networks", "authors": ["Akiyoshi Sannai", "Masaaki Imaizumi"], "authorids": ["akiyoshi.sannai@riken.jp", "imaizumi@ism.ac.jp"], "keywords": ["Deep Neural Network", "Invariance", "Symmetry", "Group", "Generalization"], "TL;DR": "We theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance.", "abstract": "We theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance. Learning problems with data that are invariant to permutations are frequently observed in various applications, for example, point cloud data and graph neural networks. Numerous methodologies have been developed and they achieve great performances, however, understanding a mechanism of the performance is still a developing problem. In this paper, we derive a theoretical generalization bound for invariant deep neural networks with a ReLU activation to clarify their mechanism. Consequently, our bound shows that the main term of their generalization gap is improved by $\\sqrt{n!}$ where $n$ is a number of permuting coordinates of data. Moreover, we prove that an approximation power of invariant deep neural networks can achieve an optimal rate, though the networks are restricted to be invariant. To achieve the results, we develop several new proof techniques such as correspondence with a fundamental domain and a scale-sensitive metric entropy.", "pdf": "/pdf/0f876144c871f8c31d4370cef2e7a60d9631da60.pdf", "paperhash": "sannai|improved_generalization_bound_of_permutation_invariant_deep_neural_networks", "original_pdf": "/attachment/6391e68e0a29b7e507547c1ba67e765e6e344d0a.pdf", "_bibtex": "@misc{\nsannai2020improved,\ntitle={Improved Generalization Bound of Permutation Invariant Deep Neural Networks},\nauthor={Akiyoshi Sannai and Masaaki Imaizumi},\nyear={2020},\nurl={https://openreview.net/forum?id=B1eiJyrtDB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "vfONePFDJh", "original": null, "number": 1, "cdate": 1576798724356, "ddate": null, "tcdate": 1576798724356, "tmdate": 1576800912161, "tddate": null, "forum": "B1eiJyrtDB", "replyto": "B1eiJyrtDB", "invitation": "ICLR.cc/2020/Conference/Paper1481/-/Decision", "content": {"decision": "Reject", "comment": "This work proves a generalization bound for permutation invariant neural networks (with ReLU activations). While it appears the proof is technically sound and the exact result is novel, reviewers did not feel that the proof significantly improves our understanding of model generalization relative to prior work. Because of this, the work is too incremental in its current form.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Generalization Bound of Permutation Invariant Deep Neural Networks", "authors": ["Akiyoshi Sannai", "Masaaki Imaizumi"], "authorids": ["akiyoshi.sannai@riken.jp", "imaizumi@ism.ac.jp"], "keywords": ["Deep Neural Network", "Invariance", "Symmetry", "Group", "Generalization"], "TL;DR": "We theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance.", "abstract": "We theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance. Learning problems with data that are invariant to permutations are frequently observed in various applications, for example, point cloud data and graph neural networks. Numerous methodologies have been developed and they achieve great performances, however, understanding a mechanism of the performance is still a developing problem. In this paper, we derive a theoretical generalization bound for invariant deep neural networks with a ReLU activation to clarify their mechanism. Consequently, our bound shows that the main term of their generalization gap is improved by $\\sqrt{n!}$ where $n$ is a number of permuting coordinates of data. Moreover, we prove that an approximation power of invariant deep neural networks can achieve an optimal rate, though the networks are restricted to be invariant. To achieve the results, we develop several new proof techniques such as correspondence with a fundamental domain and a scale-sensitive metric entropy.", "pdf": "/pdf/0f876144c871f8c31d4370cef2e7a60d9631da60.pdf", "paperhash": "sannai|improved_generalization_bound_of_permutation_invariant_deep_neural_networks", "original_pdf": "/attachment/6391e68e0a29b7e507547c1ba67e765e6e344d0a.pdf", "_bibtex": "@misc{\nsannai2020improved,\ntitle={Improved Generalization Bound of Permutation Invariant Deep Neural Networks},\nauthor={Akiyoshi Sannai and Masaaki Imaizumi},\nyear={2020},\nurl={https://openreview.net/forum?id=B1eiJyrtDB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "B1eiJyrtDB", "replyto": "B1eiJyrtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795709413, "tmdate": 1576800258160, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1481/-/Decision"}}}, {"id": "rJx0u1p19B", "original": null, "number": 2, "cdate": 1571962742264, "ddate": null, "tcdate": 1571962742264, "tmdate": 1575502153668, "tddate": null, "forum": "B1eiJyrtDB", "replyto": "B1eiJyrtDB", "invitation": "ICLR.cc/2020/Conference/Paper1481/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "N/A", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "This paper provides generalization bounds for permutation invariant neural networks where the learning problem is invariant to the permutation of input data. \n\nUnfortunately, the technical value of the content and its novelty is very limited since the proof reduces to a very basic argument that counts invariances (which is simply n! where n is the number of invariant dimensions) and uses a standard approach to give a generalization bound. Therefore, I don't think the results does not help us with better understanding of permutation invariant neural networks. \n\nUnfortunately, the paper has several typos and mistakes as well. Another non-technical issue is that apparently authors have removed the ICLR format and reduced margin to fit the paper in 10 pages which is against the spirit of page limit.\n\n***********************************\n\nAfter author rebuttals:\n\nAfter reading authors' response and reading the proofs, I realize that the formal proof is not trivial and requires more work that I assumed. However, I do not understand how this work can improve our understanding of permutation invariant networks. Therefore, I think the contributions are not significant enough for publication and my evaluation remains the same.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1481/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1481/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Generalization Bound of Permutation Invariant Deep Neural Networks", "authors": ["Akiyoshi Sannai", "Masaaki Imaizumi"], "authorids": ["akiyoshi.sannai@riken.jp", "imaizumi@ism.ac.jp"], "keywords": ["Deep Neural Network", "Invariance", "Symmetry", "Group", "Generalization"], "TL;DR": "We theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance.", "abstract": "We theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance. Learning problems with data that are invariant to permutations are frequently observed in various applications, for example, point cloud data and graph neural networks. Numerous methodologies have been developed and they achieve great performances, however, understanding a mechanism of the performance is still a developing problem. In this paper, we derive a theoretical generalization bound for invariant deep neural networks with a ReLU activation to clarify their mechanism. Consequently, our bound shows that the main term of their generalization gap is improved by $\\sqrt{n!}$ where $n$ is a number of permuting coordinates of data. Moreover, we prove that an approximation power of invariant deep neural networks can achieve an optimal rate, though the networks are restricted to be invariant. To achieve the results, we develop several new proof techniques such as correspondence with a fundamental domain and a scale-sensitive metric entropy.", "pdf": "/pdf/0f876144c871f8c31d4370cef2e7a60d9631da60.pdf", "paperhash": "sannai|improved_generalization_bound_of_permutation_invariant_deep_neural_networks", "original_pdf": "/attachment/6391e68e0a29b7e507547c1ba67e765e6e344d0a.pdf", "_bibtex": "@misc{\nsannai2020improved,\ntitle={Improved Generalization Bound of Permutation Invariant Deep Neural Networks},\nauthor={Akiyoshi Sannai and Masaaki Imaizumi},\nyear={2020},\nurl={https://openreview.net/forum?id=B1eiJyrtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1eiJyrtDB", "replyto": "B1eiJyrtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1481/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1481/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575505865703, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1481/Reviewers"], "noninvitees": [], "tcdate": 1570237736759, "tmdate": 1575505865719, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1481/-/Official_Review"}}}, {"id": "HklGNpG2jH", "original": null, "number": 8, "cdate": 1573821737647, "ddate": null, "tcdate": 1573821737647, "tmdate": 1573821802262, "tddate": null, "forum": "B1eiJyrtDB", "replyto": "HklWqgGrcr", "invitation": "ICLR.cc/2020/Conference/Paper1481/-/Official_Comment", "content": {"title": "We add discussion about $\\varepsilon^p$.", "comment": "We appreciate your critical comment.\nIn our updated paper, we add description about the point after Theorem 2 in our paper. For summary, the increasing speed of $n! \\varepsilon^p=n! \\varepsilon^{nD}$ in terms of $n$ is sufficiently fast for any $\\varepsilon$ and $D$. We appreciate if you check the point. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1481/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1481/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Generalization Bound of Permutation Invariant Deep Neural Networks", "authors": ["Akiyoshi Sannai", "Masaaki Imaizumi"], "authorids": ["akiyoshi.sannai@riken.jp", "imaizumi@ism.ac.jp"], "keywords": ["Deep Neural Network", "Invariance", "Symmetry", "Group", "Generalization"], "TL;DR": "We theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance.", "abstract": "We theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance. Learning problems with data that are invariant to permutations are frequently observed in various applications, for example, point cloud data and graph neural networks. Numerous methodologies have been developed and they achieve great performances, however, understanding a mechanism of the performance is still a developing problem. In this paper, we derive a theoretical generalization bound for invariant deep neural networks with a ReLU activation to clarify their mechanism. Consequently, our bound shows that the main term of their generalization gap is improved by $\\sqrt{n!}$ where $n$ is a number of permuting coordinates of data. Moreover, we prove that an approximation power of invariant deep neural networks can achieve an optimal rate, though the networks are restricted to be invariant. To achieve the results, we develop several new proof techniques such as correspondence with a fundamental domain and a scale-sensitive metric entropy.", "pdf": "/pdf/0f876144c871f8c31d4370cef2e7a60d9631da60.pdf", "paperhash": "sannai|improved_generalization_bound_of_permutation_invariant_deep_neural_networks", "original_pdf": "/attachment/6391e68e0a29b7e507547c1ba67e765e6e344d0a.pdf", "_bibtex": "@misc{\nsannai2020improved,\ntitle={Improved Generalization Bound of Permutation Invariant Deep Neural Networks},\nauthor={Akiyoshi Sannai and Masaaki Imaizumi},\nyear={2020},\nurl={https://openreview.net/forum?id=B1eiJyrtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1eiJyrtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1481/Authors", "ICLR.cc/2020/Conference/Paper1481/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1481/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1481/Reviewers", "ICLR.cc/2020/Conference/Paper1481/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1481/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1481/Authors|ICLR.cc/2020/Conference/Paper1481/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155374, "tmdate": 1576860545487, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1481/Authors", "ICLR.cc/2020/Conference/Paper1481/Reviewers", "ICLR.cc/2020/Conference/Paper1481/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1481/-/Official_Comment"}}}, {"id": "BJgTNhz3ir", "original": null, "number": 7, "cdate": 1573821492936, "ddate": null, "tcdate": 1573821492936, "tmdate": 1573821492936, "tddate": null, "forum": "B1eiJyrtDB", "replyto": "BkeJKjc0Kr", "invitation": "ICLR.cc/2020/Conference/Paper1481/-/Official_Comment", "content": {"title": "We add the comparison with the paper (Sokolic et al., ICML 2017) in our paper.", "comment": "We clarify the difference between our paper and okolic et al., ICML 2017 in Section in the updated version of our paper. We are glad if you check the paragraph."}, "signatures": ["ICLR.cc/2020/Conference/Paper1481/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1481/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Generalization Bound of Permutation Invariant Deep Neural Networks", "authors": ["Akiyoshi Sannai", "Masaaki Imaizumi"], "authorids": ["akiyoshi.sannai@riken.jp", "imaizumi@ism.ac.jp"], "keywords": ["Deep Neural Network", "Invariance", "Symmetry", "Group", "Generalization"], "TL;DR": "We theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance.", "abstract": "We theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance. Learning problems with data that are invariant to permutations are frequently observed in various applications, for example, point cloud data and graph neural networks. Numerous methodologies have been developed and they achieve great performances, however, understanding a mechanism of the performance is still a developing problem. In this paper, we derive a theoretical generalization bound for invariant deep neural networks with a ReLU activation to clarify their mechanism. Consequently, our bound shows that the main term of their generalization gap is improved by $\\sqrt{n!}$ where $n$ is a number of permuting coordinates of data. Moreover, we prove that an approximation power of invariant deep neural networks can achieve an optimal rate, though the networks are restricted to be invariant. To achieve the results, we develop several new proof techniques such as correspondence with a fundamental domain and a scale-sensitive metric entropy.", "pdf": "/pdf/0f876144c871f8c31d4370cef2e7a60d9631da60.pdf", "paperhash": "sannai|improved_generalization_bound_of_permutation_invariant_deep_neural_networks", "original_pdf": "/attachment/6391e68e0a29b7e507547c1ba67e765e6e344d0a.pdf", "_bibtex": "@misc{\nsannai2020improved,\ntitle={Improved Generalization Bound of Permutation Invariant Deep Neural Networks},\nauthor={Akiyoshi Sannai and Masaaki Imaizumi},\nyear={2020},\nurl={https://openreview.net/forum?id=B1eiJyrtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1eiJyrtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1481/Authors", "ICLR.cc/2020/Conference/Paper1481/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1481/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1481/Reviewers", "ICLR.cc/2020/Conference/Paper1481/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1481/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1481/Authors|ICLR.cc/2020/Conference/Paper1481/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155374, "tmdate": 1576860545487, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1481/Authors", "ICLR.cc/2020/Conference/Paper1481/Reviewers", "ICLR.cc/2020/Conference/Paper1481/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1481/-/Official_Comment"}}}, {"id": "rkxB0sz2iH", "original": null, "number": 6, "cdate": 1573821388732, "ddate": null, "tcdate": 1573821388732, "tmdate": 1573821388732, "tddate": null, "forum": "B1eiJyrtDB", "replyto": "B1eiJyrtDB", "invitation": "ICLR.cc/2020/Conference/Paper1481/-/Official_Comment", "content": {"title": "We updated the submitted paper.", "comment": "Updated points are as follow:\n- Add description to show the technical novelty and intuition of our paper. (Section 5 and 6)\n- We cite the paper Sokolic+ (2017) and discuss differences between it and our paper. (Section 5)\n- We show that the term $\\varepsilon^p$ does not provide a problem with a large $n$. (Section 3)\n- Correct several sentences and typos.\n- We omit the mistakenly loaded package and modify the format as following the template.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1481/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1481/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Generalization Bound of Permutation Invariant Deep Neural Networks", "authors": ["Akiyoshi Sannai", "Masaaki Imaizumi"], "authorids": ["akiyoshi.sannai@riken.jp", "imaizumi@ism.ac.jp"], "keywords": ["Deep Neural Network", "Invariance", "Symmetry", "Group", "Generalization"], "TL;DR": "We theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance.", "abstract": "We theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance. Learning problems with data that are invariant to permutations are frequently observed in various applications, for example, point cloud data and graph neural networks. Numerous methodologies have been developed and they achieve great performances, however, understanding a mechanism of the performance is still a developing problem. In this paper, we derive a theoretical generalization bound for invariant deep neural networks with a ReLU activation to clarify their mechanism. Consequently, our bound shows that the main term of their generalization gap is improved by $\\sqrt{n!}$ where $n$ is a number of permuting coordinates of data. Moreover, we prove that an approximation power of invariant deep neural networks can achieve an optimal rate, though the networks are restricted to be invariant. To achieve the results, we develop several new proof techniques such as correspondence with a fundamental domain and a scale-sensitive metric entropy.", "pdf": "/pdf/0f876144c871f8c31d4370cef2e7a60d9631da60.pdf", "paperhash": "sannai|improved_generalization_bound_of_permutation_invariant_deep_neural_networks", "original_pdf": "/attachment/6391e68e0a29b7e507547c1ba67e765e6e344d0a.pdf", "_bibtex": "@misc{\nsannai2020improved,\ntitle={Improved Generalization Bound of Permutation Invariant Deep Neural Networks},\nauthor={Akiyoshi Sannai and Masaaki Imaizumi},\nyear={2020},\nurl={https://openreview.net/forum?id=B1eiJyrtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1eiJyrtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1481/Authors", "ICLR.cc/2020/Conference/Paper1481/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1481/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1481/Reviewers", "ICLR.cc/2020/Conference/Paper1481/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1481/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1481/Authors|ICLR.cc/2020/Conference/Paper1481/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155374, "tmdate": 1576860545487, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1481/Authors", "ICLR.cc/2020/Conference/Paper1481/Reviewers", "ICLR.cc/2020/Conference/Paper1481/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1481/-/Official_Comment"}}}, {"id": "Hkl4hU95jr", "original": null, "number": 5, "cdate": 1573721771943, "ddate": null, "tcdate": 1573721771943, "tmdate": 1573721771943, "tddate": null, "forum": "B1eiJyrtDB", "replyto": "ryxk9laYjH", "invitation": "ICLR.cc/2020/Conference/Paper1481/-/Official_Comment", "content": {"title": "Thank you for your response.", "comment": "Thank you for agreeing on the novelty of our work.\n\nAbout significance, we are confident that it is not easy to develop proof to derive the bound improved by n!. Technically speaking, to obtain the improved bound, we have to find n! subsets of functions WITHOUT overlapping with each other. To the aim, we introduce the notion of the fundamental domain and prove that a volume of overlapping has measure zero (Specifically, Lemma 1 in our paper). Without our techniques, the improvement by n! is a folklore, but not theoretical analysis. Hence, we believe that it is significant to develop such the technique and show the improved bound.\nIf you are not agree with the importance of our achievement, please give us references which show the improvement rigorously."}, "signatures": ["ICLR.cc/2020/Conference/Paper1481/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1481/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Generalization Bound of Permutation Invariant Deep Neural Networks", "authors": ["Akiyoshi Sannai", "Masaaki Imaizumi"], "authorids": ["akiyoshi.sannai@riken.jp", "imaizumi@ism.ac.jp"], "keywords": ["Deep Neural Network", "Invariance", "Symmetry", "Group", "Generalization"], "TL;DR": "We theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance.", "abstract": "We theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance. Learning problems with data that are invariant to permutations are frequently observed in various applications, for example, point cloud data and graph neural networks. Numerous methodologies have been developed and they achieve great performances, however, understanding a mechanism of the performance is still a developing problem. In this paper, we derive a theoretical generalization bound for invariant deep neural networks with a ReLU activation to clarify their mechanism. Consequently, our bound shows that the main term of their generalization gap is improved by $\\sqrt{n!}$ where $n$ is a number of permuting coordinates of data. Moreover, we prove that an approximation power of invariant deep neural networks can achieve an optimal rate, though the networks are restricted to be invariant. To achieve the results, we develop several new proof techniques such as correspondence with a fundamental domain and a scale-sensitive metric entropy.", "pdf": "/pdf/0f876144c871f8c31d4370cef2e7a60d9631da60.pdf", "paperhash": "sannai|improved_generalization_bound_of_permutation_invariant_deep_neural_networks", "original_pdf": "/attachment/6391e68e0a29b7e507547c1ba67e765e6e344d0a.pdf", "_bibtex": "@misc{\nsannai2020improved,\ntitle={Improved Generalization Bound of Permutation Invariant Deep Neural Networks},\nauthor={Akiyoshi Sannai and Masaaki Imaizumi},\nyear={2020},\nurl={https://openreview.net/forum?id=B1eiJyrtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1eiJyrtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1481/Authors", "ICLR.cc/2020/Conference/Paper1481/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1481/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1481/Reviewers", "ICLR.cc/2020/Conference/Paper1481/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1481/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1481/Authors|ICLR.cc/2020/Conference/Paper1481/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155374, "tmdate": 1576860545487, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1481/Authors", "ICLR.cc/2020/Conference/Paper1481/Reviewers", "ICLR.cc/2020/Conference/Paper1481/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1481/-/Official_Comment"}}}, {"id": "ryxk9laYjH", "original": null, "number": 4, "cdate": 1573666951264, "ddate": null, "tcdate": 1573666951264, "tmdate": 1573666951264, "tddate": null, "forum": "B1eiJyrtDB", "replyto": "rkeUodgdsr", "invitation": "ICLR.cc/2020/Conference/Paper1481/-/Official_Comment", "content": {"title": "Thanks for your reponse", "comment": "I agree with you that generalization of invariant DNNs are not studied before. However, my main concern is the significance of the work. Basically, covering numbers count the number of different functions in the hypothesis class where the notion of different depends on some metric. Now, if there the input has invariance, one can take advantage of that and reduce this total number of different functions by n! Even though this very specific problem has not been studied before, it is not clear to me that this contribution is significant enough to be accepted at ICLR."}, "signatures": ["ICLR.cc/2020/Conference/Paper1481/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1481/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Generalization Bound of Permutation Invariant Deep Neural Networks", "authors": ["Akiyoshi Sannai", "Masaaki Imaizumi"], "authorids": ["akiyoshi.sannai@riken.jp", "imaizumi@ism.ac.jp"], "keywords": ["Deep Neural Network", "Invariance", "Symmetry", "Group", "Generalization"], "TL;DR": "We theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance.", "abstract": "We theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance. Learning problems with data that are invariant to permutations are frequently observed in various applications, for example, point cloud data and graph neural networks. Numerous methodologies have been developed and they achieve great performances, however, understanding a mechanism of the performance is still a developing problem. In this paper, we derive a theoretical generalization bound for invariant deep neural networks with a ReLU activation to clarify their mechanism. Consequently, our bound shows that the main term of their generalization gap is improved by $\\sqrt{n!}$ where $n$ is a number of permuting coordinates of data. Moreover, we prove that an approximation power of invariant deep neural networks can achieve an optimal rate, though the networks are restricted to be invariant. To achieve the results, we develop several new proof techniques such as correspondence with a fundamental domain and a scale-sensitive metric entropy.", "pdf": "/pdf/0f876144c871f8c31d4370cef2e7a60d9631da60.pdf", "paperhash": "sannai|improved_generalization_bound_of_permutation_invariant_deep_neural_networks", "original_pdf": "/attachment/6391e68e0a29b7e507547c1ba67e765e6e344d0a.pdf", "_bibtex": "@misc{\nsannai2020improved,\ntitle={Improved Generalization Bound of Permutation Invariant Deep Neural Networks},\nauthor={Akiyoshi Sannai and Masaaki Imaizumi},\nyear={2020},\nurl={https://openreview.net/forum?id=B1eiJyrtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1eiJyrtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1481/Authors", "ICLR.cc/2020/Conference/Paper1481/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1481/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1481/Reviewers", "ICLR.cc/2020/Conference/Paper1481/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1481/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1481/Authors|ICLR.cc/2020/Conference/Paper1481/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155374, "tmdate": 1576860545487, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1481/Authors", "ICLR.cc/2020/Conference/Paper1481/Reviewers", "ICLR.cc/2020/Conference/Paper1481/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1481/-/Official_Comment"}}}, {"id": "Hyg8wsedor", "original": null, "number": 3, "cdate": 1573550941749, "ddate": null, "tcdate": 1573550941749, "tmdate": 1573550941749, "tddate": null, "forum": "B1eiJyrtDB", "replyto": "BkeJKjc0Kr", "invitation": "ICLR.cc/2020/Conference/Paper1481/-/Official_Comment", "content": {"title": "Thank you for your accurate comment. ", "comment": "Thank you for your accurate comment. Especially, we would appreciate your evaluation for our technical contributions.\n\nWe also thank you for the introduction of the previous research[a].  We confirmed that their main result is very similar to ours. The superiority of our results is as follows. At first, we construct explicit invariant deep neural networks, which guarantee practical and useful methods. One of them is a new one that can achieve the same objectives as DeepSets (Zaheer 2018). Since the paper [a] is written with an abstract framework, our paper can provide useful knowledge. Secondly, our analysis is not limited to classification but can be applied to general learning methods including regression. Thirdly, our results provide a more specific analysis of permutation invariant networks, which can be used for future specific expansion and analysis."}, "signatures": ["ICLR.cc/2020/Conference/Paper1481/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1481/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Generalization Bound of Permutation Invariant Deep Neural Networks", "authors": ["Akiyoshi Sannai", "Masaaki Imaizumi"], "authorids": ["akiyoshi.sannai@riken.jp", "imaizumi@ism.ac.jp"], "keywords": ["Deep Neural Network", "Invariance", "Symmetry", "Group", "Generalization"], "TL;DR": "We theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance.", "abstract": "We theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance. Learning problems with data that are invariant to permutations are frequently observed in various applications, for example, point cloud data and graph neural networks. Numerous methodologies have been developed and they achieve great performances, however, understanding a mechanism of the performance is still a developing problem. In this paper, we derive a theoretical generalization bound for invariant deep neural networks with a ReLU activation to clarify their mechanism. Consequently, our bound shows that the main term of their generalization gap is improved by $\\sqrt{n!}$ where $n$ is a number of permuting coordinates of data. Moreover, we prove that an approximation power of invariant deep neural networks can achieve an optimal rate, though the networks are restricted to be invariant. To achieve the results, we develop several new proof techniques such as correspondence with a fundamental domain and a scale-sensitive metric entropy.", "pdf": "/pdf/0f876144c871f8c31d4370cef2e7a60d9631da60.pdf", "paperhash": "sannai|improved_generalization_bound_of_permutation_invariant_deep_neural_networks", "original_pdf": "/attachment/6391e68e0a29b7e507547c1ba67e765e6e344d0a.pdf", "_bibtex": "@misc{\nsannai2020improved,\ntitle={Improved Generalization Bound of Permutation Invariant Deep Neural Networks},\nauthor={Akiyoshi Sannai and Masaaki Imaizumi},\nyear={2020},\nurl={https://openreview.net/forum?id=B1eiJyrtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1eiJyrtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1481/Authors", "ICLR.cc/2020/Conference/Paper1481/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1481/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1481/Reviewers", "ICLR.cc/2020/Conference/Paper1481/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1481/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1481/Authors|ICLR.cc/2020/Conference/Paper1481/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155374, "tmdate": 1576860545487, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1481/Authors", "ICLR.cc/2020/Conference/Paper1481/Reviewers", "ICLR.cc/2020/Conference/Paper1481/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1481/-/Official_Comment"}}}, {"id": "rkeUodgdsr", "original": null, "number": 2, "cdate": 1573550238282, "ddate": null, "tcdate": 1573550238282, "tmdate": 1573550238282, "tddate": null, "forum": "B1eiJyrtDB", "replyto": "rJx0u1p19B", "invitation": "ICLR.cc/2020/Conference/Paper1481/-/Official_Comment", "content": {"title": "Could you give me some evidence or references?", "comment": "Thank you for your comment.\n\nAs mentioned in our paper, we developed several novel techniques as follow: (i) we prove a correspondence between invariant DNNs and DNNs on the fundamental domain, and (ii) we derive a covering number for a functional space which is sensitive to a volume of the domain of functions. To the best of our knowledge, such techniques are not commonly used in the analysis of deep neural networks.\n\nCould you give me some pieces of evidence or references which support your opinion that says our analysis follows very basis arguments? As your comment does not provide such clear evidence, we cannot find a way to discuss it with you.\n\nAbout the format of our paper, we mistakenly load the \"fullpage\" package, hence the margin of our paper is changed. About the point, we have not refutation and will modify it.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1481/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1481/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Generalization Bound of Permutation Invariant Deep Neural Networks", "authors": ["Akiyoshi Sannai", "Masaaki Imaizumi"], "authorids": ["akiyoshi.sannai@riken.jp", "imaizumi@ism.ac.jp"], "keywords": ["Deep Neural Network", "Invariance", "Symmetry", "Group", "Generalization"], "TL;DR": "We theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance.", "abstract": "We theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance. Learning problems with data that are invariant to permutations are frequently observed in various applications, for example, point cloud data and graph neural networks. Numerous methodologies have been developed and they achieve great performances, however, understanding a mechanism of the performance is still a developing problem. In this paper, we derive a theoretical generalization bound for invariant deep neural networks with a ReLU activation to clarify their mechanism. Consequently, our bound shows that the main term of their generalization gap is improved by $\\sqrt{n!}$ where $n$ is a number of permuting coordinates of data. Moreover, we prove that an approximation power of invariant deep neural networks can achieve an optimal rate, though the networks are restricted to be invariant. To achieve the results, we develop several new proof techniques such as correspondence with a fundamental domain and a scale-sensitive metric entropy.", "pdf": "/pdf/0f876144c871f8c31d4370cef2e7a60d9631da60.pdf", "paperhash": "sannai|improved_generalization_bound_of_permutation_invariant_deep_neural_networks", "original_pdf": "/attachment/6391e68e0a29b7e507547c1ba67e765e6e344d0a.pdf", "_bibtex": "@misc{\nsannai2020improved,\ntitle={Improved Generalization Bound of Permutation Invariant Deep Neural Networks},\nauthor={Akiyoshi Sannai and Masaaki Imaizumi},\nyear={2020},\nurl={https://openreview.net/forum?id=B1eiJyrtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1eiJyrtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1481/Authors", "ICLR.cc/2020/Conference/Paper1481/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1481/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1481/Reviewers", "ICLR.cc/2020/Conference/Paper1481/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1481/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1481/Authors|ICLR.cc/2020/Conference/Paper1481/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155374, "tmdate": 1576860545487, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1481/Authors", "ICLR.cc/2020/Conference/Paper1481/Reviewers", "ICLR.cc/2020/Conference/Paper1481/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1481/-/Official_Comment"}}}, {"id": "BygmiLgujB", "original": null, "number": 1, "cdate": 1573549723497, "ddate": null, "tcdate": 1573549723497, "tmdate": 1573549723497, "tddate": null, "forum": "B1eiJyrtDB", "replyto": "HklWqgGrcr", "invitation": "ICLR.cc/2020/Conference/Paper1481/-/Official_Comment", "content": {"title": "Though the rate of $\\varepsilon$ is critical, the generalization bound can be tight with large $n$.", "comment": "Thank you for your critical opinion.\n\nAs you mentioned, the order of $\\varepsilon$ is very important, thus we will add the discussion. We expect that our generalization bound may get loose when $n$ is not sufficiently large. In contrast, when $n$ is reasonably large, our bound becomes tight since $n!$ increases rapidly rather than $\\varepsilon^p$.\n\nAbout the clarity, we will modify our description and correct the typos."}, "signatures": ["ICLR.cc/2020/Conference/Paper1481/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1481/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Generalization Bound of Permutation Invariant Deep Neural Networks", "authors": ["Akiyoshi Sannai", "Masaaki Imaizumi"], "authorids": ["akiyoshi.sannai@riken.jp", "imaizumi@ism.ac.jp"], "keywords": ["Deep Neural Network", "Invariance", "Symmetry", "Group", "Generalization"], "TL;DR": "We theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance.", "abstract": "We theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance. Learning problems with data that are invariant to permutations are frequently observed in various applications, for example, point cloud data and graph neural networks. Numerous methodologies have been developed and they achieve great performances, however, understanding a mechanism of the performance is still a developing problem. In this paper, we derive a theoretical generalization bound for invariant deep neural networks with a ReLU activation to clarify their mechanism. Consequently, our bound shows that the main term of their generalization gap is improved by $\\sqrt{n!}$ where $n$ is a number of permuting coordinates of data. Moreover, we prove that an approximation power of invariant deep neural networks can achieve an optimal rate, though the networks are restricted to be invariant. To achieve the results, we develop several new proof techniques such as correspondence with a fundamental domain and a scale-sensitive metric entropy.", "pdf": "/pdf/0f876144c871f8c31d4370cef2e7a60d9631da60.pdf", "paperhash": "sannai|improved_generalization_bound_of_permutation_invariant_deep_neural_networks", "original_pdf": "/attachment/6391e68e0a29b7e507547c1ba67e765e6e344d0a.pdf", "_bibtex": "@misc{\nsannai2020improved,\ntitle={Improved Generalization Bound of Permutation Invariant Deep Neural Networks},\nauthor={Akiyoshi Sannai and Masaaki Imaizumi},\nyear={2020},\nurl={https://openreview.net/forum?id=B1eiJyrtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1eiJyrtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1481/Authors", "ICLR.cc/2020/Conference/Paper1481/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1481/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1481/Reviewers", "ICLR.cc/2020/Conference/Paper1481/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1481/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1481/Authors|ICLR.cc/2020/Conference/Paper1481/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155374, "tmdate": 1576860545487, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1481/Authors", "ICLR.cc/2020/Conference/Paper1481/Reviewers", "ICLR.cc/2020/Conference/Paper1481/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1481/-/Official_Comment"}}}, {"id": "BkeJKjc0Kr", "original": null, "number": 1, "cdate": 1571887990554, "ddate": null, "tcdate": 1571887990554, "tmdate": 1572972463130, "tddate": null, "forum": "B1eiJyrtDB", "replyto": "B1eiJyrtDB", "invitation": "ICLR.cc/2020/Conference/Paper1481/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper derives a generalization bound for permutation invariant networks. The main idea is to prove that the bound is inversely proportional to the square-root of the number of possible permutations to the input. The key result is Theorem 3 that bounds the covering number of a neural network (defined under an approximation control bound, Thm 4) using the number of permutations. The paper proves the theorem by showing that the space of input permutations can reduced to group actions over a fundamental domain, and deriving a bound for the covering number of the fundamental domain (Lemma 1), which is then extended to derive the same for the neural network setting. For the permutation invariance setting, the fundamental domain is obtained via the sorting operator. \n\nPros:\n1. The paper appears to be mathematically rigorous, and at the same time, is straightforward to follow, with useful intuitions provided whenever required. \n2. The provided theoretical result perhaps extends the work on universal approximation theorem for permutation invariant networks in Sennai et al, and Maron et al., 2019. Further, the generalization bound for permutation invariance is new to my knowledge.\n\nCons:\n1. While, the proof appears to be novel for permutation invariance per se, however I do not think the main findings in this paper or the proof approach are sufficiently novel. For example, generalization bounds under invariances have been explored previously, perhaps the most related to this paper is [a] below that already shows (in a similar vein as this paper) that the bound decreases proportional to 1/\\sqrt(T), where T is the number of invariances used. While, that work uses affine transformations of the input from a base space for the invariances (which this paper calls fundamental domain), the current paper uses permutation invariance and thus gets the bound proportional to 1/sqrt(n!). In the context of this prior work, the contribution of this paper appears incremental. The paper should cite this work and contrast against the results and proof methods in it.\n\n[a] Generalization Error of Invariant Classifiers, Sokolic et al., ICML 2017.\n\n2. The paper has several typos and grammatical errors through out, which are easily fixable though!\n\nOverall, this paper is technically rigorous, and novel in its very specific context of deriving the generalization bounds for permutation invariant networks. However, in the broader context of invariances in general and their bounds, the contribution appears to be marginal. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1481/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1481/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Generalization Bound of Permutation Invariant Deep Neural Networks", "authors": ["Akiyoshi Sannai", "Masaaki Imaizumi"], "authorids": ["akiyoshi.sannai@riken.jp", "imaizumi@ism.ac.jp"], "keywords": ["Deep Neural Network", "Invariance", "Symmetry", "Group", "Generalization"], "TL;DR": "We theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance.", "abstract": "We theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance. Learning problems with data that are invariant to permutations are frequently observed in various applications, for example, point cloud data and graph neural networks. Numerous methodologies have been developed and they achieve great performances, however, understanding a mechanism of the performance is still a developing problem. In this paper, we derive a theoretical generalization bound for invariant deep neural networks with a ReLU activation to clarify their mechanism. Consequently, our bound shows that the main term of their generalization gap is improved by $\\sqrt{n!}$ where $n$ is a number of permuting coordinates of data. Moreover, we prove that an approximation power of invariant deep neural networks can achieve an optimal rate, though the networks are restricted to be invariant. To achieve the results, we develop several new proof techniques such as correspondence with a fundamental domain and a scale-sensitive metric entropy.", "pdf": "/pdf/0f876144c871f8c31d4370cef2e7a60d9631da60.pdf", "paperhash": "sannai|improved_generalization_bound_of_permutation_invariant_deep_neural_networks", "original_pdf": "/attachment/6391e68e0a29b7e507547c1ba67e765e6e344d0a.pdf", "_bibtex": "@misc{\nsannai2020improved,\ntitle={Improved Generalization Bound of Permutation Invariant Deep Neural Networks},\nauthor={Akiyoshi Sannai and Masaaki Imaizumi},\nyear={2020},\nurl={https://openreview.net/forum?id=B1eiJyrtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1eiJyrtDB", "replyto": "B1eiJyrtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1481/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1481/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575505865703, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1481/Reviewers"], "noninvitees": [], "tcdate": 1570237736759, "tmdate": 1575505865719, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1481/-/Official_Review"}}}, {"id": "HklWqgGrcr", "original": null, "number": 3, "cdate": 1572311176765, "ddate": null, "tcdate": 1572311176765, "tmdate": 1572972463042, "tddate": null, "forum": "B1eiJyrtDB", "replyto": "B1eiJyrtDB", "invitation": "ICLR.cc/2020/Conference/Paper1481/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a derivation of a generalization bound for neural networks designed specifically to deal with permutation invariant data (such as point clouds). The heart of the contribution is that the bound includes a  1/n! (i.e. 1 / (n-factorial)) factor to the major term, where n is the number of permutable elements there are in a data example (think: number of points in a point cloud). This term goes some way towards making the bound tight.\n\nThe 1/n! factor in the bound may be an interesting development  but the novelty does appear to be limited.  Also, the authors fail to discuss that --  as part of that same term -- there is a factor: (1 / (epsilon^p)), where p is the dimension of the input and epsilon is a small error term. As p is proportional to n, and epsilon is quite small, this term could well dominate the factorial in many practical settings. A discussion of the relation between these terms is appropriate and seems to be missing.\n\nClarity:\nIn general the paper is fairly well written, but there are multiple instances of missing articles and strange idiom violations (eg. p. 4, remark 1: \"such the bound\" versus \"such a bound\")\n\nMore seriously, the proof of Lemma 1 was quite hard to follow (esp. the second paragraph). I would suggest putting less emphasis on the relatively straightforward construction of the sorting mechanism in Propositions 2 and 3, and use the space to more clearly detail the proof of Lemma 1, which is, after all, the heart of the contribution.\n\nI also found the proof of proposition 4 too confusing to easily follow. What is the interpretation of the indices (1, ..., K) on the functions?\n\nFinally, I would have liked to see some interpretation of the findings in a discussion section (or in an extended conclusion).   \n\nMinor issues:\n\n- First sentence of the abstract is difficult to parse and does not seem like an accurate assessment of the contribution of the paper. \n\n- Paragraph 2 of the introduction presents a sequence of argument whose logic seems inconsistent to me. There is a drift from a discussion of generalization of neural networks to a mention of work on the very distinct topic of the representational capacity of neural networks (i.e. universal approximation property of neural networks). The linking text \"To tackle the quesiton, ...\" is not appropriate.\n\n- Unlike Example 1, Example 2 (p.3) is not helpful in motivating the permutation invariant neural networks. The definition makes direct reference to Proposition 2 that will not be introduced for another 3 pages. \n\n- In Sec. 4.1, it seems like a phi symbol is used when I believe a null symbol was intended\n\n- Proposition 3: \"max( z_1, z_1 )\"  should be \"max( z_1, z_2 )\" with the adjustment carrying through to the other side of the equals.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1481/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1481/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improved Generalization Bound of Permutation Invariant Deep Neural Networks", "authors": ["Akiyoshi Sannai", "Masaaki Imaizumi"], "authorids": ["akiyoshi.sannai@riken.jp", "imaizumi@ism.ac.jp"], "keywords": ["Deep Neural Network", "Invariance", "Symmetry", "Group", "Generalization"], "TL;DR": "We theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance.", "abstract": "We theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance. Learning problems with data that are invariant to permutations are frequently observed in various applications, for example, point cloud data and graph neural networks. Numerous methodologies have been developed and they achieve great performances, however, understanding a mechanism of the performance is still a developing problem. In this paper, we derive a theoretical generalization bound for invariant deep neural networks with a ReLU activation to clarify their mechanism. Consequently, our bound shows that the main term of their generalization gap is improved by $\\sqrt{n!}$ where $n$ is a number of permuting coordinates of data. Moreover, we prove that an approximation power of invariant deep neural networks can achieve an optimal rate, though the networks are restricted to be invariant. To achieve the results, we develop several new proof techniques such as correspondence with a fundamental domain and a scale-sensitive metric entropy.", "pdf": "/pdf/0f876144c871f8c31d4370cef2e7a60d9631da60.pdf", "paperhash": "sannai|improved_generalization_bound_of_permutation_invariant_deep_neural_networks", "original_pdf": "/attachment/6391e68e0a29b7e507547c1ba67e765e6e344d0a.pdf", "_bibtex": "@misc{\nsannai2020improved,\ntitle={Improved Generalization Bound of Permutation Invariant Deep Neural Networks},\nauthor={Akiyoshi Sannai and Masaaki Imaizumi},\nyear={2020},\nurl={https://openreview.net/forum?id=B1eiJyrtDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1eiJyrtDB", "replyto": "B1eiJyrtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1481/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1481/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575505865703, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1481/Reviewers"], "noninvitees": [], "tcdate": 1570237736759, "tmdate": 1575505865719, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1481/-/Official_Review"}}}], "count": 13}