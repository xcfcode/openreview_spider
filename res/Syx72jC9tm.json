{"notes": [{"id": "Syx72jC9tm", "original": "SklUKGpcKX", "number": 698, "cdate": 1538087851315, "ddate": null, "tcdate": 1538087851315, "tmdate": 1556603735949, "tddate": null, "forum": "Syx72jC9tm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Invariant and Equivariant Graph Networks", "abstract": "Invariant and equivariant networks have been successfully used for learning images, sets, point clouds, and graphs. A basic challenge in developing such networks is finding the maximal collection of invariant and equivariant \\emph{linear} layers. Although this question is answered for the first three examples (for popular transformations, at-least), a full characterization of invariant and equivariant linear layers for graphs is not known. \n\nIn this paper we provide a characterization of all permutation invariant and equivariant linear layers for (hyper-)graph data, and show that their dimension, in case of edge-value graph data, is $2$ and $15$, respectively. More generally, for graph data defined on $k$-tuples of nodes, the dimension is the $k$-th and $2k$-th Bell numbers. Orthogonal bases for the layers are computed, including generalization to multi-graph data. The constant number of basis elements and their characteristics allow successfully applying the networks to different size graphs. From the theoretical point of view, our results generalize and unify recent advancement in equivariant deep learning. In particular, we show that our model is capable of approximating any message passing neural network.\n\nApplying these new linear layers in a simple deep neural network framework is shown to achieve comparable results to state-of-the-art and to have better expressivity than previous invariant and equivariant bases.\n", "keywords": ["graph learning", "equivariance", "deep learning"], "authorids": ["haggai.maron@weizmann.ac.il", "heli.benhamu@weizmann.ac.il", "nadav13@gmail.com", "yaron.lipman@weizmann.ac.il"], "authors": ["Haggai Maron", "Heli Ben-Hamu", "Nadav Shamir", "Yaron Lipman"], "TL;DR": "The paper provides a full characterization of permutation invariant and equivariant linear layers for graph data.", "pdf": "/pdf/60e3baf3636d94346d28756dc1dc32844179f622.pdf", "paperhash": "maron|invariant_and_equivariant_graph_networks", "_bibtex": "@inproceedings{\nmaron2018invariant,\ntitle={Invariant and Equivariant Graph Networks},\nauthor={Haggai Maron and Heli Ben-Hamu and Nadav Shamir and Yaron Lipman},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx72jC9tm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BJl9NfdQgE", "original": null, "number": 1, "cdate": 1544942130176, "ddate": null, "tcdate": 1544942130176, "tmdate": 1545354491109, "tddate": null, "forum": "Syx72jC9tm", "replyto": "Syx72jC9tm", "invitation": "ICLR.cc/2019/Conference/-/Paper698/Meta_Review", "content": {"metareview": "The paper provides a comprehensive study and generalisations of previous results on linear permutation invariant and equivariant operators / layers for the case of hypergraph data on multiple node sets. Reviewers indicate that the paper makes a particularly interesting and important contribution, with applications to graphs and hyper-graphs, as demonstrated in experiments. \n\nA concern was raised that the paper could be overstating its scope. A point is that the model might not actually give a complete characterization, since the analysis considers permutation action only. The authors have rephrased the claim. Following comments of the reviewer, the authors have also revised the paper to include a discussion of how the model is capable of approximating message passing networks. \n\nTwo referees give the paper a strong support. One referee considers the paper ok, but not good enough. The authors have made convincing efforts to improve issues and address the concerns. \n\n\n\n\n ", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Description of linear permutation invariant and equivariant layers"}, "signatures": ["ICLR.cc/2019/Conference/Paper698/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper698/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invariant and Equivariant Graph Networks", "abstract": "Invariant and equivariant networks have been successfully used for learning images, sets, point clouds, and graphs. A basic challenge in developing such networks is finding the maximal collection of invariant and equivariant \\emph{linear} layers. Although this question is answered for the first three examples (for popular transformations, at-least), a full characterization of invariant and equivariant linear layers for graphs is not known. \n\nIn this paper we provide a characterization of all permutation invariant and equivariant linear layers for (hyper-)graph data, and show that their dimension, in case of edge-value graph data, is $2$ and $15$, respectively. More generally, for graph data defined on $k$-tuples of nodes, the dimension is the $k$-th and $2k$-th Bell numbers. Orthogonal bases for the layers are computed, including generalization to multi-graph data. The constant number of basis elements and their characteristics allow successfully applying the networks to different size graphs. From the theoretical point of view, our results generalize and unify recent advancement in equivariant deep learning. In particular, we show that our model is capable of approximating any message passing neural network.\n\nApplying these new linear layers in a simple deep neural network framework is shown to achieve comparable results to state-of-the-art and to have better expressivity than previous invariant and equivariant bases.\n", "keywords": ["graph learning", "equivariance", "deep learning"], "authorids": ["haggai.maron@weizmann.ac.il", "heli.benhamu@weizmann.ac.il", "nadav13@gmail.com", "yaron.lipman@weizmann.ac.il"], "authors": ["Haggai Maron", "Heli Ben-Hamu", "Nadav Shamir", "Yaron Lipman"], "TL;DR": "The paper provides a full characterization of permutation invariant and equivariant linear layers for graph data.", "pdf": "/pdf/60e3baf3636d94346d28756dc1dc32844179f622.pdf", "paperhash": "maron|invariant_and_equivariant_graph_networks", "_bibtex": "@inproceedings{\nmaron2018invariant,\ntitle={Invariant and Equivariant Graph Networks},\nauthor={Haggai Maron and Heli Ben-Hamu and Nadav Shamir and Yaron Lipman},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx72jC9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper698/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353120998, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syx72jC9tm", "replyto": "Syx72jC9tm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper698/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper698/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper698/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353120998}}}, {"id": "H1lppkObRm", "original": null, "number": 7, "cdate": 1542713284609, "ddate": null, "tcdate": 1542713284609, "tmdate": 1542713284609, "tddate": null, "forum": "Syx72jC9tm", "replyto": "HylTKNLx0m", "invitation": "ICLR.cc/2019/Conference/-/Paper698/Official_Comment", "content": {"title": "Response", "comment": "Thank you for bringing to our attention these two recent works. We uploaded a revision.  These two works construct graph features that seem to be very useful for graph classification but are not directly related to our approach. We have added them to our table and updated the text accordingly. Indeed these methods outperform our method on most, but not all datasets. \nNote that we ran exactly the same 3-layer network on all datasets and still outperform other deep-learning based methods on the social network datasets. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper698/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper698/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper698/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invariant and Equivariant Graph Networks", "abstract": "Invariant and equivariant networks have been successfully used for learning images, sets, point clouds, and graphs. A basic challenge in developing such networks is finding the maximal collection of invariant and equivariant \\emph{linear} layers. Although this question is answered for the first three examples (for popular transformations, at-least), a full characterization of invariant and equivariant linear layers for graphs is not known. \n\nIn this paper we provide a characterization of all permutation invariant and equivariant linear layers for (hyper-)graph data, and show that their dimension, in case of edge-value graph data, is $2$ and $15$, respectively. More generally, for graph data defined on $k$-tuples of nodes, the dimension is the $k$-th and $2k$-th Bell numbers. Orthogonal bases for the layers are computed, including generalization to multi-graph data. The constant number of basis elements and their characteristics allow successfully applying the networks to different size graphs. From the theoretical point of view, our results generalize and unify recent advancement in equivariant deep learning. In particular, we show that our model is capable of approximating any message passing neural network.\n\nApplying these new linear layers in a simple deep neural network framework is shown to achieve comparable results to state-of-the-art and to have better expressivity than previous invariant and equivariant bases.\n", "keywords": ["graph learning", "equivariance", "deep learning"], "authorids": ["haggai.maron@weizmann.ac.il", "heli.benhamu@weizmann.ac.il", "nadav13@gmail.com", "yaron.lipman@weizmann.ac.il"], "authors": ["Haggai Maron", "Heli Ben-Hamu", "Nadav Shamir", "Yaron Lipman"], "TL;DR": "The paper provides a full characterization of permutation invariant and equivariant linear layers for graph data.", "pdf": "/pdf/60e3baf3636d94346d28756dc1dc32844179f622.pdf", "paperhash": "maron|invariant_and_equivariant_graph_networks", "_bibtex": "@inproceedings{\nmaron2018invariant,\ntitle={Invariant and Equivariant Graph Networks},\nauthor={Haggai Maron and Heli Ben-Hamu and Nadav Shamir and Yaron Lipman},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx72jC9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper698/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617243, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syx72jC9tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper698/Authors", "ICLR.cc/2019/Conference/Paper698/Reviewers", "ICLR.cc/2019/Conference/Paper698/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper698/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper698/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper698/Authors|ICLR.cc/2019/Conference/Paper698/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper698/Reviewers", "ICLR.cc/2019/Conference/Paper698/Authors", "ICLR.cc/2019/Conference/Paper698/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617243}}}, {"id": "HylTKNLx0m", "original": null, "number": 1, "cdate": 1542640773293, "ddate": null, "tcdate": 1542640773293, "tmdate": 1542640773293, "tddate": null, "forum": "Syx72jC9tm", "replyto": "Syx72jC9tm", "invitation": "ICLR.cc/2019/Conference/-/Paper698/Public_Comment", "content": {"comment": "While the proposed solution was compared to a few algorithms, some recent state-of-the-art algorithms were omitted in the experiments sections, having a misleading impression on the performance of the author's algorithm. At least the following papers should be included and argued the differences with the author's approach.  \n\n[1] Ivanov et.al, Anonymous Walk Embeddings, ICML 2018\n[2] Verma et.al, Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs, NIPS 2017\n", "title": "Missing related work in experiments."}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invariant and Equivariant Graph Networks", "abstract": "Invariant and equivariant networks have been successfully used for learning images, sets, point clouds, and graphs. A basic challenge in developing such networks is finding the maximal collection of invariant and equivariant \\emph{linear} layers. Although this question is answered for the first three examples (for popular transformations, at-least), a full characterization of invariant and equivariant linear layers for graphs is not known. \n\nIn this paper we provide a characterization of all permutation invariant and equivariant linear layers for (hyper-)graph data, and show that their dimension, in case of edge-value graph data, is $2$ and $15$, respectively. More generally, for graph data defined on $k$-tuples of nodes, the dimension is the $k$-th and $2k$-th Bell numbers. Orthogonal bases for the layers are computed, including generalization to multi-graph data. The constant number of basis elements and their characteristics allow successfully applying the networks to different size graphs. From the theoretical point of view, our results generalize and unify recent advancement in equivariant deep learning. In particular, we show that our model is capable of approximating any message passing neural network.\n\nApplying these new linear layers in a simple deep neural network framework is shown to achieve comparable results to state-of-the-art and to have better expressivity than previous invariant and equivariant bases.\n", "keywords": ["graph learning", "equivariance", "deep learning"], "authorids": ["haggai.maron@weizmann.ac.il", "heli.benhamu@weizmann.ac.il", "nadav13@gmail.com", "yaron.lipman@weizmann.ac.il"], "authors": ["Haggai Maron", "Heli Ben-Hamu", "Nadav Shamir", "Yaron Lipman"], "TL;DR": "The paper provides a full characterization of permutation invariant and equivariant linear layers for graph data.", "pdf": "/pdf/60e3baf3636d94346d28756dc1dc32844179f622.pdf", "paperhash": "maron|invariant_and_equivariant_graph_networks", "_bibtex": "@inproceedings{\nmaron2018invariant,\ntitle={Invariant and Equivariant Graph Networks},\nauthor={Haggai Maron and Heli Ben-Hamu and Nadav Shamir and Yaron Lipman},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx72jC9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper698/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311773477, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "Syx72jC9tm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper698/Authors", "ICLR.cc/2019/Conference/Paper698/Reviewers", "ICLR.cc/2019/Conference/Paper698/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper698/Authors", "ICLR.cc/2019/Conference/Paper698/Reviewers", "ICLR.cc/2019/Conference/Paper698/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311773477}}}, {"id": "ByeAJ28Opm", "original": null, "number": 6, "cdate": 1542118373611, "ddate": null, "tcdate": 1542118373611, "tmdate": 1542118373611, "tddate": null, "forum": "Syx72jC9tm", "replyto": "HJegE7RUTQ", "invitation": "ICLR.cc/2019/Conference/-/Paper698/Official_Comment", "content": {"title": "[Gilmer et al, 2017]", "comment": "Thank you for taking the time to write a response and bringing up Gilmer's message passing formulation. \n\n1) The Gilmer et al. 2017 bar: We have just uploaded a revised manuscript in which we prove that our model can approximate any message passing neural network of the general form introduced in [Gilmer et al. 2017]. \n\n2) \u201cInteraction between sets\u201d networks are not suitable for learning graphs: This claim is not entirely clear to us. The most popular way to represent a graph is by an affinity matrix that describes the interaction between every pair of nodes.  From our point of view, our work establishes a natural connection between \u201cinteraction between sets\u201d networks and graph learning. Note that both approaches utilize the adjacency structure as well as node features, so the geometric structure of the graph is indeed visible and usable by our method.\n\n3) Graph learning = message passing: Although message passing is a prominent graph learning method it is not the only approach to learning graph data. We introduce a method, based on a generalization of \u201cinteraction between sets\u201d that theoretically contains the message passing framework. In any case, we believe seeking new/different methods to learn graphs is a worthy research goal. \n\n4) Full characterization of linear layers: We have updated our contribution statement (in the introduction and abstract) to claim that we give a classification of *permutation* invariant/equivariant layers.\n   \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper698/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper698/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper698/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invariant and Equivariant Graph Networks", "abstract": "Invariant and equivariant networks have been successfully used for learning images, sets, point clouds, and graphs. A basic challenge in developing such networks is finding the maximal collection of invariant and equivariant \\emph{linear} layers. Although this question is answered for the first three examples (for popular transformations, at-least), a full characterization of invariant and equivariant linear layers for graphs is not known. \n\nIn this paper we provide a characterization of all permutation invariant and equivariant linear layers for (hyper-)graph data, and show that their dimension, in case of edge-value graph data, is $2$ and $15$, respectively. More generally, for graph data defined on $k$-tuples of nodes, the dimension is the $k$-th and $2k$-th Bell numbers. Orthogonal bases for the layers are computed, including generalization to multi-graph data. The constant number of basis elements and their characteristics allow successfully applying the networks to different size graphs. From the theoretical point of view, our results generalize and unify recent advancement in equivariant deep learning. In particular, we show that our model is capable of approximating any message passing neural network.\n\nApplying these new linear layers in a simple deep neural network framework is shown to achieve comparable results to state-of-the-art and to have better expressivity than previous invariant and equivariant bases.\n", "keywords": ["graph learning", "equivariance", "deep learning"], "authorids": ["haggai.maron@weizmann.ac.il", "heli.benhamu@weizmann.ac.il", "nadav13@gmail.com", "yaron.lipman@weizmann.ac.il"], "authors": ["Haggai Maron", "Heli Ben-Hamu", "Nadav Shamir", "Yaron Lipman"], "TL;DR": "The paper provides a full characterization of permutation invariant and equivariant linear layers for graph data.", "pdf": "/pdf/60e3baf3636d94346d28756dc1dc32844179f622.pdf", "paperhash": "maron|invariant_and_equivariant_graph_networks", "_bibtex": "@inproceedings{\nmaron2018invariant,\ntitle={Invariant and Equivariant Graph Networks},\nauthor={Haggai Maron and Heli Ben-Hamu and Nadav Shamir and Yaron Lipman},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx72jC9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper698/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617243, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syx72jC9tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper698/Authors", "ICLR.cc/2019/Conference/Paper698/Reviewers", "ICLR.cc/2019/Conference/Paper698/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper698/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper698/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper698/Authors|ICLR.cc/2019/Conference/Paper698/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper698/Reviewers", "ICLR.cc/2019/Conference/Paper698/Authors", "ICLR.cc/2019/Conference/Paper698/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617243}}}, {"id": "r1ehj9U_p7", "original": null, "number": 5, "cdate": 1542118052361, "ddate": null, "tcdate": 1542118052361, "tmdate": 1542118052361, "tddate": null, "forum": "Syx72jC9tm", "replyto": "Syx72jC9tm", "invitation": "ICLR.cc/2019/Conference/-/Paper698/Official_Comment", "content": {"title": "Revision uploaded", "comment": "We thank all the reviewers for their time and effort. We have uploaded a revised manuscript in which we have incorporated the suggestions from the reviews and comments. \n\nWe want to highlight one specific addition to the manuscript (Appendix 3) which is a proof that our model can approximate any message passing neural network that falls in the framework of Gilmer et al. [2017] (i.e., the bar set by Reviewer2 after a fruitful discussion).  \n\nNote that these additions made us overflow by several lines which can be squeezed back if the reviewers require that. "}, "signatures": ["ICLR.cc/2019/Conference/Paper698/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper698/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper698/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invariant and Equivariant Graph Networks", "abstract": "Invariant and equivariant networks have been successfully used for learning images, sets, point clouds, and graphs. A basic challenge in developing such networks is finding the maximal collection of invariant and equivariant \\emph{linear} layers. Although this question is answered for the first three examples (for popular transformations, at-least), a full characterization of invariant and equivariant linear layers for graphs is not known. \n\nIn this paper we provide a characterization of all permutation invariant and equivariant linear layers for (hyper-)graph data, and show that their dimension, in case of edge-value graph data, is $2$ and $15$, respectively. More generally, for graph data defined on $k$-tuples of nodes, the dimension is the $k$-th and $2k$-th Bell numbers. Orthogonal bases for the layers are computed, including generalization to multi-graph data. The constant number of basis elements and their characteristics allow successfully applying the networks to different size graphs. From the theoretical point of view, our results generalize and unify recent advancement in equivariant deep learning. In particular, we show that our model is capable of approximating any message passing neural network.\n\nApplying these new linear layers in a simple deep neural network framework is shown to achieve comparable results to state-of-the-art and to have better expressivity than previous invariant and equivariant bases.\n", "keywords": ["graph learning", "equivariance", "deep learning"], "authorids": ["haggai.maron@weizmann.ac.il", "heli.benhamu@weizmann.ac.il", "nadav13@gmail.com", "yaron.lipman@weizmann.ac.il"], "authors": ["Haggai Maron", "Heli Ben-Hamu", "Nadav Shamir", "Yaron Lipman"], "TL;DR": "The paper provides a full characterization of permutation invariant and equivariant linear layers for graph data.", "pdf": "/pdf/60e3baf3636d94346d28756dc1dc32844179f622.pdf", "paperhash": "maron|invariant_and_equivariant_graph_networks", "_bibtex": "@inproceedings{\nmaron2018invariant,\ntitle={Invariant and Equivariant Graph Networks},\nauthor={Haggai Maron and Heli Ben-Hamu and Nadav Shamir and Yaron Lipman},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx72jC9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper698/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617243, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syx72jC9tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper698/Authors", "ICLR.cc/2019/Conference/Paper698/Reviewers", "ICLR.cc/2019/Conference/Paper698/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper698/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper698/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper698/Authors|ICLR.cc/2019/Conference/Paper698/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper698/Reviewers", "ICLR.cc/2019/Conference/Paper698/Authors", "ICLR.cc/2019/Conference/Paper698/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617243}}}, {"id": "HJegE7RUTQ", "original": null, "number": 4, "cdate": 1542017832112, "ddate": null, "tcdate": 1542017832112, "tmdate": 1542017832112, "tddate": null, "forum": "Syx72jC9tm", "replyto": "S1lQhtby6m", "invitation": "ICLR.cc/2019/Conference/-/Paper698/Official_Comment", "content": {"title": "Response to rebuttal", "comment": "Thanks for your response.\n\nI guess there is a distinction here that is blurred in the paper, but also, to some extent, in the literature. Learning from graphs and learning from subsets of {1,...,n} are not the same thing, even if both problems can be framed in terms of a hypergraph and both problems involve equivariance to the action of S_n. \n\nThe graph of a molecular or a social network is much more than just a subset of V\\times V: it has specific geometric structure and that's exactly what a G-NN is supposed to be able to latch onto. I was critical of the early Laplacian based graph G-NN papers exactly because they only dealt with the spectrum of the Laplacian, which is an easy way out: of course it is invariant to permutations, but it doesn't tell you much about the geometry.\n\nFor this reason, being able to reproduce [Kip & Welling, 2017] doesn't impress me so much. Despite your claim, I don't regard that as a message passing algorithm, in fact, it looks like the word \"message\" doesn't even appear in the paper. [Gilmer et al, 2017] or any of the papers following it would be the bar if we are really talking about message passing.\n\nTo refine my stance a little, I do understand that equivariance of tensors to the permutation action does come up in G-NNs. For example in [Kondor et al] at each vertex they take a higher order tensor and use contractions to reduce it to a number of lower order ones. Your results could be used to count how many different ways this can be done. But this is not exactly what you describe in the paper: the group is not S_n, but only a smaller symmetric group, etc.\n\nIn summary, my main problem with the paper is that it overstates its scope. This paper is not really about graph neural networks, it is more about the \"interactions between sets\" nets. And it doesn't give a full characterization of equivariant layers, only a \"more or less  full characterization\" because the authors only consider the permutation action. The main result is Theorem 1, which is neat, but I wonder if it has the requisite technical depth or element of surprise to warrant a separate paper."}, "signatures": ["ICLR.cc/2019/Conference/Paper698/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper698/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper698/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invariant and Equivariant Graph Networks", "abstract": "Invariant and equivariant networks have been successfully used for learning images, sets, point clouds, and graphs. A basic challenge in developing such networks is finding the maximal collection of invariant and equivariant \\emph{linear} layers. Although this question is answered for the first three examples (for popular transformations, at-least), a full characterization of invariant and equivariant linear layers for graphs is not known. \n\nIn this paper we provide a characterization of all permutation invariant and equivariant linear layers for (hyper-)graph data, and show that their dimension, in case of edge-value graph data, is $2$ and $15$, respectively. More generally, for graph data defined on $k$-tuples of nodes, the dimension is the $k$-th and $2k$-th Bell numbers. Orthogonal bases for the layers are computed, including generalization to multi-graph data. The constant number of basis elements and their characteristics allow successfully applying the networks to different size graphs. From the theoretical point of view, our results generalize and unify recent advancement in equivariant deep learning. In particular, we show that our model is capable of approximating any message passing neural network.\n\nApplying these new linear layers in a simple deep neural network framework is shown to achieve comparable results to state-of-the-art and to have better expressivity than previous invariant and equivariant bases.\n", "keywords": ["graph learning", "equivariance", "deep learning"], "authorids": ["haggai.maron@weizmann.ac.il", "heli.benhamu@weizmann.ac.il", "nadav13@gmail.com", "yaron.lipman@weizmann.ac.il"], "authors": ["Haggai Maron", "Heli Ben-Hamu", "Nadav Shamir", "Yaron Lipman"], "TL;DR": "The paper provides a full characterization of permutation invariant and equivariant linear layers for graph data.", "pdf": "/pdf/60e3baf3636d94346d28756dc1dc32844179f622.pdf", "paperhash": "maron|invariant_and_equivariant_graph_networks", "_bibtex": "@inproceedings{\nmaron2018invariant,\ntitle={Invariant and Equivariant Graph Networks},\nauthor={Haggai Maron and Heli Ben-Hamu and Nadav Shamir and Yaron Lipman},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx72jC9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper698/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617243, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syx72jC9tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper698/Authors", "ICLR.cc/2019/Conference/Paper698/Reviewers", "ICLR.cc/2019/Conference/Paper698/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper698/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper698/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper698/Authors|ICLR.cc/2019/Conference/Paper698/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper698/Reviewers", "ICLR.cc/2019/Conference/Paper698/Authors", "ICLR.cc/2019/Conference/Paper698/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617243}}}, {"id": "HkxIqcKZa7", "original": null, "number": 3, "cdate": 1541671565669, "ddate": null, "tcdate": 1541671565669, "tmdate": 1541671565669, "tddate": null, "forum": "Syx72jC9tm", "replyto": "ByxBaTGy67", "invitation": "ICLR.cc/2019/Conference/-/Paper698/Official_Comment", "content": {"title": "Addressing Reviewer 3 concerns ", "comment": "We thank the reviewer for the positive comments. Below we address the main concerns.\n\nQ: \u201dApplying the model of Hartford et al. to problems where interacting sets are identical is similar to applying convolution layer to a feature vector that is not equivariant to translation... Do you agree that for this reason, all the experiments on the synthetic dataset is flawed?\u201d\n\nA: Our goal in performing the synthetic experiments was to quantify the expressive power that is  gained by adding our basis elements to [Hartford et al. 18]. We felt it is an informative experiment since [Hartford et al. 18] also discuss applying their model in the jointly exchangeable setting (page 3, second column, top paragraph). \nHaving said that, we agree with the reviewer that [Hartford et al. 18] probably cannot handle such tasks by construction. As we mentioned in our response to Reviewer1 we will change the wording of this section to better reflect that this is *not* a failure of Hartford et al. but merely a setting outside their scope due to a different assumption on the symmetry group of the data.  \nIf the reviewers feel strongly about this experiment, we are open to replace it with a discussion.  \n\n--------------------------------------------------------------------------------------------------------------------------------\nQ: \u201cSome of the benchmark datasets for the proposed task as well as some well-known methods (see Battaglia et al\u201918 and references in there) are missing\u201d.\n\nA: We did our best to survey and compare to the most related works on the dataset collection introduced in [Yanardag & Vishwanathan 2015]. These datasets contain graphs from multiple origins, where some of them consist of highly varying graph sizes (within the same dataset). In any case we will make the code available as soon as possible.    \n--------------------------------------------------------------------------------------------------------------------------------\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper698/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper698/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper698/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invariant and Equivariant Graph Networks", "abstract": "Invariant and equivariant networks have been successfully used for learning images, sets, point clouds, and graphs. A basic challenge in developing such networks is finding the maximal collection of invariant and equivariant \\emph{linear} layers. Although this question is answered for the first three examples (for popular transformations, at-least), a full characterization of invariant and equivariant linear layers for graphs is not known. \n\nIn this paper we provide a characterization of all permutation invariant and equivariant linear layers for (hyper-)graph data, and show that their dimension, in case of edge-value graph data, is $2$ and $15$, respectively. More generally, for graph data defined on $k$-tuples of nodes, the dimension is the $k$-th and $2k$-th Bell numbers. Orthogonal bases for the layers are computed, including generalization to multi-graph data. The constant number of basis elements and their characteristics allow successfully applying the networks to different size graphs. From the theoretical point of view, our results generalize and unify recent advancement in equivariant deep learning. In particular, we show that our model is capable of approximating any message passing neural network.\n\nApplying these new linear layers in a simple deep neural network framework is shown to achieve comparable results to state-of-the-art and to have better expressivity than previous invariant and equivariant bases.\n", "keywords": ["graph learning", "equivariance", "deep learning"], "authorids": ["haggai.maron@weizmann.ac.il", "heli.benhamu@weizmann.ac.il", "nadav13@gmail.com", "yaron.lipman@weizmann.ac.il"], "authors": ["Haggai Maron", "Heli Ben-Hamu", "Nadav Shamir", "Yaron Lipman"], "TL;DR": "The paper provides a full characterization of permutation invariant and equivariant linear layers for graph data.", "pdf": "/pdf/60e3baf3636d94346d28756dc1dc32844179f622.pdf", "paperhash": "maron|invariant_and_equivariant_graph_networks", "_bibtex": "@inproceedings{\nmaron2018invariant,\ntitle={Invariant and Equivariant Graph Networks},\nauthor={Haggai Maron and Heli Ben-Hamu and Nadav Shamir and Yaron Lipman},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx72jC9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper698/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617243, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syx72jC9tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper698/Authors", "ICLR.cc/2019/Conference/Paper698/Reviewers", "ICLR.cc/2019/Conference/Paper698/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper698/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper698/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper698/Authors|ICLR.cc/2019/Conference/Paper698/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper698/Reviewers", "ICLR.cc/2019/Conference/Paper698/Authors", "ICLR.cc/2019/Conference/Paper698/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617243}}}, {"id": "ByxBaTGy67", "original": null, "number": 3, "cdate": 1541512636825, "ddate": null, "tcdate": 1541512636825, "tmdate": 1541619197425, "tddate": null, "forum": "Syx72jC9tm", "replyto": "Syx72jC9tm", "invitation": "ICLR.cc/2019/Conference/-/Paper698/Official_Review", "content": {"title": "Beautiful work -- problems with experiments", "review": "The paper presents a maximally expressive parameter-sharing scheme for hypergraphs, and in general when modeling the high order interactions between elements of a set. This setting is further generalized to multiple sets. The paper shows that the number of free parameters in invariant and equivariant layers corresponds to the different partitioning of the index-set of input and output tensors. Experimental results suggest that the proposed layer can outperform existing methods in supervised learning with graphs.\n\nThe paper presents a comprehensive generalization of a recently proposed model for interaction across sets, to the setting where some of these sets are identical. This is particularly useful and important due to its applications to graphs and hyper-graphs, as demonstrated in experiments.\n\nOverall, I enjoyed reading the paper. My only concern is the experiments:\n\n1) Some of the benchmark datasets for the proposed task as well as some well-known methods (see Battaglia et al\u201918 and references in there) are missing.\n\n2) Applying the model of Hartford et al\u201918 to problems where interacting sets are identical is similar to applying convolution layer to a feature vector that is not equivariant to translation. (In both cases the equivariance group of data is a strict subgroup of the equivariance of the layer.)  Do you agree that for this reason, all the experiments on the synthetic dataset is flawed?\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper698/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Invariant and Equivariant Graph Networks", "abstract": "Invariant and equivariant networks have been successfully used for learning images, sets, point clouds, and graphs. A basic challenge in developing such networks is finding the maximal collection of invariant and equivariant \\emph{linear} layers. Although this question is answered for the first three examples (for popular transformations, at-least), a full characterization of invariant and equivariant linear layers for graphs is not known. \n\nIn this paper we provide a characterization of all permutation invariant and equivariant linear layers for (hyper-)graph data, and show that their dimension, in case of edge-value graph data, is $2$ and $15$, respectively. More generally, for graph data defined on $k$-tuples of nodes, the dimension is the $k$-th and $2k$-th Bell numbers. Orthogonal bases for the layers are computed, including generalization to multi-graph data. The constant number of basis elements and their characteristics allow successfully applying the networks to different size graphs. From the theoretical point of view, our results generalize and unify recent advancement in equivariant deep learning. In particular, we show that our model is capable of approximating any message passing neural network.\n\nApplying these new linear layers in a simple deep neural network framework is shown to achieve comparable results to state-of-the-art and to have better expressivity than previous invariant and equivariant bases.\n", "keywords": ["graph learning", "equivariance", "deep learning"], "authorids": ["haggai.maron@weizmann.ac.il", "heli.benhamu@weizmann.ac.il", "nadav13@gmail.com", "yaron.lipman@weizmann.ac.il"], "authors": ["Haggai Maron", "Heli Ben-Hamu", "Nadav Shamir", "Yaron Lipman"], "TL;DR": "The paper provides a full characterization of permutation invariant and equivariant linear layers for graph data.", "pdf": "/pdf/60e3baf3636d94346d28756dc1dc32844179f622.pdf", "paperhash": "maron|invariant_and_equivariant_graph_networks", "_bibtex": "@inproceedings{\nmaron2018invariant,\ntitle={Invariant and Equivariant Graph Networks},\nauthor={Haggai Maron and Heli Ben-Hamu and Nadav Shamir and Yaron Lipman},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx72jC9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper698/Official_Review", "cdate": 1542234400248, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Syx72jC9tm", "replyto": "Syx72jC9tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper698/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335783199, "tmdate": 1552335783199, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper698/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SyeASuDp27", "original": null, "number": 2, "cdate": 1541400645943, "ddate": null, "tcdate": 1541400645943, "tmdate": 1541533764582, "tddate": null, "forum": "Syx72jC9tm", "replyto": "Syx72jC9tm", "invitation": "ICLR.cc/2019/Conference/-/Paper698/Official_Review", "content": {"title": "Nice combinatorics, but this is not what graph neural networks actually do", "review": "Given a graph G of n vertices, the activations at each level of a graph neural network (G-NN) for G \ncan be arranged in an n^k tensor T for some k. A fundamental criterion is that this tensor must be equivariant \nto permutations of the vertices of G in the sense of each index of of T being permuted simultaneously. \n\nThis paper enumerates the set of all linear maps that satisfy this criterion, i.e., all linear maps \nwhich (the authors claim) can serve as the analog of convolution in equivariant G-NNs. \nThe authors find that for invariant neural networks such maps span a space of dimension just b(k), whereas \nfor equivariant neural networks they span a space of dimension b(2k).\n\nThe proof of this result is simple, but elegant. It hinges on the fact that the set of tensor elements of \nthe same equality type is both closed and transitive under the permutation action. Therefore, the \ndimensionality of the subspace in question is just the number of different identity types, i.e., \npartitions of either {1,...,k} or {1,...,2k}, depending on whether we are talking about invariance or \nequivariance.\n\nMy problem with the paper is that the authors' model of G-NNs doesn't actually map to what is used \nin practice or what is interesting and useful. Let me list my reservations in increasing order of significance.\n\n1. The authors claim that they give a ``full characterization'' of equivariant layers. This is not true. \nEquivariance means that there is *some* action of the symmetric group S_n on each layer, and wrt these actions \nthe network is equivariant. Collecting all the activations of a given layer together into a single object L, \nthis means that L is transformed according to some representation of S_n. Such a representation can always be \nreduced into a direct sum of the irreducible representations of S_n. The authors only consider the case then \nthe representation is the k'th power of the permutation representation (technically called the defining \nrepresentation of the S_n). This corresponds to a specific choice of irreducibles and is not the most general case. \nIn fact, this is not an unnatural choice, and all G-NNs that I know follow this route. \nNonetheless, technically, saying that they consider all possible equivariant networks is not correct.\n\n2. The paper does not discuss what happens when the input tensor is symmetric. On the surface this might seem \nlike a strength, since it just means that they can consider the more general case of undirected graphs (although \nthey should really say so). In reality, when considering higher order activations it is very misleading because \nit leads to a massive overcounting of the dimensionality of the space of convolutions. In the case of k=2, for \nexample, the dimensionality for undirected graphs is probably closer to 5 than 15 for example (I didn't count).\n\n3. Finally, and critically, in actual G-NNs, the aggregation operation in each layer is *not* \nlinear, in the sense that it involves a product of the activations of the previous layer with the adjacency \nmatrix (messages might be linear but they are only propagated along the edges of the graph). \nIn most cases this is motivated by making some reference to the geometric meaning of convolution,  \nthe Weisfeiler-Lehman algorithm or message passing in graphical models. In any case, it is critical that the \ngraph topology be reintroduced into the network at each layer. The algebraic way to see it is that each layer \nmust mix the information from the vertices, edges, hyperedges, etc.. The model in this paper could only aggregated \nedge information at the vertices. Vertex information could not be broadcast to neighboring vertices again. \nThe elemenary step of ``collecting vertex information from the neighbors but only the neighbors'' cannot be \nrealized in this model.\n\nTherefore, I feel that the model used in this paper is rather uninteresting and irrelevant for practical \npurposes. If the authors disagree, I would encourage them to explicitly write down how they think the model \ncan replicate one of the standard message passing networks. It is apparent from the 15 operations listed on \npage 11 that they have nothing to do with the graph topology at all.\n\nMinor gripes:\n\n- I wouldn't call (3) and (4) fixed point equations, that's usually used in dynamical systems. Here there is \nan entire subspace fixed by *all* permutations.\n\n- Below (1), they probably mean that ``up to permutation vec(L)=vec(L^T)''. \n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper698/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invariant and Equivariant Graph Networks", "abstract": "Invariant and equivariant networks have been successfully used for learning images, sets, point clouds, and graphs. A basic challenge in developing such networks is finding the maximal collection of invariant and equivariant \\emph{linear} layers. Although this question is answered for the first three examples (for popular transformations, at-least), a full characterization of invariant and equivariant linear layers for graphs is not known. \n\nIn this paper we provide a characterization of all permutation invariant and equivariant linear layers for (hyper-)graph data, and show that their dimension, in case of edge-value graph data, is $2$ and $15$, respectively. More generally, for graph data defined on $k$-tuples of nodes, the dimension is the $k$-th and $2k$-th Bell numbers. Orthogonal bases for the layers are computed, including generalization to multi-graph data. The constant number of basis elements and their characteristics allow successfully applying the networks to different size graphs. From the theoretical point of view, our results generalize and unify recent advancement in equivariant deep learning. In particular, we show that our model is capable of approximating any message passing neural network.\n\nApplying these new linear layers in a simple deep neural network framework is shown to achieve comparable results to state-of-the-art and to have better expressivity than previous invariant and equivariant bases.\n", "keywords": ["graph learning", "equivariance", "deep learning"], "authorids": ["haggai.maron@weizmann.ac.il", "heli.benhamu@weizmann.ac.il", "nadav13@gmail.com", "yaron.lipman@weizmann.ac.il"], "authors": ["Haggai Maron", "Heli Ben-Hamu", "Nadav Shamir", "Yaron Lipman"], "TL;DR": "The paper provides a full characterization of permutation invariant and equivariant linear layers for graph data.", "pdf": "/pdf/60e3baf3636d94346d28756dc1dc32844179f622.pdf", "paperhash": "maron|invariant_and_equivariant_graph_networks", "_bibtex": "@inproceedings{\nmaron2018invariant,\ntitle={Invariant and Equivariant Graph Networks},\nauthor={Haggai Maron and Heli Ben-Hamu and Nadav Shamir and Yaron Lipman},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx72jC9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper698/Official_Review", "cdate": 1542234400248, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Syx72jC9tm", "replyto": "Syx72jC9tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper698/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335783199, "tmdate": 1552335783199, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper698/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJgbJ0v52m", "original": null, "number": 1, "cdate": 1541205465141, "ddate": null, "tcdate": 1541205465141, "tmdate": 1541533764241, "tddate": null, "forum": "Syx72jC9tm", "replyto": "Syx72jC9tm", "invitation": "ICLR.cc/2019/Conference/-/Paper698/Official_Review", "content": {"title": "Very interesting paper", "review": "This paper explores maximally expressive linear layers for jointly exchangeable data and in doing so presents a surprisingly expressive model. I have given it a strong accept because the paper takes a very well-studied area (convolutions on graphs) and manages to find a far more expressive model (in terms of numbers of parameters) than what was previously known by carefully exploring the implications of the equivariance assumptions implied by graph data. The result is particularly interesting because the same question was asked about exchangeable matrices (instead of *jointly* exchangeable matrices) by Hartford et al. [2018] which lead to a model with 4 bases instead of the 15 bases in this model, so the additional assumption of joint exchangeability (i.e. that any permutations applied to rows of a matrix must also be applied to columns - or equivalently, the indices of the rows and columns of a matrix refer to the same items / nodes) gives far more flexibility but without losing anything with respect to the Hartford et al result (because it can be recovered using a bipartite graph construction - described below). So we have a case where an additional assumption is both useful (in that it allows for the definition of a more flexible model) and benign (because it doesn't prevent the layer from being used on the data explored in Hartford et al.). \n\nI only have a couple of concerns: \n1 - I would have liked to see more discussion about why the two results differ to give readers intuition about where the extra flexibility comes from. The additional parameters of this paper come from having parameters associated with the diagonal (intuitively: self edges get treated differently to other edges) and having parameters for the transpose of the matrix (intuitively: incoming edges are different to outgoing edges). Neither of these assumptions apply in the exchangeable setting (where the matrix may not be square so the diagonal and transpose can't be used). Because these differences aren't explained, the synthetic tasks in the experimental section make this approach look artificially good in comparison to Hartford et al.  The tasks are explicitly designed to exploit these additional parameters - so framing the synthetic experiments as, \"here are some simple functions for which we would need the additional parameters that we define\" makes sense; but arguing that Hartford et al. \"fail approximating rather simple functions\" (page 7) is misleading because the functions are precisely the functions on which you would expect Hartford et al. to fail (because it's designed for a different setting). \n2 - Those more familiar of the graph convolution literature will be more familiar with GCN [kipf et al. 2016] / GraphSAGE [Hamilton et al. 2017] / Monti et al [2017] / etc.. Most of these approaches are more restricted version of this work / Hartford et al. so we wouldn't expect them to perform any differently from the Hartford et al.  baseline on the synthetic dataset, but including them will strengthen the author's argument in favour of the work. I would have also liked to see a comparison to these methods in the the classification results.\n3 - Appendix A - the 6 parameters for the symmetric case with zero diagonal reduces to the same 4 parameters from Hartford et al. if we constrained the diagonal to be zero in the output as well as the input. This is the case when you map an exchangeable matrix into a jointly exchangeable matrix by representing it as a bipartite graph [0, X; X^T, 0]. So the two results coincide for the exchangeable case. Might be worth pointing this out. \n", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper698/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invariant and Equivariant Graph Networks", "abstract": "Invariant and equivariant networks have been successfully used for learning images, sets, point clouds, and graphs. A basic challenge in developing such networks is finding the maximal collection of invariant and equivariant \\emph{linear} layers. Although this question is answered for the first three examples (for popular transformations, at-least), a full characterization of invariant and equivariant linear layers for graphs is not known. \n\nIn this paper we provide a characterization of all permutation invariant and equivariant linear layers for (hyper-)graph data, and show that their dimension, in case of edge-value graph data, is $2$ and $15$, respectively. More generally, for graph data defined on $k$-tuples of nodes, the dimension is the $k$-th and $2k$-th Bell numbers. Orthogonal bases for the layers are computed, including generalization to multi-graph data. The constant number of basis elements and their characteristics allow successfully applying the networks to different size graphs. From the theoretical point of view, our results generalize and unify recent advancement in equivariant deep learning. In particular, we show that our model is capable of approximating any message passing neural network.\n\nApplying these new linear layers in a simple deep neural network framework is shown to achieve comparable results to state-of-the-art and to have better expressivity than previous invariant and equivariant bases.\n", "keywords": ["graph learning", "equivariance", "deep learning"], "authorids": ["haggai.maron@weizmann.ac.il", "heli.benhamu@weizmann.ac.il", "nadav13@gmail.com", "yaron.lipman@weizmann.ac.il"], "authors": ["Haggai Maron", "Heli Ben-Hamu", "Nadav Shamir", "Yaron Lipman"], "TL;DR": "The paper provides a full characterization of permutation invariant and equivariant linear layers for graph data.", "pdf": "/pdf/60e3baf3636d94346d28756dc1dc32844179f622.pdf", "paperhash": "maron|invariant_and_equivariant_graph_networks", "_bibtex": "@inproceedings{\nmaron2018invariant,\ntitle={Invariant and Equivariant Graph Networks},\nauthor={Haggai Maron and Heli Ben-Hamu and Nadav Shamir and Yaron Lipman},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx72jC9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper698/Official_Review", "cdate": 1542234400248, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Syx72jC9tm", "replyto": "Syx72jC9tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper698/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335783199, "tmdate": 1552335783199, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper698/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SkgCXcb16X", "original": null, "number": 2, "cdate": 1541507622231, "ddate": null, "tcdate": 1541507622231, "tmdate": 1541507622231, "tddate": null, "forum": "Syx72jC9tm", "replyto": "rJgbJ0v52m", "invitation": "ICLR.cc/2019/Conference/-/Paper698/Official_Comment", "content": {"title": "Addressing Reviewer 1 concerns", "comment": "We thank the reviewer for the detailed review. Below we address the main concerns.\n\n--------------------------------------------------------------------------------------------------------------------------------\nQ:\u201dso framing the synthetic experiments as, \"here are some simple functions for which we would need the additional parameters that we define\" makes sense; but arguing that Hartford et al. \"fail approximating rather simple functions\" (page 7) is misleading because the functions are precisely the functions on which you would expect Hartford et al. to fail\u201d\n\nA: We agree with the reviewer and will change our wording accordingly.\n--------------------------------------------------------------------------------------------------------------------------------\nQ:\u201dI would have liked to see more discussion about why the two results differ to give readers intuition about where the extra flexibility comes from\u201d.  \u201cthe two results coincide for the exchangeable case\u201d\n\nA: We agree with the reviewer that such a discussion will be helpful to the reader. We will add such a discussion (in addition to the short discussion at the end of Appendix 1).\n--------------------------------------------------------------------------------------------------------------------------------\nQ: Comparison to popular graph convolution methods (GCN [kipf et al. 2016] / GraphSAGE [Hamilton et al. 2017] / Monti et al [2017] / etc.).\n\nA: As discussed in our response to Reviewer 2, We will add a theoretical result that shows that our model is at least as powerful in terms of universality as [Kipf & Welling ICLR 2017]. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper698/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper698/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper698/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invariant and Equivariant Graph Networks", "abstract": "Invariant and equivariant networks have been successfully used for learning images, sets, point clouds, and graphs. A basic challenge in developing such networks is finding the maximal collection of invariant and equivariant \\emph{linear} layers. Although this question is answered for the first three examples (for popular transformations, at-least), a full characterization of invariant and equivariant linear layers for graphs is not known. \n\nIn this paper we provide a characterization of all permutation invariant and equivariant linear layers for (hyper-)graph data, and show that their dimension, in case of edge-value graph data, is $2$ and $15$, respectively. More generally, for graph data defined on $k$-tuples of nodes, the dimension is the $k$-th and $2k$-th Bell numbers. Orthogonal bases for the layers are computed, including generalization to multi-graph data. The constant number of basis elements and their characteristics allow successfully applying the networks to different size graphs. From the theoretical point of view, our results generalize and unify recent advancement in equivariant deep learning. In particular, we show that our model is capable of approximating any message passing neural network.\n\nApplying these new linear layers in a simple deep neural network framework is shown to achieve comparable results to state-of-the-art and to have better expressivity than previous invariant and equivariant bases.\n", "keywords": ["graph learning", "equivariance", "deep learning"], "authorids": ["haggai.maron@weizmann.ac.il", "heli.benhamu@weizmann.ac.il", "nadav13@gmail.com", "yaron.lipman@weizmann.ac.il"], "authors": ["Haggai Maron", "Heli Ben-Hamu", "Nadav Shamir", "Yaron Lipman"], "TL;DR": "The paper provides a full characterization of permutation invariant and equivariant linear layers for graph data.", "pdf": "/pdf/60e3baf3636d94346d28756dc1dc32844179f622.pdf", "paperhash": "maron|invariant_and_equivariant_graph_networks", "_bibtex": "@inproceedings{\nmaron2018invariant,\ntitle={Invariant and Equivariant Graph Networks},\nauthor={Haggai Maron and Heli Ben-Hamu and Nadav Shamir and Yaron Lipman},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx72jC9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper698/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617243, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syx72jC9tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper698/Authors", "ICLR.cc/2019/Conference/Paper698/Reviewers", "ICLR.cc/2019/Conference/Paper698/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper698/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper698/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper698/Authors|ICLR.cc/2019/Conference/Paper698/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper698/Reviewers", "ICLR.cc/2019/Conference/Paper698/Authors", "ICLR.cc/2019/Conference/Paper698/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617243}}}, {"id": "S1lQhtby6m", "original": null, "number": 1, "cdate": 1541507499465, "ddate": null, "tcdate": 1541507499465, "tmdate": 1541507499465, "tddate": null, "forum": "Syx72jC9tm", "replyto": "SyeASuDp27", "invitation": "ICLR.cc/2019/Conference/-/Paper698/Official_Comment", "content": {"title": "Message passing using our method...", "comment": "We thank the reviewer for the detailed review. The reviewer's main concern was the practicality of the method, and its inability to model message passing. We respectfully disagree. Below we show that our model can simulate standard message passing architectures in a simple way, as well as answer other concerns.\n\n--------------------------------------------------------------------------------------------------------------------------------\nQ: \u201cFinally, and critically, in actual G-NNs, the aggregation operation in each layer is *not* \nLinear\u2026 I feel that the model used in this paper is rather uninteresting and irrelevant for practical purposes\u201d\n\nA: We would like to address this from a few points of view.\nMessage passing usually deals with vertex data and adjacency matrix. Our goal was to work with data such as general affinity matrices and higher order tensors. For example, pairwise distance matrix (order 2 tensor), or area/congruence of triplets (order 3 tensor) which are useful representation for geometric data, for instance.\nAlso note that linear equivariant models like ours are already used in successful methods like PointNet [Qi et al. CVPR 2017], DeepSets [Zaheer et al. NIPS 2017], and [Hartford el al. ICML 2018]. \nHaving said that, our method is at-least as powerful in terms of universality as standard message passing (see next question).  We also note that our empirical results support the above mentioned theoretical results.\n\n--------------------------------------------------------------------------------------------------------------------------------\nQ: \u201cI would encourage them to explicitly write down how they think the model \ncan replicate one of the standard message passing networks\u201d.\n\nA: Thank you for raising this question. Let us show: \n\nProposition: \nOur model can represent Kipf & Welling\u2019s message passing [Kipf & Welling ICLR 2017] to arbitrary precision. \n\nProof: \nConsider input vertex data X\\in R^{n x d} (n is the number of vertices in the graph, and d is the feature depth) and adjacency/affinity matrix A\\in R^{n x n}  of the graph. In our setting we represent this data using a tensor Y\\in R^{n x n x d+1} where the first channel is the adjacency matrix A and the last d channels are diagonal matrices that hold X. We would like to approximate the function Y \\mapsto A*X. For simplicity we consider d=1 but the following generalizes readily to all d>1. A*X can be represented by first using our equivariant linear layer to replicate X values on the rows; denote this new matrix by Z \\in R^{n x n x 2}, where the first channel of Z is A and the second is the replication of X. Now multiplying entrywise the two feature channels of Z followed by summing the rows (another equivariant 2->1 operator) will provide A*X.  Since pointwise product between features is not a part of our model we can approximate it to arbitrary precision using an MLP on the feature dimension that can be written as a series of linear equivariant operators and ReLUs. (Note that MLP on the feature dimension is the way PointNet and DeepSets work.) QED\n\nWe will add this claim and proof to the paper. \n\nOne immediate corollary of this proposition is that in terms of universality our model is at-least as powerful as Kipf & Welling message passing. \n\n--------------------------------------------------------------------------------------------------------------------------------\nQ: \u201cThe authors claim that they give a ``full characterization'' of equivariant layers\u201d:  \n\nA: We give a full characterization for equivariant maps for the natural action of the S_n on the graph tensors of all orders: consistent re-labeling of the graph nodes. The reviewer is correct in pointing out that not all irreducible representations are considered. We will be happy to rephrase our claim.\n\n--------------------------------------------------------------------------------------------------------------------------------\nQ: \u201dThe paper does not discuss what happens when the input tensor is symmetric.\u201d\n\nA: This question was addressed in Appendix 1. We add a relevant quote: \n\u201cWe note that in case the input matrix is symmetric, our basis reduces to 11 elements in the first layer. If we further assume the matrix has zero diagonal we get a 6 element basis in the first layer. In both cases our model is more expressive than the 4 element basis of Hartford et al. (2018) and as the output of the first layer (or other inner states) need not be symmetric nor have zero diagonal the deeper layers can potentially make good use of the full 15 element basis.\u201d\n--------------------------------------------------------------------------------------------------------------------------------\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper698/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper698/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper698/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invariant and Equivariant Graph Networks", "abstract": "Invariant and equivariant networks have been successfully used for learning images, sets, point clouds, and graphs. A basic challenge in developing such networks is finding the maximal collection of invariant and equivariant \\emph{linear} layers. Although this question is answered for the first three examples (for popular transformations, at-least), a full characterization of invariant and equivariant linear layers for graphs is not known. \n\nIn this paper we provide a characterization of all permutation invariant and equivariant linear layers for (hyper-)graph data, and show that their dimension, in case of edge-value graph data, is $2$ and $15$, respectively. More generally, for graph data defined on $k$-tuples of nodes, the dimension is the $k$-th and $2k$-th Bell numbers. Orthogonal bases for the layers are computed, including generalization to multi-graph data. The constant number of basis elements and their characteristics allow successfully applying the networks to different size graphs. From the theoretical point of view, our results generalize and unify recent advancement in equivariant deep learning. In particular, we show that our model is capable of approximating any message passing neural network.\n\nApplying these new linear layers in a simple deep neural network framework is shown to achieve comparable results to state-of-the-art and to have better expressivity than previous invariant and equivariant bases.\n", "keywords": ["graph learning", "equivariance", "deep learning"], "authorids": ["haggai.maron@weizmann.ac.il", "heli.benhamu@weizmann.ac.il", "nadav13@gmail.com", "yaron.lipman@weizmann.ac.il"], "authors": ["Haggai Maron", "Heli Ben-Hamu", "Nadav Shamir", "Yaron Lipman"], "TL;DR": "The paper provides a full characterization of permutation invariant and equivariant linear layers for graph data.", "pdf": "/pdf/60e3baf3636d94346d28756dc1dc32844179f622.pdf", "paperhash": "maron|invariant_and_equivariant_graph_networks", "_bibtex": "@inproceedings{\nmaron2018invariant,\ntitle={Invariant and Equivariant Graph Networks},\nauthor={Haggai Maron and Heli Ben-Hamu and Nadav Shamir and Yaron Lipman},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx72jC9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper698/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617243, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syx72jC9tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper698/Authors", "ICLR.cc/2019/Conference/Paper698/Reviewers", "ICLR.cc/2019/Conference/Paper698/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper698/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper698/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper698/Authors|ICLR.cc/2019/Conference/Paper698/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper698/Reviewers", "ICLR.cc/2019/Conference/Paper698/Authors", "ICLR.cc/2019/Conference/Paper698/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617243}}}], "count": 13}