{"notes": [{"id": "rkgz2aEKDr", "original": "Bkg5v0J_vH", "number": 774, "cdate": 1569439146376, "ddate": null, "tcdate": 1569439146376, "tmdate": 1583912035368, "tddate": null, "forum": "rkgz2aEKDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "On the Variance of the Adaptive Learning Rate and Beyond", "authors": ["Liyuan Liu", "Haoming Jiang", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Jiawei Han"], "authorids": ["ll2@illinois.edu", "jianghm@gatech.edu", "penhe@microsoft.com", "wzchen@microsoft.com", "xiaodl@microsoft.com", "jfgao@microsoft.com", "hanj@illinois.edu"], "keywords": ["warmup", "adam", "adaptive learning rate", "variance"], "TL;DR": "If warmup is the answer, what is the question?", "abstract": "The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate -- its variance is problematically large in the early stage, and presume warmup works as a variance reduction technique. We provide both empirical and theoretical evidence to verify our hypothesis. We further propose Rectified Adam (RAdam), a novel variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the efficacy and robustness of RAdam. ", "code": "https://github.com/LiyuanLucasLiu/RAdam", "pdf": "/pdf/4dbb68ed4eb409f773d227c4c157de6b31b3080b.pdf", "paperhash": "liu|on_the_variance_of_the_adaptive_learning_rate_and_beyond", "_bibtex": "@inproceedings{\nLiu2020On,\ntitle={On the Variance of the Adaptive Learning Rate and Beyond},\nauthor={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgz2aEKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/aa7745b392e15796b09d3b80b5adeebe4212a906.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "yNHIuV5RC", "original": null, "number": 1, "cdate": 1576798705699, "ddate": null, "tcdate": 1576798705699, "tmdate": 1576800930439, "tddate": null, "forum": "rkgz2aEKDr", "replyto": "rkgz2aEKDr", "invitation": "ICLR.cc/2020/Conference/Paper774/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The paper considers an important topic of the warmup in deep learning, and investigates the problem of the adaptive learning rate. While the paper is somewhat borderline, the reviewers agree that it might be useful to present it to the  ICLR community.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Variance of the Adaptive Learning Rate and Beyond", "authors": ["Liyuan Liu", "Haoming Jiang", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Jiawei Han"], "authorids": ["ll2@illinois.edu", "jianghm@gatech.edu", "penhe@microsoft.com", "wzchen@microsoft.com", "xiaodl@microsoft.com", "jfgao@microsoft.com", "hanj@illinois.edu"], "keywords": ["warmup", "adam", "adaptive learning rate", "variance"], "TL;DR": "If warmup is the answer, what is the question?", "abstract": "The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate -- its variance is problematically large in the early stage, and presume warmup works as a variance reduction technique. We provide both empirical and theoretical evidence to verify our hypothesis. We further propose Rectified Adam (RAdam), a novel variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the efficacy and robustness of RAdam. ", "code": "https://github.com/LiyuanLucasLiu/RAdam", "pdf": "/pdf/4dbb68ed4eb409f773d227c4c157de6b31b3080b.pdf", "paperhash": "liu|on_the_variance_of_the_adaptive_learning_rate_and_beyond", "_bibtex": "@inproceedings{\nLiu2020On,\ntitle={On the Variance of the Adaptive Learning Rate and Beyond},\nauthor={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgz2aEKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/aa7745b392e15796b09d3b80b5adeebe4212a906.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rkgz2aEKDr", "replyto": "rkgz2aEKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795722771, "tmdate": 1576800274139, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper774/-/Decision"}}}, {"id": "HkxT4aeSor", "original": null, "number": 8, "cdate": 1573354805338, "ddate": null, "tcdate": 1573354805338, "tmdate": 1573574461616, "tddate": null, "forum": "rkgz2aEKDr", "replyto": "Hye4w1Tyjr", "invitation": "ICLR.cc/2020/Conference/Paper774/-/Official_Comment", "content": {"title": "Official Response (2/2)", "comment": "> **Math Derivations**\n\nOne purpose of our study is to rigorously explore the underpinning of warmup \u2014 we believe that only functioning as the outcome of math derivations, our method can explicitly handle the variance issue. However, the comment criticizes math derivations as insufficient to justify our algorithm design, referring to them as \u201cmagic masquerading\u201d and \u201csmoke and mirrors\u201d.\n\nWe respectfully disagree with this statement, as math is an important and powerful tool to formulate and verify our understanding. We believe in order to perform a rigorous analysis, math derivations are necessary and helpful.\n\nIn our study, we conduct theoretical analysis besides controlled experiments and find that math agrees with our hypothesis (it is the large variance caused the training instability). Specifically, our theory shows that, at the beginning of training, the variance of the adaptive learning rate can be undesirably large, verifying the existence of the variance issue. Additionally, this inspires us to introduce a mathematically sound rectification term to handle the variance. \n\n> **Downgrading to SGDM**\n\nA large portion of the comment is about the issue of the RAdam downgrading (i.e.,\"4 timesteps of momentum SGD\"), a byproduct determined by math derivations. Although this is not the main result of our paper, we'd like to clarify why it is designed this way below.\n\nHere, we find the troublesome large variance of the adaptive learning rate can cause training instability (thus we focus on the magnitude instead of the divergence). From our theoretical analysis of the adaptive learning rate variance, we derive the rectification term to handle this issue. However, we are constrained from applying such a rectification term at the very beginning of training, since the variance of the estimated adaptive learning rate is not well-defined (in other words, divergent). Therefore, we downgrade the algorithm to SGDM in this stage. \n\nAlthough this stage only contains several gradient updates, these updates could be quite damaging (e.g., in our Figure 2, the gradient distribution is distorted within 10 gradient updates). Intuitively, updates with divergent adaptive learning rate variance could be more damaging than the ones with converged variance, as divergent variance implies more instability. As a case study, we performed experiments on the CIFAR10 dataset. Five-run average results are summarized below. The optimizer fails to get an equally reliably model when changing the first 4 updates to Adam, yet the influence of switching is less deleterious when we change 5-8 updates instead. This result verifies our intuition and is in agreement with our theory \u2014 the first few updates could be more damaging than later updates. By saying that, we still want to emphasize that this part (downgrading to SGDM) is only a minor part of our algorithm design whereas our main focus is on the mechanism of warmup and the derivation of the rectification term.\n\n+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n|                                                                            Performance on CIFAR10 (lr = 0.1)                                                              |\n+-----------------------------------------------------------------------------------------------------+-------------+---------------+------------------+\n|                                                                                                                                    |   test acc |  train loss |   train error |\n+-----------------------------------------------------------------------------------------------------+-------------+---------------+------------------+\n| All steps: RAdam                                                                                                     |       91.08 |         0.021 |             0.74  |\n+-----------------------------------------------------------------------------------------------------+-------------+---------------+------------------+\n| 1-4 steps (divergent variance): Adam; 5+ steps: RAdam                                 |      89.98 |          0.060 |             2.12  |\n+-----------------------------------------------------------------------------------------------------+-------------+---------------+------------------+\n| 1-4 steps: SGD; 5-8 steps (convergent variance): Adam; 8+ steps: Radam  |      90.29 |         0.038 |              1.34  |\n+-----------------------------------------------------------------------------------------------------+-------------+---------------+------------------+\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper774/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Variance of the Adaptive Learning Rate and Beyond", "authors": ["Liyuan Liu", "Haoming Jiang", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Jiawei Han"], "authorids": ["ll2@illinois.edu", "jianghm@gatech.edu", "penhe@microsoft.com", "wzchen@microsoft.com", "xiaodl@microsoft.com", "jfgao@microsoft.com", "hanj@illinois.edu"], "keywords": ["warmup", "adam", "adaptive learning rate", "variance"], "TL;DR": "If warmup is the answer, what is the question?", "abstract": "The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate -- its variance is problematically large in the early stage, and presume warmup works as a variance reduction technique. We provide both empirical and theoretical evidence to verify our hypothesis. We further propose Rectified Adam (RAdam), a novel variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the efficacy and robustness of RAdam. ", "code": "https://github.com/LiyuanLucasLiu/RAdam", "pdf": "/pdf/4dbb68ed4eb409f773d227c4c157de6b31b3080b.pdf", "paperhash": "liu|on_the_variance_of_the_adaptive_learning_rate_and_beyond", "_bibtex": "@inproceedings{\nLiu2020On,\ntitle={On the Variance of the Adaptive Learning Rate and Beyond},\nauthor={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgz2aEKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/aa7745b392e15796b09d3b80b5adeebe4212a906.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgz2aEKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference/Paper774/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper774/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper774/Reviewers", "ICLR.cc/2020/Conference/Paper774/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper774/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper774/Authors|ICLR.cc/2020/Conference/Paper774/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166425, "tmdate": 1576860549570, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference/Paper774/Reviewers", "ICLR.cc/2020/Conference/Paper774/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper774/-/Official_Comment"}}}, {"id": "Hyl4O6eHiH", "original": null, "number": 9, "cdate": 1573354860469, "ddate": null, "tcdate": 1573354860469, "tmdate": 1573354860469, "tddate": null, "forum": "rkgz2aEKDr", "replyto": "Hye4w1Tyjr", "invitation": "ICLR.cc/2020/Conference/Paper774/-/Official_Comment", "content": {"title": "Official Response (1/2)", "comment": "The comment summarizes our work as \u201cincremental and poorly-justified\u201d, mainly based on the similarity between RAdam and two warmup schedulers; however, we find that such similarity ends up supporting our intuition \u2014 the needs of warmup comes from the problematic adaptive learning rate, since these two schedulers are entirely controlled by $\\beta_2$ (the only hyper-parameter for calculating the adaptive learning rate). \n\nBefore we address the comment in detail, we want to point out that the main focus of our study is on exploring the underlying mechanism of warmup. Our analysis reveals that the training instability (at least in our NMT case) is mainly caused by adaptive learning rate, and warmup is needed to handle the large variance of the adaptive learning rate at the early stage of training. Inspired by the analysis, we propose a new variant of Adam by introducing a rectification term to explicitly control the variance of the adaptive learning rate.\n\n> **Comparing with Heuristic LR Scheduler**\n\nIn their manuscript, it says \u201cRAdam and the untuned rule-of-thumb warmup schedules are more or less interchangeable\u201d, thus \u201cRAdam: perform 4 iterations of momentum SGD, then use Adam with fixed warmup\u201d.\n\nHowever, since the fixed warmup schedulers (in the above argument) are controlled solely by the choice of $\\beta_2$, only the adaptive learning rate is taken into consideration. Therefore, their designs are based on an intuition similar to ours \u2014 in the first stage of training, due to the use of insufficient samples for estimating the adaptive learning rate, a warmup stage is needed (and should be customized via the choice of \\beta_2). Also, as shown in Figure 3 of the shared manuscript, the compared lr scheduler is very similar to our current design; it is not a surprise to find that their performances are also similar. \n\nTo put it differently, we think that this phenomenon supports our intuition on the relationship between training instability and the lack of enough samples for adaptive learning rate estimation. \n\n> **Adaptive Learning Rate Variance**\n\nIn the comment, it says \u201cThe manuscript does not show a causal relationship between the variance of the adaptive learning rate and training instability.\u201d \n\nVariance is a measure of variability, i.e., lack of consistency or fixed pattern. Therefore, we pick variance as the measure to study stability [1]. Intuitively, if the adaptive learning rate is not stable, it will have a large variance.\n\nTo seek the origin of training instability, we designed two controlled experiments (Adam-2k and Adam-eps; see Sec 3.1). Specifically, Adam-2k narrows down the problem to the adaptive learning rate, and Adam-eps shows the convergent problem can be avoided by stabilizing the adaptive learning rate (large eps can be viewed as a small-gradient filter). Accordingly, these experimental results suggest that the adaptive learning rate\u2019s instability (large variance) is a major, if not the only, contributor to training instability. \n\n[1] Beard R.E., Pentik\u00e4inen T., Pesonen E. (1984) Variance as a measure of stability. In: Risk Theory. Monographs on Statistics and Applied Probability, vol 20. Springer, Dordrecht\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper774/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Variance of the Adaptive Learning Rate and Beyond", "authors": ["Liyuan Liu", "Haoming Jiang", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Jiawei Han"], "authorids": ["ll2@illinois.edu", "jianghm@gatech.edu", "penhe@microsoft.com", "wzchen@microsoft.com", "xiaodl@microsoft.com", "jfgao@microsoft.com", "hanj@illinois.edu"], "keywords": ["warmup", "adam", "adaptive learning rate", "variance"], "TL;DR": "If warmup is the answer, what is the question?", "abstract": "The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate -- its variance is problematically large in the early stage, and presume warmup works as a variance reduction technique. We provide both empirical and theoretical evidence to verify our hypothesis. We further propose Rectified Adam (RAdam), a novel variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the efficacy and robustness of RAdam. ", "code": "https://github.com/LiyuanLucasLiu/RAdam", "pdf": "/pdf/4dbb68ed4eb409f773d227c4c157de6b31b3080b.pdf", "paperhash": "liu|on_the_variance_of_the_adaptive_learning_rate_and_beyond", "_bibtex": "@inproceedings{\nLiu2020On,\ntitle={On the Variance of the Adaptive Learning Rate and Beyond},\nauthor={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgz2aEKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/aa7745b392e15796b09d3b80b5adeebe4212a906.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgz2aEKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference/Paper774/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper774/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper774/Reviewers", "ICLR.cc/2020/Conference/Paper774/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper774/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper774/Authors|ICLR.cc/2020/Conference/Paper774/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166425, "tmdate": 1576860549570, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference/Paper774/Reviewers", "ICLR.cc/2020/Conference/Paper774/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper774/-/Official_Comment"}}}, {"id": "Skl7-oeHiB", "original": null, "number": 5, "cdate": 1573354235211, "ddate": null, "tcdate": 1573354235211, "tmdate": 1573354496489, "tddate": null, "forum": "rkgz2aEKDr", "replyto": "HJxzMg4AYH", "invitation": "ICLR.cc/2020/Conference/Paper774/-/Official_Comment", "content": {"title": "Re: Official Blind Review #3", "comment": "Thank you for your review and feedback. \n\nAlthough the warmup technology has been demonstrated to be useful in several applications and domains, it is not regarded as a common practice, partially because it is unclear why we need such technologies. In this study, we aim to uncover its underpinnings and identify an important yet long-overlooked issue: the adaptive learning rate has a problematically large variance in the early stage of training, due to the lack of enough samples. Based on our analysis, we further propose a rectification term to address this issue. In our experiments, we show that it works well on various tasks/domains. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper774/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Variance of the Adaptive Learning Rate and Beyond", "authors": ["Liyuan Liu", "Haoming Jiang", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Jiawei Han"], "authorids": ["ll2@illinois.edu", "jianghm@gatech.edu", "penhe@microsoft.com", "wzchen@microsoft.com", "xiaodl@microsoft.com", "jfgao@microsoft.com", "hanj@illinois.edu"], "keywords": ["warmup", "adam", "adaptive learning rate", "variance"], "TL;DR": "If warmup is the answer, what is the question?", "abstract": "The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate -- its variance is problematically large in the early stage, and presume warmup works as a variance reduction technique. We provide both empirical and theoretical evidence to verify our hypothesis. We further propose Rectified Adam (RAdam), a novel variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the efficacy and robustness of RAdam. ", "code": "https://github.com/LiyuanLucasLiu/RAdam", "pdf": "/pdf/4dbb68ed4eb409f773d227c4c157de6b31b3080b.pdf", "paperhash": "liu|on_the_variance_of_the_adaptive_learning_rate_and_beyond", "_bibtex": "@inproceedings{\nLiu2020On,\ntitle={On the Variance of the Adaptive Learning Rate and Beyond},\nauthor={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgz2aEKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/aa7745b392e15796b09d3b80b5adeebe4212a906.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgz2aEKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference/Paper774/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper774/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper774/Reviewers", "ICLR.cc/2020/Conference/Paper774/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper774/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper774/Authors|ICLR.cc/2020/Conference/Paper774/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166425, "tmdate": 1576860549570, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference/Paper774/Reviewers", "ICLR.cc/2020/Conference/Paper774/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper774/-/Official_Comment"}}}, {"id": "SyenwjxriB", "original": null, "number": 6, "cdate": 1573354339768, "ddate": null, "tcdate": 1573354339768, "tmdate": 1573354485419, "tddate": null, "forum": "rkgz2aEKDr", "replyto": "SyeHRMd1cS", "invitation": "ICLR.cc/2020/Conference/Paper774/-/Official_Comment", "content": {"title": "Re: Official Blind Review #1", "comment": "Thank you for your review and feedback. We understand your concern and hope that our response will alleviate it. More specifically,\n\n> On the comparison between SGDM and adaptive optimization algorithms\n\nIt is true that for datasets like ImageNet, the state of the art resnet performance is usually achieved by SGD with momentum (SGDM). In our case, we observe a similar phenomenon, and we suggest it because the hyper-parameters are tuned for the SGDM and may be sub-optimal for other algorithms.\n\nComparing to SGDM, the adaptive optimization algorithms usually converge faster and are more robust to the choice of hyper-parameters, thus have been viewed as the default choice in many applications [1,2]. Based on our experience, it requires non-trivial efforts to make SGDM achieve similar performance for cases like training Transformers. \n\n1. Reimers, Nils, and Iryna Gurevych. \"Optimal hyperparameters for deep lstm-networks for sequence labeling tasks.\" arXiv preprint arXiv:1707.06799 (2017).\n2. Popel, Martin, and Ond\u0159ej Bojar. \"Training tips for the transformer model.\" The Prague Bulletin of Mathematical Linguistics 110.1 (2018): 43-70.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper774/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Variance of the Adaptive Learning Rate and Beyond", "authors": ["Liyuan Liu", "Haoming Jiang", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Jiawei Han"], "authorids": ["ll2@illinois.edu", "jianghm@gatech.edu", "penhe@microsoft.com", "wzchen@microsoft.com", "xiaodl@microsoft.com", "jfgao@microsoft.com", "hanj@illinois.edu"], "keywords": ["warmup", "adam", "adaptive learning rate", "variance"], "TL;DR": "If warmup is the answer, what is the question?", "abstract": "The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate -- its variance is problematically large in the early stage, and presume warmup works as a variance reduction technique. We provide both empirical and theoretical evidence to verify our hypothesis. We further propose Rectified Adam (RAdam), a novel variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the efficacy and robustness of RAdam. ", "code": "https://github.com/LiyuanLucasLiu/RAdam", "pdf": "/pdf/4dbb68ed4eb409f773d227c4c157de6b31b3080b.pdf", "paperhash": "liu|on_the_variance_of_the_adaptive_learning_rate_and_beyond", "_bibtex": "@inproceedings{\nLiu2020On,\ntitle={On the Variance of the Adaptive Learning Rate and Beyond},\nauthor={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgz2aEKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/aa7745b392e15796b09d3b80b5adeebe4212a906.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgz2aEKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference/Paper774/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper774/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper774/Reviewers", "ICLR.cc/2020/Conference/Paper774/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper774/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper774/Authors|ICLR.cc/2020/Conference/Paper774/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166425, "tmdate": 1576860549570, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference/Paper774/Reviewers", "ICLR.cc/2020/Conference/Paper774/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper774/-/Official_Comment"}}}, {"id": "BJeTkngrsH", "original": null, "number": 7, "cdate": 1573354469445, "ddate": null, "tcdate": 1573354469445, "tmdate": 1573354469445, "tddate": null, "forum": "rkgz2aEKDr", "replyto": "Hkxsh6bZcS", "invitation": "ICLR.cc/2020/Conference/Paper774/-/Official_Comment", "content": {"title": "Re: Official Blind Review #2", "comment": "Thank you for your review and feedback. We understand your concern and hope that our response will alleviate them. More specifically,\n\n> On theoretic analysis of the impact of the large variance\n\nIn our study, we focus on exploring the underlying mechanism of the warmup technology, and find that it is non-trivial to theoretically identify the existence of the variance issue. Due to the complicated nature of neural networks, we believe that it would be even more challenging to establish a general and direct connection between large variance and the neural network behavior \u2014 in fact, a theoretical analysis of the neural network behavior by itself is a big challenge. Therefore, we believe that these questions deserve a more in-depth analysis, and we would like to leave it to future work. \nHere, we want to borrow some insights from recently proposed theories to intuitively illustrate why large variance of the adaptive learning rate is harmful in a simplified case. It is worth mentioning that these results are based on simple model structures (e.g. two-layer CNN/ResNet) and strong data distribution assumptions [1, 2, 3]. \n\nIt has been shown that there are bad local optima when optimizing neural networks [2, 3]; and it requires the learning rate of the gradient descent algorithm (not SGD) to be controlled within a range, in order to avoid being trapped in the regions of bad local optima and converging to the global optimum [2, 3]. In other words, the learning rate cannot be too large and has to be set within a range. Such condition might be compromised by the large variance of the adaptive learning rate \u2014 as variance is a measure of variability (i.e., lack of consistency or fixed pattern), the adaptive learning rate with a larger variance is less likely to be held within the desired range.\n\n1. Ge, Rong, et al. \"Learning two-layer neural networks with symmetric inputs.\" International Conference on Learning Representations (ICLR), 2019.\n2. Du, Simon S., et al. \"Gradient descent learns one-hidden-layer cnn: Don't be afraid of spurious local minima.\" International Conference on Machine Learning (ICML) 2019.\n3. Liu, Tianyi, et al. \"Towards Understanding the Importance of Shortcut Connections in Residual Networks.\"Annual Conference on Neural Information Processing Systems (NeurIPS), 2019.\n\n> On the accuracy of SGDM and adaptive optimization algorithms\n\nIt is true that for datasets like ImageNet, the state of the art resnet performance is usually achieved by SGD with momentum (SGDM). In our case, we observe a similar phenomenon, and we suggest it because the hyper-parameters are tuned for SGDM and may be sub-optimal for other algorithms. At the same time, our proposed RAdam algorithm shows more robustness towards learning rate changes. It is also worth mentioning that, although RAdam fails to outperform SGD in terms of test accuracy, it results in faster convergence, lower training loss and better training performance (e.g., the training accuracy of SGD, Adam, and RAdam on ImageNet are 69.57, 69.12 and 70.30 respectively)."}, "signatures": ["ICLR.cc/2020/Conference/Paper774/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Variance of the Adaptive Learning Rate and Beyond", "authors": ["Liyuan Liu", "Haoming Jiang", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Jiawei Han"], "authorids": ["ll2@illinois.edu", "jianghm@gatech.edu", "penhe@microsoft.com", "wzchen@microsoft.com", "xiaodl@microsoft.com", "jfgao@microsoft.com", "hanj@illinois.edu"], "keywords": ["warmup", "adam", "adaptive learning rate", "variance"], "TL;DR": "If warmup is the answer, what is the question?", "abstract": "The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate -- its variance is problematically large in the early stage, and presume warmup works as a variance reduction technique. We provide both empirical and theoretical evidence to verify our hypothesis. We further propose Rectified Adam (RAdam), a novel variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the efficacy and robustness of RAdam. ", "code": "https://github.com/LiyuanLucasLiu/RAdam", "pdf": "/pdf/4dbb68ed4eb409f773d227c4c157de6b31b3080b.pdf", "paperhash": "liu|on_the_variance_of_the_adaptive_learning_rate_and_beyond", "_bibtex": "@inproceedings{\nLiu2020On,\ntitle={On the Variance of the Adaptive Learning Rate and Beyond},\nauthor={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgz2aEKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/aa7745b392e15796b09d3b80b5adeebe4212a906.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgz2aEKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference/Paper774/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper774/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper774/Reviewers", "ICLR.cc/2020/Conference/Paper774/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper774/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper774/Authors|ICLR.cc/2020/Conference/Paper774/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166425, "tmdate": 1576860549570, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference/Paper774/Reviewers", "ICLR.cc/2020/Conference/Paper774/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper774/-/Official_Comment"}}}, {"id": "Hye4w1Tyjr", "original": null, "number": 4, "cdate": 1573011291626, "ddate": null, "tcdate": 1573011291626, "tmdate": 1573011363210, "tddate": null, "forum": "rkgz2aEKDr", "replyto": "rkgz2aEKDr", "invitation": "ICLR.cc/2020/Conference/Paper774/-/Public_Comment", "content": {"title": "Public comments on the present manuscript", "comment": "Hello,\n\nWe would like to provide a couple of comments about this present manuscript proposing the RAdam algorithm. These comments are based on our work in shorturl.at/uyzY8 (suitably edited to preserve anonymity of the manuscript\u2019s authors).\n\nOn the \"divergent variance\" correction:\n\n* RAdam is 4 timesteps of momentum SGD, followed by Adam with a fixed warmup schedule. For all values of $\\beta_2$ usable in practice, the condition \"if the variance is tractable, i.e., $\\rho_t = \\rho_{\\infty} - 2 t \\beta_2^t / (1 - \\beta_2^t) > 4$\" (Algorithm 2, Line 9) is precisely equivalent to the condition \"if $t > 4$\". Neglecting to state this lends an air of \u201csmoke and mirrors\u201d by obfuscating the true operation of the algorithm under the imprimatur of mathematical sophistication.\n\n* The manuscript does not demonstrate the necessity of 4 steps of momentum SGD on any task. Indeed, we were unable to find any realistic setting where 4 steps of momentum SGD ended up doing anything remotely discernible. In the absence of any practical justification of an arbitrary 4 steps of momentum, we view the \"divergent variance\" correction as magic masquerading as math.\n\nIgnoring the 4 steps of momentum SGD, RAdam is precisely Adam with a fixed learning rate warmup schedule. This raises the following points:\n\n* The manuscript attempts to separate RAdam from standard \"heuristic\" warmup schedules in both motivation and operation. In truth, RAdam without the 4 steps of momentum SGD is a specific functional form for heuristic warmup that depends on $\\beta_2$.\n\n* We found that RAdam performs identically to linear warmup over $2 / (1 - \\beta_2)$ timesteps, across all domains evaluated in the manuscript.\n\n* The manuscript claims as RAdam\u2019s primary benefit that no manual tuning of a warmup schedule is needed for robust operation. To the extent that this claim is true, it is also true of linear warmup given the above.\n\nIn our opinion, a complex method should not be advanced over a simpler method unless the complex method brings something useful to the table. There is no evidence that RAdam brings anything to the table above and beyond the simplest of warmup methods, such as linear warmup over $2 / (1 - \\beta_2)$ timesteps.\n\nFinally, we would like to point out the following about the underlying motivation for RAdam and learning rate warmup:\n\n* The manuscript does not show a causal relationship between the variance of the adaptive learning rate and training instability, nor does it show any significant consequence of \"divergent variance\" versus \u201cconvergent variance\u201d (i.e. comparing timestep 4 and timestep 5) \u2013 probably because there are none.\n\n* While it may be true that the variance of the adaptive learning rate (i.e. inverse second moments) is /correlated/ with training instability, any causal relationship to training instability is likely with the parameter update magnitudes; understanding the latter requires jointly analyzing the first moments and second moments, as they are extremely dependent on each other.\n\nIn summary, we believe that endorsing this manuscript in present form with inclusion at ICLR will encourage incremental and poorly-justified future work in new Adam variants and learning rate warmup.\n\n- Jerry Ma and Denis Yarats\n"}, "signatures": ["~Jerry_Ma1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Jerry_Ma1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Variance of the Adaptive Learning Rate and Beyond", "authors": ["Liyuan Liu", "Haoming Jiang", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Jiawei Han"], "authorids": ["ll2@illinois.edu", "jianghm@gatech.edu", "penhe@microsoft.com", "wzchen@microsoft.com", "xiaodl@microsoft.com", "jfgao@microsoft.com", "hanj@illinois.edu"], "keywords": ["warmup", "adam", "adaptive learning rate", "variance"], "TL;DR": "If warmup is the answer, what is the question?", "abstract": "The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate -- its variance is problematically large in the early stage, and presume warmup works as a variance reduction technique. We provide both empirical and theoretical evidence to verify our hypothesis. We further propose Rectified Adam (RAdam), a novel variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the efficacy and robustness of RAdam. ", "code": "https://github.com/LiyuanLucasLiu/RAdam", "pdf": "/pdf/4dbb68ed4eb409f773d227c4c157de6b31b3080b.pdf", "paperhash": "liu|on_the_variance_of_the_adaptive_learning_rate_and_beyond", "_bibtex": "@inproceedings{\nLiu2020On,\ntitle={On the Variance of the Adaptive Learning Rate and Beyond},\nauthor={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgz2aEKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/aa7745b392e15796b09d3b80b5adeebe4212a906.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgz2aEKDr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504204361, "tmdate": 1576860582856, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference/Paper774/Reviewers", "ICLR.cc/2020/Conference/Paper774/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper774/-/Public_Comment"}}}, {"id": "HJxzMg4AYH", "original": null, "number": 1, "cdate": 1571860490504, "ddate": null, "tcdate": 1571860490504, "tmdate": 1572972553989, "tddate": null, "forum": "rkgz2aEKDr", "replyto": "rkgz2aEKDr", "invitation": "ICLR.cc/2020/Conference/Paper774/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "Authors propose a way to rectify the variance of the adaptive learning rate (RAdam) and apply the optimizer to applications in image classification, language modeling and neural machine translation. The experiments demonstrate not only a strong results over baseline Adam with warmup learning rate but the robustness of the optimizer. The authors additionally demonstrate the theoretical justification behind their optimizer, however I am not very qualified to make the judgement on the theory. Overall judging from the authors description of approach and experimental results, I recommend acceptance.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper774/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper774/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Variance of the Adaptive Learning Rate and Beyond", "authors": ["Liyuan Liu", "Haoming Jiang", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Jiawei Han"], "authorids": ["ll2@illinois.edu", "jianghm@gatech.edu", "penhe@microsoft.com", "wzchen@microsoft.com", "xiaodl@microsoft.com", "jfgao@microsoft.com", "hanj@illinois.edu"], "keywords": ["warmup", "adam", "adaptive learning rate", "variance"], "TL;DR": "If warmup is the answer, what is the question?", "abstract": "The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate -- its variance is problematically large in the early stage, and presume warmup works as a variance reduction technique. We provide both empirical and theoretical evidence to verify our hypothesis. We further propose Rectified Adam (RAdam), a novel variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the efficacy and robustness of RAdam. ", "code": "https://github.com/LiyuanLucasLiu/RAdam", "pdf": "/pdf/4dbb68ed4eb409f773d227c4c157de6b31b3080b.pdf", "paperhash": "liu|on_the_variance_of_the_adaptive_learning_rate_and_beyond", "_bibtex": "@inproceedings{\nLiu2020On,\ntitle={On the Variance of the Adaptive Learning Rate and Beyond},\nauthor={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgz2aEKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/aa7745b392e15796b09d3b80b5adeebe4212a906.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkgz2aEKDr", "replyto": "rkgz2aEKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper774/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper774/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576558325269, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper774/Reviewers"], "noninvitees": [], "tcdate": 1570237747283, "tmdate": 1576558325286, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper774/-/Official_Review"}}}, {"id": "SyeHRMd1cS", "original": null, "number": 2, "cdate": 1571943117456, "ddate": null, "tcdate": 1571943117456, "tmdate": 1572972553942, "tddate": null, "forum": "rkgz2aEKDr", "replyto": "rkgz2aEKDr", "invitation": "ICLR.cc/2020/Conference/Paper774/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "I haven't worked in this area before, and my knowledge to the topic of designing optimizers for improving stochastic gradient descent is limited to the level of advanced ML courses at graduate school. Nevertheless, below I try my best to evaluate the technicality of this paper\n\n====================\nIn this work the authors studied the variance issue of the adaptive learning rate and aim to justify the warm-start heuristic. They also demonstrate that  the convergence issue of many of the stochastic gradient descent algorithms is due to large variance induced by the adaptive learning rate in the early stage of training. To tackle this issue, they proposed a variant of ADAM, which is known as rectified ADAM, whose learning rate not only takes the momentum into the account, but it also adapts to the variance of the previous gradient updates.  \n\nIn the first part of the paper, the authors analyzed the variance issue exists in the existing ADAM algorithm, such that with limited samples in the early stage of training, the variance of the adaptive learning rate becomes rather large and it induces high variance to the gradient update to ADAM. In general I found this theoretical justification on the observation of variance issue in ADAM sound, and quite intuitive. In the second part, they proposed the algorithm, namely rectified ADAM, where the difference here to take the second moment  of the gradient into account when updating the adaptive learning rate. They showed that the variance of the adaptive learning rate with such rectification is more numerically stable (especially when variance is intractable in vanilla ADAM), and under some regularity assumption it decreases in the order or O(1/\\rho_t).\n\nIn extensive numerical studies of supervised learning, the authors showed that RADAM achieves a better accuracy than ADAM (although in Table 1, I am a bit puzzled why the best accuracy is indeed from SGD, if so, what's the point of all adaptive learning rates, is that because SGD requires extensive lr tuning?) Because the superiority in accuracy they also showed that RADAM manages to have more stable training and achieves lower training loss, which is quite interesting. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper774/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper774/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Variance of the Adaptive Learning Rate and Beyond", "authors": ["Liyuan Liu", "Haoming Jiang", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Jiawei Han"], "authorids": ["ll2@illinois.edu", "jianghm@gatech.edu", "penhe@microsoft.com", "wzchen@microsoft.com", "xiaodl@microsoft.com", "jfgao@microsoft.com", "hanj@illinois.edu"], "keywords": ["warmup", "adam", "adaptive learning rate", "variance"], "TL;DR": "If warmup is the answer, what is the question?", "abstract": "The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate -- its variance is problematically large in the early stage, and presume warmup works as a variance reduction technique. We provide both empirical and theoretical evidence to verify our hypothesis. We further propose Rectified Adam (RAdam), a novel variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the efficacy and robustness of RAdam. ", "code": "https://github.com/LiyuanLucasLiu/RAdam", "pdf": "/pdf/4dbb68ed4eb409f773d227c4c157de6b31b3080b.pdf", "paperhash": "liu|on_the_variance_of_the_adaptive_learning_rate_and_beyond", "_bibtex": "@inproceedings{\nLiu2020On,\ntitle={On the Variance of the Adaptive Learning Rate and Beyond},\nauthor={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgz2aEKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/aa7745b392e15796b09d3b80b5adeebe4212a906.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkgz2aEKDr", "replyto": "rkgz2aEKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper774/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper774/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576558325269, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper774/Reviewers"], "noninvitees": [], "tcdate": 1570237747283, "tmdate": 1576558325286, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper774/-/Official_Review"}}}, {"id": "Hkxsh6bZcS", "original": null, "number": 3, "cdate": 1572048306839, "ddate": null, "tcdate": 1572048306839, "tmdate": 1572972553899, "tddate": null, "forum": "rkgz2aEKDr", "replyto": "rkgz2aEKDr", "invitation": "ICLR.cc/2020/Conference/Paper774/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this work, authors show that the bad performance of Adam is from the large variance of adaptive learning rate at the beginning of the training.\nPros:\n1.\tAuthors demonstrate that the variance of the first few stages is large, which may interpret the degradation in the performance of Adam.\n2.\tThe empirical study supports the claim about the large variance.\n\nCons:\n1.\tTheoretically, authors didn\u2019t illustrate why the large variance can result in the bad performance in terms of, e.g., convergence rate, generalization error, etc.\n2.\tThe performance of the proposed algorithm is still worse than SGD and it makes the analysis less attractive.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper774/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper774/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Variance of the Adaptive Learning Rate and Beyond", "authors": ["Liyuan Liu", "Haoming Jiang", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Jiawei Han"], "authorids": ["ll2@illinois.edu", "jianghm@gatech.edu", "penhe@microsoft.com", "wzchen@microsoft.com", "xiaodl@microsoft.com", "jfgao@microsoft.com", "hanj@illinois.edu"], "keywords": ["warmup", "adam", "adaptive learning rate", "variance"], "TL;DR": "If warmup is the answer, what is the question?", "abstract": "The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate -- its variance is problematically large in the early stage, and presume warmup works as a variance reduction technique. We provide both empirical and theoretical evidence to verify our hypothesis. We further propose Rectified Adam (RAdam), a novel variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the efficacy and robustness of RAdam. ", "code": "https://github.com/LiyuanLucasLiu/RAdam", "pdf": "/pdf/4dbb68ed4eb409f773d227c4c157de6b31b3080b.pdf", "paperhash": "liu|on_the_variance_of_the_adaptive_learning_rate_and_beyond", "_bibtex": "@inproceedings{\nLiu2020On,\ntitle={On the Variance of the Adaptive Learning Rate and Beyond},\nauthor={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgz2aEKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/aa7745b392e15796b09d3b80b5adeebe4212a906.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkgz2aEKDr", "replyto": "rkgz2aEKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper774/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper774/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576558325269, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper774/Reviewers"], "noninvitees": [], "tcdate": 1570237747283, "tmdate": 1576558325286, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper774/-/Official_Review"}}}, {"id": "HJgZiW0ddH", "original": null, "number": 4, "cdate": 1570460057362, "ddate": null, "tcdate": 1570460057362, "tmdate": 1570460057362, "tddate": null, "forum": "rkgz2aEKDr", "replyto": "S1ew6gRO_B", "invitation": "ICLR.cc/2020/Conference/Paper774/-/Official_Comment", "content": {"comment": "Yes, your understanding is correct. ", "title": "Re: Adam-2k"}, "signatures": ["ICLR.cc/2020/Conference/Paper774/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Variance of the Adaptive Learning Rate and Beyond", "authors": ["Liyuan Liu", "Haoming Jiang", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Jiawei Han"], "authorids": ["ll2@illinois.edu", "jianghm@gatech.edu", "penhe@microsoft.com", "wzchen@microsoft.com", "xiaodl@microsoft.com", "jfgao@microsoft.com", "hanj@illinois.edu"], "keywords": ["warmup", "adam", "adaptive learning rate", "variance"], "TL;DR": "If warmup is the answer, what is the question?", "abstract": "The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate -- its variance is problematically large in the early stage, and presume warmup works as a variance reduction technique. We provide both empirical and theoretical evidence to verify our hypothesis. We further propose Rectified Adam (RAdam), a novel variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the efficacy and robustness of RAdam. ", "code": "https://github.com/LiyuanLucasLiu/RAdam", "pdf": "/pdf/4dbb68ed4eb409f773d227c4c157de6b31b3080b.pdf", "paperhash": "liu|on_the_variance_of_the_adaptive_learning_rate_and_beyond", "_bibtex": "@inproceedings{\nLiu2020On,\ntitle={On the Variance of the Adaptive Learning Rate and Beyond},\nauthor={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgz2aEKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/aa7745b392e15796b09d3b80b5adeebe4212a906.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgz2aEKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference/Paper774/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper774/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper774/Reviewers", "ICLR.cc/2020/Conference/Paper774/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper774/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper774/Authors|ICLR.cc/2020/Conference/Paper774/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166425, "tmdate": 1576860549570, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference/Paper774/Reviewers", "ICLR.cc/2020/Conference/Paper774/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper774/-/Official_Comment"}}}, {"id": "S1ew6gRO_B", "original": null, "number": 3, "cdate": 1570459839399, "ddate": null, "tcdate": 1570459839399, "tmdate": 1570459839399, "tddate": null, "forum": "rkgz2aEKDr", "replyto": "BkgVzLSdOS", "invitation": "ICLR.cc/2020/Conference/Paper774/-/Public_Comment", "content": {"comment": "\"In the first two thousand batches, Adam-2k will only update the moving average of the second moment, and freeze both the first moment and the parameter value; after these two thousand batches, Adam-2k will start to update parameters in the vanilla-Adam way.\" \n\nWhen you say that parameters are frozen for first 2000 iteration, does it mean that first 2000 iterations you compute gradients for the same weights, and that you use these gradients only to update second momentum without updating first moment and weights?\n\n", "title": "Adam-2K"}, "signatures": ["~Boris_Ginsburg1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Boris_Ginsburg1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Variance of the Adaptive Learning Rate and Beyond", "authors": ["Liyuan Liu", "Haoming Jiang", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Jiawei Han"], "authorids": ["ll2@illinois.edu", "jianghm@gatech.edu", "penhe@microsoft.com", "wzchen@microsoft.com", "xiaodl@microsoft.com", "jfgao@microsoft.com", "hanj@illinois.edu"], "keywords": ["warmup", "adam", "adaptive learning rate", "variance"], "TL;DR": "If warmup is the answer, what is the question?", "abstract": "The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate -- its variance is problematically large in the early stage, and presume warmup works as a variance reduction technique. We provide both empirical and theoretical evidence to verify our hypothesis. We further propose Rectified Adam (RAdam), a novel variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the efficacy and robustness of RAdam. ", "code": "https://github.com/LiyuanLucasLiu/RAdam", "pdf": "/pdf/4dbb68ed4eb409f773d227c4c157de6b31b3080b.pdf", "paperhash": "liu|on_the_variance_of_the_adaptive_learning_rate_and_beyond", "_bibtex": "@inproceedings{\nLiu2020On,\ntitle={On the Variance of the Adaptive Learning Rate and Beyond},\nauthor={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgz2aEKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/aa7745b392e15796b09d3b80b5adeebe4212a906.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgz2aEKDr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504204361, "tmdate": 1576860582856, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference/Paper774/Reviewers", "ICLR.cc/2020/Conference/Paper774/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper774/-/Public_Comment"}}}, {"id": "BkgVzLSdOS", "original": null, "number": 3, "cdate": 1570424331903, "ddate": null, "tcdate": 1570424331903, "tmdate": 1570424331903, "tddate": null, "forum": "rkgz2aEKDr", "replyto": "HJla5-ydOH", "invitation": "ICLR.cc/2020/Conference/Paper774/-/Official_Comment", "content": {"comment": "Thanks for asking. I did an experiment and it shows: treating the first 2001 batches as one large batch suffers from the same convergence problem with the vanilla-Adam (in our NMT experiment as in Figure 1). It further verifies our hypothesis that the adaptive learning rate has undesirably large variance and blocks the algorithm from getting a reasonable performance. \n\nEmpirically, improper learning rate setting can lead to convergence problems. Still, to explain the underlying mechanism of warmup, it requires a detailed analysis on the difference between the initial phrase and the later phrase (e.g., why it requires a smaller learning rate in the beginning; or with the same update rule and learning rate, why the update is more problematic in the beginning).\n\nBased on our experiments, after getting enough samples to estimate the adaptive learning rate (instead of the gradient direction), we can avoid the convergence problem as in Figure 1.\n\nWe will explore the problem why warmup helps SGD in the future, and thanks for pointing out one potential explanation. ", "title": "Re: Why does warmup help"}, "signatures": ["ICLR.cc/2020/Conference/Paper774/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Variance of the Adaptive Learning Rate and Beyond", "authors": ["Liyuan Liu", "Haoming Jiang", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Jiawei Han"], "authorids": ["ll2@illinois.edu", "jianghm@gatech.edu", "penhe@microsoft.com", "wzchen@microsoft.com", "xiaodl@microsoft.com", "jfgao@microsoft.com", "hanj@illinois.edu"], "keywords": ["warmup", "adam", "adaptive learning rate", "variance"], "TL;DR": "If warmup is the answer, what is the question?", "abstract": "The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate -- its variance is problematically large in the early stage, and presume warmup works as a variance reduction technique. We provide both empirical and theoretical evidence to verify our hypothesis. We further propose Rectified Adam (RAdam), a novel variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the efficacy and robustness of RAdam. ", "code": "https://github.com/LiyuanLucasLiu/RAdam", "pdf": "/pdf/4dbb68ed4eb409f773d227c4c157de6b31b3080b.pdf", "paperhash": "liu|on_the_variance_of_the_adaptive_learning_rate_and_beyond", "_bibtex": "@inproceedings{\nLiu2020On,\ntitle={On the Variance of the Adaptive Learning Rate and Beyond},\nauthor={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgz2aEKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/aa7745b392e15796b09d3b80b5adeebe4212a906.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgz2aEKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference/Paper774/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper774/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper774/Reviewers", "ICLR.cc/2020/Conference/Paper774/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper774/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper774/Authors|ICLR.cc/2020/Conference/Paper774/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166425, "tmdate": 1576860549570, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference/Paper774/Reviewers", "ICLR.cc/2020/Conference/Paper774/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper774/-/Official_Comment"}}}, {"id": "HJla5-ydOH", "original": null, "number": 2, "cdate": 1570398612735, "ddate": null, "tcdate": 1570398612735, "tmdate": 1570398612735, "tddate": null, "forum": "rkgz2aEKDr", "replyto": "SJlhBzow_H", "invitation": "ICLR.cc/2020/Conference/Paper774/-/Public_Comment", "content": {"comment": "Question: If you  would combine all samples from first 2K iterations into one large batch, would you still need warmup?\n \n\"The reason why sometimes warmup also helps SGD still lacks theoretical support. We believe this topic deserves more in-depth analysis and is beyond the scope of this study. \"\nOne explanation is that during initial phase, the main issue is not number of observed samples, but the fact that the  update is much larger than weights (LARS, LAMB, ....).", "title": "Why does warmup help?"}, "signatures": ["~Boris_Ginsburg1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Boris_Ginsburg1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Variance of the Adaptive Learning Rate and Beyond", "authors": ["Liyuan Liu", "Haoming Jiang", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Jiawei Han"], "authorids": ["ll2@illinois.edu", "jianghm@gatech.edu", "penhe@microsoft.com", "wzchen@microsoft.com", "xiaodl@microsoft.com", "jfgao@microsoft.com", "hanj@illinois.edu"], "keywords": ["warmup", "adam", "adaptive learning rate", "variance"], "TL;DR": "If warmup is the answer, what is the question?", "abstract": "The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate -- its variance is problematically large in the early stage, and presume warmup works as a variance reduction technique. We provide both empirical and theoretical evidence to verify our hypothesis. We further propose Rectified Adam (RAdam), a novel variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the efficacy and robustness of RAdam. ", "code": "https://github.com/LiyuanLucasLiu/RAdam", "pdf": "/pdf/4dbb68ed4eb409f773d227c4c157de6b31b3080b.pdf", "paperhash": "liu|on_the_variance_of_the_adaptive_learning_rate_and_beyond", "_bibtex": "@inproceedings{\nLiu2020On,\ntitle={On the Variance of the Adaptive Learning Rate and Beyond},\nauthor={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgz2aEKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/aa7745b392e15796b09d3b80b5adeebe4212a906.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgz2aEKDr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504204361, "tmdate": 1576860582856, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference/Paper774/Reviewers", "ICLR.cc/2020/Conference/Paper774/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper774/-/Public_Comment"}}}, {"id": "S1ljoi3Dur", "original": null, "number": 2, "cdate": 1570388898978, "ddate": null, "tcdate": 1570388898978, "tmdate": 1570388898978, "tddate": null, "forum": "rkgz2aEKDr", "replyto": "rkgz2aEKDr", "invitation": "ICLR.cc/2020/Conference/Paper774/-/Official_Comment", "content": {"comment": "We've found several typos (i.e., in Algorithm 1, line 5, Equation 4 and Equation 7). Sorry for the mistake and the corrected version can be found in: https://drive.google.com/open?id=1UQbRm66IMPP8_HycUIP-txV2vNx9SneT", "title": "Corrections of Typos"}, "signatures": ["ICLR.cc/2020/Conference/Paper774/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Variance of the Adaptive Learning Rate and Beyond", "authors": ["Liyuan Liu", "Haoming Jiang", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Jiawei Han"], "authorids": ["ll2@illinois.edu", "jianghm@gatech.edu", "penhe@microsoft.com", "wzchen@microsoft.com", "xiaodl@microsoft.com", "jfgao@microsoft.com", "hanj@illinois.edu"], "keywords": ["warmup", "adam", "adaptive learning rate", "variance"], "TL;DR": "If warmup is the answer, what is the question?", "abstract": "The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate -- its variance is problematically large in the early stage, and presume warmup works as a variance reduction technique. We provide both empirical and theoretical evidence to verify our hypothesis. We further propose Rectified Adam (RAdam), a novel variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the efficacy and robustness of RAdam. ", "code": "https://github.com/LiyuanLucasLiu/RAdam", "pdf": "/pdf/4dbb68ed4eb409f773d227c4c157de6b31b3080b.pdf", "paperhash": "liu|on_the_variance_of_the_adaptive_learning_rate_and_beyond", "_bibtex": "@inproceedings{\nLiu2020On,\ntitle={On the Variance of the Adaptive Learning Rate and Beyond},\nauthor={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgz2aEKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/aa7745b392e15796b09d3b80b5adeebe4212a906.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgz2aEKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference/Paper774/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper774/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper774/Reviewers", "ICLR.cc/2020/Conference/Paper774/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper774/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper774/Authors|ICLR.cc/2020/Conference/Paper774/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166425, "tmdate": 1576860549570, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference/Paper774/Reviewers", "ICLR.cc/2020/Conference/Paper774/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper774/-/Official_Comment"}}}, {"id": "SJlhBzow_H", "original": null, "number": 1, "cdate": 1570382403534, "ddate": null, "tcdate": 1570382403534, "tmdate": 1570382403534, "tddate": null, "forum": "rkgz2aEKDr", "replyto": "B1l02LcDuB", "invitation": "ICLR.cc/2020/Conference/Paper774/-/Official_Comment", "content": {"comment": "Thanks for asking and pointing out the typo, we will fix it in the next version. : -)\n\n1. Our study is motivated by the phenomenon that: Adam-without-warmup fails for NMT, even with small batch size. As in Theorem 1, the adaptive learning rate has a larger variance in the early stage of training than the later stage. Also, by providing two thousand additional batches to estimate the learning rate, we can avoid the convergence problem met by the vanilla-Adam. \n\n2. We agree the warmup is originally designed for large-minibatch-SGD [0], based on the intuition that the network changes rapidly in the early stage. However, we find that this intuition does not explain why Adam requires warmup. Notice that, Adam-2k can also avoid the convergence problem: it uses the same learning rate and initialization but has a better estimation of the adaptive learning rate (more details below). Therefore, we suggest that although warmup helps both Adam and SGD, they are for different reasons. \n\nIn the first two thousand batches, Adam-2k will only update the moving average of the second moment, and freeze both the first moment and the parameter value; after these two thousand batches, Adam-2k will start to update parameters in the vanilla-Adam way. That is to say, Adam-2k (avoids convergence problem) and Adam (suffers from convergence problem) has the same initialization, learning rate scheduler and update rule; their only difference is Adam-2k has additional samples to estimate the adaptive learning rate. Accordingly, we think the root cause of the convergence issue is the lack of sufficient data samples in the early stage (to estimate the adaptive learning rate). \n\nThe reason why sometimes warmup also helps SGD still lacks theoretical support. We believe this topic deserves more in-depth analysis and is beyond the scope of this study. \n\n[0] Goyal et al, Accurate, Large Minibatch SGD: Training Imagenet in 1 Hour, 2017\n", "title": "Although warmup helps both Adam and SGD, they are for different reasons. "}, "signatures": ["ICLR.cc/2020/Conference/Paper774/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Variance of the Adaptive Learning Rate and Beyond", "authors": ["Liyuan Liu", "Haoming Jiang", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Jiawei Han"], "authorids": ["ll2@illinois.edu", "jianghm@gatech.edu", "penhe@microsoft.com", "wzchen@microsoft.com", "xiaodl@microsoft.com", "jfgao@microsoft.com", "hanj@illinois.edu"], "keywords": ["warmup", "adam", "adaptive learning rate", "variance"], "TL;DR": "If warmup is the answer, what is the question?", "abstract": "The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate -- its variance is problematically large in the early stage, and presume warmup works as a variance reduction technique. We provide both empirical and theoretical evidence to verify our hypothesis. We further propose Rectified Adam (RAdam), a novel variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the efficacy and robustness of RAdam. ", "code": "https://github.com/LiyuanLucasLiu/RAdam", "pdf": "/pdf/4dbb68ed4eb409f773d227c4c157de6b31b3080b.pdf", "paperhash": "liu|on_the_variance_of_the_adaptive_learning_rate_and_beyond", "_bibtex": "@inproceedings{\nLiu2020On,\ntitle={On the Variance of the Adaptive Learning Rate and Beyond},\nauthor={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgz2aEKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/aa7745b392e15796b09d3b80b5adeebe4212a906.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgz2aEKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference/Paper774/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper774/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper774/Reviewers", "ICLR.cc/2020/Conference/Paper774/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper774/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper774/Authors|ICLR.cc/2020/Conference/Paper774/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166425, "tmdate": 1576860549570, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference/Paper774/Reviewers", "ICLR.cc/2020/Conference/Paper774/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper774/-/Official_Comment"}}}, {"id": "B1l02LcDuB", "original": null, "number": 1, "cdate": 1570379446135, "ddate": null, "tcdate": 1570379446135, "tmdate": 1570379609196, "tddate": null, "forum": "rkgz2aEKDr", "replyto": "rkgz2aEKDr", "invitation": "ICLR.cc/2020/Conference/Paper774/-/Public_Comment", "content": {"comment": "The main claim of the paper is that \"the lack of sufficient data samples in the early stage is the root cause of the convergence issue\" (section 3.1). How you would explain the following observations:\n1. Adam works well with  small batch size without warmup. Why in this case the small amount of samples doesn't impact the stability?\n2. Learning rate warm-up was introduced for large batch training. large batch usually have thousands of samples. Then what is the source  of instability during initial training phase?\n\nBtw, there is a typo in Algorithm 1, line 5: v_t instead of l_t ", "title": "Source of instability during initial training phase?"}, "signatures": ["~Boris_Ginsburg1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Boris_Ginsburg1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Variance of the Adaptive Learning Rate and Beyond", "authors": ["Liyuan Liu", "Haoming Jiang", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Jiawei Han"], "authorids": ["ll2@illinois.edu", "jianghm@gatech.edu", "penhe@microsoft.com", "wzchen@microsoft.com", "xiaodl@microsoft.com", "jfgao@microsoft.com", "hanj@illinois.edu"], "keywords": ["warmup", "adam", "adaptive learning rate", "variance"], "TL;DR": "If warmup is the answer, what is the question?", "abstract": "The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate -- its variance is problematically large in the early stage, and presume warmup works as a variance reduction technique. We provide both empirical and theoretical evidence to verify our hypothesis. We further propose Rectified Adam (RAdam), a novel variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the efficacy and robustness of RAdam. ", "code": "https://github.com/LiyuanLucasLiu/RAdam", "pdf": "/pdf/4dbb68ed4eb409f773d227c4c157de6b31b3080b.pdf", "paperhash": "liu|on_the_variance_of_the_adaptive_learning_rate_and_beyond", "_bibtex": "@inproceedings{\nLiu2020On,\ntitle={On the Variance of the Adaptive Learning Rate and Beyond},\nauthor={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgz2aEKDr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/aa7745b392e15796b09d3b80b5adeebe4212a906.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgz2aEKDr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504204361, "tmdate": 1576860582856, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper774/Authors", "ICLR.cc/2020/Conference/Paper774/Reviewers", "ICLR.cc/2020/Conference/Paper774/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper774/-/Public_Comment"}}}], "count": 18}