{"notes": [{"id": "SkxWnkStvS", "original": "r1xL5x1KwS", "number": 1941, "cdate": 1569439656935, "ddate": null, "tcdate": 1569439656935, "tmdate": 1577168281945, "tddate": null, "forum": "SkxWnkStvS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Searching for Stage-wise Neural Graphs In the Limit", "authors": ["Xin Zhou", "Dejing Dou", "Boyang Li"], "authorids": ["chow459@gmail.com", "doudejing@baidu.com", "libo0001@gmail.com"], "keywords": ["neural architecture search", "graphon", "random graphs"], "TL;DR": "Graphon is a good search space for neural architecture search and empirically produces good networks.", "abstract": "Search space is a key consideration for neural architecture search. Recently, Xie et al. (2019a) found that randomly generated networks from the same distribution perform similarly, which suggest we should search for random graph distributions instead of graphs. We propose graphon as a new search space. A graphon is the limit of Cauchy sequence of graphs and a scale-free probabilistic distribution, from which graphs of different number of vertices can be drawn. This property enables us to perform NAS using fast, low-capacity models and scale the found models up when necessary. We develop an algorithm for NAS in the space of graphons and empirically demonstrate that it can find stage-wise graphs that outperform DenseNet and other baselines on ImageNet. ", "pdf": "/pdf/bf6daea9d9c7b272d16bea44974e39d8cb755c3b.pdf", "paperhash": "zhou|searching_for_stagewise_neural_graphs_in_the_limit", "original_pdf": "/attachment/466cebc922fa146c1a2c1267d599a8ce5cc360ff.pdf", "_bibtex": "@misc{\nzhou2020searching,\ntitle={Searching for Stage-wise Neural Graphs In the Limit},\nauthor={Xin Zhou and Dejing Dou and Boyang Li},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxWnkStvS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "YJswe4ejDS", "original": null, "number": 1, "cdate": 1576798736493, "ddate": null, "tcdate": 1576798736493, "tmdate": 1576800899879, "tddate": null, "forum": "SkxWnkStvS", "replyto": "SkxWnkStvS", "invitation": "ICLR.cc/2020/Conference/Paper1941/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a graphon-based search space for neural architecture search. Unfortunately, the paper as currently stands and the small effect sizes in the experimental results raise questions about the merits of actually employing such a search space for the specific task of NAS. The reviewers expressed concerns that the results do not convincingly support graphon being a superior search space as claimed in the paper. \n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Searching for Stage-wise Neural Graphs In the Limit", "authors": ["Xin Zhou", "Dejing Dou", "Boyang Li"], "authorids": ["chow459@gmail.com", "doudejing@baidu.com", "libo0001@gmail.com"], "keywords": ["neural architecture search", "graphon", "random graphs"], "TL;DR": "Graphon is a good search space for neural architecture search and empirically produces good networks.", "abstract": "Search space is a key consideration for neural architecture search. Recently, Xie et al. (2019a) found that randomly generated networks from the same distribution perform similarly, which suggest we should search for random graph distributions instead of graphs. We propose graphon as a new search space. A graphon is the limit of Cauchy sequence of graphs and a scale-free probabilistic distribution, from which graphs of different number of vertices can be drawn. This property enables us to perform NAS using fast, low-capacity models and scale the found models up when necessary. We develop an algorithm for NAS in the space of graphons and empirically demonstrate that it can find stage-wise graphs that outperform DenseNet and other baselines on ImageNet. ", "pdf": "/pdf/bf6daea9d9c7b272d16bea44974e39d8cb755c3b.pdf", "paperhash": "zhou|searching_for_stagewise_neural_graphs_in_the_limit", "original_pdf": "/attachment/466cebc922fa146c1a2c1267d599a8ce5cc360ff.pdf", "_bibtex": "@misc{\nzhou2020searching,\ntitle={Searching for Stage-wise Neural Graphs In the Limit},\nauthor={Xin Zhou and Dejing Dou and Boyang Li},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxWnkStvS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SkxWnkStvS", "replyto": "SkxWnkStvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795711106, "tmdate": 1576800260238, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1941/-/Decision"}}}, {"id": "B1ljIu0jjr", "original": null, "number": 3, "cdate": 1573804114993, "ddate": null, "tcdate": 1573804114993, "tmdate": 1573804114993, "tddate": null, "forum": "SkxWnkStvS", "replyto": "B1xkzupttH", "invitation": "ICLR.cc/2020/Conference/Paper1941/-/Official_Comment", "content": {"title": "Responses to reviewer #1", "comment": "I agree that their proposed model allows for more architectures but in practice it is not much stronger than WS-G. \n\nWe have updated results and as graph sizes increase, performance gaps become more apparent and we go up to Densenet 264 where connectivity improvements results in improvements of up to 0.8%.\n\n\nThe argumentation with respect to parameters is unclear to me. On one hand, you manually influence the number of parameters, on the other you argue that you use less parameters. Obviously, you chose that your baselines have more parameters. \n\nControl over the number of parameters: The single hyperparameter we can adjust for every stage is the growth rate c. A node that has k input will have kc input channels and c output channels. Here k is determined by the randomly sampled graph (different for each of the six training sessions) and out of our control. Thus, our control over the number of parameters is imprecise. We try to match all parameters. When that's not possible, we err on giving the baselines more parameters in order to create a harsh test. \n\nHow do results for WS-G look like if you reduce its parameters to match yours?\nAs many newly added experiments suggest, in the range we investigated, more parameters always lead to performance improvements.\n \nSpecifically, we produced two variants of WS-G that have slightly parameter counts. We report the average of six training sessions.\n\n\t\t                                      ImageNet-2012 Val\t\t\t\tImageNet V2 Test\t\t\t\n\t              # Param  \tTop 1\tStdev\tTop 5\tStdev\tTop 1\tStdev\tTop 5\tStdev\nWS-G 169 +\t14.54M\t77.11\t0.06\t        93.44\t0.05\t       65.23\t0.41  \t85.84\t0.15\nWS-G 169\t14.23M\t76.94\t0.06\t        93.37\t0.07\t       65.18\t0.23\t        85.79\t0.13 \n\n\nThe results show that, even reducing the parameters from 14.54 M to 14.23M has a discernible effect on the performance (a reduction of 0.17% on ImageNet and 0.05% on ImageNet V2)\n\n\nIn fact, you were searching for an architecture on CIFAR-10 but you did not report your results here. Instead you only report your transferred results to ImageNet. Is it possible that you also report results on CIFAR-10? \n\nWe answer this in common responses 5.\n\n\nFinally, you do not discuss that your graph contains only one kind of node. In many NAS methods the search space contains various types of operations. Do you think this is a problem? Is there a trivial way to extend your method to cover this as well?\n\nThe goal of this paper is to optimize only the connections between homogeneous nodes, but each node can contain multiple different operations. As the reviewer rightly guessed, extending this to allow different operations in the same graph is possible but beyond the scope of this paper. For example, the digraphon formulation provides a way to have different types of connections in the graph. Digraphon is concerned with the direction of connections, but we can easily employ different activation functions, pooling, or any other neural operators as connections.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1941/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1941/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Searching for Stage-wise Neural Graphs In the Limit", "authors": ["Xin Zhou", "Dejing Dou", "Boyang Li"], "authorids": ["chow459@gmail.com", "doudejing@baidu.com", "libo0001@gmail.com"], "keywords": ["neural architecture search", "graphon", "random graphs"], "TL;DR": "Graphon is a good search space for neural architecture search and empirically produces good networks.", "abstract": "Search space is a key consideration for neural architecture search. Recently, Xie et al. (2019a) found that randomly generated networks from the same distribution perform similarly, which suggest we should search for random graph distributions instead of graphs. We propose graphon as a new search space. A graphon is the limit of Cauchy sequence of graphs and a scale-free probabilistic distribution, from which graphs of different number of vertices can be drawn. This property enables us to perform NAS using fast, low-capacity models and scale the found models up when necessary. We develop an algorithm for NAS in the space of graphons and empirically demonstrate that it can find stage-wise graphs that outperform DenseNet and other baselines on ImageNet. ", "pdf": "/pdf/bf6daea9d9c7b272d16bea44974e39d8cb755c3b.pdf", "paperhash": "zhou|searching_for_stagewise_neural_graphs_in_the_limit", "original_pdf": "/attachment/466cebc922fa146c1a2c1267d599a8ce5cc360ff.pdf", "_bibtex": "@misc{\nzhou2020searching,\ntitle={Searching for Stage-wise Neural Graphs In the Limit},\nauthor={Xin Zhou and Dejing Dou and Boyang Li},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxWnkStvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkxWnkStvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1941/Authors", "ICLR.cc/2020/Conference/Paper1941/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1941/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1941/Reviewers", "ICLR.cc/2020/Conference/Paper1941/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1941/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1941/Authors|ICLR.cc/2020/Conference/Paper1941/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148643, "tmdate": 1576860534687, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1941/Authors", "ICLR.cc/2020/Conference/Paper1941/Reviewers", "ICLR.cc/2020/Conference/Paper1941/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1941/-/Official_Comment"}}}, {"id": "HyxpXICoor", "original": null, "number": 2, "cdate": 1573803556927, "ddate": null, "tcdate": 1573803556927, "tmdate": 1573803556927, "tddate": null, "forum": "SkxWnkStvS", "replyto": "BklbTSL0Fr", "invitation": "ICLR.cc/2020/Conference/Paper1941/-/Official_Comment", "content": {"title": "Responses to reviewer #2", "comment": "We thank the reviewer for useful insight and comments. Here are responses to individual questions.\n\n1.  It simply ignore all other NAS works and just compares with the baseline DenseNet and random deletion/walk (WS-G). \n\nMost works on NAS are concerned with the structure of a single cell. After a cell is found, many cells are stacked on top of each other in order to build large-capacity models. This approach is orthogonal and complementary to our work, which is concerned with the connections among such cells. Thus, a direct comparison with these works would not provide evidence that could support or contradict our main claim.\n \nFew papers aim to optimize the stage-wise graph. This is at least partially due to the lack of methods to scale a small graph learned on small datasets to match the needs of a large dataset, which this paper provides. We did compare with an existing work that considers the stage-wise graph, which is the WS model found by the randomly wired network paper. Xie et al. (2019) showed that the WS model is competitive with several NAS works including AmoebaNet, PNAS and DARTS.\n\nDespite that, the gain (accuracy +0.17% than DenseNet baseline) is very marginal compared to other approaches:  random-wire (accuracy +2% than resent50 baseline), FBNet (accuracy +2% than MobileNetv2 baseline).\n\nAs discussed in the general response (1a), we have updated the paper with more experiments with improved results (up to 0.8% over DenseNet). The main goal of the experiments is to create fair comparisons and isolate the effect of the stage-wise\n\n2. According to Section 5.1, the search is performed on CIFAR-10, but there is no evaluation on CIFAR-10 at all. The only results are reported for ImageNet instead, which is kind of strange.\n\nAs of results on CIFAR-10, recent performance improvements on are mostly achieved by regularization techniques rather than neural architecture. For this reason, we are afraid that CIFAR-10 may not have enough discriminating capability to separate different baselines. Instead, we added many more experiments, including on the newly proposed ImageNet V2 test set. Some results we have on CIFAR-10 are: 93.80% for WS and 93.93% for the graph we found.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1941/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1941/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Searching for Stage-wise Neural Graphs In the Limit", "authors": ["Xin Zhou", "Dejing Dou", "Boyang Li"], "authorids": ["chow459@gmail.com", "doudejing@baidu.com", "libo0001@gmail.com"], "keywords": ["neural architecture search", "graphon", "random graphs"], "TL;DR": "Graphon is a good search space for neural architecture search and empirically produces good networks.", "abstract": "Search space is a key consideration for neural architecture search. Recently, Xie et al. (2019a) found that randomly generated networks from the same distribution perform similarly, which suggest we should search for random graph distributions instead of graphs. We propose graphon as a new search space. A graphon is the limit of Cauchy sequence of graphs and a scale-free probabilistic distribution, from which graphs of different number of vertices can be drawn. This property enables us to perform NAS using fast, low-capacity models and scale the found models up when necessary. We develop an algorithm for NAS in the space of graphons and empirically demonstrate that it can find stage-wise graphs that outperform DenseNet and other baselines on ImageNet. ", "pdf": "/pdf/bf6daea9d9c7b272d16bea44974e39d8cb755c3b.pdf", "paperhash": "zhou|searching_for_stagewise_neural_graphs_in_the_limit", "original_pdf": "/attachment/466cebc922fa146c1a2c1267d599a8ce5cc360ff.pdf", "_bibtex": "@misc{\nzhou2020searching,\ntitle={Searching for Stage-wise Neural Graphs In the Limit},\nauthor={Xin Zhou and Dejing Dou and Boyang Li},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxWnkStvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkxWnkStvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1941/Authors", "ICLR.cc/2020/Conference/Paper1941/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1941/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1941/Reviewers", "ICLR.cc/2020/Conference/Paper1941/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1941/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1941/Authors|ICLR.cc/2020/Conference/Paper1941/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148643, "tmdate": 1576860534687, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1941/Authors", "ICLR.cc/2020/Conference/Paper1941/Reviewers", "ICLR.cc/2020/Conference/Paper1941/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1941/-/Official_Comment"}}}, {"id": "Hyxu-HCojB", "original": null, "number": 1, "cdate": 1573803264124, "ddate": null, "tcdate": 1573803264124, "tmdate": 1573803264124, "tddate": null, "forum": "SkxWnkStvS", "replyto": "SkxWnkStvS", "invitation": "ICLR.cc/2020/Conference/Paper1941/-/Official_Comment", "content": {"title": "Common responses to all reviewers", "comment": "We thank the reviewers for valuable comments and responses. \n\nWe have uploaded a revised version of the paper including the following changes. \nMore extensive experiments on bigger networks and an additional test set, ImageNet V2, which provides a more accurate estimate of generalization performance. The same method on bigger graphs yields bigger performance gaps up to 0.8% over DenseNet. \n\nImprovements in writing to further clarify our main points.\n\nOn our contribution: Most existing work on architecture transfer in NAS focus on the cell structure, which is stacked consequentially to build large networks. In this paper, we study the problem of transfering and expanding the stage-wise graph from a small dataset to a large dataset. We fill a gap in NAS research because (1) few work investigated the search for stage-wise graphs and (2) there is no known algorithm for transferring small stage-wise graphs.\nTo validate our approach, we applied the transfer technique on two graphs. First, We expand the WS(4, 0.25) graph, defined on 32 nodes, to the graph of 64 nodes used in Denset-264. Second, we expand the 11-node graph we found on CIFAR-10 to various DenseNet settings.  We showed that, after expansion, both maintain their performance lead over DenseNet. \n\nThe purpose of our experiment is to show that this approach is feasible and beneficial under fair comparisons. We use the same setup as much as possible across all baselines. We feel this should be encouraged as this helps in isolating the contribution of the proposed technique. \n\nAs of results on CIFAR-10, recent performance improvements on are mostly achieved by regularization techniques rather than neural architecture. For this reason, we are afraid that CIFAR-10 may not have enough discriminating capability to separate different baselines. Instead, we added many more experiments, including on the newly proposed ImageNet V2 test set. Some results we have on CIFAR-10 are: 93.80% for WS and 93.93% for the graph we found. \nA small technical comment is that we improved the accuracies of the DenseNet-121 group due to improved use of the PyTorch API (switching to nn.sequential improves performance)"}, "signatures": ["ICLR.cc/2020/Conference/Paper1941/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1941/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Searching for Stage-wise Neural Graphs In the Limit", "authors": ["Xin Zhou", "Dejing Dou", "Boyang Li"], "authorids": ["chow459@gmail.com", "doudejing@baidu.com", "libo0001@gmail.com"], "keywords": ["neural architecture search", "graphon", "random graphs"], "TL;DR": "Graphon is a good search space for neural architecture search and empirically produces good networks.", "abstract": "Search space is a key consideration for neural architecture search. Recently, Xie et al. (2019a) found that randomly generated networks from the same distribution perform similarly, which suggest we should search for random graph distributions instead of graphs. We propose graphon as a new search space. A graphon is the limit of Cauchy sequence of graphs and a scale-free probabilistic distribution, from which graphs of different number of vertices can be drawn. This property enables us to perform NAS using fast, low-capacity models and scale the found models up when necessary. We develop an algorithm for NAS in the space of graphons and empirically demonstrate that it can find stage-wise graphs that outperform DenseNet and other baselines on ImageNet. ", "pdf": "/pdf/bf6daea9d9c7b272d16bea44974e39d8cb755c3b.pdf", "paperhash": "zhou|searching_for_stagewise_neural_graphs_in_the_limit", "original_pdf": "/attachment/466cebc922fa146c1a2c1267d599a8ce5cc360ff.pdf", "_bibtex": "@misc{\nzhou2020searching,\ntitle={Searching for Stage-wise Neural Graphs In the Limit},\nauthor={Xin Zhou and Dejing Dou and Boyang Li},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxWnkStvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkxWnkStvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1941/Authors", "ICLR.cc/2020/Conference/Paper1941/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1941/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1941/Reviewers", "ICLR.cc/2020/Conference/Paper1941/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1941/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1941/Authors|ICLR.cc/2020/Conference/Paper1941/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148643, "tmdate": 1576860534687, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1941/Authors", "ICLR.cc/2020/Conference/Paper1941/Reviewers", "ICLR.cc/2020/Conference/Paper1941/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1941/-/Official_Comment"}}}, {"id": "B1xkzupttH", "original": null, "number": 1, "cdate": 1571571719095, "ddate": null, "tcdate": 1571571719095, "tmdate": 1572972403785, "tddate": null, "forum": "SkxWnkStvS", "replyto": "SkxWnkStvS", "invitation": "ICLR.cc/2020/Conference/Paper1941/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors propose a new search space based on graphons and explore some of its benefits such as certain theoretical properties. The architecture search shares similarities with DARTS. An important difference is that the network parameters are not shared.\nThe paper is well-written and the authors consider that the typical reader will not be familiar with graphons. I agree that their proposed model allows for more architectures but in practice it is not much stronger than WS-G. The argumentation with respect to parameters is unclear to me. On one hand, you manually influence the number of parameters, on the other you argue that you use less parameters. Obviously, you chose that your baselines have more parameters. How do results for WS-G look like if you reduce its parameters to match yours? In fact, you were searching for an architecture on CIFAR-10 but you did not report your results here. Instead you only report your transferred results to ImageNet. Is it possible that you also report results on CIFAR-10? Finally, you do not discuss that your graph contains only one kind of node. In many NAS methods the search space contains various types of operations. Do you think this is a problem? Is there a trivial way to extend your method to cover this as well?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1941/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1941/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Searching for Stage-wise Neural Graphs In the Limit", "authors": ["Xin Zhou", "Dejing Dou", "Boyang Li"], "authorids": ["chow459@gmail.com", "doudejing@baidu.com", "libo0001@gmail.com"], "keywords": ["neural architecture search", "graphon", "random graphs"], "TL;DR": "Graphon is a good search space for neural architecture search and empirically produces good networks.", "abstract": "Search space is a key consideration for neural architecture search. Recently, Xie et al. (2019a) found that randomly generated networks from the same distribution perform similarly, which suggest we should search for random graph distributions instead of graphs. We propose graphon as a new search space. A graphon is the limit of Cauchy sequence of graphs and a scale-free probabilistic distribution, from which graphs of different number of vertices can be drawn. This property enables us to perform NAS using fast, low-capacity models and scale the found models up when necessary. We develop an algorithm for NAS in the space of graphons and empirically demonstrate that it can find stage-wise graphs that outperform DenseNet and other baselines on ImageNet. ", "pdf": "/pdf/bf6daea9d9c7b272d16bea44974e39d8cb755c3b.pdf", "paperhash": "zhou|searching_for_stagewise_neural_graphs_in_the_limit", "original_pdf": "/attachment/466cebc922fa146c1a2c1267d599a8ce5cc360ff.pdf", "_bibtex": "@misc{\nzhou2020searching,\ntitle={Searching for Stage-wise Neural Graphs In the Limit},\nauthor={Xin Zhou and Dejing Dou and Boyang Li},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxWnkStvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkxWnkStvS", "replyto": "SkxWnkStvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1941/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1941/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576486689050, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1941/Reviewers"], "noninvitees": [], "tcdate": 1570237730072, "tmdate": 1576486689064, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1941/-/Official_Review"}}}, {"id": "BklbTSL0Fr", "original": null, "number": 2, "cdate": 1571870136678, "ddate": null, "tcdate": 1571870136678, "tmdate": 1572972403746, "tddate": null, "forum": "SkxWnkStvS", "replyto": "SkxWnkStvS", "invitation": "ICLR.cc/2020/Conference/Paper1941/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes a new graphon-based search space. Unlike most other NAS works that search for exact network structures, this paper aims to search for the random graph distribution with graphon. Overall, it provides some new angles for NAS search space design, but the experimental results are very weak.\n\n1.  It simply ignore all other NAS works and just compares with the baseline DenseNet and random deletion/walk (WS-G). Despite that, the gain (accuracy +0.17% than DenseNet baseline) is very marginal compared to other approaches:  random-wire (accuracy +2% than resent50 baseline), FBNet (accuracy +2% than MobileNetv2 baseline).\n2. According to Section 5.1, the search is performed on CIFAR-10, but there is no evaluation on CIFAR-10 at all. The only results are reported for ImageNet instead, which is kind of strange.\n\nGiven these weak results, I cannot accept this paper in the current form."}, "signatures": ["ICLR.cc/2020/Conference/Paper1941/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1941/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Searching for Stage-wise Neural Graphs In the Limit", "authors": ["Xin Zhou", "Dejing Dou", "Boyang Li"], "authorids": ["chow459@gmail.com", "doudejing@baidu.com", "libo0001@gmail.com"], "keywords": ["neural architecture search", "graphon", "random graphs"], "TL;DR": "Graphon is a good search space for neural architecture search and empirically produces good networks.", "abstract": "Search space is a key consideration for neural architecture search. Recently, Xie et al. (2019a) found that randomly generated networks from the same distribution perform similarly, which suggest we should search for random graph distributions instead of graphs. We propose graphon as a new search space. A graphon is the limit of Cauchy sequence of graphs and a scale-free probabilistic distribution, from which graphs of different number of vertices can be drawn. This property enables us to perform NAS using fast, low-capacity models and scale the found models up when necessary. We develop an algorithm for NAS in the space of graphons and empirically demonstrate that it can find stage-wise graphs that outperform DenseNet and other baselines on ImageNet. ", "pdf": "/pdf/bf6daea9d9c7b272d16bea44974e39d8cb755c3b.pdf", "paperhash": "zhou|searching_for_stagewise_neural_graphs_in_the_limit", "original_pdf": "/attachment/466cebc922fa146c1a2c1267d599a8ce5cc360ff.pdf", "_bibtex": "@misc{\nzhou2020searching,\ntitle={Searching for Stage-wise Neural Graphs In the Limit},\nauthor={Xin Zhou and Dejing Dou and Boyang Li},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxWnkStvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkxWnkStvS", "replyto": "SkxWnkStvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1941/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1941/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576486689050, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1941/Reviewers"], "noninvitees": [], "tcdate": 1570237730072, "tmdate": 1576486689064, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1941/-/Official_Review"}}}], "count": 7}