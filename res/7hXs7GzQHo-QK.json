{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1363649760000, "tcdate": 1363649760000, "number": 1, "id": "g05Ygn6IJZ0iX", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "7hXs7GzQHo-QK", "replyto": "E6HmsiyOvphK_", "signatures": ["Charles Cadieu"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your review and feedback. (>-mark indicates quote from review)\r\n\r\n> Because of the many design choices to be made in reducing neural data to a feature representation (the use of multi units rather than singular units, time averaging, short presentation times--many of which are discussed by the authors in the text), the resulting V4/IT performance is likely a lower bound on the true performance. To surpass a lower bound is good news, but to be a useful metric for future research efforts, this lower bound would should lie above current models' performance. The fact that the Krizhevsky model already outperforms V4/IT means there is less reason to compare future representation algorithms using the proposed metric in its current form.\r\n\r\nThese are good points.  We did not know what to expect before we began measuring models and have been quite surprised by the performance of the Krizhevsky et al. model.  Even given that this model surpasses IT, we still believe it is a relevant benchmark for algorithmic research.  There are many interesting factors that go into the performance that will be worthwhile exploring, especially those related to efficiency (our opinion).\r\n\r\nFurthermore, given the assumed \u201clower-bound\u201d nature of the neural representation, we hope that this effort will encourage experimentalists to collect higher lower-bounds of the neural representation.  Ideally, over time, we imagine a scenario similar to the progression in computer vision of increasingly challenging benchmarks of neural representation.\r\n\r\n> The kernel analysis metric asks whether neural and artificial data can achieve similar classification performance for a given model complexity, but this is a separate question from asking whether the neural representation is similar to the artificial representation; e.g., for a classification task, one could imagine many different pairwise similarity structures that would remain linearly separable (or said with the standard metaphor, both a bird and a plane can fly, but rely on different mechanisms). While some aspects of the neural response may be task irrelevant, it may be complementary to augment the KA-AUC approach with a similarity-based approach. This could also be computed from the collected data and would help map levels within a computational model to visual brain areas. In general a more extensive discussion of and contrast with the Kriegeskorte approach would be helpful.\r\n\r\nThis is a very good point.  We think matching neural and model representations at ever increasing levels of detail is an important pursuit.  Generally, we consider a sort of \u201chierarchy of measures\u201d of increasing specificity between neural responses and model responses.  The one we have proposed here is relatively abstract, and task dependent by intention.  The methods and approach of Kriegeskorte measures a, relatively, more constraining mapping between neural and model representations.  As the current manuscript is longer than the conference organizers had hoped, we will reserve a more extensive discussion of the Kriegeskorte approach for a longer journal version of the manuscript.  In ultimately choosing a measure, which level of abstraction one chooses to be satisfied with is largely dependent on one\u2019s goals."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "The Neural Representation Benchmark and its Evaluation on Brain and\r\n    Machine", "decision": "conferenceOral-iclr2013-conference", "abstract": "A key requirement for the development of effective learning representations is their evaluation and comparison to representations we know to be effective. In natural sensory domains, the community has viewed the brain as a source of inspiration and as an implicit benchmark for success. However, it has not been possible to directly test representational learning algorithms directly against the representations contained in neural systems. Here, we propose a new benchmark for visual representations on which we have directly tested the neural representation in multiple visual cortical areas in macaque (utilizing data from [Majaj et al., 2012]), and on which any computer vision algorithm that produces a feature space can be tested. The benchmark measures the effectiveness of the neural or machine representation by computing the classification loss on the ordered eigendecomposition of a kernel matrix [Montavon et al., 2011]. In our analysis we find that the neural representation in visual area IT is superior to visual area V4, indicating an increase in representational performance in higher levels of the cortical visual hierarchy. In our analysis of representational learning algorithms, we find that a number of current algorithms approach the representational performance of V4. Impressively, we find that a recent supervised algorithm [Krizhevsky et al., 2012] achieves performance equal to that of IT for an intermediate level of image variation difficulty, and performs between V4 and IT at a higher difficulty level. We believe this result represents a major milestone: it is the first learning algorithm we have found that produces a representation on par with IT on this task of intermediate difficulty. We hope that this benchmark will serve as an initial rallying point for further correspondence between representations derived in brains and machines.", "pdf": "https://arxiv.org/abs/1301.3530", "paperhash": "cadieu|the_neural_representation_benchmark_and_its_evaluation_on_brain_and_machine", "keywords": [], "conflicts": [], "authors": ["Charles Cadieu", "Ha Hong", "Dan Yamins", "Nicolas Pinto", "Najib J. Majaj", "James J. DiCarlo"], "authorids": ["c.cadieu@gmail.com", "hahong@mit.edu", "yamins@mit.edu", "nicolas.pinto@gmail.com", "najib.majaj@nyu.edu", "dicarlo@mit.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363649700000, "tcdate": 1363649700000, "number": 1, "id": "bbQXGy3KgUrcP", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "7hXs7GzQHo-QK", "replyto": "zzS1zF0bHj6V7", "signatures": ["Charles Cadieu"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your review and feedback.  Here are some specific replies. (>-mark indicates quote from review)\r\n\r\n> * The two macaque subjects in the study by Majaj et al (2012) are unlikely to have been exposed to images of 3 object categories in the dataset: cars, planes or other animals such as cows and elephants. They may have been exposed to images from the 4 remaining object classes: faces, chairs, tables and fruits. By consequence, their V4 or IT cortical areas might not be trained to recognize, even after prolonged exposure, that the image of a car at an angle is still a car with a variation, and not another type of objects. The authors do raise the question whether the neural representation could be enhanced with increased exposure.\r\n\r\nSome additional information, not included in the paper:\r\n\t* Our data suggest that during the passive viewing paradigm there is no change in classifier performance trained on the early part of the recording vs. a later part of the recording.  So we see no exposure dependent classifier improvement through time.  \r\n\t* When examining per-category classifier performance, there is no obvious pattern between the two sets of categories you point out (cars/planes/animals vs. faces/chairs/tables/fruits).\r\n\t* The absolute performance of classifiers trained for Cars, or Planes or Animals does not seem to be significantly different from classifiers trained on the other categories.\r\nIt remains an interesting question how the neural representational performance would change through training the animal to make the desired categorizations.\r\n\r\n> * The paper does mention that only about a hundred sites, on the cortex surface, are selected for the image categorization task, compared to all the tens of thousands of hidden units in the deep architecture. Some further discussion on the fairness of such a comparison would be welcome.\r\n\r\nOne important point is that the measure we have chosen, by measuring accuracy against complexity, allows us to compare representations of different dimensionality.  How a representation is affected by subsampling depends on the properties of that representation, and it appears that the neural representation is quite robust to such subsampling.  For example, we have attempted to estimate the convergence of our measure as we increase the number of recording sites, from within our sample.  It has been somewhat surprising to us that this curve appears to asymptote so quickly, but of course this may be due to a sampling bias in the procedure.\r\n\r\nThere are a number of factors that may bias the neural results related to sampling such a small number of sites from the cortex.  Here is a short discussion of some of these factors:\r\n\t* Neurons that are close together in cortical space are typically correlated.  This indicates that the number of relevant dimensions is far less than the total number of neurons in cortex.  This is in-line with the fast convergence we observe of our measurement with increasing the number of sites.\r\n\t* The placement of the grids, and the spacing between electrodes in the grid may affect our measurement.\r\n\t* We examine multi-unit activity, instead of individual neurons.  At the least this indicates that we are recording from more neurons than the number of sites.  We estimate that the number of total neurons we are recording from is about 5x times the number of multi-units (estimated using the spike count ratio between multi-units and single-units collected in V4 and IT in our lab).  It is not clear how single units would change our result, if at all.\r\n\t* A point that may not be obvious is that an inherit property of electrophysiology is that we are \u201cblind\u201d to the neurons that do not fire during our experimental procedure.  Therefore, we may be introducing a bias by recording from only active neurons and \u201cdiscarding\u201d neurons that are not active.  This would also introduce an underestimate to the number of potential neurons we are effectively recording.  Note that including such silent neurons would not affect our kernel analysis measure, just the estimate of the total number of neurons we recorded.\r\n\t* We have a hardware limitation that limits us to recording 128 sites at a time.  For a given animal, we chose the top-128 best visually driven sites.  \u201cVisual drivenness\u201d was measured with a separate pilot image set (see Rust and DiCarlo 2012 and Chou et al.).  Roughly this measure is the mean across the top 10% of absolute per-image d-primes between an image and blank.  The top 10% is cross-validated and the absolute value is necessary to account for inhibitory sites.  This sampling bias may affect our measure by discarding neural activity not relevant for the task, thus increasing or KA-AUC estimate of the neural representation.\r\n\r\nOne final point, at this technological point in time, we are only able to record from 128 multi-unit sites simultaneously.  We achieve the total number of sites through multiple recording sessions and multiple animals.  Given these limitations, this dataset is cutting-edge in terms of the number of sites, the number of images presented, and the number of repetitions of each image, especially for IT cortex recordings.\r\n\r\n> The Gaussian kernel uses a single coefficient sigma for all the features (i.e., all the neurons / hidden units). On one hand, the neural data are taken on the visual cortex areas V4 and IT, where all the electrode sites are expected to measure information that is relevant for image recognition tasks in general, and the deep learning architectures were all trained on image classification tasks. On the other hand, not all the features (hidden units or electrode sites) are equally relevant, all the time, to all these tasks, but their values are all scaled nevertheless. Would it make sense to tune the individual per-feature sigma coefficients in the Gaussian kernel, as in Chapelle et al (2002) 'Choosing multiple parameters for support vector machines'?\r\n\r\nUnder our proposed methodology, modifying the representation, even by rescaling dimensions, during test time is not allowed.  It would be reasonable to take a representation, apply the method in Chapelle et al (2002) on the training set, and thus create a new representation to be used during testing.  This sounds like a good idea, and we are interested to see what else the community comes up with!\r\n\r\n> Are all the 5 references by Pinto et al. necessary for this paper?\r\n\r\nMost are, but we will remove the Cosyne 2010 abstract and the FG 2011 paper in the next revision, as these points are covered by the remaining references.\r\n\r\n> The authors do not indicate how the images from the dataset were split among the two monkeys (were they shown the same images, or two, different, random sets of images?) and how the neural observations from the different electrode sites (58 IT and 70 V4 sites on one monkey, 110 IT and 58 V4 sites on the other monkey) were grouped. My guess is that the same sets of images were shown to the two monkeys and that their responses were concatenated into IT or V4 matrices of site vs image.  \r\n\r\nYou are correct.  The same sets of images (all of them) were shown to each of the monkeys.  The sites from each monkey IT cortex were concatenated, as were the sites from each monkey V4 cortex.  We will update the text to clarify.\r\n\r\n> The authors do not need to mention the low computational complexity of the LSE loss (section 2.2). It is not more complex than the logistic loss and the real point is what they say about intra-class variance and inter-class variance.  \r\n\r\nThanks for the feedback, we will update the text.\r\n\r\n> I do not fully understand the protocol in section 2.3, namely: 'we evaluate 10 pre-defined subsets of images, each taking 80% of the data from each variation level'.  \r\n\r\nWe have updated the text to clarify.:\r\nFor each variation level, we compute the kernel analysis curve and KA-AUC ten times, each time sampling 80% of the images with replacement.  The ten samples for each variation level are fixed for all representations.\r\n\r\n> Is total dimensionality D equal to the number of samples n?\r\n\r\nYes.  We indicate this now in the text.\r\n\r\nWill update arXiv posting shortly."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "The Neural Representation Benchmark and its Evaluation on Brain and\r\n    Machine", "decision": "conferenceOral-iclr2013-conference", "abstract": "A key requirement for the development of effective learning representations is their evaluation and comparison to representations we know to be effective. In natural sensory domains, the community has viewed the brain as a source of inspiration and as an implicit benchmark for success. However, it has not been possible to directly test representational learning algorithms directly against the representations contained in neural systems. Here, we propose a new benchmark for visual representations on which we have directly tested the neural representation in multiple visual cortical areas in macaque (utilizing data from [Majaj et al., 2012]), and on which any computer vision algorithm that produces a feature space can be tested. The benchmark measures the effectiveness of the neural or machine representation by computing the classification loss on the ordered eigendecomposition of a kernel matrix [Montavon et al., 2011]. In our analysis we find that the neural representation in visual area IT is superior to visual area V4, indicating an increase in representational performance in higher levels of the cortical visual hierarchy. In our analysis of representational learning algorithms, we find that a number of current algorithms approach the representational performance of V4. Impressively, we find that a recent supervised algorithm [Krizhevsky et al., 2012] achieves performance equal to that of IT for an intermediate level of image variation difficulty, and performs between V4 and IT at a higher difficulty level. We believe this result represents a major milestone: it is the first learning algorithm we have found that produces a representation on par with IT on this task of intermediate difficulty. We hope that this benchmark will serve as an initial rallying point for further correspondence between representations derived in brains and machines.", "pdf": "https://arxiv.org/abs/1301.3530", "paperhash": "cadieu|the_neural_representation_benchmark_and_its_evaluation_on_brain_and_machine", "keywords": [], "conflicts": [], "authors": ["Charles Cadieu", "Ha Hong", "Dan Yamins", "Nicolas Pinto", "Najib J. Majaj", "James J. DiCarlo"], "authorids": ["c.cadieu@gmail.com", "hahong@mit.edu", "yamins@mit.edu", "nicolas.pinto@gmail.com", "najib.majaj@nyu.edu", "dicarlo@mit.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363649460000, "tcdate": 1363649460000, "number": 1, "id": "RRN_zPMIpEzTn", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "7hXs7GzQHo-QK", "replyto": "fD8BKQYEClkvP", "signatures": ["Charles Cadieu"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your review and feedback.  Here are some comments on your suggestions:\r\n\r\n> The dataset used in the paper is composed of objects that are superposed to an independent background. While authors motivate their choice by controlling the factors of variations in the representation, it would be interesting to know whether machine learning or brain representations benefit most from this particular setting.\r\n\r\nAs you point out, we inevitably have to make trade-offs when designing our experiments.  Your feedback on removing this controlled variation as an interesting question helps us to design future datasets for experiments.\r\n\r\n> This paper also raises the important question of what is the best way of comparing representations. One can wonder, for example, whether the reduced set of kernels considered here (Gaussian kernels with multiple scales) introduces some bias in favor of 'Gaussian-friendly' representations.\r\n\r\nWe agree that exploring the effect of the kernel choice is an interesting direction.  We hope to include this in future work (possibly a longer journal version)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "The Neural Representation Benchmark and its Evaluation on Brain and\r\n    Machine", "decision": "conferenceOral-iclr2013-conference", "abstract": "A key requirement for the development of effective learning representations is their evaluation and comparison to representations we know to be effective. In natural sensory domains, the community has viewed the brain as a source of inspiration and as an implicit benchmark for success. However, it has not been possible to directly test representational learning algorithms directly against the representations contained in neural systems. Here, we propose a new benchmark for visual representations on which we have directly tested the neural representation in multiple visual cortical areas in macaque (utilizing data from [Majaj et al., 2012]), and on which any computer vision algorithm that produces a feature space can be tested. The benchmark measures the effectiveness of the neural or machine representation by computing the classification loss on the ordered eigendecomposition of a kernel matrix [Montavon et al., 2011]. In our analysis we find that the neural representation in visual area IT is superior to visual area V4, indicating an increase in representational performance in higher levels of the cortical visual hierarchy. In our analysis of representational learning algorithms, we find that a number of current algorithms approach the representational performance of V4. Impressively, we find that a recent supervised algorithm [Krizhevsky et al., 2012] achieves performance equal to that of IT for an intermediate level of image variation difficulty, and performs between V4 and IT at a higher difficulty level. We believe this result represents a major milestone: it is the first learning algorithm we have found that produces a representation on par with IT on this task of intermediate difficulty. We hope that this benchmark will serve as an initial rallying point for further correspondence between representations derived in brains and machines.", "pdf": "https://arxiv.org/abs/1301.3530", "paperhash": "cadieu|the_neural_representation_benchmark_and_its_evaluation_on_brain_and_machine", "keywords": [], "conflicts": [], "authors": ["Charles Cadieu", "Ha Hong", "Dan Yamins", "Nicolas Pinto", "Najib J. Majaj", "James J. DiCarlo"], "authorids": ["c.cadieu@gmail.com", "hahong@mit.edu", "yamins@mit.edu", "nicolas.pinto@gmail.com", "najib.majaj@nyu.edu", "dicarlo@mit.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362226860000, "tcdate": 1362226860000, "number": 2, "id": "E6HmsiyOvphK_", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "7hXs7GzQHo-QK", "replyto": "7hXs7GzQHo-QK", "signatures": ["anonymous reviewer d59c"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of The Neural Representation Benchmark and its Evaluation on Brain and\r\n    Machine", "review": "This paper assesses feature learning algorithms by comparing their performance on an object classification task to that of Macaque IT and V4 neurons. The work provides a new dataset of images, an analysis method for comparing feature representations based on kernel analysis, and neural feature vectors recorded from V4 and IT neurons in response to these images. The authors evaluate a number of recent representational learning algorithms, and identify a recent approach based on deep convolutional networks outperforms V4 and IT neurons.\r\n\r\nThe paper is the first of its kind in providing easy tools to evaluate new representations against high level neural visual representations. It's comparison method differs from prior work by investigating representational learning with respect to a task, and hence is less influenced by potentially task-irrelevant idiosyncrasies of the neural response. The final conclusion reached, that recent models are beginning to surpass V4 and IT models, is very interesting. The authors have clearly explained their rationale behind the many design choices required, and their choices seem very reasonable.\r\n\r\nBecause of the many design choices to be made in reducing neural data to a feature representation (the use of multi units rather than singular units, time averaging, short presentation times--many of which are discussed by the authors in the text), the resulting V4/IT performance is likely a lower bound on the true performance. To surpass a lower bound is good news, but to be a useful metric for future research efforts, this lower bound would should lie above current models' performance. The fact that the Krizhevsky model already outperforms V4/IT means there is less reason to compare future representation algorithms using the proposed metric in its current form.\r\n\r\nThe kernel analysis metric asks whether neural and artificial data can achieve similar classification performance for a given model complexity, but this is a separate question from asking whether the neural representation is similar to the artificial representation; e.g., for a classification task, one could imagine many different pairwise similarity structures that would remain linearly separable (or said with the standard metaphor, both a bird and a plane can fly, but rely on different mechanisms). While some aspects of the neural response may be task irrelevant, it may be complementary to augment the KA-AUC approach with a similarity-based approach. This could also be computed from the collected data and would help map levels within a computational model to visual brain areas. In general a more extensive discussion of and contrast with the Kriegeskorte approach would be helpful."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "The Neural Representation Benchmark and its Evaluation on Brain and\r\n    Machine", "decision": "conferenceOral-iclr2013-conference", "abstract": "A key requirement for the development of effective learning representations is their evaluation and comparison to representations we know to be effective. In natural sensory domains, the community has viewed the brain as a source of inspiration and as an implicit benchmark for success. However, it has not been possible to directly test representational learning algorithms directly against the representations contained in neural systems. Here, we propose a new benchmark for visual representations on which we have directly tested the neural representation in multiple visual cortical areas in macaque (utilizing data from [Majaj et al., 2012]), and on which any computer vision algorithm that produces a feature space can be tested. The benchmark measures the effectiveness of the neural or machine representation by computing the classification loss on the ordered eigendecomposition of a kernel matrix [Montavon et al., 2011]. In our analysis we find that the neural representation in visual area IT is superior to visual area V4, indicating an increase in representational performance in higher levels of the cortical visual hierarchy. In our analysis of representational learning algorithms, we find that a number of current algorithms approach the representational performance of V4. Impressively, we find that a recent supervised algorithm [Krizhevsky et al., 2012] achieves performance equal to that of IT for an intermediate level of image variation difficulty, and performs between V4 and IT at a higher difficulty level. We believe this result represents a major milestone: it is the first learning algorithm we have found that produces a representation on par with IT on this task of intermediate difficulty. We hope that this benchmark will serve as an initial rallying point for further correspondence between representations derived in brains and machines.", "pdf": "https://arxiv.org/abs/1301.3530", "paperhash": "cadieu|the_neural_representation_benchmark_and_its_evaluation_on_brain_and_machine", "keywords": [], "conflicts": [], "authors": ["Charles Cadieu", "Ha Hong", "Dan Yamins", "Nicolas Pinto", "Najib J. Majaj", "James J. DiCarlo"], "authorids": ["c.cadieu@gmail.com", "hahong@mit.edu", "yamins@mit.edu", "nicolas.pinto@gmail.com", "najib.majaj@nyu.edu", "dicarlo@mit.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362225300000, "tcdate": 1362225300000, "number": 1, "id": "zzS1zF0bHj6V7", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "7hXs7GzQHo-QK", "replyto": "7hXs7GzQHo-QK", "signatures": ["anonymous reviewer 4738"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of The Neural Representation Benchmark and its Evaluation on Brain and\r\n    Machine", "review": "This paper applies the methodology for 'kernel analysis of deep networks' (Montavon et al, 2011) to the neural code measured on two areas (V4 and IT) on the visual cortex of the macaque. It compares, on the same test set, the biological responses of V4 or IT (spike counts measured at about 100 electrode sites) to the hidden unit activations on the penultimate layer of several state-of-the-art deep learning architectures trained on large image datasets: the 10 million YouTube images and deep sparse auto-encoder paper by Le et al (2012), a convolutional network by Krizhevsky et al (2012), two papers by Pinto et al, one on the V1 model, another on the high throughput L3 model class and the unsupervised learning paper by Coates et al (2012).\r\n\r\nThe authors show that the IT area of the visual cortex seems to have a neural code that is more discriminative than the neural code of the V4 area for a 7-class image categorization task under variations of pose, position and scale. The authors also show that one supervised deep learning algorithm (Krizhevsky et al, 2012) even produces hidden layer representation that seems to outperform IT on that task.\r\n\r\n\r\nPros, novelty and quality:\r\n\r\nThis paper is the first to apply the same method for evaluating feature representations of both the biological neural code (measured on the visual cortex of a primate) and of hidden unit activations in state-of-the-art methods for image classification. It provides an extensive comparison of the penultimate hidden layer of several deep learning algorithms, vs. the V4 and IT areas of the visual cortex of two macaques. As such, it provides insight into which algorithms make a good hidden representation of images.\r\n\r\nThe method for evaluating the feature representations is essentially non-parametric and provides a robust way to assess the complexity of the decision boundary. The kernel analysis method measures what percentage of the information coming from the sample images is required to successfully train a nonlinear Gaussian SVM-like classifier on the features (neural code or hidden unit activations), or a linear classifier in the dual space, for a simple image categorization task. The kernel PCA approach of keeping the top d eigenvectors of the kernel matrix in the dual solution is more robust than the cross-validation performance or than the number of support vectors, when the number of samples is small.\r\n\r\nThe paper is well written, the claims are well supported by the experiments. The metric used in this study is robust and the main results (IT vs V4, Krizhevsky et al 2012 vs IT on high variations) are statistically significant.\r\n\r\nCons:\r\n\r\nThere are no cons per se in this paper, only limitations in the methodology (linked to the choice of the dataset) that could be improved upon by using a more extensive dataset. Most of these limitations have been preemptively mentioned and discussed by the authors in section 4.\r\n\r\n* The two macaque subjects in the study by Majaj et al (2012) are unlikely to have been exposed to images of 3 object categories in the dataset: cars, planes or other animals such as cows and elephants. They may have been exposed to images from the 4 remaining object classes: faces, chairs, tables and fruits. By consequence, their V4 or IT cortical areas might not be trained to recognize, even after prolonged exposure, that the image of a car at an angle is still a car with a variation, and not another type of objects. The authors do raise the question whether the neural representation could be enhanced with increased exposure.\r\n\r\n* The paper does mention that only about a hundred sites, on the cortex surface, are selected for the image categorization task, compared to all the tens of thousands of hidden units in the deep architecture. Some further discussion on the fairness of such a comparison would be welcome.\r\n\r\nOther comments:\r\n\r\n* The Gaussian kernel uses a single coefficient sigma for all the features (i.e., all the neurons / hidden units). On one hand, the neural data are taken on the visual cortex areas V4 and IT, where all the electrode sites are expected to measure information that is relevant for image recognition tasks in general, and the deep learning architectures were all trained on image classification tasks. On the other hand, not all the features (hidden units or electrode sites) are equally relevant, all the time, to all these tasks, but their values are all scaled nevertheless. Would it make sense to tune the individual per-feature sigma coefficients in the Gaussian kernel, as in Chapelle et al (2002) 'Choosing multiple parameters for support vector machines'?\r\n\r\n* Are all the 5 references by Pinto et al. necessary for this paper?\r\n\r\nMinor comments:\r\n\r\n* The authors do not indicate how the images from the dataset were split among the two monkeys (were they shown the same images, or two, different, random sets of images?) and how the neural observations from the different electrode sites (58 IT and 70 V4 sites on one monkey, 110 IT and 58 V4 sites on the other monkey) were grouped. My guess is that the same sets of images were shown to the two monkeys and that their responses were concatenated into IT or V4 matrices of site vs image.\r\n\r\n* The authors do not need to mention the low computational complexity of the LSE loss (section 2.2). It is not more complex than the logistic loss and the real point is what they say about intra-class variance and inter-class variance.\r\n\r\n* I do not fully understand the protocol in section 2.3, namely: 'we evaluate 10 pre-defined subsets of images, each taking 80% of the data from each variation level'.\r\n\r\n* Is total dimensionality D equal to the number of samples n?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "The Neural Representation Benchmark and its Evaluation on Brain and\r\n    Machine", "decision": "conferenceOral-iclr2013-conference", "abstract": "A key requirement for the development of effective learning representations is their evaluation and comparison to representations we know to be effective. In natural sensory domains, the community has viewed the brain as a source of inspiration and as an implicit benchmark for success. However, it has not been possible to directly test representational learning algorithms directly against the representations contained in neural systems. Here, we propose a new benchmark for visual representations on which we have directly tested the neural representation in multiple visual cortical areas in macaque (utilizing data from [Majaj et al., 2012]), and on which any computer vision algorithm that produces a feature space can be tested. The benchmark measures the effectiveness of the neural or machine representation by computing the classification loss on the ordered eigendecomposition of a kernel matrix [Montavon et al., 2011]. In our analysis we find that the neural representation in visual area IT is superior to visual area V4, indicating an increase in representational performance in higher levels of the cortical visual hierarchy. In our analysis of representational learning algorithms, we find that a number of current algorithms approach the representational performance of V4. Impressively, we find that a recent supervised algorithm [Krizhevsky et al., 2012] achieves performance equal to that of IT for an intermediate level of image variation difficulty, and performs between V4 and IT at a higher difficulty level. We believe this result represents a major milestone: it is the first learning algorithm we have found that produces a representation on par with IT on this task of intermediate difficulty. We hope that this benchmark will serve as an initial rallying point for further correspondence between representations derived in brains and machines.", "pdf": "https://arxiv.org/abs/1301.3530", "paperhash": "cadieu|the_neural_representation_benchmark_and_its_evaluation_on_brain_and_machine", "keywords": [], "conflicts": [], "authors": ["Charles Cadieu", "Ha Hong", "Dan Yamins", "Nicolas Pinto", "Najib J. Majaj", "James J. DiCarlo"], "authorids": ["c.cadieu@gmail.com", "hahong@mit.edu", "yamins@mit.edu", "nicolas.pinto@gmail.com", "najib.majaj@nyu.edu", "dicarlo@mit.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362156600000, "tcdate": 1362156600000, "number": 3, "id": "fD8BKQYEClkvP", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "7hXs7GzQHo-QK", "replyto": "7hXs7GzQHo-QK", "signatures": ["anonymous reviewer b28a"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of The Neural Representation Benchmark and its Evaluation on Brain and\r\n    Machine", "review": "The paper presents a benchmark for comparing representations of image data in brains and machines. The benchmark consists of looking at how the image categorization task is encoded in the leading kernel principal components of the representation, thus leading to an analysis of complexity and noise. The paper contains extensive experiments based on a representive set of state-of-the-art learning algorithms on the machine learning side, and real recordings of macaques brain activity on the neural side.\r\n\r\nThe research presented in this paper is well-conducted, timely and highly innovative. It is to my knowledge the first time, that representations obtained with state-of-the-art machine learning techniques for vision are systematically compared with real neural representations. The authors motivate the use of kernel analysis, by the inbuilt robustness to sample size being desirable in this heterogeneous setting.\r\n\r\nThe dataset used in the paper is composed of objects that are superposed to an independent background. While authors motivate their choice by controlling the factors of variations in the representation, it would be interesting to know whether machine learning or brain representations benefit most from this particular setting.\r\n\r\nThis paper also raises the important question of what is the best way of comparing representations. One can wonder, for example, whether the reduced set of kernels considered here (Gaussian kernels with multiple scales) introduces some bias in favor of 'Gaussian-friendly' representations. Also, as suggested by the authors, it could be that the way neural recordings are represented leads to underestimating their discriminative ability."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "The Neural Representation Benchmark and its Evaluation on Brain and\r\n    Machine", "decision": "conferenceOral-iclr2013-conference", "abstract": "A key requirement for the development of effective learning representations is their evaluation and comparison to representations we know to be effective. In natural sensory domains, the community has viewed the brain as a source of inspiration and as an implicit benchmark for success. However, it has not been possible to directly test representational learning algorithms directly against the representations contained in neural systems. Here, we propose a new benchmark for visual representations on which we have directly tested the neural representation in multiple visual cortical areas in macaque (utilizing data from [Majaj et al., 2012]), and on which any computer vision algorithm that produces a feature space can be tested. The benchmark measures the effectiveness of the neural or machine representation by computing the classification loss on the ordered eigendecomposition of a kernel matrix [Montavon et al., 2011]. In our analysis we find that the neural representation in visual area IT is superior to visual area V4, indicating an increase in representational performance in higher levels of the cortical visual hierarchy. In our analysis of representational learning algorithms, we find that a number of current algorithms approach the representational performance of V4. Impressively, we find that a recent supervised algorithm [Krizhevsky et al., 2012] achieves performance equal to that of IT for an intermediate level of image variation difficulty, and performs between V4 and IT at a higher difficulty level. We believe this result represents a major milestone: it is the first learning algorithm we have found that produces a representation on par with IT on this task of intermediate difficulty. We hope that this benchmark will serve as an initial rallying point for further correspondence between representations derived in brains and machines.", "pdf": "https://arxiv.org/abs/1301.3530", "paperhash": "cadieu|the_neural_representation_benchmark_and_its_evaluation_on_brain_and_machine", "keywords": [], "conflicts": [], "authors": ["Charles Cadieu", "Ha Hong", "Dan Yamins", "Nicolas Pinto", "Najib J. Majaj", "James J. DiCarlo"], "authorids": ["c.cadieu@gmail.com", "hahong@mit.edu", "yamins@mit.edu", "nicolas.pinto@gmail.com", "najib.majaj@nyu.edu", "dicarlo@mit.edu"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358460000000, "tcdate": 1358460000000, "number": 32, "id": "7hXs7GzQHo-QK", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "7hXs7GzQHo-QK", "signatures": ["c.cadieu@gmail.com"], "readers": ["everyone"], "content": {"title": "The Neural Representation Benchmark and its Evaluation on Brain and\r\n    Machine", "decision": "conferenceOral-iclr2013-conference", "abstract": "A key requirement for the development of effective learning representations is their evaluation and comparison to representations we know to be effective. In natural sensory domains, the community has viewed the brain as a source of inspiration and as an implicit benchmark for success. However, it has not been possible to directly test representational learning algorithms directly against the representations contained in neural systems. Here, we propose a new benchmark for visual representations on which we have directly tested the neural representation in multiple visual cortical areas in macaque (utilizing data from [Majaj et al., 2012]), and on which any computer vision algorithm that produces a feature space can be tested. The benchmark measures the effectiveness of the neural or machine representation by computing the classification loss on the ordered eigendecomposition of a kernel matrix [Montavon et al., 2011]. In our analysis we find that the neural representation in visual area IT is superior to visual area V4, indicating an increase in representational performance in higher levels of the cortical visual hierarchy. In our analysis of representational learning algorithms, we find that a number of current algorithms approach the representational performance of V4. Impressively, we find that a recent supervised algorithm [Krizhevsky et al., 2012] achieves performance equal to that of IT for an intermediate level of image variation difficulty, and performs between V4 and IT at a higher difficulty level. We believe this result represents a major milestone: it is the first learning algorithm we have found that produces a representation on par with IT on this task of intermediate difficulty. We hope that this benchmark will serve as an initial rallying point for further correspondence between representations derived in brains and machines.", "pdf": "https://arxiv.org/abs/1301.3530", "paperhash": "cadieu|the_neural_representation_benchmark_and_its_evaluation_on_brain_and_machine", "keywords": [], "conflicts": [], "authors": ["Charles Cadieu", "Ha Hong", "Dan Yamins", "Nicolas Pinto", "Najib J. Majaj", "James J. DiCarlo"], "authorids": ["c.cadieu@gmail.com", "hahong@mit.edu", "yamins@mit.edu", "nicolas.pinto@gmail.com", "najib.majaj@nyu.edu", "dicarlo@mit.edu"]}, "writers": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 7}