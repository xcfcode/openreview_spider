{"notes": [{"id": "r1gPtjcH_N", "original": "H1lzM-YB_4", "number": 58, "cdate": 1553472382576, "ddate": null, "tcdate": 1553472382576, "tmdate": 1562082112891, "tddate": null, "forum": "r1gPtjcH_N", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "content": {"title": "Improving Sample Complexity with Observational Supervision", "authors": ["Khaled Saab", "Jared Dunnmon", "Alexander Ratner", "Daniel Rubin", "Christopher Re"], "authorids": ["ksaab@stanford.edu", "jdunnmon@stanford.edu", "ajratner@stanford.edu", "dlrubin@stanford.edu", "chrismre@stanford.edu"], "keywords": ["observational supervision", "eye tracking", "gaze data", "limited labeled data"], "TL;DR": "We explore using passively collected eye-tracking data to reduce the amount of labeled data needed during training.", "abstract": "Supervised machine learning models for high-value computer vision applications such as medical image classification often require large datasets labeled by domain experts, which are slow to collect, expensive to maintain, and static with respect to changes in the data distribution. In this context, we assess the utility of observational supervision, where we take advantage of passively-collected signals such as eye tracking or \u201cgaze\u201d data, to reduce the amount of hand-labeled data needed for model training. Specifically, we leverage gaze information to directly supervise a visual attention layer by penalizing disagreement between the spatial regions the human labeler looked at the longest and those that most heavily influence model output. We present evidence that constraining the model in this way can reduce the number of labeled examples required to achieve a given performance level by as much as 50%, and that gaze information is most helpful on more difficult tasks.", "pdf": "/pdf/fd794bc28c6ead9010f7fb59c539de1273710d2f.pdf", "paperhash": "saab|improving_sample_complexity_with_observational_supervision"}, "signatures": ["ICLR.cc/2019/Workshop/LLD"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "cdate": 1548689671889, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "content": {"authors": {"values-regex": ".*"}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1548689671889, "tmdate": 1557933709646, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/LLD"], "details": {"writable": true}}}, "tauthor": "OpenReview.net"}, {"id": "SyxHVDhDF4", "original": null, "number": 1, "cdate": 1554659116525, "ddate": null, "tcdate": 1554659116525, "tmdate": 1555512018908, "tddate": null, "forum": "r1gPtjcH_N", "replyto": "r1gPtjcH_N", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper58/Official_Review", "content": {"title": "Interesting idea with encouraging initial results", "review": "This work proposed to use gaze information in order to reduce the sample complexity of a model and the needed labeling effort to get a target performance. The proposed method uses an attention layer and adds a penalty to reduce the gap between downsampled human attention maps and class activation maps. The experimental results show an improvement especially for middle sized samples, and a higher effect for harder tasks.\n\nThe idea and method is overall interesting, and would be interesting to discuss. One direction to build on this work, other than extending the experiments to more complex, larger scale applications, is to try to leverage more information from the human attention maps, as downsampling throws away some of the information that can be interesting for training.", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper58/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper58/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Sample Complexity with Observational Supervision", "authors": ["Khaled Saab", "Jared Dunnmon", "Alexander Ratner", "Daniel Rubin", "Christopher Re"], "authorids": ["ksaab@stanford.edu", "jdunnmon@stanford.edu", "ajratner@stanford.edu", "dlrubin@stanford.edu", "chrismre@stanford.edu"], "keywords": ["observational supervision", "eye tracking", "gaze data", "limited labeled data"], "TL;DR": "We explore using passively collected eye-tracking data to reduce the amount of labeled data needed during training.", "abstract": "Supervised machine learning models for high-value computer vision applications such as medical image classification often require large datasets labeled by domain experts, which are slow to collect, expensive to maintain, and static with respect to changes in the data distribution. In this context, we assess the utility of observational supervision, where we take advantage of passively-collected signals such as eye tracking or \u201cgaze\u201d data, to reduce the amount of hand-labeled data needed for model training. Specifically, we leverage gaze information to directly supervise a visual attention layer by penalizing disagreement between the spatial regions the human labeler looked at the longest and those that most heavily influence model output. We present evidence that constraining the model in this way can reduce the number of labeled examples required to achieve a given performance level by as much as 50%, and that gaze information is most helpful on more difficult tasks.", "pdf": "/pdf/fd794bc28c6ead9010f7fb59c539de1273710d2f.pdf", "paperhash": "saab|improving_sample_complexity_with_observational_supervision"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper58/Official_Review", "cdate": 1553713411849, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "r1gPtjcH_N", "replyto": "r1gPtjcH_N", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper58/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper58/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713411849, "tmdate": 1555511822010, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper58/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "Skg_bLZFtE", "original": null, "number": 2, "cdate": 1554744831950, "ddate": null, "tcdate": 1554744831950, "tmdate": 1555511882104, "tddate": null, "forum": "r1gPtjcH_N", "replyto": "r1gPtjcH_N", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper58/Official_Review", "content": {"title": "Simple idea to incorporate gaze data into standard CNN architectures for image classification.", "review": "This paper presents a simple method to incorporate gaze signals into standard CNNs for image classification, adding an extra term in the loss function. The term is based in the difference between the Class Activation Map obtained from the model, and the human map constructed using the eye tracking information. The authors apply their method to the POET dataset and report interesting results when using different sizes for the training set. They show that the gazed network achieved equivalent performance to that of a standard CNN using less training data for intermediate data regimes.\n\nThe paper well written. It presents a simple idea which has a lot of potential, specially in the context of medical data (as suggested by the authors in their planned future works). Some comments I would like to see in the camera ready version of this work:\n\n- It is not clear how the human attention map is constructed. The authors just say that this is obtained by \u201cintegrating the eye tracking signal in time\u201d. Since this is a crucial element in their framework, I would like to see a detailed description of how this is obtained. If space constraint is a problem, you could just add an appendix section with this info.\n- In the orange line in Figure 2 (the line associated to the standard CNN) I do not see the std. This value is reported in table 2 (Appendix B), so I guess this can be a problem related to image transparency. Please, fix this problem so that we can see the confidence interval for the standard CNN as we can do with the gazed CNN.\n- Do you think this idea could be also useful to improve image segmentation based on CNNs with limited data?\n\nMinor corrections:\n- In Section 4: \u201cTo test the this hypothesis\u201d should be \u201cTo test this hypothesis\u201d.\n", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper58/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper58/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Sample Complexity with Observational Supervision", "authors": ["Khaled Saab", "Jared Dunnmon", "Alexander Ratner", "Daniel Rubin", "Christopher Re"], "authorids": ["ksaab@stanford.edu", "jdunnmon@stanford.edu", "ajratner@stanford.edu", "dlrubin@stanford.edu", "chrismre@stanford.edu"], "keywords": ["observational supervision", "eye tracking", "gaze data", "limited labeled data"], "TL;DR": "We explore using passively collected eye-tracking data to reduce the amount of labeled data needed during training.", "abstract": "Supervised machine learning models for high-value computer vision applications such as medical image classification often require large datasets labeled by domain experts, which are slow to collect, expensive to maintain, and static with respect to changes in the data distribution. In this context, we assess the utility of observational supervision, where we take advantage of passively-collected signals such as eye tracking or \u201cgaze\u201d data, to reduce the amount of hand-labeled data needed for model training. Specifically, we leverage gaze information to directly supervise a visual attention layer by penalizing disagreement between the spatial regions the human labeler looked at the longest and those that most heavily influence model output. We present evidence that constraining the model in this way can reduce the number of labeled examples required to achieve a given performance level by as much as 50%, and that gaze information is most helpful on more difficult tasks.", "pdf": "/pdf/fd794bc28c6ead9010f7fb59c539de1273710d2f.pdf", "paperhash": "saab|improving_sample_complexity_with_observational_supervision"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper58/Official_Review", "cdate": 1553713411849, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "r1gPtjcH_N", "replyto": "r1gPtjcH_N", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper58/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper58/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713411849, "tmdate": 1555511822010, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper58/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "r1g5kgNYKV", "original": null, "number": 1, "cdate": 1554755553791, "ddate": null, "tcdate": 1554755553791, "tmdate": 1555510985849, "tddate": null, "forum": "r1gPtjcH_N", "replyto": "r1gPtjcH_N", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper58/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Sample Complexity with Observational Supervision", "authors": ["Khaled Saab", "Jared Dunnmon", "Alexander Ratner", "Daniel Rubin", "Christopher Re"], "authorids": ["ksaab@stanford.edu", "jdunnmon@stanford.edu", "ajratner@stanford.edu", "dlrubin@stanford.edu", "chrismre@stanford.edu"], "keywords": ["observational supervision", "eye tracking", "gaze data", "limited labeled data"], "TL;DR": "We explore using passively collected eye-tracking data to reduce the amount of labeled data needed during training.", "abstract": "Supervised machine learning models for high-value computer vision applications such as medical image classification often require large datasets labeled by domain experts, which are slow to collect, expensive to maintain, and static with respect to changes in the data distribution. In this context, we assess the utility of observational supervision, where we take advantage of passively-collected signals such as eye tracking or \u201cgaze\u201d data, to reduce the amount of hand-labeled data needed for model training. Specifically, we leverage gaze information to directly supervise a visual attention layer by penalizing disagreement between the spatial regions the human labeler looked at the longest and those that most heavily influence model output. We present evidence that constraining the model in this way can reduce the number of labeled examples required to achieve a given performance level by as much as 50%, and that gaze information is most helpful on more difficult tasks.", "pdf": "/pdf/fd794bc28c6ead9010f7fb59c539de1273710d2f.pdf", "paperhash": "saab|improving_sample_complexity_with_observational_supervision"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper58/Decision", "cdate": 1554736073326, "reply": {"forum": "r1gPtjcH_N", "replyto": "r1gPtjcH_N", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554736073326, "tmdate": 1555510965531, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}], "count": 4}