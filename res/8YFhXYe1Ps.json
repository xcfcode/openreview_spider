{"notes": [{"id": "8YFhXYe1Ps", "original": "nckQcAUHEeg", "number": 3736, "cdate": 1601308415900, "ddate": null, "tcdate": 1601308415900, "tmdate": 1614985780581, "tddate": null, "forum": "8YFhXYe1Ps", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces", "authorids": ["~Leon_Sixt1", "schuessler@tu-berlin.de", "philipp@itp.tu-berlin.de", "~Tim_Landgraf1"], "authors": ["Leon Sixt", "Martin Schuessler", "Philipp Wei\u00df", "Tim Landgraf"], "keywords": ["Interpretable Machine Learning", "Counterfactuals", "Computer Vision", "Human Evaluation", "User Study"], "abstract": "Current state of the art computer vision applications rely on highly complex models. Their interpretability is mostly limited to post-hoc methods which are not guaranteed to be faithful to the model. To elucidate a model\u2019s decision, we present a novel interpretable model based on an invertible deep convolutional network. Our model generates meaningful, faithful, and ideal counterfactuals. Using PCA on the classifier\u2019s input, we can also create \u201cisofactuals\u201d\u2013 image interpolations with the same outcome but visually meaningful different features. Counter- and isofactuals can be used to identify positive and negative evidence in an image. This can also be visualized with heatmaps. We evaluate our approach against gradient-based attribution methods, which we find to produce meaningless adversarial perturbations.  Using our method, we reveal biases in three different datasets. In a human subject experiment, we test whether non-experts find our method useful to spot spurious correlations learned by a model. Our work is a step towards more trustworthy explanations for computer vision.", "one-sentence_summary": "We use invertible neural networks to generate ideal counterfactuals and isofactuals.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sixt|interpretability_through_invertibility_a_deep_convolutional_network_with_ideal_counterfactuals_and_isosurfaces", "supplementary_material": "/attachment/74b00c8e6e7ea503ac48760cf7c184afada069eb.zip", "pdf": "/pdf/83628964eb4ac5f2e2665ccd6ea038a08a70ff43.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1YGiD4CuCZ", "_bibtex": "@misc{\nsixt2021interpretability,\ntitle={Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces},\nauthor={Leon Sixt and Martin Schuessler and Philipp Wei{\\ss} and Tim Landgraf},\nyear={2021},\nurl={https://openreview.net/forum?id=8YFhXYe1Ps}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "A5Y2Zp-pkh", "original": null, "number": 1, "cdate": 1610040349364, "ddate": null, "tcdate": 1610040349364, "tmdate": 1610473938285, "tddate": null, "forum": "8YFhXYe1Ps", "replyto": "8YFhXYe1Ps", "invitation": "ICLR.cc/2021/Conference/Paper3736/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "All the reviewers agree that the paper presents an interesting idea, and the main concern raised by the reviewers was the clarity of the paper. I believe that the authors have improved the presentation of the paper after rebuttal, however, I still believe that the paper woudl require another round of reviews before being ready for publication, in order to properly assess its contributions. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces", "authorids": ["~Leon_Sixt1", "schuessler@tu-berlin.de", "philipp@itp.tu-berlin.de", "~Tim_Landgraf1"], "authors": ["Leon Sixt", "Martin Schuessler", "Philipp Wei\u00df", "Tim Landgraf"], "keywords": ["Interpretable Machine Learning", "Counterfactuals", "Computer Vision", "Human Evaluation", "User Study"], "abstract": "Current state of the art computer vision applications rely on highly complex models. Their interpretability is mostly limited to post-hoc methods which are not guaranteed to be faithful to the model. To elucidate a model\u2019s decision, we present a novel interpretable model based on an invertible deep convolutional network. Our model generates meaningful, faithful, and ideal counterfactuals. Using PCA on the classifier\u2019s input, we can also create \u201cisofactuals\u201d\u2013 image interpolations with the same outcome but visually meaningful different features. Counter- and isofactuals can be used to identify positive and negative evidence in an image. This can also be visualized with heatmaps. We evaluate our approach against gradient-based attribution methods, which we find to produce meaningless adversarial perturbations.  Using our method, we reveal biases in three different datasets. In a human subject experiment, we test whether non-experts find our method useful to spot spurious correlations learned by a model. Our work is a step towards more trustworthy explanations for computer vision.", "one-sentence_summary": "We use invertible neural networks to generate ideal counterfactuals and isofactuals.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sixt|interpretability_through_invertibility_a_deep_convolutional_network_with_ideal_counterfactuals_and_isosurfaces", "supplementary_material": "/attachment/74b00c8e6e7ea503ac48760cf7c184afada069eb.zip", "pdf": "/pdf/83628964eb4ac5f2e2665ccd6ea038a08a70ff43.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1YGiD4CuCZ", "_bibtex": "@misc{\nsixt2021interpretability,\ntitle={Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces},\nauthor={Leon Sixt and Martin Schuessler and Philipp Wei{\\ss} and Tim Landgraf},\nyear={2021},\nurl={https://openreview.net/forum?id=8YFhXYe1Ps}\n}"}, "tags": [], "invitation": {"reply": {"forum": "8YFhXYe1Ps", "replyto": "8YFhXYe1Ps", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040349347, "tmdate": 1610473938263, "id": "ICLR.cc/2021/Conference/Paper3736/-/Decision"}}}, {"id": "qRc354yIWoL", "original": null, "number": 1, "cdate": 1603801669974, "ddate": null, "tcdate": 1603801669974, "tmdate": 1607422265995, "tddate": null, "forum": "8YFhXYe1Ps", "replyto": "8YFhXYe1Ps", "invitation": "ICLR.cc/2021/Conference/Paper3736/-/Official_Review", "content": {"title": "Interesting idea needing more work", "review": "Update after revision\n------------------------------\nI thank the authors for their work on this paper. The second reading was more pleasant. I agree with the authors that performing a user-study is an important effort, that should be encouraged. I however still believe that, if not benefitial to the user, the complexity of the method can be a drawback. I also wished that more comparisons, but especially other data modalities were investigated. I have updated my rating to reflect the improvement in the text.\n\nShort summary\n-----------------------\nThe authors propose a technique based on an invertible network to provide counterfactuals relative to one class of interest. The counterfactuals can be interpolated across an isosurface, displaying parameters which do not affect the model\u2019s decision. The authors propose an attribution map based on those counterfactuals and evaluate counterfactuals in a qualitative manner, based on their own observations on 3 datasets, as well as based on a human-grounded evaluation on a synthetic dataset. \n\nStrengths\n---------------\nThe use of an invertible dataset is rather novel in the field of explainability, and the relationship between the obtained counterfactuals and gradient-based interpolation methods is interesting. The human-grounded evaluation is definitely a large undertaking that is not often performed to assess the usefulness of interpretability techniques.\n\nWeaknesses\n-------------------\nI have identified several weaknesses of the work that justify my recommendation:\n- the (lack of) clarity of the text.\n- the assessment of the technique, as the results of the human-grounded evaluation are mixed, with users not being significantly more accurate in finding confounding factors compared to a baseline technique.\n- the limitations of the technique, not discussed in depth. For instance, I can see difficulties in evaluating the effect of classes that are not present as \u201ctraining classes\u201d in the dataset, which requires a large labeling effort. In addition, how the technique would transpose to non-image datasets, or whether there are limitations in the invertible architectures to consider should be mentioned.\n\nNovelty\n-----------\nThe \u201cRelated works\u201d section is rather limited, which makes it difficult to evaluate. In general, the use of invertible networks as interpretable networks is novel.\n\nClarity\n---------\nClarity was a major weakness of this work for me:\n- the datasets are illustrated in figures but not mentioned until much later\n-  the maths are described in sections that seem unrelated to each other, without depicting the relationships between the different steps\n- multiple concepts are unclear (see detailed comments)\n- the motivations are not clearly explained\n\nRigor\n--------\nI found the qualitative evaluation on the 3 datasets unconvincing, as it is unclear whether the same conclusions could not have been reached using other techniques.\nWhile I was most interested by the discussion around the generation of counterfactuals based on the invertible network compared to based on the integration of gradients, I wished there was a definition of an \u201cideal\u201d counterfactual, qualitative or (preferably) quantitative. The single example provided in the main text is appealing but this requires more evidence to me.\nFinally, the \u201csaliency\u201d maps defined in this work do not seem to be used later on in the work. I doubt that looking at them would improve human evaluation of a model\u2019s behavior.\n\nDetailed comments\n-----------------------------\n- Counterfactuals: their quality seems subject to appreciation and confirmation bias, especially on potentially cherry picked examples. To assess their quality, I would suggest to use the BAM dataset (Yang and Kim, 2019, https://github.com/google-research-datasets/bam) which was generated to benchmark attribution methods. I would overall suggest the use of this dataset for assessing the faithfulness (sensitivity, specificity) of the proposed approach.\n- The choice of the mice dataset should be justified as this doesn\u2019t seem like an obvious choice to assess the quality of attribution techniques. It is quite difficult to estimate any effect, and feels like qualitative evaluation is biased by the authors\u2019 remarks given the lack of knowledge of the problem.\n- There should be more details about the Two4Two dataset and its motivations, as well as how it relates to other datasets (e.g. Goyal et al., 2019)\n- How does the proposed approach relate to \u201ccompleteness\u201d (Sundararajan et al., 2017)?\n- What is the mathematical justification to resize the saliency map of an intermediate layer to the input resolution? Is there a citation for this process showing that this is a reasonable assumption?\n- I am confused by the section on saliency maps: what does h represent? The activations at an intermediate layer? The motivation is unclear: what are the authors trying to highlight in these \u201csaliency maps\u201d? Are these computed attributions or are these L1 distance between activations (in %) between x and x_tilde? Or is it a cosine distance (as suggested by the next sentence mentioning the angle?)\n- The tasks used for illustration are not described in the text. Examples of y and epsilon should be provided.\n- Is the technique limited to the model\u2019s predicted classes?\n- How is \u201cideal\u201d counterfactual described and mathematically verified?\n- The relationship between counterfactuals and e.g. integrated gradients is unclear: the first clearly needs a model that can generate data, while the latter integrates the gradients between a baseline (defined by the user) and the input. More details and explanations are required to make this relationship clearer.\n- What are the participants in the human-based study viewing? Are they comparing the counterfactuals to e.g. SmoothGrad maps, or the saliency as defined per the proposed approach?\n- It is unclear what the participants answered: Figure 5a mentions that the main score is \u201cstrongly disagree\u201d for \u201carms\u201d (both baseline and interpolation) while the text refers to \u201cstrongly agree\u201d. Example questions would help.\n- The results of the human-grounded study are not very conclusive. Note: please correct for multiple comparisons due to multiple statistical testing of the same effect.\n- Kim et al., 2018 already displayed that human users were performing poorly at identifying a network\u2019s decision behavior based on saliency maps. A better comparison could have relied on TCAV instead, especially as the concepts can easily be mapped to the features given the synthetic dataset. This could have made a stronger case for the use of invertible networks, especially as Goyash et al (2019) mention the use of counterfactuals based on concepts.\n- How about non-image datasets?\n\nMinor\n-------\n- Intro: I would suggest using \u201ctransparency\u201d rather than \u201cinterpretability\u201d when referring to logistic regression (e.g. Lipton, 2016). The interpretability of linear model weights is indeed debatable, as weights will depend on the regularization and signal-to-noise ratio in the data (Haufe et al., 2014).\n- No clear flow between the different works in the intro. No clear motivation behind counterfactuals.\n- proofreading: paper is quite hard to follow and minor changes to grammar (e.g. \u201cTheir similarity is easy to seen\u201d) makes it more difficult to assess. The quality of the writing deteriorates in sections 3, 4 and 5.\n- It is unclear what scale delta epsilon represents, and whether we can expect the norm of the different techniques to be comparable. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3736/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3736/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces", "authorids": ["~Leon_Sixt1", "schuessler@tu-berlin.de", "philipp@itp.tu-berlin.de", "~Tim_Landgraf1"], "authors": ["Leon Sixt", "Martin Schuessler", "Philipp Wei\u00df", "Tim Landgraf"], "keywords": ["Interpretable Machine Learning", "Counterfactuals", "Computer Vision", "Human Evaluation", "User Study"], "abstract": "Current state of the art computer vision applications rely on highly complex models. Their interpretability is mostly limited to post-hoc methods which are not guaranteed to be faithful to the model. To elucidate a model\u2019s decision, we present a novel interpretable model based on an invertible deep convolutional network. Our model generates meaningful, faithful, and ideal counterfactuals. Using PCA on the classifier\u2019s input, we can also create \u201cisofactuals\u201d\u2013 image interpolations with the same outcome but visually meaningful different features. Counter- and isofactuals can be used to identify positive and negative evidence in an image. This can also be visualized with heatmaps. We evaluate our approach against gradient-based attribution methods, which we find to produce meaningless adversarial perturbations.  Using our method, we reveal biases in three different datasets. In a human subject experiment, we test whether non-experts find our method useful to spot spurious correlations learned by a model. Our work is a step towards more trustworthy explanations for computer vision.", "one-sentence_summary": "We use invertible neural networks to generate ideal counterfactuals and isofactuals.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sixt|interpretability_through_invertibility_a_deep_convolutional_network_with_ideal_counterfactuals_and_isosurfaces", "supplementary_material": "/attachment/74b00c8e6e7ea503ac48760cf7c184afada069eb.zip", "pdf": "/pdf/83628964eb4ac5f2e2665ccd6ea038a08a70ff43.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1YGiD4CuCZ", "_bibtex": "@misc{\nsixt2021interpretability,\ntitle={Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces},\nauthor={Leon Sixt and Martin Schuessler and Philipp Wei{\\ss} and Tim Landgraf},\nyear={2021},\nurl={https://openreview.net/forum?id=8YFhXYe1Ps}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "8YFhXYe1Ps", "replyto": "8YFhXYe1Ps", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3736/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538070575, "tmdate": 1606915758180, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3736/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3736/-/Official_Review"}}}, {"id": "o7xomokGxe8", "original": null, "number": 5, "cdate": 1604887972325, "ddate": null, "tcdate": 1604887972325, "tmdate": 1606791332394, "tddate": null, "forum": "8YFhXYe1Ps", "replyto": "8YFhXYe1Ps", "invitation": "ICLR.cc/2021/Conference/Paper3736/-/Official_Review", "content": {"title": "While technically incremental, the work provides interesting method to generate multiple kinds of explanations counterfactuals, saliency maps and what is called as \"isosurface\" of the classifer. Interesting case-study and user study demonstrate potential benefit of the method comparing two types of explanations", "review": "1. The authors propose a method to derive counterfactuals, saliency maps, and so called isofactuals using invertible neural networks. The quality of the generated explanations are compared visually with existing baselines. \n\n2. I found the structure of the paper confusing and lacking in clear elicitation of contributions. For example, once explanation methods are provided in Sec 2, the motivation of Section 3 to purely compare gradient methods is highly unstructured. In here, its unclear why changes in independent principal components while generating counterfactuals not desirable especially if as the authors suggest, the prediction changes. Even if the changes are not observable to humans. This is a highly unusual aspect of the counterfactuals where the counterfactuals shouldn't just explain large changes to logits but changes to predictions as well. \n\n3. The objective of each evaluation data-set and study should also be clearly outlined before proceeding to the details. Currently the paper leaves the reader to figure out the main contributions at the cost of hampering the paper's technical significance. \n\n4. What is the goal of the user study? Why is the baseline any other method of generating counterfactuals but merely conditioned examples? \n\nI strongly suggest restructuring the paper to fix above concerns and provide a clear justification for their experimental setup.\n\n5. I also strongly recommend the authors to move away from evaluating their models against celebA labels such as \"Attractive\" which are rife with ethical concerns. I understand that those are the only options available in celebA but I would recommend using more neutral class labels for experiments if face dataset is an important part of their evaluations.\n\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nThe authors have done a reasonable job at addressing my concerns and I have increased my score from 5 to 6. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3736/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3736/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces", "authorids": ["~Leon_Sixt1", "schuessler@tu-berlin.de", "philipp@itp.tu-berlin.de", "~Tim_Landgraf1"], "authors": ["Leon Sixt", "Martin Schuessler", "Philipp Wei\u00df", "Tim Landgraf"], "keywords": ["Interpretable Machine Learning", "Counterfactuals", "Computer Vision", "Human Evaluation", "User Study"], "abstract": "Current state of the art computer vision applications rely on highly complex models. Their interpretability is mostly limited to post-hoc methods which are not guaranteed to be faithful to the model. To elucidate a model\u2019s decision, we present a novel interpretable model based on an invertible deep convolutional network. Our model generates meaningful, faithful, and ideal counterfactuals. Using PCA on the classifier\u2019s input, we can also create \u201cisofactuals\u201d\u2013 image interpolations with the same outcome but visually meaningful different features. Counter- and isofactuals can be used to identify positive and negative evidence in an image. This can also be visualized with heatmaps. We evaluate our approach against gradient-based attribution methods, which we find to produce meaningless adversarial perturbations.  Using our method, we reveal biases in three different datasets. In a human subject experiment, we test whether non-experts find our method useful to spot spurious correlations learned by a model. Our work is a step towards more trustworthy explanations for computer vision.", "one-sentence_summary": "We use invertible neural networks to generate ideal counterfactuals and isofactuals.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sixt|interpretability_through_invertibility_a_deep_convolutional_network_with_ideal_counterfactuals_and_isosurfaces", "supplementary_material": "/attachment/74b00c8e6e7ea503ac48760cf7c184afada069eb.zip", "pdf": "/pdf/83628964eb4ac5f2e2665ccd6ea038a08a70ff43.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1YGiD4CuCZ", "_bibtex": "@misc{\nsixt2021interpretability,\ntitle={Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces},\nauthor={Leon Sixt and Martin Schuessler and Philipp Wei{\\ss} and Tim Landgraf},\nyear={2021},\nurl={https://openreview.net/forum?id=8YFhXYe1Ps}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "8YFhXYe1Ps", "replyto": "8YFhXYe1Ps", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3736/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538070575, "tmdate": 1606915758180, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3736/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3736/-/Official_Review"}}}, {"id": "AflV7AWzFQt", "original": null, "number": 10, "cdate": 1606306092572, "ddate": null, "tcdate": 1606306092572, "tmdate": 1606306445026, "tddate": null, "forum": "8YFhXYe1Ps", "replyto": "xO8zcGzcp_6", "invitation": "ICLR.cc/2021/Conference/Paper3736/-/Official_Comment", "content": {"title": "R2", "comment": "\n> How about non-image datasets?\n\nWe discuss other data domains in the conclusion. Our method mainly depends on the availability of an inverse, PCA and the architecture of the network (e.g. where to put the classifier) and could probably be adapted to other domains. \nMinor\nIntro: I would suggest using \u201ctransparency\u201d rather than \u201cinterpretability\u201d when referring to logistic regression (e.g. Lipton, 2016). The interpretability of linear model weights is indeed debatable, as weights will depend on the regularization and signal-to-noise ratio in the data (Haufe et al., 2014).\nNo clear flow between the different works in the intro. No clear motivation behind counterfactuals.\nproofreading: paper is quite hard to follow and minor changes to grammar (e.g. \u201cTheir similarity is easy to seen\u201d) makes it more difficult to assess. The quality of the writing deteriorates in sections 3, 4 and 5.\nIt is unclear what scale delta epsilon represents, and whether we can expect the norm of the different techniques to be comparable.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3736/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3736/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces", "authorids": ["~Leon_Sixt1", "schuessler@tu-berlin.de", "philipp@itp.tu-berlin.de", "~Tim_Landgraf1"], "authors": ["Leon Sixt", "Martin Schuessler", "Philipp Wei\u00df", "Tim Landgraf"], "keywords": ["Interpretable Machine Learning", "Counterfactuals", "Computer Vision", "Human Evaluation", "User Study"], "abstract": "Current state of the art computer vision applications rely on highly complex models. Their interpretability is mostly limited to post-hoc methods which are not guaranteed to be faithful to the model. To elucidate a model\u2019s decision, we present a novel interpretable model based on an invertible deep convolutional network. Our model generates meaningful, faithful, and ideal counterfactuals. Using PCA on the classifier\u2019s input, we can also create \u201cisofactuals\u201d\u2013 image interpolations with the same outcome but visually meaningful different features. Counter- and isofactuals can be used to identify positive and negative evidence in an image. This can also be visualized with heatmaps. We evaluate our approach against gradient-based attribution methods, which we find to produce meaningless adversarial perturbations.  Using our method, we reveal biases in three different datasets. In a human subject experiment, we test whether non-experts find our method useful to spot spurious correlations learned by a model. Our work is a step towards more trustworthy explanations for computer vision.", "one-sentence_summary": "We use invertible neural networks to generate ideal counterfactuals and isofactuals.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sixt|interpretability_through_invertibility_a_deep_convolutional_network_with_ideal_counterfactuals_and_isosurfaces", "supplementary_material": "/attachment/74b00c8e6e7ea503ac48760cf7c184afada069eb.zip", "pdf": "/pdf/83628964eb4ac5f2e2665ccd6ea038a08a70ff43.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1YGiD4CuCZ", "_bibtex": "@misc{\nsixt2021interpretability,\ntitle={Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces},\nauthor={Leon Sixt and Martin Schuessler and Philipp Wei{\\ss} and Tim Landgraf},\nyear={2021},\nurl={https://openreview.net/forum?id=8YFhXYe1Ps}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8YFhXYe1Ps", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3736/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3736/Authors|ICLR.cc/2021/Conference/Paper3736/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834346, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3736/-/Official_Comment"}}}, {"id": "xO8zcGzcp_6", "original": null, "number": 9, "cdate": 1606306076244, "ddate": null, "tcdate": 1606306076244, "tmdate": 1606306436162, "tddate": null, "forum": "8YFhXYe1Ps", "replyto": "_WsnMRbeLHB", "invitation": "ICLR.cc/2021/Conference/Paper3736/-/Official_Comment", "content": {"title": "R2", "comment": "\n>I am confused by the section on saliency maps: what does h represent? The activations at an intermediate layer? The motivation is unclear: what are the authors trying to highlight in these \u201csaliency maps\u201d? Are these computed attributions or are these L1 distance between activations (in %) between x and x_tilde? Or is it a cosine distance (as suggested by the next sentence mentioning the angle?)\n\nWe agree that the mathematical formulation was not clear. The score is based on the dot-product between the change $|\\Delta h|$ and feature activation $h$. We have clarified this in the manuscript. \n\n> The tasks used for illustration are not described in the text. Examples of y and epsilon should be provided.\n\nWe added a respective comment to the manuscript. \n\n\n> Is the technique limited to the model\u2019s predicted classes?\n\nYou could add classifiers, finetune them and then explain them. Or you could also cluster intermediate features and invert them back.\n\n> How is \u201cideal\u201d counterfactual described and mathematically verified?\n\nWe added a definition of ideal counterfactual. Mathematically, there must exist a path from a startpoint $x$ to $\\tilde x$ such that the gradient of the path is perfectly aligned with the gradient of the classifier.\n\n> The relationship between counterfactuals and e.g. integrated gradients is unclear: [..]\nWe have improved the description of this section substantially. \n\n\n> What are the participants in the human-based study viewing? Are they comparing the counterfactuals to e.g. SmoothGrad maps, or the saliency as defined per the proposed approach?\n\nThe participants were assigned two groups. Each group only saw one explanation technique (our counterfactuals or the baseline). Hence, they are not comparing methods. We reworked the section to amke this clearer.\n\n> It is unclear what the participants answered: Figure 5a mentions that the main score is \u201cstrongly disagree\u201d for \u201carms\u201d (both baseline and interpolation) while the text refers to \u201cstrongly agree\u201d. Example questions would help.\n\nUnfortunately, the labels were flipped. We apologize and thank you for pointing this out.\n\n> The results of the human-grounded study are not very conclusive. Note: please correct for multiple comparisons due to multiple statistical testing of the same effect.\n\nWe have added a discussion of our results in the manuscript. Please see the general response to points raised by all reviewers. \n\nFollowing your advice, we have now applied the Bonferroni-correction. The results remain unchanged. Thank you for making us aware of this!\n\n\n> Kim et al., 2018 already displayed that human users were performing poorly at identifying a network\u2019s decision behavior based on saliency maps. A better comparison could have relied on TCAV instead, especially as the concepts can easily be mapped to the features given the synthetic dataset. This could have made a stronger case for the use of invertible networks, especially as Goyash et al (2019) mention the use of counterfactuals based on concepts.\n\nWhile it is true that (Kim et al., 2018) show the limitations of saliency maps, our comparison between directional derivative and gradient adds theoretical evidence against gradient-based attribution method. \n\nWe decided against using TCAV in our evaluation as we tackle different questions. TCAV requires manually labeled concepts and we provide a method to discover possible concepts worth annotating. Instead of reporting the correlations between the different attributes and the logit score, we could have reported the TCAV scores. To us, the simple correlation seems more straightforward and comprehensible.\n\n(Goyal et al., 2019) extends TCAV to estimate the causal effect of concepts. We do report a similar causal effect score as Goyal for the Two4Two dataset, where we control the data generation process. The main advantage of our model is that it uses the same model to classify and generate the explanations. While we could have implemented the work by Goyal for invertible neural networks, the potential insights would be a comparison between VAEs and invertible neural networks.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3736/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3736/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces", "authorids": ["~Leon_Sixt1", "schuessler@tu-berlin.de", "philipp@itp.tu-berlin.de", "~Tim_Landgraf1"], "authors": ["Leon Sixt", "Martin Schuessler", "Philipp Wei\u00df", "Tim Landgraf"], "keywords": ["Interpretable Machine Learning", "Counterfactuals", "Computer Vision", "Human Evaluation", "User Study"], "abstract": "Current state of the art computer vision applications rely on highly complex models. Their interpretability is mostly limited to post-hoc methods which are not guaranteed to be faithful to the model. To elucidate a model\u2019s decision, we present a novel interpretable model based on an invertible deep convolutional network. Our model generates meaningful, faithful, and ideal counterfactuals. Using PCA on the classifier\u2019s input, we can also create \u201cisofactuals\u201d\u2013 image interpolations with the same outcome but visually meaningful different features. Counter- and isofactuals can be used to identify positive and negative evidence in an image. This can also be visualized with heatmaps. We evaluate our approach against gradient-based attribution methods, which we find to produce meaningless adversarial perturbations.  Using our method, we reveal biases in three different datasets. In a human subject experiment, we test whether non-experts find our method useful to spot spurious correlations learned by a model. Our work is a step towards more trustworthy explanations for computer vision.", "one-sentence_summary": "We use invertible neural networks to generate ideal counterfactuals and isofactuals.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sixt|interpretability_through_invertibility_a_deep_convolutional_network_with_ideal_counterfactuals_and_isosurfaces", "supplementary_material": "/attachment/74b00c8e6e7ea503ac48760cf7c184afada069eb.zip", "pdf": "/pdf/83628964eb4ac5f2e2665ccd6ea038a08a70ff43.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1YGiD4CuCZ", "_bibtex": "@misc{\nsixt2021interpretability,\ntitle={Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces},\nauthor={Leon Sixt and Martin Schuessler and Philipp Wei{\\ss} and Tim Landgraf},\nyear={2021},\nurl={https://openreview.net/forum?id=8YFhXYe1Ps}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8YFhXYe1Ps", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3736/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3736/Authors|ICLR.cc/2021/Conference/Paper3736/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834346, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3736/-/Official_Comment"}}}, {"id": "0ef8tjEhCNy", "original": null, "number": 4, "cdate": 1606305364836, "ddate": null, "tcdate": 1606305364836, "tmdate": 1606306404395, "tddate": null, "forum": "8YFhXYe1Ps", "replyto": "5sUGratera", "invitation": "ICLR.cc/2021/Conference/Paper3736/-/Official_Comment", "content": {"title": "Answer R1", "comment": "> The writing often lacks clarity and the usage of space can be more judicious. [...]\n\nWe have substantially reworked the writing for better balance. We have expanded the methods and rewritten the isosurface section to improve clarity.\n> Focusing on elaborating the method, and maybe one less dataset would improve readability by moving extra evaluation to appendix. [...]\n\nWe followed your advice and moved large parts of the mice evaluation to the appendix.\n> The results of the human subject study are not very convincing. [...]\n\nUnfortunately, the labels in the user study figure were swapped in the original version of the manuscript which may have led to the impression that participants find a lot of irrelevant patterns. We apologize and hope that the corrected figure and improved description clarifies our findings. \n\n> The appendix (Fig. 9) also shows that the subjects thought both the proposed method and the baseline were equally good.[...]\n\nWe improved the layout of the figures and corrected the mistakes you mentioned. Regarding the results of the user study, both methods received comparable ratings. However, both ratings were considerably high and show that we were successful in designing both a usable counterfactual generation method and a baseline that users found useful as well. We discussed the implications of this result in the conclusions of the updated manuscript. On a sidenote: we preregistered our experiment and reported a standardised subjective ratings scale. It would be great if this (in ML rather non-standard) practice would be adopted in interpretability research because it shows that we care about rigorous evaluation with those that are supposed to use explanations: the users.\n\n> Minor formatting issues: mismatched quotes throughout (striped, zebra, etc.),[...]\n\nThank you, we fixed all of them. \n\n> Overall, the paper has some good ideas and interesting analysis[...]I am marginally inclined for it to be accepted.\n\nThank you. We agree with your criticism and reworked the entire manuscript substantially. We hope you agree this merits an updated rating :)\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3736/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3736/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces", "authorids": ["~Leon_Sixt1", "schuessler@tu-berlin.de", "philipp@itp.tu-berlin.de", "~Tim_Landgraf1"], "authors": ["Leon Sixt", "Martin Schuessler", "Philipp Wei\u00df", "Tim Landgraf"], "keywords": ["Interpretable Machine Learning", "Counterfactuals", "Computer Vision", "Human Evaluation", "User Study"], "abstract": "Current state of the art computer vision applications rely on highly complex models. Their interpretability is mostly limited to post-hoc methods which are not guaranteed to be faithful to the model. To elucidate a model\u2019s decision, we present a novel interpretable model based on an invertible deep convolutional network. Our model generates meaningful, faithful, and ideal counterfactuals. Using PCA on the classifier\u2019s input, we can also create \u201cisofactuals\u201d\u2013 image interpolations with the same outcome but visually meaningful different features. Counter- and isofactuals can be used to identify positive and negative evidence in an image. This can also be visualized with heatmaps. We evaluate our approach against gradient-based attribution methods, which we find to produce meaningless adversarial perturbations.  Using our method, we reveal biases in three different datasets. In a human subject experiment, we test whether non-experts find our method useful to spot spurious correlations learned by a model. Our work is a step towards more trustworthy explanations for computer vision.", "one-sentence_summary": "We use invertible neural networks to generate ideal counterfactuals and isofactuals.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sixt|interpretability_through_invertibility_a_deep_convolutional_network_with_ideal_counterfactuals_and_isosurfaces", "supplementary_material": "/attachment/74b00c8e6e7ea503ac48760cf7c184afada069eb.zip", "pdf": "/pdf/83628964eb4ac5f2e2665ccd6ea038a08a70ff43.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1YGiD4CuCZ", "_bibtex": "@misc{\nsixt2021interpretability,\ntitle={Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces},\nauthor={Leon Sixt and Martin Schuessler and Philipp Wei{\\ss} and Tim Landgraf},\nyear={2021},\nurl={https://openreview.net/forum?id=8YFhXYe1Ps}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8YFhXYe1Ps", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3736/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3736/Authors|ICLR.cc/2021/Conference/Paper3736/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834346, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3736/-/Official_Comment"}}}, {"id": "_WsnMRbeLHB", "original": null, "number": 8, "cdate": 1606306046333, "ddate": null, "tcdate": 1606306046333, "tmdate": 1606306330121, "tddate": null, "forum": "8YFhXYe1Ps", "replyto": "qRc354yIWoL", "invitation": "ICLR.cc/2021/Conference/Paper3736/-/Official_Comment", "content": {"title": "R2", "comment": "R2: Interesting idea needing more work\n> the (lack of) clarity of the text.\n> the assessment of the technique, as the results of the human-grounded evaluation are mixed, with users not being significantly more accurate in finding confounding factors compared to a baseline technique.\n\nThis is correct. However, please also take into your consideration that the study still demonstrates the usefulness of our method as well as some of its shortcomings. In contrast to many other evaluations, which do not even consider baselines (e.g. Ribeiro et al. (2016); Singla et al.(2020)) or don\u2019t even evaluate their methods with human subjects at all, we made an effort to create a simple but strong baseline, taking into consideration findings from HCI about usability issues of explanation techniques. \n\nWe would like to ask reviewers to consider the rigor put into the study design and the identification of a good baseline technique as additional contributions of our paper. After all, this might inspire more human evaluations in the machine learning community, if they are deemed valuable. \n\n> the limitations of the technique, [...] should be mentioned.\n\nWe mentioned the added computation costs in our submitted version and also the challenge and possible approaches to apply invertible networks to RNN or GraphNN. We now also state clearer that our method requires us to use a custom network architecture. Since labeling data isn\u2019t a requirement specific to our model, but affects rather all architectures we have not mentioned this point. \n\n> Novelty, The \u201cRelated works\u201d is rather limited,\n\nWe agree and rewrote the related work section and commented on the similarities and differences in greater detail.\n\n\n> [Rigor] I found the qualitative evaluation on the 3 datasets unconvincing [...]\n\nWe agree that possible conclusions might have reached using other techniques. In our user-study, we show that users can reach similar conclusions using a simple baseline. We now state in the related work section that GAN based counterfactuals could probably create similar looking images. However, they cannot guarantee that the explanations are faithful to the model. \n\n> [Rigor] While I was most interested by the discussion around the generation of counterfactuals based on the invertible network compared to based on the integration of gradients, I wished there was a definition of an \u201cideal\u201d counterfactual, qualitative or (preferably) quantitative. The single example provided in the main text is appealing but this requires more evidence to me.\nWe agree and have now included such a definition at the beginning of section 2.\n\nWe agree that saliency maps can be inconclusive, no matter what method has been used to generate them. There are more and more studies being published showing evidence for that. We have now included a small summary of such findings in our paper along with the remark that the same useability concern applies to our saliency maps as well. Regardless, our saliency maps are faithful to the model, which is an important and unique contribution in comparison to other methods. A saliency map of a counterfactual highlighting the entire face is still correct as many labels (gender, ethnicity, age, attractiveness) impact the whole face.\n\n> Counterfactuals: their quality seems subject to appreciation and confirmation bias, especially on potentially cherry picked examples. \n\nThe quality of counterfactuals is evaluated in the users study, where they were picked randomly not manually. Most examples in the paper are based on the principal components of the dataset (see Figure 1b) and therefore not cherry picked. We also show a number of random samples along principal components in the Appendix. \n\n\n> There should be more details about the Two4Two dataset and its motivations, as well as how it relates to other datasets (e.g. Goyal et al., 2019)\n\nWe provide an additional description of the Two4Two dataset in the appendix and relate it to other datasets, e.g. the BAM dataset.\n\n> How does the proposed approach relate to \u201ccompleteness\u201d (Sundararajan et al., 2017)?\n\nCompleteness requires that the sum of the attribution map equals the difference of the logit score between the image and baseline. Our saliency maps do not fulfill completeness as defined by (Sundararajan et al., 2017). We do look at the differences between counterfactual examples and they can be arbitrarily large as \u03c6 can stretch the space. As a side note: we believe that \u201cCompleteness\u201d fails to account for the non-linear stretching of neural networks and is not a property to require (but this is a different discussion).\n\n> What is the mathematical justification to resize the saliency map [...]\nAs activations remain localized in a convolutional network, this operation is valid. The lower resolutional feature map still matches the feature locations well. Grad-CAM for example does the rescaling even using the last convolutional feature map.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3736/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3736/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces", "authorids": ["~Leon_Sixt1", "schuessler@tu-berlin.de", "philipp@itp.tu-berlin.de", "~Tim_Landgraf1"], "authors": ["Leon Sixt", "Martin Schuessler", "Philipp Wei\u00df", "Tim Landgraf"], "keywords": ["Interpretable Machine Learning", "Counterfactuals", "Computer Vision", "Human Evaluation", "User Study"], "abstract": "Current state of the art computer vision applications rely on highly complex models. Their interpretability is mostly limited to post-hoc methods which are not guaranteed to be faithful to the model. To elucidate a model\u2019s decision, we present a novel interpretable model based on an invertible deep convolutional network. Our model generates meaningful, faithful, and ideal counterfactuals. Using PCA on the classifier\u2019s input, we can also create \u201cisofactuals\u201d\u2013 image interpolations with the same outcome but visually meaningful different features. Counter- and isofactuals can be used to identify positive and negative evidence in an image. This can also be visualized with heatmaps. We evaluate our approach against gradient-based attribution methods, which we find to produce meaningless adversarial perturbations.  Using our method, we reveal biases in three different datasets. In a human subject experiment, we test whether non-experts find our method useful to spot spurious correlations learned by a model. Our work is a step towards more trustworthy explanations for computer vision.", "one-sentence_summary": "We use invertible neural networks to generate ideal counterfactuals and isofactuals.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sixt|interpretability_through_invertibility_a_deep_convolutional_network_with_ideal_counterfactuals_and_isosurfaces", "supplementary_material": "/attachment/74b00c8e6e7ea503ac48760cf7c184afada069eb.zip", "pdf": "/pdf/83628964eb4ac5f2e2665ccd6ea038a08a70ff43.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1YGiD4CuCZ", "_bibtex": "@misc{\nsixt2021interpretability,\ntitle={Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces},\nauthor={Leon Sixt and Martin Schuessler and Philipp Wei{\\ss} and Tim Landgraf},\nyear={2021},\nurl={https://openreview.net/forum?id=8YFhXYe1Ps}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8YFhXYe1Ps", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3736/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3736/Authors|ICLR.cc/2021/Conference/Paper3736/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834346, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3736/-/Official_Comment"}}}, {"id": "TPOe5A1-1d7", "original": null, "number": 5, "cdate": 1606305397080, "ddate": null, "tcdate": 1606305397080, "tmdate": 1606306186463, "tddate": null, "forum": "8YFhXYe1Ps", "replyto": "ZF17IvHJIus", "invitation": "ICLR.cc/2021/Conference/Paper3736/-/Official_Comment", "content": {"title": "Answer R4", "comment": "R4: Use of invertible CNNs to construct counterfactuals and isosurfaces\n> The reviewer finds the manuscript hard to follow [...]\n\nWe agree with your criticism and reworked the entire manuscript substantially.\n\n\n> The descriptions about saliency maps are less relevant to the main idea [...]\n\nWe agree that the description had ample room for improvement and we invested much time in focusing the text for improved clarity (we hope you agree). \n\n> The comparison between simple gradient and direction derivative is less fair, as the directional derivative makes use of the very information direction [...]\n\nWe compare all methods on the same model running the same integration. The gradient and the directional derivative d\\phi^{-1}/dw make both use of the direction w, as you can write both as the Jacobi Matrix J * w  and J^{-1} w. The reason for their different results is that J^{-1} is suited to translate w to image space and J is not. We therefore think the comparison is fair. However, we have rewritten the description of the method (sec. 3) and hope this improved clarity.\nIf you visualize \\phi^{-1}(\\phi(x) + a w) directly, you will get the same result as when integrating d\\phi^{-1}/dw over the respective length.\n\n> The human study may need to conduct another set of control experiments to show that only original training images (not counterfactual interpolations) are helpful for uses to identify CNN patterns and biases. [...]\n\nWe have implemented this in the baseline condition: users are presented with images from the validation set, sorted by their logits. Since the study was a between-group design with each group only seeing one explanation technique, we can isolate the effect of studying only images and no interpolation. We concluded that studying examples this way is indeed helpful, but also does not allow more than half of the participants to detect the subtle rotation bias. This rejects the theory that users can find shortcuts easily on this dataset. While it is still a relatively simple dataset, it still allowed us to create user tasks that are not trivial. The writing in that section was very condensed and lacked clarity. We improved the text and have integrated your comment in the new version.\n\n> [Other minor comments] Figure 1: There is no explanation for (a). What is w The reader may not understand it for the first reading.\n\nThanks for pointing this out, we have improved the caption. \n\n> [Other minor comments] Figure 4: The reviewer believes normalized scores on the top of the images make better sense.\n\nDo you mean not the raw logit value but rather the probability? We think the logit value provides a better summary as a change from 5 to 50 corresponds to a probability from 0.993 to 1-1e-22 which can be better understood as logit.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3736/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3736/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces", "authorids": ["~Leon_Sixt1", "schuessler@tu-berlin.de", "philipp@itp.tu-berlin.de", "~Tim_Landgraf1"], "authors": ["Leon Sixt", "Martin Schuessler", "Philipp Wei\u00df", "Tim Landgraf"], "keywords": ["Interpretable Machine Learning", "Counterfactuals", "Computer Vision", "Human Evaluation", "User Study"], "abstract": "Current state of the art computer vision applications rely on highly complex models. Their interpretability is mostly limited to post-hoc methods which are not guaranteed to be faithful to the model. To elucidate a model\u2019s decision, we present a novel interpretable model based on an invertible deep convolutional network. Our model generates meaningful, faithful, and ideal counterfactuals. Using PCA on the classifier\u2019s input, we can also create \u201cisofactuals\u201d\u2013 image interpolations with the same outcome but visually meaningful different features. Counter- and isofactuals can be used to identify positive and negative evidence in an image. This can also be visualized with heatmaps. We evaluate our approach against gradient-based attribution methods, which we find to produce meaningless adversarial perturbations.  Using our method, we reveal biases in three different datasets. In a human subject experiment, we test whether non-experts find our method useful to spot spurious correlations learned by a model. Our work is a step towards more trustworthy explanations for computer vision.", "one-sentence_summary": "We use invertible neural networks to generate ideal counterfactuals and isofactuals.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sixt|interpretability_through_invertibility_a_deep_convolutional_network_with_ideal_counterfactuals_and_isosurfaces", "supplementary_material": "/attachment/74b00c8e6e7ea503ac48760cf7c184afada069eb.zip", "pdf": "/pdf/83628964eb4ac5f2e2665ccd6ea038a08a70ff43.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1YGiD4CuCZ", "_bibtex": "@misc{\nsixt2021interpretability,\ntitle={Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces},\nauthor={Leon Sixt and Martin Schuessler and Philipp Wei{\\ss} and Tim Landgraf},\nyear={2021},\nurl={https://openreview.net/forum?id=8YFhXYe1Ps}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8YFhXYe1Ps", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3736/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3736/Authors|ICLR.cc/2021/Conference/Paper3736/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834346, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3736/-/Official_Comment"}}}, {"id": "qius7XfZWRf", "original": null, "number": 11, "cdate": 1606306160886, "ddate": null, "tcdate": 1606306160886, "tmdate": 1606306160886, "tddate": null, "forum": "8YFhXYe1Ps", "replyto": "LPM9kMZqeaw", "invitation": "ICLR.cc/2021/Conference/Paper3736/-/Official_Comment", "content": {"title": "R3", "comment": "> In Figure 5(b), it's not clear how real images for baseline condition are sampled. To allow proper comparison, as in counterfactual design, three images should be selected as primary images, the rest images should be sampled based on its minimum distance to the primary images but a different label.\n\nWe have received several comments about the baseline from other reviewers too. We improved the description and motivation of the baseline integrating many of the reviewer\u2019s remarks. Thank you!\n\nWe understand your suggestion as follows: You suggest to recreate counterfactual interpolation by grouping similar images together for one row but with different logit scores. While this would provide a counterfactual interpolation based on real images, it would likely be disadvantageous for the baseline. Consider that the main pattern in the dataset was that some objects (arms) change their location relative to other objects.  If we measure the similarity in pixel space, similarly colored images would be grouped together -- providing a false impression that color does not change and hence is not important for the prediction. Participants may not notice the colour bias this way. A more complex approach based on intermediate features could come with its own challenges and so we decided for the baseline with least bias. \n\n>[...]the authors identified principal components that correspond to attributes like gender, smiling.[...]A quantitative analysis is required to demonstrate the dependency of the \"attractive\" attribute on other attributes. [...]\n\nFollowing your comment, we included the correlation coefficients between the \u201cAttractive\u201d logit and the other properties in the CelebA dataset."}, "signatures": ["ICLR.cc/2021/Conference/Paper3736/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3736/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces", "authorids": ["~Leon_Sixt1", "schuessler@tu-berlin.de", "philipp@itp.tu-berlin.de", "~Tim_Landgraf1"], "authors": ["Leon Sixt", "Martin Schuessler", "Philipp Wei\u00df", "Tim Landgraf"], "keywords": ["Interpretable Machine Learning", "Counterfactuals", "Computer Vision", "Human Evaluation", "User Study"], "abstract": "Current state of the art computer vision applications rely on highly complex models. Their interpretability is mostly limited to post-hoc methods which are not guaranteed to be faithful to the model. To elucidate a model\u2019s decision, we present a novel interpretable model based on an invertible deep convolutional network. Our model generates meaningful, faithful, and ideal counterfactuals. Using PCA on the classifier\u2019s input, we can also create \u201cisofactuals\u201d\u2013 image interpolations with the same outcome but visually meaningful different features. Counter- and isofactuals can be used to identify positive and negative evidence in an image. This can also be visualized with heatmaps. We evaluate our approach against gradient-based attribution methods, which we find to produce meaningless adversarial perturbations.  Using our method, we reveal biases in three different datasets. In a human subject experiment, we test whether non-experts find our method useful to spot spurious correlations learned by a model. Our work is a step towards more trustworthy explanations for computer vision.", "one-sentence_summary": "We use invertible neural networks to generate ideal counterfactuals and isofactuals.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sixt|interpretability_through_invertibility_a_deep_convolutional_network_with_ideal_counterfactuals_and_isosurfaces", "supplementary_material": "/attachment/74b00c8e6e7ea503ac48760cf7c184afada069eb.zip", "pdf": "/pdf/83628964eb4ac5f2e2665ccd6ea038a08a70ff43.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1YGiD4CuCZ", "_bibtex": "@misc{\nsixt2021interpretability,\ntitle={Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces},\nauthor={Leon Sixt and Martin Schuessler and Philipp Wei{\\ss} and Tim Landgraf},\nyear={2021},\nurl={https://openreview.net/forum?id=8YFhXYe1Ps}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8YFhXYe1Ps", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3736/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3736/Authors|ICLR.cc/2021/Conference/Paper3736/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834346, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3736/-/Official_Comment"}}}, {"id": "LPM9kMZqeaw", "original": null, "number": 6, "cdate": 1606305564663, "ddate": null, "tcdate": 1606305564663, "tmdate": 1606306146018, "tddate": null, "forum": "8YFhXYe1Ps", "replyto": "d8oBJqnpCFB", "invitation": "ICLR.cc/2021/Conference/Paper3736/-/Official_Comment", "content": {"title": "R3", "comment": "R3: interesting idea but the execution and writing left a lot to be desired, seems not proof-read!    \n>  The paper an interesting and potentially important idea. But at times, the text is difficult to read.\n\nWe agree with your criticism and reworked the entire manuscript substantially.\n\n> The conclusion from Figure 2 is not clear. [...]\n\nWe removed these remarks from the figure caption. Instead, we discuss them in detail in the CelebA evaluation. \n\n\n> In Figure 2 and Figure 3(b), a label showing the different principal components considered in each column, and the logit/prediction of the classifier for each row will improve the figure's readability.\n\nWe have adopted your advice, thank you!\n\n> The saliency maps in Figure 3(a) highlight almost the entire face; hence they are inconclusive. \n\nWe agree that saliency maps can be inconclusive, no matter what method has been used to generate them. There are more and more studies being published showing evidence for that. We have now included a small summary of such findings in our paper along with the remark that the same useability concern applies to our saliency maps as well. Regardless, our saliency maps are faithful to the model, which is an important and unique contribution in comparison to other methods. A saliency map of a counterfactual highlighting the entire face is still correct as many labels (gender, ethnicity, age, attractiveness) impact the whole face. \n\n\n> Figure 4(a) shows the final results [...] To understand the results, it would be helpful to show some examples over which the integration took place. \n\nFigure 4a (now 3b) shows the original image on the left. We have not included intermediate integration steps as they basically show interpolations between the original and the final image. We, however, provide our code so an interested reader can investigate these steps. \n\n\n> An example of a random sample, with minimal/no changes along the independent factors for the proposed method, [...]\n\nWe provide examples with no change along the independent factors are given by the counterfactual interpolation. The astronaut integrated along the directional derivative in the old Figure 4a is an example with minimal change along the independent factors.\n\n\n> The number reported at the end of section 3 on \"the gradient of x and the directional derivative dx/dw\" should be reported in a table [...]\n\nYou are right, we should have reported those numbers in a table. We, however, decided to exclude the self-similarity comparison between the gradient and the directional derivative to focus on the other results.\n\n> The directional derivative dx/dw, in the model, is w.r.t the weight vector of the binary classifier, trained to identify the label. Related work by Kim et al. (2018)[...]also used directional derivatives w.r.t to binary classifiers, trained to identify a human-defined concept. A comparison with this method will help the reader understand the different applications of directional derivatives and how directional derivatives can be used without an invertible network. \n\nWe have added a short comment on the relationship with (and the main difference to) TCAV - which does not compute the derivative on an invertible neural network.\n\n\n\n> Table 1 doesn't report the results for the supervised method for celebA and tow4two datasets. \n\nWe have now included these numbers in the updated version of the manuscript.\n\n> The numbers reported in Table 2 lacks a coherent conclusion. The corr. data and corr. change columns have values in similar ranges. Please elaborate and discuss the results. \n\nWe extended the figures caption to discuss the results.\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3736/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3736/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces", "authorids": ["~Leon_Sixt1", "schuessler@tu-berlin.de", "philipp@itp.tu-berlin.de", "~Tim_Landgraf1"], "authors": ["Leon Sixt", "Martin Schuessler", "Philipp Wei\u00df", "Tim Landgraf"], "keywords": ["Interpretable Machine Learning", "Counterfactuals", "Computer Vision", "Human Evaluation", "User Study"], "abstract": "Current state of the art computer vision applications rely on highly complex models. Their interpretability is mostly limited to post-hoc methods which are not guaranteed to be faithful to the model. To elucidate a model\u2019s decision, we present a novel interpretable model based on an invertible deep convolutional network. Our model generates meaningful, faithful, and ideal counterfactuals. Using PCA on the classifier\u2019s input, we can also create \u201cisofactuals\u201d\u2013 image interpolations with the same outcome but visually meaningful different features. Counter- and isofactuals can be used to identify positive and negative evidence in an image. This can also be visualized with heatmaps. We evaluate our approach against gradient-based attribution methods, which we find to produce meaningless adversarial perturbations.  Using our method, we reveal biases in three different datasets. In a human subject experiment, we test whether non-experts find our method useful to spot spurious correlations learned by a model. Our work is a step towards more trustworthy explanations for computer vision.", "one-sentence_summary": "We use invertible neural networks to generate ideal counterfactuals and isofactuals.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sixt|interpretability_through_invertibility_a_deep_convolutional_network_with_ideal_counterfactuals_and_isosurfaces", "supplementary_material": "/attachment/74b00c8e6e7ea503ac48760cf7c184afada069eb.zip", "pdf": "/pdf/83628964eb4ac5f2e2665ccd6ea038a08a70ff43.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1YGiD4CuCZ", "_bibtex": "@misc{\nsixt2021interpretability,\ntitle={Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces},\nauthor={Leon Sixt and Martin Schuessler and Philipp Wei{\\ss} and Tim Landgraf},\nyear={2021},\nurl={https://openreview.net/forum?id=8YFhXYe1Ps}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8YFhXYe1Ps", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3736/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3736/Authors|ICLR.cc/2021/Conference/Paper3736/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834346, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3736/-/Official_Comment"}}}, {"id": "2tX2E1p_wa", "original": null, "number": 7, "cdate": 1606305609201, "ddate": null, "tcdate": 1606305609201, "tmdate": 1606305609201, "tddate": null, "forum": "8YFhXYe1Ps", "replyto": "8YFhXYe1Ps", "invitation": "ICLR.cc/2021/Conference/Paper3736/-/Official_Comment", "content": {"title": "General Rebuttal Answer", "comment": "We want to thank the reviewer for their time, effort and detailed feedback. Their comments helped to significantly improve this paper. We address each reviewer\u2019s comment in-line and provide an overview here. \n\nWe generally found the reviews to be a fair assessment of our paper. The reviewers pointed out strengths of our paper such as the novelty (R2), combining a generative and discriminative model (R3), the comparison of the directional derivative dx/dw to the gradient (R2), directional derivatives for constructing counterfactuals (R4), using PCA to create isosurfaces (R4) and conducting a rigorous user study (R2, R4).\n\nThe main point of criticism was the quality of the presentation. The manuscript lacked clarity and was oftentimes confusing. Some reviewers raised concerns about the design and the results of our user study.\n\nBased on the reviewers' feedback, we rewrote large parts of the paper. We now state our motivations and contributions clearer. We restructured  the method section, adding clearer definitions for counterfactuals and ideal counterfactuals. We moved the comparison of the gradient with the directional derivative into the evaluation. For each dataset, we provide a justification of why we used it and how it fits into the overall evaluation of our method. Some parts were moved to the appendix to increase focus on the most important aspects and reduce clutter.  Additionally, improved the related work section mentioning how related methods differ from our approach. We think this improved clarity throughout the paper. \n\nWith respect to the evaluation, we added an analysis to the celebA section where we now report results of correlations that confirm different hypotheses. Regarding the user study, we found and fixed a severe mistake in a figure that was probably one cause of confusion, heartfelt apologies! Some reviewers criticised that our methods could not beat the baseline. While this is true, we want to emphasize that 1) conducting a user study itself is a valuable contribution that most papers in the field are still hesitant conducting, 2) both methods (our conterfactual and the baseline) have provided users with sufficient information to discover relevant and irrelevant features. This just indicates that our baseline was strong and we discuss this result and it\u2019s potential implications for interpretability research in the conclusions of our manuscript. \n\nWe hope our substantial rework finds your liking, some additional results will find their way into the manuscript until the camera-ready version is due. If you agree that these changes improved our manuscript, we would be grateful for an increased rating.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3736/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3736/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces", "authorids": ["~Leon_Sixt1", "schuessler@tu-berlin.de", "philipp@itp.tu-berlin.de", "~Tim_Landgraf1"], "authors": ["Leon Sixt", "Martin Schuessler", "Philipp Wei\u00df", "Tim Landgraf"], "keywords": ["Interpretable Machine Learning", "Counterfactuals", "Computer Vision", "Human Evaluation", "User Study"], "abstract": "Current state of the art computer vision applications rely on highly complex models. Their interpretability is mostly limited to post-hoc methods which are not guaranteed to be faithful to the model. To elucidate a model\u2019s decision, we present a novel interpretable model based on an invertible deep convolutional network. Our model generates meaningful, faithful, and ideal counterfactuals. Using PCA on the classifier\u2019s input, we can also create \u201cisofactuals\u201d\u2013 image interpolations with the same outcome but visually meaningful different features. Counter- and isofactuals can be used to identify positive and negative evidence in an image. This can also be visualized with heatmaps. We evaluate our approach against gradient-based attribution methods, which we find to produce meaningless adversarial perturbations.  Using our method, we reveal biases in three different datasets. In a human subject experiment, we test whether non-experts find our method useful to spot spurious correlations learned by a model. Our work is a step towards more trustworthy explanations for computer vision.", "one-sentence_summary": "We use invertible neural networks to generate ideal counterfactuals and isofactuals.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sixt|interpretability_through_invertibility_a_deep_convolutional_network_with_ideal_counterfactuals_and_isosurfaces", "supplementary_material": "/attachment/74b00c8e6e7ea503ac48760cf7c184afada069eb.zip", "pdf": "/pdf/83628964eb4ac5f2e2665ccd6ea038a08a70ff43.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1YGiD4CuCZ", "_bibtex": "@misc{\nsixt2021interpretability,\ntitle={Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces},\nauthor={Leon Sixt and Martin Schuessler and Philipp Wei{\\ss} and Tim Landgraf},\nyear={2021},\nurl={https://openreview.net/forum?id=8YFhXYe1Ps}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8YFhXYe1Ps", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3736/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3736/Authors|ICLR.cc/2021/Conference/Paper3736/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834346, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3736/-/Official_Comment"}}}, {"id": "bw-xsqOCWht", "original": null, "number": 3, "cdate": 1606305316860, "ddate": null, "tcdate": 1606305316860, "tmdate": 1606305316860, "tddate": null, "forum": "8YFhXYe1Ps", "replyto": "o7xomokGxe8", "invitation": "ICLR.cc/2021/Conference/Paper3736/-/Official_Comment", "content": {"title": "Answer R5", "comment": "\n\n\n> 2. I found the structure of the paper confusing and lacking in clear elicitation of contributions. \n\nWe have substantially restructured the entire manuscript. The contributions are clearly stated at the end of the introduction. We moved Sec 2 to the end of the evaluation and provide appropriate motivation for it.\n> In here [Sec 3], its unclear why changes in independent principal components while generating counterfactuals not desirable especially if as the authors suggest, the prediction changes. Even if the changes are not observable to humans. This is a highly unusual aspect of the counterfactuals where the counterfactuals shouldn't just explain large changes to logits but changes to predictions as well.\n\nWe agree with the definition put forward in (Wachter et. al, 2018) which has  the\u201cclosest possible world\u201d requirement which states that counterfactuals should not change unrelated properties. If they do, it would be hard to tell which change was responsible for the change in prediction. Small and invisible changes do not provide the user with information.\n\n\n> 3. The objective of each evaluation data-set and study should also be clearly outlined before proceeding to the details. Currently the paper leaves the reader to figure out the main contributions at the cost of hampering the paper's technical significance.\n\nWe have restructured this part of the manuscript and clarified this in the updated version of the manuscript.\n> What is the goal of the user study? Why is the baseline any other method of generating counterfactuals but merely conditioned examples? \nWe have now stated the goal in the first paragraph in the corresponding section. We also added more detail about our reasoning for not considering other counterfactual methods or saliency maps and discussed this choice. \n\n> I also strongly recommend the authors to move away from evaluating their models against celebA labels such as \"Attractive\" which are rife with ethical concerns. I understand that those are the only options available in celebA but I would recommend using more neutral class labels for experiments if face dataset is an important part of their evaluations.\n\nWe share these ethical concerns. We chose celebA exactly because it was already criticised due to its many shortcomings. We expected to be able to confirm and investigate biases that we knew existed and to emphasize the advantages of our method. We now state ethical concerns in the manuscript explicitly."}, "signatures": ["ICLR.cc/2021/Conference/Paper3736/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3736/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces", "authorids": ["~Leon_Sixt1", "schuessler@tu-berlin.de", "philipp@itp.tu-berlin.de", "~Tim_Landgraf1"], "authors": ["Leon Sixt", "Martin Schuessler", "Philipp Wei\u00df", "Tim Landgraf"], "keywords": ["Interpretable Machine Learning", "Counterfactuals", "Computer Vision", "Human Evaluation", "User Study"], "abstract": "Current state of the art computer vision applications rely on highly complex models. Their interpretability is mostly limited to post-hoc methods which are not guaranteed to be faithful to the model. To elucidate a model\u2019s decision, we present a novel interpretable model based on an invertible deep convolutional network. Our model generates meaningful, faithful, and ideal counterfactuals. Using PCA on the classifier\u2019s input, we can also create \u201cisofactuals\u201d\u2013 image interpolations with the same outcome but visually meaningful different features. Counter- and isofactuals can be used to identify positive and negative evidence in an image. This can also be visualized with heatmaps. We evaluate our approach against gradient-based attribution methods, which we find to produce meaningless adversarial perturbations.  Using our method, we reveal biases in three different datasets. In a human subject experiment, we test whether non-experts find our method useful to spot spurious correlations learned by a model. Our work is a step towards more trustworthy explanations for computer vision.", "one-sentence_summary": "We use invertible neural networks to generate ideal counterfactuals and isofactuals.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sixt|interpretability_through_invertibility_a_deep_convolutional_network_with_ideal_counterfactuals_and_isosurfaces", "supplementary_material": "/attachment/74b00c8e6e7ea503ac48760cf7c184afada069eb.zip", "pdf": "/pdf/83628964eb4ac5f2e2665ccd6ea038a08a70ff43.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1YGiD4CuCZ", "_bibtex": "@misc{\nsixt2021interpretability,\ntitle={Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces},\nauthor={Leon Sixt and Martin Schuessler and Philipp Wei{\\ss} and Tim Landgraf},\nyear={2021},\nurl={https://openreview.net/forum?id=8YFhXYe1Ps}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8YFhXYe1Ps", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3736/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3736/Authors|ICLR.cc/2021/Conference/Paper3736/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3736/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834346, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3736/-/Official_Comment"}}}, {"id": "d8oBJqnpCFB", "original": null, "number": 2, "cdate": 1603926239499, "ddate": null, "tcdate": 1603926239499, "tmdate": 1605023946612, "tddate": null, "forum": "8YFhXYe1Ps", "replyto": "8YFhXYe1Ps", "invitation": "ICLR.cc/2021/Conference/Paper3736/-/Official_Review", "content": {"title": "interesting idea but the execution and writing left a lot to be desired, seems not proof-read!", "review": "Summary:  The paper presents a promising idea to build interpretable models by combining discriminative and generative approach. The proposed model uses an invertible neural network to model the data distribution. The invertibility helps in transforming the learned feature vector back to the image domain. A linear discriminative classifier is trained on the feature vector to perform binary classification. Using the inverse function, the model generates a counterfactual explanation by inverting a modified logit score to create a new image as an explanation. The authors further construct an orthogonal basis using PCA, such that modifying feature vector in those directions results in no change in the classifier's prediction. Decomposing the feature space into such a basis helps discover potential biases in the dataset and the classification model. The experiments compare the proposed method's performance with fully discriminative models and post-hoc interpretability methods such as gradient-based saliency maps.\n  \nMajor \n-----------------\n\n\u2022\tThe paper an interesting and potentially important idea. But at times, the text is difficult to read. \n\u2022\tThe conclusion from Figure 2 is not clear. From the caption and the figure, it is not clear which attributes such as smiling; gender are important for the classifier's positive /negative attractive decision.   \n\u2022\tIn Figure 2 and Figure 3(b), a label showing the different principal components considered in each column, and the logit/prediction of the classifier for each row will improve the figure's readability. \n\u2022\tSaliency maps highlight important regions of an image for the prediction decision. The saliency maps in Figure 3(a) highlight almost the entire face; hence they are inconclusive. \n\u2022\tFigure 4(a) shows the final results after integrating the original image along with different derivatives. To understand the results, it would be helpful to show some examples over which the integration took place. \n\u2022\tAn example of a random sample, with minimal/no changes along the independent factors for the proposed method, as compared to positive changes by other methods, will help in understanding the results in Figure 4(b). \n\u2022\tThe number reported at the end of section 3 on \"the gradient of x and the directional derivative dx/dw\" should be reported in a table to allow a proper comparison between different methods. \n\u2022\tThe directional derivative dx/dw, in the model, is w.r.t the weight vector of the binary classifier, trained to identify the label. Related work by Kim et al. (2018), \"Interpretability beyond feature attribution: Quantitative testing with concept activation vectors\", also used directional derivatives w.r.t to binary classifiers, trained to identify a human-defined concept. A comparison with this method will help the reader understand the different applications of directional derivatives and how directional derivatives can be used without an invertible network. \n\u2022\tTable 1 doesn't report the results for the supervised method for celebA and tow4two datasets. \n\u2022\tThe numbers reported in Table 2 lacks a coherent conclusion. The corr. data and corr. change columns have values in similar ranges. Please elaborate and discuss the results. \n\u2022\tIn Figure 5(b), it's not clear how real images for baseline condition are sampled. To allow proper comparison, as in counterfactual design, three images should be selected as primary images, the rest images should be sampled based on its minimum distance to the primary images but a different label.  \n\u2022\tIn the Evaluation section, for celebA, the authors identified principal components that correspond to attributes like gender, smiling. The results are reported in a qualitative manner, with inferences draw by showing a few examples only. A quantitative analysis is required to demonstrate the dependency of the \"attractive\" attribute on other attributes. The results shown in Figure 6 of the appendix don't have a thorough caption to illustrate the findings. \n\nMinor: \n------------\n\n\u2022\tThe caption for Figure 1 has typos.  \n\u2022\tIn Figure 4(a), the caption doesn't describe the top label. \n\u2022\tThe opening quotation marks are inverted throughout the text. \n\u2022\tTable 1 and Table 2 are shown after the references. They should be placed with the main text before the references. \n\u2022\tThe label of Figure 5(b) has a very small font.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3736/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3736/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces", "authorids": ["~Leon_Sixt1", "schuessler@tu-berlin.de", "philipp@itp.tu-berlin.de", "~Tim_Landgraf1"], "authors": ["Leon Sixt", "Martin Schuessler", "Philipp Wei\u00df", "Tim Landgraf"], "keywords": ["Interpretable Machine Learning", "Counterfactuals", "Computer Vision", "Human Evaluation", "User Study"], "abstract": "Current state of the art computer vision applications rely on highly complex models. Their interpretability is mostly limited to post-hoc methods which are not guaranteed to be faithful to the model. To elucidate a model\u2019s decision, we present a novel interpretable model based on an invertible deep convolutional network. Our model generates meaningful, faithful, and ideal counterfactuals. Using PCA on the classifier\u2019s input, we can also create \u201cisofactuals\u201d\u2013 image interpolations with the same outcome but visually meaningful different features. Counter- and isofactuals can be used to identify positive and negative evidence in an image. This can also be visualized with heatmaps. We evaluate our approach against gradient-based attribution methods, which we find to produce meaningless adversarial perturbations.  Using our method, we reveal biases in three different datasets. In a human subject experiment, we test whether non-experts find our method useful to spot spurious correlations learned by a model. Our work is a step towards more trustworthy explanations for computer vision.", "one-sentence_summary": "We use invertible neural networks to generate ideal counterfactuals and isofactuals.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sixt|interpretability_through_invertibility_a_deep_convolutional_network_with_ideal_counterfactuals_and_isosurfaces", "supplementary_material": "/attachment/74b00c8e6e7ea503ac48760cf7c184afada069eb.zip", "pdf": "/pdf/83628964eb4ac5f2e2665ccd6ea038a08a70ff43.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1YGiD4CuCZ", "_bibtex": "@misc{\nsixt2021interpretability,\ntitle={Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces},\nauthor={Leon Sixt and Martin Schuessler and Philipp Wei{\\ss} and Tim Landgraf},\nyear={2021},\nurl={https://openreview.net/forum?id=8YFhXYe1Ps}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "8YFhXYe1Ps", "replyto": "8YFhXYe1Ps", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3736/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538070575, "tmdate": 1606915758180, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3736/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3736/-/Official_Review"}}}, {"id": "ZF17IvHJIus", "original": null, "number": 3, "cdate": 1603955102347, "ddate": null, "tcdate": 1603955102347, "tmdate": 1605023946539, "tddate": null, "forum": "8YFhXYe1Ps", "replyto": "8YFhXYe1Ps", "invitation": "ICLR.cc/2021/Conference/Paper3736/-/Official_Review", "content": {"title": "Use of invertible CNNs to construct counterfactuals and isosurfaces", "review": "This paper describes a computational method to construct ideal counterfactuals and isosurfaces via invertible CNNs, and uses it to reveal biases in three different datasets. \n\nStrengths:\n1. The use of directional derivative to construct ideal counterfactuals is interesting.\n2. Leveraging PCA to construct isosurfaces is neat.\n3. The human study is a plus, where the stimuli are based on counterfactual interpolations created by the proposed method.\n\nWeaknesses:\n1. The reviewer finds the manuscript hard to follow, especially Section II. The authors may come up with a clearer presentation.\n2. The descriptions about saliency maps are less relevant to the main idea, further confounding the reviewer.\n3. The comparison between simple gradient and direction derivative is less fair, as the directional derivative makes use of the very information direction w (e.g., the direction of no sunglass -> sunglass).  What happens if we visualize $\\phi^{-1}(\\phi(x)+ \\alpha w)$ directly, for different values of $\\alpha$.\n4. The human study may need to conduct another set of control experiments to show that only original training images (not counterfactual interpolations) are $\\textbf{less}$ helpful for uses to identify CNN patterns and biases.  The reviewer conjectures that for this simple TWO2TWO data, the subjects may spot shortcuts easily even using original training images.\n\nOther minor comments:\n1. Figure 1: There is no explanation for (a). What is $w$? The reader may not understand it for the first reading.\n2. Figure 4: The reviewer believes normalized scores on the top of the images make better sense.\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3736/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3736/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces", "authorids": ["~Leon_Sixt1", "schuessler@tu-berlin.de", "philipp@itp.tu-berlin.de", "~Tim_Landgraf1"], "authors": ["Leon Sixt", "Martin Schuessler", "Philipp Wei\u00df", "Tim Landgraf"], "keywords": ["Interpretable Machine Learning", "Counterfactuals", "Computer Vision", "Human Evaluation", "User Study"], "abstract": "Current state of the art computer vision applications rely on highly complex models. Their interpretability is mostly limited to post-hoc methods which are not guaranteed to be faithful to the model. To elucidate a model\u2019s decision, we present a novel interpretable model based on an invertible deep convolutional network. Our model generates meaningful, faithful, and ideal counterfactuals. Using PCA on the classifier\u2019s input, we can also create \u201cisofactuals\u201d\u2013 image interpolations with the same outcome but visually meaningful different features. Counter- and isofactuals can be used to identify positive and negative evidence in an image. This can also be visualized with heatmaps. We evaluate our approach against gradient-based attribution methods, which we find to produce meaningless adversarial perturbations.  Using our method, we reveal biases in three different datasets. In a human subject experiment, we test whether non-experts find our method useful to spot spurious correlations learned by a model. Our work is a step towards more trustworthy explanations for computer vision.", "one-sentence_summary": "We use invertible neural networks to generate ideal counterfactuals and isofactuals.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sixt|interpretability_through_invertibility_a_deep_convolutional_network_with_ideal_counterfactuals_and_isosurfaces", "supplementary_material": "/attachment/74b00c8e6e7ea503ac48760cf7c184afada069eb.zip", "pdf": "/pdf/83628964eb4ac5f2e2665ccd6ea038a08a70ff43.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1YGiD4CuCZ", "_bibtex": "@misc{\nsixt2021interpretability,\ntitle={Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces},\nauthor={Leon Sixt and Martin Schuessler and Philipp Wei{\\ss} and Tim Landgraf},\nyear={2021},\nurl={https://openreview.net/forum?id=8YFhXYe1Ps}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "8YFhXYe1Ps", "replyto": "8YFhXYe1Ps", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3736/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538070575, "tmdate": 1606915758180, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3736/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3736/-/Official_Review"}}}, {"id": "5sUGratera", "original": null, "number": 4, "cdate": 1604100490558, "ddate": null, "tcdate": 1604100490558, "tmdate": 1605023946471, "tddate": null, "forum": "8YFhXYe1Ps", "replyto": "8YFhXYe1Ps", "invitation": "ICLR.cc/2021/Conference/Paper3736/-/Official_Review", "content": {"title": "Relevant. Lacks clarity. Mildly convincing results.", "review": "In this paper, the authors propose a method for generating counterfactuals (visually \u201csimilar\u201d examples with different labels) and isofactuals (visually \u201cdifferent\u201d examples with the same label) using an invertible convolutional network. A human study shows that providing these counterfactual and isofactual images in a systematic way can help participants understand a model\u2019s bias better.\n\nInterpretability of machine learning models is becoming progressively more important as these models continue to proliferate in sensitive applications such as medicine, finance, law, etc. A large body of interpretability efforts is around post hoc methods where an explanation is generated given certain probes into a blackbox function. On the other hand, the proposed model imposes certain constraints on the model to yield these explanations. The paper has some interesting ideas, and the qualitative evaluation is helpful is conveying those ideas. The discussion around gradient wrt the input image vs directional derivative is fairly insightful with convincing qualitative and quantitative results (Fig. 4). However, I did have some conerns:\n\n1. The writing often lacks clarity and the usage of space can be more judicious. For example, the description of the main method is severely lacking, and is lumped into a few short paragraphs (section 2). I needed to reread the section a few times to understand the gist since important details are either missing or relegated to the appendix. I am still unsure about the development of isosurface section. On the other hand, excessive details are present in the evaluation section, e.g. in-depth discussion of mice characteristics. Focusing on elaborating the method, and maybe one less dataset would improve readability by moving extra evaluation to appendix. Something like, \u201cwe observe similar patterns with other tested datasets, which are presented in the appendix.\u201d\n\n2. The results of the human subject study are not very convincing. While the subjects were able to better detect model\u2019s biases with the systematic presentation using the proposed method, they also spuriously discovered irrelevant patterns (background and blocks). The appendix (Fig. 9) also shows that the subjects thought both the proposed method and the baseline were equally good. [Digression: Fig. 9 is poorly processed with missing words, repeated legend, shuffled axes, etc.).\n\n3. Minor formatting issues: mismatched quotes throughout (striped, zebra, etc.), Fig. 4 caption (there not ideal), \u201cdifferent to the original\u201d, etc.\n\nOverall, the paper has some good ideas and interesting analysis but falls short on clarity and fully convincing the reader about the results. I am marginally inclined for it to be accepted. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3736/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3736/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces", "authorids": ["~Leon_Sixt1", "schuessler@tu-berlin.de", "philipp@itp.tu-berlin.de", "~Tim_Landgraf1"], "authors": ["Leon Sixt", "Martin Schuessler", "Philipp Wei\u00df", "Tim Landgraf"], "keywords": ["Interpretable Machine Learning", "Counterfactuals", "Computer Vision", "Human Evaluation", "User Study"], "abstract": "Current state of the art computer vision applications rely on highly complex models. Their interpretability is mostly limited to post-hoc methods which are not guaranteed to be faithful to the model. To elucidate a model\u2019s decision, we present a novel interpretable model based on an invertible deep convolutional network. Our model generates meaningful, faithful, and ideal counterfactuals. Using PCA on the classifier\u2019s input, we can also create \u201cisofactuals\u201d\u2013 image interpolations with the same outcome but visually meaningful different features. Counter- and isofactuals can be used to identify positive and negative evidence in an image. This can also be visualized with heatmaps. We evaluate our approach against gradient-based attribution methods, which we find to produce meaningless adversarial perturbations.  Using our method, we reveal biases in three different datasets. In a human subject experiment, we test whether non-experts find our method useful to spot spurious correlations learned by a model. Our work is a step towards more trustworthy explanations for computer vision.", "one-sentence_summary": "We use invertible neural networks to generate ideal counterfactuals and isofactuals.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sixt|interpretability_through_invertibility_a_deep_convolutional_network_with_ideal_counterfactuals_and_isosurfaces", "supplementary_material": "/attachment/74b00c8e6e7ea503ac48760cf7c184afada069eb.zip", "pdf": "/pdf/83628964eb4ac5f2e2665ccd6ea038a08a70ff43.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1YGiD4CuCZ", "_bibtex": "@misc{\nsixt2021interpretability,\ntitle={Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces},\nauthor={Leon Sixt and Martin Schuessler and Philipp Wei{\\ss} and Tim Landgraf},\nyear={2021},\nurl={https://openreview.net/forum?id=8YFhXYe1Ps}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "8YFhXYe1Ps", "replyto": "8YFhXYe1Ps", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3736/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538070575, "tmdate": 1606915758180, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3736/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3736/-/Official_Review"}}}], "count": 16}