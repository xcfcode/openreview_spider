{"notes": [{"id": "rJl0ceBtDH", "original": "SJxWS1bYwS", "number": 2490, "cdate": 1569439893656, "ddate": null, "tcdate": 1569439893656, "tmdate": 1577168227208, "tddate": null, "forum": "rJl0ceBtDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["akulg2@illinois.edu", "yangliu@ucsc.edu"], "title": "Semi-Supervised Boosting via Self Labelling", "authors": ["Akul Goyal", "Yang Liu"], "pdf": "/pdf/c0ab73437ca5988485b87ecfb59b1be26fd83de5.pdf", "TL;DR": "In this paper we introduce Boosting via Self Labelling (BSL), a solution to semi-supervised boosting when there is a very limited access to labelled instances.", "abstract": "Attention to semi-supervised learning grows in machine learning as the price to expertly label data increases. Like most previous works in the area, we focus on improving an algorithm's ability to discover the inherent property of the entire dataset from a few expertly labelled samples. In this paper we introduce Boosting via Self Labelling (BSL), a solution to semi-supervised boosting when there is only limited access to labelled instances. Our goal is to learn a classifier that is trained on a data set that is generated by combining the generalization of different algorithms which have been trained with a limited amount of supervised training samples. Our method builds upon a combination of several different components. First, an inference aided ensemble algorithm developed on a set of weak classifiers will offer the initial noisy labels. Second, an agreement based estimation approach will return the average error rates of the noisy labels. Third and finally, a noise-resistant boosting algorithm will train over the noisy labels and their error rates to describe the underlying structure as closely as possible. We provide both analytical justifications and experimental results to back the performance of our model. Based on several benchmark datasets, our results demonstrate that BSL is able to outperform state-of-the-art semi-supervised methods consistently, achieving over 90% test accuracy with only 10% of the data being labelled.", "keywords": ["semi-supervised learning", "boosting", "noise-resistant"], "paperhash": "goyal|semisupervised_boosting_via_self_labelling", "original_pdf": "/attachment/c0ab73437ca5988485b87ecfb59b1be26fd83de5.pdf", "_bibtex": "@misc{\ngoyal2020semisupervised,\ntitle={Semi-Supervised Boosting via Self Labelling},\nauthor={Akul Goyal and Yang Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJl0ceBtDH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "6hMYK3CMlf", "original": null, "number": 1, "cdate": 1576798750304, "ddate": null, "tcdate": 1576798750304, "tmdate": 1576800885508, "tddate": null, "forum": "rJl0ceBtDH", "replyto": "rJl0ceBtDH", "invitation": "ICLR.cc/2020/Conference/Paper2490/-/Decision", "content": {"decision": "Reject", "comment": "The paper presents a new semi-supervised boosting approach. \n\nAs reviewers pointed out and AC acknowledge, the paper is not ready to publish in various aspects: (a) limited novelty/contribution, (b) reproducibility issue and (c) arguable assumptions.\n\nHence, I recommend rejection.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akulg2@illinois.edu", "yangliu@ucsc.edu"], "title": "Semi-Supervised Boosting via Self Labelling", "authors": ["Akul Goyal", "Yang Liu"], "pdf": "/pdf/c0ab73437ca5988485b87ecfb59b1be26fd83de5.pdf", "TL;DR": "In this paper we introduce Boosting via Self Labelling (BSL), a solution to semi-supervised boosting when there is a very limited access to labelled instances.", "abstract": "Attention to semi-supervised learning grows in machine learning as the price to expertly label data increases. Like most previous works in the area, we focus on improving an algorithm's ability to discover the inherent property of the entire dataset from a few expertly labelled samples. In this paper we introduce Boosting via Self Labelling (BSL), a solution to semi-supervised boosting when there is only limited access to labelled instances. Our goal is to learn a classifier that is trained on a data set that is generated by combining the generalization of different algorithms which have been trained with a limited amount of supervised training samples. Our method builds upon a combination of several different components. First, an inference aided ensemble algorithm developed on a set of weak classifiers will offer the initial noisy labels. Second, an agreement based estimation approach will return the average error rates of the noisy labels. Third and finally, a noise-resistant boosting algorithm will train over the noisy labels and their error rates to describe the underlying structure as closely as possible. We provide both analytical justifications and experimental results to back the performance of our model. Based on several benchmark datasets, our results demonstrate that BSL is able to outperform state-of-the-art semi-supervised methods consistently, achieving over 90% test accuracy with only 10% of the data being labelled.", "keywords": ["semi-supervised learning", "boosting", "noise-resistant"], "paperhash": "goyal|semisupervised_boosting_via_self_labelling", "original_pdf": "/attachment/c0ab73437ca5988485b87ecfb59b1be26fd83de5.pdf", "_bibtex": "@misc{\ngoyal2020semisupervised,\ntitle={Semi-Supervised Boosting via Self Labelling},\nauthor={Akul Goyal and Yang Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJl0ceBtDH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rJl0ceBtDH", "replyto": "rJl0ceBtDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795724678, "tmdate": 1576800276363, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2490/-/Decision"}}}, {"id": "S1lx1ydnsS", "original": null, "number": 5, "cdate": 1573842648503, "ddate": null, "tcdate": 1573842648503, "tmdate": 1573842648503, "tddate": null, "forum": "rJl0ceBtDH", "replyto": "HyguTu9msB", "invitation": "ICLR.cc/2020/Conference/Paper2490/-/Official_Comment", "content": {"title": "RE: Author response", "comment": "I have read the other reviews and authors' responses. They do not change my view of the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper2490/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2490/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akulg2@illinois.edu", "yangliu@ucsc.edu"], "title": "Semi-Supervised Boosting via Self Labelling", "authors": ["Akul Goyal", "Yang Liu"], "pdf": "/pdf/c0ab73437ca5988485b87ecfb59b1be26fd83de5.pdf", "TL;DR": "In this paper we introduce Boosting via Self Labelling (BSL), a solution to semi-supervised boosting when there is a very limited access to labelled instances.", "abstract": "Attention to semi-supervised learning grows in machine learning as the price to expertly label data increases. Like most previous works in the area, we focus on improving an algorithm's ability to discover the inherent property of the entire dataset from a few expertly labelled samples. In this paper we introduce Boosting via Self Labelling (BSL), a solution to semi-supervised boosting when there is only limited access to labelled instances. Our goal is to learn a classifier that is trained on a data set that is generated by combining the generalization of different algorithms which have been trained with a limited amount of supervised training samples. Our method builds upon a combination of several different components. First, an inference aided ensemble algorithm developed on a set of weak classifiers will offer the initial noisy labels. Second, an agreement based estimation approach will return the average error rates of the noisy labels. Third and finally, a noise-resistant boosting algorithm will train over the noisy labels and their error rates to describe the underlying structure as closely as possible. We provide both analytical justifications and experimental results to back the performance of our model. Based on several benchmark datasets, our results demonstrate that BSL is able to outperform state-of-the-art semi-supervised methods consistently, achieving over 90% test accuracy with only 10% of the data being labelled.", "keywords": ["semi-supervised learning", "boosting", "noise-resistant"], "paperhash": "goyal|semisupervised_boosting_via_self_labelling", "original_pdf": "/attachment/c0ab73437ca5988485b87ecfb59b1be26fd83de5.pdf", "_bibtex": "@misc{\ngoyal2020semisupervised,\ntitle={Semi-Supervised Boosting via Self Labelling},\nauthor={Akul Goyal and Yang Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJl0ceBtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJl0ceBtDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2490/Authors", "ICLR.cc/2020/Conference/Paper2490/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2490/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2490/Reviewers", "ICLR.cc/2020/Conference/Paper2490/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2490/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2490/Authors|ICLR.cc/2020/Conference/Paper2490/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140577, "tmdate": 1576860556377, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2490/Authors", "ICLR.cc/2020/Conference/Paper2490/Reviewers", "ICLR.cc/2020/Conference/Paper2490/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2490/-/Official_Comment"}}}, {"id": "SJl5VtlVsS", "original": null, "number": 4, "cdate": 1573288241703, "ddate": null, "tcdate": 1573288241703, "tmdate": 1573288241703, "tddate": null, "forum": "rJl0ceBtDH", "replyto": "SylvK_qmiH", "invitation": "ICLR.cc/2020/Conference/Paper2490/-/Official_Comment", "content": {"title": "still not reproducible", "comment": "Thank the authors for clarifying. But even when taking the clarification into account, I'd still be shocked if any educated researcher can reproduce the experiments given the details in the paper/comments. I definitely suggest the authors to put more focus on reproducibility. Thanks."}, "signatures": ["ICLR.cc/2020/Conference/Paper2490/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2490/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akulg2@illinois.edu", "yangliu@ucsc.edu"], "title": "Semi-Supervised Boosting via Self Labelling", "authors": ["Akul Goyal", "Yang Liu"], "pdf": "/pdf/c0ab73437ca5988485b87ecfb59b1be26fd83de5.pdf", "TL;DR": "In this paper we introduce Boosting via Self Labelling (BSL), a solution to semi-supervised boosting when there is a very limited access to labelled instances.", "abstract": "Attention to semi-supervised learning grows in machine learning as the price to expertly label data increases. Like most previous works in the area, we focus on improving an algorithm's ability to discover the inherent property of the entire dataset from a few expertly labelled samples. In this paper we introduce Boosting via Self Labelling (BSL), a solution to semi-supervised boosting when there is only limited access to labelled instances. Our goal is to learn a classifier that is trained on a data set that is generated by combining the generalization of different algorithms which have been trained with a limited amount of supervised training samples. Our method builds upon a combination of several different components. First, an inference aided ensemble algorithm developed on a set of weak classifiers will offer the initial noisy labels. Second, an agreement based estimation approach will return the average error rates of the noisy labels. Third and finally, a noise-resistant boosting algorithm will train over the noisy labels and their error rates to describe the underlying structure as closely as possible. We provide both analytical justifications and experimental results to back the performance of our model. Based on several benchmark datasets, our results demonstrate that BSL is able to outperform state-of-the-art semi-supervised methods consistently, achieving over 90% test accuracy with only 10% of the data being labelled.", "keywords": ["semi-supervised learning", "boosting", "noise-resistant"], "paperhash": "goyal|semisupervised_boosting_via_self_labelling", "original_pdf": "/attachment/c0ab73437ca5988485b87ecfb59b1be26fd83de5.pdf", "_bibtex": "@misc{\ngoyal2020semisupervised,\ntitle={Semi-Supervised Boosting via Self Labelling},\nauthor={Akul Goyal and Yang Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJl0ceBtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJl0ceBtDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2490/Authors", "ICLR.cc/2020/Conference/Paper2490/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2490/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2490/Reviewers", "ICLR.cc/2020/Conference/Paper2490/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2490/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2490/Authors|ICLR.cc/2020/Conference/Paper2490/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140577, "tmdate": 1576860556377, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2490/Authors", "ICLR.cc/2020/Conference/Paper2490/Reviewers", "ICLR.cc/2020/Conference/Paper2490/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2490/-/Official_Comment"}}}, {"id": "HyguTu9msB", "original": null, "number": 3, "cdate": 1573263552148, "ddate": null, "tcdate": 1573263552148, "tmdate": 1573263552148, "tddate": null, "forum": "rJl0ceBtDH", "replyto": "BJlqncT3KH", "invitation": "ICLR.cc/2020/Conference/Paper2490/-/Official_Comment", "content": {"title": "Addressing Some Concerns Over Error Rate", "comment": "The parameters on the model could have been explained better. The 20 experiments was running the experiment 20 times with a random split occurring every time. Variance was something that could have helped add, but is not extremely important.\n\nThe point of the estimated error and actual error was that they were very close. You cannot get an exact estimate."}, "signatures": ["ICLR.cc/2020/Conference/Paper2490/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2490/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akulg2@illinois.edu", "yangliu@ucsc.edu"], "title": "Semi-Supervised Boosting via Self Labelling", "authors": ["Akul Goyal", "Yang Liu"], "pdf": "/pdf/c0ab73437ca5988485b87ecfb59b1be26fd83de5.pdf", "TL;DR": "In this paper we introduce Boosting via Self Labelling (BSL), a solution to semi-supervised boosting when there is a very limited access to labelled instances.", "abstract": "Attention to semi-supervised learning grows in machine learning as the price to expertly label data increases. Like most previous works in the area, we focus on improving an algorithm's ability to discover the inherent property of the entire dataset from a few expertly labelled samples. In this paper we introduce Boosting via Self Labelling (BSL), a solution to semi-supervised boosting when there is only limited access to labelled instances. Our goal is to learn a classifier that is trained on a data set that is generated by combining the generalization of different algorithms which have been trained with a limited amount of supervised training samples. Our method builds upon a combination of several different components. First, an inference aided ensemble algorithm developed on a set of weak classifiers will offer the initial noisy labels. Second, an agreement based estimation approach will return the average error rates of the noisy labels. Third and finally, a noise-resistant boosting algorithm will train over the noisy labels and their error rates to describe the underlying structure as closely as possible. We provide both analytical justifications and experimental results to back the performance of our model. Based on several benchmark datasets, our results demonstrate that BSL is able to outperform state-of-the-art semi-supervised methods consistently, achieving over 90% test accuracy with only 10% of the data being labelled.", "keywords": ["semi-supervised learning", "boosting", "noise-resistant"], "paperhash": "goyal|semisupervised_boosting_via_self_labelling", "original_pdf": "/attachment/c0ab73437ca5988485b87ecfb59b1be26fd83de5.pdf", "_bibtex": "@misc{\ngoyal2020semisupervised,\ntitle={Semi-Supervised Boosting via Self Labelling},\nauthor={Akul Goyal and Yang Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJl0ceBtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJl0ceBtDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2490/Authors", "ICLR.cc/2020/Conference/Paper2490/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2490/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2490/Reviewers", "ICLR.cc/2020/Conference/Paper2490/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2490/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2490/Authors|ICLR.cc/2020/Conference/Paper2490/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140577, "tmdate": 1576860556377, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2490/Authors", "ICLR.cc/2020/Conference/Paper2490/Reviewers", "ICLR.cc/2020/Conference/Paper2490/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2490/-/Official_Comment"}}}, {"id": "SylvK_qmiH", "original": null, "number": 2, "cdate": 1573263487137, "ddate": null, "tcdate": 1573263487137, "tmdate": 1573263487137, "tddate": null, "forum": "rJl0ceBtDH", "replyto": "HJlClhzCtS", "invitation": "ICLR.cc/2020/Conference/Paper2490/-/Official_Comment", "content": {"title": "Clarification of How we conducted the Experiments", "comment": "Turning the linear regression into binary case was simply putting a limit whether the price was over a certain limit or not.\n\nThe parameters for many of the classifiers were based on the original values that were given based on the scikit learn models."}, "signatures": ["ICLR.cc/2020/Conference/Paper2490/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2490/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akulg2@illinois.edu", "yangliu@ucsc.edu"], "title": "Semi-Supervised Boosting via Self Labelling", "authors": ["Akul Goyal", "Yang Liu"], "pdf": "/pdf/c0ab73437ca5988485b87ecfb59b1be26fd83de5.pdf", "TL;DR": "In this paper we introduce Boosting via Self Labelling (BSL), a solution to semi-supervised boosting when there is a very limited access to labelled instances.", "abstract": "Attention to semi-supervised learning grows in machine learning as the price to expertly label data increases. Like most previous works in the area, we focus on improving an algorithm's ability to discover the inherent property of the entire dataset from a few expertly labelled samples. In this paper we introduce Boosting via Self Labelling (BSL), a solution to semi-supervised boosting when there is only limited access to labelled instances. Our goal is to learn a classifier that is trained on a data set that is generated by combining the generalization of different algorithms which have been trained with a limited amount of supervised training samples. Our method builds upon a combination of several different components. First, an inference aided ensemble algorithm developed on a set of weak classifiers will offer the initial noisy labels. Second, an agreement based estimation approach will return the average error rates of the noisy labels. Third and finally, a noise-resistant boosting algorithm will train over the noisy labels and their error rates to describe the underlying structure as closely as possible. We provide both analytical justifications and experimental results to back the performance of our model. Based on several benchmark datasets, our results demonstrate that BSL is able to outperform state-of-the-art semi-supervised methods consistently, achieving over 90% test accuracy with only 10% of the data being labelled.", "keywords": ["semi-supervised learning", "boosting", "noise-resistant"], "paperhash": "goyal|semisupervised_boosting_via_self_labelling", "original_pdf": "/attachment/c0ab73437ca5988485b87ecfb59b1be26fd83de5.pdf", "_bibtex": "@misc{\ngoyal2020semisupervised,\ntitle={Semi-Supervised Boosting via Self Labelling},\nauthor={Akul Goyal and Yang Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJl0ceBtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJl0ceBtDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2490/Authors", "ICLR.cc/2020/Conference/Paper2490/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2490/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2490/Reviewers", "ICLR.cc/2020/Conference/Paper2490/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2490/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2490/Authors|ICLR.cc/2020/Conference/Paper2490/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140577, "tmdate": 1576860556377, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2490/Authors", "ICLR.cc/2020/Conference/Paper2490/Reviewers", "ICLR.cc/2020/Conference/Paper2490/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2490/-/Official_Comment"}}}, {"id": "HJx8lu9QjS", "original": null, "number": 1, "cdate": 1573263341990, "ddate": null, "tcdate": 1573263341990, "tmdate": 1573263341990, "tddate": null, "forum": "rJl0ceBtDH", "replyto": "rkluCySRYr", "invitation": "ICLR.cc/2020/Conference/Paper2490/-/Official_Comment", "content": {"title": "Clarification of Our Novelty", "comment": "The novelty in this paper lies in a couple different avenues. Overall this paper is implemented as a framework to help increase the overall prediction accuracy on semi-supervised data. Beyond the framework, the paper produced a novel approach of applying the Natarajan et al. 2013 loss function to a set of supervised learning algorithms under a crowdsourcing environment. The theoretical proof and implementation of this function are shown to perform within the experimental section compared to other semi-supervised algorithms.\n\nWe evaluated our paper on the same datasets that were used in the 2013 paper.\n\nMultiple works that achieve 99 percent accuracy when all training data known are not something that would be interesting to put in the paper as our semi-supervised methods would not compare very well."}, "signatures": ["ICLR.cc/2020/Conference/Paper2490/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2490/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akulg2@illinois.edu", "yangliu@ucsc.edu"], "title": "Semi-Supervised Boosting via Self Labelling", "authors": ["Akul Goyal", "Yang Liu"], "pdf": "/pdf/c0ab73437ca5988485b87ecfb59b1be26fd83de5.pdf", "TL;DR": "In this paper we introduce Boosting via Self Labelling (BSL), a solution to semi-supervised boosting when there is a very limited access to labelled instances.", "abstract": "Attention to semi-supervised learning grows in machine learning as the price to expertly label data increases. Like most previous works in the area, we focus on improving an algorithm's ability to discover the inherent property of the entire dataset from a few expertly labelled samples. In this paper we introduce Boosting via Self Labelling (BSL), a solution to semi-supervised boosting when there is only limited access to labelled instances. Our goal is to learn a classifier that is trained on a data set that is generated by combining the generalization of different algorithms which have been trained with a limited amount of supervised training samples. Our method builds upon a combination of several different components. First, an inference aided ensemble algorithm developed on a set of weak classifiers will offer the initial noisy labels. Second, an agreement based estimation approach will return the average error rates of the noisy labels. Third and finally, a noise-resistant boosting algorithm will train over the noisy labels and their error rates to describe the underlying structure as closely as possible. We provide both analytical justifications and experimental results to back the performance of our model. Based on several benchmark datasets, our results demonstrate that BSL is able to outperform state-of-the-art semi-supervised methods consistently, achieving over 90% test accuracy with only 10% of the data being labelled.", "keywords": ["semi-supervised learning", "boosting", "noise-resistant"], "paperhash": "goyal|semisupervised_boosting_via_self_labelling", "original_pdf": "/attachment/c0ab73437ca5988485b87ecfb59b1be26fd83de5.pdf", "_bibtex": "@misc{\ngoyal2020semisupervised,\ntitle={Semi-Supervised Boosting via Self Labelling},\nauthor={Akul Goyal and Yang Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJl0ceBtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJl0ceBtDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2490/Authors", "ICLR.cc/2020/Conference/Paper2490/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2490/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2490/Reviewers", "ICLR.cc/2020/Conference/Paper2490/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2490/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2490/Authors|ICLR.cc/2020/Conference/Paper2490/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140577, "tmdate": 1576860556377, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2490/Authors", "ICLR.cc/2020/Conference/Paper2490/Reviewers", "ICLR.cc/2020/Conference/Paper2490/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2490/-/Official_Comment"}}}, {"id": "BJlqncT3KH", "original": null, "number": 1, "cdate": 1571769010041, "ddate": null, "tcdate": 1571769010041, "tmdate": 1572972331627, "tddate": null, "forum": "rJl0ceBtDH", "replyto": "rJl0ceBtDH", "invitation": "ICLR.cc/2020/Conference/Paper2490/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "In this paper, the authors present an approach for semi-supervised learning which combines noisy labels with boosting. In a first step, the labeled instances are used to train a set of classifiers, and these are used to create noisy labels for the unlabeled instances. Then, an EM procedure is used to estimate the noise level of each instance. Finally, a version of AdaBoost which accounts for instance noise levels is proposed to create a final classifier. A limited set of experiments suggests the proposed approach is competitive with existing approaches.\n\nMajor Comments\n\nAs a non-expert in this area, I had trouble identifying the novel contributions of this work. For example, many of the results in Section 3 (noise-resistant AdaBoost) seem to replicate, or follow closely, the results of [Natarajan et al., 2013]. Similarly, using EM to assign pseudo-labels has been extensively studied in the literature [Lee, WREPL 2013; Chapelle and Zien, AISTATS 2005; Kang et al., ECCV 2018; Rottman et al., ICMLA 2018]. \n\n\nThe experiments are very poorly described, so it is difficult to gauge if they are valid:\n\nMost importantly, the authors point out that the estimated error rates do not always match the actual error rates. Since this seems to be one of the most important factors of the proposed approach, further investigation should be performed to answer questions like: why is the error rate not estimated well? on what type of datasets? can more/better supervised learners help? In some cases (Diabetes, Thyroid, Heart), the actual noise rate increases with more labeled samples. What does that mean?\n\nSecond, the proposed approach seems to have a number of important hyperparameters, including the number of supervised models trained and their hyperparameters, the parameters of the Beta distribution used as a prior on the noise estimation, and the hyperparameters of the AdaBoost algorithm. Likewise, all of the competing algorithms also have hyperparameters which are known to affect performance (e.g., learning rate for NNs). The paper does not mention how (or if) a validation set was used to select these.\n\nThird, while the caption of Table 1 mentions that 20 trials were used, it is not clear if this was some sort of k-fold cross validation, Monte Carlo, cross validation, the same splits but with different random seeds, etc. Additionally, the variance across the different trials should be given; otherwise, it is not possible to tell if any of the empirical results are significant.\n\nMinor Comments:\n\nThe references are not consistently formatted.\n\nThis paper is very notation heavy. It would be helpful to include a \u201ctable of symbols\u201d for the reader in an appendix.\n\nAdditionally, the notation in the paper is not consistent. For example, both \u201c$M$\u201d and \u201c$\\mathcal{M}$ are used to indicate the number of models trained on the labeled data. Later on, \u201c$\\mathcal{M}$\u201d is also used to refer to the set of trained classifiers. The first bullet point in Step 3 of the pseudocode seems to suggest that each classifier is trained on a single labeled data point. The equation at the bottom of Page 5 used \\theta, but it does not seem to be defined.\n\nThe discussion on experts, spammers, and adversaries could be helpful if this terminology were used throughout the paper; however, it is used in only one paragraph. \n\nThe main body of the paper should mention that proofs are given in the appendices.\n\nFor context, if may be helpful to mention that graph convolutional networks and other representation learning techniques are commonly used for semi-supervised learning (e.g., [Kipf and Welling, ICML 2016]). Those approaches are quite different (and lack any sort of theoretical guarantees, for the most part), though, so empirical comparisons may not be so meaningful.\n\nIt would be helpful to give a sentence or two on the intuition behind what the proofs are showing. For a non-expert, they are very difficult to follow.\n\nDo the various proofs still hold when the datasets are artificially balanced (with respect to the last paragraph in Section 4)?\n\nIt would be helpful to include the performance using the complete labeled dataset for comparison.\n\nStratified sampling could be used to ensure both classes are present in the training data. Also, \u201c0.99%\u201d -> \u201c99%\u201d.\n\nBesides accuracy, some measure like AuROC or the F1 score which account for class imbalance should be given.\n\nTypos, etc.\n\n\u201cLogitboost tested against\u201d -> \u201cLogitboost were tested against\u201d\n\n\n\u201ctherefore is not\u201d -> \u201ctherefore, having a lot of labeled data is not\u201d\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2490/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2490/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akulg2@illinois.edu", "yangliu@ucsc.edu"], "title": "Semi-Supervised Boosting via Self Labelling", "authors": ["Akul Goyal", "Yang Liu"], "pdf": "/pdf/c0ab73437ca5988485b87ecfb59b1be26fd83de5.pdf", "TL;DR": "In this paper we introduce Boosting via Self Labelling (BSL), a solution to semi-supervised boosting when there is a very limited access to labelled instances.", "abstract": "Attention to semi-supervised learning grows in machine learning as the price to expertly label data increases. Like most previous works in the area, we focus on improving an algorithm's ability to discover the inherent property of the entire dataset from a few expertly labelled samples. In this paper we introduce Boosting via Self Labelling (BSL), a solution to semi-supervised boosting when there is only limited access to labelled instances. Our goal is to learn a classifier that is trained on a data set that is generated by combining the generalization of different algorithms which have been trained with a limited amount of supervised training samples. Our method builds upon a combination of several different components. First, an inference aided ensemble algorithm developed on a set of weak classifiers will offer the initial noisy labels. Second, an agreement based estimation approach will return the average error rates of the noisy labels. Third and finally, a noise-resistant boosting algorithm will train over the noisy labels and their error rates to describe the underlying structure as closely as possible. We provide both analytical justifications and experimental results to back the performance of our model. Based on several benchmark datasets, our results demonstrate that BSL is able to outperform state-of-the-art semi-supervised methods consistently, achieving over 90% test accuracy with only 10% of the data being labelled.", "keywords": ["semi-supervised learning", "boosting", "noise-resistant"], "paperhash": "goyal|semisupervised_boosting_via_self_labelling", "original_pdf": "/attachment/c0ab73437ca5988485b87ecfb59b1be26fd83de5.pdf", "_bibtex": "@misc{\ngoyal2020semisupervised,\ntitle={Semi-Supervised Boosting via Self Labelling},\nauthor={Akul Goyal and Yang Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJl0ceBtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJl0ceBtDH", "replyto": "rJl0ceBtDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2490/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2490/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575576604290, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2490/Reviewers"], "noninvitees": [], "tcdate": 1570237722095, "tmdate": 1575576604303, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2490/-/Official_Review"}}}, {"id": "HJlClhzCtS", "original": null, "number": 2, "cdate": 1571855349707, "ddate": null, "tcdate": 1571855349707, "tmdate": 1572972331579, "tddate": null, "forum": "rJl0ceBtDH", "replyto": "rJl0ceBtDH", "invitation": "ICLR.cc/2020/Conference/Paper2490/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a new semi-supervised boosting approach. The approach takes a set of supervised learning algorithms to simulate \"crowd-source\" labels of the unlabeled data, which are then used to generate a noisy label per unlabeled instance. The noise level is then estimated with an agreement-based scheme, and fed to a modified AdaBoost algorithm that is more noise-tolerant given the noise level. Some theoretical guarantee of the modified AdaBoost algorithm is derived and promising experiment results are demonstrated.\n\nMy suggestion is to reject the paper, with the following key reasons.\n\n(1) Contribution is insufficient, or perhaps not well-highlighted. For the three pieces of contribution, Section 4.1 (self-labeling, which is highlighted within the title) seems to be a trivial borrowing of an existing idea in crowd sourcing from 1979. It is not clear whether Section 4.2 (error estimation) is an original contribution or not, but even if it is original Lemma 1 seems marginally trivial. Section 3 (noise-resistant AdaBoost) plugs a known surrogate loss for noisy labels into AdaBoost. But despite the ugly math, the results seem to be equivalent to a heuristically-shrunk alpha_t for AdaBoost. None of the pieces seem to make a solid contribution to the problem of interest.\n\n(2) Assumptions are not reasonable. Section 3 and Sections 4.2 both rely on \"homogeneous error rates\" which does not seem to be the case when the noise is generated from classifier-target mismatch. In particular, the noisy will only happen in mismatch areas, and not happen in other areas, making it non-homogeneous. The authors did not discuss the rationality of this assumption and/or how it affects the designed approach. In Section 4.2, there is another assumption that \"in practice we can balance the dataset\", which might be true for the labeled part through sampling, but not necessarily true for the unlabeled part. So it is not clear whether this assumption can be met. Section 4.2 also assumes that \"the probability can be estimated through the data\" but did not mention how large the data needs to be for an accurate estimation.\n\n(3) Experiments cannot be easily replicated. To begin with, the authors claim to use 10 classifiers from scikit-learn as the initial labeler, but the exact 10 (including parameters) are not pinged down. In the data sets, there is a procedure \"or turned linear regression datasets into binary labels\" that does not seem sufficiently clear for replication. It is not clear whether \"feature normalization\" considers only the training set or the whole training+test set.\n\nHaving said that, there are some other suggestions:\n\n(4) Writing needs improvement. Many of the parts contains unnecessarily ugly math notations without motivation. Even the core Section 3 looks like a LaTeX math demo than a clear illustration of scientific ideas.\n\n(5) It is not clear what the importance of Theorem 1 is. There doesn't seem to be a guarantee of gamma_t > 0 given the authors' definition of hat{epsilon}_t (worse case error of the two classes), and then the first part of Theorem 1 is not fast decreasing. It is not clear whether the N in the second term is N_noisy. In any case, the theorem is not clearly described enough to help understand the contribution of the paper.\n\n(6) A baseline that should be considered is to treat the noisy labels as \"soft labels\" and then apply confidence-based boosting.\n\nImproved Boosting Algorithms Using Confidence-rated Predictions, Schapire and Singer 1999.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2490/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2490/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akulg2@illinois.edu", "yangliu@ucsc.edu"], "title": "Semi-Supervised Boosting via Self Labelling", "authors": ["Akul Goyal", "Yang Liu"], "pdf": "/pdf/c0ab73437ca5988485b87ecfb59b1be26fd83de5.pdf", "TL;DR": "In this paper we introduce Boosting via Self Labelling (BSL), a solution to semi-supervised boosting when there is a very limited access to labelled instances.", "abstract": "Attention to semi-supervised learning grows in machine learning as the price to expertly label data increases. Like most previous works in the area, we focus on improving an algorithm's ability to discover the inherent property of the entire dataset from a few expertly labelled samples. In this paper we introduce Boosting via Self Labelling (BSL), a solution to semi-supervised boosting when there is only limited access to labelled instances. Our goal is to learn a classifier that is trained on a data set that is generated by combining the generalization of different algorithms which have been trained with a limited amount of supervised training samples. Our method builds upon a combination of several different components. First, an inference aided ensemble algorithm developed on a set of weak classifiers will offer the initial noisy labels. Second, an agreement based estimation approach will return the average error rates of the noisy labels. Third and finally, a noise-resistant boosting algorithm will train over the noisy labels and their error rates to describe the underlying structure as closely as possible. We provide both analytical justifications and experimental results to back the performance of our model. Based on several benchmark datasets, our results demonstrate that BSL is able to outperform state-of-the-art semi-supervised methods consistently, achieving over 90% test accuracy with only 10% of the data being labelled.", "keywords": ["semi-supervised learning", "boosting", "noise-resistant"], "paperhash": "goyal|semisupervised_boosting_via_self_labelling", "original_pdf": "/attachment/c0ab73437ca5988485b87ecfb59b1be26fd83de5.pdf", "_bibtex": "@misc{\ngoyal2020semisupervised,\ntitle={Semi-Supervised Boosting via Self Labelling},\nauthor={Akul Goyal and Yang Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJl0ceBtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJl0ceBtDH", "replyto": "rJl0ceBtDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2490/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2490/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575576604290, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2490/Reviewers"], "noninvitees": [], "tcdate": 1570237722095, "tmdate": 1575576604303, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2490/-/Official_Review"}}}, {"id": "rkluCySRYr", "original": null, "number": 3, "cdate": 1571864527749, "ddate": null, "tcdate": 1571864527749, "tmdate": 1572972331531, "tddate": null, "forum": "rJl0ceBtDH", "replyto": "rJl0ceBtDH", "invitation": "ICLR.cc/2020/Conference/Paper2490/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposed a method combining boosting with semi-supervised learning to handle classification problems when only partial data points have labels available. The method first trains a classifier with the true labels, and predicts labels for unlabeled data (with some error rate), then a bias boosting method is applied on the larger dataset to construct the final classifier. I find the topic interesting but I'm concerned about the novelty level of the paper. Here some further comments.\n\n1. Theorem 1 seems interesting and it will form a strong result if the assumption \\rhp_{+{ = \\rho_{-} is removed.\n\n2. Lemma 1 \"in practice ...\", how to balance the dataset to make sure the two class have similar size? The labeled data can be tailored to ensure this, but one cannot make it happen for the unlabeled data.\n\n3. In the experiments, UCI datasets seem not comprehensive to demonstrate the advantage of the proposed method. More datasets with higher volume could be better.  Also, how is the result compared to the case when all the training data labels are known? What is the gap like?\n\n4. Some typos and writing issues, like equation (6) unbalanced brackets."}, "signatures": ["ICLR.cc/2020/Conference/Paper2490/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2490/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akulg2@illinois.edu", "yangliu@ucsc.edu"], "title": "Semi-Supervised Boosting via Self Labelling", "authors": ["Akul Goyal", "Yang Liu"], "pdf": "/pdf/c0ab73437ca5988485b87ecfb59b1be26fd83de5.pdf", "TL;DR": "In this paper we introduce Boosting via Self Labelling (BSL), a solution to semi-supervised boosting when there is a very limited access to labelled instances.", "abstract": "Attention to semi-supervised learning grows in machine learning as the price to expertly label data increases. Like most previous works in the area, we focus on improving an algorithm's ability to discover the inherent property of the entire dataset from a few expertly labelled samples. In this paper we introduce Boosting via Self Labelling (BSL), a solution to semi-supervised boosting when there is only limited access to labelled instances. Our goal is to learn a classifier that is trained on a data set that is generated by combining the generalization of different algorithms which have been trained with a limited amount of supervised training samples. Our method builds upon a combination of several different components. First, an inference aided ensemble algorithm developed on a set of weak classifiers will offer the initial noisy labels. Second, an agreement based estimation approach will return the average error rates of the noisy labels. Third and finally, a noise-resistant boosting algorithm will train over the noisy labels and their error rates to describe the underlying structure as closely as possible. We provide both analytical justifications and experimental results to back the performance of our model. Based on several benchmark datasets, our results demonstrate that BSL is able to outperform state-of-the-art semi-supervised methods consistently, achieving over 90% test accuracy with only 10% of the data being labelled.", "keywords": ["semi-supervised learning", "boosting", "noise-resistant"], "paperhash": "goyal|semisupervised_boosting_via_self_labelling", "original_pdf": "/attachment/c0ab73437ca5988485b87ecfb59b1be26fd83de5.pdf", "_bibtex": "@misc{\ngoyal2020semisupervised,\ntitle={Semi-Supervised Boosting via Self Labelling},\nauthor={Akul Goyal and Yang Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJl0ceBtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJl0ceBtDH", "replyto": "rJl0ceBtDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2490/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2490/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575576604290, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2490/Reviewers"], "noninvitees": [], "tcdate": 1570237722095, "tmdate": 1575576604303, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2490/-/Official_Review"}}}], "count": 10}