{"notes": [{"id": "rkx1m2C5YQ", "original": "rklhjGAqKm", "number": 1319, "cdate": 1538087958815, "ddate": null, "tcdate": 1538087958815, "tmdate": 1545355384207, "tddate": null, "forum": "rkx1m2C5YQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Recurrent Kalman Networks: Factorized Inference in High-Dimensional Deep Feature Spaces", "abstract": "In order to integrate uncertainty estimates into deep time-series modelling, Kalman Filters (KFs) (Kalman et al., 1960) have been integrated with deep learning models. Yet, such approaches typically rely on approximate inference techniques such as variational inference which makes learning more complex and often less scalable due to approximation errors. We propose a new deep approach to Kalman filtering which can be learned directly in an end-to-end manner using backpropagation without additional approximations. Our approach uses a high-dimensional factorized latent state representation for which the Kalman updates simplify to scalar operations and thus avoids hard to backpropagate, computationally heavy and potentially unstable matrix inversions. Moreover, we use locally linear dynamic models to efficiently propagate the latent state to the next time\nstep. While our locally linear modelling and factorization assumptions are in general not true for the original low-dimensional state space of the system, the network finds a high-dimensional latent space where these assumptions hold to perform efficient inference. This state representation is learned jointly with the transition and noise models. The resulting network architecture, which we call Recurrent Kalman Network (RKN), can be used for any time-series data, similar to a LSTM (Hochreiter and Schmidhuber, 1997) but uses an explicit representation of uncertainty. As shown by our experiments, the RKN obtains much more accurate uncertainty estimates than an LSTM or Gated Recurrent Units (GRUs) (Cho et al., 2014) while also showing a slightly improved prediction performance and outperforms various recent generative models on an image imputation task.", "keywords": ["state estimation", "recurrent neural networks", "Kalman Filter", "deep learning"], "authorids": ["philippbecker93@googlemail.com", "hpandya@lincoln.ac.uk", "gebhardt@ias.tu-darmstadt.de", "irobotcheng@gmail.com", "gneumann@lincoln.ac.uk"], "authors": ["Philipp Becker", "Harit Pandya", "Gregor H.W. Gebhardt", "Cheng Zhao", "Gerhard Neumann"], "TL;DR": "Kalman Filter based recurrent model for efficient state estimation,  principled uncertainty handling and end to end learning of dynamic models in high dimensional spaces.", "pdf": "/pdf/a4e11950337d63d757521b324d8c10575c7d19dd.pdf", "paperhash": "becker|recurrent_kalman_networks_factorized_inference_in_highdimensional_deep_feature_spaces", "_bibtex": "@misc{\nbecker2019recurrent,\ntitle={Recurrent Kalman Networks: Factorized Inference in High-Dimensional Deep Feature Spaces},\nauthor={Philipp Becker and Harit Pandya and Gregor H.W. Gebhardt and Cheng Zhao and Gerhard Neumann},\nyear={2019},\nurl={https://openreview.net/forum?id=rkx1m2C5YQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1l-kGSWgE", "original": null, "number": 1, "cdate": 1544798680752, "ddate": null, "tcdate": 1544798680752, "tmdate": 1545354525890, "tddate": null, "forum": "rkx1m2C5YQ", "replyto": "rkx1m2C5YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1319/Meta_Review", "content": {"metareview": "A lot of work has appeared recently on recurrent state space models. So although this paper is in general considered favorable by the reviewers it is unclear exactly how the paper places itself in that (crowded) space. So rejection with a strong encouragement to update and resubmission is encouraged. ", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Borderline - but missing clarity"}, "signatures": ["ICLR.cc/2019/Conference/Paper1319/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1319/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Kalman Networks: Factorized Inference in High-Dimensional Deep Feature Spaces", "abstract": "In order to integrate uncertainty estimates into deep time-series modelling, Kalman Filters (KFs) (Kalman et al., 1960) have been integrated with deep learning models. Yet, such approaches typically rely on approximate inference techniques such as variational inference which makes learning more complex and often less scalable due to approximation errors. We propose a new deep approach to Kalman filtering which can be learned directly in an end-to-end manner using backpropagation without additional approximations. Our approach uses a high-dimensional factorized latent state representation for which the Kalman updates simplify to scalar operations and thus avoids hard to backpropagate, computationally heavy and potentially unstable matrix inversions. Moreover, we use locally linear dynamic models to efficiently propagate the latent state to the next time\nstep. While our locally linear modelling and factorization assumptions are in general not true for the original low-dimensional state space of the system, the network finds a high-dimensional latent space where these assumptions hold to perform efficient inference. This state representation is learned jointly with the transition and noise models. The resulting network architecture, which we call Recurrent Kalman Network (RKN), can be used for any time-series data, similar to a LSTM (Hochreiter and Schmidhuber, 1997) but uses an explicit representation of uncertainty. As shown by our experiments, the RKN obtains much more accurate uncertainty estimates than an LSTM or Gated Recurrent Units (GRUs) (Cho et al., 2014) while also showing a slightly improved prediction performance and outperforms various recent generative models on an image imputation task.", "keywords": ["state estimation", "recurrent neural networks", "Kalman Filter", "deep learning"], "authorids": ["philippbecker93@googlemail.com", "hpandya@lincoln.ac.uk", "gebhardt@ias.tu-darmstadt.de", "irobotcheng@gmail.com", "gneumann@lincoln.ac.uk"], "authors": ["Philipp Becker", "Harit Pandya", "Gregor H.W. Gebhardt", "Cheng Zhao", "Gerhard Neumann"], "TL;DR": "Kalman Filter based recurrent model for efficient state estimation,  principled uncertainty handling and end to end learning of dynamic models in high dimensional spaces.", "pdf": "/pdf/a4e11950337d63d757521b324d8c10575c7d19dd.pdf", "paperhash": "becker|recurrent_kalman_networks_factorized_inference_in_highdimensional_deep_feature_spaces", "_bibtex": "@misc{\nbecker2019recurrent,\ntitle={Recurrent Kalman Networks: Factorized Inference in High-Dimensional Deep Feature Spaces},\nauthor={Philipp Becker and Harit Pandya and Gregor H.W. Gebhardt and Cheng Zhao and Gerhard Neumann},\nyear={2019},\nurl={https://openreview.net/forum?id=rkx1m2C5YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1319/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352881844, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkx1m2C5YQ", "replyto": "rkx1m2C5YQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1319/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1319/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1319/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352881844}}}, {"id": "Syx2wN5c37", "original": null, "number": 1, "cdate": 1541215332219, "ddate": null, "tcdate": 1541215332219, "tmdate": 1543288230115, "tddate": null, "forum": "rkx1m2C5YQ", "replyto": "rkx1m2C5YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1319/Official_Review", "content": {"title": "Nice work but misses some significant work from the literature ", "review": "This paper proposes, Recurrent Kalman Network, a modified Kaman filter in which the latent dynamics is projected into a higher dimensional space; efficient inference in this high-dimensional latent space is possible due to the space being locally linear. The state representation, transition, and observation models are learned jointly by backpropagation.  \nThe paper is well written and the model is clearly explained; I also like the simplicity of the idea that uses the same machinery of Kalman filter.  However, I believe the authors can improve the presentation of the model and empirical evaluation. \n\nIn terms of model presentation, the authors can compare the model with a large set of deep recurrent models that have recently been proposed for modeling time series with nonlinear latent dynamics (e.g. Variational Sequential Monte Carlo, Structured inference networks for nonlinear state space models, Black box variational inference for state space models, Composing graphical models with neural networks for structured representations and fast inference, etc.). For instance, a table of some of these models with their pros and cons can be helpful for guiding the reader.\n\nIn terms of model evaluation, the paper needs a better evaluation section specifically on the generative models (see examples above) that are much  more suitable for modeling uncertainty compared to LSTM/GRU. More specifically, another approach for alleviating the limitations of Kalman filter would be to use non-linear transitions based on some non-linear functions approximation. This approach has been proposed in deep Kaman filter (Krishnan 2015) and it would be interesting to see how well your model performs compared to that for modeling uncertainty and computing predictive log-likelihood. \n\nIn conclusion, I think the paper presents a nice idea but it requires more work in order to pass the ICLR acceptance threshold.\n\n----------------------------------------------------------\nThe authors have addressed my comments and as a result I changed my rating to 6. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1319/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Recurrent Kalman Networks: Factorized Inference in High-Dimensional Deep Feature Spaces", "abstract": "In order to integrate uncertainty estimates into deep time-series modelling, Kalman Filters (KFs) (Kalman et al., 1960) have been integrated with deep learning models. Yet, such approaches typically rely on approximate inference techniques such as variational inference which makes learning more complex and often less scalable due to approximation errors. We propose a new deep approach to Kalman filtering which can be learned directly in an end-to-end manner using backpropagation without additional approximations. Our approach uses a high-dimensional factorized latent state representation for which the Kalman updates simplify to scalar operations and thus avoids hard to backpropagate, computationally heavy and potentially unstable matrix inversions. Moreover, we use locally linear dynamic models to efficiently propagate the latent state to the next time\nstep. While our locally linear modelling and factorization assumptions are in general not true for the original low-dimensional state space of the system, the network finds a high-dimensional latent space where these assumptions hold to perform efficient inference. This state representation is learned jointly with the transition and noise models. The resulting network architecture, which we call Recurrent Kalman Network (RKN), can be used for any time-series data, similar to a LSTM (Hochreiter and Schmidhuber, 1997) but uses an explicit representation of uncertainty. As shown by our experiments, the RKN obtains much more accurate uncertainty estimates than an LSTM or Gated Recurrent Units (GRUs) (Cho et al., 2014) while also showing a slightly improved prediction performance and outperforms various recent generative models on an image imputation task.", "keywords": ["state estimation", "recurrent neural networks", "Kalman Filter", "deep learning"], "authorids": ["philippbecker93@googlemail.com", "hpandya@lincoln.ac.uk", "gebhardt@ias.tu-darmstadt.de", "irobotcheng@gmail.com", "gneumann@lincoln.ac.uk"], "authors": ["Philipp Becker", "Harit Pandya", "Gregor H.W. Gebhardt", "Cheng Zhao", "Gerhard Neumann"], "TL;DR": "Kalman Filter based recurrent model for efficient state estimation,  principled uncertainty handling and end to end learning of dynamic models in high dimensional spaces.", "pdf": "/pdf/a4e11950337d63d757521b324d8c10575c7d19dd.pdf", "paperhash": "becker|recurrent_kalman_networks_factorized_inference_in_highdimensional_deep_feature_spaces", "_bibtex": "@misc{\nbecker2019recurrent,\ntitle={Recurrent Kalman Networks: Factorized Inference in High-Dimensional Deep Feature Spaces},\nauthor={Philipp Becker and Harit Pandya and Gregor H.W. Gebhardt and Cheng Zhao and Gerhard Neumann},\nyear={2019},\nurl={https://openreview.net/forum?id=rkx1m2C5YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1319/Official_Review", "cdate": 1542234255951, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkx1m2C5YQ", "replyto": "rkx1m2C5YQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1319/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335920916, "tmdate": 1552335920916, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1319/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkxkuiQNAX", "original": null, "number": 3, "cdate": 1542892391448, "ddate": null, "tcdate": 1542892391448, "tmdate": 1542892391448, "tddate": null, "forum": "rkx1m2C5YQ", "replyto": "Syx2wN5c37", "invitation": "ICLR.cc/2019/Conference/-/Paper1319/Official_Comment", "content": {"title": "Paper revised based on reviewers comments ", "comment": "Thank you for your time and valuable feedback that helped us to improve the paper considerably. We implemented the following improvements to the paper based on the comments of the reviewers:\n\n1.) We added a qualitative comparison to recent probabilistic generative model approaches that also use KF related techniques, see Table 1. This analysis shows that our approach is more general while allowing for more simple learning methods than other probabilistic SOTA methods.\n\n2.) We explicitly stated the update equations used by the RKN (see section 2.4) and clarified Figure 1 to better visualize the general structure.\n\n3.) We explicitly state the likelihood objective in section 2.5\n\n4.) We added a quantitative comparison to Embed to Control (Watter et. al 2015), Structured Inference Networks (Krishnan et al. 2017) and the Kalman Variational Autoencoder (Fraccaro et al. 2017) on the image imputation tasks for the pendulum and the quadlink. See Table 3 and section 3.3. Those experiments show that our approach outperforms the other models despite using a much easier optimization scheme and less information (smoothing vs. filtering).\n\n5.) Reedited abstract, introduction and related work to clarify the contribution of the paper in relation to recent SOTA.\n\nFor more specific answers to the comments, please see below:\n\n\u201cIn terms of model presentation\u2026 \u201c - We reworked the related work section and added a table comparing our approach to a variety of recent works on a qualitative level. This comparison shows while most generative modelling approaches have been used to predict future or missing images, they can not be used directly for state estimation. Our model is more general and can be used straightforwardly for image prediction as well as for state estimation. Moreover, our model does not require the use of approximate inference methods such as variational inference but can be learned directly in an end to end manner. We believe that this is one of the main reasons why the RKN outperforms all other generative models in the imputation experiment that is now added in Section 3. \n\n\u201cIn terms of model evaluation\u201d -  We added a quantitative comparison in Section 3 where we compare our approach to different generative models from the literature. The results show that while the RKN is using a much simpler model and/or learning method than related approaches, we significantly outperform them on the image imputation task in terms of the log-likelihood loss function. Note that we had to switch from a Gaussian to a Bernoulli distribution for creating images to make this comparison easier\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1319/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1319/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1319/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Kalman Networks: Factorized Inference in High-Dimensional Deep Feature Spaces", "abstract": "In order to integrate uncertainty estimates into deep time-series modelling, Kalman Filters (KFs) (Kalman et al., 1960) have been integrated with deep learning models. Yet, such approaches typically rely on approximate inference techniques such as variational inference which makes learning more complex and often less scalable due to approximation errors. We propose a new deep approach to Kalman filtering which can be learned directly in an end-to-end manner using backpropagation without additional approximations. Our approach uses a high-dimensional factorized latent state representation for which the Kalman updates simplify to scalar operations and thus avoids hard to backpropagate, computationally heavy and potentially unstable matrix inversions. Moreover, we use locally linear dynamic models to efficiently propagate the latent state to the next time\nstep. While our locally linear modelling and factorization assumptions are in general not true for the original low-dimensional state space of the system, the network finds a high-dimensional latent space where these assumptions hold to perform efficient inference. This state representation is learned jointly with the transition and noise models. The resulting network architecture, which we call Recurrent Kalman Network (RKN), can be used for any time-series data, similar to a LSTM (Hochreiter and Schmidhuber, 1997) but uses an explicit representation of uncertainty. As shown by our experiments, the RKN obtains much more accurate uncertainty estimates than an LSTM or Gated Recurrent Units (GRUs) (Cho et al., 2014) while also showing a slightly improved prediction performance and outperforms various recent generative models on an image imputation task.", "keywords": ["state estimation", "recurrent neural networks", "Kalman Filter", "deep learning"], "authorids": ["philippbecker93@googlemail.com", "hpandya@lincoln.ac.uk", "gebhardt@ias.tu-darmstadt.de", "irobotcheng@gmail.com", "gneumann@lincoln.ac.uk"], "authors": ["Philipp Becker", "Harit Pandya", "Gregor H.W. Gebhardt", "Cheng Zhao", "Gerhard Neumann"], "TL;DR": "Kalman Filter based recurrent model for efficient state estimation,  principled uncertainty handling and end to end learning of dynamic models in high dimensional spaces.", "pdf": "/pdf/a4e11950337d63d757521b324d8c10575c7d19dd.pdf", "paperhash": "becker|recurrent_kalman_networks_factorized_inference_in_highdimensional_deep_feature_spaces", "_bibtex": "@misc{\nbecker2019recurrent,\ntitle={Recurrent Kalman Networks: Factorized Inference in High-Dimensional Deep Feature Spaces},\nauthor={Philipp Becker and Harit Pandya and Gregor H.W. Gebhardt and Cheng Zhao and Gerhard Neumann},\nyear={2019},\nurl={https://openreview.net/forum?id=rkx1m2C5YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1319/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615799, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkx1m2C5YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1319/Authors", "ICLR.cc/2019/Conference/Paper1319/Reviewers", "ICLR.cc/2019/Conference/Paper1319/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1319/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1319/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1319/Authors|ICLR.cc/2019/Conference/Paper1319/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1319/Reviewers", "ICLR.cc/2019/Conference/Paper1319/Authors", "ICLR.cc/2019/Conference/Paper1319/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615799}}}, {"id": "S1ln-iQ4Rm", "original": null, "number": 2, "cdate": 1542892292325, "ddate": null, "tcdate": 1542892292325, "tmdate": 1542892292325, "tddate": null, "forum": "rkx1m2C5YQ", "replyto": "BklzonzohX", "invitation": "ICLR.cc/2019/Conference/-/Paper1319/Official_Comment", "content": {"title": "Paper revised based on reviewers comments", "comment": "Thank you for your time and valuable feedback that helped us to improve the paper considerably. We implemented the following improvements to the paper based on the comments of the reviewers:\n\n1.) We added a qualitative comparison to recent probabilistic generative model approaches that also use KF related techniques, see Table 1. This analysis shows that our approach is more general while allowing for more simple learning methods than other probabilistic SOTA methods.\n\n2.) We explicitly stated the update equations used by the RKN (see section 2.4) and clarified Figure 1 to better visualize the general structure.\n\n3.) We explicitly state the likelihood objective in section 2.5\n\n4.) We added a quantitative comparison to Embed to Control (Watter et. al 2015), Structured Inference Networks (Krishnan et al. 2017) and the Kalman Variational Autoencoder (Fraccaro et al. 2017) on the image imputation tasks for the pendulum and the quadlink. See Table 3 and section 3.3. Those experiments show that our approach outperforms the other models despite using a much easier optimization scheme and less information (smoothing vs. filtering).\n\n5.) Reedited abstract, introduction and related work to clarify the contribution of the paper in relation to recent SOTA.\n\nFor more specific answers to the comments, please see below:\n\n\u201cThe observation noise sigma^obs is a function of the observation itself. This seems strange\u201d - For high dimensional observations such as images, the amount of noise can often be inferred from the observation itself. For example, if certain relevant aspects of the scenario are occluded in an image this is useful information which can be extracted from the image. \nHence, making the variance depend on the images is quite common. It is done in the  approaches using variational autoencoder we compared to (Watter et al. 2015, Karl et al. 2016, Fraccaro et al. 2017) and was also a crucial aspect of the BackpropKF (Haarnoja et al. 2016)\n\nIn fact, making sigma^obs dependent on the observation is crucial for our approach since it allows the encoder to express the uncertainty of its current estimate and allows the model to ignore the current observation if it is not useful. This property is needed in all our experiments.  \n\n\u201cI believe that a more detailed...\u201d - We reworked the related work section, added a table comparing our approach to a variety of recent work including probabilistic generative models on a qualitative level. The main result of this qualitative comparison is that, while our model is more general than most of the current SOTA approaches as it can be used straightforwardly for state estimation as well as image prediction, it also offers the simplest training method (end-to-end training by backpropagation) without the need of approximate inference methods such as variational inference, that is required by most probabilistic generative models.  \nWe also added a quantitative comparison in Section 3 on the image imputation task that allows a direct comparison to the probabilistic generative models. While our approach offers a much simpler and more direct learning approach, it outperforms more complex models considerably on this task.  Note that we had to switch from a Gaussian to a Bernoulli distribution for generating images to make this comparison easier.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1319/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1319/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1319/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Kalman Networks: Factorized Inference in High-Dimensional Deep Feature Spaces", "abstract": "In order to integrate uncertainty estimates into deep time-series modelling, Kalman Filters (KFs) (Kalman et al., 1960) have been integrated with deep learning models. Yet, such approaches typically rely on approximate inference techniques such as variational inference which makes learning more complex and often less scalable due to approximation errors. We propose a new deep approach to Kalman filtering which can be learned directly in an end-to-end manner using backpropagation without additional approximations. Our approach uses a high-dimensional factorized latent state representation for which the Kalman updates simplify to scalar operations and thus avoids hard to backpropagate, computationally heavy and potentially unstable matrix inversions. Moreover, we use locally linear dynamic models to efficiently propagate the latent state to the next time\nstep. While our locally linear modelling and factorization assumptions are in general not true for the original low-dimensional state space of the system, the network finds a high-dimensional latent space where these assumptions hold to perform efficient inference. This state representation is learned jointly with the transition and noise models. The resulting network architecture, which we call Recurrent Kalman Network (RKN), can be used for any time-series data, similar to a LSTM (Hochreiter and Schmidhuber, 1997) but uses an explicit representation of uncertainty. As shown by our experiments, the RKN obtains much more accurate uncertainty estimates than an LSTM or Gated Recurrent Units (GRUs) (Cho et al., 2014) while also showing a slightly improved prediction performance and outperforms various recent generative models on an image imputation task.", "keywords": ["state estimation", "recurrent neural networks", "Kalman Filter", "deep learning"], "authorids": ["philippbecker93@googlemail.com", "hpandya@lincoln.ac.uk", "gebhardt@ias.tu-darmstadt.de", "irobotcheng@gmail.com", "gneumann@lincoln.ac.uk"], "authors": ["Philipp Becker", "Harit Pandya", "Gregor H.W. Gebhardt", "Cheng Zhao", "Gerhard Neumann"], "TL;DR": "Kalman Filter based recurrent model for efficient state estimation,  principled uncertainty handling and end to end learning of dynamic models in high dimensional spaces.", "pdf": "/pdf/a4e11950337d63d757521b324d8c10575c7d19dd.pdf", "paperhash": "becker|recurrent_kalman_networks_factorized_inference_in_highdimensional_deep_feature_spaces", "_bibtex": "@misc{\nbecker2019recurrent,\ntitle={Recurrent Kalman Networks: Factorized Inference in High-Dimensional Deep Feature Spaces},\nauthor={Philipp Becker and Harit Pandya and Gregor H.W. Gebhardt and Cheng Zhao and Gerhard Neumann},\nyear={2019},\nurl={https://openreview.net/forum?id=rkx1m2C5YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1319/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615799, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkx1m2C5YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1319/Authors", "ICLR.cc/2019/Conference/Paper1319/Reviewers", "ICLR.cc/2019/Conference/Paper1319/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1319/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1319/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1319/Authors|ICLR.cc/2019/Conference/Paper1319/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1319/Reviewers", "ICLR.cc/2019/Conference/Paper1319/Authors", "ICLR.cc/2019/Conference/Paper1319/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615799}}}, {"id": "Byec4cQ4AX", "original": null, "number": 1, "cdate": 1542892082422, "ddate": null, "tcdate": 1542892082422, "tmdate": 1542892170496, "tddate": null, "forum": "rkx1m2C5YQ", "replyto": "Syg1qfyRn7", "invitation": "ICLR.cc/2019/Conference/-/Paper1319/Official_Comment", "content": {"title": "Paper revised based on reviewers comments ", "comment": "Thank you for your time and valuable feedback that helped us to improve the paper considerably. We implemented the following improvements to the paper based on the comments of the reviewers:\n\n1.) We added a qualitative comparison to recent probabilistic generative model approaches that also use KF related techniques, see Table 1. This analysis shows that our approach is more general while allowing for more simple learning methods than other probabilistic SOTA methods.\n\n2.) We explicitly stated the update equations used by the RKN (see section 2.4) and clarified Figure 1 to better visualize the general structure.\n\n3.) We explicitly state the likelihood objective in section 2.5\n\n4.) We added a quantitative comparison to Embed to Control (Watter et. al 2015), Structured Inference Networks (Krishnan et al. 2017) and the Kalman Variational Autoencoder (Fraccaro et al. 2017) on the image imputation tasks for the pendulum and the quadlink. See Table 3 and section 3.3. Those experiments show that our approach outperforms the other models despite using a much easier optimization scheme and less information (smoothing vs. filtering).\n\n5.) Reedited abstract, introduction and related work to clarify the contribution of the paper in relation to recent SOTA.\n\nFor more specific answers to the comments, please see below:\n\n\u201cThe article does not provide any probability density ..\u201d - We stated the likelihood objective more explicitly in section 2.5.\n\n \u201c...and there are no connections to probabilistic generative models.\u201d - We reworked the related work section, added a table comparing our approach to a variety of recent work including probabilistic generative models on a qualitative level. The main result of this qualitative comparison is that, while our model is more general than most of the current SOTA approaches as it can be used straightforwardly for state estimation as well as image prediction, it also offers the simplest training method (end-to-end training by backpropagation) without the need of approximate inference methods such as variational inference, that is required by most probabilistic generative models.  \nWe also added a quantitative comparison in Section 3 on the image imputation task that allows a direct comparison to the probabilistic generative models. While our approach offers a much simpler and more direct learning approach, it outperforms more complex models considerably on this task.  Note that we had to switch from a Gaussian to a Bernoulli distribution for generating images to make this comparison easier.\n\n\u201cthe Preliminaries section uses formulas before defining them\u201d - We fixed that.\n\n\u201cAlso, explicitly writing...\u201d  - We reworked Figure 1 and moved some elaboration on the equations from the appendix to the main part (specifically, section 2.4). We hope that clarifies our presentation.  "}, "signatures": ["ICLR.cc/2019/Conference/Paper1319/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1319/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1319/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Kalman Networks: Factorized Inference in High-Dimensional Deep Feature Spaces", "abstract": "In order to integrate uncertainty estimates into deep time-series modelling, Kalman Filters (KFs) (Kalman et al., 1960) have been integrated with deep learning models. Yet, such approaches typically rely on approximate inference techniques such as variational inference which makes learning more complex and often less scalable due to approximation errors. We propose a new deep approach to Kalman filtering which can be learned directly in an end-to-end manner using backpropagation without additional approximations. Our approach uses a high-dimensional factorized latent state representation for which the Kalman updates simplify to scalar operations and thus avoids hard to backpropagate, computationally heavy and potentially unstable matrix inversions. Moreover, we use locally linear dynamic models to efficiently propagate the latent state to the next time\nstep. While our locally linear modelling and factorization assumptions are in general not true for the original low-dimensional state space of the system, the network finds a high-dimensional latent space where these assumptions hold to perform efficient inference. This state representation is learned jointly with the transition and noise models. The resulting network architecture, which we call Recurrent Kalman Network (RKN), can be used for any time-series data, similar to a LSTM (Hochreiter and Schmidhuber, 1997) but uses an explicit representation of uncertainty. As shown by our experiments, the RKN obtains much more accurate uncertainty estimates than an LSTM or Gated Recurrent Units (GRUs) (Cho et al., 2014) while also showing a slightly improved prediction performance and outperforms various recent generative models on an image imputation task.", "keywords": ["state estimation", "recurrent neural networks", "Kalman Filter", "deep learning"], "authorids": ["philippbecker93@googlemail.com", "hpandya@lincoln.ac.uk", "gebhardt@ias.tu-darmstadt.de", "irobotcheng@gmail.com", "gneumann@lincoln.ac.uk"], "authors": ["Philipp Becker", "Harit Pandya", "Gregor H.W. Gebhardt", "Cheng Zhao", "Gerhard Neumann"], "TL;DR": "Kalman Filter based recurrent model for efficient state estimation,  principled uncertainty handling and end to end learning of dynamic models in high dimensional spaces.", "pdf": "/pdf/a4e11950337d63d757521b324d8c10575c7d19dd.pdf", "paperhash": "becker|recurrent_kalman_networks_factorized_inference_in_highdimensional_deep_feature_spaces", "_bibtex": "@misc{\nbecker2019recurrent,\ntitle={Recurrent Kalman Networks: Factorized Inference in High-Dimensional Deep Feature Spaces},\nauthor={Philipp Becker and Harit Pandya and Gregor H.W. Gebhardt and Cheng Zhao and Gerhard Neumann},\nyear={2019},\nurl={https://openreview.net/forum?id=rkx1m2C5YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1319/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615799, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkx1m2C5YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1319/Authors", "ICLR.cc/2019/Conference/Paper1319/Reviewers", "ICLR.cc/2019/Conference/Paper1319/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1319/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1319/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1319/Authors|ICLR.cc/2019/Conference/Paper1319/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1319/Reviewers", "ICLR.cc/2019/Conference/Paper1319/Authors", "ICLR.cc/2019/Conference/Paper1319/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615799}}}, {"id": "Syg1qfyRn7", "original": null, "number": 3, "cdate": 1541431942732, "ddate": null, "tcdate": 1541431942732, "tmdate": 1541533237849, "tddate": null, "forum": "rkx1m2C5YQ", "replyto": "rkx1m2C5YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1319/Official_Review", "content": {"title": "Interesting model needs more context", "review": "This paper presents a particular architecture for a probabilistic recurrent neural network that is based on ideas from Kalman filtering. Whereas Kalman filters are used to infer the state of a known generative model (a linear-Gaussian dynamical system), here, the authors jointly learn a recursive filter without explicitly formulating a generative model of the data.\n\nThe paper deals with an important problem and the approach has many appealing characteristics: it learns a state representation and its associated transition dynamics, it learns nonlinear filter that can be used online and it learns encoders/decoders from high-dimensional observations to the state.\n\nThe article does not provide any probability density (even though learning happens by maximizing a likelihood) and there are no connections to probabilistic generative models. In my opinion this is a pity since this would shed more light into the characteristics of the proposed approach. \n\nI believe that the model could be presented more clearly. For example, the Preliminaries section uses formulas before defining them. Also, explicitly writing the high-level chain of computations from o_t and z_{t-1}^+ to o_t^+ and s_t^+ would be extremely useful. Even more than Fig. 1, in my opinion.\n\nAll in all, I have found this an interesting architecture for a RNN but would have appreciated more insight into its relationships with the large body of generative probabilistic state-space models and the methods to perform inference on them.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1319/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Kalman Networks: Factorized Inference in High-Dimensional Deep Feature Spaces", "abstract": "In order to integrate uncertainty estimates into deep time-series modelling, Kalman Filters (KFs) (Kalman et al., 1960) have been integrated with deep learning models. Yet, such approaches typically rely on approximate inference techniques such as variational inference which makes learning more complex and often less scalable due to approximation errors. We propose a new deep approach to Kalman filtering which can be learned directly in an end-to-end manner using backpropagation without additional approximations. Our approach uses a high-dimensional factorized latent state representation for which the Kalman updates simplify to scalar operations and thus avoids hard to backpropagate, computationally heavy and potentially unstable matrix inversions. Moreover, we use locally linear dynamic models to efficiently propagate the latent state to the next time\nstep. While our locally linear modelling and factorization assumptions are in general not true for the original low-dimensional state space of the system, the network finds a high-dimensional latent space where these assumptions hold to perform efficient inference. This state representation is learned jointly with the transition and noise models. The resulting network architecture, which we call Recurrent Kalman Network (RKN), can be used for any time-series data, similar to a LSTM (Hochreiter and Schmidhuber, 1997) but uses an explicit representation of uncertainty. As shown by our experiments, the RKN obtains much more accurate uncertainty estimates than an LSTM or Gated Recurrent Units (GRUs) (Cho et al., 2014) while also showing a slightly improved prediction performance and outperforms various recent generative models on an image imputation task.", "keywords": ["state estimation", "recurrent neural networks", "Kalman Filter", "deep learning"], "authorids": ["philippbecker93@googlemail.com", "hpandya@lincoln.ac.uk", "gebhardt@ias.tu-darmstadt.de", "irobotcheng@gmail.com", "gneumann@lincoln.ac.uk"], "authors": ["Philipp Becker", "Harit Pandya", "Gregor H.W. Gebhardt", "Cheng Zhao", "Gerhard Neumann"], "TL;DR": "Kalman Filter based recurrent model for efficient state estimation,  principled uncertainty handling and end to end learning of dynamic models in high dimensional spaces.", "pdf": "/pdf/a4e11950337d63d757521b324d8c10575c7d19dd.pdf", "paperhash": "becker|recurrent_kalman_networks_factorized_inference_in_highdimensional_deep_feature_spaces", "_bibtex": "@misc{\nbecker2019recurrent,\ntitle={Recurrent Kalman Networks: Factorized Inference in High-Dimensional Deep Feature Spaces},\nauthor={Philipp Becker and Harit Pandya and Gregor H.W. Gebhardt and Cheng Zhao and Gerhard Neumann},\nyear={2019},\nurl={https://openreview.net/forum?id=rkx1m2C5YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1319/Official_Review", "cdate": 1542234255951, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkx1m2C5YQ", "replyto": "rkx1m2C5YQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1319/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335920916, "tmdate": 1552335920916, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1319/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BklzonzohX", "original": null, "number": 2, "cdate": 1541250202259, "ddate": null, "tcdate": 1541250202259, "tmdate": 1541533237648, "tddate": null, "forum": "rkx1m2C5YQ", "replyto": "rkx1m2C5YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1319/Official_Review", "content": {"title": "Interesting idea, but insufficient comparison to existing work", "review": "PAPER SUMMARY\n-------------\nThis paper proposes a method for inferring the latent state and making predictions based on a sequence of observations. The idea is to map the observation to a latent space where the relation to the latent state is linear, and the dynamics of the latent state are locally linear. Therefore, in this latent space a Kalman filter can be applied to infer the current state and predict the next state, including uncertainty estimates. Finally, the predicted latent state is mapped to a prediction for the observation or some other variable of interest.\n\nThe experiments show that the proposed approach slightly outperform LSTM and a GRU based approaches.\n\n\nPOSITIVE ASPECTS\n----------------\n- The idea of applying a Kalman filter in a latent space is interesting.\n- The experimental results show that the proposed approach outperforms LSTM and a GRU based approaches.\n- The paper is well written.\n\nNEGATIVE ASPECTS\n----------------\n- The observation noise sigma^obs is a function of the observation itself. This seems strange, since typically the observation does not contain itself the information about how much it has been corrupted by noise. This choice should be discussed in more detail, especially what kind of assumptions this implies about the underlying process.\n- I believe that a more detailed comparison to existing approaches finding a latent space from a sequence of observations would be necessary, both on a technical as well as on an experimental level. For instance, a technical comparison to the approach from Watter et al. 2015 would be appropriate, since it is similar in the sense that the latent space is optimized to have locally linear dynamics. \nFurthermore, an experimental comparison to Watter et al. 2015 and [1] would be relevant.\n\n\n\n[1] Wahlstr\u00f6m et al. 2015 - From Pixels to Torques - Policy Learning with Deep Dynamical Models\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1319/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Kalman Networks: Factorized Inference in High-Dimensional Deep Feature Spaces", "abstract": "In order to integrate uncertainty estimates into deep time-series modelling, Kalman Filters (KFs) (Kalman et al., 1960) have been integrated with deep learning models. Yet, such approaches typically rely on approximate inference techniques such as variational inference which makes learning more complex and often less scalable due to approximation errors. We propose a new deep approach to Kalman filtering which can be learned directly in an end-to-end manner using backpropagation without additional approximations. Our approach uses a high-dimensional factorized latent state representation for which the Kalman updates simplify to scalar operations and thus avoids hard to backpropagate, computationally heavy and potentially unstable matrix inversions. Moreover, we use locally linear dynamic models to efficiently propagate the latent state to the next time\nstep. While our locally linear modelling and factorization assumptions are in general not true for the original low-dimensional state space of the system, the network finds a high-dimensional latent space where these assumptions hold to perform efficient inference. This state representation is learned jointly with the transition and noise models. The resulting network architecture, which we call Recurrent Kalman Network (RKN), can be used for any time-series data, similar to a LSTM (Hochreiter and Schmidhuber, 1997) but uses an explicit representation of uncertainty. As shown by our experiments, the RKN obtains much more accurate uncertainty estimates than an LSTM or Gated Recurrent Units (GRUs) (Cho et al., 2014) while also showing a slightly improved prediction performance and outperforms various recent generative models on an image imputation task.", "keywords": ["state estimation", "recurrent neural networks", "Kalman Filter", "deep learning"], "authorids": ["philippbecker93@googlemail.com", "hpandya@lincoln.ac.uk", "gebhardt@ias.tu-darmstadt.de", "irobotcheng@gmail.com", "gneumann@lincoln.ac.uk"], "authors": ["Philipp Becker", "Harit Pandya", "Gregor H.W. Gebhardt", "Cheng Zhao", "Gerhard Neumann"], "TL;DR": "Kalman Filter based recurrent model for efficient state estimation,  principled uncertainty handling and end to end learning of dynamic models in high dimensional spaces.", "pdf": "/pdf/a4e11950337d63d757521b324d8c10575c7d19dd.pdf", "paperhash": "becker|recurrent_kalman_networks_factorized_inference_in_highdimensional_deep_feature_spaces", "_bibtex": "@misc{\nbecker2019recurrent,\ntitle={Recurrent Kalman Networks: Factorized Inference in High-Dimensional Deep Feature Spaces},\nauthor={Philipp Becker and Harit Pandya and Gregor H.W. Gebhardt and Cheng Zhao and Gerhard Neumann},\nyear={2019},\nurl={https://openreview.net/forum?id=rkx1m2C5YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1319/Official_Review", "cdate": 1542234255951, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkx1m2C5YQ", "replyto": "rkx1m2C5YQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1319/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335920916, "tmdate": 1552335920916, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1319/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}