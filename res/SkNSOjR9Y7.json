{"notes": [{"id": "SkNSOjR9Y7", "original": "Byxymjt9KX", "number": 354, "cdate": 1538087789469, "ddate": null, "tcdate": 1538087789469, "tmdate": 1545355415075, "tddate": null, "forum": "SkNSOjR9Y7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Training Variational Auto Encoders with Discrete Latent Representations using Importance Sampling", "abstract": "The Variational Auto Encoder (VAE) is a popular generative \nlatent variable model that is often \napplied for representation learning.\nStandard VAEs assume continuous valued \nlatent variables and are trained by maximization\nof the evidence lower bound (ELBO). Conventional methods obtain a \ndifferentiable estimate of the ELBO with reparametrized sampling and\noptimize it with Stochastic Gradient Descend (SGD). However, this is not possible if \nwe want to train VAEs with discrete valued latent variables, \nsince reparametrized sampling is not possible. Till now, there\nexist no simple solutions to circumvent this problem.\nIn this paper, we propose an easy method to train VAEs \nwith binary or categorically valued latent representations. Therefore, we use a differentiable\nestimator for the ELBO which is based on importance sampling. In experiments, we verify the approach and\ntrain two different VAEs architectures with Bernoulli and \nCategorically distributed latent representations on two different benchmark\ndatasets.\t", "keywords": ["Variational Auto Encoder", "Importance Sampling", "Discrete latent representation"], "authorids": ["alexander.bartler@iss.uni-stuttgart.de", "felix.wiewel@iss.uni-stuttgart.de", "bin.yang@iss.uni-stuttgart.de", "lukas.mauch@iss.uni-stuttgart.de"], "authors": ["Alexander Bartler", "Felix Wiewel", "Bin Yang", "Lukas Mauch"], "TL;DR": "We propose an easy method to train Variational Auto Encoders (VAE) with discrete latent representations, using importance sampling", "pdf": "/pdf/318f07aca7c3f09bb812679921111173d672668c.pdf", "paperhash": "bartler|training_variational_auto_encoders_with_discrete_latent_representations_using_importance_sampling", "_bibtex": "@misc{\nbartler2019training,\ntitle={Training Variational Auto Encoders with Discrete Latent Representations using Importance Sampling},\nauthor={Alexander Bartler and Felix Wiewel and Bin Yang and Lukas Mauch},\nyear={2019},\nurl={https://openreview.net/forum?id=SkNSOjR9Y7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "H1gStaMeyE", "original": null, "number": 1, "cdate": 1543675261507, "ddate": null, "tcdate": 1543675261507, "tmdate": 1545354499698, "tddate": null, "forum": "SkNSOjR9Y7", "replyto": "SkNSOjR9Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper354/Meta_Review", "content": {"metareview": "The paper is addressing an important problem, but misses many related references (see Reviewer 2's comments for a long list of highly relevant papers). \n\nMore importantly, as Reviewer 3 pointed out (which the AC fully agrees): \n\n\"The gradient estimator the paper proposes is the REINFORCE estimator [Williams, ML 1992] re-derived through importance sampling.\"\n\n\"The equivalence would not be exact if the authors chose the importance distribution to be different than the variational approximation q(z|x), so there still may be room for novelty in their proposal, but in the current draft only q(z|x) is considered.\"\n\n", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Missing important references and the proposed algorithm is essentially the well-known REINFORCE estimator"}, "signatures": ["ICLR.cc/2019/Conference/Paper354/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper354/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Variational Auto Encoders with Discrete Latent Representations using Importance Sampling", "abstract": "The Variational Auto Encoder (VAE) is a popular generative \nlatent variable model that is often \napplied for representation learning.\nStandard VAEs assume continuous valued \nlatent variables and are trained by maximization\nof the evidence lower bound (ELBO). Conventional methods obtain a \ndifferentiable estimate of the ELBO with reparametrized sampling and\noptimize it with Stochastic Gradient Descend (SGD). However, this is not possible if \nwe want to train VAEs with discrete valued latent variables, \nsince reparametrized sampling is not possible. Till now, there\nexist no simple solutions to circumvent this problem.\nIn this paper, we propose an easy method to train VAEs \nwith binary or categorically valued latent representations. Therefore, we use a differentiable\nestimator for the ELBO which is based on importance sampling. In experiments, we verify the approach and\ntrain two different VAEs architectures with Bernoulli and \nCategorically distributed latent representations on two different benchmark\ndatasets.\t", "keywords": ["Variational Auto Encoder", "Importance Sampling", "Discrete latent representation"], "authorids": ["alexander.bartler@iss.uni-stuttgart.de", "felix.wiewel@iss.uni-stuttgart.de", "bin.yang@iss.uni-stuttgart.de", "lukas.mauch@iss.uni-stuttgart.de"], "authors": ["Alexander Bartler", "Felix Wiewel", "Bin Yang", "Lukas Mauch"], "TL;DR": "We propose an easy method to train Variational Auto Encoders (VAE) with discrete latent representations, using importance sampling", "pdf": "/pdf/318f07aca7c3f09bb812679921111173d672668c.pdf", "paperhash": "bartler|training_variational_auto_encoders_with_discrete_latent_representations_using_importance_sampling", "_bibtex": "@misc{\nbartler2019training,\ntitle={Training Variational Auto Encoders with Discrete Latent Representations using Importance Sampling},\nauthor={Alexander Bartler and Felix Wiewel and Bin Yang and Lukas Mauch},\nyear={2019},\nurl={https://openreview.net/forum?id=SkNSOjR9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper354/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353244966, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkNSOjR9Y7", "replyto": "SkNSOjR9Y7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper354/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper354/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper354/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353244966}}}, {"id": "BJxYsjJchQ", "original": null, "number": 3, "cdate": 1541172129294, "ddate": null, "tcdate": 1541172129294, "tmdate": 1541534064784, "tddate": null, "forum": "SkNSOjR9Y7", "replyto": "SkNSOjR9Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper354/Official_Review", "content": {"title": "A potentially nice idea that needs more thorough evaluation and discussion of related work", "review": "In computing the gradient of the ELBO, the main challenge lies in computing the gradient of the reconstruction loss with respect to the encoder parameters. VAEs traditionally rely on reparameterization in order to obtain a low-variance estimate, but there are a number of other gradient estimators that one can apply. The authors here proprose to use a trick that is known, but perhaps not widely known: If we introduce an importance sampling distribution, then we can use samples from this distribution to compute an importance-weighted estimate of the gradient. The idea is now that we can compute the gradient w.r.t. the encoder parameters as a simple importance-sampling estimate, which obviates then need for reparameterization, or likelihood-ratio estimators. The authors then apply this trick to train VAEs with discrete latent variables.\n\nWhile I think that the idea that the authors present in this paper is worth further exploration, the paper in its current form is not sufficiently mature to appear at ICLR. The two areas where this paper would benefit from improvement are\n\n1. Discussion of related work. \n\nWhile the authors seem to suggest that there has been no work on VAEs with discrete latent variables, there has in fact been quite a lot of work, including work on VAEs that contain both discrete and continuous variables (e.g. [8-10], but I'm almost certainly missing further references). There has also been a large body of work on continuous relaxations of discrete variables that are amenable to reparameterization (e.g. [6-7], and references therein). There has also been a line of work relating importance sampling to variational objectives (see [1-3] as key references). Finally, there is also related work on reweighted-wake-sleep style objectives (see [4]) which similarly don't require reparameterization. From what I can tell, none of these references are cited or discussed as related work. In order to place this work in context, I would rewrite 2 to discuss approaches to gradient estimation in this space, which then makes it much easier to explain how this approach differs. \n\n2. Empirical evaluation.\n\nThe authors only evaluate on MNIST and F-MNIST, and don't compare to any existing approaches. More than a couple of reconstructions, what I would like to see is an analysis of gradient variances, asymptotic ELBO estimates. I would also like to see a larger set of problems. Finally I would like to see a clear comparison to other methods based on, e.g., continuous relaxations. \n\n\nReferences\n\n[1] Y. Burda, R. Grosse, and R. Salakhutdinov, \u201cImportance Weighted Autoencoders,\u201d arXiv:1509.00519 [cs, stat], Sep. 2015.\n\n[2] T. Rainforth et al., \u201cTighter Variational Bounds are Not Necessarily Better,\u201d arXiv:1802.04537 [cs, stat], Feb. 2018.\n\n[3] G. Tucker, D. Lawson, S. Gu, and C. J. Maddison, \u201cDoubly Reparameterized Gradient Estimators for Monte Carlo Objectives,\u201d arXiv:1810.04152 [cs, stat], Oct. 2018.\n\n[4] T. A. Le, A. R. Kosiorek, N. Siddharth, Y. W. Teh, and F. Wood, \u201cRevisiting Reweighted Wake-Sleep,\u201d arXiv:1805.10469 [cs, stat], May 2018.\n\n[5] A. Mnih and D. J. Rezende, \u201cVariational inference for Monte Carlo objectives,\u201d arXiv:1602.06725 [cs, stat], Feb. 2016.\n\n[6] G. Tucker, A. Mnih, C. J. Maddison, J. Lawson, and J. Sohl-Dickstein, \u201cREBAR: Low-variance, unbiased gradient estimates for discrete latent variable models,\u201d in Advances in Neural Information Processing Systems, 2017, pp. 2624\u20132633.\n\n[7] W. Grathwohl, D. Choi, Y. Wu, G. Roeder, and D. Duvenaud, \u201cBackpropagation through the Void: Optimizing control variates for black-box gradient estimation,\u201d arXiv preprint arXiv:1711.00123, 2017.\n\n[8] J. T. Rolfe, \u201cDiscrete Variational Autoencoders,\u201d arXiv:1609.02200 [cs, stat], Sep. 2016.\n\n[9] E. Dupont, \u201cLearning Disentangled Joint Continuous and Discrete Representations,\u201d arXiv:1804.00104 [cs, stat], Mar. 2018.\n\n[10] B. Esmaeili et al., \u201cStructured Disentangled Representations,\u201d arXiv:1804.02086 [cs, stat], Apr. 2018.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper354/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Variational Auto Encoders with Discrete Latent Representations using Importance Sampling", "abstract": "The Variational Auto Encoder (VAE) is a popular generative \nlatent variable model that is often \napplied for representation learning.\nStandard VAEs assume continuous valued \nlatent variables and are trained by maximization\nof the evidence lower bound (ELBO). Conventional methods obtain a \ndifferentiable estimate of the ELBO with reparametrized sampling and\noptimize it with Stochastic Gradient Descend (SGD). However, this is not possible if \nwe want to train VAEs with discrete valued latent variables, \nsince reparametrized sampling is not possible. Till now, there\nexist no simple solutions to circumvent this problem.\nIn this paper, we propose an easy method to train VAEs \nwith binary or categorically valued latent representations. Therefore, we use a differentiable\nestimator for the ELBO which is based on importance sampling. In experiments, we verify the approach and\ntrain two different VAEs architectures with Bernoulli and \nCategorically distributed latent representations on two different benchmark\ndatasets.\t", "keywords": ["Variational Auto Encoder", "Importance Sampling", "Discrete latent representation"], "authorids": ["alexander.bartler@iss.uni-stuttgart.de", "felix.wiewel@iss.uni-stuttgart.de", "bin.yang@iss.uni-stuttgart.de", "lukas.mauch@iss.uni-stuttgart.de"], "authors": ["Alexander Bartler", "Felix Wiewel", "Bin Yang", "Lukas Mauch"], "TL;DR": "We propose an easy method to train Variational Auto Encoders (VAE) with discrete latent representations, using importance sampling", "pdf": "/pdf/318f07aca7c3f09bb812679921111173d672668c.pdf", "paperhash": "bartler|training_variational_auto_encoders_with_discrete_latent_representations_using_importance_sampling", "_bibtex": "@misc{\nbartler2019training,\ntitle={Training Variational Auto Encoders with Discrete Latent Representations using Importance Sampling},\nauthor={Alexander Bartler and Felix Wiewel and Bin Yang and Lukas Mauch},\nyear={2019},\nurl={https://openreview.net/forum?id=SkNSOjR9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper354/Official_Review", "cdate": 1542234480259, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkNSOjR9Y7", "replyto": "SkNSOjR9Y7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper354/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335705605, "tmdate": 1552335705605, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper354/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SkxYMKKu2Q", "original": null, "number": 2, "cdate": 1541081360852, "ddate": null, "tcdate": 1541081360852, "tmdate": 1541534064532, "tddate": null, "forum": "SkNSOjR9Y7", "replyto": "SkNSOjR9Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper354/Official_Review", "content": {"title": "It is a simple and trivial  extension of VAE with important sampling", "review": "This paper propose to use important sampling to optimize VAE with discrete latent variables. Basically, the methods proposed by this paper is rather simple and trivial. There are some discussions on why important sampling is not a good choice for VAE. Please refer: https://stats.stackexchange.com/q/255756\n\nMoreover, if you focus on VAE with discrete latent variable, you should compare at least with Gumbel-Softmax: https://arxiv.org/abs/1611.01144\n", "rating": "1: Trivial or wrong", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper354/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Variational Auto Encoders with Discrete Latent Representations using Importance Sampling", "abstract": "The Variational Auto Encoder (VAE) is a popular generative \nlatent variable model that is often \napplied for representation learning.\nStandard VAEs assume continuous valued \nlatent variables and are trained by maximization\nof the evidence lower bound (ELBO). Conventional methods obtain a \ndifferentiable estimate of the ELBO with reparametrized sampling and\noptimize it with Stochastic Gradient Descend (SGD). However, this is not possible if \nwe want to train VAEs with discrete valued latent variables, \nsince reparametrized sampling is not possible. Till now, there\nexist no simple solutions to circumvent this problem.\nIn this paper, we propose an easy method to train VAEs \nwith binary or categorically valued latent representations. Therefore, we use a differentiable\nestimator for the ELBO which is based on importance sampling. In experiments, we verify the approach and\ntrain two different VAEs architectures with Bernoulli and \nCategorically distributed latent representations on two different benchmark\ndatasets.\t", "keywords": ["Variational Auto Encoder", "Importance Sampling", "Discrete latent representation"], "authorids": ["alexander.bartler@iss.uni-stuttgart.de", "felix.wiewel@iss.uni-stuttgart.de", "bin.yang@iss.uni-stuttgart.de", "lukas.mauch@iss.uni-stuttgart.de"], "authors": ["Alexander Bartler", "Felix Wiewel", "Bin Yang", "Lukas Mauch"], "TL;DR": "We propose an easy method to train Variational Auto Encoders (VAE) with discrete latent representations, using importance sampling", "pdf": "/pdf/318f07aca7c3f09bb812679921111173d672668c.pdf", "paperhash": "bartler|training_variational_auto_encoders_with_discrete_latent_representations_using_importance_sampling", "_bibtex": "@misc{\nbartler2019training,\ntitle={Training Variational Auto Encoders with Discrete Latent Representations using Importance Sampling},\nauthor={Alexander Bartler and Felix Wiewel and Bin Yang and Lukas Mauch},\nyear={2019},\nurl={https://openreview.net/forum?id=SkNSOjR9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper354/Official_Review", "cdate": 1542234480259, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkNSOjR9Y7", "replyto": "SkNSOjR9Y7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper354/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335705605, "tmdate": 1552335705605, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper354/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ryejpQThsX", "original": null, "number": 1, "cdate": 1540309955441, "ddate": null, "tcdate": 1540309955441, "tmdate": 1541534064324, "tddate": null, "forum": "SkNSOjR9Y7", "replyto": "SkNSOjR9Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper354/Official_Review", "content": {"title": "Rederivation of REINFORCE Estimator", "review": "Summary:\nThis paper proposes training VAEs with discrete latent variables by importance sampling the expected log likelihood (ELL) term in the ELBO, which is the problematic term since it is not amenable to reparametrization gradients.  For the importance sampling distribution, they choose the variational distribution itself, making the ELL gradient E[(d q(z|x) / d \\theta) \\log p(x|z) / q(z|x)].  Experiments are reported for MNIST and Fashion-MNIST using Bernoulli and categorical latent variables.\n\nCritique:\nThe gradient estimator the paper proposes is the REINFORCE estimator [Williams, ML 1992] re-derived through importance sampling.  The equivalence can be seen just by expanding the derivative of log q in REINFORCE: E[log p(x|z) d log q(z|x)] = E[ (log p(x|z) / q(z|x)) d q(z|x) ], which is the exact estimator the paper proposes.  REINFORCE has been previously used for variational inference [Paisley et al., ICML 2012; Ranganath et al, AISTATS 2014] and deep generative models [Mnih & Gregor; ICML 2014] and recently extended for various control variates [Tucker et al., NIPS 2017].   The equivalence would not be exact if the authors chose the importance distribution to be different than the variational approximation q(z|x), so there still may be room for novelty in their proposal, but in the current draft only q(z|x) is considered.  \n\nConclusion: Due to lack of novelty, I recommend rejection.\n\n\nMiscellaneous points:\n\u201c...there exist no simple solutions to circumvent this problem.\u201d  The Gumbel-softmax trick is fairly simple (although an approximation) [Jang et al., ICLR 2017; Maddison et al., ICLR 2017]. \n\n\u201c...after training q(z|x) is a very good approximation to the true posterior p(z|x).\u201d  That\u2019s not necessarily true.  \n\nEquation #2 should be just equal to Equation #1.\n\n\u201cKingma & Welling (2013) proposed to minimize L(\\theta) using stochastic gradient descent on a training set...\u201d. First uses of stochastic gradient for VI were [Sato, NC 2001; Platt et al., NIPS 2008; Hoffman et al., JMLR 2013].  Kingma & Welling [ICLR  2014] were the first to introduce reparameterized stochastic gradients.\n\nBefore Equation #11, the reference to Equation #4 should be to Equation #5.\n\n\u201c...the weighting...depends only on \\theta_D and not on \\theta_E\u201d (p 4). D and E should be switched.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper354/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Variational Auto Encoders with Discrete Latent Representations using Importance Sampling", "abstract": "The Variational Auto Encoder (VAE) is a popular generative \nlatent variable model that is often \napplied for representation learning.\nStandard VAEs assume continuous valued \nlatent variables and are trained by maximization\nof the evidence lower bound (ELBO). Conventional methods obtain a \ndifferentiable estimate of the ELBO with reparametrized sampling and\noptimize it with Stochastic Gradient Descend (SGD). However, this is not possible if \nwe want to train VAEs with discrete valued latent variables, \nsince reparametrized sampling is not possible. Till now, there\nexist no simple solutions to circumvent this problem.\nIn this paper, we propose an easy method to train VAEs \nwith binary or categorically valued latent representations. Therefore, we use a differentiable\nestimator for the ELBO which is based on importance sampling. In experiments, we verify the approach and\ntrain two different VAEs architectures with Bernoulli and \nCategorically distributed latent representations on two different benchmark\ndatasets.\t", "keywords": ["Variational Auto Encoder", "Importance Sampling", "Discrete latent representation"], "authorids": ["alexander.bartler@iss.uni-stuttgart.de", "felix.wiewel@iss.uni-stuttgart.de", "bin.yang@iss.uni-stuttgart.de", "lukas.mauch@iss.uni-stuttgart.de"], "authors": ["Alexander Bartler", "Felix Wiewel", "Bin Yang", "Lukas Mauch"], "TL;DR": "We propose an easy method to train Variational Auto Encoders (VAE) with discrete latent representations, using importance sampling", "pdf": "/pdf/318f07aca7c3f09bb812679921111173d672668c.pdf", "paperhash": "bartler|training_variational_auto_encoders_with_discrete_latent_representations_using_importance_sampling", "_bibtex": "@misc{\nbartler2019training,\ntitle={Training Variational Auto Encoders with Discrete Latent Representations using Importance Sampling},\nauthor={Alexander Bartler and Felix Wiewel and Bin Yang and Lukas Mauch},\nyear={2019},\nurl={https://openreview.net/forum?id=SkNSOjR9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper354/Official_Review", "cdate": 1542234480259, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkNSOjR9Y7", "replyto": "SkNSOjR9Y7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper354/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335705605, "tmdate": 1552335705605, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper354/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}