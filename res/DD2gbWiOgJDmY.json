{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1363264440000, "tcdate": 1363264440000, "number": 2, "id": "8sJwMe5ZwE8uz", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "DD2gbWiOgJDmY", "replyto": "DD2gbWiOgJDmY", "signatures": ["Oriol Vinyals, Yangqing Jia, Trevor Darrell"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We agree with the reviewer regarding the existence of better dictionary learning methods, and note that many of these are also related to corresponding advanced Nystrom sampling methods, such as [Zhang et al. Improved Nystrom low-rank approximation and error analysis. ICML 08]. These methods could improve performance in absolute terms, but that is an orthogonal issue to our main results.  Nonetheless, we think this is a valuable observation, and will include a discussion of these points in the final version of this paper.\r\n\r\nThe relationship between a kernel error bound and classification accuracy is discussed in more detail in [Cortes et al. On the Impact of Kernel Approximation on Learning Accuracy. AISTATS 2010]. The main result is that the bounds are proportional, verifying our empirical claims. We will add this reference to the paper.\r\n\r\nRegarding the comment on fitting the shape of the curve, we are only using the first two points to fit the 'constants' given in the bound, so the fact that it extrapolates well in many tasks gives us confidence that the bound is accurate."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Why Size Matters: Feature Coding as Nystrom Sampling", "decision": "conferenceOral-iclr2013-workshop", "abstract": "Recently, the computer vision and machine learning community has been in favor of feature extraction pipelines that rely on a coding step followed by a linear classifier, due to their overall simplicity, well understood properties of linear classifiers, and their computational efficiency. In this paper we propose a novel view of this pipeline based on kernel methods and Nystrom sampling. In particular, we focus on the coding of a data point with a local representation based on a dictionary with fewer elements than the number of data points, and view it as an approximation to the actual function that would compute pair-wise similarity to all data points (often too many to compute in practice), followed by a Nystrom sampling step to select a subset of all data points. Furthermore, since bounds are known on the approximation power of Nystrom sampling as a function of how many samples (i.e. dictionary size) we consider, we can derive bounds on the approximation of the exact (but expensive to compute) kernel matrix, and use it as a proxy to predict accuracy as a function of the dictionary size, which has been observed to increase but also to saturate as we increase its size. This model may help explaining the positive effect of the codebook size and justifying the need to stack more layers (often referred to as deep learning), as flat models empirically saturate as we add more complexity.", "pdf": "https://arxiv.org/abs/1301.5348", "paperhash": "vinyals|why_size_matters_feature_coding_as_nystrom_sampling", "keywords": [], "conflicts": [], "authors": ["Oriol Vinyals", "Yangqing Jia", "Trevor Darrell"], "authorids": ["oriol18@gmail.com", "jiayq84@gmail.com", "trevordarrell@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362202140000, "tcdate": 1362202140000, "number": 3, "id": "EW9REhyYQcESw", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "DD2gbWiOgJDmY", "replyto": "DD2gbWiOgJDmY", "signatures": ["anonymous reviewer 1024"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Why Size Matters: Feature Coding as Nystrom Sampling", "review": "The authors provide an analysis of the accuracy bounds of feature coding + linear classifier pipelines. They predict an approximate accuracy bound given the dictionary size and correctly estimate the phenomenon observed in the literature where accuracy increases with dictionary size but also saturates.\r\n\r\nPros:\r\n- Demonstrates limitations of shallow models and analytically justifies the use of deeper models."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Why Size Matters: Feature Coding as Nystrom Sampling", "decision": "conferenceOral-iclr2013-workshop", "abstract": "Recently, the computer vision and machine learning community has been in favor of feature extraction pipelines that rely on a coding step followed by a linear classifier, due to their overall simplicity, well understood properties of linear classifiers, and their computational efficiency. In this paper we propose a novel view of this pipeline based on kernel methods and Nystrom sampling. In particular, we focus on the coding of a data point with a local representation based on a dictionary with fewer elements than the number of data points, and view it as an approximation to the actual function that would compute pair-wise similarity to all data points (often too many to compute in practice), followed by a Nystrom sampling step to select a subset of all data points. Furthermore, since bounds are known on the approximation power of Nystrom sampling as a function of how many samples (i.e. dictionary size) we consider, we can derive bounds on the approximation of the exact (but expensive to compute) kernel matrix, and use it as a proxy to predict accuracy as a function of the dictionary size, which has been observed to increase but also to saturate as we increase its size. This model may help explaining the positive effect of the codebook size and justifying the need to stack more layers (often referred to as deep learning), as flat models empirically saturate as we add more complexity.", "pdf": "https://arxiv.org/abs/1301.5348", "paperhash": "vinyals|why_size_matters_feature_coding_as_nystrom_sampling", "keywords": [], "conflicts": [], "authors": ["Oriol Vinyals", "Yangqing Jia", "Trevor Darrell"], "authorids": ["oriol18@gmail.com", "jiayq84@gmail.com", "trevordarrell@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362196320000, "tcdate": 1362196320000, "number": 1, "id": "oxSZoe2BGRoB6", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "DD2gbWiOgJDmY", "replyto": "DD2gbWiOgJDmY", "signatures": ["anonymous reviewer 998c"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Why Size Matters: Feature Coding as Nystrom Sampling", "review": "This paper presents a theoretical analysis and empirical validation of a novel view of feature extraction systems based on the idea of Nystrom sampling for kernel methods.  The main idea is to analyze the kernel matrix for a feature space defined by an off-the-shelf feature extraction system.  In such a system, a bound is identified for the error in representing the 'full' dictionary composed of all data points by a Nystrom approximated version (i.e., represented by subsampling the data points randomly).  The bound is then extended to show that the approximate kernel matrix obtained using the Nystrom-sampled dictionary is close to the true kernel matrix, and it is argued that the quality of the approximation is a reasonable proxy for the classification error we can expect after training.  It is shown that this approximation model qualitatively predicts the monotonic rise in accuracy of feature extraction with larger dictionaries and saturation of performance in experiments.\r\n\r\nThis is a short paper, but the main idea and analysis are interesting.  It is nice to have some theoretical machinery to talk about the empirical finding of rising, saturating performance.  In some places I think more detail could have been useful.\r\n\r\nOne undiscussed point is the fact that many dictionary-learning methods do more than populate the dictionary with exemplars so it's possible that a 'learning' method might do substantially better (perhaps reaching top performance much sooner).  This doesn't appear to be terribly important in low-dimensional spaces where sampling strategies work about as well as learning, but could be critical for high-dimensional spaces (where sampling might asymptote much more slowly than learning).  It seems worth explaining the limitations of this analysis and how it relates to learning.  \r\n\r\nA few other questions / comments:\r\n\r\nThe calibration of constants for the bound in the experiments was not clear to me.  How is the mapping from the bound (Eq. 2) to classification accuracy actually done?\r\n\r\nThe empirical validation of the lower bound relies on a calibration procedure that, as I understand it, effectively ends up rescaling a fixed-shape curve to fit observed trend in accuracy on the real problem.  As a result, it seems like we could come up with a 'nonsense' bound that happened to have such a shape and then make a similar empirical claim.  Is there a way to extend the analysis to rule this out?  Or perhaps I misunderstand the origin of the shape of this curve.\r\n\r\nPros:  \r\n(1)  A novel view of feature extraction that appears to yield a reasonable explanation for the widely observed performance curves of these methods is presented.  I don't know how much profit this view might yield, but perhaps that will be made clear by the 'overshooting' method foreshadowed in the conclusion.\r\n(2) A pleasingly short read adequate to cover the main idea.  (Though a few more details might be nice.)\r\n\r\nCons:\r\n(1)  How this bound relates to the more common case of 'trained' dictionaries is unclear.\r\n(2)  The empirical validation shows the basic relationship qualitatively, but it is possible that this does not adequately validate the theoretical ideas and their connection to the observed phenomenon."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Why Size Matters: Feature Coding as Nystrom Sampling", "decision": "conferenceOral-iclr2013-workshop", "abstract": "Recently, the computer vision and machine learning community has been in favor of feature extraction pipelines that rely on a coding step followed by a linear classifier, due to their overall simplicity, well understood properties of linear classifiers, and their computational efficiency. In this paper we propose a novel view of this pipeline based on kernel methods and Nystrom sampling. In particular, we focus on the coding of a data point with a local representation based on a dictionary with fewer elements than the number of data points, and view it as an approximation to the actual function that would compute pair-wise similarity to all data points (often too many to compute in practice), followed by a Nystrom sampling step to select a subset of all data points. Furthermore, since bounds are known on the approximation power of Nystrom sampling as a function of how many samples (i.e. dictionary size) we consider, we can derive bounds on the approximation of the exact (but expensive to compute) kernel matrix, and use it as a proxy to predict accuracy as a function of the dictionary size, which has been observed to increase but also to saturate as we increase its size. This model may help explaining the positive effect of the codebook size and justifying the need to stack more layers (often referred to as deep learning), as flat models empirically saturate as we add more complexity.", "pdf": "https://arxiv.org/abs/1301.5348", "paperhash": "vinyals|why_size_matters_feature_coding_as_nystrom_sampling", "keywords": [], "conflicts": [], "authors": ["Oriol Vinyals", "Yangqing Jia", "Trevor Darrell"], "authorids": ["oriol18@gmail.com", "jiayq84@gmail.com", "trevordarrell@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1359008100000, "tcdate": 1359008100000, "number": 29, "id": "DD2gbWiOgJDmY", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "DD2gbWiOgJDmY", "signatures": ["oriol18@gmail.com"], "readers": ["everyone"], "content": {"title": "Why Size Matters: Feature Coding as Nystrom Sampling", "decision": "conferenceOral-iclr2013-workshop", "abstract": "Recently, the computer vision and machine learning community has been in favor of feature extraction pipelines that rely on a coding step followed by a linear classifier, due to their overall simplicity, well understood properties of linear classifiers, and their computational efficiency. In this paper we propose a novel view of this pipeline based on kernel methods and Nystrom sampling. In particular, we focus on the coding of a data point with a local representation based on a dictionary with fewer elements than the number of data points, and view it as an approximation to the actual function that would compute pair-wise similarity to all data points (often too many to compute in practice), followed by a Nystrom sampling step to select a subset of all data points. Furthermore, since bounds are known on the approximation power of Nystrom sampling as a function of how many samples (i.e. dictionary size) we consider, we can derive bounds on the approximation of the exact (but expensive to compute) kernel matrix, and use it as a proxy to predict accuracy as a function of the dictionary size, which has been observed to increase but also to saturate as we increase its size. This model may help explaining the positive effect of the codebook size and justifying the need to stack more layers (often referred to as deep learning), as flat models empirically saturate as we add more complexity.", "pdf": "https://arxiv.org/abs/1301.5348", "paperhash": "vinyals|why_size_matters_feature_coding_as_nystrom_sampling", "keywords": [], "conflicts": [], "authors": ["Oriol Vinyals", "Yangqing Jia", "Trevor Darrell"], "authorids": ["oriol18@gmail.com", "jiayq84@gmail.com", "trevordarrell@gmail.com"]}, "writers": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 4}