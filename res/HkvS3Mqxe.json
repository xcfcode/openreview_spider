{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396417707, "tcdate": 1486396417707, "number": 1, "id": "rycZhzIug", "invitation": "ICLR.cc/2017/conference/-/paper183/acceptance", "forum": "HkvS3Mqxe", "replyto": "HkvS3Mqxe", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "Unfortunately this paper is not competitive enough for ICLR. The paper is focusing on efficiency where for the results to be credible it is of utmost importance to present experiments on large scale data and state-of-the-art models.\n I am afraid the reviewers were not able to take into account the latest drafts of the paper that were submitted in January, but those submissions came very late."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Coarse Pruning of Convolutional Neural Networks with Random Masks", "abstract": "The learning capability of a neural network improves with increasing depth at\nhigher computational costs. Wider layers with dense kernel connectivity patterns\nfurther increase this cost and may hinder real-time inference. We propose feature\nmap and kernel level pruning for reducing the computational complexity of\na deep convolutional neural network. Pruning feature maps reduces the width\nof a layer and hence does not need any sparse representation. Further, kernel\npruning changes the dense connectivity pattern into a sparse one. Due to coarse\nnature, these pruning granularities can be exploited by GPUs and VLSI based\nimplementations. We propose a simple strategy to choose the least adversarial\npruning masks. The proposed approach is generic and can select good pruning\nmasks for feature map, kernel and intra-kernel pruning. The pruning masks are\ngenerated randomly, and the best performing one is selected using the evaluation\nset. The sufficient number of random pruning masks to try depends on the pruning\nratio, and is around 100 when 40% complexity reduction is needed. The pruned\nnetwork is retrained to compensate for the loss in accuracy. We have extensively\nevaluated the proposed approach with the CIFAR-10, SVHN and MNIST datasets.\nExperiments with the CIFAR-10 dataset show that more than 85% sparsity can be\ninduced in the convolution layers with less than 1% increase in the misclassification\nrate of the baseline network.", "pdf": "/pdf/0ce16b21425061456ee6180b8db03d13fdb2dca9.pdf", "TL;DR": "This work has proposed a new pruning strategy for CNN. Further, feature map and kernel pruning granularities are proposed for good pruning ratios and simple sparse representation.", "paperhash": "anwar|coarse_pruning_of_convolutional_neural_networks_with_random_masks", "keywords": [], "conflicts": ["snu.ac.kr", "dsp.snu.ac.kr"], "authors": ["Sajid Anwar", "Wonyong Sung"], "authorids": ["sajid@dsp.snu.ac.kr", "wysung@snu.ac.kr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396418237, "id": "ICLR.cc/2017/conference/-/paper183/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HkvS3Mqxe", "replyto": "HkvS3Mqxe", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396418237}}}, {"tddate": null, "tmdate": 1485446297794, "tcdate": 1485446297794, "number": 3, "id": "S1Mo35vvl", "invitation": "ICLR.cc/2017/conference/-/paper183/official/comment", "forum": "HkvS3Mqxe", "replyto": "ByI9N1Xvx", "signatures": ["ICLR.cc/2017/conference/paper183/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper183/AnonReviewer1"], "content": {"title": "Reaction to author responses", "comment": "I have read through the author responses and I will maintain my original rating (5). I appreciate the efforts to clarify the draft, but ultimately do not find the experiments on CIFAR-100, CIFAR-10, SVHN, and MNIST with outdated VGG networks very compelling."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Coarse Pruning of Convolutional Neural Networks with Random Masks", "abstract": "The learning capability of a neural network improves with increasing depth at\nhigher computational costs. Wider layers with dense kernel connectivity patterns\nfurther increase this cost and may hinder real-time inference. We propose feature\nmap and kernel level pruning for reducing the computational complexity of\na deep convolutional neural network. Pruning feature maps reduces the width\nof a layer and hence does not need any sparse representation. Further, kernel\npruning changes the dense connectivity pattern into a sparse one. Due to coarse\nnature, these pruning granularities can be exploited by GPUs and VLSI based\nimplementations. We propose a simple strategy to choose the least adversarial\npruning masks. The proposed approach is generic and can select good pruning\nmasks for feature map, kernel and intra-kernel pruning. The pruning masks are\ngenerated randomly, and the best performing one is selected using the evaluation\nset. The sufficient number of random pruning masks to try depends on the pruning\nratio, and is around 100 when 40% complexity reduction is needed. The pruned\nnetwork is retrained to compensate for the loss in accuracy. We have extensively\nevaluated the proposed approach with the CIFAR-10, SVHN and MNIST datasets.\nExperiments with the CIFAR-10 dataset show that more than 85% sparsity can be\ninduced in the convolution layers with less than 1% increase in the misclassification\nrate of the baseline network.", "pdf": "/pdf/0ce16b21425061456ee6180b8db03d13fdb2dca9.pdf", "TL;DR": "This work has proposed a new pruning strategy for CNN. Further, feature map and kernel pruning granularities are proposed for good pruning ratios and simple sparse representation.", "paperhash": "anwar|coarse_pruning_of_convolutional_neural_networks_with_random_masks", "keywords": [], "conflicts": ["snu.ac.kr", "dsp.snu.ac.kr"], "authors": ["Sajid Anwar", "Wonyong Sung"], "authorids": ["sajid@dsp.snu.ac.kr", "wysung@snu.ac.kr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287696463, "id": "ICLR.cc/2017/conference/-/paper183/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "HkvS3Mqxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper183/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper183/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper183/reviewers", "ICLR.cc/2017/conference/paper183/areachairs"], "cdate": 1485287696463}}}, {"tddate": null, "tmdate": 1485400882020, "tcdate": 1485137038053, "number": 2, "id": "ByI9N1Xvx", "invitation": "ICLR.cc/2017/conference/-/paper183/official/comment", "forum": "HkvS3Mqxe", "replyto": "HkvS3Mqxe", "signatures": ["ICLR.cc/2017/conference/paper183/areachair1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper183/areachair1"], "content": {"title": "Reactions to author responses", "comment": "Dear reviewers,\n\ncan you please take a look at the responses by the authors and add a comment indicating that you have taken them into consideration?\n\nThanks!\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Coarse Pruning of Convolutional Neural Networks with Random Masks", "abstract": "The learning capability of a neural network improves with increasing depth at\nhigher computational costs. Wider layers with dense kernel connectivity patterns\nfurther increase this cost and may hinder real-time inference. We propose feature\nmap and kernel level pruning for reducing the computational complexity of\na deep convolutional neural network. Pruning feature maps reduces the width\nof a layer and hence does not need any sparse representation. Further, kernel\npruning changes the dense connectivity pattern into a sparse one. Due to coarse\nnature, these pruning granularities can be exploited by GPUs and VLSI based\nimplementations. We propose a simple strategy to choose the least adversarial\npruning masks. The proposed approach is generic and can select good pruning\nmasks for feature map, kernel and intra-kernel pruning. The pruning masks are\ngenerated randomly, and the best performing one is selected using the evaluation\nset. The sufficient number of random pruning masks to try depends on the pruning\nratio, and is around 100 when 40% complexity reduction is needed. The pruned\nnetwork is retrained to compensate for the loss in accuracy. We have extensively\nevaluated the proposed approach with the CIFAR-10, SVHN and MNIST datasets.\nExperiments with the CIFAR-10 dataset show that more than 85% sparsity can be\ninduced in the convolution layers with less than 1% increase in the misclassification\nrate of the baseline network.", "pdf": "/pdf/0ce16b21425061456ee6180b8db03d13fdb2dca9.pdf", "TL;DR": "This work has proposed a new pruning strategy for CNN. Further, feature map and kernel pruning granularities are proposed for good pruning ratios and simple sparse representation.", "paperhash": "anwar|coarse_pruning_of_convolutional_neural_networks_with_random_masks", "keywords": [], "conflicts": ["snu.ac.kr", "dsp.snu.ac.kr"], "authors": ["Sajid Anwar", "Wonyong Sung"], "authorids": ["sajid@dsp.snu.ac.kr", "wysung@snu.ac.kr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287696463, "id": "ICLR.cc/2017/conference/-/paper183/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "HkvS3Mqxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper183/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper183/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper183/reviewers", "ICLR.cc/2017/conference/paper183/areachairs"], "cdate": 1485287696463}}}, {"tddate": null, "tmdate": 1485179206502, "tcdate": 1481767008025, "number": 1, "id": "ryuDdOkEx", "invitation": "ICLR.cc/2017/conference/-/paper183/official/review", "forum": "HkvS3Mqxe", "replyto": "HkvS3Mqxe", "signatures": ["ICLR.cc/2017/conference/paper183/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper183/AnonReviewer4"], "content": {"title": "N random trails to get best pruning?", "rating": "6: Marginally above acceptance threshold", "review": "Summary: There are many different pruning techniques to reduce memory footprint of CNN models, and those techniques have different granularities (layer, maps, kernel or intra kernel), pruning ratio and sparsity of representation. The work proposes a method to choose the best pruning masks out to many trials. Tested on CIFAR-10, SVHN and MNIST.\n\nPros:\nProposes a method to choose pruning mask out of N trials. \nAnalysis on different pruning methods.\n\nCons & Questions:\n\u201cThe proposed strategy selects the best pruned network through N random pruning trials. This approach enables one to select pruning mask in one shot and is simpler than the multi-step technique.\u201d How can one get the best pruning mask in one shot if you ran N random pruning trials? (answered)\nMissing tests of the approach with bigger CNN: like AlexNet, VGG, GoogLeNet or ResNet. (extended to VGG ok)\nSince reducing model size for embedded systems is the final goal, then showing how much memory space in MB is saved with the proposed technique compared with other approaches like Han et al. (2015) would be good.\n\nMisc:\nTypo in figure 6 a) caption: \u201cFeatuer\u201d (corrected)\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Coarse Pruning of Convolutional Neural Networks with Random Masks", "abstract": "The learning capability of a neural network improves with increasing depth at\nhigher computational costs. Wider layers with dense kernel connectivity patterns\nfurther increase this cost and may hinder real-time inference. We propose feature\nmap and kernel level pruning for reducing the computational complexity of\na deep convolutional neural network. Pruning feature maps reduces the width\nof a layer and hence does not need any sparse representation. Further, kernel\npruning changes the dense connectivity pattern into a sparse one. Due to coarse\nnature, these pruning granularities can be exploited by GPUs and VLSI based\nimplementations. We propose a simple strategy to choose the least adversarial\npruning masks. The proposed approach is generic and can select good pruning\nmasks for feature map, kernel and intra-kernel pruning. The pruning masks are\ngenerated randomly, and the best performing one is selected using the evaluation\nset. The sufficient number of random pruning masks to try depends on the pruning\nratio, and is around 100 when 40% complexity reduction is needed. The pruned\nnetwork is retrained to compensate for the loss in accuracy. We have extensively\nevaluated the proposed approach with the CIFAR-10, SVHN and MNIST datasets.\nExperiments with the CIFAR-10 dataset show that more than 85% sparsity can be\ninduced in the convolution layers with less than 1% increase in the misclassification\nrate of the baseline network.", "pdf": "/pdf/0ce16b21425061456ee6180b8db03d13fdb2dca9.pdf", "TL;DR": "This work has proposed a new pruning strategy for CNN. Further, feature map and kernel pruning granularities are proposed for good pruning ratios and simple sparse representation.", "paperhash": "anwar|coarse_pruning_of_convolutional_neural_networks_with_random_masks", "keywords": [], "conflicts": ["snu.ac.kr", "dsp.snu.ac.kr"], "authors": ["Sajid Anwar", "Wonyong Sung"], "authorids": ["sajid@dsp.snu.ac.kr", "wysung@snu.ac.kr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512672431, "id": "ICLR.cc/2017/conference/-/paper183/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper183/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper183/AnonReviewer4", "ICLR.cc/2017/conference/paper183/AnonReviewer3", "ICLR.cc/2017/conference/paper183/AnonReviewer1"], "reply": {"forum": "HkvS3Mqxe", "replyto": "HkvS3Mqxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper183/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper183/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512672431}}}, {"tddate": null, "tmdate": 1485018314980, "tcdate": 1481843118864, "number": 2, "id": "BJwnboxEl", "invitation": "ICLR.cc/2017/conference/-/paper183/official/review", "forum": "HkvS3Mqxe", "replyto": "HkvS3Mqxe", "signatures": ["ICLR.cc/2017/conference/paper183/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper183/AnonReviewer3"], "content": {"title": "Useful topic, but not very solid techinique", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposes two pruning methods to reduce the computation of deep neural network. In particular, whole feature maps and the kernel connections can be removed with not much decrease of classification accuracy. \nHowever, this paper also has the following problems. \n1)\tThe method is somehow trivial, since the pruning masks are mainly chosen by simple random sampling. The novelty and scalability are both limited. \n2)\tExperiment results are mainly focused on the classification rate and the ideal complexity. As a paper on improving computation efficiency, it should include results on practical time consumption. It is very common that reducing numbers of operations may not lead to reduced computational time on a highly parallel platform (e.g., GPU). \n3)\tIt is more important to improve the computational efficiency on large-scale models (e.g., ImageNet classification network) than on small models (e.g., MNIST, CIFAR network). However, results on large-scale network is missing.\n4)\t(*Logical validity of the proposed method*) For feature map pruning, what if just to train reduced-size network is trained from scratch without transfer any knowledge from the pretrained large network? Is it possible to get the same accuracy? If so, it will simply indicate the hyper-parameter is not optimal for the original network. Experimental results are necessary to clarify the necessity of feature map pruning. \nNote that I agree with that a smaller network may be more generalizable than a larger network. \n\n----------------------------------------------\n\nComments to the authors's response:\n\nThanks for replying to my comments. \n\n1) I still believe that the proposed methods are trivial.\n2) It is nice to show GPU implementation. Compared to existing toolboxes (e.g., Torch, Caffe, TensorFlow), is the implementation of convolution efficient enough?\n3) Experiments on Cifar-100 are helpful (better than cifar-10), but it is not really large-scale, where speed-up is not so critical. ImageNet and Places datasets are examples of large-scale datasets.\n4) The author did not reply to the question wrt the validity of the proposed methods. This question is critical.   \n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Coarse Pruning of Convolutional Neural Networks with Random Masks", "abstract": "The learning capability of a neural network improves with increasing depth at\nhigher computational costs. Wider layers with dense kernel connectivity patterns\nfurther increase this cost and may hinder real-time inference. We propose feature\nmap and kernel level pruning for reducing the computational complexity of\na deep convolutional neural network. Pruning feature maps reduces the width\nof a layer and hence does not need any sparse representation. Further, kernel\npruning changes the dense connectivity pattern into a sparse one. Due to coarse\nnature, these pruning granularities can be exploited by GPUs and VLSI based\nimplementations. We propose a simple strategy to choose the least adversarial\npruning masks. The proposed approach is generic and can select good pruning\nmasks for feature map, kernel and intra-kernel pruning. The pruning masks are\ngenerated randomly, and the best performing one is selected using the evaluation\nset. The sufficient number of random pruning masks to try depends on the pruning\nratio, and is around 100 when 40% complexity reduction is needed. The pruned\nnetwork is retrained to compensate for the loss in accuracy. We have extensively\nevaluated the proposed approach with the CIFAR-10, SVHN and MNIST datasets.\nExperiments with the CIFAR-10 dataset show that more than 85% sparsity can be\ninduced in the convolution layers with less than 1% increase in the misclassification\nrate of the baseline network.", "pdf": "/pdf/0ce16b21425061456ee6180b8db03d13fdb2dca9.pdf", "TL;DR": "This work has proposed a new pruning strategy for CNN. Further, feature map and kernel pruning granularities are proposed for good pruning ratios and simple sparse representation.", "paperhash": "anwar|coarse_pruning_of_convolutional_neural_networks_with_random_masks", "keywords": [], "conflicts": ["snu.ac.kr", "dsp.snu.ac.kr"], "authors": ["Sajid Anwar", "Wonyong Sung"], "authorids": ["sajid@dsp.snu.ac.kr", "wysung@snu.ac.kr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512672431, "id": "ICLR.cc/2017/conference/-/paper183/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper183/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper183/AnonReviewer4", "ICLR.cc/2017/conference/paper183/AnonReviewer3", "ICLR.cc/2017/conference/paper183/AnonReviewer1"], "reply": {"forum": "HkvS3Mqxe", "replyto": "HkvS3Mqxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper183/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper183/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512672431}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484753204230, "tcdate": 1478270014750, "number": 183, "id": "HkvS3Mqxe", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HkvS3Mqxe", "signatures": ["~Sajid_Anwar1"], "readers": ["everyone"], "content": {"title": "Coarse Pruning of Convolutional Neural Networks with Random Masks", "abstract": "The learning capability of a neural network improves with increasing depth at\nhigher computational costs. Wider layers with dense kernel connectivity patterns\nfurther increase this cost and may hinder real-time inference. We propose feature\nmap and kernel level pruning for reducing the computational complexity of\na deep convolutional neural network. Pruning feature maps reduces the width\nof a layer and hence does not need any sparse representation. Further, kernel\npruning changes the dense connectivity pattern into a sparse one. Due to coarse\nnature, these pruning granularities can be exploited by GPUs and VLSI based\nimplementations. We propose a simple strategy to choose the least adversarial\npruning masks. The proposed approach is generic and can select good pruning\nmasks for feature map, kernel and intra-kernel pruning. The pruning masks are\ngenerated randomly, and the best performing one is selected using the evaluation\nset. The sufficient number of random pruning masks to try depends on the pruning\nratio, and is around 100 when 40% complexity reduction is needed. The pruned\nnetwork is retrained to compensate for the loss in accuracy. We have extensively\nevaluated the proposed approach with the CIFAR-10, SVHN and MNIST datasets.\nExperiments with the CIFAR-10 dataset show that more than 85% sparsity can be\ninduced in the convolution layers with less than 1% increase in the misclassification\nrate of the baseline network.", "pdf": "/pdf/0ce16b21425061456ee6180b8db03d13fdb2dca9.pdf", "TL;DR": "This work has proposed a new pruning strategy for CNN. Further, feature map and kernel pruning granularities are proposed for good pruning ratios and simple sparse representation.", "paperhash": "anwar|coarse_pruning_of_convolutional_neural_networks_with_random_masks", "keywords": [], "conflicts": ["snu.ac.kr", "dsp.snu.ac.kr"], "authors": ["Sajid Anwar", "Wonyong Sung"], "authorids": ["sajid@dsp.snu.ac.kr", "wysung@snu.ac.kr"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481939786434, "tcdate": 1481939768313, "number": 3, "id": "rJxBszzNg", "invitation": "ICLR.cc/2017/conference/-/paper183/official/review", "forum": "HkvS3Mqxe", "replyto": "HkvS3Mqxe", "signatures": ["ICLR.cc/2017/conference/paper183/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper183/AnonReviewer1"], "content": {"title": "Paper review", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposes a simple randomized algorithm for selecting which weights in a ConvNet to prune in order to reduce theoretical FLOPs when evaluating a deep neural network. The paper provides a nice taxonomy or pruning granularity from coarse (layer-wise) to fine (intra-kernel). The pruning strategy is empirically driven and uses a validation set to select the best model from N randomly pruned models. Makes claims in the intro about this being \"one shot\" and \"near optimal\" that cannot be supported: it is \"N-shot\" in the sense that N networks are generated and tested and there is no evidence or theory that the found solution is \"near optimal.\"\n\nPros:\n- Nice taxonomy of pruning levels\n- Comparison to the recent weight-sum pruning method\n\nCons:\n- Experimental evaluation does not touch upon recent models (ResNets) and large scale datasets (ImageNet)\n- Paper is somewhat hard to follow\n- Feature map pruning can obviously accelerate computation without specialized sparse implementations of convolution, but this is not the case for finer grained sparsity; since this paper considers fine-grained sparsity it should provide some evidence that introducing that sparsity can yield performance improvements\n\nAnother experimental downside is that the paper does not evaluate the impact of filter pruning on transfer learning. For example, there is not much direct interest in the tasks of MNIST, CIFAR10, or even ImageNet. Instead, a main interest in both academia and industry is the value of the learned representation for transferring to other tasks. One might expect pruning to harm transfer learning. It's possible that the while the main task has about the same performance, transfer learning is strongly hurt. This paper has missed an opportunity to explore that direction.\n\nIn summary, the proposed method is simple, which is good, but the experimental evaluation is somewhat incomplete and does not cover recent models and larger scale datasets.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Coarse Pruning of Convolutional Neural Networks with Random Masks", "abstract": "The learning capability of a neural network improves with increasing depth at\nhigher computational costs. Wider layers with dense kernel connectivity patterns\nfurther increase this cost and may hinder real-time inference. We propose feature\nmap and kernel level pruning for reducing the computational complexity of\na deep convolutional neural network. Pruning feature maps reduces the width\nof a layer and hence does not need any sparse representation. Further, kernel\npruning changes the dense connectivity pattern into a sparse one. Due to coarse\nnature, these pruning granularities can be exploited by GPUs and VLSI based\nimplementations. We propose a simple strategy to choose the least adversarial\npruning masks. The proposed approach is generic and can select good pruning\nmasks for feature map, kernel and intra-kernel pruning. The pruning masks are\ngenerated randomly, and the best performing one is selected using the evaluation\nset. The sufficient number of random pruning masks to try depends on the pruning\nratio, and is around 100 when 40% complexity reduction is needed. The pruned\nnetwork is retrained to compensate for the loss in accuracy. We have extensively\nevaluated the proposed approach with the CIFAR-10, SVHN and MNIST datasets.\nExperiments with the CIFAR-10 dataset show that more than 85% sparsity can be\ninduced in the convolution layers with less than 1% increase in the misclassification\nrate of the baseline network.", "pdf": "/pdf/0ce16b21425061456ee6180b8db03d13fdb2dca9.pdf", "TL;DR": "This work has proposed a new pruning strategy for CNN. Further, feature map and kernel pruning granularities are proposed for good pruning ratios and simple sparse representation.", "paperhash": "anwar|coarse_pruning_of_convolutional_neural_networks_with_random_masks", "keywords": [], "conflicts": ["snu.ac.kr", "dsp.snu.ac.kr"], "authors": ["Sajid Anwar", "Wonyong Sung"], "authorids": ["sajid@dsp.snu.ac.kr", "wysung@snu.ac.kr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512672431, "id": "ICLR.cc/2017/conference/-/paper183/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper183/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper183/AnonReviewer4", "ICLR.cc/2017/conference/paper183/AnonReviewer3", "ICLR.cc/2017/conference/paper183/AnonReviewer1"], "reply": {"forum": "HkvS3Mqxe", "replyto": "HkvS3Mqxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper183/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper183/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512672431}}}, {"tddate": null, "tmdate": 1480738523620, "tcdate": 1480738523610, "number": 3, "id": "SkVyvTyQg", "invitation": "ICLR.cc/2017/conference/-/paper183/pre-review/question", "forum": "HkvS3Mqxe", "replyto": "HkvS3Mqxe", "signatures": ["ICLR.cc/2017/conference/paper183/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper183/AnonReviewer3"], "content": {"title": "Experiments", "question": "In Figure 7, why Kernel running can sometimes beat the baseline? Does it mean the baseline architecture is sub-optimal in terms of kernel numbers?\n\nIs there any actual number for the speed-up in the testing phase? Using sparsity to speed up computation is not very trivial, so I am curious the actual speed-up that can be practically achieved. \n\nThe same question as AnonReviewer4 asked: speeding up large-scale networks, like VGGNet, is much more critical in practice. It will be very helpful to get results for at least one large-scale network. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Coarse Pruning of Convolutional Neural Networks with Random Masks", "abstract": "The learning capability of a neural network improves with increasing depth at\nhigher computational costs. Wider layers with dense kernel connectivity patterns\nfurther increase this cost and may hinder real-time inference. We propose feature\nmap and kernel level pruning for reducing the computational complexity of\na deep convolutional neural network. Pruning feature maps reduces the width\nof a layer and hence does not need any sparse representation. Further, kernel\npruning changes the dense connectivity pattern into a sparse one. Due to coarse\nnature, these pruning granularities can be exploited by GPUs and VLSI based\nimplementations. We propose a simple strategy to choose the least adversarial\npruning masks. The proposed approach is generic and can select good pruning\nmasks for feature map, kernel and intra-kernel pruning. The pruning masks are\ngenerated randomly, and the best performing one is selected using the evaluation\nset. The sufficient number of random pruning masks to try depends on the pruning\nratio, and is around 100 when 40% complexity reduction is needed. The pruned\nnetwork is retrained to compensate for the loss in accuracy. We have extensively\nevaluated the proposed approach with the CIFAR-10, SVHN and MNIST datasets.\nExperiments with the CIFAR-10 dataset show that more than 85% sparsity can be\ninduced in the convolution layers with less than 1% increase in the misclassification\nrate of the baseline network.", "pdf": "/pdf/0ce16b21425061456ee6180b8db03d13fdb2dca9.pdf", "TL;DR": "This work has proposed a new pruning strategy for CNN. Further, feature map and kernel pruning granularities are proposed for good pruning ratios and simple sparse representation.", "paperhash": "anwar|coarse_pruning_of_convolutional_neural_networks_with_random_masks", "keywords": [], "conflicts": ["snu.ac.kr", "dsp.snu.ac.kr"], "authors": ["Sajid Anwar", "Wonyong Sung"], "authorids": ["sajid@dsp.snu.ac.kr", "wysung@snu.ac.kr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959420128, "id": "ICLR.cc/2017/conference/-/paper183/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper183/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper183/AnonReviewer1", "ICLR.cc/2017/conference/paper183/AnonReviewer4", "ICLR.cc/2017/conference/paper183/AnonReviewer3"], "reply": {"forum": "HkvS3Mqxe", "replyto": "HkvS3Mqxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper183/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper183/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959420128}}}, {"tddate": null, "tmdate": 1480431426329, "tcdate": 1480431426325, "number": 2, "id": "SycHDGizx", "invitation": "ICLR.cc/2017/conference/-/paper183/pre-review/question", "forum": "HkvS3Mqxe", "replyto": "HkvS3Mqxe", "signatures": ["ICLR.cc/2017/conference/paper183/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper183/AnonReviewer4"], "content": {"title": "Do you have number for ImageNet?", "question": "Have tested your approach with bigger CNN, like AlexNet or VGG or GoogLeNet, on bigger datasets ImageNet?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Coarse Pruning of Convolutional Neural Networks with Random Masks", "abstract": "The learning capability of a neural network improves with increasing depth at\nhigher computational costs. Wider layers with dense kernel connectivity patterns\nfurther increase this cost and may hinder real-time inference. We propose feature\nmap and kernel level pruning for reducing the computational complexity of\na deep convolutional neural network. Pruning feature maps reduces the width\nof a layer and hence does not need any sparse representation. Further, kernel\npruning changes the dense connectivity pattern into a sparse one. Due to coarse\nnature, these pruning granularities can be exploited by GPUs and VLSI based\nimplementations. We propose a simple strategy to choose the least adversarial\npruning masks. The proposed approach is generic and can select good pruning\nmasks for feature map, kernel and intra-kernel pruning. The pruning masks are\ngenerated randomly, and the best performing one is selected using the evaluation\nset. The sufficient number of random pruning masks to try depends on the pruning\nratio, and is around 100 when 40% complexity reduction is needed. The pruned\nnetwork is retrained to compensate for the loss in accuracy. We have extensively\nevaluated the proposed approach with the CIFAR-10, SVHN and MNIST datasets.\nExperiments with the CIFAR-10 dataset show that more than 85% sparsity can be\ninduced in the convolution layers with less than 1% increase in the misclassification\nrate of the baseline network.", "pdf": "/pdf/0ce16b21425061456ee6180b8db03d13fdb2dca9.pdf", "TL;DR": "This work has proposed a new pruning strategy for CNN. Further, feature map and kernel pruning granularities are proposed for good pruning ratios and simple sparse representation.", "paperhash": "anwar|coarse_pruning_of_convolutional_neural_networks_with_random_masks", "keywords": [], "conflicts": ["snu.ac.kr", "dsp.snu.ac.kr"], "authors": ["Sajid Anwar", "Wonyong Sung"], "authorids": ["sajid@dsp.snu.ac.kr", "wysung@snu.ac.kr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959420128, "id": "ICLR.cc/2017/conference/-/paper183/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper183/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper183/AnonReviewer1", "ICLR.cc/2017/conference/paper183/AnonReviewer4", "ICLR.cc/2017/conference/paper183/AnonReviewer3"], "reply": {"forum": "HkvS3Mqxe", "replyto": "HkvS3Mqxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper183/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper183/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959420128}}}, {"tddate": null, "tmdate": 1480375184109, "tcdate": 1480375184105, "number": 1, "id": "BJdqiE5Gg", "invitation": "ICLR.cc/2017/conference/-/paper183/pre-review/question", "forum": "HkvS3Mqxe", "replyto": "HkvS3Mqxe", "signatures": ["ICLR.cc/2017/conference/paper183/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper183/AnonReviewer1"], "content": {"title": "What does MCR mean?", "question": "The acronym \"MCR\" is used throughout the paper, but I could not find its definition anywhere. It seems to be \"misclassification rate.\" Please confirm or correct. Thanks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Coarse Pruning of Convolutional Neural Networks with Random Masks", "abstract": "The learning capability of a neural network improves with increasing depth at\nhigher computational costs. Wider layers with dense kernel connectivity patterns\nfurther increase this cost and may hinder real-time inference. We propose feature\nmap and kernel level pruning for reducing the computational complexity of\na deep convolutional neural network. Pruning feature maps reduces the width\nof a layer and hence does not need any sparse representation. Further, kernel\npruning changes the dense connectivity pattern into a sparse one. Due to coarse\nnature, these pruning granularities can be exploited by GPUs and VLSI based\nimplementations. We propose a simple strategy to choose the least adversarial\npruning masks. The proposed approach is generic and can select good pruning\nmasks for feature map, kernel and intra-kernel pruning. The pruning masks are\ngenerated randomly, and the best performing one is selected using the evaluation\nset. The sufficient number of random pruning masks to try depends on the pruning\nratio, and is around 100 when 40% complexity reduction is needed. The pruned\nnetwork is retrained to compensate for the loss in accuracy. We have extensively\nevaluated the proposed approach with the CIFAR-10, SVHN and MNIST datasets.\nExperiments with the CIFAR-10 dataset show that more than 85% sparsity can be\ninduced in the convolution layers with less than 1% increase in the misclassification\nrate of the baseline network.", "pdf": "/pdf/0ce16b21425061456ee6180b8db03d13fdb2dca9.pdf", "TL;DR": "This work has proposed a new pruning strategy for CNN. Further, feature map and kernel pruning granularities are proposed for good pruning ratios and simple sparse representation.", "paperhash": "anwar|coarse_pruning_of_convolutional_neural_networks_with_random_masks", "keywords": [], "conflicts": ["snu.ac.kr", "dsp.snu.ac.kr"], "authors": ["Sajid Anwar", "Wonyong Sung"], "authorids": ["sajid@dsp.snu.ac.kr", "wysung@snu.ac.kr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959420128, "id": "ICLR.cc/2017/conference/-/paper183/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper183/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper183/AnonReviewer1", "ICLR.cc/2017/conference/paper183/AnonReviewer4", "ICLR.cc/2017/conference/paper183/AnonReviewer3"], "reply": {"forum": "HkvS3Mqxe", "replyto": "HkvS3Mqxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper183/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper183/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959420128}}}], "count": 10}