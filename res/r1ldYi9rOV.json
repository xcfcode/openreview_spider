{"notes": [{"id": "r1ldYi9rOV", "original": "Syxd7OYr_V", "number": 61, "cdate": 1553472384152, "ddate": null, "tcdate": 1553472384152, "tmdate": 1562082112282, "tddate": null, "forum": "r1ldYi9rOV", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "content": {"title": "Few-Shot Regression via Learned Basis Functions", "authors": ["Yi Loo", "Swee Kiat Lim", "Gemma Roig", "Ngai-Man Cheung"], "authorids": ["loo_yi@sutd.edu.sg", "sweekiat_lim@mymail.sutd.edu.sg", "gemma_roig@sutd.edu.sg", "ngaiman_cheung@sutd.edu.sg"], "keywords": ["Few-Shot Leaning", "Regression", "Learning Basis Functions", "Few-Shot Regression"], "TL;DR": "We propose a few-shot learning model that is tailored specifically for regression tasks", "abstract": "The recent rise in popularity of few-shot learning algorithms has enabled models to quickly adapt to new tasks based on only a few training samples. Previous few-shot learning works have mainly focused on classification and reinforcement learning. \nIn this paper, we propose a few-shot meta-learning system that focuses exclusively on regression tasks. Our model is based on the idea that the degree of freedom of the unknown function can be significantly reduced if it is represented as a linear combination of a set of appropriate basis functions. This enables a few labelled samples to approximate the function. We design a Feature Extractor network to encode basis functions for a task distribution, and a  Weights Generator to generate the weight vector for a novel task. We show that our model outperforms the current state of the art meta-learning methods in various regression tasks.", "pdf": "/pdf/c01b48e2710c9249959266f53a2424b5309f8852.pdf", "paperhash": "loo|fewshot_regression_via_learned_basis_functions"}, "signatures": ["ICLR.cc/2019/Workshop/LLD"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "cdate": 1548689671889, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "content": {"authors": {"values-regex": ".*"}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1548689671889, "tmdate": 1557933709646, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/LLD"], "details": {"writable": true}}}, "tauthor": "OpenReview.net"}, {"id": "Ske0LUqPFE", "original": null, "number": 1, "cdate": 1554650709817, "ddate": null, "tcdate": 1554650709817, "tmdate": 1555512020638, "tddate": null, "forum": "r1ldYi9rOV", "replyto": "r1ldYi9rOV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper61/Official_Review", "content": {"title": "This paper proposes an interesting way learn spasre feature representation to address small sample regression problems.", "review": "This paper proposes a shot-learning method for small sample regression problems. Each regression problem consists of several tasks, each defines an input and output relation. Given samples of different tasks, the goal is to learn these task-dependent relations. The idea is first to learn a sparse feature representations which is task-independent. Then using the feature, output, and task label information to learn a task-specific mapping from the feature to output. These two steps are realized by Feature extractor and Weight generator. When given a new task, one needs to learn the task label of input mapping from the feature to output. This requires to output samples specific to the new task, which is done by task label generator.\n\nThe method seems to be novel and it works well on several regression problems. The idea of sparsity seems to be essential to achieve good estimation. It deserves further understanding. Following the result in Table 1 and 2, the Task Label and its generator plays an important role in 1d case. In 2d, the result is less conclusive since the confidence interval is too big to compare tasks with task label generator and no task label generator, not sure if it is due to the task label generator\u2019s error.\n\nOverall, the idea and results are interesting. The effects of adding task label could also be done on the two new regression problems. This would be interesting to discuss as well.  ", "rating": "3: Marginally above acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper61/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper61/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Few-Shot Regression via Learned Basis Functions", "authors": ["Yi Loo", "Swee Kiat Lim", "Gemma Roig", "Ngai-Man Cheung"], "authorids": ["loo_yi@sutd.edu.sg", "sweekiat_lim@mymail.sutd.edu.sg", "gemma_roig@sutd.edu.sg", "ngaiman_cheung@sutd.edu.sg"], "keywords": ["Few-Shot Leaning", "Regression", "Learning Basis Functions", "Few-Shot Regression"], "TL;DR": "We propose a few-shot learning model that is tailored specifically for regression tasks", "abstract": "The recent rise in popularity of few-shot learning algorithms has enabled models to quickly adapt to new tasks based on only a few training samples. Previous few-shot learning works have mainly focused on classification and reinforcement learning. \nIn this paper, we propose a few-shot meta-learning system that focuses exclusively on regression tasks. Our model is based on the idea that the degree of freedom of the unknown function can be significantly reduced if it is represented as a linear combination of a set of appropriate basis functions. This enables a few labelled samples to approximate the function. We design a Feature Extractor network to encode basis functions for a task distribution, and a  Weights Generator to generate the weight vector for a novel task. We show that our model outperforms the current state of the art meta-learning methods in various regression tasks.", "pdf": "/pdf/c01b48e2710c9249959266f53a2424b5309f8852.pdf", "paperhash": "loo|fewshot_regression_via_learned_basis_functions"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper61/Official_Review", "cdate": 1553713411289, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "r1ldYi9rOV", "replyto": "r1ldYi9rOV", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper61/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper61/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713411289, "tmdate": 1555511821446, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper61/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "S1lolYlnt4", "original": null, "number": 2, "cdate": 1554938099135, "ddate": null, "tcdate": 1554938099135, "tmdate": 1555511878433, "tddate": null, "forum": "r1ldYi9rOV", "replyto": "r1ldYi9rOV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper61/Official_Review", "content": {"title": "Interesting method proposed", "review": "The paper proposes a method that learns a regression model with a few samples.\n\nPros:\n- It is an interesting application.\n- Original work and clearly explained. Mathematically sound.\n- It outperforms other methods.\n\nCons:\n- Just a few examples in the results section. Part of the results were attached as appendices. Looking at the results, my question would be how the different models compared in Tables 1 and 2 perform in the different regression data sets. Only one model is compared for each regression data set.\n", "rating": "3: Marginally above acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper61/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper61/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Few-Shot Regression via Learned Basis Functions", "authors": ["Yi Loo", "Swee Kiat Lim", "Gemma Roig", "Ngai-Man Cheung"], "authorids": ["loo_yi@sutd.edu.sg", "sweekiat_lim@mymail.sutd.edu.sg", "gemma_roig@sutd.edu.sg", "ngaiman_cheung@sutd.edu.sg"], "keywords": ["Few-Shot Leaning", "Regression", "Learning Basis Functions", "Few-Shot Regression"], "TL;DR": "We propose a few-shot learning model that is tailored specifically for regression tasks", "abstract": "The recent rise in popularity of few-shot learning algorithms has enabled models to quickly adapt to new tasks based on only a few training samples. Previous few-shot learning works have mainly focused on classification and reinforcement learning. \nIn this paper, we propose a few-shot meta-learning system that focuses exclusively on regression tasks. Our model is based on the idea that the degree of freedom of the unknown function can be significantly reduced if it is represented as a linear combination of a set of appropriate basis functions. This enables a few labelled samples to approximate the function. We design a Feature Extractor network to encode basis functions for a task distribution, and a  Weights Generator to generate the weight vector for a novel task. We show that our model outperforms the current state of the art meta-learning methods in various regression tasks.", "pdf": "/pdf/c01b48e2710c9249959266f53a2424b5309f8852.pdf", "paperhash": "loo|fewshot_regression_via_learned_basis_functions"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper61/Official_Review", "cdate": 1553713411289, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "r1ldYi9rOV", "replyto": "r1ldYi9rOV", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper61/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper61/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713411289, "tmdate": 1555511821446, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper61/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "r1ll_Fm3KN", "original": null, "number": 1, "cdate": 1554950503637, "ddate": null, "tcdate": 1554950503637, "tmdate": 1555510983608, "tddate": null, "forum": "r1ldYi9rOV", "replyto": "r1ldYi9rOV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper61/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Few-Shot Regression via Learned Basis Functions", "authors": ["Yi Loo", "Swee Kiat Lim", "Gemma Roig", "Ngai-Man Cheung"], "authorids": ["loo_yi@sutd.edu.sg", "sweekiat_lim@mymail.sutd.edu.sg", "gemma_roig@sutd.edu.sg", "ngaiman_cheung@sutd.edu.sg"], "keywords": ["Few-Shot Leaning", "Regression", "Learning Basis Functions", "Few-Shot Regression"], "TL;DR": "We propose a few-shot learning model that is tailored specifically for regression tasks", "abstract": "The recent rise in popularity of few-shot learning algorithms has enabled models to quickly adapt to new tasks based on only a few training samples. Previous few-shot learning works have mainly focused on classification and reinforcement learning. \nIn this paper, we propose a few-shot meta-learning system that focuses exclusively on regression tasks. Our model is based on the idea that the degree of freedom of the unknown function can be significantly reduced if it is represented as a linear combination of a set of appropriate basis functions. This enables a few labelled samples to approximate the function. We design a Feature Extractor network to encode basis functions for a task distribution, and a  Weights Generator to generate the weight vector for a novel task. We show that our model outperforms the current state of the art meta-learning methods in various regression tasks.", "pdf": "/pdf/c01b48e2710c9249959266f53a2424b5309f8852.pdf", "paperhash": "loo|fewshot_regression_via_learned_basis_functions"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper61/Decision", "cdate": 1554736072743, "reply": {"forum": "r1ldYi9rOV", "replyto": "r1ldYi9rOV", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554736072743, "tmdate": 1555510966078, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}], "count": 4}