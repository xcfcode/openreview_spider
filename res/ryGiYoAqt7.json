{"notes": [{"id": "ryGiYoAqt7", "original": "B1g1R4c5tm", "number": 476, "cdate": 1538087810980, "ddate": null, "tcdate": 1538087810980, "tmdate": 1545355410115, "tddate": null, "forum": "ryGiYoAqt7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Learning  agents with prioritization and parameter noise in continuous state and action space", "abstract": "Reinforcement  Learning  (RL) problem can be solved in two different ways - the Value function-based approach and the policy optimization-based approach - to eventually arrive at an optimal policy for the given environment. One of the recent breakthroughs in reinforcement learning is the use of deep neural networks as function approximators to approximate the value function or q-function in a reinforcement learning scheme. This has led to results with agents automatically learning how to play games like alpha-go showing better-than-human performance. Deep Q-learning networks (DQN) and  Deep Deterministic Policy Gradient (DDPG) are two such methods that have shown state-of-the-art results in recent times. Among the many variants of RL, an important class of problems is where the state and action spaces are continuous --- autonomous robots, autonomous vehicles, optimal control are all examples of such problems that can lend themselves naturally to reinforcement based algorithms, and have continuous state and action spaces. In this paper, we adapt and combine approaches such as DQN and DDPG in novel ways to outperform the earlier results for continuous state and action space problems.  We believe these results are a valuable addition to the fast-growing body of results on Reinforcement Learning, more so for continuous state and action space problems.", "keywords": ["reinforcement learning", "continuous action space", "prioritization", "parameter", "noise", "policy gradients"], "authorids": ["rajesh.dm@iiitb.ac.in", "gsr@iiitb.ac.in"], "authors": ["Rajesh Devaraddi", "G. Srinivasaraghavan"], "TL;DR": "Improving the performance of an RL agent in the continuous action and state space domain by using prioritised experience replay and parameter noise.", "pdf": "/pdf/a36561edcadcf5a168252742d99bcc132e87f5c5.pdf", "paperhash": "devaraddi|learning_agents_with_prioritization_and_parameter_noise_in_continuous_state_and_action_space", "_bibtex": "@misc{\ndevaraddi2019learning,\ntitle={Learning  agents with prioritization and parameter noise in continuous state and action space},\nauthor={Rajesh Devaraddi and G. Srinivasaraghavan},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGiYoAqt7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HyeK5OMJxV", "original": null, "number": 1, "cdate": 1544657041351, "ddate": null, "tcdate": 1544657041351, "tmdate": 1545354504018, "tddate": null, "forum": "ryGiYoAqt7", "replyto": "ryGiYoAqt7", "invitation": "ICLR.cc/2019/Conference/-/Paper476/Meta_Review", "content": {"metareview": "The authors take two algorithmic components that were proposed in the context of discrete-action RL - priority replay and parameter noise - and evaluate them with DDPG on continuous control tasks. The different approaches are nicely summarized by the authors, however the contribution of the paper is extremely limited. There is no novelty in the proposed approaches, the empirical evaluation is inconclusive and limited, and there is no analysis or additional insights or results. The AC and the reviewers agree that this paper is not strong enough for ICLR.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "metareview"}, "signatures": ["ICLR.cc/2019/Conference/Paper476/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper476/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning  agents with prioritization and parameter noise in continuous state and action space", "abstract": "Reinforcement  Learning  (RL) problem can be solved in two different ways - the Value function-based approach and the policy optimization-based approach - to eventually arrive at an optimal policy for the given environment. One of the recent breakthroughs in reinforcement learning is the use of deep neural networks as function approximators to approximate the value function or q-function in a reinforcement learning scheme. This has led to results with agents automatically learning how to play games like alpha-go showing better-than-human performance. Deep Q-learning networks (DQN) and  Deep Deterministic Policy Gradient (DDPG) are two such methods that have shown state-of-the-art results in recent times. Among the many variants of RL, an important class of problems is where the state and action spaces are continuous --- autonomous robots, autonomous vehicles, optimal control are all examples of such problems that can lend themselves naturally to reinforcement based algorithms, and have continuous state and action spaces. In this paper, we adapt and combine approaches such as DQN and DDPG in novel ways to outperform the earlier results for continuous state and action space problems.  We believe these results are a valuable addition to the fast-growing body of results on Reinforcement Learning, more so for continuous state and action space problems.", "keywords": ["reinforcement learning", "continuous action space", "prioritization", "parameter", "noise", "policy gradients"], "authorids": ["rajesh.dm@iiitb.ac.in", "gsr@iiitb.ac.in"], "authors": ["Rajesh Devaraddi", "G. Srinivasaraghavan"], "TL;DR": "Improving the performance of an RL agent in the continuous action and state space domain by using prioritised experience replay and parameter noise.", "pdf": "/pdf/a36561edcadcf5a168252742d99bcc132e87f5c5.pdf", "paperhash": "devaraddi|learning_agents_with_prioritization_and_parameter_noise_in_continuous_state_and_action_space", "_bibtex": "@misc{\ndevaraddi2019learning,\ntitle={Learning  agents with prioritization and parameter noise in continuous state and action space},\nauthor={Rajesh Devaraddi and G. Srinivasaraghavan},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGiYoAqt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper476/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353203625, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryGiYoAqt7", "replyto": "ryGiYoAqt7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper476/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper476/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper476/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353203625}}}, {"id": "BkeQx84T2X", "original": null, "number": 3, "cdate": 1541387755300, "ddate": null, "tcdate": 1541387755300, "tmdate": 1541533963645, "tddate": null, "forum": "ryGiYoAqt7", "replyto": "ryGiYoAqt7", "invitation": "ICLR.cc/2019/Conference/-/Paper476/Official_Review", "content": {"title": "Barely any novelty", "review": "The paper proposes an augmentation of the DDPG algorithm with prioritized experience replay plus parameter noise. Empirical evaluations of the proposed algorithm are conducted on Mujoco benchmarks while the results are mixed.\n\nAs far as I can see, the paper contains almost no novelty as it crudely puts together three existing algorithms without presenting enough motivation. This can be clearly seen even from the structuring of the paper, since before the experimental section, only a short two-paragraph subsection (4.1) and an algorithm chart are devoted to the description of the main ideas. Furthermore, the algorithm itself is a just simple addition of well-known techniques (DDPG + prioritized experience replay + parameter noise) none of which is proposed in the current paper. Finally, as shown in the experimental sections, I don't see a evidence that the proposed algorithm consistently outperform the baseline.\n\nTo sum up, I believe the submission is below the novelty threshold for a publication at ICLR.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper476/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning  agents with prioritization and parameter noise in continuous state and action space", "abstract": "Reinforcement  Learning  (RL) problem can be solved in two different ways - the Value function-based approach and the policy optimization-based approach - to eventually arrive at an optimal policy for the given environment. One of the recent breakthroughs in reinforcement learning is the use of deep neural networks as function approximators to approximate the value function or q-function in a reinforcement learning scheme. This has led to results with agents automatically learning how to play games like alpha-go showing better-than-human performance. Deep Q-learning networks (DQN) and  Deep Deterministic Policy Gradient (DDPG) are two such methods that have shown state-of-the-art results in recent times. Among the many variants of RL, an important class of problems is where the state and action spaces are continuous --- autonomous robots, autonomous vehicles, optimal control are all examples of such problems that can lend themselves naturally to reinforcement based algorithms, and have continuous state and action spaces. In this paper, we adapt and combine approaches such as DQN and DDPG in novel ways to outperform the earlier results for continuous state and action space problems.  We believe these results are a valuable addition to the fast-growing body of results on Reinforcement Learning, more so for continuous state and action space problems.", "keywords": ["reinforcement learning", "continuous action space", "prioritization", "parameter", "noise", "policy gradients"], "authorids": ["rajesh.dm@iiitb.ac.in", "gsr@iiitb.ac.in"], "authors": ["Rajesh Devaraddi", "G. Srinivasaraghavan"], "TL;DR": "Improving the performance of an RL agent in the continuous action and state space domain by using prioritised experience replay and parameter noise.", "pdf": "/pdf/a36561edcadcf5a168252742d99bcc132e87f5c5.pdf", "paperhash": "devaraddi|learning_agents_with_prioritization_and_parameter_noise_in_continuous_state_and_action_space", "_bibtex": "@misc{\ndevaraddi2019learning,\ntitle={Learning  agents with prioritization and parameter noise in continuous state and action space},\nauthor={Rajesh Devaraddi and G. Srinivasaraghavan},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGiYoAqt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper476/Official_Review", "cdate": 1542234452727, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryGiYoAqt7", "replyto": "ryGiYoAqt7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper476/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335732884, "tmdate": 1552335732884, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper476/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1ez5NE52m", "original": null, "number": 2, "cdate": 1541190793866, "ddate": null, "tcdate": 1541190793866, "tmdate": 1541533963443, "tddate": null, "forum": "ryGiYoAqt7", "replyto": "ryGiYoAqt7", "invitation": "ICLR.cc/2019/Conference/-/Paper476/Official_Review", "content": {"title": "Interesting paper but limited novelty", "review": "This paper combines elements of two existing reinforcement learning approaches, namely, Deep Q-learning Networks (DQN) with Prioritised Experience Replay (PER) and Deep Deterministic Policy Gradient (DDPG) to propose the Prioritized Deep Deterministic Policy Gradient (PDDPG) algorithm. The problem is interesting and there is a nice review of relevant work. The algorithm has a limited novelty with a simple modification of the DDPG algorithm to add the PER component. Experiment results show improvements in certain simulation environments. However, the paper lacks insight on how and why results are improved on some settings while performing worse than the others. Detailed comments are as follows:\n\n1. Algorithm 1 is not self-contained. Yes, I understand that it is a slight modification to DDPG with changes being Line 11 and 16. But p_i^alpha is not defined anywhere in Algorithm 1. How the transition probabilties are updated on Line 16 is also not clear to me.\n\n2. It would be better if multiple simulation runs on the same experiment can be performed to have a more reliable display of performance.\n\n3. Section 6 is on Parameter Space Noise for Exploration. This is not the authors' proposed work so it is strange to have a separate section here. In the end of Section 1, the authors wrote that \"We then use the concept of parameter space noise for exploration and show that this further improves the rewards achieved.\" This seems to be a bold claim from the varying performance displayed in Figure 2-4. Similar to Comment 2, more simulation runs and statistical tests need to be conducted to support this claim.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper476/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning  agents with prioritization and parameter noise in continuous state and action space", "abstract": "Reinforcement  Learning  (RL) problem can be solved in two different ways - the Value function-based approach and the policy optimization-based approach - to eventually arrive at an optimal policy for the given environment. One of the recent breakthroughs in reinforcement learning is the use of deep neural networks as function approximators to approximate the value function or q-function in a reinforcement learning scheme. This has led to results with agents automatically learning how to play games like alpha-go showing better-than-human performance. Deep Q-learning networks (DQN) and  Deep Deterministic Policy Gradient (DDPG) are two such methods that have shown state-of-the-art results in recent times. Among the many variants of RL, an important class of problems is where the state and action spaces are continuous --- autonomous robots, autonomous vehicles, optimal control are all examples of such problems that can lend themselves naturally to reinforcement based algorithms, and have continuous state and action spaces. In this paper, we adapt and combine approaches such as DQN and DDPG in novel ways to outperform the earlier results for continuous state and action space problems.  We believe these results are a valuable addition to the fast-growing body of results on Reinforcement Learning, more so for continuous state and action space problems.", "keywords": ["reinforcement learning", "continuous action space", "prioritization", "parameter", "noise", "policy gradients"], "authorids": ["rajesh.dm@iiitb.ac.in", "gsr@iiitb.ac.in"], "authors": ["Rajesh Devaraddi", "G. Srinivasaraghavan"], "TL;DR": "Improving the performance of an RL agent in the continuous action and state space domain by using prioritised experience replay and parameter noise.", "pdf": "/pdf/a36561edcadcf5a168252742d99bcc132e87f5c5.pdf", "paperhash": "devaraddi|learning_agents_with_prioritization_and_parameter_noise_in_continuous_state_and_action_space", "_bibtex": "@misc{\ndevaraddi2019learning,\ntitle={Learning  agents with prioritization and parameter noise in continuous state and action space},\nauthor={Rajesh Devaraddi and G. Srinivasaraghavan},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGiYoAqt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper476/Official_Review", "cdate": 1542234452727, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryGiYoAqt7", "replyto": "ryGiYoAqt7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper476/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335732884, "tmdate": 1552335732884, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper476/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1ePXjQXh7", "original": null, "number": 1, "cdate": 1540729631512, "ddate": null, "tcdate": 1540729631512, "tmdate": 1541533963235, "tddate": null, "forum": "ryGiYoAqt7", "replyto": "ryGiYoAqt7", "invitation": "ICLR.cc/2019/Conference/-/Paper476/Official_Review", "content": {"title": "Limited novelty", "review": "The paper proposes PDDPG, a combination of prioritized experience replay, parameter noise exploration, and DDPG. Different combinations are then evaluated on MuJoCo domains, and the results are mixed. \n\nThe novelty of the work is limited, and the results are hard to interpret: sometimes PDDPG performs better, sometimes worse, and the training curves are only obtained with a single random seed. Also presented results are substantially worse than current state of the art (e.g., TD3, SAC).\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper476/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning  agents with prioritization and parameter noise in continuous state and action space", "abstract": "Reinforcement  Learning  (RL) problem can be solved in two different ways - the Value function-based approach and the policy optimization-based approach - to eventually arrive at an optimal policy for the given environment. One of the recent breakthroughs in reinforcement learning is the use of deep neural networks as function approximators to approximate the value function or q-function in a reinforcement learning scheme. This has led to results with agents automatically learning how to play games like alpha-go showing better-than-human performance. Deep Q-learning networks (DQN) and  Deep Deterministic Policy Gradient (DDPG) are two such methods that have shown state-of-the-art results in recent times. Among the many variants of RL, an important class of problems is where the state and action spaces are continuous --- autonomous robots, autonomous vehicles, optimal control are all examples of such problems that can lend themselves naturally to reinforcement based algorithms, and have continuous state and action spaces. In this paper, we adapt and combine approaches such as DQN and DDPG in novel ways to outperform the earlier results for continuous state and action space problems.  We believe these results are a valuable addition to the fast-growing body of results on Reinforcement Learning, more so for continuous state and action space problems.", "keywords": ["reinforcement learning", "continuous action space", "prioritization", "parameter", "noise", "policy gradients"], "authorids": ["rajesh.dm@iiitb.ac.in", "gsr@iiitb.ac.in"], "authors": ["Rajesh Devaraddi", "G. Srinivasaraghavan"], "TL;DR": "Improving the performance of an RL agent in the continuous action and state space domain by using prioritised experience replay and parameter noise.", "pdf": "/pdf/a36561edcadcf5a168252742d99bcc132e87f5c5.pdf", "paperhash": "devaraddi|learning_agents_with_prioritization_and_parameter_noise_in_continuous_state_and_action_space", "_bibtex": "@misc{\ndevaraddi2019learning,\ntitle={Learning  agents with prioritization and parameter noise in continuous state and action space},\nauthor={Rajesh Devaraddi and G. Srinivasaraghavan},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGiYoAqt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper476/Official_Review", "cdate": 1542234452727, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryGiYoAqt7", "replyto": "ryGiYoAqt7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper476/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335732884, "tmdate": 1552335732884, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper476/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}