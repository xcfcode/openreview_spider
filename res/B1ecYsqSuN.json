{"notes": [{"id": "B1ecYsqSuN", "original": "H1xzIBqH_4", "number": 64, "cdate": 1553472385694, "ddate": null, "tcdate": 1553472385694, "tmdate": 1562082111669, "tddate": null, "forum": "B1ecYsqSuN", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "content": {"title": "INCORPORATING BILINGUAL DICTIONARIES FOR LOW RESOURCE SEMI-SUPERVISED NEURAL MACHINE TRANSLATION", "authors": ["Mihir Kale", "Sreyashi Nag", "Varun Lakshinarasimhan", "Swapnil Singhavi"], "authorids": ["mihirkale815@gmail.com", "sreyashinag28@gmail.com", "vbl@andrew.cmu.edu", "ssinghav@andrew.cmu.edu"], "keywords": [], "TL;DR": "We use bilingual dictionaries for data augmentation for neural machine translation", "abstract": "We explore ways of incorporating bilingual dictionaries to enable semi-supervised\nneural machine translation. Conventional back-translation methods have shown\nsuccess in leveraging target side monolingual data. However, since the quality of\nback-translation models is tied to the size of the available parallel corpora, this\ncould adversely impact the synthetically generated sentences in a low resource\nsetting. We propose a simple data augmentation technique to address both this\nshortcoming. We incorporate widely available bilingual dictionaries that yield\nword-by-word translations to generate synthetic sentences. This automatically\nexpands the vocabulary of the model while maintaining high quality content. Our\nmethod shows an appreciable improvement in performance over strong baselines.", "pdf": "/pdf/0182398bb5851483c8d6e0bc9a36c160c276760a.pdf", "paperhash": "kale|incorporating_bilingual_dictionaries_for_low_resource_semisupervised_neural_machine_translation"}, "signatures": ["ICLR.cc/2019/Workshop/LLD"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "cdate": 1548689671889, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "content": {"authors": {"values-regex": ".*"}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1548689671889, "tmdate": 1557933709646, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/LLD"], "details": {"writable": true}}}, "tauthor": "OpenReview.net"}, {"id": "BkghQVbBYE", "original": null, "number": 1, "cdate": 1554482212103, "ddate": null, "tcdate": 1554482212103, "tmdate": 1555512026786, "tddate": null, "forum": "B1ecYsqSuN", "replyto": "B1ecYsqSuN", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper64/Official_Review", "content": {"title": "Simple technique for improving low-resource translation when bilingual dictionaries are given.", "review": "This paper investigates the idea of using bilingual dictionaries to create synthetic sources for target-side monolingual data in order to improve over NMT models trained with small amounts of parallel data. \nThis strategy is compared with back-translation and copying the target to the source side and evaluated on TED data for de-en and es-en in a simulated low-resource and a domain adaptation setting. The empirical results show that when little parallel data is available  in addition to bilingual dictionaries, this method can outperform back-translation and copying.\n\nPros:\n- Written clearly\n- Reproducible (hyperparameters, data)\n- Evaluation shows improvements of the proposed model over baselines, despite the simplicity of the data and the noise in the sources.\n- Effect of data sizes are studied.\n- Good review of related work.\n\nCons:\n- The low-resource setting is only simulated. It would have been to take a truely low-resource language and evaluate the methods on that (e.g. the other language pairs presented in Qi et al. 2018).\n- The requirement of bilingual dictionaries and their coverage and their domain dependence is not discussed. If little parallel data is available, can we simply assume the existence of large dictionaries?\n- It is assumed that the word-by-word dictionary translation \"at least ensures that the words in the synthetic sentences are accurate\" (\u00a74). This is critical since it ignores the problem of polysemy - one word in the target language can often have more than one meaning in the source language: which one is picked for generating the synthetic sentence?\n\nIn summary, despite its clarity and simplicity, I don't find the paper very creative regarding the methodology, and it does not sufficiently answer the question when dictionaries outperform back-translation, since the properties of the additional resource, i.e. the dictionary, are not discussed/investigated, neither its limitations. It would have been interesting to see the same approach in a truely low-resource problem where the dictionary might be limited as well.\n\nDetails:\n- Consider changing the acronym of the method, it seems widely adopted for World of Warcraft.\n- Table 1 has encoding problems for \u00e1, \u00f2 etc.", "rating": "3: Marginally above acceptance threshold", "confidence": "3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper64/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper64/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "INCORPORATING BILINGUAL DICTIONARIES FOR LOW RESOURCE SEMI-SUPERVISED NEURAL MACHINE TRANSLATION", "authors": ["Mihir Kale", "Sreyashi Nag", "Varun Lakshinarasimhan", "Swapnil Singhavi"], "authorids": ["mihirkale815@gmail.com", "sreyashinag28@gmail.com", "vbl@andrew.cmu.edu", "ssinghav@andrew.cmu.edu"], "keywords": [], "TL;DR": "We use bilingual dictionaries for data augmentation for neural machine translation", "abstract": "We explore ways of incorporating bilingual dictionaries to enable semi-supervised\nneural machine translation. Conventional back-translation methods have shown\nsuccess in leveraging target side monolingual data. However, since the quality of\nback-translation models is tied to the size of the available parallel corpora, this\ncould adversely impact the synthetically generated sentences in a low resource\nsetting. We propose a simple data augmentation technique to address both this\nshortcoming. We incorporate widely available bilingual dictionaries that yield\nword-by-word translations to generate synthetic sentences. This automatically\nexpands the vocabulary of the model while maintaining high quality content. Our\nmethod shows an appreciable improvement in performance over strong baselines.", "pdf": "/pdf/0182398bb5851483c8d6e0bc9a36c160c276760a.pdf", "paperhash": "kale|incorporating_bilingual_dictionaries_for_low_resource_semisupervised_neural_machine_translation"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper64/Official_Review", "cdate": 1553713410722, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "B1ecYsqSuN", "replyto": "B1ecYsqSuN", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper64/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper64/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713410722, "tmdate": 1555511820892, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper64/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "S1geNV6BKN", "original": null, "number": 2, "cdate": 1554531368271, "ddate": null, "tcdate": 1554531368271, "tmdate": 1555512024935, "tddate": null, "forum": "B1ecYsqSuN", "replyto": "B1ecYsqSuN", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper64/Official_Review", "content": {"title": "Good paper. Approach is simple and intuitive, yet effective. The paper lacks a comparison to/discussion with a closely related work. Requires a few formatting fixes ", "review": "Paper summary:\nThis paper targets machine translation of low-resource languages, where the main problem of current approaches is dealing with Out-Of-Vocabulary words. Given a small parallel corpus of the source and target language, the authors propose a data augmentation technique using dictionaries of the source-target languages. Specifically, given a relatively small parallel corpus of source and target sentences, and given a new set of sentences in the target language (English, in this work) and a dictionary from the target to the source language, the set of sentences are translated word by word using the dictionary the source language (Germen, and Spanish in this work), then the original sentence and the resulting sentences are added to target and source datasets in the parallel corpus, respectively. \nThe authors compare their proposed methods to two existing techniques: Back translation, COPY which copies OOV words into the sentence without translation. In their various experiments, the results convey the effectiveness of the proposed approach where it achieves an increase in the BLEU score.\n\nPros.\n1-\tThe paper suits the workshop domain.\n2-\tThe proposed approach is simple, yet it performs on par with the other baseline methods. \n3-\tThe proposed model is evaluated in different scenarios, and the experimental details are provided in the paper. \n4-\tGenerally, the paper is well-written and easy to follow. \n5-\tThe authors discussed how their proposed method has higher coverage on both the target and source languages, in contrast to the COPY method which targets only the target language. I think this is an important contribution and could replace the third contribution.\n6-\tThe authors discussed one of the potential side effects that word-by-word translation can cause, which is the syntax/grammar correctness of the resulting sentence. \nCons.\n1-\tI believe this paper should include a comparison with Sennrich et al 2015 below or at least a discussion on why it was excluded. This work was proposed to address the same problem that the authors target. In this work, sub-words are used as the tokens for translation in order to address the OOV problem. Specifically, an external dataset is used to get a list of most common subwords instead of full words. \nSennrich, R., Haddow, B., & Birch, A. (2015). Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909.\n\n2-\tUsing a dictionary for word by word translation is very similar to the idea of using synonyms to mask a writing style. Both these ideas result in not only syntactic issues but semantic ones as well. For example, \u2018Kicked the bucket\u2019 which means \u2018passed away\u2019 will lose its meaning if translated word by word. The syntactic part was covered properly in Section 4, while the semantic part was partially covered in Section 5.4 where the authors discuss domain adaptation. In my opinion, (and of course as the results show) using COPY with the authors\u2019 method should be elaborated more, both in the discussion and the experiments.   \n\nAdditional minor (formatting) issues:\n-Table 1 is not clear. First, since the study is performed on two languages, the caption should specify this example is on which language. Second, since different approaches target a different part of the corpus (either the source or the target language) I suggest separating them either in two smaller tables or by having an empty row. What I see in this table is an alternation between languages and I find it a bit confusing. \n-Tables 2 and 3. Please add \u2018using BLUE score\u2019 to the captions. That would be faster to spot compared to looking for it in the text. \n", "rating": "3: Marginally above acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper64/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper64/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "INCORPORATING BILINGUAL DICTIONARIES FOR LOW RESOURCE SEMI-SUPERVISED NEURAL MACHINE TRANSLATION", "authors": ["Mihir Kale", "Sreyashi Nag", "Varun Lakshinarasimhan", "Swapnil Singhavi"], "authorids": ["mihirkale815@gmail.com", "sreyashinag28@gmail.com", "vbl@andrew.cmu.edu", "ssinghav@andrew.cmu.edu"], "keywords": [], "TL;DR": "We use bilingual dictionaries for data augmentation for neural machine translation", "abstract": "We explore ways of incorporating bilingual dictionaries to enable semi-supervised\nneural machine translation. Conventional back-translation methods have shown\nsuccess in leveraging target side monolingual data. However, since the quality of\nback-translation models is tied to the size of the available parallel corpora, this\ncould adversely impact the synthetically generated sentences in a low resource\nsetting. We propose a simple data augmentation technique to address both this\nshortcoming. We incorporate widely available bilingual dictionaries that yield\nword-by-word translations to generate synthetic sentences. This automatically\nexpands the vocabulary of the model while maintaining high quality content. Our\nmethod shows an appreciable improvement in performance over strong baselines.", "pdf": "/pdf/0182398bb5851483c8d6e0bc9a36c160c276760a.pdf", "paperhash": "kale|incorporating_bilingual_dictionaries_for_low_resource_semisupervised_neural_machine_translation"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper64/Official_Review", "cdate": 1553713410722, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "B1ecYsqSuN", "replyto": "B1ecYsqSuN", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper64/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper64/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713410722, "tmdate": 1555511820892, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper64/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "SJlQvanz5V", "original": null, "number": 1, "cdate": 1555381595018, "ddate": null, "tcdate": 1555381595018, "tmdate": 1555510977019, "tddate": null, "forum": "B1ecYsqSuN", "replyto": "B1ecYsqSuN", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper64/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "INCORPORATING BILINGUAL DICTIONARIES FOR LOW RESOURCE SEMI-SUPERVISED NEURAL MACHINE TRANSLATION", "authors": ["Mihir Kale", "Sreyashi Nag", "Varun Lakshinarasimhan", "Swapnil Singhavi"], "authorids": ["mihirkale815@gmail.com", "sreyashinag28@gmail.com", "vbl@andrew.cmu.edu", "ssinghav@andrew.cmu.edu"], "keywords": [], "TL;DR": "We use bilingual dictionaries for data augmentation for neural machine translation", "abstract": "We explore ways of incorporating bilingual dictionaries to enable semi-supervised\nneural machine translation. Conventional back-translation methods have shown\nsuccess in leveraging target side monolingual data. However, since the quality of\nback-translation models is tied to the size of the available parallel corpora, this\ncould adversely impact the synthetically generated sentences in a low resource\nsetting. We propose a simple data augmentation technique to address both this\nshortcoming. We incorporate widely available bilingual dictionaries that yield\nword-by-word translations to generate synthetic sentences. This automatically\nexpands the vocabulary of the model while maintaining high quality content. Our\nmethod shows an appreciable improvement in performance over strong baselines.", "pdf": "/pdf/0182398bb5851483c8d6e0bc9a36c160c276760a.pdf", "paperhash": "kale|incorporating_bilingual_dictionaries_for_low_resource_semisupervised_neural_machine_translation"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper64/Decision", "cdate": 1554736072185, "reply": {"forum": "B1ecYsqSuN", "replyto": "B1ecYsqSuN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554736072185, "tmdate": 1555510966636, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}], "count": 4}