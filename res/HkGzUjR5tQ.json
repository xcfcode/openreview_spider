{"notes": [{"id": "HkGzUjR5tQ", "original": "SJg5pzlqKm", "number": 154, "cdate": 1538087753889, "ddate": null, "tcdate": 1538087753889, "tmdate": 1545355441592, "tddate": null, "forum": "HkGzUjR5tQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "DATNet: Dual Adversarial Transfer for Low-resource Named Entity Recognition", "abstract": "We propose a new architecture termed Dual Adversarial Transfer Network (DATNet) for addressing low-resource Named Entity Recognition (NER). Specifically, two variants of DATNet, i.e., DATNet-F and DATNet-P, are proposed to explore effective feature fusion between high and low resource. To address the noisy and imbalanced training data, we propose a novel Generalized Resource-Adversarial Discriminator (GRAD). Additionally, adversarial training is adopted to boost model generalization. We examine the effects of different components in DATNet across domains and languages and show that significant improvement can be obtained especially for low-resource data. Without augmenting any additional hand-crafted features, we achieve new state-of-the-art performances on CoNLL and Twitter NER---88.16% F1 for Spanish, 53.43% F1 for WNUT-2016, and 42.83% F1 for WNUT-2017.", "keywords": ["Low-resource", "Named Entity Recognition"], "authorids": ["joey.tianyi.zhou@gmail.com", "isaac.changhau@gmail.com", "jindi15@mit.edu", "hongyuanzhu.cn@gmail.com", "gohsm@ihpc.a-star.edu.sg", "kenkwok@ihpc.a-star.edu.sg"], "authors": ["Joey Tianyi Zhou", "Hao Zhang", "Di Jin", "Hongyuan Zhu", "Rick Siow Mong Goh", "Kenneth Kwok"], "TL;DR": "We propose a new  architecture termed Dual Adversarial Transfer Network (DATNet) for addressing low-resource Named Entity Recognition (NER) and achieve new state-of-the-art performances on CoNLL and Twitter NER.", "pdf": "/pdf/490de072f0df42fa74520f68c323b0bd81419230.pdf", "paperhash": "zhou|datnet_dual_adversarial_transfer_for_lowresource_named_entity_recognition", "_bibtex": "@misc{\nzhou2019datnet,\ntitle={{DATN}et: Dual Adversarial Transfer for Low-resource Named Entity Recognition},\nauthor={Joey Tianyi Zhou and Hao Zhang and Di Jin and Hongyuan Zhu and Rick Siow Mong Goh and Kenneth Kwok},\nyear={2019},\nurl={https://openreview.net/forum?id=HkGzUjR5tQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Skg1g7FggN", "original": null, "number": 1, "cdate": 1544749798841, "ddate": null, "tcdate": 1544749798841, "tmdate": 1545354475972, "tddate": null, "forum": "HkGzUjR5tQ", "replyto": "HkGzUjR5tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper154/Meta_Review", "content": {"metareview": "A focused contribution that is clearly presented. That being said, the task of low-resource named entity recognition is fairly narrow and it is hard to tell how significant the empirical results are. The paper could be much stronger if it evaluated on a second task (and third task). Right now it is unclear whether the technique would generalize to other tasks.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Reject"}, "signatures": ["ICLR.cc/2019/Conference/Paper154/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper154/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DATNet: Dual Adversarial Transfer for Low-resource Named Entity Recognition", "abstract": "We propose a new architecture termed Dual Adversarial Transfer Network (DATNet) for addressing low-resource Named Entity Recognition (NER). Specifically, two variants of DATNet, i.e., DATNet-F and DATNet-P, are proposed to explore effective feature fusion between high and low resource. To address the noisy and imbalanced training data, we propose a novel Generalized Resource-Adversarial Discriminator (GRAD). Additionally, adversarial training is adopted to boost model generalization. We examine the effects of different components in DATNet across domains and languages and show that significant improvement can be obtained especially for low-resource data. Without augmenting any additional hand-crafted features, we achieve new state-of-the-art performances on CoNLL and Twitter NER---88.16% F1 for Spanish, 53.43% F1 for WNUT-2016, and 42.83% F1 for WNUT-2017.", "keywords": ["Low-resource", "Named Entity Recognition"], "authorids": ["joey.tianyi.zhou@gmail.com", "isaac.changhau@gmail.com", "jindi15@mit.edu", "hongyuanzhu.cn@gmail.com", "gohsm@ihpc.a-star.edu.sg", "kenkwok@ihpc.a-star.edu.sg"], "authors": ["Joey Tianyi Zhou", "Hao Zhang", "Di Jin", "Hongyuan Zhu", "Rick Siow Mong Goh", "Kenneth Kwok"], "TL;DR": "We propose a new  architecture termed Dual Adversarial Transfer Network (DATNet) for addressing low-resource Named Entity Recognition (NER) and achieve new state-of-the-art performances on CoNLL and Twitter NER.", "pdf": "/pdf/490de072f0df42fa74520f68c323b0bd81419230.pdf", "paperhash": "zhou|datnet_dual_adversarial_transfer_for_lowresource_named_entity_recognition", "_bibtex": "@misc{\nzhou2019datnet,\ntitle={{DATN}et: Dual Adversarial Transfer for Low-resource Named Entity Recognition},\nauthor={Joey Tianyi Zhou and Hao Zhang and Di Jin and Hongyuan Zhu and Rick Siow Mong Goh and Kenneth Kwok},\nyear={2019},\nurl={https://openreview.net/forum?id=HkGzUjR5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper154/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353317884, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkGzUjR5tQ", "replyto": "HkGzUjR5tQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper154/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper154/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper154/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353317884}}}, {"id": "Skg5diLZCm", "original": null, "number": 3, "cdate": 1542708082030, "ddate": null, "tcdate": 1542708082030, "tmdate": 1542708082030, "tddate": null, "forum": "HkGzUjR5tQ", "replyto": "rklsdRj5h7", "invitation": "ICLR.cc/2019/Conference/-/Paper154/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "\nComment 1: \u201c...the technical novelty of each component is limited. GRAD, for example, is rather a minor modification of Language Adversarial Discriminator (Kim et al., 2017) with a scalar weight parameter on the loss. \u2026 for this authors need to report quantitative results of the (Base + AT + F/P-transfer with AD...\u201d\n\nResponse 1\uff1aThanks for your comments. The major difference between our GRAD and that proposed by (Kim et al, 2017) is that we introduced the source weight \\alpha and instance weight \\gamma to address the influence of imbalanced training data and dominant easy training samples. The instance weight is adaptively learned automatically from data, which has not been explored in related literature to the best of our knowledge. The effectiveness of these components with elegant and reasonable modeling is shown in Table 3 by comparing our performance with other state-of-the-art.  We also added a more quantitative comparison of model F/P-transfer without AD,  with AD,  and with GRAD  in Table 6.  From the results, we found that GRAD consistently outperforms AD across different settings.  \n\n-------------------------------------\nComment 2:  \"Authors use both source and target data to train their \u201cbase model\u201d as well \u2026e.g. presumably by merging the source and target dataset as well as their embedding matrices, etc.\"\n\nResponse 2:  Thank you for pointing out this misunderstanding. Our base model only uses the target data.  We have clarified this in the footnote of Table  4.  \n\n------------------------------------\nComment 3:  \"It is important that authors report if there ever is a negative transfer case \u2026 at varying resource scenarios -- especially at sufficiently-resourced cases.\"\n\nResponse 3: Thanks for the suggestion. In the revision,  we also experiment on a large-scale cross-lingual named entity dataset which was brought out by (Pan  et al.,2017)  and contains 282 languages for evaluation. We choose 9 languages in our experiments and the results are summarized in Table 4.  We observe that for the high-resource scenario, say, when the target language data is sufficient, the improvements of transfer learning are not very distinct compared with that for low-resource scenario under in-family in-branch case,  and we also find that there is no effect by transferring knowledge from Arabic to Galician and Ukrainian, which we suspect is caused by the great linguistic differences between source and target languages, since, for example, Arabic and Galician are from totally different linguistic families.\n\n--------------------------------\nComment 4: \u201cthe aforementioned baseline (naive merge of the source and target data) is obviously an important baseline to report.\u201d\n\nResponse 4: Training with the naive merge of the source and target data is equivalent to our baseline (Base model +  F-T  without AD) shown in Table 6.  \n\n--------------------------------\nComment 5:  \"The confusion comes mainly because some of the SOTA results the authors quote are in-domain training / in-domain evaluation results on respective languages, and some are cross-domain results -- yet they are all under \u201ccross-lingual/domain\u201d columns in Table 2\".\n\nResponse 5:  Thanks for your suggestion.  We have updated the Table  3 in the revision with the separation for the mono-language/domain and cross-language/domain.  \n\n----------------------------\nComment 6: \" It would be interesting to report the learned optimal \\alpha value for each different setting (at varying r or training size) to see if the authors\u2019 intuition is met.\"\n\nResponse 6:  Yes,  \\alpha is tunable.  We analyze the discriminator weight \\alpha in GRAD and results are summarized in Table  8. From the results, it is interesting to find that \\alpha is directly proportional to the data ratio \\rho, basically, which means that more target training data requires larger \\alpha (i.e., smaller 1-\\alpha to reduce training emphasis on the target domain) to achieve better performance.\n\n-----------------------------\nComment 7:  \"- Section 3.2.3, \u201c... (GRAD) to enable adaptive weights for [each sample]\u201d \u2192 I think it reads better with [each resource] or [each source and target] unless you meant \\alpha_i for each sentence.\"\n\nResponse 7: Here \\alpha is indeed adapted for each sentence. We have clarified it in the corresponding place in the revision like this: \"To overcome this issue, we further propose Generalized Resource-Adversarial Discriminator (GRAD) to enable adaptive weights for each sample (note that the sample here means each sentence of resource), which focuses the model training on hard samples.\"\n\n---------------------------\nComment 8:  \"Section 3.2.5, \u201c... recently, adversarial samples are [wisely] incorporated\u201d \u2192 [widely]\"\n\nResponse 8: Thanks for this comment. We have replaced \"wisely\" with \"widely\".\n\n---------------------------\nComment 9: \"Fonts for figures could be bigger.\"\n\nResponse 9: Thank you for this suggestion. We have enlarged the font of figures for clearness in the revision.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper154/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper154/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper154/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DATNet: Dual Adversarial Transfer for Low-resource Named Entity Recognition", "abstract": "We propose a new architecture termed Dual Adversarial Transfer Network (DATNet) for addressing low-resource Named Entity Recognition (NER). Specifically, two variants of DATNet, i.e., DATNet-F and DATNet-P, are proposed to explore effective feature fusion between high and low resource. To address the noisy and imbalanced training data, we propose a novel Generalized Resource-Adversarial Discriminator (GRAD). Additionally, adversarial training is adopted to boost model generalization. We examine the effects of different components in DATNet across domains and languages and show that significant improvement can be obtained especially for low-resource data. Without augmenting any additional hand-crafted features, we achieve new state-of-the-art performances on CoNLL and Twitter NER---88.16% F1 for Spanish, 53.43% F1 for WNUT-2016, and 42.83% F1 for WNUT-2017.", "keywords": ["Low-resource", "Named Entity Recognition"], "authorids": ["joey.tianyi.zhou@gmail.com", "isaac.changhau@gmail.com", "jindi15@mit.edu", "hongyuanzhu.cn@gmail.com", "gohsm@ihpc.a-star.edu.sg", "kenkwok@ihpc.a-star.edu.sg"], "authors": ["Joey Tianyi Zhou", "Hao Zhang", "Di Jin", "Hongyuan Zhu", "Rick Siow Mong Goh", "Kenneth Kwok"], "TL;DR": "We propose a new  architecture termed Dual Adversarial Transfer Network (DATNet) for addressing low-resource Named Entity Recognition (NER) and achieve new state-of-the-art performances on CoNLL and Twitter NER.", "pdf": "/pdf/490de072f0df42fa74520f68c323b0bd81419230.pdf", "paperhash": "zhou|datnet_dual_adversarial_transfer_for_lowresource_named_entity_recognition", "_bibtex": "@misc{\nzhou2019datnet,\ntitle={{DATN}et: Dual Adversarial Transfer for Low-resource Named Entity Recognition},\nauthor={Joey Tianyi Zhou and Hao Zhang and Di Jin and Hongyuan Zhu and Rick Siow Mong Goh and Kenneth Kwok},\nyear={2019},\nurl={https://openreview.net/forum?id=HkGzUjR5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper154/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617257, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkGzUjR5tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper154/Authors", "ICLR.cc/2019/Conference/Paper154/Reviewers", "ICLR.cc/2019/Conference/Paper154/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper154/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper154/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper154/Authors|ICLR.cc/2019/Conference/Paper154/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper154/Reviewers", "ICLR.cc/2019/Conference/Paper154/Authors", "ICLR.cc/2019/Conference/Paper154/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617257}}}, {"id": "SJl90tU-07", "original": null, "number": 2, "cdate": 1542707666064, "ddate": null, "tcdate": 1542707666064, "tmdate": 1542707666064, "tddate": null, "forum": "HkGzUjR5tQ", "replyto": "rJeJabBc27", "invitation": "ICLR.cc/2019/Conference/-/Paper154/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "\nComment 1:  \"I find that the description of related work, especially in the introduction, does not credit past contributions sufficiently. For one, large parallel corpora do exist for many languages, albeit some of them may not be sufficiently ample in named entities to facilitate cross-lingual NER. Yet, for the fortunate ones, such corpora do make for rather reasonable NER taggers via multi-source projection (cf. Enghoff et al., W-NUT 2018). Absent is the prominent work by Mayhew et al. (2017) in cross-lingual NER, as well as Pan et al. (2017) who engage with evaluation in 282 languages.\" \n\nResponse 1: Thanks for pointing out the missing references. We have updated the related work section as suggested.  \n\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\nComment 2:  \"This unfair account of related work would not trouble me as much if it weren't coupled with an experiment in \"low-resource\" NER that features---Spanish and Dutch as target languages. Firstly, these languages are rich in resources, after all, they featured in CoNLL 2003, for one. Secondly, they are closely related to English as the source language, and any simulated low-resource scenario that features both the injection of target-language data *and* a very closely related source language is simply *not* representative of any true low-resource scenario.\nThis experiment setup troubles me, especially in light of real and synthetic NER data available to test the setup for true low-resource languages: from silver data by Al-Rfou et al. (2015) or Pan et al. (2017), via Mayhew et al. (2017) or Cotterrell and Duh (2017) who test on 10-15 gold datasets, etc., real low-resource NER data that is multilingual can be found. Any paper that in 2018 claims to do low-resource NER and then simulates a setup with Dutch and Spanish is a poor scholarship in my submission, regardless of the clever model.\"\n\nResponse 2: Thanks for the suggestion. In the revision, we further conducted the experiment on a  large-scale cross-lingual named entity dataset in [Pan  et al.,2017] which contains 282 languages for evaluation.  We chose 9 languages (Galician, West Frisian,  Ukrainian,  Marathi,  Spanish,  Dutch,  Russian,  Hindi,  Arabic) in our experiments given their low resource-nature and different linguistic nature. We follow the same setting as [Cotterrell and Duh (2017)] to simulate high- and low- resource scenarios. Table 4 summarizes the results of our methods under different cross-lingual transfer settings as well as the comparison with Cotterrell and Duh (2017). From the results,  we can clearly observe the superiority of our proposed model over the state-of-the-art, which can demonstrate that our model can really transfer from high-resource NER data to low-resource. \n\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\nComment 3:  \u201cthe use of \"lingual\" as noun is rather off-putting, at least to me\u201d\n\nResponse 3: Thank you for this comment. In the revision, we replaced the term \"lingual\" with \"language\". \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper154/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper154/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper154/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DATNet: Dual Adversarial Transfer for Low-resource Named Entity Recognition", "abstract": "We propose a new architecture termed Dual Adversarial Transfer Network (DATNet) for addressing low-resource Named Entity Recognition (NER). Specifically, two variants of DATNet, i.e., DATNet-F and DATNet-P, are proposed to explore effective feature fusion between high and low resource. To address the noisy and imbalanced training data, we propose a novel Generalized Resource-Adversarial Discriminator (GRAD). Additionally, adversarial training is adopted to boost model generalization. We examine the effects of different components in DATNet across domains and languages and show that significant improvement can be obtained especially for low-resource data. Without augmenting any additional hand-crafted features, we achieve new state-of-the-art performances on CoNLL and Twitter NER---88.16% F1 for Spanish, 53.43% F1 for WNUT-2016, and 42.83% F1 for WNUT-2017.", "keywords": ["Low-resource", "Named Entity Recognition"], "authorids": ["joey.tianyi.zhou@gmail.com", "isaac.changhau@gmail.com", "jindi15@mit.edu", "hongyuanzhu.cn@gmail.com", "gohsm@ihpc.a-star.edu.sg", "kenkwok@ihpc.a-star.edu.sg"], "authors": ["Joey Tianyi Zhou", "Hao Zhang", "Di Jin", "Hongyuan Zhu", "Rick Siow Mong Goh", "Kenneth Kwok"], "TL;DR": "We propose a new  architecture termed Dual Adversarial Transfer Network (DATNet) for addressing low-resource Named Entity Recognition (NER) and achieve new state-of-the-art performances on CoNLL and Twitter NER.", "pdf": "/pdf/490de072f0df42fa74520f68c323b0bd81419230.pdf", "paperhash": "zhou|datnet_dual_adversarial_transfer_for_lowresource_named_entity_recognition", "_bibtex": "@misc{\nzhou2019datnet,\ntitle={{DATN}et: Dual Adversarial Transfer for Low-resource Named Entity Recognition},\nauthor={Joey Tianyi Zhou and Hao Zhang and Di Jin and Hongyuan Zhu and Rick Siow Mong Goh and Kenneth Kwok},\nyear={2019},\nurl={https://openreview.net/forum?id=HkGzUjR5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper154/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617257, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkGzUjR5tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper154/Authors", "ICLR.cc/2019/Conference/Paper154/Reviewers", "ICLR.cc/2019/Conference/Paper154/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper154/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper154/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper154/Authors|ICLR.cc/2019/Conference/Paper154/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper154/Reviewers", "ICLR.cc/2019/Conference/Paper154/Authors", "ICLR.cc/2019/Conference/Paper154/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617257}}}, {"id": "HylBY88Z0X", "original": null, "number": 1, "cdate": 1542706812933, "ddate": null, "tcdate": 1542706812933, "tmdate": 1542706812933, "tddate": null, "forum": "HkGzUjR5tQ", "replyto": "SklaLn3rTX", "invitation": "ICLR.cc/2019/Conference/-/Paper154/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Comment 1:  \"...Could the method, for instance, be applied to other labeling tasks: POS tagging, morphological features... \" \nResponse  1:  Yes, our proposed architecture is easy to be applied to any other sequence labeling task.  However, the scope of this paper is focused on the NER task, which is the most challenging and representative task among all the sequence labeling tasks. And we appreciate this suggestion and plan to extend this method to more related tasks in the future.\n-------------------------------------------------------------------------------------------------------------------------------------------------------------\nComment 2: \"In low resource scenarios often methods work that do stop working at some point with more resources. For this methods the boundary when this effect occurs would be interesting to explore; Figure 2 goes into this direction which is a quite nice study but the boundary is not explored further; by using, for instance, the English NER data. Additionally, the performance on the English dataset would indicate what the method could perform in comparison to current SOTA for normal resource setting - you could use some of the low resources in addition. The English dataset was used but only to exploit it for the transfer learning. \"\n\nResponse  2:  The target of this paper is to transfer the knowledge obtained from high-resource language to the low-resource language so that we can boost the performance of low-resource NER, which is exactly what transfer learning aims at. Trying to beat the state-of-the-art high-resource NER such as the English NER (even with the help of other low-resources) is not within the scope of this work. Indeed, in order to further validate the effectiveness of our model on transfer learning of low-resources NER, we experimented on 9 more low-resources languages and proved that our model outperforms the state-of-the-art across the in/cross-family and in/cross-branch scenarios. \n-------------------------------------------------------------------------------------------------------------------------------------------------------------\nComment 3: \"Table 2 is a good overview of SOTA. I really wonder about the variance of the results of the system which can be depending on the network quite large. Why not running repetition test, this would enable the authors to report variance and statistically significance between the baseline and their other systems.\"\n\nResponse  3:  Yes,  in the revision version,  we updated the Table 3 with the averaged F1-scores and the standard deviation over 10 repetitive runs on each task.  \n-------------------------------------------------------------------------------------------------------------------------------------------------------------\nComment 4:  \u201cI wonder also how more standard exploitation of additional data would do such as Bert, ELMO or older methods such as up-training - this would help to get a more complete picture and strength the paper further.\u201d\n\nResponse 4:  In this paper, we aim at exploring the effectiveness of our proposed model, which is orthogonal to those methods that utilize the additional external data. That is to say, it is easy to apply the methods exploiting the external unlabeled data such as ELMO to our system, which for sure can strength the performance. But the scope of this work is still to demonstrate that our proposed transfer learning architecture can better and more efficient transfer knowledge from high-resources to low-resources. Overall, we thank you for this good suggestion, and we leave the exploitation of additional data for future work.\n-------------------------------------------------------------------------------------------------------------------------------------------------------------\nComment 5: \u201cThe paper could be stronger by applying the method to other tasks too as stated the authors - this is a \u2018new architectures\u2019 (for NER) which triggers the question and does it generalize to other tasks? In conclusion, there is even the claim as a statement! that this can be generalized to other NLP task without actually trying. I think this can not be claimed in the conclusions without pursuing this in some other task and I suggest to tune this down.\u201d\n\nResponse 5: Thank you so much for mentioning this over-statement out. We have now removed the corresponding statement in the conclusion section. As aforementioned, we plan to generalize the proposed system to other NLP tasks to further validate its effectiveness on a more broad range of NLP problems in the future work."}, "signatures": ["ICLR.cc/2019/Conference/Paper154/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper154/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper154/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DATNet: Dual Adversarial Transfer for Low-resource Named Entity Recognition", "abstract": "We propose a new architecture termed Dual Adversarial Transfer Network (DATNet) for addressing low-resource Named Entity Recognition (NER). Specifically, two variants of DATNet, i.e., DATNet-F and DATNet-P, are proposed to explore effective feature fusion between high and low resource. To address the noisy and imbalanced training data, we propose a novel Generalized Resource-Adversarial Discriminator (GRAD). Additionally, adversarial training is adopted to boost model generalization. We examine the effects of different components in DATNet across domains and languages and show that significant improvement can be obtained especially for low-resource data. Without augmenting any additional hand-crafted features, we achieve new state-of-the-art performances on CoNLL and Twitter NER---88.16% F1 for Spanish, 53.43% F1 for WNUT-2016, and 42.83% F1 for WNUT-2017.", "keywords": ["Low-resource", "Named Entity Recognition"], "authorids": ["joey.tianyi.zhou@gmail.com", "isaac.changhau@gmail.com", "jindi15@mit.edu", "hongyuanzhu.cn@gmail.com", "gohsm@ihpc.a-star.edu.sg", "kenkwok@ihpc.a-star.edu.sg"], "authors": ["Joey Tianyi Zhou", "Hao Zhang", "Di Jin", "Hongyuan Zhu", "Rick Siow Mong Goh", "Kenneth Kwok"], "TL;DR": "We propose a new  architecture termed Dual Adversarial Transfer Network (DATNet) for addressing low-resource Named Entity Recognition (NER) and achieve new state-of-the-art performances on CoNLL and Twitter NER.", "pdf": "/pdf/490de072f0df42fa74520f68c323b0bd81419230.pdf", "paperhash": "zhou|datnet_dual_adversarial_transfer_for_lowresource_named_entity_recognition", "_bibtex": "@misc{\nzhou2019datnet,\ntitle={{DATN}et: Dual Adversarial Transfer for Low-resource Named Entity Recognition},\nauthor={Joey Tianyi Zhou and Hao Zhang and Di Jin and Hongyuan Zhu and Rick Siow Mong Goh and Kenneth Kwok},\nyear={2019},\nurl={https://openreview.net/forum?id=HkGzUjR5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper154/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617257, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkGzUjR5tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper154/Authors", "ICLR.cc/2019/Conference/Paper154/Reviewers", "ICLR.cc/2019/Conference/Paper154/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper154/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper154/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper154/Authors|ICLR.cc/2019/Conference/Paper154/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper154/Reviewers", "ICLR.cc/2019/Conference/Paper154/Authors", "ICLR.cc/2019/Conference/Paper154/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617257}}}, {"id": "SklaLn3rTX", "original": null, "number": 3, "cdate": 1541946452663, "ddate": null, "tcdate": 1541946452663, "tmdate": 1541946452663, "tddate": null, "forum": "HkGzUjR5tQ", "replyto": "HkGzUjR5tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper154/Official_Review", "content": {"title": "Nicely and clear written paper with innovative aspects. The results look strong but could increase confidence in results via reporting of variance / stat sig; Paper is a bit narrow as applied new method to one task only.", "review": "The authors propose a new architecture Dual Adversarial Transfer Network for addressing low-resource NER. They achieve a new SOTA on low resource language. The authors compare a base-line with two alternatives based on variants of GANs.  \n\nThe results go beyond SOTA for low resource NER which seems a solid contribution. The paper is well and clearly written and I would be able to replicate the experiments. I wonder if this is a new architecture that works well for one task or if it could be applied to other tasks too. This would strengthen the approach and paper quite a bit. Could the method for instance be applied to other labeling tasks: POS tagging, morphological features. This would increase the potential impact substantially.  \n  \nIn low resource scenarios often methods work that do stop working at a some point with more resources. For this methods the boundary when this effect occurs would be interesting to explore; Figure 2 goes into this direction which is a quite nice study but the boundary is not  explored further; by using for instance the English NER data. Additionally, the performance on the English data set would indicate what the method could perform in comparison to current SOTA for normal resource setting - you could use some of the low resources in addition. The English data set was used but only to exploit it for the transfer learning. \nTable 2 is a good overview on SOTA. I really wonder about the variance of the results of the system which can be depending on the network quite large. Why not running repetition test, this  would enable the authors to report variance and statistically significance between the baseline and their other systems. \nI wonder also how more standard exploitation of additional data would do such as Bert, ELMO or older methods such as up-training - this would help to get a more complete picture and strength the paper further. \n\nThe paper could be stronger by applying the method to other task too as stated the authors - this is a \u2018new architectures\u2019 (for NER) which triggers the question and does it generalize to other tasks? In the conclusion there is even the claim as a statement! that this can be generalized to other NLP task without actually trying. I think, this can not be claimed in the conclusions without pursuing this in some other task and I suggest to tune this down.   \n\nOverall:\nNicely and clear written paper containing innovative elements. The results look strong too me but due to the lack of variance and stat sig., I am not sure if they are really super strong. The paper could be stronger by applying the method to other task too as stated \u2018new architectures\u2019 which triggers the question if the method generalizes to other tasks?   \n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper154/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DATNet: Dual Adversarial Transfer for Low-resource Named Entity Recognition", "abstract": "We propose a new architecture termed Dual Adversarial Transfer Network (DATNet) for addressing low-resource Named Entity Recognition (NER). Specifically, two variants of DATNet, i.e., DATNet-F and DATNet-P, are proposed to explore effective feature fusion between high and low resource. To address the noisy and imbalanced training data, we propose a novel Generalized Resource-Adversarial Discriminator (GRAD). Additionally, adversarial training is adopted to boost model generalization. We examine the effects of different components in DATNet across domains and languages and show that significant improvement can be obtained especially for low-resource data. Without augmenting any additional hand-crafted features, we achieve new state-of-the-art performances on CoNLL and Twitter NER---88.16% F1 for Spanish, 53.43% F1 for WNUT-2016, and 42.83% F1 for WNUT-2017.", "keywords": ["Low-resource", "Named Entity Recognition"], "authorids": ["joey.tianyi.zhou@gmail.com", "isaac.changhau@gmail.com", "jindi15@mit.edu", "hongyuanzhu.cn@gmail.com", "gohsm@ihpc.a-star.edu.sg", "kenkwok@ihpc.a-star.edu.sg"], "authors": ["Joey Tianyi Zhou", "Hao Zhang", "Di Jin", "Hongyuan Zhu", "Rick Siow Mong Goh", "Kenneth Kwok"], "TL;DR": "We propose a new  architecture termed Dual Adversarial Transfer Network (DATNet) for addressing low-resource Named Entity Recognition (NER) and achieve new state-of-the-art performances on CoNLL and Twitter NER.", "pdf": "/pdf/490de072f0df42fa74520f68c323b0bd81419230.pdf", "paperhash": "zhou|datnet_dual_adversarial_transfer_for_lowresource_named_entity_recognition", "_bibtex": "@misc{\nzhou2019datnet,\ntitle={{DATN}et: Dual Adversarial Transfer for Low-resource Named Entity Recognition},\nauthor={Joey Tianyi Zhou and Hao Zhang and Di Jin and Hongyuan Zhu and Rick Siow Mong Goh and Kenneth Kwok},\nyear={2019},\nurl={https://openreview.net/forum?id=HkGzUjR5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper154/Official_Review", "cdate": 1542234526167, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkGzUjR5tQ", "replyto": "HkGzUjR5tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper154/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335660878, "tmdate": 1552335660878, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper154/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rklsdRj5h7", "original": null, "number": 2, "cdate": 1541222003403, "ddate": null, "tcdate": 1541222003403, "tmdate": 1541534237836, "tddate": null, "forum": "HkGzUjR5tQ", "replyto": "HkGzUjR5tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper154/Official_Review", "content": {"title": "Interesting approach for cross-domain / cross-lingual NER with solid empirical results; limited technical novelty", "review": "<Summary>\nAuthors propose the new \u201cDATNet\u201d for the NER task, which extends the base neural model for NER (Bi-LSTM+CRF sequence model with input represented with CharCNN-word embeddings) with the following two main components: (1) GRAD: a language (or resource) adversarial discriminator with adaptive weights that regularize source-target data imbalance, and (2) additional adversarial training approaches that perturb input samples in the embeddings space.\n\nThe paper reports big improvement over their baseline approaches without having to rely on other auxiliary or hand-crafted features. The experiment is performed for various low-resource scenarios (varying training data size). \n\n<Comments>\n- While the idea of applying \u201cdual adversarial\u201d approaches is new in the context of NER, the technical novelty of each component is limited. GRAD, for example, is rather a minor modification of Language Adversarial Discriminator (Kim et al., 2017) with a scalar weight parameter on loss. The empirical superiority of the proposed method (GRAD) over normal AD approaches cannot be claimed either -- for this authors need to report quantitative results of the (Base + AT + F/P-transfer with AD -- or e.g. \\alpha fixed at 0.5 or at some other rate), which I believe is missing in all figures and tables. Visualization of resulting feature distribution (Figure 3) is interesting to look at, but that alone does not suffice. There is no technical novelty in applying adversarial training either, except that it was used in the context of NER.\n\n\n- Authors use both source and target data to train their \u201cbase model\u201d as well (\u201c... we exploit all the source data and target data ...\u201c), e.g. presumably by merging the source and target dataset as well as their embedding matrices, etc. It is important that authors report if there ever is a negative transfer case (e.g. a base model trained with just target data may outperform models trained with source+target data) at varying resource scenarios -- especially at sufficiently-resourced cases.\nIf by any chance their \u201cbase model\u201d refers to in-domain training / in-domain testing results on target, the aforementioned baseline (naive merge of source and target data) is obviously an important baseline to report. I suggest that authors provide these details or clarify. (The confusion comes mainly because some of the SOTA results the authors quote are in-domain training / in-domain evaluation results on respective languages, and some are cross-domain results -- yet they are all under \u201ccross-lingual/domain\u201d columns in Table 2).\n\n- It would be interesting to report the learned optimal \\alpha value for each different setting (at varying r or training size) to see if the authors\u2019 intuition is met. On a related note, from the manuscript alone, it is not entirely clear if \\alpha is a learnable parameter or a tunable hyper parameter -- by context I believe it is a model parameter. If they are tuned, authors need to report these values.\n\n<Nit> \n- Section 3.2.3, \u201c... (GRAD) to enable adaptive weights for [each sample]\u201d \u2192 I think it reads better with [each resource] or [each source and target], unless you meant \\alpha_i for each sentence.\n- Section 3.2.5, \u201c... recently, adversarial samples are [wisely] incorporated\u201d \u2192 [widely]\n- Fonts for figures could be bigger. ", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper154/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DATNet: Dual Adversarial Transfer for Low-resource Named Entity Recognition", "abstract": "We propose a new architecture termed Dual Adversarial Transfer Network (DATNet) for addressing low-resource Named Entity Recognition (NER). Specifically, two variants of DATNet, i.e., DATNet-F and DATNet-P, are proposed to explore effective feature fusion between high and low resource. To address the noisy and imbalanced training data, we propose a novel Generalized Resource-Adversarial Discriminator (GRAD). Additionally, adversarial training is adopted to boost model generalization. We examine the effects of different components in DATNet across domains and languages and show that significant improvement can be obtained especially for low-resource data. Without augmenting any additional hand-crafted features, we achieve new state-of-the-art performances on CoNLL and Twitter NER---88.16% F1 for Spanish, 53.43% F1 for WNUT-2016, and 42.83% F1 for WNUT-2017.", "keywords": ["Low-resource", "Named Entity Recognition"], "authorids": ["joey.tianyi.zhou@gmail.com", "isaac.changhau@gmail.com", "jindi15@mit.edu", "hongyuanzhu.cn@gmail.com", "gohsm@ihpc.a-star.edu.sg", "kenkwok@ihpc.a-star.edu.sg"], "authors": ["Joey Tianyi Zhou", "Hao Zhang", "Di Jin", "Hongyuan Zhu", "Rick Siow Mong Goh", "Kenneth Kwok"], "TL;DR": "We propose a new  architecture termed Dual Adversarial Transfer Network (DATNet) for addressing low-resource Named Entity Recognition (NER) and achieve new state-of-the-art performances on CoNLL and Twitter NER.", "pdf": "/pdf/490de072f0df42fa74520f68c323b0bd81419230.pdf", "paperhash": "zhou|datnet_dual_adversarial_transfer_for_lowresource_named_entity_recognition", "_bibtex": "@misc{\nzhou2019datnet,\ntitle={{DATN}et: Dual Adversarial Transfer for Low-resource Named Entity Recognition},\nauthor={Joey Tianyi Zhou and Hao Zhang and Di Jin and Hongyuan Zhu and Rick Siow Mong Goh and Kenneth Kwok},\nyear={2019},\nurl={https://openreview.net/forum?id=HkGzUjR5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper154/Official_Review", "cdate": 1542234526167, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkGzUjR5tQ", "replyto": "HkGzUjR5tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper154/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335660878, "tmdate": 1552335660878, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper154/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJeJabBc27", "original": null, "number": 1, "cdate": 1541194167435, "ddate": null, "tcdate": 1541194167435, "tmdate": 1541534237585, "tddate": null, "forum": "HkGzUjR5tQ", "replyto": "HkGzUjR5tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper154/Official_Review", "content": {"title": "Interesting model, thorough exposition and analysis, but partly flawed experiment setup", "review": "The paper introduces a novel architecture for low-resource named entity tagging: a dual adversarial transfer network, in which fusion between high- and low-resource, or high- and low-noise data is achieved via also novel resource-adversarial discriminator.\n\nThe model is interesting, novel, clearly exposed in sufficient detail, and warrants publication as such. The idea to unify representation differences and data imbalance under one model is noteworthy.\n\nI find that the description of related work, especially in the introduction, does not credit past contributions sufficiently. For one, large parallel corpora do exist for many languages, albeit some of them may not be sufficiently ample in named entities to facilitate cross-lingual NER. Yet, for the fortunate ones, such corpora do make for rather reasonable NER taggers via multi-source projection (cf. Enghoff et al., W-NUT 2018). Absent is the prominent work by Mayhew et al. (2017) in cross-lingual NER, as well as Pan et al. (2017) who engage with evaluation in 282 languages.\n\nThis unfair account of related work would not trouble me as much if it weren't coupled with an experiment in \"low-resource\" NER that features---Spanish and Dutch as target languages. Firstly, these languages are rich in resources, after all, they featured in CoNLL 2003, for one. Secondly, they are closely related to English as the source language, and any simulated low-resource scenario that features both the injection of target-language data *and* a very closely related source language is simply *not* representative of any true low-resource scenario.\n\nThis experiment setup troubles me, especially in light of real and synthetic NER data available to test the setup for true low-resource languages: from silver data by Al-Rfou et al. (2015) or Pan et al. (2017), via Mayhew et al. (2017) or Cotterrell and Duh (2017) who test on 10-15 gold datasets, etc., real low-resource NER data that is multilingual can be found. Any paper that in 2018 claims to do low-resource NER and then simulates a setup with Dutch and Spanish is poor scholarship in my submission, regardless of the clever model.\n\nI do let the clever model upvote my review, but not beyond borderline.\n\nMinor:\n- the use of \"lingual\" as noun is rather off-putting, at least to me", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper154/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DATNet: Dual Adversarial Transfer for Low-resource Named Entity Recognition", "abstract": "We propose a new architecture termed Dual Adversarial Transfer Network (DATNet) for addressing low-resource Named Entity Recognition (NER). Specifically, two variants of DATNet, i.e., DATNet-F and DATNet-P, are proposed to explore effective feature fusion between high and low resource. To address the noisy and imbalanced training data, we propose a novel Generalized Resource-Adversarial Discriminator (GRAD). Additionally, adversarial training is adopted to boost model generalization. We examine the effects of different components in DATNet across domains and languages and show that significant improvement can be obtained especially for low-resource data. Without augmenting any additional hand-crafted features, we achieve new state-of-the-art performances on CoNLL and Twitter NER---88.16% F1 for Spanish, 53.43% F1 for WNUT-2016, and 42.83% F1 for WNUT-2017.", "keywords": ["Low-resource", "Named Entity Recognition"], "authorids": ["joey.tianyi.zhou@gmail.com", "isaac.changhau@gmail.com", "jindi15@mit.edu", "hongyuanzhu.cn@gmail.com", "gohsm@ihpc.a-star.edu.sg", "kenkwok@ihpc.a-star.edu.sg"], "authors": ["Joey Tianyi Zhou", "Hao Zhang", "Di Jin", "Hongyuan Zhu", "Rick Siow Mong Goh", "Kenneth Kwok"], "TL;DR": "We propose a new  architecture termed Dual Adversarial Transfer Network (DATNet) for addressing low-resource Named Entity Recognition (NER) and achieve new state-of-the-art performances on CoNLL and Twitter NER.", "pdf": "/pdf/490de072f0df42fa74520f68c323b0bd81419230.pdf", "paperhash": "zhou|datnet_dual_adversarial_transfer_for_lowresource_named_entity_recognition", "_bibtex": "@misc{\nzhou2019datnet,\ntitle={{DATN}et: Dual Adversarial Transfer for Low-resource Named Entity Recognition},\nauthor={Joey Tianyi Zhou and Hao Zhang and Di Jin and Hongyuan Zhu and Rick Siow Mong Goh and Kenneth Kwok},\nyear={2019},\nurl={https://openreview.net/forum?id=HkGzUjR5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper154/Official_Review", "cdate": 1542234526167, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkGzUjR5tQ", "replyto": "HkGzUjR5tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper154/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335660878, "tmdate": 1552335660878, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper154/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}