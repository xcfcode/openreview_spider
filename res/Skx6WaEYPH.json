{"notes": [{"id": "Skx6WaEYPH", "original": "Skxuk-FLDH", "number": 393, "cdate": 1569438981301, "ddate": null, "tcdate": 1569438981301, "tmdate": 1577168286410, "tddate": null, "forum": "Skx6WaEYPH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Bandlimiting Neural Networks Against Adversarial Attacks", "authors": ["Yuping Lin", "Kasra Ahmadi K. A.", "Hui Jiang"], "authorids": ["yuping@eecs.yorku.ca", "kasraah@eecs.yorku.ca", "hj@cse.yorku.ca"], "keywords": ["adversarial examples", "adversarial attack defense", "neural network", "Fourier analysis"], "TL;DR": "An insight into the reason of adversarial vulnerability, an effective defense method against adversarial attacks.", "abstract": "In this paper, we study the adversarial attack and defence problem in deep learning from the perspective of Fourier analysis. We first explicitly compute the Fourier transform of deep ReLU neural networks and show that there exist decaying but non-zero high frequency components in the Fourier spectrum of neural networks. We then demonstrate that the vulnerability of neural networks towards adversarial samples can be attributed to these insignificant but non-zero high frequency components. Based on this analysis, we propose to use a simple post-averaging technique to smooth out these high frequency components to improve the robustness of neural networks against adversarial attacks. Experimental results on the ImageNet and the CIFAR-10 datasets have shown that our proposed method is universally effective to defend many existing adversarial attacking methods proposed in the literature, including FGSM, PGD, DeepFool and C&W attacks. Our post-averaging method is simple since it does not require any re-training, and meanwhile it can successfully defend over 80-96% of the adversarial samples generated by these methods without introducing significant performance degradation (less than 2%) on the original clean images.", "pdf": "/pdf/0b105ef70558ff48d04952195ddaca263fec6c59.pdf", "paperhash": "lin|bandlimiting_neural_networks_against_adversarial_attacks", "original_pdf": "/attachment/0b105ef70558ff48d04952195ddaca263fec6c59.pdf", "_bibtex": "@misc{\nlin2020bandlimiting,\ntitle={Bandlimiting Neural Networks Against Adversarial Attacks},\nauthor={Yuping Lin and Kasra Ahmadi K. A. and Hui Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=Skx6WaEYPH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "HxS2JDJ94a", "original": null, "number": 1, "cdate": 1576798695140, "ddate": null, "tcdate": 1576798695140, "tmdate": 1576800940441, "tddate": null, "forum": "Skx6WaEYPH", "replyto": "Skx6WaEYPH", "invitation": "ICLR.cc/2020/Conference/Paper393/-/Decision", "content": {"decision": "Reject", "comment": "The reviewers recommend rejection due to various concerns about novelty and experimental validation. The authors have not provided a response.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bandlimiting Neural Networks Against Adversarial Attacks", "authors": ["Yuping Lin", "Kasra Ahmadi K. A.", "Hui Jiang"], "authorids": ["yuping@eecs.yorku.ca", "kasraah@eecs.yorku.ca", "hj@cse.yorku.ca"], "keywords": ["adversarial examples", "adversarial attack defense", "neural network", "Fourier analysis"], "TL;DR": "An insight into the reason of adversarial vulnerability, an effective defense method against adversarial attacks.", "abstract": "In this paper, we study the adversarial attack and defence problem in deep learning from the perspective of Fourier analysis. We first explicitly compute the Fourier transform of deep ReLU neural networks and show that there exist decaying but non-zero high frequency components in the Fourier spectrum of neural networks. We then demonstrate that the vulnerability of neural networks towards adversarial samples can be attributed to these insignificant but non-zero high frequency components. Based on this analysis, we propose to use a simple post-averaging technique to smooth out these high frequency components to improve the robustness of neural networks against adversarial attacks. Experimental results on the ImageNet and the CIFAR-10 datasets have shown that our proposed method is universally effective to defend many existing adversarial attacking methods proposed in the literature, including FGSM, PGD, DeepFool and C&W attacks. Our post-averaging method is simple since it does not require any re-training, and meanwhile it can successfully defend over 80-96% of the adversarial samples generated by these methods without introducing significant performance degradation (less than 2%) on the original clean images.", "pdf": "/pdf/0b105ef70558ff48d04952195ddaca263fec6c59.pdf", "paperhash": "lin|bandlimiting_neural_networks_against_adversarial_attacks", "original_pdf": "/attachment/0b105ef70558ff48d04952195ddaca263fec6c59.pdf", "_bibtex": "@misc{\nlin2020bandlimiting,\ntitle={Bandlimiting Neural Networks Against Adversarial Attacks},\nauthor={Yuping Lin and Kasra Ahmadi K. A. and Hui Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=Skx6WaEYPH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Skx6WaEYPH", "replyto": "Skx6WaEYPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795729625, "tmdate": 1576800282251, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper393/-/Decision"}}}, {"id": "rJxkmnwOKS", "original": null, "number": 1, "cdate": 1571482647370, "ddate": null, "tcdate": 1571482647370, "tmdate": 1572972600975, "tddate": null, "forum": "Skx6WaEYPH", "replyto": "Skx6WaEYPH", "invitation": "ICLR.cc/2020/Conference/Paper393/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper makes three contributions: 1) analyzing the number of linear regions and the Fourier spectrum of fully-connected neural networks with ReLU activations; 2) proposing an ensemble method as a defense against adversarial evasion attacks; 3) analyzing the performance of that defense against state-of-the-art attacks on CIFAR-10 and ImageNet data.\n\nMy main concern is regarding the originality and significance of this work. Most of the results regarding the piece-wise linear geometry of fully-connected neural networks with ReLU activations are known. The authors should reference and discuss the relation of their work in particular to [1]. The Fourier transformation results look straightforward to me. I am not convinced either that they are indeed \"essential\", as the authors write, for analyzing the nature of adversarial samples. In fact, I find the discussion in Section 2.2 rather vague; for instance, what is the formal notion of \"tiny unlearned regions\" of neural networks? The authors mention an experiment in which they observed that an adversarial perturbation on ImageNet led to the crossing of N=5278 hyperplanes \"per layer\", which sounds like an interesting finding, however, more detail is needed to understand how it was obtained.\n\nThe proposed defense is not very original either; the authors should discuss it in comparison e.g. to [2] and [3] who also considered ensemble-based methods to defend against adversarial perturbations. The experimental results in Table 1 & 2 look promising at a first glance, however, [4] has demonstrated that ensemble-based, randomized defenses can be fairly easily defeated (e.g. using expectations over transformations when computing the gradients for adversarial sample generation). The authors need to report the performance of their defense under such white-box settings.\n\nThe strong performance of the FGSM attack in Figure 3 is surprising. PGD is strictly stronger than FGSM, so its defense rate shouldn't be higher than the one for FGSM. I disagree with the authors' explanation that FGSM \"generates much larger perturbations\"; if the epsilon parameter is set properly in the PGD attack, it should exhaust that perturbation budget (assuming that the step size and number of iterations are large enough). On the other hand, I don't understand how the authors were able to constrain the L-inf norm of the C&W and DeepFool adversarial samples (since those attacks aim to minimize L2 norms).\n\n\n[1] Montufar et al., On the Number of Linear Regions of Deep Neural Networks. NeurIPS, 2014.\n[2] Liu et al., Towards robust neural networks via random self-ensemble. ECCV, 2018.\n[3] Cao & Gong, Mitigating evasion attacks to deep neural networks via region-based classification. ACSAC, 2017.\n[4] Athalye et al., Obfuscated gradients give a false sense of security. ICML, 2018.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper393/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper393/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bandlimiting Neural Networks Against Adversarial Attacks", "authors": ["Yuping Lin", "Kasra Ahmadi K. A.", "Hui Jiang"], "authorids": ["yuping@eecs.yorku.ca", "kasraah@eecs.yorku.ca", "hj@cse.yorku.ca"], "keywords": ["adversarial examples", "adversarial attack defense", "neural network", "Fourier analysis"], "TL;DR": "An insight into the reason of adversarial vulnerability, an effective defense method against adversarial attacks.", "abstract": "In this paper, we study the adversarial attack and defence problem in deep learning from the perspective of Fourier analysis. We first explicitly compute the Fourier transform of deep ReLU neural networks and show that there exist decaying but non-zero high frequency components in the Fourier spectrum of neural networks. We then demonstrate that the vulnerability of neural networks towards adversarial samples can be attributed to these insignificant but non-zero high frequency components. Based on this analysis, we propose to use a simple post-averaging technique to smooth out these high frequency components to improve the robustness of neural networks against adversarial attacks. Experimental results on the ImageNet and the CIFAR-10 datasets have shown that our proposed method is universally effective to defend many existing adversarial attacking methods proposed in the literature, including FGSM, PGD, DeepFool and C&W attacks. Our post-averaging method is simple since it does not require any re-training, and meanwhile it can successfully defend over 80-96% of the adversarial samples generated by these methods without introducing significant performance degradation (less than 2%) on the original clean images.", "pdf": "/pdf/0b105ef70558ff48d04952195ddaca263fec6c59.pdf", "paperhash": "lin|bandlimiting_neural_networks_against_adversarial_attacks", "original_pdf": "/attachment/0b105ef70558ff48d04952195ddaca263fec6c59.pdf", "_bibtex": "@misc{\nlin2020bandlimiting,\ntitle={Bandlimiting Neural Networks Against Adversarial Attacks},\nauthor={Yuping Lin and Kasra Ahmadi K. A. and Hui Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=Skx6WaEYPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Skx6WaEYPH", "replyto": "Skx6WaEYPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper393/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper393/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575752586758, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper393/Reviewers"], "noninvitees": [], "tcdate": 1570237752803, "tmdate": 1575752586770, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper393/-/Official_Review"}}}, {"id": "SyxSBsG15r", "original": null, "number": 2, "cdate": 1571920701417, "ddate": null, "tcdate": 1571920701417, "tmdate": 1572972600942, "tddate": null, "forum": "Skx6WaEYPH", "replyto": "Skx6WaEYPH", "invitation": "ICLR.cc/2020/Conference/Paper393/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Overview:\nThe paper is dedicated to studying adversarial attack and defense problems from the perspective of Fourier analysis. They demonstrate that the adversarial vulnerability of neural networks can be attributed to non-zero high-frequency components. Then, the author proposes a simple post average approach to smooth out the insignificant high-frequency components, which can improve the adversarial robustness of neural networks. They conduct extensive experiments on ImageNet and CIFAR-10 to defend existing attacks, including FGSM, PGD, DeepFool, and C&W attacks.\n\nStrength Bullets:\n1. The logic chain of this paper is complete. The author first gives an interesting observation that the non-zero high-frequency components may cause the vulnerability of neural networks. Then they propose a post average technique to reduce the high-frequency components and improve robustness.\n2. The paper is well organized. Some figures, like fig1, are intuitive and interesting.\n\nWeakness Bullets:\n1. How can we get formula (4)? The author needs to provide more explanation.\n2. It is confused about how to pick directional vectors v for input x. Does the author treat the input RGB image (i.e. 3 times 32 times 32) as a one-dimensional vector? Then the author chooses k other vectors in the neighborhood. There need more details and rational explanations. The author can use the visualization method (i.e. tSNE) the illustrate the relationship among samples.\n3. For different sampling strategies, the author needs to provide an ablation study.\n4. The most arguable point is the experiment setting. I think it is a weak white-box experiment setting. The author uses the attacked set which is generated from the undefended model. It means the author doesn't attack his post-processing method. Thus, the improved robustness can be the result of gradient masking introduced by the author's techniques. BTW, it is totally traceable to attack the paper's approach. We can formula equation (7) and equation (8) as two layers in the front and at the end of networks. Then you can calculate the gradient for a complete model setting. The author needs to provides more convincing results.\n5. The author needs to compare with other post- or pre- processing methods, even the normal adversarial training. Also, the paper should contain detailed settings about attacks. For example, how many iterations do you run for PGD attacks? This is an important factor for trustworthy results.\n6. [Minor] Quotation mask mistake in the second row of page 2.\n\nRecommendation:\nDue to the limits of experiments design and setting, this is a weak reject."}, "signatures": ["ICLR.cc/2020/Conference/Paper393/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper393/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bandlimiting Neural Networks Against Adversarial Attacks", "authors": ["Yuping Lin", "Kasra Ahmadi K. A.", "Hui Jiang"], "authorids": ["yuping@eecs.yorku.ca", "kasraah@eecs.yorku.ca", "hj@cse.yorku.ca"], "keywords": ["adversarial examples", "adversarial attack defense", "neural network", "Fourier analysis"], "TL;DR": "An insight into the reason of adversarial vulnerability, an effective defense method against adversarial attacks.", "abstract": "In this paper, we study the adversarial attack and defence problem in deep learning from the perspective of Fourier analysis. We first explicitly compute the Fourier transform of deep ReLU neural networks and show that there exist decaying but non-zero high frequency components in the Fourier spectrum of neural networks. We then demonstrate that the vulnerability of neural networks towards adversarial samples can be attributed to these insignificant but non-zero high frequency components. Based on this analysis, we propose to use a simple post-averaging technique to smooth out these high frequency components to improve the robustness of neural networks against adversarial attacks. Experimental results on the ImageNet and the CIFAR-10 datasets have shown that our proposed method is universally effective to defend many existing adversarial attacking methods proposed in the literature, including FGSM, PGD, DeepFool and C&W attacks. Our post-averaging method is simple since it does not require any re-training, and meanwhile it can successfully defend over 80-96% of the adversarial samples generated by these methods without introducing significant performance degradation (less than 2%) on the original clean images.", "pdf": "/pdf/0b105ef70558ff48d04952195ddaca263fec6c59.pdf", "paperhash": "lin|bandlimiting_neural_networks_against_adversarial_attacks", "original_pdf": "/attachment/0b105ef70558ff48d04952195ddaca263fec6c59.pdf", "_bibtex": "@misc{\nlin2020bandlimiting,\ntitle={Bandlimiting Neural Networks Against Adversarial Attacks},\nauthor={Yuping Lin and Kasra Ahmadi K. A. and Hui Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=Skx6WaEYPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Skx6WaEYPH", "replyto": "Skx6WaEYPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper393/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper393/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575752586758, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper393/Reviewers"], "noninvitees": [], "tcdate": 1570237752803, "tmdate": 1575752586770, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper393/-/Official_Review"}}}, {"id": "Syg-EVrJ5r", "original": null, "number": 3, "cdate": 1571931176713, "ddate": null, "tcdate": 1571931176713, "tmdate": 1572972600895, "tddate": null, "forum": "Skx6WaEYPH", "replyto": "Skx6WaEYPH", "invitation": "ICLR.cc/2020/Conference/Paper393/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes an approach for improving robustness of already trained artificial neural networks with relu activation functions. The main motivation comes from signal processing where robustness is typically obtained via averaging moduli of Fourier coefficients over some frequency band (e.g., mel-frequency coefficients and deep scattering spectrum are based on this principle). The strategy amounts to sampling several random direction vectors in a ball of constant radius centered at a training example and averaging their predictions. The empirical estimate of the expected predictor value over the ball centered at a training example is used as its hypothesis value.\n\nThe approach is introduced by first expressing an artificial neural network with relu activations as a piecewise linear function (Definition 2.1, Lemma 2.2, Theorem 2.3). The paper then makes an observation that the instance space can be further sub-divided such that the network can be written as a sum of piecewise functions defined over regions given by positive linear combinations of linearly independent vectors (Definition 2.4, Theorem 2.5). A linear transformation of the instance space then allows for writing that function in canonical basis and computing its Fourier transform (Lemma 2.6, Theorem 2.7). The paper then makes an observation that the inverse of the linear transform used for making a change of basis (mapping to the canonical basis) can introduce instabilities to piecewise linear components defined over small regions (the inverse matrix appears in the Fourier transform). To address this instability, the paper relies on prior work by Jiang et al. (1999, 2003) and assigns the expected value over some ball centered at a training example as its prediction (which should be equivalent to performing averaging over bands in the Fourier domain). The integral is analytically intractable and, thus, the authors do an empirical estimate by averaging values in different random directions around the example. The experiments show that the method does not exhibit any serious instability with respect to the number points selected in that way.\n\nThe strategy does not require any additional training of the network and can be easily applied to already trained models with relu activations.\n\nIn the experiments, the approach is evaluated on standard adversarial attacking mechanisms: fast gradient sign method (Goodfellow et al., 2014), projected gradient descent method (Kurakin et al., 2016; Madry et al., 2017), deep fool attack method (Moosavi-Dezfooli et al., 2016) and L_2 method (Carlini & Wagner, 2017). I am not very familiar with the related work but this seems to be sufficient number of baselines to assess the effectiveness of the approach. The experiments are performed on ImageNet and Cifar10 datasets and show that the approach can make standard ResNet models more robust to the listed attacking strategies. It might be useful to test the approach with several different architectures (e.g., multi-layer perceptrons with different number of hidden layers, mix of convolutional and fully connected blocks etc).\n\nIn summary, the paper is well written and easy follow. The idea itself is simple but the intuition behind it is rather interesting and (to the best of my knowledge) provides novel insights into the workings of artificial neural networks with relu activations."}, "signatures": ["ICLR.cc/2020/Conference/Paper393/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper393/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bandlimiting Neural Networks Against Adversarial Attacks", "authors": ["Yuping Lin", "Kasra Ahmadi K. A.", "Hui Jiang"], "authorids": ["yuping@eecs.yorku.ca", "kasraah@eecs.yorku.ca", "hj@cse.yorku.ca"], "keywords": ["adversarial examples", "adversarial attack defense", "neural network", "Fourier analysis"], "TL;DR": "An insight into the reason of adversarial vulnerability, an effective defense method against adversarial attacks.", "abstract": "In this paper, we study the adversarial attack and defence problem in deep learning from the perspective of Fourier analysis. We first explicitly compute the Fourier transform of deep ReLU neural networks and show that there exist decaying but non-zero high frequency components in the Fourier spectrum of neural networks. We then demonstrate that the vulnerability of neural networks towards adversarial samples can be attributed to these insignificant but non-zero high frequency components. Based on this analysis, we propose to use a simple post-averaging technique to smooth out these high frequency components to improve the robustness of neural networks against adversarial attacks. Experimental results on the ImageNet and the CIFAR-10 datasets have shown that our proposed method is universally effective to defend many existing adversarial attacking methods proposed in the literature, including FGSM, PGD, DeepFool and C&W attacks. Our post-averaging method is simple since it does not require any re-training, and meanwhile it can successfully defend over 80-96% of the adversarial samples generated by these methods without introducing significant performance degradation (less than 2%) on the original clean images.", "pdf": "/pdf/0b105ef70558ff48d04952195ddaca263fec6c59.pdf", "paperhash": "lin|bandlimiting_neural_networks_against_adversarial_attacks", "original_pdf": "/attachment/0b105ef70558ff48d04952195ddaca263fec6c59.pdf", "_bibtex": "@misc{\nlin2020bandlimiting,\ntitle={Bandlimiting Neural Networks Against Adversarial Attacks},\nauthor={Yuping Lin and Kasra Ahmadi K. A. and Hui Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=Skx6WaEYPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Skx6WaEYPH", "replyto": "Skx6WaEYPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper393/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper393/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575752586758, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper393/Reviewers"], "noninvitees": [], "tcdate": 1570237752803, "tmdate": 1575752586770, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper393/-/Official_Review"}}}], "count": 5}