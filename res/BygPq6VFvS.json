{"notes": [{"id": "BygPq6VFvS", "original": "HylWoyRDPS", "number": 711, "cdate": 1569439119444, "ddate": null, "tcdate": 1569439119444, "tmdate": 1577168246764, "tddate": null, "forum": "BygPq6VFvS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["nxphi47@gmail.com", "sjoty@salesforce.com", "ng0155ng@e.ntu.edu.sg"], "title": "Enhancing Attention with Explicit Phrasal Alignments", "authors": ["Xuan-Phi Nguyen", "Shafiq Joty", "Thanh-Tung Nguyen"], "pdf": "/pdf/846366884ccb6624de9f8ff9db0534f56f468548.pdf", "abstract": "The attention mechanism is an indispensable component of any state-of-the-art neural machine translation system. However, existing attention methods are often token-based and ignore the importance of phrasal alignments, which are the backbone of phrase-based statistical machine translation. We propose a novel phrase-based attention method to model n-grams of tokens as the basic attention entities, and design multi-headed phrasal attentions within the Transformer architecture to perform token-to-token and token-to-phrase mappings. Our approach yields improvements in English-German, English-Russian and English-French translation tasks on the standard WMT'14 test set. Furthermore, our phrasal attention method shows improvements on the one-billion-word language modeling benchmark.\n", "keywords": ["NMT", "Phrasal Attention", "Machine Translation", "Language Modeling"], "paperhash": "nguyen|enhancing_attention_with_explicit_phrasal_alignments", "original_pdf": "/attachment/846366884ccb6624de9f8ff9db0534f56f468548.pdf", "_bibtex": "@misc{\nnguyen2020enhancing,\ntitle={Enhancing Attention with Explicit Phrasal Alignments},\nauthor={Xuan-Phi Nguyen and Shafiq Joty and Thanh-Tung Nguyen},\nyear={2020},\nurl={https://openreview.net/forum?id=BygPq6VFvS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "ZCcoumT7i_", "original": null, "number": 1, "cdate": 1576798703957, "ddate": null, "tcdate": 1576798703957, "tmdate": 1576800932101, "tddate": null, "forum": "BygPq6VFvS", "replyto": "BygPq6VFvS", "invitation": "ICLR.cc/2020/Conference/Paper711/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a phrase-based attention method to model word n-grams (as opposed to single words) as the basic attention units. Multi-headed phrasal attentions are designed within the Transformer architecture to perform token-to-token and token-to-phrase mappings. Some improvements are shown in English-German, English-Russian and English-French translation tasks on the standard WMT'14 test set, and on the one-billion-word language modeling benchmark.\n\nWhile the proposed approach is interesting and takes inspiration in the notion of phrases used in phrase-based machine translation, with some positive empirical results, the technical novelty of this paper is rather limited, and the experiments could be more solid. While it is understandable that lack of computational resources made it hard to experiment with larger models (e.g. Transformer-big), perhaps it would be interesting to try on language pairs with fewer resources (smaller datasets), where base models are more competitive.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["nxphi47@gmail.com", "sjoty@salesforce.com", "ng0155ng@e.ntu.edu.sg"], "title": "Enhancing Attention with Explicit Phrasal Alignments", "authors": ["Xuan-Phi Nguyen", "Shafiq Joty", "Thanh-Tung Nguyen"], "pdf": "/pdf/846366884ccb6624de9f8ff9db0534f56f468548.pdf", "abstract": "The attention mechanism is an indispensable component of any state-of-the-art neural machine translation system. However, existing attention methods are often token-based and ignore the importance of phrasal alignments, which are the backbone of phrase-based statistical machine translation. We propose a novel phrase-based attention method to model n-grams of tokens as the basic attention entities, and design multi-headed phrasal attentions within the Transformer architecture to perform token-to-token and token-to-phrase mappings. Our approach yields improvements in English-German, English-Russian and English-French translation tasks on the standard WMT'14 test set. Furthermore, our phrasal attention method shows improvements on the one-billion-word language modeling benchmark.\n", "keywords": ["NMT", "Phrasal Attention", "Machine Translation", "Language Modeling"], "paperhash": "nguyen|enhancing_attention_with_explicit_phrasal_alignments", "original_pdf": "/attachment/846366884ccb6624de9f8ff9db0534f56f468548.pdf", "_bibtex": "@misc{\nnguyen2020enhancing,\ntitle={Enhancing Attention with Explicit Phrasal Alignments},\nauthor={Xuan-Phi Nguyen and Shafiq Joty and Thanh-Tung Nguyen},\nyear={2020},\nurl={https://openreview.net/forum?id=BygPq6VFvS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BygPq6VFvS", "replyto": "BygPq6VFvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795728764, "tmdate": 1576800281234, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper711/-/Decision"}}}, {"id": "Byl2Sx7aYH", "original": null, "number": 1, "cdate": 1571790916234, "ddate": null, "tcdate": 1571790916234, "tmdate": 1572972561823, "tddate": null, "forum": "BygPq6VFvS", "replyto": "BygPq6VFvS", "invitation": "ICLR.cc/2020/Conference/Paper711/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an extension of the attention module that explicitly incorporates phrase information. Using convolution, attention scores are obtained independently for each n-gram type, and then combined. Transformer models with the proposed phrase attention are evaluated on multiple translation tasks, as well as on language modelling, generally obtaining better results than by simply increasing model size.\n\nI lean towards the acceptance of the paper. The approach is fairly well motivated, likely easy to implement and results are mostly convincing. However, some claims may be too strong and I had difficulty understanding some parts of the approach.\n\nI find the idea interesting. Standard attention is unbiased to distance (but sensitive to it because of positional embeddings). Phrasal attention may be a useful learning bias, giving particular importance to nearby words.\n\nOn 3 WMT'14 translation tasks, the proposed approach leads to improvements between 0.7 and 1.8 BLEU with respect to Transformer Base. Running each model with different random seeds and presenting statistical significance results would be ideal, but such runs can be expensive given the size of the datasets. Using phrasal attention appears to be more efficient than simply increasing model size. In addition to the number of parameters, the number of FLOPs per update might also be useful to know. Phrasal attention also leads to lower perplexity on a large-scale modeling task, although I can't confidently evaluate the importance of this result.\n\nWhile results in the model interpretation section are cherry-picked, they illustrate that the model can use the additional capacity provided by phrasal attention. There is also clear qualitative differences between layers.\n\nSome equations are confusing. For example, in Eq. 5, the right-most argument of Conv_n() appears to be of dimension nxdx1, but convolutions are defined for dimension nxdxd. I would suggest going over the presentation of phrasal attention carefully (or correct me if I interpreted the notation wrongly).\n\nSome claims made in the paper may be too strong. While there are similarities between alignment and attention, they are not necessarily interchangeable in neural models. For example, (Koehn and Knowles. Six Challenges for Neural Machine Translation) show that they can be mismatched (Fig. 9).\n\nMoreover, while input embeddings (and arguably the last decoder hidden layer) mostly contain token-level information, intermediate representations merge information from multiple positions. As such, at a given layer, it is not guaranteed the the i^{th} vector is a representation i^{th} token. For example, (Voita et al. The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives) show that the mutual information between an input token and its corresponding encoder representation diminishes as depth increases. As such, neighbouring representations may not represent n-grams.\n\nIt would be appropriate to compare the proposed approach to (Hao et al. Multi-Granularity Self-Attention for Neural Machine Translation). However, this is very recent work (September 5 on ArXiv), so it would be understandable for the authors not to know about it.\n\nQuestions:\n\nWhile searching for related work, I found an earlier submitted version of this paper (\"Phrase-Based Attentions\", submitted to ICLR 2019). The reported numbers differ from the current version. Why?"}, "signatures": ["ICLR.cc/2020/Conference/Paper711/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper711/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["nxphi47@gmail.com", "sjoty@salesforce.com", "ng0155ng@e.ntu.edu.sg"], "title": "Enhancing Attention with Explicit Phrasal Alignments", "authors": ["Xuan-Phi Nguyen", "Shafiq Joty", "Thanh-Tung Nguyen"], "pdf": "/pdf/846366884ccb6624de9f8ff9db0534f56f468548.pdf", "abstract": "The attention mechanism is an indispensable component of any state-of-the-art neural machine translation system. However, existing attention methods are often token-based and ignore the importance of phrasal alignments, which are the backbone of phrase-based statistical machine translation. We propose a novel phrase-based attention method to model n-grams of tokens as the basic attention entities, and design multi-headed phrasal attentions within the Transformer architecture to perform token-to-token and token-to-phrase mappings. Our approach yields improvements in English-German, English-Russian and English-French translation tasks on the standard WMT'14 test set. Furthermore, our phrasal attention method shows improvements on the one-billion-word language modeling benchmark.\n", "keywords": ["NMT", "Phrasal Attention", "Machine Translation", "Language Modeling"], "paperhash": "nguyen|enhancing_attention_with_explicit_phrasal_alignments", "original_pdf": "/attachment/846366884ccb6624de9f8ff9db0534f56f468548.pdf", "_bibtex": "@misc{\nnguyen2020enhancing,\ntitle={Enhancing Attention with Explicit Phrasal Alignments},\nauthor={Xuan-Phi Nguyen and Shafiq Joty and Thanh-Tung Nguyen},\nyear={2020},\nurl={https://openreview.net/forum?id=BygPq6VFvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BygPq6VFvS", "replyto": "BygPq6VFvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper711/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper711/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575671874179, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper711/Reviewers"], "noninvitees": [], "tcdate": 1570237748195, "tmdate": 1575671874194, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper711/-/Official_Review"}}}, {"id": "rkxr4LdpYH", "original": null, "number": 2, "cdate": 1571812909361, "ddate": null, "tcdate": 1571812909361, "tmdate": 1572972561775, "tddate": null, "forum": "BygPq6VFvS", "replyto": "BygPq6VFvS", "invitation": "ICLR.cc/2020/Conference/Paper711/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This work aims to incorporate phrase representation into attention mechanism. The proposed method is straightforward (which is good): a convolution window with size n is used to calculate representation for an n-gram, which then replaces the token representation in a standard attention model. The paper implements the multihead version of the proposed phrase-based attention, more specifically, in a transformer model. Experiments with machine translation and language modeling show that it outperforms the token attention counterpart.\n\nMy main concerns are about the experiments: \n\n- For the translation experiments, the transformer numbers are lower than those reported by Vaswani et al. (2017) across the board, for both the \"base\" and the \"big\" settings. I didn't find convincing reason for this. Could the authors comment on this, and also on why they do not directly compare against Vaswani et al. (2017). The same for the language modeling experiments.\n\n- Table 3. To the best of my knowledge, neither Vaswani et al. (2017) or Shaw et al. (2018) report language modeling results. I'm guessing the authors use their implementation and establish their own baselines, which is okay. But please indicate this instead of putting a citation in the table, which can be misleading.\n\n- Minor: the start of Section 3 reads like source code and comment, and is a bit hard for me to follow.\n\nI do not recommend the that the paper is accepted, until the authors address my concerns on the baselines. \n\n\nMissing references:\nPaLM: A Hybrid Parser and Language Model. https://arxiv.org/abs/1909.02134"}, "signatures": ["ICLR.cc/2020/Conference/Paper711/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper711/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["nxphi47@gmail.com", "sjoty@salesforce.com", "ng0155ng@e.ntu.edu.sg"], "title": "Enhancing Attention with Explicit Phrasal Alignments", "authors": ["Xuan-Phi Nguyen", "Shafiq Joty", "Thanh-Tung Nguyen"], "pdf": "/pdf/846366884ccb6624de9f8ff9db0534f56f468548.pdf", "abstract": "The attention mechanism is an indispensable component of any state-of-the-art neural machine translation system. However, existing attention methods are often token-based and ignore the importance of phrasal alignments, which are the backbone of phrase-based statistical machine translation. We propose a novel phrase-based attention method to model n-grams of tokens as the basic attention entities, and design multi-headed phrasal attentions within the Transformer architecture to perform token-to-token and token-to-phrase mappings. Our approach yields improvements in English-German, English-Russian and English-French translation tasks on the standard WMT'14 test set. Furthermore, our phrasal attention method shows improvements on the one-billion-word language modeling benchmark.\n", "keywords": ["NMT", "Phrasal Attention", "Machine Translation", "Language Modeling"], "paperhash": "nguyen|enhancing_attention_with_explicit_phrasal_alignments", "original_pdf": "/attachment/846366884ccb6624de9f8ff9db0534f56f468548.pdf", "_bibtex": "@misc{\nnguyen2020enhancing,\ntitle={Enhancing Attention with Explicit Phrasal Alignments},\nauthor={Xuan-Phi Nguyen and Shafiq Joty and Thanh-Tung Nguyen},\nyear={2020},\nurl={https://openreview.net/forum?id=BygPq6VFvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BygPq6VFvS", "replyto": "BygPq6VFvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper711/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper711/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575671874179, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper711/Reviewers"], "noninvitees": [], "tcdate": 1570237748195, "tmdate": 1575671874194, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper711/-/Official_Review"}}}, {"id": "BkxB9WY0KS", "original": null, "number": 3, "cdate": 1571881356917, "ddate": null, "tcdate": 1571881356917, "tmdate": 1572972561731, "tddate": null, "forum": "BygPq6VFvS", "replyto": "BygPq6VFvS", "invitation": "ICLR.cc/2020/Conference/Paper711/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This work proposes an attention mechanism that directly reflects the phrasal correspondence by employing convolution. The n-gram aware attention is incorporated into Transformer and shows gains in translation and language modeling tasks.\n\nIt is a novel and straightforward way to incorporate phrasal relation in the attention mechanism. The gains reported in this paper are meaningful, thought might not be SOTA. The analysis on attention is also interesting in that the phrasal relation is employed in lower layers, but not in the higher layer.\n\nOther comment:\n\n- It is not clear to me how the n-gram aware phrasal attention is incorporated into multi-headed attention described in section 3.2. Did you completely remove the multi-head attention, but used only n-gram attentions? Or, did you keep multi-head attention and incorporated phrasal attention for each head?\n\n- It is unfortunate that this work does not report empirical results when applied to a big model configuration. Did you encounter OOM when running on a big model?"}, "signatures": ["ICLR.cc/2020/Conference/Paper711/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper711/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["nxphi47@gmail.com", "sjoty@salesforce.com", "ng0155ng@e.ntu.edu.sg"], "title": "Enhancing Attention with Explicit Phrasal Alignments", "authors": ["Xuan-Phi Nguyen", "Shafiq Joty", "Thanh-Tung Nguyen"], "pdf": "/pdf/846366884ccb6624de9f8ff9db0534f56f468548.pdf", "abstract": "The attention mechanism is an indispensable component of any state-of-the-art neural machine translation system. However, existing attention methods are often token-based and ignore the importance of phrasal alignments, which are the backbone of phrase-based statistical machine translation. We propose a novel phrase-based attention method to model n-grams of tokens as the basic attention entities, and design multi-headed phrasal attentions within the Transformer architecture to perform token-to-token and token-to-phrase mappings. Our approach yields improvements in English-German, English-Russian and English-French translation tasks on the standard WMT'14 test set. Furthermore, our phrasal attention method shows improvements on the one-billion-word language modeling benchmark.\n", "keywords": ["NMT", "Phrasal Attention", "Machine Translation", "Language Modeling"], "paperhash": "nguyen|enhancing_attention_with_explicit_phrasal_alignments", "original_pdf": "/attachment/846366884ccb6624de9f8ff9db0534f56f468548.pdf", "_bibtex": "@misc{\nnguyen2020enhancing,\ntitle={Enhancing Attention with Explicit Phrasal Alignments},\nauthor={Xuan-Phi Nguyen and Shafiq Joty and Thanh-Tung Nguyen},\nyear={2020},\nurl={https://openreview.net/forum?id=BygPq6VFvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BygPq6VFvS", "replyto": "BygPq6VFvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper711/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper711/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575671874179, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper711/Reviewers"], "noninvitees": [], "tcdate": 1570237748195, "tmdate": 1575671874194, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper711/-/Official_Review"}}}], "count": 5}