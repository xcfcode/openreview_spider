{"notes": [{"id": "H1xJhJStPS", "original": "SJlVHlytPB", "number": 1937, "cdate": 1569439655244, "ddate": null, "tcdate": 1569439655244, "tmdate": 1577168287199, "tddate": null, "forum": "H1xJhJStPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["maxence.ernoult@u-psud.fr", "julie.grollier@cnrs-thales.fr", "damien.querlioz@u-psud.fr", "yoshua.bengio@mila.quebec", "benjamin.scellier@umontreal.ca"], "title": "Equilibrium Propagation with Continual Weight Updates", "authors": ["Maxence Ernoult", "Julie Grollier", "Damien Querlioz", "Yoshua Bengio", "Benjamin Scellier"], "pdf": "/pdf/9d45be17d2ebaf971278460a76bde19d4d5d0291.pdf", "TL;DR": "We propose a continual version of Equilibrium Propagation, where neuron and synapse dynamics occur simultaneously throughout the second phase, with theoretical guarantees and numerical simulations.", "abstract": "Equilibrium Propagation (EP) is a learning algorithm that bridges Machine Learning and Neuroscience, by computing gradients closely matching those of Backpropagation Through Time (BPTT), but with a learning rule local in space.\nGiven an input x and associated target y, EP proceeds in two phases: in the first phase neurons evolve freely towards a first steady state; in the second phase output neurons are nudged towards y until they reach a second steady state.\nHowever, in existing implementations of EP, the learning rule is not local in time:\nthe weight update is performed after the dynamics of the second phase have converged and requires information of the first phase that is no longer available physically.\nThis is a major impediment to the biological plausibility of EP and its efficient hardware implementation.\nIn this work, we propose a version of EP named Continual Equilibrium Propagation (C-EP) where neuron and synapse dynamics occur simultaneously throughout the second phase, so that the weight update becomes local in time. We prove theoretically that, provided the learning rates are sufficiently small, at each time step of the second phase the dynamics of neurons and synapses follow the gradients of the loss given by BPTT (Theorem 1).\nWe demonstrate training with C-EP on MNIST and generalize C-EP to neural networks where neurons are connected by asymmetric connections. We show through experiments that the more the network updates follows the gradients of BPTT, the best it performs in terms of training. These results bring EP a step closer to biology while maintaining its intimate link with backpropagation.", "code": "https://drive.google.com/open?id=1oZtzBTu8zZgvAopyK2sQg2bpcsrzwTrp", "keywords": ["Biologically Plausible Neural Networks", "Equilibrium Propagation"], "paperhash": "ernoult|equilibrium_propagation_with_continual_weight_updates", "original_pdf": "/attachment/b9705324e7ca49bf396f1ea3160b2c94dc7c3076.pdf", "_bibtex": "@misc{\nernoult2020equilibrium,\ntitle={Equilibrium Propagation with Continual Weight Updates},\nauthor={Maxence Ernoult and Julie Grollier and Damien Querlioz and Yoshua Bengio and Benjamin Scellier},\nyear={2020},\nurl={https://openreview.net/forum?id=H1xJhJStPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "K23TsejCZ", "original": null, "number": 1, "cdate": 1576798736373, "ddate": null, "tcdate": 1576798736373, "tmdate": 1576800899999, "tddate": null, "forum": "H1xJhJStPS", "replyto": "H1xJhJStPS", "invitation": "ICLR.cc/2020/Conference/Paper1937/-/Decision", "content": {"decision": "Reject", "comment": "Main content: paper introduces a new variant of equilibrium propagation algorithm that continually updates the weights making it unnecessary to save steady states. T\n\nSummary of discussion:\nreviewer 1: likes the idea but points out many issues with the proofs. \nreviewer 2: he really likes the novelty of paper, but review is not detailed, particularly discussing pros/cons. \nreviewer 3: likes the ideas but has questions on proofs, and also questions why MNIST is used as the evaluation tasks.\nRecommendation: interesting idea but writing/proofs could be clarified better. Vote reject.\n\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maxence.ernoult@u-psud.fr", "julie.grollier@cnrs-thales.fr", "damien.querlioz@u-psud.fr", "yoshua.bengio@mila.quebec", "benjamin.scellier@umontreal.ca"], "title": "Equilibrium Propagation with Continual Weight Updates", "authors": ["Maxence Ernoult", "Julie Grollier", "Damien Querlioz", "Yoshua Bengio", "Benjamin Scellier"], "pdf": "/pdf/9d45be17d2ebaf971278460a76bde19d4d5d0291.pdf", "TL;DR": "We propose a continual version of Equilibrium Propagation, where neuron and synapse dynamics occur simultaneously throughout the second phase, with theoretical guarantees and numerical simulations.", "abstract": "Equilibrium Propagation (EP) is a learning algorithm that bridges Machine Learning and Neuroscience, by computing gradients closely matching those of Backpropagation Through Time (BPTT), but with a learning rule local in space.\nGiven an input x and associated target y, EP proceeds in two phases: in the first phase neurons evolve freely towards a first steady state; in the second phase output neurons are nudged towards y until they reach a second steady state.\nHowever, in existing implementations of EP, the learning rule is not local in time:\nthe weight update is performed after the dynamics of the second phase have converged and requires information of the first phase that is no longer available physically.\nThis is a major impediment to the biological plausibility of EP and its efficient hardware implementation.\nIn this work, we propose a version of EP named Continual Equilibrium Propagation (C-EP) where neuron and synapse dynamics occur simultaneously throughout the second phase, so that the weight update becomes local in time. We prove theoretically that, provided the learning rates are sufficiently small, at each time step of the second phase the dynamics of neurons and synapses follow the gradients of the loss given by BPTT (Theorem 1).\nWe demonstrate training with C-EP on MNIST and generalize C-EP to neural networks where neurons are connected by asymmetric connections. We show through experiments that the more the network updates follows the gradients of BPTT, the best it performs in terms of training. These results bring EP a step closer to biology while maintaining its intimate link with backpropagation.", "code": "https://drive.google.com/open?id=1oZtzBTu8zZgvAopyK2sQg2bpcsrzwTrp", "keywords": ["Biologically Plausible Neural Networks", "Equilibrium Propagation"], "paperhash": "ernoult|equilibrium_propagation_with_continual_weight_updates", "original_pdf": "/attachment/b9705324e7ca49bf396f1ea3160b2c94dc7c3076.pdf", "_bibtex": "@misc{\nernoult2020equilibrium,\ntitle={Equilibrium Propagation with Continual Weight Updates},\nauthor={Maxence Ernoult and Julie Grollier and Damien Querlioz and Yoshua Bengio and Benjamin Scellier},\nyear={2020},\nurl={https://openreview.net/forum?id=H1xJhJStPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1xJhJStPS", "replyto": "H1xJhJStPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795708506, "tmdate": 1576800256947, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1937/-/Decision"}}}, {"id": "ByxzbYN3oS", "original": null, "number": 8, "cdate": 1573828858004, "ddate": null, "tcdate": 1573828858004, "tmdate": 1573828858004, "tddate": null, "forum": "H1xJhJStPS", "replyto": "H1xJhJStPS", "invitation": "ICLR.cc/2020/Conference/Paper1937/-/Official_Comment", "content": {"title": "Overview of revisions and responses", "comment": "We thank the reviewers for their valuable comments, which have help us improve our manuscript - see our revised version.\n\nBased on their feedback, we have now revised our manuscript with the following amendments:\n\n1- To address the request of Reviewer # 3, we have now defined precisely what we meant by \"biological plausibility\" in our context of study. We have emphasized that the main motivation of our study was not the development of a model of biological learning but to make EP better comply with hardware constraints which in particular require both the locality in space and time of the learning rule. For this purpose, we have amended the abstract, the introduction and the discussion.\n\n2- We have proceeded to a change of terminology in the whole manuscript: the quantity $\\Delta_{\\theta}^{\\rm C-EP}$ is no longer called an 'update' but a 'normalized update' to stress that $\\Delta_{\\theta}^{\\rm C-EP}$ is not the effective parameter update (which is $\\eta\\Delta_{\\theta}^{\\rm C-EP}$) but the update normalized by the learning rate $\\eta$.\nThis addresses 1-A) of our answer to Reviewer # 1. \n\n3- We have clarified the proof of Lemma 2 in Appendix A.3, taking the limit $\\eta \\to 0 $ with $\\eta > 0$. This addresses 1-B) of our answer to Reviewer # 1.\n\n4- We have clarified the link between the total parameter update of C-EP and the standard learning rule of EP ($\\Delta\\theta = \\frac{\\eta}{\\beta}\\left(\\frac{\\partial\\Phi}{\\partial s}(s^\\beta_*) - \\frac{\\partial\\Phi}{\\partial s}(s_*) \\right)$) after the derivation of Lemma 2 in Appendix A.3. This addresses 1-C) of our answer to Reviewer # 1.\n\n5- We have clarified in the introduction that our work addresses two issues of EP: first that its learning rule is not local in time, second that it relies on the requirement of a primitive function $\\Phi$ for the transition function $F$; the first issue is addressed with the C-EP algorithm, the second with C-VF algorithm. This addresses 2-A) of our answer to Reviewer # 1.\n\n6- In section 4, we now make a clear distinction between the training algorithms (C-EP and C-VF) and the models they train. What was previously called the 'C-EP model' has become the 'Vanilla RNN with symmetric weights trained with C-EP'. Likewise, the 'C-VF model' has become the 'Vanilla RNN with asymmetric weights trained with C-VF'. We have also stressed after Eq.(12) (of the revised manuscript), after introducing the Vanilla RNN model with asymmetric weights trained with C-VF that although in this case the dynamics do not derive from a primitive function $\\Phi$, Theorem 1 can be generalized to this setting, by referring to the related Appendix where this generalization is derived. This addresses 2-C) of our answer to Reviewer # 1.\n\n7- We have explained in details why our vanilla RNN model with symmetric weights trained with C-EP described in Section 4.1 extends to deep architectures with any number of layers with symmetric connections. Also, we have explicitly written the primitive function $\\Phi$ for all our models. For this purpose we have added mathematical details in Appendix E describing all the models used in the papers, on a simple example and we now refer to this Appendix in Section 4.1. This addresses 2-B) and 2-D) of our answer to Reviewer # 1.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1937/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1937/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maxence.ernoult@u-psud.fr", "julie.grollier@cnrs-thales.fr", "damien.querlioz@u-psud.fr", "yoshua.bengio@mila.quebec", "benjamin.scellier@umontreal.ca"], "title": "Equilibrium Propagation with Continual Weight Updates", "authors": ["Maxence Ernoult", "Julie Grollier", "Damien Querlioz", "Yoshua Bengio", "Benjamin Scellier"], "pdf": "/pdf/9d45be17d2ebaf971278460a76bde19d4d5d0291.pdf", "TL;DR": "We propose a continual version of Equilibrium Propagation, where neuron and synapse dynamics occur simultaneously throughout the second phase, with theoretical guarantees and numerical simulations.", "abstract": "Equilibrium Propagation (EP) is a learning algorithm that bridges Machine Learning and Neuroscience, by computing gradients closely matching those of Backpropagation Through Time (BPTT), but with a learning rule local in space.\nGiven an input x and associated target y, EP proceeds in two phases: in the first phase neurons evolve freely towards a first steady state; in the second phase output neurons are nudged towards y until they reach a second steady state.\nHowever, in existing implementations of EP, the learning rule is not local in time:\nthe weight update is performed after the dynamics of the second phase have converged and requires information of the first phase that is no longer available physically.\nThis is a major impediment to the biological plausibility of EP and its efficient hardware implementation.\nIn this work, we propose a version of EP named Continual Equilibrium Propagation (C-EP) where neuron and synapse dynamics occur simultaneously throughout the second phase, so that the weight update becomes local in time. We prove theoretically that, provided the learning rates are sufficiently small, at each time step of the second phase the dynamics of neurons and synapses follow the gradients of the loss given by BPTT (Theorem 1).\nWe demonstrate training with C-EP on MNIST and generalize C-EP to neural networks where neurons are connected by asymmetric connections. We show through experiments that the more the network updates follows the gradients of BPTT, the best it performs in terms of training. These results bring EP a step closer to biology while maintaining its intimate link with backpropagation.", "code": "https://drive.google.com/open?id=1oZtzBTu8zZgvAopyK2sQg2bpcsrzwTrp", "keywords": ["Biologically Plausible Neural Networks", "Equilibrium Propagation"], "paperhash": "ernoult|equilibrium_propagation_with_continual_weight_updates", "original_pdf": "/attachment/b9705324e7ca49bf396f1ea3160b2c94dc7c3076.pdf", "_bibtex": "@misc{\nernoult2020equilibrium,\ntitle={Equilibrium Propagation with Continual Weight Updates},\nauthor={Maxence Ernoult and Julie Grollier and Damien Querlioz and Yoshua Bengio and Benjamin Scellier},\nyear={2020},\nurl={https://openreview.net/forum?id=H1xJhJStPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1xJhJStPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1937/Authors", "ICLR.cc/2020/Conference/Paper1937/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1937/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1937/Reviewers", "ICLR.cc/2020/Conference/Paper1937/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1937/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1937/Authors|ICLR.cc/2020/Conference/Paper1937/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148702, "tmdate": 1576860532494, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1937/Authors", "ICLR.cc/2020/Conference/Paper1937/Reviewers", "ICLR.cc/2020/Conference/Paper1937/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1937/-/Official_Comment"}}}, {"id": "HJxY6VU4sS", "original": null, "number": 4, "cdate": 1573311680698, "ddate": null, "tcdate": 1573311680698, "tmdate": 1573568116549, "tddate": null, "forum": "H1xJhJStPS", "replyto": "rygnS_8nYH", "invitation": "ICLR.cc/2020/Conference/Paper1937/-/Official_Comment", "content": {"title": "Biological plausibility of C-EP", "comment": "We would like to thank the reviewer for his/her comments. Based on this feedback, in the revised version of the paper, we have decided to define explicitly what is meant by biological plausibility in this work, to clarify that continual EP does not aim at being a model of biological learning, and to discuss explicitly the aspects of this algorithm that still  differ from biological learning. \n\nBrains learn using learning rules that have to be local in space and time. Error backpropagation  is particularly non biologically plausible in this regard, as it is fundamentally non local, both in space and time.  Our interest is to propose learning rules that feature the two localities, and this is what we define here as being \u00ab\u00a0biologically plausible\u00a0\u00bb. In this work, we build on EP, which is already local in space, and propose C-EP, which adds locality in time. An important motivation for the development of such minimally biologically-plausible learning rules is that they could be used for the development of extremely energy efficient learning-capable hardware.\n\nWe want to be clear that Continual EP does not aim at being a model of biological learning in that it would account for how the brain works or how animals learn. Continual EP does indeed retain considerable differences with actual brain learning. As the Reviewer says, the learning paradigm that is the closest to the way the brain actually learns is Reinforcement Learning. Also, C-EP is evaluated on the MNIST dataset, as the whole current EP literature, which is indeed a conceptual and not a realistic biological task. On the other hand, the use of this task allows a natural bridge with conventional machine learning research. Finally, the equations used in C-EP have no ties with neuroscience experiments.\n\nWe propose to make an important overhaul of the introduction and of the discussion of the paper to clarify these points about the nature of our work in the next few days."}, "signatures": ["ICLR.cc/2020/Conference/Paper1937/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1937/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maxence.ernoult@u-psud.fr", "julie.grollier@cnrs-thales.fr", "damien.querlioz@u-psud.fr", "yoshua.bengio@mila.quebec", "benjamin.scellier@umontreal.ca"], "title": "Equilibrium Propagation with Continual Weight Updates", "authors": ["Maxence Ernoult", "Julie Grollier", "Damien Querlioz", "Yoshua Bengio", "Benjamin Scellier"], "pdf": "/pdf/9d45be17d2ebaf971278460a76bde19d4d5d0291.pdf", "TL;DR": "We propose a continual version of Equilibrium Propagation, where neuron and synapse dynamics occur simultaneously throughout the second phase, with theoretical guarantees and numerical simulations.", "abstract": "Equilibrium Propagation (EP) is a learning algorithm that bridges Machine Learning and Neuroscience, by computing gradients closely matching those of Backpropagation Through Time (BPTT), but with a learning rule local in space.\nGiven an input x and associated target y, EP proceeds in two phases: in the first phase neurons evolve freely towards a first steady state; in the second phase output neurons are nudged towards y until they reach a second steady state.\nHowever, in existing implementations of EP, the learning rule is not local in time:\nthe weight update is performed after the dynamics of the second phase have converged and requires information of the first phase that is no longer available physically.\nThis is a major impediment to the biological plausibility of EP and its efficient hardware implementation.\nIn this work, we propose a version of EP named Continual Equilibrium Propagation (C-EP) where neuron and synapse dynamics occur simultaneously throughout the second phase, so that the weight update becomes local in time. We prove theoretically that, provided the learning rates are sufficiently small, at each time step of the second phase the dynamics of neurons and synapses follow the gradients of the loss given by BPTT (Theorem 1).\nWe demonstrate training with C-EP on MNIST and generalize C-EP to neural networks where neurons are connected by asymmetric connections. We show through experiments that the more the network updates follows the gradients of BPTT, the best it performs in terms of training. These results bring EP a step closer to biology while maintaining its intimate link with backpropagation.", "code": "https://drive.google.com/open?id=1oZtzBTu8zZgvAopyK2sQg2bpcsrzwTrp", "keywords": ["Biologically Plausible Neural Networks", "Equilibrium Propagation"], "paperhash": "ernoult|equilibrium_propagation_with_continual_weight_updates", "original_pdf": "/attachment/b9705324e7ca49bf396f1ea3160b2c94dc7c3076.pdf", "_bibtex": "@misc{\nernoult2020equilibrium,\ntitle={Equilibrium Propagation with Continual Weight Updates},\nauthor={Maxence Ernoult and Julie Grollier and Damien Querlioz and Yoshua Bengio and Benjamin Scellier},\nyear={2020},\nurl={https://openreview.net/forum?id=H1xJhJStPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1xJhJStPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1937/Authors", "ICLR.cc/2020/Conference/Paper1937/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1937/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1937/Reviewers", "ICLR.cc/2020/Conference/Paper1937/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1937/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1937/Authors|ICLR.cc/2020/Conference/Paper1937/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148702, "tmdate": 1576860532494, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1937/Authors", "ICLR.cc/2020/Conference/Paper1937/Reviewers", "ICLR.cc/2020/Conference/Paper1937/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1937/-/Official_Comment"}}}, {"id": "H1ewjm8EiH", "original": null, "number": 2, "cdate": 1573311391231, "ddate": null, "tcdate": 1573311391231, "tmdate": 1573487777431, "tddate": null, "forum": "H1xJhJStPS", "replyto": "BkxSJJoJ9H", "invitation": "ICLR.cc/2020/Conference/Paper1937/-/Official_Comment", "content": {"title": "Equivalence between EP and C-EP ", "comment": "We thank the reviewer for his/her comments.\n\n1 - Concerning the equivalence between EP and C-EP (Lemma 2, p.~11), which states that $\\lim_{\\eta \\to 0} \\Delta_{\\theta}^{\\rm C-EP}(\\eta, \\beta, t) = \\Delta_{\\theta}^{\\rm EP}(\\beta, t)$. \n \nA) The first point that we want to clarify deals with what we call an `update'.\nIn machine learning in general, one usually distinguishes between the error gradient $\\frac{\\partial L}{\\partial \\theta}$ and the update $\\Delta\\theta = \\eta \\frac{\\partial L}{\\partial \\theta}$, which is the gradient rescaled by a learning rate $\\eta$.\nIn C-EP in contrast, what we deceivingly call an `update' and denote $\\Delta_\\theta^{\\rm C-EP}(\\beta,\\eta,t)$ actually corresponds to the gradient, not the update itself.\nTo get the actual parameter update in C-EP one needs to rescale $\\Delta_\\theta^{\\rm C-EP}(\\beta,\\eta,t)$ by $\\eta$ ;\nthe actual update is $\\eta \\; \\Delta_\\theta^{\\rm C-EP}(\\beta,\\eta,t)$, so that $\\theta_{t+1}^{\\eta,\\beta} = \\theta_{t}^{\\eta,\\beta} + \\eta \\; \\Delta_{\\theta}^{\\rm C-EP}(\\eta, \\beta, t)$. \nFor this reason, when $\\eta$ is tiny (or even zero), it is not contradictory that $\\theta_{t}^{\\eta, \\beta}$ does not change while $\\Delta_\\theta^{\\rm C-EP}(\\beta,\\eta,t)$ is non-zero -- more generally in machine learning it is not incompatible that the update is zero while the gradient is non-zero, if the learning rate is $\\eta = 0$.\nIn the rest of our answer, to better convey the idea that $\\Delta_\\theta^{\\rm C-EP}(\\beta,\\eta,t)$ corresponds to a gradient (and not an update in the usual sense),\nwe will refer to it as the `forward-time gradient' of C-EP.\nThe term `update' will be used to refer to $\\eta\\Delta_\\theta^{\\rm C-EP}$.\nWe also propose to change the terminology in the whole manuscript (not done yet).\n\nB) Although Lemma~2 holds in the limit $\\eta \\to 0$,  in practice there is however a trade-off between taking $\\eta$ small enough so that the forward-time gradient $\\Delta_\\theta^{\\rm C-EP}(\\beta,\\eta,t)$ is close enough to $\\Delta_\\theta^{\\rm EP}(\\beta, t)$, but not too tiny so that the parameter update $\\eta \\Delta_\\theta^{\\rm C-EP}(\\beta,\\eta,t)$ is not too small to ensure the loss is optimized within a reasonable time (see the bottom of p.6 of the submitted manuscript and Appendix~F.2 p.30). Taking $\\eta = 0$ in practice is thus excluded.\nIn the proof of Lemma 2 in Appendix A.3, we take $\\eta = 0$ because it is mathematically equivalent to taking the limit $\\eta \\to 0$ (with $\\eta >0$), by continuity. We have clarified the proof in the revised version of the manuscript.\n\nC) To understand the equivalence of C-EP and EP, one key thing to have in mind is that if the second phase of EP is run for $K$ steps (i.e. if it takes $K$ steps to get from the first steady state $s_*$ to the second steady state $s_*^\\beta$) then the total forward-time gradient (in the sense defined above) of EP is not $\\Delta_\\theta^{\\rm EP}(\\beta,K)$ but $\\Delta_\\theta^{\\rm EP}(\\beta,0) + \\Delta_\\theta^{\\rm EP}(\\beta,1) + \\cdots + \\Delta_\\theta^{\\rm EP}(\\beta,K)$. To see why this is the case, one has to look at the definition of $\\Delta_\\theta^{\\rm EP}(\\beta,t)$ (see Appendix~A.3, Eq.~(18)).\nNow, let us denote $\\Delta_\\theta^{\\rm EP}(\\beta,{\\rm tot}) = \\Delta_\\theta^{\\rm EP}(\\beta,0) +\\Delta_\\theta^{\\rm EP}(\\beta,1) + \\cdots + \\Delta_\\theta^{\\rm EP}(\\beta,K)$ the total gradient of EP, for short.\nIf we do the suggested procedure, keeping $\\eta = 0$ for the first $K-1$ steps and changing $\\eta$ to a positive value at time step $K$, then the effective update of C-EP is $\\theta^{\\beta,\\eta}_{K} - \\theta^{\\beta,\\eta}_{K-1}$ (also equal to $\\eta \\Delta_\\theta^{\\rm C-EP}(\\beta,\\eta,K)$ ). By Lemma 2, this C-EP update is close to $\\eta \\Delta_\\theta^{\\rm EP}(\\beta,K)$, but not to the total update of EP, which is $\\eta \\Delta_\\theta^{\\rm EP}(\\beta,{\\rm tot})$.\nConversely, if we keep a constant $\\eta$ positive but sufficiently small throughout the second phase, the total parameter update of C-EP at the end of the second phase (after $K$ time steps) is approximately equal to the total parameter update of EP:\n    $\\theta^{\\beta,\\eta}_K - \\theta^{\\beta,\\eta}_0 = \\sum_{t=0}^{K-1}(\\theta_{t+1}^{\\eta, \\beta} - \\theta_{t}^{\\eta, \\beta})  = \\sum_{t=0}^{K-1} \\eta \\Delta_{\\theta}^{\\rm C-EP}(\\eta, \\beta, t) $\n    $\\approx \\sum_{t=0}^{K-1} \\eta \\Delta_{\\theta}^{\\rm EP}(\\beta, t) = \\sum_{t=0}^{K-1} \\eta \\frac{1}{\\beta} \\left(\\frac{\\partial \\Phi}{\\partial \\theta}(s_{t+1}^\\beta) -\\frac{\\partial \\Phi}{\\partial\\theta}(s_t)\\right) =  \\eta\\frac{1}{\\beta}\\left(\\frac{\\partial \\Phi}{\\partial \\theta}(s_*^\\beta) -\\frac{\\partial \\Phi}{\\partial\\theta}(s_*)\\right)$\nTo derive the above equation, we have successively used: a telescoping sum, the definition of $\\Delta_{\\theta}^{\\rm C-EP}(\\eta, \\beta, t)$, Lemma 2, the definition of $\\Delta_{\\theta}^{\\rm EP}(\\beta, t)$, and another telescoping sum. We propose to include this equation after the proof of Lemma 2, Appendix A.3, p.11."}, "signatures": ["ICLR.cc/2020/Conference/Paper1937/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1937/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maxence.ernoult@u-psud.fr", "julie.grollier@cnrs-thales.fr", "damien.querlioz@u-psud.fr", "yoshua.bengio@mila.quebec", "benjamin.scellier@umontreal.ca"], "title": "Equilibrium Propagation with Continual Weight Updates", "authors": ["Maxence Ernoult", "Julie Grollier", "Damien Querlioz", "Yoshua Bengio", "Benjamin Scellier"], "pdf": "/pdf/9d45be17d2ebaf971278460a76bde19d4d5d0291.pdf", "TL;DR": "We propose a continual version of Equilibrium Propagation, where neuron and synapse dynamics occur simultaneously throughout the second phase, with theoretical guarantees and numerical simulations.", "abstract": "Equilibrium Propagation (EP) is a learning algorithm that bridges Machine Learning and Neuroscience, by computing gradients closely matching those of Backpropagation Through Time (BPTT), but with a learning rule local in space.\nGiven an input x and associated target y, EP proceeds in two phases: in the first phase neurons evolve freely towards a first steady state; in the second phase output neurons are nudged towards y until they reach a second steady state.\nHowever, in existing implementations of EP, the learning rule is not local in time:\nthe weight update is performed after the dynamics of the second phase have converged and requires information of the first phase that is no longer available physically.\nThis is a major impediment to the biological plausibility of EP and its efficient hardware implementation.\nIn this work, we propose a version of EP named Continual Equilibrium Propagation (C-EP) where neuron and synapse dynamics occur simultaneously throughout the second phase, so that the weight update becomes local in time. We prove theoretically that, provided the learning rates are sufficiently small, at each time step of the second phase the dynamics of neurons and synapses follow the gradients of the loss given by BPTT (Theorem 1).\nWe demonstrate training with C-EP on MNIST and generalize C-EP to neural networks where neurons are connected by asymmetric connections. We show through experiments that the more the network updates follows the gradients of BPTT, the best it performs in terms of training. These results bring EP a step closer to biology while maintaining its intimate link with backpropagation.", "code": "https://drive.google.com/open?id=1oZtzBTu8zZgvAopyK2sQg2bpcsrzwTrp", "keywords": ["Biologically Plausible Neural Networks", "Equilibrium Propagation"], "paperhash": "ernoult|equilibrium_propagation_with_continual_weight_updates", "original_pdf": "/attachment/b9705324e7ca49bf396f1ea3160b2c94dc7c3076.pdf", "_bibtex": "@misc{\nernoult2020equilibrium,\ntitle={Equilibrium Propagation with Continual Weight Updates},\nauthor={Maxence Ernoult and Julie Grollier and Damien Querlioz and Yoshua Bengio and Benjamin Scellier},\nyear={2020},\nurl={https://openreview.net/forum?id=H1xJhJStPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1xJhJStPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1937/Authors", "ICLR.cc/2020/Conference/Paper1937/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1937/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1937/Reviewers", "ICLR.cc/2020/Conference/Paper1937/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1937/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1937/Authors|ICLR.cc/2020/Conference/Paper1937/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148702, "tmdate": 1576860532494, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1937/Authors", "ICLR.cc/2020/Conference/Paper1937/Reviewers", "ICLR.cc/2020/Conference/Paper1937/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1937/-/Official_Comment"}}}, {"id": "SJlBGB8Nir", "original": null, "number": 5, "cdate": 1573311756791, "ddate": null, "tcdate": 1573311756791, "tmdate": 1573311756791, "tddate": null, "forum": "H1xJhJStPS", "replyto": "H1e010yl9r", "invitation": "ICLR.cc/2020/Conference/Paper1937/-/Official_Comment", "content": {"title": "Answer to Review #2", "comment": "We thank the reviewer for his/her comments, and are happy that he/she appreciated our work."}, "signatures": ["ICLR.cc/2020/Conference/Paper1937/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1937/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maxence.ernoult@u-psud.fr", "julie.grollier@cnrs-thales.fr", "damien.querlioz@u-psud.fr", "yoshua.bengio@mila.quebec", "benjamin.scellier@umontreal.ca"], "title": "Equilibrium Propagation with Continual Weight Updates", "authors": ["Maxence Ernoult", "Julie Grollier", "Damien Querlioz", "Yoshua Bengio", "Benjamin Scellier"], "pdf": "/pdf/9d45be17d2ebaf971278460a76bde19d4d5d0291.pdf", "TL;DR": "We propose a continual version of Equilibrium Propagation, where neuron and synapse dynamics occur simultaneously throughout the second phase, with theoretical guarantees and numerical simulations.", "abstract": "Equilibrium Propagation (EP) is a learning algorithm that bridges Machine Learning and Neuroscience, by computing gradients closely matching those of Backpropagation Through Time (BPTT), but with a learning rule local in space.\nGiven an input x and associated target y, EP proceeds in two phases: in the first phase neurons evolve freely towards a first steady state; in the second phase output neurons are nudged towards y until they reach a second steady state.\nHowever, in existing implementations of EP, the learning rule is not local in time:\nthe weight update is performed after the dynamics of the second phase have converged and requires information of the first phase that is no longer available physically.\nThis is a major impediment to the biological plausibility of EP and its efficient hardware implementation.\nIn this work, we propose a version of EP named Continual Equilibrium Propagation (C-EP) where neuron and synapse dynamics occur simultaneously throughout the second phase, so that the weight update becomes local in time. We prove theoretically that, provided the learning rates are sufficiently small, at each time step of the second phase the dynamics of neurons and synapses follow the gradients of the loss given by BPTT (Theorem 1).\nWe demonstrate training with C-EP on MNIST and generalize C-EP to neural networks where neurons are connected by asymmetric connections. We show through experiments that the more the network updates follows the gradients of BPTT, the best it performs in terms of training. These results bring EP a step closer to biology while maintaining its intimate link with backpropagation.", "code": "https://drive.google.com/open?id=1oZtzBTu8zZgvAopyK2sQg2bpcsrzwTrp", "keywords": ["Biologically Plausible Neural Networks", "Equilibrium Propagation"], "paperhash": "ernoult|equilibrium_propagation_with_continual_weight_updates", "original_pdf": "/attachment/b9705324e7ca49bf396f1ea3160b2c94dc7c3076.pdf", "_bibtex": "@misc{\nernoult2020equilibrium,\ntitle={Equilibrium Propagation with Continual Weight Updates},\nauthor={Maxence Ernoult and Julie Grollier and Damien Querlioz and Yoshua Bengio and Benjamin Scellier},\nyear={2020},\nurl={https://openreview.net/forum?id=H1xJhJStPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1xJhJStPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1937/Authors", "ICLR.cc/2020/Conference/Paper1937/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1937/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1937/Reviewers", "ICLR.cc/2020/Conference/Paper1937/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1937/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1937/Authors|ICLR.cc/2020/Conference/Paper1937/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148702, "tmdate": 1576860532494, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1937/Authors", "ICLR.cc/2020/Conference/Paper1937/Reviewers", "ICLR.cc/2020/Conference/Paper1937/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1937/-/Official_Comment"}}}, {"id": "SyxII4IEiB", "original": null, "number": 3, "cdate": 1573311566278, "ddate": null, "tcdate": 1573311566278, "tmdate": 1573311566278, "tddate": null, "forum": "H1xJhJStPS", "replyto": "H1ewjm8EiH", "invitation": "ICLR.cc/2020/Conference/Paper1937/-/Official_Comment", "content": {"title": "Properties of the transition function F", "comment": "2 - Concerning the properties of the transition function F:\n\nA) First of all, in the revised version of the manuscript we are going to clarify the primary goal of our work, which is to address two issues related to the biological plausibility of EP: the first is the fact that the learning rule of EP is not local in time, the second is the requirement of a primitive function  $\\Phi$ for the transition function $F$.\n\nC-EP solves the first problem but not the second: it still relies on the biologically unrealistic assumption that $F=\\frac{\\partial \\Phi}{\\partial s}$. This is precisely this constraint that motivates the second part of our work, where we introduce the C-VF model that gets rid of this assumption.\n\nB) In our \"C-EP model\" with a symmetric weight matrix $W$ (section 4.1), the transition function $F$ (almost) derives from a primitive function $\\Phi$.\nThis property is true for any topology (not just a fully connected recurrent network) as long as existing connections have symmetric values: this includes networks with multiple layers (deep networks), in which case the variable $s$ represents the concatenation of all the layers of neurons, and the weight matrix $W$ is a block sparse concatenation of all the layers of weights. More explicitly, denoting the layers of neurons $s^0$, $s^1$, ..., $s^N$, with $W_{n, n+1}$ connecting the layers $s^n$ and $s^{n+1}$ (in both directions), then $s = (s^0, s^1, \\dots, s^N)^\\top$ and\n\n$W =\n\\begin{bmatrix} \n0           & W_{01}      & 0           & 0      & 0              & 0         \\\\\nW_{01}^\\top & 0           & W_{12}      & 0      & 0              & 0         \\\\\n0           & W_{12}^\\top & 0           & W_{23} & 0              & 0         \\\\\n0           & 0           & W_{23}^\\top & 0      & \\ddots         & 0         \\\\\n0           & 0           & 0           & \\ddots & 0              & W_{N-1,N} \\\\\n0           & 0           & 0           & 0      & W_{N-1,N}^\\top & 0\n\\end{bmatrix}$\n\nWe propose to add this clarification in Appendix~E.\n\nAlternatively, to see why $F$ (almost) derives from a function $\\Phi$ in the setting with multiple layers, it is also possible to directly redefine the function $\\Phi$ in this specific case: we define  $\\Phi = \\sum_{n} (s^n)^\\top \\cdot W_{n, n+1}\\cdot s^{n+1}$. In the revised version of the manuscript, we are going to amend Appendix~E, in which the models with multiple layers are detailed, by writing explicitly the form of the function $\\Phi$ for each of them.\n\nC) In the C-VF model of section 4.1, the weight matrix is no longer assumed to be symmetric, thus there is no primitive function $\\Phi$, and therefore Theorem 1 does not apply.\nAlthough our study of the C-VF model is mostly experimental, we also prove a generalisation of the GDD theorem that holds in this more general setting (Theorem 2 in Appendix D.2). Fig.5 illustrates this generalisation of the GDD theorem.\nWe have clarified this in the paragraph after Eq.(11) in the revised manuscript.\n\nD) We clarify here a point that we had not explained in our manuscript.\nThe theory of our paper (section 3) directly assumes a function $\\Phi$ and defines the transition function as $F = \\frac{\\partial \\Phi}{\\partial s}$.\nIn the experimental section however (section 4), we proceed the other way around: we first define $F$, then show the existence of a $\\Phi$ such that $F \\approx \\frac{\\partial \\Phi}{\\partial s}$, which $\\Phi$ can finally be used to compute the quantities of the form $\\frac{\\partial \\Phi}{\\partial \\theta}$ required in the learning rule.\n\nMore concretely, let us consider the case of the C-EP model of section 4.1.\nWe first define the dynamics $s_{t+1} = \\sigma(W\\cdot s_t)$.\nThis dynamics can be rewritten in the form $s_{t+1} = F(s_t,W)$ with the transition function $F(s,W) = \\sigma(W\\cdot s)$.\nIn this case, if we define $\\Phi(s,W) = \\frac{1}{2}s^\\top\\cdot W\\cdot s$, we can compute $\\frac{\\partial \\Phi}{\\partial s} = W\\cdot s$,\nand then notice that $F \\approx \\frac{\\partial \\Phi}{\\partial s}$ if we ignore $\\sigma$.\nNow that we have the analytical expression of $\\Phi$, we can also use it to compute $\\frac{\\partial \\Phi}{\\partial W}(s,W) = s^\\top \\cdot s$.\nFinally we can compute the forward-time gradient of C-EP, which reads $\\Delta_W^{\\rm C-EP}(\\beta,\\eta,t) = \\frac{1}{\\beta} \\left( s_{t+1}^{{\\beta,\\eta}^\\top} \\cdot s_{t+1}^{\\beta,\\eta} - s_t^{{\\beta,\\eta}^\\top} \\cdot s_t^{\\beta,\\eta} \\right)$."}, "signatures": ["ICLR.cc/2020/Conference/Paper1937/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1937/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maxence.ernoult@u-psud.fr", "julie.grollier@cnrs-thales.fr", "damien.querlioz@u-psud.fr", "yoshua.bengio@mila.quebec", "benjamin.scellier@umontreal.ca"], "title": "Equilibrium Propagation with Continual Weight Updates", "authors": ["Maxence Ernoult", "Julie Grollier", "Damien Querlioz", "Yoshua Bengio", "Benjamin Scellier"], "pdf": "/pdf/9d45be17d2ebaf971278460a76bde19d4d5d0291.pdf", "TL;DR": "We propose a continual version of Equilibrium Propagation, where neuron and synapse dynamics occur simultaneously throughout the second phase, with theoretical guarantees and numerical simulations.", "abstract": "Equilibrium Propagation (EP) is a learning algorithm that bridges Machine Learning and Neuroscience, by computing gradients closely matching those of Backpropagation Through Time (BPTT), but with a learning rule local in space.\nGiven an input x and associated target y, EP proceeds in two phases: in the first phase neurons evolve freely towards a first steady state; in the second phase output neurons are nudged towards y until they reach a second steady state.\nHowever, in existing implementations of EP, the learning rule is not local in time:\nthe weight update is performed after the dynamics of the second phase have converged and requires information of the first phase that is no longer available physically.\nThis is a major impediment to the biological plausibility of EP and its efficient hardware implementation.\nIn this work, we propose a version of EP named Continual Equilibrium Propagation (C-EP) where neuron and synapse dynamics occur simultaneously throughout the second phase, so that the weight update becomes local in time. We prove theoretically that, provided the learning rates are sufficiently small, at each time step of the second phase the dynamics of neurons and synapses follow the gradients of the loss given by BPTT (Theorem 1).\nWe demonstrate training with C-EP on MNIST and generalize C-EP to neural networks where neurons are connected by asymmetric connections. We show through experiments that the more the network updates follows the gradients of BPTT, the best it performs in terms of training. These results bring EP a step closer to biology while maintaining its intimate link with backpropagation.", "code": "https://drive.google.com/open?id=1oZtzBTu8zZgvAopyK2sQg2bpcsrzwTrp", "keywords": ["Biologically Plausible Neural Networks", "Equilibrium Propagation"], "paperhash": "ernoult|equilibrium_propagation_with_continual_weight_updates", "original_pdf": "/attachment/b9705324e7ca49bf396f1ea3160b2c94dc7c3076.pdf", "_bibtex": "@misc{\nernoult2020equilibrium,\ntitle={Equilibrium Propagation with Continual Weight Updates},\nauthor={Maxence Ernoult and Julie Grollier and Damien Querlioz and Yoshua Bengio and Benjamin Scellier},\nyear={2020},\nurl={https://openreview.net/forum?id=H1xJhJStPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1xJhJStPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1937/Authors", "ICLR.cc/2020/Conference/Paper1937/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1937/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1937/Reviewers", "ICLR.cc/2020/Conference/Paper1937/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1937/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1937/Authors|ICLR.cc/2020/Conference/Paper1937/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148702, "tmdate": 1576860532494, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1937/Authors", "ICLR.cc/2020/Conference/Paper1937/Reviewers", "ICLR.cc/2020/Conference/Paper1937/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1937/-/Official_Comment"}}}, {"id": "rygnS_8nYH", "original": null, "number": 1, "cdate": 1571739716481, "ddate": null, "tcdate": 1571739716481, "tmdate": 1572972404356, "tddate": null, "forum": "H1xJhJStPS", "replyto": "H1xJhJStPS", "invitation": "ICLR.cc/2020/Conference/Paper1937/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper is concerned with biologically plausible models of learning. It takes equilibrium propagation -- where updates depend on local spatial information, in the sense that the information is available at each neuron -- and modifies the algorithm so updates are also local in time, thus obtaining C-EP. The key insight is that the updates in EP can be written as a telescoping sum over time points, eq (5).\n\nMaking equilibrium propagation more biologically plausible is an interesting technical contribution. But, taking a step back, the setup is misguided. It is true that humans can solve classification problems. And various animals can be trained to do so as well. However, it should be obvious that animals learn to solve these problems by reinforcement learning -- they are literally given rewards like sugar water for correct answers. \n\nMNIST is an unusual dataset with a stark constrast between foreground and background that is far from biologically plausible. I know it has a long and important history in machine learning, but if you are interested in biologically plausible learning then it is simply the wrong dataset to start with from both an evolutionary and developmental perspective. It\u2019s not the kind of problem evolution started with, nor is it the kind of problem human babies start with. \n\nMaybe C-EP can be repurposed into a component of some much larger, biologically plausible learning system that does a mixture of RL and unsupervised learning. Maybe not. The MNIST results provide no indication.\n\nThe authors have done a lot of solid work analysing BPTT, RBT, and C-EP. I suspect they are far more interested in understanding and designing efficient mechanisms for temporal credit assignment than they are in biological learning. That work can and should stand on its own feet.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1937/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1937/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maxence.ernoult@u-psud.fr", "julie.grollier@cnrs-thales.fr", "damien.querlioz@u-psud.fr", "yoshua.bengio@mila.quebec", "benjamin.scellier@umontreal.ca"], "title": "Equilibrium Propagation with Continual Weight Updates", "authors": ["Maxence Ernoult", "Julie Grollier", "Damien Querlioz", "Yoshua Bengio", "Benjamin Scellier"], "pdf": "/pdf/9d45be17d2ebaf971278460a76bde19d4d5d0291.pdf", "TL;DR": "We propose a continual version of Equilibrium Propagation, where neuron and synapse dynamics occur simultaneously throughout the second phase, with theoretical guarantees and numerical simulations.", "abstract": "Equilibrium Propagation (EP) is a learning algorithm that bridges Machine Learning and Neuroscience, by computing gradients closely matching those of Backpropagation Through Time (BPTT), but with a learning rule local in space.\nGiven an input x and associated target y, EP proceeds in two phases: in the first phase neurons evolve freely towards a first steady state; in the second phase output neurons are nudged towards y until they reach a second steady state.\nHowever, in existing implementations of EP, the learning rule is not local in time:\nthe weight update is performed after the dynamics of the second phase have converged and requires information of the first phase that is no longer available physically.\nThis is a major impediment to the biological plausibility of EP and its efficient hardware implementation.\nIn this work, we propose a version of EP named Continual Equilibrium Propagation (C-EP) where neuron and synapse dynamics occur simultaneously throughout the second phase, so that the weight update becomes local in time. We prove theoretically that, provided the learning rates are sufficiently small, at each time step of the second phase the dynamics of neurons and synapses follow the gradients of the loss given by BPTT (Theorem 1).\nWe demonstrate training with C-EP on MNIST and generalize C-EP to neural networks where neurons are connected by asymmetric connections. We show through experiments that the more the network updates follows the gradients of BPTT, the best it performs in terms of training. These results bring EP a step closer to biology while maintaining its intimate link with backpropagation.", "code": "https://drive.google.com/open?id=1oZtzBTu8zZgvAopyK2sQg2bpcsrzwTrp", "keywords": ["Biologically Plausible Neural Networks", "Equilibrium Propagation"], "paperhash": "ernoult|equilibrium_propagation_with_continual_weight_updates", "original_pdf": "/attachment/b9705324e7ca49bf396f1ea3160b2c94dc7c3076.pdf", "_bibtex": "@misc{\nernoult2020equilibrium,\ntitle={Equilibrium Propagation with Continual Weight Updates},\nauthor={Maxence Ernoult and Julie Grollier and Damien Querlioz and Yoshua Bengio and Benjamin Scellier},\nyear={2020},\nurl={https://openreview.net/forum?id=H1xJhJStPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1xJhJStPS", "replyto": "H1xJhJStPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1937/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1937/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575770688346, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1937/Reviewers"], "noninvitees": [], "tcdate": 1570237730130, "tmdate": 1575770688361, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1937/-/Official_Review"}}}, {"id": "BkxSJJoJ9H", "original": null, "number": 2, "cdate": 1571954397328, "ddate": null, "tcdate": 1571954397328, "tmdate": 1572972404319, "tddate": null, "forum": "H1xJhJStPS", "replyto": "H1xJhJStPS", "invitation": "ICLR.cc/2020/Conference/Paper1937/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "I think it is an intriguing paper, but unfortunately left me a bit confused. I have to admit is not a topic I'm really versed in, so it might be that this affected my evaluation of the work. But also, as a paper submitted to ICLR I would expect the paper to be self-contained and be able to provide all the details needed. \n\nI do appreciate the authors providing in the appendix the proofs of the other theorems even if they come form other works. \n\n\nThe paper introduces C-EP, an extension of  a previously introduced algorithm EP, such that it becomes biologically plausible. In particular EP is local in space but not in time (you need the steady state of the recurrent state after the first stage at the end of the second stage to get your gradients). I think this is fair, and the need for biological plausibility is well motivated in the beginning of the work.  \n\nMy first issue is with the proof for the equivalence between EP and C-EP. This is done by taking the limit of eta goes to 0. I think I must be missing something. But the proof relies on eta being small enough such that \\theta_i = \\theta (i.e. theta does not change). Given this state evolves the same way as for EP, because we basically not changing theta. \nYet the crux of my issue is exactly here. The proof relies on the fact that we don't change theta. So then when you converged on the second phase, isn't theta the same as theta_0? So you haven't actually learned anything!? Basically looking at the delta isn't this just misleading? \nOk lets assume that on the last step you allow yourself to change eta to be non-zero. (I.e. we are just after the delta in theta, and what to show we can get the same delta in theta as EP which is how the proof is phrased). Then in that difference aren't you looking at s_{t+1} and s_t rather than s_{t+1} and s_0, which is what EP would do? In EP you have s^\\beta_* - s_*. This is not what you get if you don't update theta and apply C-EP? \n\nI think there might be something I'm missing about the mathematical argument here. \n\nAt a higher-level question, we talk about the transition function F as being a gradient vector field, i.e. there exist a phi such that F is d phi/d theta.  Why is this assumption biologically plausable ? Parametrizing gradient vector fields in general is far from trivial, and require very specific structure of the neural implementation of F to be true. Several works have looked at parametrizing gradient vector fields (https://arxiv.org/abs/1906.01563, https://arxiv.org/pdf/1608.05343.pdf) and the answer is that without parametrizing it by actually taking the gradient of a function there is not much of a choice. \nIncidentally, here we exploit that  F = sigma (Wx), with W symmetric. This is a paramtrization of a gradient vector field, i.e. of xU, where UU^T =W I think. But if you want to make F deep than it becomes non-trivial to restrict it to gradient vector field. Is the assumption that we never want to move away from vanilla RNNs? And W symmetric is also not biologically plausible. In C-EP you say is not needed to be symmetric, but that implicitly means there exist no phi and everything that follows breaks, no? \n\nI'm also confused by how one has access to d phi / ds and d phi / d theta. Again I feel like I'm missing information and the formalism is not introduced in a way that it is easy to parse. My understand is that you have an RNN that updates the state s. And the transfer function of this RNN is meant to be d phi / ds, which is trues if the recurrent weight is symmetric. Fine. But then why do we have access to d phi/ dtheta? Who is this function? Is the assumption that d s / dtheta is something we can compute in a biologically plausible way? Is this something that is obvious? \n \n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1937/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1937/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maxence.ernoult@u-psud.fr", "julie.grollier@cnrs-thales.fr", "damien.querlioz@u-psud.fr", "yoshua.bengio@mila.quebec", "benjamin.scellier@umontreal.ca"], "title": "Equilibrium Propagation with Continual Weight Updates", "authors": ["Maxence Ernoult", "Julie Grollier", "Damien Querlioz", "Yoshua Bengio", "Benjamin Scellier"], "pdf": "/pdf/9d45be17d2ebaf971278460a76bde19d4d5d0291.pdf", "TL;DR": "We propose a continual version of Equilibrium Propagation, where neuron and synapse dynamics occur simultaneously throughout the second phase, with theoretical guarantees and numerical simulations.", "abstract": "Equilibrium Propagation (EP) is a learning algorithm that bridges Machine Learning and Neuroscience, by computing gradients closely matching those of Backpropagation Through Time (BPTT), but with a learning rule local in space.\nGiven an input x and associated target y, EP proceeds in two phases: in the first phase neurons evolve freely towards a first steady state; in the second phase output neurons are nudged towards y until they reach a second steady state.\nHowever, in existing implementations of EP, the learning rule is not local in time:\nthe weight update is performed after the dynamics of the second phase have converged and requires information of the first phase that is no longer available physically.\nThis is a major impediment to the biological plausibility of EP and its efficient hardware implementation.\nIn this work, we propose a version of EP named Continual Equilibrium Propagation (C-EP) where neuron and synapse dynamics occur simultaneously throughout the second phase, so that the weight update becomes local in time. We prove theoretically that, provided the learning rates are sufficiently small, at each time step of the second phase the dynamics of neurons and synapses follow the gradients of the loss given by BPTT (Theorem 1).\nWe demonstrate training with C-EP on MNIST and generalize C-EP to neural networks where neurons are connected by asymmetric connections. We show through experiments that the more the network updates follows the gradients of BPTT, the best it performs in terms of training. These results bring EP a step closer to biology while maintaining its intimate link with backpropagation.", "code": "https://drive.google.com/open?id=1oZtzBTu8zZgvAopyK2sQg2bpcsrzwTrp", "keywords": ["Biologically Plausible Neural Networks", "Equilibrium Propagation"], "paperhash": "ernoult|equilibrium_propagation_with_continual_weight_updates", "original_pdf": "/attachment/b9705324e7ca49bf396f1ea3160b2c94dc7c3076.pdf", "_bibtex": "@misc{\nernoult2020equilibrium,\ntitle={Equilibrium Propagation with Continual Weight Updates},\nauthor={Maxence Ernoult and Julie Grollier and Damien Querlioz and Yoshua Bengio and Benjamin Scellier},\nyear={2020},\nurl={https://openreview.net/forum?id=H1xJhJStPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1xJhJStPS", "replyto": "H1xJhJStPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1937/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1937/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575770688346, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1937/Reviewers"], "noninvitees": [], "tcdate": 1570237730130, "tmdate": 1575770688361, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1937/-/Official_Review"}}}, {"id": "H1e010yl9r", "original": null, "number": 3, "cdate": 1571974630314, "ddate": null, "tcdate": 1571974630314, "tmdate": 1572972404266, "tddate": null, "forum": "H1xJhJStPS", "replyto": "H1xJhJStPS", "invitation": "ICLR.cc/2020/Conference/Paper1937/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: this paper introduces a new variant of equilibrium propagation algorithm that continually updates the weights making it unnecessary to save steady states. The also mathematically prove the GDD property and show the effectiveness of \n their algorithm (Continual-EP) on MNIST. They also show C-EP is conceptually closer to biological neurons than EP.\n\nThis paper tackles an important problem in bridging the gap between artificial neural networks and biological neurons. It is well-motivated and stands well in the literature as it improves its precedent algorithm (EP). The contributions are clear and well-supported by mathematical proofs. The experiments are accurately designed and results are convincing. I recommend accepting this paper as a plausible contribution to both fields. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1937/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1937/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["maxence.ernoult@u-psud.fr", "julie.grollier@cnrs-thales.fr", "damien.querlioz@u-psud.fr", "yoshua.bengio@mila.quebec", "benjamin.scellier@umontreal.ca"], "title": "Equilibrium Propagation with Continual Weight Updates", "authors": ["Maxence Ernoult", "Julie Grollier", "Damien Querlioz", "Yoshua Bengio", "Benjamin Scellier"], "pdf": "/pdf/9d45be17d2ebaf971278460a76bde19d4d5d0291.pdf", "TL;DR": "We propose a continual version of Equilibrium Propagation, where neuron and synapse dynamics occur simultaneously throughout the second phase, with theoretical guarantees and numerical simulations.", "abstract": "Equilibrium Propagation (EP) is a learning algorithm that bridges Machine Learning and Neuroscience, by computing gradients closely matching those of Backpropagation Through Time (BPTT), but with a learning rule local in space.\nGiven an input x and associated target y, EP proceeds in two phases: in the first phase neurons evolve freely towards a first steady state; in the second phase output neurons are nudged towards y until they reach a second steady state.\nHowever, in existing implementations of EP, the learning rule is not local in time:\nthe weight update is performed after the dynamics of the second phase have converged and requires information of the first phase that is no longer available physically.\nThis is a major impediment to the biological plausibility of EP and its efficient hardware implementation.\nIn this work, we propose a version of EP named Continual Equilibrium Propagation (C-EP) where neuron and synapse dynamics occur simultaneously throughout the second phase, so that the weight update becomes local in time. We prove theoretically that, provided the learning rates are sufficiently small, at each time step of the second phase the dynamics of neurons and synapses follow the gradients of the loss given by BPTT (Theorem 1).\nWe demonstrate training with C-EP on MNIST and generalize C-EP to neural networks where neurons are connected by asymmetric connections. We show through experiments that the more the network updates follows the gradients of BPTT, the best it performs in terms of training. These results bring EP a step closer to biology while maintaining its intimate link with backpropagation.", "code": "https://drive.google.com/open?id=1oZtzBTu8zZgvAopyK2sQg2bpcsrzwTrp", "keywords": ["Biologically Plausible Neural Networks", "Equilibrium Propagation"], "paperhash": "ernoult|equilibrium_propagation_with_continual_weight_updates", "original_pdf": "/attachment/b9705324e7ca49bf396f1ea3160b2c94dc7c3076.pdf", "_bibtex": "@misc{\nernoult2020equilibrium,\ntitle={Equilibrium Propagation with Continual Weight Updates},\nauthor={Maxence Ernoult and Julie Grollier and Damien Querlioz and Yoshua Bengio and Benjamin Scellier},\nyear={2020},\nurl={https://openreview.net/forum?id=H1xJhJStPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1xJhJStPS", "replyto": "H1xJhJStPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1937/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1937/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575770688346, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1937/Reviewers"], "noninvitees": [], "tcdate": 1570237730130, "tmdate": 1575770688361, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1937/-/Official_Review"}}}], "count": 10}