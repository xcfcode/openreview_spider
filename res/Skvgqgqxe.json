{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488568213301, "tcdate": 1478261231294, "number": 164, "id": "Skvgqgqxe", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Skvgqgqxe", "signatures": ["~Dani_Yogatama1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn\ntree-structured neural networks for computing representations of natural language sentences.\nIn contrast with prior work on tree-structured models, in which the trees are either provided as input or\npredicted using supervision from explicit treebank annotations,\nthe tree structures in this work are optimized to improve performance on a downstream task.\nExperiments demonstrate the benefit of\nlearning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations.\nWe analyze the induced trees and show that while they discover\nsome linguistically intuitive structures (e.g., noun phrases, simple verb phrases),\nthey are different than conventional English syntactic structures.", "pdf": "/pdf/806aec0935bef790b284e92b5f8975eea34a5103.pdf", "paperhash": "yogatama|learning_to_compose_words_into_sentences_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "authorids": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 24, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396403625, "tcdate": 1486396403625, "number": 1, "id": "rk2enGUdg", "invitation": "ICLR.cc/2017/conference/-/paper164/acceptance", "forum": "Skvgqgqxe", "replyto": "Skvgqgqxe", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "All the reviewers agreed that the research direction is very interesting, and generally find the results promising. We could quibble a bit about the results not being really state-of-the-art and the choice of baselines, but I think the main claims are well supported by the experiments (i.e. the induce grammar appears to be useful for the problem in question, within the specific class of models). There are still clearly many issues unresolved, and we are yet to see if this class of methods (RL / implicit structure-based) can lead to state-of-the-art results on any important NLP problem. But it is too much to ask from a submission. I see the paper as a strong contribution to the conference.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn\ntree-structured neural networks for computing representations of natural language sentences.\nIn contrast with prior work on tree-structured models, in which the trees are either provided as input or\npredicted using supervision from explicit treebank annotations,\nthe tree structures in this work are optimized to improve performance on a downstream task.\nExperiments demonstrate the benefit of\nlearning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations.\nWe analyze the induced trees and show that while they discover\nsome linguistically intuitive structures (e.g., noun phrases, simple verb phrases),\nthey are different than conventional English syntactic structures.", "pdf": "/pdf/806aec0935bef790b284e92b5f8975eea34a5103.pdf", "paperhash": "yogatama|learning_to_compose_words_into_sentences_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "authorids": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396405451, "id": "ICLR.cc/2017/conference/-/paper164/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Skvgqgqxe", "replyto": "Skvgqgqxe", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396405451}}}, {"tddate": null, "tmdate": 1484960310475, "tcdate": 1481916896274, "number": 1, "id": "B1OyMaWNg", "invitation": "ICLR.cc/2017/conference/-/paper164/official/review", "forum": "Skvgqgqxe", "replyto": "Skvgqgqxe", "signatures": ["ICLR.cc/2017/conference/paper164/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper164/AnonReviewer1"], "content": {"title": "Weak experimental results", "rating": "6: Marginally above acceptance threshold", "review": "In this paper, the authors propose a new method to learn hierarchical representations of sentences, based on reinforcement learning. They propose to learn a neural shift-reduce parser, such that the induced tree structures lead to good performance on a downstream task. They use reinforcement learning (more specifically, the policy gradient method REINFORCE) to learn their model. The reward of the algorithm is the evaluation metric of the downstream task. The authors compare two settings, (1) no structure information is given (hence, the only supervision comes from the downstream task) and (2) actions from an external parser is used as supervision to train the policy network, in addition to the supervision from the downstream task. The proposed approach is evaluated on four tasks: sentiment analysis, semantic relatedness, textual entailment and sentence generation.\n\nI like the idea of learning tree representations of text which are useful for a downstream task. The paper is clear and well written. However, I am not convinced by the experimental results presented in the paper. Indeed, on most tasks, the proposed model is far from state-of-the-art models:\n - sentiment analysis, 86.5 v.s. 89.7 (accuracy);\n - semantic relatedness, 0.32 v.s. 0.25 (MSE);\n - textual entailment, 80.5 v.s. 84.6 (accuracy).\nFrom the results presented in the paper, it is hard to know if these results are due to the model, or because of the reinforcement learning algorithm.\n\nPROS:\n - interesting idea: learning structures of sentences adapted for a downstream task.\n - well written paper.\nCONS:\n - weak experimental results (do not really support the claim of the authors).\n\nMinor comments:\nIn the second paragraph of the introduction, one might argue that bag-of-words is also a predominant approach to represent sentences.\nParagraph titles (e.g. in section 3.2) should have a period at the end.\n\n----------------------------------------------------------------------------------------------------------------------\nUPDATE\n\nI am still not convinced by the results presented in the paper, and in particular by the fact that one must combine the words in a different way than left-to-right to obtain state of the art results.\nHowever, I do agree that this is an interesting research direction, and that the results presented in the paper are promising. I am thus updating my score from 5 to 6.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn\ntree-structured neural networks for computing representations of natural language sentences.\nIn contrast with prior work on tree-structured models, in which the trees are either provided as input or\npredicted using supervision from explicit treebank annotations,\nthe tree structures in this work are optimized to improve performance on a downstream task.\nExperiments demonstrate the benefit of\nlearning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations.\nWe analyze the induced trees and show that while they discover\nsome linguistically intuitive structures (e.g., noun phrases, simple verb phrases),\nthey are different than conventional English syntactic structures.", "pdf": "/pdf/806aec0935bef790b284e92b5f8975eea34a5103.pdf", "paperhash": "yogatama|learning_to_compose_words_into_sentences_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "authorids": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482545730188, "id": "ICLR.cc/2017/conference/-/paper164/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper164/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper164/AnonReviewer1", "ICLR.cc/2017/conference/paper164/AnonReviewer2", "ICLR.cc/2017/conference/paper164/AnonReviewer3"], "reply": {"forum": "Skvgqgqxe", "replyto": "Skvgqgqxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper164/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper164/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482545730188}}}, {"tddate": null, "tmdate": 1482545729597, "tcdate": 1482545729597, "number": 3, "id": "r19SqUiNe", "invitation": "ICLR.cc/2017/conference/-/paper164/official/review", "forum": "Skvgqgqxe", "replyto": "Skvgqgqxe", "signatures": ["ICLR.cc/2017/conference/paper164/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper164/AnonReviewer3"], "content": {"title": "Accept", "rating": "7: Good paper, accept", "review": "I have not much to add to my pre-review comments.\nIt's a very well written paper with an interesting idea.\nLots of people currently want to combine RL with NLP. It is very en vogue.\nNobody has gotten that to work yet in any really groundbreaking or influential way that results in actually superior performance on any highly relevant or competitive NLP task.\nMost people struggle with the fact that NLP requires very efficient methods on very large datasets and RL is super slow.\nHence, I believe this direction hasn't shown much promise yet and it's not yet clear it ever will due to the slowness of RL.\nBut many directions need to be explored and maybe eventually they will reach a point where they become relevant.\n\nIt is interesting to learn the obviously inherent grammatical structure in language though sadly again, the trees here do not yet capture much of what our intuitions are.\n\nRegardless, it's an interesting exploration, worthy of being discussed at the conference.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn\ntree-structured neural networks for computing representations of natural language sentences.\nIn contrast with prior work on tree-structured models, in which the trees are either provided as input or\npredicted using supervision from explicit treebank annotations,\nthe tree structures in this work are optimized to improve performance on a downstream task.\nExperiments demonstrate the benefit of\nlearning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations.\nWe analyze the induced trees and show that while they discover\nsome linguistically intuitive structures (e.g., noun phrases, simple verb phrases),\nthey are different than conventional English syntactic structures.", "pdf": "/pdf/806aec0935bef790b284e92b5f8975eea34a5103.pdf", "paperhash": "yogatama|learning_to_compose_words_into_sentences_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "authorids": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482545730188, "id": "ICLR.cc/2017/conference/-/paper164/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper164/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper164/AnonReviewer1", "ICLR.cc/2017/conference/paper164/AnonReviewer2", "ICLR.cc/2017/conference/paper164/AnonReviewer3"], "reply": {"forum": "Skvgqgqxe", "replyto": "Skvgqgqxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper164/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper164/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482545730188}}}, {"tddate": null, "tmdate": 1482169164366, "tcdate": 1482168902010, "number": 2, "id": "rkCS99SVl", "invitation": "ICLR.cc/2017/conference/-/paper164/official/review", "forum": "Skvgqgqxe", "replyto": "Skvgqgqxe", "signatures": ["ICLR.cc/2017/conference/paper164/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper164/AnonReviewer2"], "content": {"title": "official review", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The paper proposes to use reinforcement learning to learn how to compose the words in a sentence, i.e. parse tree, that can be helpful for the downstream tasks. To do that, the shift-reduce framework is employed and RL is used to learn the policy of the two actions SHIFT and REDUCE. The experiments on four datasets (SST, SICK, IMDB, and SNLI) show that the proposed approach outperformed the approach using predefined tree structures (e.g. left-to-right, right-to-left). \n\nThe paper is well written and has two good points. Firstly, the idea of using RL to learn parse trees using downstream tasks is very interesting and novel. And employing the shift-reduce framework is a very smart choice because the set of actions is minimal (shift and reduce). Secondly, what shown in the paper somewhat confirms the need of parse trees. This is indeed interesting because of the current debate on whether syntax is helpful.\n\nI have the following comments:\n- it seems that the authors weren't aware of some recent work using RL to learn structures for composition, e.g. Andreas et al (2016).\n- because different composition functions (e.g. LSTM, GRU, or classical recursive neural net) have different inductive biases, I was wondering if the tree structures found by the model would be independent from the composition function choice.\n- because RNNs in theory are equivalent to Turing machines, I was wondering if restricting the expressiveness of the model (e.g. reducing the dimension) can help the model focus on discovering more helpful tree structures.\n\nRef:\nAndreas et al. Learning to Compose Neural Networks for Question Answering. NAACL 2016", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn\ntree-structured neural networks for computing representations of natural language sentences.\nIn contrast with prior work on tree-structured models, in which the trees are either provided as input or\npredicted using supervision from explicit treebank annotations,\nthe tree structures in this work are optimized to improve performance on a downstream task.\nExperiments demonstrate the benefit of\nlearning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations.\nWe analyze the induced trees and show that while they discover\nsome linguistically intuitive structures (e.g., noun phrases, simple verb phrases),\nthey are different than conventional English syntactic structures.", "pdf": "/pdf/806aec0935bef790b284e92b5f8975eea34a5103.pdf", "paperhash": "yogatama|learning_to_compose_words_into_sentences_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "authorids": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482545730188, "id": "ICLR.cc/2017/conference/-/paper164/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper164/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper164/AnonReviewer1", "ICLR.cc/2017/conference/paper164/AnonReviewer2", "ICLR.cc/2017/conference/paper164/AnonReviewer3"], "reply": {"forum": "Skvgqgqxe", "replyto": "Skvgqgqxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper164/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper164/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482545730188}}}, {"tddate": null, "tmdate": 1481921772966, "tcdate": 1481921436251, "number": 16, "id": "ryNsmAZ4l", "invitation": "ICLR.cc/2017/conference/-/paper164/public/comment", "forum": "Skvgqgqxe", "replyto": "B1OyMaWNg", "signatures": ["~Dani_Yogatama1"], "readers": ["everyone"], "writers": ["~Dani_Yogatama1"], "content": {"title": "re: experiment results", "comment": "Thank you for your thoughtful review.\n\nWhile we completely agree that it would be nice to get SOTA results, we strongly disagree with the statement that our experimental results do not support our claim.\nOur claim is not that learning to compose words into sentences would give SOTA results, but that it is better *under similar experimental condition* than right to left, left to right, bidirectional, and supervised syntax orders.\n\nOur experiments were carefully designed to specifically compare different ways to compose words into sentences (e.g. left to right, supervised, latent syntax, etc.), using *exactly* the same model.\nThe hypothesis that we wanted to test in our experiments is whether learning composition orders can give better results for the same model architecture, precisely because, as stated in your review, we would like to know that the improvements come from using reinforcement learning, and not due to different model architectures.\nSince the only major difference among our models is in word composition orders, our results strongly demonstrate that learning task-specific composition orders with reinforcement learning improves performance on downstream tasks.\nWe performed these experiments on four different models (four tasks), and the results are consistent throughout these tasks.\n\nAdditionally, note that results such as 89.7 accuracy on sentiment analysis or 84.6 on textual entailment are obtained with different model architectures that are different to ours (NSE, Munkhdalai & Yu, 2016).\nWe provide these numbers for reference, but comparing our results with theirs gives little insight on whether learning to compose words into sentences with reinforcement learning is better than left to right, right to left, or supervised syntax composition orders.\n\nOn the sentiment analysis dataset, models that are most similar to ours (Tai et al., 2015)  achieved accuracies of 85.7 (dependency LSTM)  and 88.0 (constituency LSTM). Our reported result is 86.5, and our best result on subsequent experiment with bigger model is 87.1, which is in the range of results from other work using similar models.\n\nSimilarly, on the textual entailment task, the most similar model is Bowman et al., 2016, that achieves 83.2 for \"supervised syntax\" and 80.6 for \"left-to-right syntax\".\nWe've run experiments with 3.6 more parameters on this dataset (see our reply to comment from feedback from ICLR 2017 conference paper164 AnonReviewer3 below for more details), and obtained best result of 81.3 for latent syntax and 80.7 for left-to-right syntax (which is slightly higher than the left-to-right result from Bowman et al., 2016, although we still have fewer number of parameters).\n\nFor the SICK experiments, as we stated in one of our comments below, we were unable to replicate their experiment results after exhaustive tuning, even for the standard left to right LSTM (MSE 0.28). \nSince our experimental setup is nearly similar to Tai et al., except for the number of examples per minibatch (1 vs. 25), we hypothesize that on this very small dataset, training with minibatches is important to reduce the variance of the updates."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn\ntree-structured neural networks for computing representations of natural language sentences.\nIn contrast with prior work on tree-structured models, in which the trees are either provided as input or\npredicted using supervision from explicit treebank annotations,\nthe tree structures in this work are optimized to improve performance on a downstream task.\nExperiments demonstrate the benefit of\nlearning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations.\nWe analyze the induced trees and show that while they discover\nsome linguistically intuitive structures (e.g., noun phrases, simple verb phrases),\nthey are different than conventional English syntactic structures.", "pdf": "/pdf/806aec0935bef790b284e92b5f8975eea34a5103.pdf", "paperhash": "yogatama|learning_to_compose_words_into_sentences_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "authorids": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287704719, "id": "ICLR.cc/2017/conference/-/paper164/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Skvgqgqxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper164/reviewers", "ICLR.cc/2017/conference/paper164/areachairs"], "cdate": 1485287704719}}}, {"tddate": null, "tmdate": 1481514206100, "tcdate": 1481514206094, "number": 15, "id": "r1Uy6come", "invitation": "ICLR.cc/2017/conference/-/paper164/public/comment", "forum": "Skvgqgqxe", "replyto": "HkLRxTvmx", "signatures": ["~Dani_Yogatama1"], "readers": ["everyone"], "writers": ["~Dani_Yogatama1"], "content": {"title": "re: citation", "comment": "Thanks for spotting this, was meant to cite Socher et al., 2012 instead of Socher et al., 2011. Fixed."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn\ntree-structured neural networks for computing representations of natural language sentences.\nIn contrast with prior work on tree-structured models, in which the trees are either provided as input or\npredicted using supervision from explicit treebank annotations,\nthe tree structures in this work are optimized to improve performance on a downstream task.\nExperiments demonstrate the benefit of\nlearning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations.\nWe analyze the induced trees and show that while they discover\nsome linguistically intuitive structures (e.g., noun phrases, simple verb phrases),\nthey are different than conventional English syntactic structures.", "pdf": "/pdf/806aec0935bef790b284e92b5f8975eea34a5103.pdf", "paperhash": "yogatama|learning_to_compose_words_into_sentences_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "authorids": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287704719, "id": "ICLR.cc/2017/conference/-/paper164/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Skvgqgqxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper164/reviewers", "ICLR.cc/2017/conference/paper164/areachairs"], "cdate": 1485287704719}}}, {"tddate": null, "tmdate": 1481513204967, "tcdate": 1481513204960, "number": 14, "id": "rypet5j7g", "invitation": "ICLR.cc/2017/conference/-/paper164/public/comment", "forum": "Skvgqgqxe", "replyto": "SJc8CfiXe", "signatures": ["~Dani_Yogatama1"], "readers": ["everyone"], "writers": ["~Dani_Yogatama1"], "content": {"title": "re: SST experiments", "comment": "Thank you for your question.\n\nFor SST, similar to previous work on this dataset, we added the annotated phrases to the training data (i.e., each annotated phrase became a new training example).\n\nFor each training example, the model still only receives a reward after constructing the final tree and predicting its label.\nWhen learning from shorter phrases, it is true that the model does not need to perform many actions before receiving its reward.\nSince there are overlapping examples due to phrase-level annotations (e.g., \"this movie is good\", \"movie is good\", \"is good\", \"good\"), when learning trees for longer phrases (or the full sentence), the model could have an implicit bias towards composing shorter phrases that it has seen in the past first. \nHowever, the model still has the flexibility to try and settle on different structures if they turn out to be more predictive of the longer-phrase (or the full sentence) label, because the reward for a longer-phrase example does not use the shorter-phrase annotations. \nWe chose this setup since the final goal of the task is to predict sentiment at the sentence level. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn\ntree-structured neural networks for computing representations of natural language sentences.\nIn contrast with prior work on tree-structured models, in which the trees are either provided as input or\npredicted using supervision from explicit treebank annotations,\nthe tree structures in this work are optimized to improve performance on a downstream task.\nExperiments demonstrate the benefit of\nlearning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations.\nWe analyze the induced trees and show that while they discover\nsome linguistically intuitive structures (e.g., noun phrases, simple verb phrases),\nthey are different than conventional English syntactic structures.", "pdf": "/pdf/806aec0935bef790b284e92b5f8975eea34a5103.pdf", "paperhash": "yogatama|learning_to_compose_words_into_sentences_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "authorids": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287704719, "id": "ICLR.cc/2017/conference/-/paper164/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Skvgqgqxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper164/reviewers", "ICLR.cc/2017/conference/paper164/areachairs"], "cdate": 1485287704719}}}, {"tddate": null, "tmdate": 1481510305572, "tcdate": 1481510305565, "number": 13, "id": "BJqi6FiQe", "invitation": "ICLR.cc/2017/conference/-/paper164/public/comment", "forum": "Skvgqgqxe", "replyto": "rJGBe7oQl", "signatures": ["~Dani_Yogatama1"], "readers": ["everyone"], "writers": ["~Dani_Yogatama1"], "content": {"title": "re: implementation", "comment": "Thank you for your question. Our implementation is in DyNet. We used TensorFlow in our preliminary experiments but were unable to make it run as fast as DyNet for this specific problem so we performed all our experiments in DyNet."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn\ntree-structured neural networks for computing representations of natural language sentences.\nIn contrast with prior work on tree-structured models, in which the trees are either provided as input or\npredicted using supervision from explicit treebank annotations,\nthe tree structures in this work are optimized to improve performance on a downstream task.\nExperiments demonstrate the benefit of\nlearning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations.\nWe analyze the induced trees and show that while they discover\nsome linguistically intuitive structures (e.g., noun phrases, simple verb phrases),\nthey are different than conventional English syntactic structures.", "pdf": "/pdf/806aec0935bef790b284e92b5f8975eea34a5103.pdf", "paperhash": "yogatama|learning_to_compose_words_into_sentences_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "authorids": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287704719, "id": "ICLR.cc/2017/conference/-/paper164/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Skvgqgqxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper164/reviewers", "ICLR.cc/2017/conference/paper164/areachairs"], "cdate": 1485287704719}}}, {"tddate": null, "tmdate": 1481482298368, "tcdate": 1481482298360, "number": 2, "id": "rJGBe7oQl", "invitation": "ICLR.cc/2017/conference/-/paper164/official/comment", "forum": "Skvgqgqxe", "replyto": "Skvgqgqxe", "signatures": ["ICLR.cc/2017/conference/paper164/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper164/AnonReviewer2"], "content": {"title": "implementation ", "comment": "It was slow to train the model because you had to build a different new computational graph for each sentence. I was wondering if the slow speed is due to the way you implemented (e.g. tensorflow, python). Dynet is designed especially for this kind of problem, did you try it? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn\ntree-structured neural networks for computing representations of natural language sentences.\nIn contrast with prior work on tree-structured models, in which the trees are either provided as input or\npredicted using supervision from explicit treebank annotations,\nthe tree structures in this work are optimized to improve performance on a downstream task.\nExperiments demonstrate the benefit of\nlearning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations.\nWe analyze the induced trees and show that while they discover\nsome linguistically intuitive structures (e.g., noun phrases, simple verb phrases),\nthey are different than conventional English syntactic structures.", "pdf": "/pdf/806aec0935bef790b284e92b5f8975eea34a5103.pdf", "paperhash": "yogatama|learning_to_compose_words_into_sentences_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "authorids": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287704590, "id": "ICLR.cc/2017/conference/-/paper164/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "Skvgqgqxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper164/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper164/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper164/reviewers", "ICLR.cc/2017/conference/paper164/areachairs"], "cdate": 1485287704590}}}, {"tddate": null, "tmdate": 1481481809706, "tcdate": 1481481809698, "number": 3, "id": "SJc8CfiXe", "invitation": "ICLR.cc/2017/conference/-/paper164/pre-review/question", "forum": "Skvgqgqxe", "replyto": "Skvgqgqxe", "signatures": ["ICLR.cc/2017/conference/paper164/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper164/AnonReviewer2"], "content": {"title": "Experiments on SST", "question": "How did you train your model on SST? Because SST was annotated at phrase level (every internal node in a parse tree was given a sentiment label), I think you can train your model such that a reward is given after every (or few) action(s)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn\ntree-structured neural networks for computing representations of natural language sentences.\nIn contrast with prior work on tree-structured models, in which the trees are either provided as input or\npredicted using supervision from explicit treebank annotations,\nthe tree structures in this work are optimized to improve performance on a downstream task.\nExperiments demonstrate the benefit of\nlearning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations.\nWe analyze the induced trees and show that while they discover\nsome linguistically intuitive structures (e.g., noun phrases, simple verb phrases),\nthey are different than conventional English syntactic structures.", "pdf": "/pdf/806aec0935bef790b284e92b5f8975eea34a5103.pdf", "paperhash": "yogatama|learning_to_compose_words_into_sentences_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "authorids": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481481810334, "id": "ICLR.cc/2017/conference/-/paper164/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper164/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper164/AnonReviewer3", "ICLR.cc/2017/conference/paper164/AnonReviewer1", "ICLR.cc/2017/conference/paper164/AnonReviewer2"], "reply": {"forum": "Skvgqgqxe", "replyto": "Skvgqgqxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper164/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper164/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481481810334}}}, {"tddate": null, "tmdate": 1481261261564, "tcdate": 1481261261557, "number": 12, "id": "HkLRxTvmx", "invitation": "ICLR.cc/2017/conference/-/paper164/public/comment", "forum": "Skvgqgqxe", "replyto": "Skvgqgqxe", "signatures": ["~Kazuma_Hashimoto1"], "readers": ["everyone"], "writers": ["~Kazuma_Hashimoto1"], "content": {"title": "Previous work on semi-supervised binary tree constructions", "comment": "Dear authors,\n\nI've read the interesting paper and learned nice ideas.\n\nNow I've found somewhat incorrect mention in your paper.\nIn Introduction, it is said that trees are provided with sentences in Socher et al. (2011), but they jointly learn the binary tree structures according to the target task (sentiment classification) although the approach is different (reinforcement learning or autoencoder).\n\nBest,\n  Kazuma"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn\ntree-structured neural networks for computing representations of natural language sentences.\nIn contrast with prior work on tree-structured models, in which the trees are either provided as input or\npredicted using supervision from explicit treebank annotations,\nthe tree structures in this work are optimized to improve performance on a downstream task.\nExperiments demonstrate the benefit of\nlearning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations.\nWe analyze the induced trees and show that while they discover\nsome linguistically intuitive structures (e.g., noun phrases, simple verb phrases),\nthey are different than conventional English syntactic structures.", "pdf": "/pdf/806aec0935bef790b284e92b5f8975eea34a5103.pdf", "paperhash": "yogatama|learning_to_compose_words_into_sentences_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "authorids": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287704719, "id": "ICLR.cc/2017/conference/-/paper164/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Skvgqgqxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper164/reviewers", "ICLR.cc/2017/conference/paper164/areachairs"], "cdate": 1485287704719}}}, {"tddate": null, "tmdate": 1481234838495, "tcdate": 1481234838490, "number": 11, "id": "ryC5F8vme", "invitation": "ICLR.cc/2017/conference/-/paper164/public/comment", "forum": "Skvgqgqxe", "replyto": "SkgRxIIZe", "signatures": ["~Dani_Yogatama1"], "readers": ["everyone"], "writers": ["~Dani_Yogatama1"], "content": {"title": "binary tree results", "comment": "We have run the experiments and obtained the following results:\n- Stanford Sentiment Treebank: 85.1\n- Semantic relatedness: 0.455\n- SNLI: 77.4\n- Sentence generation: 103.3\n\nWe think that these results make sense since there is no intuitive justification that words should be composed into sentences according to a balanced binary tree structure.\nWe will include the results in the next version of the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn\ntree-structured neural networks for computing representations of natural language sentences.\nIn contrast with prior work on tree-structured models, in which the trees are either provided as input or\npredicted using supervision from explicit treebank annotations,\nthe tree structures in this work are optimized to improve performance on a downstream task.\nExperiments demonstrate the benefit of\nlearning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations.\nWe analyze the induced trees and show that while they discover\nsome linguistically intuitive structures (e.g., noun phrases, simple verb phrases),\nthey are different than conventional English syntactic structures.", "pdf": "/pdf/806aec0935bef790b284e92b5f8975eea34a5103.pdf", "paperhash": "yogatama|learning_to_compose_words_into_sentences_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "authorids": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287704719, "id": "ICLR.cc/2017/conference/-/paper164/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Skvgqgqxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper164/reviewers", "ICLR.cc/2017/conference/paper164/areachairs"], "cdate": 1485287704719}}}, {"tddate": null, "tmdate": 1481234087403, "tcdate": 1481234087397, "number": 10, "id": "rJk28LvQl", "invitation": "ICLR.cc/2017/conference/-/paper164/public/comment", "forum": "Skvgqgqxe", "replyto": "SyfCrHRzg", "signatures": ["~Dani_Yogatama1"], "readers": ["everyone"], "writers": ["~Dani_Yogatama1"], "content": {"title": "re: Feedback", "comment": "Thank you for your feedback. \nWe agree that it would be nice if we could run larger models to obtain better numbers.\nWe would like to reiterate that the main difficulty of training our proposed method, as described in Section 4, is that the computation graph needs to be reconstructed for every example at every iteration depending on the samples from the policy network. \nAs a result, the model is not easily parallelizable, and the solution to train a larger model is not as trivial as allocating more resources.\n\nWe have observed that increasing the model size does improve the performance of all models (of course, we agree that at some point it will start to overfit, but we are not there yet).\nFor example, in the SNLI task, increasing the number of parameters by 3.7m improves the performance of the latent syntax model from 80.5 to 81.3, left to right from 80.2 to 80.7, etc. \nWe will include results for at least one of the tasks that illustrate that the model still gets better as we increase its size in the next version of the paper.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn\ntree-structured neural networks for computing representations of natural language sentences.\nIn contrast with prior work on tree-structured models, in which the trees are either provided as input or\npredicted using supervision from explicit treebank annotations,\nthe tree structures in this work are optimized to improve performance on a downstream task.\nExperiments demonstrate the benefit of\nlearning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations.\nWe analyze the induced trees and show that while they discover\nsome linguistically intuitive structures (e.g., noun phrases, simple verb phrases),\nthey are different than conventional English syntactic structures.", "pdf": "/pdf/806aec0935bef790b284e92b5f8975eea34a5103.pdf", "paperhash": "yogatama|learning_to_compose_words_into_sentences_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "authorids": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287704719, "id": "ICLR.cc/2017/conference/-/paper164/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Skvgqgqxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper164/reviewers", "ICLR.cc/2017/conference/paper164/areachairs"], "cdate": 1485287704719}}}, {"tddate": null, "tmdate": 1481232359734, "tcdate": 1481232359727, "number": 9, "id": "SJlgg8Dmx", "invitation": "ICLR.cc/2017/conference/-/paper164/public/comment", "forum": "Skvgqgqxe", "replyto": "ryK8N3kmg", "signatures": ["~Dani_Yogatama1"], "readers": ["everyone"], "writers": ["~Dani_Yogatama1"], "content": {"title": "re: Experiments", "comment": "Thank you for your questions. \n\n- We agree that comparing the proposed approach to methods that use differentiable data structures such as stack RNNs is an interesting direction. Our preliminary experiments suggest that it is not trivial to scale our model with such data structures. so we leave this very interesting idea for future work.\n\n- Initialization: for the semantic relatedness, natural language inference, and sentence generation tasks, the variance between different initializations is small (e.g., the semi supervised syntax model, the MSE variance for semantic relatedness is 0.0001). Note that we initialize the word embedding matrix in all cases, which we found to be important to stabilize the training process. For the sentiment analysis task, the variance is slightly higher (e.g., the variance of the semi-supervised syntax model is 0.26). However, in most cases the general trend remains consistent with semi-supervised and latent syntax models outperforming models with fixed structures.\n\n- The shift operation pushes the embedding vector of the current word and the initial memory state into the stack."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn\ntree-structured neural networks for computing representations of natural language sentences.\nIn contrast with prior work on tree-structured models, in which the trees are either provided as input or\npredicted using supervision from explicit treebank annotations,\nthe tree structures in this work are optimized to improve performance on a downstream task.\nExperiments demonstrate the benefit of\nlearning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations.\nWe analyze the induced trees and show that while they discover\nsome linguistically intuitive structures (e.g., noun phrases, simple verb phrases),\nthey are different than conventional English syntactic structures.", "pdf": "/pdf/806aec0935bef790b284e92b5f8975eea34a5103.pdf", "paperhash": "yogatama|learning_to_compose_words_into_sentences_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "authorids": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287704719, "id": "ICLR.cc/2017/conference/-/paper164/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Skvgqgqxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper164/reviewers", "ICLR.cc/2017/conference/paper164/areachairs"], "cdate": 1485287704719}}}, {"tddate": null, "tmdate": 1480733776808, "tcdate": 1480733776800, "number": 2, "id": "ryK8N3kmg", "invitation": "ICLR.cc/2017/conference/-/paper164/pre-review/question", "forum": "Skvgqgqxe", "replyto": "Skvgqgqxe", "signatures": ["ICLR.cc/2017/conference/paper164/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper164/AnonReviewer1"], "content": {"title": "Experiments", "question": "I like the idea of learning structure or representation of text which is useful for a task of interest. I have a few questions regarding the experiments.\n\nWould it make sense to compare the proposed approach to methods that learn use differentiable data structure, such as the stack RNNs introduced by Grefenstette et al. (2015) and Joulin & Mikolov (2015)?\n\nThe authors state that the best model (out of 3 random initialization) is kept. What is the importance of the initialization? In particular, what is the variance between the different initializations?\n\nFinally, if I understand the model correctly, the shift operation correspond to a regular LSTM update (with pushing the resulting hidden/memory representation on the stack). Is that correct?\n\nGrefenstette et al. (2015). Learning to Transduce with Unbounded Memory. In NIPS.\nJoulin & Mikolov (2015). Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets. In NIPS.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn\ntree-structured neural networks for computing representations of natural language sentences.\nIn contrast with prior work on tree-structured models, in which the trees are either provided as input or\npredicted using supervision from explicit treebank annotations,\nthe tree structures in this work are optimized to improve performance on a downstream task.\nExperiments demonstrate the benefit of\nlearning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations.\nWe analyze the induced trees and show that while they discover\nsome linguistically intuitive structures (e.g., noun phrases, simple verb phrases),\nthey are different than conventional English syntactic structures.", "pdf": "/pdf/806aec0935bef790b284e92b5f8975eea34a5103.pdf", "paperhash": "yogatama|learning_to_compose_words_into_sentences_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "authorids": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481481810334, "id": "ICLR.cc/2017/conference/-/paper164/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper164/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper164/AnonReviewer3", "ICLR.cc/2017/conference/paper164/AnonReviewer1", "ICLR.cc/2017/conference/paper164/AnonReviewer2"], "reply": {"forum": "Skvgqgqxe", "replyto": "Skvgqgqxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper164/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper164/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481481810334}}}, {"tddate": null, "tmdate": 1480639945607, "tcdate": 1480639945603, "number": 1, "id": "SyfCrHRzg", "invitation": "ICLR.cc/2017/conference/-/paper164/pre-review/question", "forum": "Skvgqgqxe", "replyto": "Skvgqgqxe", "signatures": ["ICLR.cc/2017/conference/paper164/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper164/AnonReviewer3"], "content": {"title": "Feedback", "question": "I like this paper a lot. Great and clear model description.\nI find the excuse for the not so stellar performance not convincing though.\nI'm sure DeepMind has the resources to train a slightly larger model with more parameters, even if it takes a few days.\nThe paper would really be stronger if it didn't include the excuse that your model would just be better if it was bigger. Show at least a plot with increasing performance based on increasing # of parameters. Or ideally just run with larger models to be really convincing.\nMaybe the model will start to overfit if it's bigger and not get better?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn\ntree-structured neural networks for computing representations of natural language sentences.\nIn contrast with prior work on tree-structured models, in which the trees are either provided as input or\npredicted using supervision from explicit treebank annotations,\nthe tree structures in this work are optimized to improve performance on a downstream task.\nExperiments demonstrate the benefit of\nlearning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations.\nWe analyze the induced trees and show that while they discover\nsome linguistically intuitive structures (e.g., noun phrases, simple verb phrases),\nthey are different than conventional English syntactic structures.", "pdf": "/pdf/806aec0935bef790b284e92b5f8975eea34a5103.pdf", "paperhash": "yogatama|learning_to_compose_words_into_sentences_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "authorids": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481481810334, "id": "ICLR.cc/2017/conference/-/paper164/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper164/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper164/AnonReviewer3", "ICLR.cc/2017/conference/paper164/AnonReviewer1", "ICLR.cc/2017/conference/paper164/AnonReviewer2"], "reply": {"forum": "Skvgqgqxe", "replyto": "Skvgqgqxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper164/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper164/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481481810334}}}, {"tddate": null, "tmdate": 1479923015831, "tcdate": 1479923015824, "number": 8, "id": "HJeISUQGg", "invitation": "ICLR.cc/2017/conference/-/paper164/public/comment", "forum": "Skvgqgqxe", "replyto": "H1Rp631fl", "signatures": ["~Dani_Yogatama1"], "readers": ["everyone"], "writers": ["~Dani_Yogatama1"], "content": {"title": "re: supervised syntax baselines", "comment": "Thank you for your comment. \n\nOur \"supervised syntax\" baseline is similar to the constituency tree LSTM of Tai et al with different parameters for the left and right child nodes, whereas the dependency tree LSTM uses the sum of unordered child hidden states in its composition function.\n\nFor the Stanford Sentiment Treebank experiments, we believe that the difference in accuracies is mainly due to the larger number of parameters in Tai et al models (more than twice as many parameters).\n\nFor the SICK experiments, we were unable to replicate their experiment results after exhaustive tuning, even for the standard left to right LSTM. \nAlthough the overall trend of the results is similar (supervised syntax is better than left to right), our MSE numbers are much higher.\nOur experimental setup is nearly similar to Tai et al., except for the number of examples per minibatch (1 vs. 25).\nFor this reason, one possible hypothesis is that on this very small dataset, training with minibatches is important to reduce the variance of the updates.\n\nNonetheless, our experimental setup is exactly the same for the supervised syntax baseline (and other baselines) and latent/semi-supervised syntax models, with the only difference being the composition order of the models.\nOur results on various tasks show that using latent/semi-supervised syntax improve over models with predefined structures.\nThey provide strong evidence that if we have better a supervised syntax model, its latent/semi-supervised syntax variants will be even better."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn\ntree-structured neural networks for computing representations of natural language sentences.\nIn contrast with prior work on tree-structured models, in which the trees are either provided as input or\npredicted using supervision from explicit treebank annotations,\nthe tree structures in this work are optimized to improve performance on a downstream task.\nExperiments demonstrate the benefit of\nlearning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations.\nWe analyze the induced trees and show that while they discover\nsome linguistically intuitive structures (e.g., noun phrases, simple verb phrases),\nthey are different than conventional English syntactic structures.", "pdf": "/pdf/806aec0935bef790b284e92b5f8975eea34a5103.pdf", "paperhash": "yogatama|learning_to_compose_words_into_sentences_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "authorids": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287704719, "id": "ICLR.cc/2017/conference/-/paper164/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Skvgqgqxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper164/reviewers", "ICLR.cc/2017/conference/paper164/areachairs"], "cdate": 1485287704719}}}, {"tddate": null, "tmdate": 1479687621821, "tcdate": 1479687621816, "number": 7, "id": "H1Rp631fl", "invitation": "ICLR.cc/2017/conference/-/paper164/public/comment", "forum": "Skvgqgqxe", "replyto": "Skvgqgqxe", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Why is the current \"supervised syntax\" performing worse than older \"supervised syntax\" baselines?", "comment": "In several of the evaluation, the \"constituency tree LSTM\" and/or \"dependency tree LSTM\" methods perform much better than all of your proposed models, including the \"supervised syntax\" model, and sometimes even with the same number of parameters. What is the difference between your \"supervised syntax\" method and these tree LSTMs? why doesn't the supervised syntax approach perform at least as good as the Tai et al models?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn\ntree-structured neural networks for computing representations of natural language sentences.\nIn contrast with prior work on tree-structured models, in which the trees are either provided as input or\npredicted using supervision from explicit treebank annotations,\nthe tree structures in this work are optimized to improve performance on a downstream task.\nExperiments demonstrate the benefit of\nlearning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations.\nWe analyze the induced trees and show that while they discover\nsome linguistically intuitive structures (e.g., noun phrases, simple verb phrases),\nthey are different than conventional English syntactic structures.", "pdf": "/pdf/806aec0935bef790b284e92b5f8975eea34a5103.pdf", "paperhash": "yogatama|learning_to_compose_words_into_sentences_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "authorids": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287704719, "id": "ICLR.cc/2017/conference/-/paper164/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Skvgqgqxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper164/reviewers", "ICLR.cc/2017/conference/paper164/areachairs"], "cdate": 1485287704719}}}, {"tddate": null, "tmdate": 1479142693756, "tcdate": 1479142693746, "number": 6, "id": "rJCXpvDWg", "invitation": "ICLR.cc/2017/conference/-/paper164/public/comment", "forum": "Skvgqgqxe", "replyto": "SkaYAHP-g", "signatures": ["~Dani_Yogatama1"], "readers": ["everyone"], "writers": ["~Dani_Yogatama1"], "content": {"title": "Thanks!", "comment": "We've added NTI and NSE results to Table 2."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn\ntree-structured neural networks for computing representations of natural language sentences.\nIn contrast with prior work on tree-structured models, in which the trees are either provided as input or\npredicted using supervision from explicit treebank annotations,\nthe tree structures in this work are optimized to improve performance on a downstream task.\nExperiments demonstrate the benefit of\nlearning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations.\nWe analyze the induced trees and show that while they discover\nsome linguistically intuitive structures (e.g., noun phrases, simple verb phrases),\nthey are different than conventional English syntactic structures.", "pdf": "/pdf/806aec0935bef790b284e92b5f8975eea34a5103.pdf", "paperhash": "yogatama|learning_to_compose_words_into_sentences_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "authorids": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287704719, "id": "ICLR.cc/2017/conference/-/paper164/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Skvgqgqxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper164/reviewers", "ICLR.cc/2017/conference/paper164/areachairs"], "cdate": 1485287704719}}}, {"tddate": null, "tmdate": 1479134853023, "tcdate": 1479134853019, "number": 5, "id": "SkaYAHP-g", "invitation": "ICLR.cc/2017/conference/-/paper164/public/comment", "forum": "Skvgqgqxe", "replyto": "Syy18JW-l", "signatures": ["~Tsendsuren_Munkhdalai1"], "readers": ["everyone"], "writers": ["~Tsendsuren_Munkhdalai1"], "content": {"title": "NTI and NSE: # of parameters", "comment": "Sure! Here are the approximate numbers. However, I am not sure whether we should include the word embeddings as training parameters since in NTI and NSE experiment we don't fine-tune the embeddings as a part of the model parameters, via label supervision. The parameters of the main model are trained only.\n\nNTI-SLSTM: 2.0M + 2.4M (word embedding) = 4.4M\nNTI-SLSTM-LSTM: 2.8M + 2.4M = 4.8M\n\nNSE: 3.0M + 2.4M = 5.4M\n\n\nThanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn\ntree-structured neural networks for computing representations of natural language sentences.\nIn contrast with prior work on tree-structured models, in which the trees are either provided as input or\npredicted using supervision from explicit treebank annotations,\nthe tree structures in this work are optimized to improve performance on a downstream task.\nExperiments demonstrate the benefit of\nlearning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations.\nWe analyze the induced trees and show that while they discover\nsome linguistically intuitive structures (e.g., noun phrases, simple verb phrases),\nthey are different than conventional English syntactic structures.", "pdf": "/pdf/806aec0935bef790b284e92b5f8975eea34a5103.pdf", "paperhash": "yogatama|learning_to_compose_words_into_sentences_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "authorids": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287704719, "id": "ICLR.cc/2017/conference/-/paper164/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Skvgqgqxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper164/reviewers", "ICLR.cc/2017/conference/paper164/areachairs"], "cdate": 1485287704719}}}, {"tddate": null, "tmdate": 1479069896247, "tcdate": 1479069896241, "number": 4, "id": "SkgRxIIZe", "invitation": "ICLR.cc/2017/conference/-/paper164/public/comment", "forum": "Skvgqgqxe", "replyto": "BklBwcQ-g", "signatures": ["~Dani_Yogatama1"], "readers": ["everyone"], "writers": ["~Dani_Yogatama1"], "content": {"title": "Re: comparison vs NTI", "comment": "Thank you for your comment. We agree that it would be interesting to see how well a binary tree baseline would perform. \nWe will run the experiments and update the paper when we have the results.\n\nThanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn\ntree-structured neural networks for computing representations of natural language sentences.\nIn contrast with prior work on tree-structured models, in which the trees are either provided as input or\npredicted using supervision from explicit treebank annotations,\nthe tree structures in this work are optimized to improve performance on a downstream task.\nExperiments demonstrate the benefit of\nlearning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations.\nWe analyze the induced trees and show that while they discover\nsome linguistically intuitive structures (e.g., noun phrases, simple verb phrases),\nthey are different than conventional English syntactic structures.", "pdf": "/pdf/806aec0935bef790b284e92b5f8975eea34a5103.pdf", "paperhash": "yogatama|learning_to_compose_words_into_sentences_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "authorids": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287704719, "id": "ICLR.cc/2017/conference/-/paper164/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Skvgqgqxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper164/reviewers", "ICLR.cc/2017/conference/paper164/areachairs"], "cdate": 1485287704719}}}, {"tddate": null, "tmdate": 1478891320298, "tcdate": 1478891320293, "number": 3, "id": "BklBwcQ-g", "invitation": "ICLR.cc/2017/conference/-/paper164/public/comment", "forum": "Skvgqgqxe", "replyto": "Skvgqgqxe", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "comparison vs NTI", "comment": "Big fan of this work. Related to the above comment, have you guys tried a simple baseline where you just assume a binary tree? I imagine it will do better than left-to-right and right-to-left (given the success of NTI), but will probably do worse than the proposed method."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn\ntree-structured neural networks for computing representations of natural language sentences.\nIn contrast with prior work on tree-structured models, in which the trees are either provided as input or\npredicted using supervision from explicit treebank annotations,\nthe tree structures in this work are optimized to improve performance on a downstream task.\nExperiments demonstrate the benefit of\nlearning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations.\nWe analyze the induced trees and show that while they discover\nsome linguistically intuitive structures (e.g., noun phrases, simple verb phrases),\nthey are different than conventional English syntactic structures.", "pdf": "/pdf/806aec0935bef790b284e92b5f8975eea34a5103.pdf", "paperhash": "yogatama|learning_to_compose_words_into_sentences_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "authorids": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287704719, "id": "ICLR.cc/2017/conference/-/paper164/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Skvgqgqxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper164/reviewers", "ICLR.cc/2017/conference/paper164/areachairs"], "cdate": 1485287704719}}}, {"tddate": null, "tmdate": 1478714839074, "tcdate": 1478714839067, "number": 2, "id": "Syy18JW-l", "invitation": "ICLR.cc/2017/conference/-/paper164/public/comment", "forum": "Skvgqgqxe", "replyto": "r1orpReZe", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Thanks for your comment", "comment": "Could you please let us know the approximate numbers of parameters (including word embeddings) for NTI and NSE on this task so we can include this information and add the results to Table 2?\n\nThanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn\ntree-structured neural networks for computing representations of natural language sentences.\nIn contrast with prior work on tree-structured models, in which the trees are either provided as input or\npredicted using supervision from explicit treebank annotations,\nthe tree structures in this work are optimized to improve performance on a downstream task.\nExperiments demonstrate the benefit of\nlearning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations.\nWe analyze the induced trees and show that while they discover\nsome linguistically intuitive structures (e.g., noun phrases, simple verb phrases),\nthey are different than conventional English syntactic structures.", "pdf": "/pdf/806aec0935bef790b284e92b5f8975eea34a5103.pdf", "paperhash": "yogatama|learning_to_compose_words_into_sentences_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "authorids": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287704719, "id": "ICLR.cc/2017/conference/-/paper164/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Skvgqgqxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper164/reviewers", "ICLR.cc/2017/conference/paper164/areachairs"], "cdate": 1485287704719}}}, {"tddate": null, "tmdate": 1478713202760, "tcdate": 1478712643373, "number": 1, "id": "r1orpReZe", "invitation": "ICLR.cc/2017/conference/-/paper164/public/comment", "forum": "Skvgqgqxe", "replyto": "Skvgqgqxe", "signatures": ["~Tsendsuren_Munkhdalai1"], "readers": ["everyone"], "writers": ["~Tsendsuren_Munkhdalai1"], "content": {"title": "Results on Stanford Sentiment Treebank", "comment": "Table 2 is missing some recent results on this task. Please see the NTI and NSE results on the same task [1,2]. NTI is particularly relevant to this work because it encodes a sentence with an n-ary tree (i.e. binary tree) instead of using a parser output or learning to compose.\n\n\nThanks,\n\n\n\nRef:\n\n1. Munkhdalai, Tsendsuren, and Hong Yu. \"Neural Tree Indexers for Text Understanding.\" arXiv preprint arXiv:1607.04492 (2016).\n2. Munkhdalai, Tsendsuren, and Hong Yu. \"Neural Semantic Encoders.\" arXiv preprint arXiv:1607.04315 (2016)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn\ntree-structured neural networks for computing representations of natural language sentences.\nIn contrast with prior work on tree-structured models, in which the trees are either provided as input or\npredicted using supervision from explicit treebank annotations,\nthe tree structures in this work are optimized to improve performance on a downstream task.\nExperiments demonstrate the benefit of\nlearning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations.\nWe analyze the induced trees and show that while they discover\nsome linguistically intuitive structures (e.g., noun phrases, simple verb phrases),\nthey are different than conventional English syntactic structures.", "pdf": "/pdf/806aec0935bef790b284e92b5f8975eea34a5103.pdf", "paperhash": "yogatama|learning_to_compose_words_into_sentences_with_reinforcement_learning", "conflicts": ["google.com"], "keywords": [], "authors": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "authorids": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287704719, "id": "ICLR.cc/2017/conference/-/paper164/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Skvgqgqxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper164/reviewers", "ICLR.cc/2017/conference/paper164/areachairs"], "cdate": 1485287704719}}}], "count": 25}