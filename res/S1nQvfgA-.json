{"notes": [{"tddate": null, "ddate": null, "tmdate": 1519327775012, "tcdate": 1509070628119, "number": 224, "cdate": 1518730184831, "id": "S1nQvfgA-", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "S1nQvfgA-", "original": "HkiQPzeRb", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Semantically Decomposing the Latent Spaces of Generative Adversarial Networks", "abstract": "We propose a new algorithm for training generative adversarial networks to jointly learn latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs). In practice, this means that by fixing the identity portion of latent codes, we can generate diverse images of the same subject, and by fixing the observation portion we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose. Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code. Corresponding samples from the real dataset consist of two distinct photographs of the same subject. In order to fool the discriminator, the generator must produce images that are both photorealistic, distinct, and appear to depict the same person. We augment both the DCGAN and BEGAN approaches with Siamese discriminators to accommodate pairwise training. Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm\u2019s ability to generate convincing, identity-matched photographs.", "pdf": "/pdf/7d489bf176b131aab3fa9e3f5308aad74c7ea8c7.pdf", "TL;DR": "SD-GANs disentangle latent codes according to known commonalities in a dataset (e.g. photographs depicting the same person).", "paperhash": "donahue|semantically_decomposing_the_latent_spaces_of_generative_adversarial_networks", "_bibtex": "@inproceedings{\ndonahue2018semantically,\ntitle={Semantically Decomposing the Latent Spaces of Generative Adversarial Networks},\nauthor={Chris Donahue and Akshay Balsubramani and Julian McAuley and Zachary C. Lipton},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1nQvfgA-},\n}", "keywords": ["disentangled representations", "generative adversarial networks", "generative modeling", "image synthesis"], "authors": ["Chris Donahue", "Zachary C. Lipton", "Akshay Balsubramani", "Julian McAuley"], "authorids": ["cdonahue@ucsd.edu", "zlipton@cmu.edu", "abalsubr@stanford.edu", "jmcauley@cs.ucsd.edu"]}, "nonreaders": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260096328, "tcdate": 1517249408523, "number": 188, "cdate": 1517249408504, "id": "rJuKQJ6BG", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "S1nQvfgA-", "replyto": "S1nQvfgA-", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "The paper proposes a GAN based approach for disentangling identity (or class information) from style. The supervision needed is the identity label for each image. Overall, the reviewers agree that the paper makes a novel contribution along the line of work on disentangling 'style' from 'content'. ", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semantically Decomposing the Latent Spaces of Generative Adversarial Networks", "abstract": "We propose a new algorithm for training generative adversarial networks to jointly learn latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs). In practice, this means that by fixing the identity portion of latent codes, we can generate diverse images of the same subject, and by fixing the observation portion we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose. Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code. Corresponding samples from the real dataset consist of two distinct photographs of the same subject. In order to fool the discriminator, the generator must produce images that are both photorealistic, distinct, and appear to depict the same person. We augment both the DCGAN and BEGAN approaches with Siamese discriminators to accommodate pairwise training. Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm\u2019s ability to generate convincing, identity-matched photographs.", "pdf": "/pdf/7d489bf176b131aab3fa9e3f5308aad74c7ea8c7.pdf", "TL;DR": "SD-GANs disentangle latent codes according to known commonalities in a dataset (e.g. photographs depicting the same person).", "paperhash": "donahue|semantically_decomposing_the_latent_spaces_of_generative_adversarial_networks", "_bibtex": "@inproceedings{\ndonahue2018semantically,\ntitle={Semantically Decomposing the Latent Spaces of Generative Adversarial Networks},\nauthor={Chris Donahue and Akshay Balsubramani and Julian McAuley and Zachary C. Lipton},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1nQvfgA-},\n}", "keywords": ["disentangled representations", "generative adversarial networks", "generative modeling", "image synthesis"], "authors": ["Chris Donahue", "Zachary C. Lipton", "Akshay Balsubramani", "Julian McAuley"], "authorids": ["cdonahue@ucsd.edu", "zlipton@cmu.edu", "abalsubr@stanford.edu", "jmcauley@cs.ucsd.edu"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642411560, "tcdate": 1511581245917, "number": 1, "cdate": 1511581245917, "id": "SkIH8vIef", "invitation": "ICLR.cc/2018/Conference/-/Paper224/Official_Review", "forum": "S1nQvfgA-", "replyto": "S1nQvfgA-", "signatures": ["ICLR.cc/2018/Conference/Paper224/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Progress in disentangling identity from other factors using GANs.", "rating": "6: Marginally above acceptance threshold", "review": "Quality\nThe paper is well written and the model is simple and clearly explained. The idea for disentangling identity from other factors of variation using identity-matched image pairs is quite simple, but the experimental results on faces and shoes are impressive.\n\nClarity\nThe model and its training objective are simple and clearly explained.\n\nOriginality\nThere are now many, many papers on generative models with disentangled feature representations, including with GANs. However, to my knowledge this is the first paper showing very compelling results using this particular setup of identity-aligned images.\n\nSignificance\nDisentangled generative models are an important line of work in my opinion. This paper presents a very simple but apparently effective way of disentangling identity from other factors, and implements in two of the more recent GAN architectures.\n\nSuggestion for an experiment - can you do few shot image generation? A simple way to do it would be to train an encoder from image \u2192 identity encoding. Then, given one or a few images of a new person\u2019s face or a new shoe, you could estimate the identity latent variable, and then generate many additional samples.\n\nPros\n- Very simple and effective disentangling technique for GANs.\n- Great execution, compelling samples on both faces and shoes.\n\nCons\n- Only two factors of variations are disentangled in this model. Could it be generalized to specify more than just two, e.g. lighting, pose, viewpoint, etc?\n- Not much technically new or surprising compared to past work on disentangling generative models.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semantically Decomposing the Latent Spaces of Generative Adversarial Networks", "abstract": "We propose a new algorithm for training generative adversarial networks to jointly learn latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs). In practice, this means that by fixing the identity portion of latent codes, we can generate diverse images of the same subject, and by fixing the observation portion we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose. Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code. Corresponding samples from the real dataset consist of two distinct photographs of the same subject. In order to fool the discriminator, the generator must produce images that are both photorealistic, distinct, and appear to depict the same person. We augment both the DCGAN and BEGAN approaches with Siamese discriminators to accommodate pairwise training. Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm\u2019s ability to generate convincing, identity-matched photographs.", "pdf": "/pdf/7d489bf176b131aab3fa9e3f5308aad74c7ea8c7.pdf", "TL;DR": "SD-GANs disentangle latent codes according to known commonalities in a dataset (e.g. photographs depicting the same person).", "paperhash": "donahue|semantically_decomposing_the_latent_spaces_of_generative_adversarial_networks", "_bibtex": "@inproceedings{\ndonahue2018semantically,\ntitle={Semantically Decomposing the Latent Spaces of Generative Adversarial Networks},\nauthor={Chris Donahue and Akshay Balsubramani and Julian McAuley and Zachary C. Lipton},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1nQvfgA-},\n}", "keywords": ["disentangled representations", "generative adversarial networks", "generative modeling", "image synthesis"], "authors": ["Chris Donahue", "Zachary C. Lipton", "Akshay Balsubramani", "Julian McAuley"], "authorids": ["cdonahue@ucsd.edu", "zlipton@cmu.edu", "abalsubr@stanford.edu", "jmcauley@cs.ucsd.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642411466, "id": "ICLR.cc/2018/Conference/-/Paper224/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper224/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper224/AnonReviewer2", "ICLR.cc/2018/Conference/Paper224/AnonReviewer3", "ICLR.cc/2018/Conference/Paper224/AnonReviewer1"], "reply": {"forum": "S1nQvfgA-", "replyto": "S1nQvfgA-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper224/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642411466}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642411518, "tcdate": 1511719628366, "number": 2, "cdate": 1511719628366, "id": "ryEAzYOlM", "invitation": "ICLR.cc/2018/Conference/-/Paper224/Official_Review", "forum": "S1nQvfgA-", "replyto": "S1nQvfgA-", "signatures": ["ICLR.cc/2018/Conference/Paper224/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Review summary of \"Semantically Decomposing the Latent Spaces of Generative Adversarial Networks\"", "rating": "6: Marginally above acceptance threshold", "review": "Summary:\n\nThis paper investigated the problem of controlled image generation. Assuming images can be disentangled by identity-related factors and style factors, this paper proposed an algorithm that produces a pair of images with the same identity. Compared to standard GAN framework, this algorithm first generated two latent variables for the pair images. The two latent variables are partially shared reflecting the shared identity information. The generator then transformed the latent variables into high-resolution images with a deconvolution decoder networks. The discriminator was used to distinguish paired images from database or paired images sampled by the algorithm. Experiments were conducted using DCGAN and BEGAN on portrait images and shoe product images. Qualitative results demonstrated that the learned style representations capture viewpoint, illumination and background color while the identity was well preserved by the identity-related representations.\n\n\n== Novelty & Significance ==\nPaired image generation is an interesting topic but this has been explored to some extent. Compared to existing coupled generation pipeline such as CoGAN, I can see the proposed formulation is more application-driven.\n\n== Technical Quality ==\nIn Figure 3, the portrait images in the second row and fourth row look quite similar. I wonder if the trained model works with only limited variability (in terms of identity).\nIn Figure 4, the viewpoint is quite limited (only 4 viewpoints are provided).\n\nI am not very convinced whether SD-GAN is a generic algorithm for controlled image generation. Based on the current results, I suspect it only works in fairly constrained settings. \nIt would be good to know if it actually works in more challenging datasets such as SUN bedroom, CUB and Oxford Flowers. \n\n\u201cthe AC-DCGAN model cannot imagine new identities\u201d\nI feel the author of this paper made an unfair argument when comparing AC-DCGAN with the proposed method. First, during training, the proposed SD-GAN needs to access the identity information and there is only limited identity in the dataset. Based on the presentation, it is not very clear how does the model generate novel identities (in contrast to simply interpolating existing identities). For example, is it possible to generate novel viewpoints in Figure 4?\n\nMissing references on conditional image generation and coupled image generation:\n-- Generative Adversarial Text-to-Image Synthesis. Reed et al., In ICML 2016.\n-- Attribute2Image: Conditional Image Generation from Visual Attributes. Yan et al., In ECCV 2016.\n-- Domain Separation Networks. Bousmalis et al., In NIPS 2016.\n-- Unsupervised Image-to-Image Translation Networks. Liu et al., In NIPS 2017.\n\nOverall, I rate this paper slightly above borderline. It showed some good visualization results on controlled image generation. But the comparison to AC-GAN is not very fair, since the identity pairs are fully supervised for the proposed method. As far as I can see, there are no clear-cut improvements quantitatively. Also, there is no comparison with CoGAN, which I believe is the most relevant work for coupled image generation. \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Semantically Decomposing the Latent Spaces of Generative Adversarial Networks", "abstract": "We propose a new algorithm for training generative adversarial networks to jointly learn latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs). In practice, this means that by fixing the identity portion of latent codes, we can generate diverse images of the same subject, and by fixing the observation portion we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose. Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code. Corresponding samples from the real dataset consist of two distinct photographs of the same subject. In order to fool the discriminator, the generator must produce images that are both photorealistic, distinct, and appear to depict the same person. We augment both the DCGAN and BEGAN approaches with Siamese discriminators to accommodate pairwise training. Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm\u2019s ability to generate convincing, identity-matched photographs.", "pdf": "/pdf/7d489bf176b131aab3fa9e3f5308aad74c7ea8c7.pdf", "TL;DR": "SD-GANs disentangle latent codes according to known commonalities in a dataset (e.g. photographs depicting the same person).", "paperhash": "donahue|semantically_decomposing_the_latent_spaces_of_generative_adversarial_networks", "_bibtex": "@inproceedings{\ndonahue2018semantically,\ntitle={Semantically Decomposing the Latent Spaces of Generative Adversarial Networks},\nauthor={Chris Donahue and Akshay Balsubramani and Julian McAuley and Zachary C. Lipton},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1nQvfgA-},\n}", "keywords": ["disentangled representations", "generative adversarial networks", "generative modeling", "image synthesis"], "authors": ["Chris Donahue", "Zachary C. Lipton", "Akshay Balsubramani", "Julian McAuley"], "authorids": ["cdonahue@ucsd.edu", "zlipton@cmu.edu", "abalsubr@stanford.edu", "jmcauley@cs.ucsd.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642411466, "id": "ICLR.cc/2018/Conference/-/Paper224/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper224/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper224/AnonReviewer2", "ICLR.cc/2018/Conference/Paper224/AnonReviewer3", "ICLR.cc/2018/Conference/Paper224/AnonReviewer1"], "reply": {"forum": "S1nQvfgA-", "replyto": "S1nQvfgA-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper224/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642411466}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642411480, "tcdate": 1511754485583, "number": 3, "cdate": 1511754485583, "id": "BkAls-Kgf", "invitation": "ICLR.cc/2018/Conference/-/Paper224/Official_Review", "forum": "S1nQvfgA-", "replyto": "S1nQvfgA-", "signatures": ["ICLR.cc/2018/Conference/Paper224/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Review from AnonReviewer1", "rating": "7: Good paper, accept", "review": "[Overview]\n\nIn this paper, the authors proposed a model called SD-GAN, to decompose semantical component of the input in GAN. Specifically, the authors proposed a novel architecture to decompose the identity latent code and non-identity latent code. In this new architecture, the generator is unchanged while the discriminator takes pair data as the input, and output the decision of whether two images are from the same identity or not. By training the whole model with a conventional GAN-training regime, SD-GAN learns to take a part of the input Z as the identity information, and the other part of input Z as the non-identity (or attribute) information. In the experiments, the authors demonstrate that the proposed SD-GAN could generate images preserving the same identity with diverse attributes, such as pose, age, expression, etc. Compared with AC-GAN, the proposed SD-GAN achieved better performance in both automatically evaluation metric (FaceNet) and Human Study. In the appendix, the authors further presented ablated qualitative results in various settings.\n\n[Strengths]\n\n1. This paper proposed a simple but effective generative adversarial network, called SD-GAN, to decompose the input latent code of GAN into separate semantical parts. Specifically, it is mainly instantiated on face images, to decompose the identity part and non-identity part in the latent code. Unlike the previous works such as AC-GAN, SD-GAN exploited a Siamese network to replace the conventional discriminator used in GAN. By this way, SD-GAN could generate images of novel identities, rather than being constrained to those identities used during training. I think this is a very good property. Due to this, SD-GAN consumes much less memory than AC-GAN, when training on a large number of identities.\n\n2. In the experiment section, the authors quantitatively evaluate the generated images based on two methods, one is using a pre-trained FaceNet model to measure the verification accuracy and one is human study. When evaluated based on FaceNet, the proposed SD-GAN achieved higher accuracy and obtained more diverse face images, compared with AC-GAN. In human study, SD-GAN achieved comparable verification accuracy, while higher diversity than AC-GAN. The authors further presented ablated experiments in the Appendix.\n\n[Comments]\n\nThis paper presents a novel model to decompose the latent code in a semantic manner. However, I have several questions about the model:\n\n1. Why would SD-GAN not generate images merely have a smaller number of identities or just a few identities? In Algorithm 1, the authors trained the model by sampling one identity vector, which is then concatenated to two observation vectors. In this case, the generator always takes the same identity vectors, and the discriminator is used to distinguish these fake same-identity pair and the real same-identity pair from training data. As such, even if the generator generates the same identity, say mean identity, given different identity vectors, the generated images can still obtain a lower discrimination loss. Without any explicite constraint to enforce the generator to generate different identity with different identity vectors, I am wondering what makes SD-GAN be able to generate diverse identities? \n\n2. Still about the identity diversity. Though the authors showed the identity-matched diversity in the experiments, the diversity across identity on the generated images is not evaluated. The authors should also evaluate this kind of identity. Generally, AC-GAN could generate as many identities as the number of identities in training data. I am curious about whether SD-GAN could generate comparable diverse identity to AC-GAN. One simple way is to evaluate the whole generated image set using Inception Score based on a Pre-trained face identification network; Another way is to directly use the generated images to train a verification model or identification model and evaluate it on real images. Though compared with AC-GAN, SD-GAN achieved better identity verification performance and sample diversity, I suspect the identity diversity is discounted, though SD-GAN has the property of generating novel identities. Furthermore,  the authors should also compare the general quality of generated samples with DC-GAN and BEGAN as well (at least qualitatively), apart from the comparison to AC-GAN on the identity-matched generation.\n\n3. When making the comparison with related work, the authors mentioned that Info-GAN was not able to determine which factors are assigned to each dimension. I think this is not precise. The lack of this property is because there are no data annotations. Given the data annotations, Info-GAN can be easily augmented with such property by sending the real images into the discriminator for classification. Also, there is a typo in the caption of Fig. 10. It looks like each column shares the same identity vector instead of each row.\n\n[Summary]\n\nThis paper proposed a new model called SD-GAN to decompose the input latent code of GAN into two separete semantical parts, one for identity and one for observations. Unlike AC-GAN, SD-GAN exploited a Siamese architecture in discriminator. By this way, SD-GAN could not only generate more identity-matched face image pairs but also more diverse samples with the same identity,  compared with AC-GAN. I think this is a good idea for decomposing the semantical parts in the latent code, in the sense that it can imagine new face identities and consumes less memory during training. Overall, I think this is a good paper. However, as I mentioned above, I am still not clear why SD-GAN could generate diverse identities without any constraints to make the model do that. Also, the authors should further evaluate the diversity of identity and compare it with AC-GAN.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semantically Decomposing the Latent Spaces of Generative Adversarial Networks", "abstract": "We propose a new algorithm for training generative adversarial networks to jointly learn latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs). In practice, this means that by fixing the identity portion of latent codes, we can generate diverse images of the same subject, and by fixing the observation portion we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose. Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code. Corresponding samples from the real dataset consist of two distinct photographs of the same subject. In order to fool the discriminator, the generator must produce images that are both photorealistic, distinct, and appear to depict the same person. We augment both the DCGAN and BEGAN approaches with Siamese discriminators to accommodate pairwise training. Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm\u2019s ability to generate convincing, identity-matched photographs.", "pdf": "/pdf/7d489bf176b131aab3fa9e3f5308aad74c7ea8c7.pdf", "TL;DR": "SD-GANs disentangle latent codes according to known commonalities in a dataset (e.g. photographs depicting the same person).", "paperhash": "donahue|semantically_decomposing_the_latent_spaces_of_generative_adversarial_networks", "_bibtex": "@inproceedings{\ndonahue2018semantically,\ntitle={Semantically Decomposing the Latent Spaces of Generative Adversarial Networks},\nauthor={Chris Donahue and Akshay Balsubramani and Julian McAuley and Zachary C. Lipton},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1nQvfgA-},\n}", "keywords": ["disentangled representations", "generative adversarial networks", "generative modeling", "image synthesis"], "authors": ["Chris Donahue", "Zachary C. Lipton", "Akshay Balsubramani", "Julian McAuley"], "authorids": ["cdonahue@ucsd.edu", "zlipton@cmu.edu", "abalsubr@stanford.edu", "jmcauley@cs.ucsd.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642411466, "id": "ICLR.cc/2018/Conference/-/Paper224/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper224/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper224/AnonReviewer2", "ICLR.cc/2018/Conference/Paper224/AnonReviewer3", "ICLR.cc/2018/Conference/Paper224/AnonReviewer1"], "reply": {"forum": "S1nQvfgA-", "replyto": "S1nQvfgA-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper224/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642411466}}}, {"tddate": null, "ddate": null, "tmdate": 1513037121558, "tcdate": 1513037121558, "number": 4, "cdate": 1513037121558, "id": "H19BT53-f", "invitation": "ICLR.cc/2018/Conference/-/Paper224/Official_Comment", "forum": "S1nQvfgA-", "replyto": "ryEAzYOlM", "signatures": ["ICLR.cc/2018/Conference/Paper224/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper224/Authors"], "content": {"title": "Response to AnonReviewer3", "comment": "\nThank you for your response and for pointing out missing references. We have added them to our related work section in the updated manuscript. Please see our general response above with regard to identity diversity.\n\nWe did not quantitatively compare to CoGAN as their problem objectives are different from our own. CoGANs learn to translate images between domains with binary attributes such as blond/brown hair or glasses/no-glasses without parallel data. How one might extend CoGANs to learn a manifold over thousands of identities is non-obvious.\n\nAs you suggested, we would also like to run studies on other types of data (the Oxford Flowers dataset that you recommended is particularly enticing), but leave this as an avenue for our future explorations."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semantically Decomposing the Latent Spaces of Generative Adversarial Networks", "abstract": "We propose a new algorithm for training generative adversarial networks to jointly learn latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs). In practice, this means that by fixing the identity portion of latent codes, we can generate diverse images of the same subject, and by fixing the observation portion we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose. Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code. Corresponding samples from the real dataset consist of two distinct photographs of the same subject. In order to fool the discriminator, the generator must produce images that are both photorealistic, distinct, and appear to depict the same person. We augment both the DCGAN and BEGAN approaches with Siamese discriminators to accommodate pairwise training. Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm\u2019s ability to generate convincing, identity-matched photographs.", "pdf": "/pdf/7d489bf176b131aab3fa9e3f5308aad74c7ea8c7.pdf", "TL;DR": "SD-GANs disentangle latent codes according to known commonalities in a dataset (e.g. photographs depicting the same person).", "paperhash": "donahue|semantically_decomposing_the_latent_spaces_of_generative_adversarial_networks", "_bibtex": "@inproceedings{\ndonahue2018semantically,\ntitle={Semantically Decomposing the Latent Spaces of Generative Adversarial Networks},\nauthor={Chris Donahue and Akshay Balsubramani and Julian McAuley and Zachary C. Lipton},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1nQvfgA-},\n}", "keywords": ["disentangled representations", "generative adversarial networks", "generative modeling", "image synthesis"], "authors": ["Chris Donahue", "Zachary C. Lipton", "Akshay Balsubramani", "Julian McAuley"], "authorids": ["cdonahue@ucsd.edu", "zlipton@cmu.edu", "abalsubr@stanford.edu", "jmcauley@cs.ucsd.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737276, "id": "ICLR.cc/2018/Conference/-/Paper224/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "S1nQvfgA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper224/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper224/Authors|ICLR.cc/2018/Conference/Paper224/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper224/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper224/Authors|ICLR.cc/2018/Conference/Paper224/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper224/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper224/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper224/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper224/Reviewers", "ICLR.cc/2018/Conference/Paper224/Authors", "ICLR.cc/2018/Conference/Paper224/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737276}}}, {"tddate": null, "ddate": null, "tmdate": 1513036986253, "tcdate": 1513036986253, "number": 3, "cdate": 1513036986253, "id": "SyGahq2WG", "invitation": "ICLR.cc/2018/Conference/-/Paper224/Official_Comment", "forum": "S1nQvfgA-", "replyto": "SkIH8vIef", "signatures": ["ICLR.cc/2018/Conference/Paper224/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper224/Authors"], "content": {"title": "Response to AnonReviewer2", "comment": "\nThank you for your insights. We are glad that you found our paper to be well-written and our method to be both simple and effective. Your intuition is correct that SD-GAN could be extended to disentangle multiple factors of variation. We are actively investigating this and plan to publish these results in future work. You are also correct that SD-GAN could be modified for few-shot image generation; please see Appendix B (and the relevant Figure 8) for our preliminary investigation and results in this setting."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semantically Decomposing the Latent Spaces of Generative Adversarial Networks", "abstract": "We propose a new algorithm for training generative adversarial networks to jointly learn latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs). In practice, this means that by fixing the identity portion of latent codes, we can generate diverse images of the same subject, and by fixing the observation portion we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose. Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code. Corresponding samples from the real dataset consist of two distinct photographs of the same subject. In order to fool the discriminator, the generator must produce images that are both photorealistic, distinct, and appear to depict the same person. We augment both the DCGAN and BEGAN approaches with Siamese discriminators to accommodate pairwise training. Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm\u2019s ability to generate convincing, identity-matched photographs.", "pdf": "/pdf/7d489bf176b131aab3fa9e3f5308aad74c7ea8c7.pdf", "TL;DR": "SD-GANs disentangle latent codes according to known commonalities in a dataset (e.g. photographs depicting the same person).", "paperhash": "donahue|semantically_decomposing_the_latent_spaces_of_generative_adversarial_networks", "_bibtex": "@inproceedings{\ndonahue2018semantically,\ntitle={Semantically Decomposing the Latent Spaces of Generative Adversarial Networks},\nauthor={Chris Donahue and Akshay Balsubramani and Julian McAuley and Zachary C. Lipton},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1nQvfgA-},\n}", "keywords": ["disentangled representations", "generative adversarial networks", "generative modeling", "image synthesis"], "authors": ["Chris Donahue", "Zachary C. Lipton", "Akshay Balsubramani", "Julian McAuley"], "authorids": ["cdonahue@ucsd.edu", "zlipton@cmu.edu", "abalsubr@stanford.edu", "jmcauley@cs.ucsd.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737276, "id": "ICLR.cc/2018/Conference/-/Paper224/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "S1nQvfgA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper224/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper224/Authors|ICLR.cc/2018/Conference/Paper224/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper224/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper224/Authors|ICLR.cc/2018/Conference/Paper224/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper224/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper224/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper224/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper224/Reviewers", "ICLR.cc/2018/Conference/Paper224/Authors", "ICLR.cc/2018/Conference/Paper224/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737276}}}, {"tddate": null, "ddate": null, "tmdate": 1513036914102, "tcdate": 1513036914102, "number": 2, "cdate": 1513036914102, "id": "rkqOhq2bM", "invitation": "ICLR.cc/2018/Conference/-/Paper224/Official_Comment", "forum": "S1nQvfgA-", "replyto": "BkAls-Kgf", "signatures": ["ICLR.cc/2018/Conference/Paper224/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper224/Authors"], "content": {"title": "Response to AnonReviewer1", "comment": "\nThank you for your detailed comments. In addition to adding information about identity diversity to the manuscript (see general response above), we offer the following responses to your other inquiries:\n\n1) Why would SD-GAN not generate images merely have a smaller number of identities or just a few identities?\n\nIf SD-GANs always produced images depicting the same identity regardless of the identity vector, the discriminator would be able to easily label these samples as fake on the basis that they always depict the same subject. This is the same reason that regular GANs are able to produce images that differ in subject identity.\n\n3) When making the comparison with related work, the authors mentioned that Info-GAN was not able to determine which factors are assigned to each dimension. I think this is not precise.\n\nThank you for pointing out that our statement about InfoGAN was imprecise. Our original purpose in making this claim was to indicate that there is no *obvious* way to hold identity fixed while varying contingent factors in vanilla InfoGAN, and hence no way to directly compare it to our SD-GAN algorithm. However, as you pointed out, InfoGAN is fully unsupervised while SD-GAN is not. We have updated our related work section to clarify the distinction between InfoGAN and SD-GAN."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semantically Decomposing the Latent Spaces of Generative Adversarial Networks", "abstract": "We propose a new algorithm for training generative adversarial networks to jointly learn latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs). In practice, this means that by fixing the identity portion of latent codes, we can generate diverse images of the same subject, and by fixing the observation portion we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose. Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code. Corresponding samples from the real dataset consist of two distinct photographs of the same subject. In order to fool the discriminator, the generator must produce images that are both photorealistic, distinct, and appear to depict the same person. We augment both the DCGAN and BEGAN approaches with Siamese discriminators to accommodate pairwise training. Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm\u2019s ability to generate convincing, identity-matched photographs.", "pdf": "/pdf/7d489bf176b131aab3fa9e3f5308aad74c7ea8c7.pdf", "TL;DR": "SD-GANs disentangle latent codes according to known commonalities in a dataset (e.g. photographs depicting the same person).", "paperhash": "donahue|semantically_decomposing_the_latent_spaces_of_generative_adversarial_networks", "_bibtex": "@inproceedings{\ndonahue2018semantically,\ntitle={Semantically Decomposing the Latent Spaces of Generative Adversarial Networks},\nauthor={Chris Donahue and Akshay Balsubramani and Julian McAuley and Zachary C. Lipton},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1nQvfgA-},\n}", "keywords": ["disentangled representations", "generative adversarial networks", "generative modeling", "image synthesis"], "authors": ["Chris Donahue", "Zachary C. Lipton", "Akshay Balsubramani", "Julian McAuley"], "authorids": ["cdonahue@ucsd.edu", "zlipton@cmu.edu", "abalsubr@stanford.edu", "jmcauley@cs.ucsd.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737276, "id": "ICLR.cc/2018/Conference/-/Paper224/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "S1nQvfgA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper224/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper224/Authors|ICLR.cc/2018/Conference/Paper224/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper224/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper224/Authors|ICLR.cc/2018/Conference/Paper224/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper224/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper224/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper224/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper224/Reviewers", "ICLR.cc/2018/Conference/Paper224/Authors", "ICLR.cc/2018/Conference/Paper224/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737276}}}, {"tddate": null, "ddate": null, "tmdate": 1513036810802, "tcdate": 1513036810802, "number": 1, "cdate": 1513036810802, "id": "rJ7G3q3Zz", "invitation": "ICLR.cc/2018/Conference/-/Paper224/Official_Comment", "forum": "S1nQvfgA-", "replyto": "S1nQvfgA-", "signatures": ["ICLR.cc/2018/Conference/Paper224/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper224/Authors"], "content": {"title": "General response to reviews", "comment": "We would like to thank all of the reviewers for their thoughtful comments and suggestions. We are glad that reviewers found our method simple, our results compelling, and our paper to be well-written. While the overall response was positive, reviewers expressed minor concerns related to identity diversity in our generated results. We have uploaded a new version of the paper that will hopefully clarify.\n\n[Identity diversity]\n\nIn the original manuscript, we report one measure of identity diversity. In Table 1, our All-Div metric reports the mean diversity (one minus MS-SSIM) for 10k pairs with random identity. While All-Div captures diversity at the pixel level, it is perhaps not the best measure of *semantic* (perceived) diversity. In our updated manuscript, we report another metric in Table 1: the false accept rate (FAR) of FaceNet and human annotators. A higher FAR indicates some evidence of lower identity diversity. By this metric, SD-GANs produce images with lower but comparable diversity to those from AC-DCGAN, and both have lower diversity than the real data. We have added these details to Table 1."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semantically Decomposing the Latent Spaces of Generative Adversarial Networks", "abstract": "We propose a new algorithm for training generative adversarial networks to jointly learn latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs). In practice, this means that by fixing the identity portion of latent codes, we can generate diverse images of the same subject, and by fixing the observation portion we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose. Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code. Corresponding samples from the real dataset consist of two distinct photographs of the same subject. In order to fool the discriminator, the generator must produce images that are both photorealistic, distinct, and appear to depict the same person. We augment both the DCGAN and BEGAN approaches with Siamese discriminators to accommodate pairwise training. Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm\u2019s ability to generate convincing, identity-matched photographs.", "pdf": "/pdf/7d489bf176b131aab3fa9e3f5308aad74c7ea8c7.pdf", "TL;DR": "SD-GANs disentangle latent codes according to known commonalities in a dataset (e.g. photographs depicting the same person).", "paperhash": "donahue|semantically_decomposing_the_latent_spaces_of_generative_adversarial_networks", "_bibtex": "@inproceedings{\ndonahue2018semantically,\ntitle={Semantically Decomposing the Latent Spaces of Generative Adversarial Networks},\nauthor={Chris Donahue and Akshay Balsubramani and Julian McAuley and Zachary C. Lipton},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=S1nQvfgA-},\n}", "keywords": ["disentangled representations", "generative adversarial networks", "generative modeling", "image synthesis"], "authors": ["Chris Donahue", "Zachary C. Lipton", "Akshay Balsubramani", "Julian McAuley"], "authorids": ["cdonahue@ucsd.edu", "zlipton@cmu.edu", "abalsubr@stanford.edu", "jmcauley@cs.ucsd.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737276, "id": "ICLR.cc/2018/Conference/-/Paper224/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "S1nQvfgA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper224/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper224/Authors|ICLR.cc/2018/Conference/Paper224/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper224/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper224/Authors|ICLR.cc/2018/Conference/Paper224/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper224/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper224/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper224/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper224/Reviewers", "ICLR.cc/2018/Conference/Paper224/Authors", "ICLR.cc/2018/Conference/Paper224/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737276}}}], "count": 9}