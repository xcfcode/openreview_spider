{"notes": [{"id": "rJl05AVtwB", "original": "rke-DkKuwH", "number": 1302, "cdate": 1569439382441, "ddate": null, "tcdate": 1569439382441, "tmdate": 1577168235413, "tddate": null, "forum": "rJl05AVtwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Chordal-GCN: Exploiting sparsity in training large-scale graph convolutional networks", "authors": ["Xin Jiang*", "Kewei Cheng*", "Song Jiang*", "Yizhou Sun"], "authorids": ["jiangxjames@ucla.edu", "viviancheng@cs.ucla.edu", "songjiang@cs.ucla.edu", "yzsun@cs.ucla.edu"], "keywords": ["graph convolutional network", "semi-supervised learning"], "abstract": "Despite the impressive success of graph convolutional networks (GCNs) on numerous applications, training on large-scale sparse networks remains challenging. Current algorithms require large memory space for storing GCN outputs as well as all the intermediate embeddings. Besides, most of these algorithms involves either random sampling or an approximation of the adjacency matrix, which might unfortunately lose important structure information. In this paper, we propose Chordal-GCN for semi-supervised node classification. The proposed model utilizes the exact graph structure (i.e., without sampling or approximation), while requires limited memory resources compared with the original GCN. Moreover, it leverages the sparsity pattern as well as the clustering structure of the graph. The proposed model first decomposes a large-scale sparse network into several small dense subgraphs (called cliques), and constructs a clique tree. By traversing the tree, GCN training is performed clique by clique, and connections between cliques are exploited via the tree hierarchy. Furthermore, we implement Chordal-GCN on large-scale datasets and demonstrate superior performance.\n", "pdf": "/pdf/1222fba0549d706b48143f965a3f266916f73bcb.pdf", "paperhash": "jiang|chordalgcn_exploiting_sparsity_in_training_largescale_graph_convolutional_networks", "TL;DR": "Chordal-GCN is a scalable graph neural network which exploits the exact graph structure (i.e., without approximation or sampling) and requires limited memory usage.", "code": "https://www.dropbox.com/s/0vby5gbu9qkbigr/chordal-gcn.zip?dl=0", "original_pdf": "/attachment/327700c18fda20197e0303f4e3f8784dd34392be.pdf", "_bibtex": "@misc{\njiang*2020chordalgcn,\ntitle={Chordal-{\\{}GCN{\\}}: Exploiting sparsity in training large-scale graph convolutional networks},\nauthor={Xin Jiang* and Kewei Cheng* and Song Jiang* and Yizhou Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=rJl05AVtwB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "L79S-TI9yw", "original": null, "number": 1, "cdate": 1576798719833, "ddate": null, "tcdate": 1576798719833, "tmdate": 1576800916718, "tddate": null, "forum": "rJl05AVtwB", "replyto": "rJl05AVtwB", "invitation": "ICLR.cc/2020/Conference/Paper1302/-/Decision", "content": {"decision": "Reject", "comment": "The submission is proposed a rejection based on majority review.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Chordal-GCN: Exploiting sparsity in training large-scale graph convolutional networks", "authors": ["Xin Jiang*", "Kewei Cheng*", "Song Jiang*", "Yizhou Sun"], "authorids": ["jiangxjames@ucla.edu", "viviancheng@cs.ucla.edu", "songjiang@cs.ucla.edu", "yzsun@cs.ucla.edu"], "keywords": ["graph convolutional network", "semi-supervised learning"], "abstract": "Despite the impressive success of graph convolutional networks (GCNs) on numerous applications, training on large-scale sparse networks remains challenging. Current algorithms require large memory space for storing GCN outputs as well as all the intermediate embeddings. Besides, most of these algorithms involves either random sampling or an approximation of the adjacency matrix, which might unfortunately lose important structure information. In this paper, we propose Chordal-GCN for semi-supervised node classification. The proposed model utilizes the exact graph structure (i.e., without sampling or approximation), while requires limited memory resources compared with the original GCN. Moreover, it leverages the sparsity pattern as well as the clustering structure of the graph. The proposed model first decomposes a large-scale sparse network into several small dense subgraphs (called cliques), and constructs a clique tree. By traversing the tree, GCN training is performed clique by clique, and connections between cliques are exploited via the tree hierarchy. Furthermore, we implement Chordal-GCN on large-scale datasets and demonstrate superior performance.\n", "pdf": "/pdf/1222fba0549d706b48143f965a3f266916f73bcb.pdf", "paperhash": "jiang|chordalgcn_exploiting_sparsity_in_training_largescale_graph_convolutional_networks", "TL;DR": "Chordal-GCN is a scalable graph neural network which exploits the exact graph structure (i.e., without approximation or sampling) and requires limited memory usage.", "code": "https://www.dropbox.com/s/0vby5gbu9qkbigr/chordal-gcn.zip?dl=0", "original_pdf": "/attachment/327700c18fda20197e0303f4e3f8784dd34392be.pdf", "_bibtex": "@misc{\njiang*2020chordalgcn,\ntitle={Chordal-{\\{}GCN{\\}}: Exploiting sparsity in training large-scale graph convolutional networks},\nauthor={Xin Jiang* and Kewei Cheng* and Song Jiang* and Yizhou Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=rJl05AVtwB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rJl05AVtwB", "replyto": "rJl05AVtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795703484, "tmdate": 1576800250862, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1302/-/Decision"}}}, {"id": "SJxy62zaFB", "original": null, "number": 2, "cdate": 1571790006753, "ddate": null, "tcdate": 1571790006753, "tmdate": 1574365605198, "tddate": null, "forum": "rJl05AVtwB", "replyto": "rJl05AVtwB", "invitation": "ICLR.cc/2020/Conference/Paper1302/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "This paper leverages the clique tree decomposition of the graph and design a new variant of GCN which does graph convolution on each clique and penalize the inconsistent prediction made on separators of each node and its children. Experiments on citation networks and the reddit network show that the proposed method is efficient.\n\nOverall, this paper could be a significant contribution on improving GCN, with the caveat for some clarifications on the model and experiments. Given these clarifications in an author response, I would be willing to increase the score.\n\nPros:\n\n1, I like the idea of exploiting graph decomposition. In my opinion, it may not only improve the scalability but also help the model learn representations which better capture the structure or speed up the learning process. It would be great if authors could show some evidence along this line.\n\n2, The examples in Figure 2 and 3 are very helpful in understanding the concepts related to the clique tree decomposition.\n\n3, The summarization of time and memory complexity is very helpful in comparing different models. \n\n4, I read the detailed questions and responses in the open review. It helps me understand more details about the experiments. Besides the typo of Table 2, I tend to believe that the experimental setup is reasonable and results are convincing although I did not run the code by myself. \n\nCons & Questions:\n\n1, The main motivation of exploiting the graph decomposition is to save memory such that GCN could be applied to large scale graphs without sacrificing the structural information. However, the scale of the largest experiments is still less impressive. To strengthen the paper, it would be great to try larger graph datasets which have been used in the literature.\n\n2, I am confused by the writing on the final prediction made by the model. In particular, do you only keep the prediction of residual or do you average the predictions on the separators? It may be interesting to explore different ways of making predictions based on this decomposition based inference. In general, it would be great to separate the writing of loss (learning) and prediction (inference).\n\n3, Why does Chordal-GCN take significant more epochs than GCN on Reddit and less epochs on all other datasets?\n\nSuggestion:\n\n1, I think the clique tree is very similar if not the same with the junction tree given the node ordering (see section 2.5.2 of [1]). It would be great to discuss the relationship between your chordal graph representation and the tree decomposition used by the probabilistic inference algorithms of graphical models. From the perspective of complexity, the junction tree method and yours both highly depend on the tree-width. Also, linking to probabilistic inference could help better motivate the method since tree-based inference algorithm is shown to converge faster in the literature.\n\n2, It would be great to discuss and or compare with [2] as it uses graph partition algorithms to get clusters and apply GNN with a propagation schedule which alternates between within-cluster and between-cluster. It is closely related to the chordal-GCN as it uses the decomposition of graph clustering directly rather than the clique tree. Decomposition like multiple overlapping spanning trees are also studied in [2].\n\n[1] Wainwright, M.J. and Jordan, M.I., 2008. Graphical models, exponential families, and variational inference. Foundations and Trends\u00ae in Machine Learning, 1(1\u20132), pp.1-305.\n\n[2] Liao, R., Brockschmidt, M., Tarlow, D., Gaunt, A.L., Urtasun, R. and Zemel, R., 2018. Graph partition neural networks for semi-supervised classification. arXiv preprint arXiv:1803.06272.\n\n======================================================================================================\n\nThank authors for the thorough reply! After I read authors' rebuttal and other reviewers' comments, I would like to keep my original rating. Again, I like this idea and believe better exploiting structure in the propagation could improve the inference in many ways. I hope authors could keep improving it, e.g., better motivating the proposed method (memory saving is just one angle which sometimes may need more engineering work to fully verify) and change the experiments accordingly.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1302/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1302/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Chordal-GCN: Exploiting sparsity in training large-scale graph convolutional networks", "authors": ["Xin Jiang*", "Kewei Cheng*", "Song Jiang*", "Yizhou Sun"], "authorids": ["jiangxjames@ucla.edu", "viviancheng@cs.ucla.edu", "songjiang@cs.ucla.edu", "yzsun@cs.ucla.edu"], "keywords": ["graph convolutional network", "semi-supervised learning"], "abstract": "Despite the impressive success of graph convolutional networks (GCNs) on numerous applications, training on large-scale sparse networks remains challenging. Current algorithms require large memory space for storing GCN outputs as well as all the intermediate embeddings. Besides, most of these algorithms involves either random sampling or an approximation of the adjacency matrix, which might unfortunately lose important structure information. In this paper, we propose Chordal-GCN for semi-supervised node classification. The proposed model utilizes the exact graph structure (i.e., without sampling or approximation), while requires limited memory resources compared with the original GCN. Moreover, it leverages the sparsity pattern as well as the clustering structure of the graph. The proposed model first decomposes a large-scale sparse network into several small dense subgraphs (called cliques), and constructs a clique tree. By traversing the tree, GCN training is performed clique by clique, and connections between cliques are exploited via the tree hierarchy. Furthermore, we implement Chordal-GCN on large-scale datasets and demonstrate superior performance.\n", "pdf": "/pdf/1222fba0549d706b48143f965a3f266916f73bcb.pdf", "paperhash": "jiang|chordalgcn_exploiting_sparsity_in_training_largescale_graph_convolutional_networks", "TL;DR": "Chordal-GCN is a scalable graph neural network which exploits the exact graph structure (i.e., without approximation or sampling) and requires limited memory usage.", "code": "https://www.dropbox.com/s/0vby5gbu9qkbigr/chordal-gcn.zip?dl=0", "original_pdf": "/attachment/327700c18fda20197e0303f4e3f8784dd34392be.pdf", "_bibtex": "@misc{\njiang*2020chordalgcn,\ntitle={Chordal-{\\{}GCN{\\}}: Exploiting sparsity in training large-scale graph convolutional networks},\nauthor={Xin Jiang* and Kewei Cheng* and Song Jiang* and Yizhou Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=rJl05AVtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJl05AVtwB", "replyto": "rJl05AVtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1302/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1302/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575635778646, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1302/Reviewers"], "noninvitees": [], "tcdate": 1570237720485, "tmdate": 1575635778660, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1302/-/Official_Review"}}}, {"id": "Bkl3gxj2iB", "original": null, "number": 5, "cdate": 1573855220116, "ddate": null, "tcdate": 1573855220116, "tmdate": 1573855622696, "tddate": null, "forum": "rJl05AVtwB", "replyto": "BJeCB_sT_H", "invitation": "ICLR.cc/2020/Conference/Paper1302/-/Official_Comment", "content": {"title": "Thank you for your comments.", "comment": "We thank the reviewer for the constructive reviews. We addressed the questions and concerns of the reviewer accordingly in the following.\n\nWeakness:\n1. One major concern is that the reduction in memory usage is not large enough to justify the huge increment in running-time. For example, on Cora dataset, the memory is reduced by 4x while the running time is 16x compared to vanilla GCN.\n\nAmong all the baselines, vanilla GCN always takes the minimum time. Chordal-GCN is slower because Chordal-GCN executes more than one GCN model in every epoch, i.e., one GCN for one clique in Chordal-GCN. However, the running time of Chordal-GCN is comparable to other baselines.\n\nActually, in Chordal-GCN there is a trade-off between the memory and the training time. If we divide the graph into more cliques, then the required memory, which is determined by the largest clique size, will be reduced. But at the same time, the training time will increase because every clique corresponds to one GCN model. We will explore this balance issue in the future.\n\n2. It is not clear why Chordal-GCN can achieve better accuracy compared to vanilla GCN. Since Chordal-GCN serves as approximation for GCN as the authors claim that using entire graph will provide better accuracy. As a result, the vanilla GCN is expected to achieve better accuracy. It would be better if the authors could provide more intuition and explanations.\n\nBoth Chordal-GCN and vanilla GCN use the entire graph---without sampling or any other approximation, so we don\u2019t treat our model as an \u2018\u2018approximation\u2019\u2019 of GCN. Instead, Chordal-GCN modifies the GCN training procedure such that the training can be performed in a distributed manner.\n\nAn intuition is provided in paragraph 3 of the introduction section: in most citation networks, highly-cited papers should have impacts on multiple communities. Thus, Chordal-GCN treats the \u2018\u2018inter-cluster\u2019\u2019 links and the \u2018\u2018intra-cluster\u2019\u2019 links differently (while GCN doesn\u2019t make a distinction). In training a particular node, Chordal-GCN considers the influence of its neighbors in the same clique first, and the impacts from other cliques are handled via the consistency loss. This different treatment might explain an increase in the accuracy.\n\n3. The evaluation does not take the graph preprocessing into consideration. The authors should report the time and memory taken to carry out the preprocessing steps as well.\n\nThe most expensive step in preprocessing is building the clique tree, of which the time and memory are both linear in the number of nonzeros in the adjacency matrix A.\n\n4. For most real-world large-scale industry networks, it is hard to fit the graph into memory. Though the GCN training part could run in distributed way, it is not clear how to efficiently build the clique tree in similar method.\nThe SOTA methods for building clique trees [1,2] can be easily extended to a distributed version. Basically, one can start with the node with the largest ID, and then find the clique this node is in. Other neighbors of this node (with largest ID) are certainly in the children clique. Thereby, every time we only need to know the neighbors of the current node, instead of the entire graph.\n\n5. Given the main purpose of the algorithm is to reduce memory usage for large-scale networks, it is expected to see experiments on larger graphs where the large memory footprint becomes a real issue.\n\nThank you for your suggestion. we plan to include this result in our final draft.\n\nDetailed comments:\n\n1. The description in Section 2.2 is not very clear. It would be better if the authors could provide a more detailed introduction to clique tree and the algorithms used.\n\nThank you for your advice and sorry for the confusion. The algorithm for building the clique tree is too sophisticated to explain in two pages, and is not the focus of our paper. So we choose not to include in the preliminaries. We hope the unclearness in Section 2.2 does not affect the understanding of our main model.\n\n2. For the Chordal-GCN in algorithm 1, for epoch 2 onwards, do we also add consistent-loss when training leaves as well?\n\nNo. The consistency loss involves the current clique and its children in the clique tree. Since leaves of a tree never have a child (ch(i) is a empty set in Eq. (2)), there is no consistency loss when training leaves.\n\n[1] P. Buneman. A characterization of rigid circuit graphs. Discrete Mathematics. 9:205-212, 1974.\n[2] F. Gavril. The intersection graphs of subtrees in trees are exactly chordal graphs. Journal of Combinatorial Theory Series B, 16:47-56, 1974.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1302/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1302/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Chordal-GCN: Exploiting sparsity in training large-scale graph convolutional networks", "authors": ["Xin Jiang*", "Kewei Cheng*", "Song Jiang*", "Yizhou Sun"], "authorids": ["jiangxjames@ucla.edu", "viviancheng@cs.ucla.edu", "songjiang@cs.ucla.edu", "yzsun@cs.ucla.edu"], "keywords": ["graph convolutional network", "semi-supervised learning"], "abstract": "Despite the impressive success of graph convolutional networks (GCNs) on numerous applications, training on large-scale sparse networks remains challenging. Current algorithms require large memory space for storing GCN outputs as well as all the intermediate embeddings. Besides, most of these algorithms involves either random sampling or an approximation of the adjacency matrix, which might unfortunately lose important structure information. In this paper, we propose Chordal-GCN for semi-supervised node classification. The proposed model utilizes the exact graph structure (i.e., without sampling or approximation), while requires limited memory resources compared with the original GCN. Moreover, it leverages the sparsity pattern as well as the clustering structure of the graph. The proposed model first decomposes a large-scale sparse network into several small dense subgraphs (called cliques), and constructs a clique tree. By traversing the tree, GCN training is performed clique by clique, and connections between cliques are exploited via the tree hierarchy. Furthermore, we implement Chordal-GCN on large-scale datasets and demonstrate superior performance.\n", "pdf": "/pdf/1222fba0549d706b48143f965a3f266916f73bcb.pdf", "paperhash": "jiang|chordalgcn_exploiting_sparsity_in_training_largescale_graph_convolutional_networks", "TL;DR": "Chordal-GCN is a scalable graph neural network which exploits the exact graph structure (i.e., without approximation or sampling) and requires limited memory usage.", "code": "https://www.dropbox.com/s/0vby5gbu9qkbigr/chordal-gcn.zip?dl=0", "original_pdf": "/attachment/327700c18fda20197e0303f4e3f8784dd34392be.pdf", "_bibtex": "@misc{\njiang*2020chordalgcn,\ntitle={Chordal-{\\{}GCN{\\}}: Exploiting sparsity in training large-scale graph convolutional networks},\nauthor={Xin Jiang* and Kewei Cheng* and Song Jiang* and Yizhou Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=rJl05AVtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJl05AVtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1302/Authors", "ICLR.cc/2020/Conference/Paper1302/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1302/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1302/Reviewers", "ICLR.cc/2020/Conference/Paper1302/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1302/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1302/Authors|ICLR.cc/2020/Conference/Paper1302/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158088, "tmdate": 1576860553070, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1302/Authors", "ICLR.cc/2020/Conference/Paper1302/Reviewers", "ICLR.cc/2020/Conference/Paper1302/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1302/-/Official_Comment"}}}, {"id": "ryxVOko3sH", "original": null, "number": 3, "cdate": 1573855083898, "ddate": null, "tcdate": 1573855083898, "tmdate": 1573855546047, "tddate": null, "forum": "rJl05AVtwB", "replyto": "B1e8GSdatH", "invitation": "ICLR.cc/2020/Conference/Paper1302/-/Official_Comment", "content": {"title": "Thank you for your comments", "comment": "We thank the reviewer for the constructive reviews. We addressed the questions and concerns of the reviewer accordingly in the following.\n\n1) To my best knowledge, the proposed Chordal- match SOTA results on Cora, Citeceer, and Pubmed. However, since these datasets are small and easy to run, I would like to see the mean and standard deviation of the accuracy of all models you ran. Can you also provide the results of the commonly used \"random split setting\"[1]?\n\nAns: The split in [1] is chosen by the authors, and thus fixed. Existing work [7] has proven that this split of dataset has a significant influence on the classification result. We follow the random held-out strategy and randomly split the dataset for training and test multiple times. This random split strategy is used in all baseline models, and we think this is a fair and consistent setting.\n\n2) What is the epoch time of the Chordal-GCN? Can you also report it in Table 2? Without including the pre-processing time, we don't know the overall training time of the method.\n\nAns: We have already reported the epoch time in Table 2.\n\nFor the preprocessing, the main bottleneck is to build the clique tree, of which the time complexity is linear in the number of nonzeros in the adjacency matrix. In our current implementation, the preprocessing mainly depends on the python package Chompack, and thus it takes more time than Cluster-GCN---as the preprocess in Cluster-GCN depends on a C package.\n\n3) Given that the main concern is the memory usage, the authors should compare to a strong baseline, SGC [2], which is a linear classifier trained on top of propagated features with memory/space complexity O(d) when using mini-batch training. This is much smaller than the proposed method O(Lc_2d + Ld^2).\nAlso, SGC is at least two magnitudes faster to train (2.7s vs 0.987*410=367.8s + unknown pre-processing time) and more accurate (94.9 vs 94.2) than the proposed Chordal-GCN on the largest Reddit dataset. The authors emphasize that the proposed method is scalable. Please compare it to SGC in Table 2. \nNevertheless, there is some chance that the authors can apply the same method to SGC and speed it up further as long as the preprocessing time is relatively small.\n\nAns: Thank you for your suggestion. In our biased opinion, Chordal-GCN, as well as all the baselines we used, is a modification in the training phase of the vanilla GCN. Comparatively, SGC should be treated as a totally different graph neural network model. In light of the model structure, SGC only has one layer, and the graph structure $A$ is aggregated with the feature information $X$ before training. This preprocessing step facilitates training but is \u2018\u2018expensive\u2019\u2019 in nature: In preprocessing, one should compute $A^K X$ (in our notation). Computing matrix power $A^K$ involves eigenvalue decomposition of $A$, which is $O(n^3)$. The short preprocessing time in the SGC paper is attributed to highly optimized numerical linear algebra package BLAS and LAPACK, which are written in Fortran language. If clique tree building can be carried out in Fortran or C, the preprocessing time will be limited ($O(||A||_0)$) in Chorda-GCN.\n\n4) Based on Table 2, Cluster-GCN uses less memory and is more accurate and faster to train than Chordal-GCN. Can you justify why people should use the proposed method instead? \n\nAns: In the first three datasets (Cora, Citeseer, Pubmed), Chordal-GCN uses less memory and achieves better results than Cluster-GCN. We also provide an intuitive explanation in Section 1. \n\nThe outlier in Reddit it due to a wrong selection of hyperparameters. With our current hyperparameters, Reddit is decomposed into a giant clique and some small cliques. This unbalanced decomposition causes the abnormal memory cost. We will improve the results in the Reddit dataset by finding better hyperparameters in our final draft. \n\n5) There are some missing citations. These papers [3,4,5,6] achieved previous SOTA results and should be included in the Tables. \n\nAns: Thank you for bringing these papers into our attention. However, there are so many papers on GCN models and we can only compare the most related ones. We argue that Chordal-GCN, as well as all the baselines we use, focuses on the training phase of the vanilla GCN. Thereby, variants of GCN models are not considered as baselines in our submission, despite their interestingness.\n\nReferences:\n[1-6] same as above\n[7] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, Stephan G\u00fcnneman: Pitfalls of Graph Neural Network Evaluation (NeurIPS workshop 2018)\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1302/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1302/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Chordal-GCN: Exploiting sparsity in training large-scale graph convolutional networks", "authors": ["Xin Jiang*", "Kewei Cheng*", "Song Jiang*", "Yizhou Sun"], "authorids": ["jiangxjames@ucla.edu", "viviancheng@cs.ucla.edu", "songjiang@cs.ucla.edu", "yzsun@cs.ucla.edu"], "keywords": ["graph convolutional network", "semi-supervised learning"], "abstract": "Despite the impressive success of graph convolutional networks (GCNs) on numerous applications, training on large-scale sparse networks remains challenging. Current algorithms require large memory space for storing GCN outputs as well as all the intermediate embeddings. Besides, most of these algorithms involves either random sampling or an approximation of the adjacency matrix, which might unfortunately lose important structure information. In this paper, we propose Chordal-GCN for semi-supervised node classification. The proposed model utilizes the exact graph structure (i.e., without sampling or approximation), while requires limited memory resources compared with the original GCN. Moreover, it leverages the sparsity pattern as well as the clustering structure of the graph. The proposed model first decomposes a large-scale sparse network into several small dense subgraphs (called cliques), and constructs a clique tree. By traversing the tree, GCN training is performed clique by clique, and connections between cliques are exploited via the tree hierarchy. Furthermore, we implement Chordal-GCN on large-scale datasets and demonstrate superior performance.\n", "pdf": "/pdf/1222fba0549d706b48143f965a3f266916f73bcb.pdf", "paperhash": "jiang|chordalgcn_exploiting_sparsity_in_training_largescale_graph_convolutional_networks", "TL;DR": "Chordal-GCN is a scalable graph neural network which exploits the exact graph structure (i.e., without approximation or sampling) and requires limited memory usage.", "code": "https://www.dropbox.com/s/0vby5gbu9qkbigr/chordal-gcn.zip?dl=0", "original_pdf": "/attachment/327700c18fda20197e0303f4e3f8784dd34392be.pdf", "_bibtex": "@misc{\njiang*2020chordalgcn,\ntitle={Chordal-{\\{}GCN{\\}}: Exploiting sparsity in training large-scale graph convolutional networks},\nauthor={Xin Jiang* and Kewei Cheng* and Song Jiang* and Yizhou Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=rJl05AVtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJl05AVtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1302/Authors", "ICLR.cc/2020/Conference/Paper1302/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1302/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1302/Reviewers", "ICLR.cc/2020/Conference/Paper1302/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1302/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1302/Authors|ICLR.cc/2020/Conference/Paper1302/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158088, "tmdate": 1576860553070, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1302/Authors", "ICLR.cc/2020/Conference/Paper1302/Reviewers", "ICLR.cc/2020/Conference/Paper1302/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1302/-/Official_Comment"}}}, {"id": "rygP3JsniB", "original": null, "number": 4, "cdate": 1573855150588, "ddate": null, "tcdate": 1573855150588, "tmdate": 1573855353551, "tddate": null, "forum": "rJl05AVtwB", "replyto": "SJxy62zaFB", "invitation": "ICLR.cc/2020/Conference/Paper1302/-/Official_Comment", "content": {"title": "Responds to Review #1", "comment": "We thank the reviewer for the constructive reviews. We addressed the questions and concerns of the reviewer accordingly in the following.\n\nWeakness\n1, Thank you for your suggestion. We plan to include this result in our final draft.\n\n2, In our current submission, the testing procedure is identical to the vanilla GCN because storing the entire graph is not the main problem (storing GCN parameters and embeddings needs much more space than storing the graph). That is why we only include the training phase in Algorithm 1. Nevertheless, your suggestion might motivate further study in another direction: Comparison between the labels predicted by the entire graph and those predicted by every clique might enlighten better understanding on the graph structure. For example, when the predictions are different, does it mean that the current clique has a \u2018\u2018negative\u2019\u2019 impact on predicting this node; and is this negative impact due to noise in the network, or in the feature matrix? We believe all these questions are interesting but challenging, and thus beyond the scope of this paper.\n\n3, The outlier in the Reddit dataset is attributed to a bad choice of hyperparameters. With our current hyperparameters, Reddit is decomposed into a giant clique and some small cliques. This unbalanced decomposition cause the abnormal results. We will improve the results in the Reddit dataset by finding better hyperparameters in our final draft.\n\nSuggestion:\n1, The clique tree in our paper is indeed the same as the junction tree in [1] because any chordal graph must have a clique tree with the running intersection property (in Definition 2.1 of [1]). The only minor difference is that, in our definition, every node in the clique tree is a \u2018\u2018maximal\u2019\u2019 clique while in some literature, the junction tree does not require maximality. In our humble opinion, the term \u2018\u2018clique tree\u2019\u2019 is more used by the chordal sparsity community while the term \u2018\u2018junction tree\u2019\u2019 is often used by researchers in graphical models and probabilistic networks.\n\nWe would have motivated our idea from the perspective of message passing and probabilistic inference. However, we try to avoid the illusion that Chordal-GCN is a new, sophisticated variant of GCN model; instead, we would like to persuade readers that Chordal-GCN is a modification in the GCN training procedure. After all, the idea of linking GCN to probabilistic inference itself is interesting and worth further exploration.\n\nMoreover, we appreciate it a lot if you can notify us with any efficient C implementation of building junction trees, and the tree-based inference algorithms. We are happy to compare the performance of chordal decomposition and the junction tree decomposition in [1]. Although our model still remains the same, the actual running time (especially the preprocessing time) might be improved dramatically.\n\n2, Thank you for bringing the paper [2] into our attention. In our biased opinion, it is more similar to Cluster-GCN [3]. GPNN [2] finds a partition of the nodes and treats inter-cluster and intra-cluster links differently while Chordal-GCN somehow finds a \u2018\u2018partition\u2019\u2019 of the edges and treats clique separators and residuals in a different manner. Building minimum spanning trees in [2] is also used for node partition purpose.\n\nAfter all, it is a very interesting and highly related paper. We have added the citation and will consider it as an important baseline in our final draft.\n\nLast but not least, we would like to thank you again for your affirmation in our paper. We also appreciate your helpful and insightful comments and suggestions.\n\n[1] Wainwright, M.J. and Jordan, M.I., 2008. Graphical models, exponential families, and variational inference. Foundations and Trends\u00ae in Machine Learning, 1(1\u20132), pp.1-305.\n[2] Liao, R., Brockschmidt, M., Tarlow, D., Gaunt, A.L., Urtasun, R. and Zemel, R., 2018. Graph partition neural networks for semi-supervised classification. arXiv preprint arXiv:1803.06272.\n[3] W.-L. Chiang, X. Liu, S. Si, Y. Li, S. Bengio, and C.-J. Hsieh. Cluster-GCN: an efficient algorithm for training deep and large graph convolutional networks. KDD 2019.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1302/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1302/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Chordal-GCN: Exploiting sparsity in training large-scale graph convolutional networks", "authors": ["Xin Jiang*", "Kewei Cheng*", "Song Jiang*", "Yizhou Sun"], "authorids": ["jiangxjames@ucla.edu", "viviancheng@cs.ucla.edu", "songjiang@cs.ucla.edu", "yzsun@cs.ucla.edu"], "keywords": ["graph convolutional network", "semi-supervised learning"], "abstract": "Despite the impressive success of graph convolutional networks (GCNs) on numerous applications, training on large-scale sparse networks remains challenging. Current algorithms require large memory space for storing GCN outputs as well as all the intermediate embeddings. Besides, most of these algorithms involves either random sampling or an approximation of the adjacency matrix, which might unfortunately lose important structure information. In this paper, we propose Chordal-GCN for semi-supervised node classification. The proposed model utilizes the exact graph structure (i.e., without sampling or approximation), while requires limited memory resources compared with the original GCN. Moreover, it leverages the sparsity pattern as well as the clustering structure of the graph. The proposed model first decomposes a large-scale sparse network into several small dense subgraphs (called cliques), and constructs a clique tree. By traversing the tree, GCN training is performed clique by clique, and connections between cliques are exploited via the tree hierarchy. Furthermore, we implement Chordal-GCN on large-scale datasets and demonstrate superior performance.\n", "pdf": "/pdf/1222fba0549d706b48143f965a3f266916f73bcb.pdf", "paperhash": "jiang|chordalgcn_exploiting_sparsity_in_training_largescale_graph_convolutional_networks", "TL;DR": "Chordal-GCN is a scalable graph neural network which exploits the exact graph structure (i.e., without approximation or sampling) and requires limited memory usage.", "code": "https://www.dropbox.com/s/0vby5gbu9qkbigr/chordal-gcn.zip?dl=0", "original_pdf": "/attachment/327700c18fda20197e0303f4e3f8784dd34392be.pdf", "_bibtex": "@misc{\njiang*2020chordalgcn,\ntitle={Chordal-{\\{}GCN{\\}}: Exploiting sparsity in training large-scale graph convolutional networks},\nauthor={Xin Jiang* and Kewei Cheng* and Song Jiang* and Yizhou Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=rJl05AVtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJl05AVtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1302/Authors", "ICLR.cc/2020/Conference/Paper1302/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1302/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1302/Reviewers", "ICLR.cc/2020/Conference/Paper1302/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1302/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1302/Authors|ICLR.cc/2020/Conference/Paper1302/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158088, "tmdate": 1576860553070, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1302/Authors", "ICLR.cc/2020/Conference/Paper1302/Reviewers", "ICLR.cc/2020/Conference/Paper1302/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1302/-/Official_Comment"}}}, {"id": "BJeCB_sT_H", "original": null, "number": 1, "cdate": 1570777157805, "ddate": null, "tcdate": 1570777157805, "tmdate": 1572972316806, "tddate": null, "forum": "rJl05AVtwB", "replyto": "rJl05AVtwB", "invitation": "ICLR.cc/2020/Conference/Paper1302/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this paper, the authors propose a new method referred to Chordal-GCN to optimize memory usage in large graphs. The authors borrow the ideas from Chordal Sparsity theory to first build a client tree. Then mini-batch updates are carried out individually on each clique from the leaves following the GCN loss. The authors add an additional consistency loss between shared node with children cliques. Experiments are carried out in four networks with comparison to several baselines.\n\nStrength:\n1. The authors study an interesting and important problem to reduce memory usage for GCN in large-scale graphs. The usage of chordal sparsity is interesting and innovative.\n2. The authors carry out ablation study on the consistency loss components in the algorithm.\n\nWeakness:\n1. One major concern is that the reduction in memory usage is not large enough to justify the huge increment in running-time. For example, on Cora dataset, the memory is reduced by 4x while the running time is 16x compared to vanilla GCN.\n2. It is not clear why Chordal-GCN can achieve better accuracy compared to vanilla GCN. Since Chordal-GCN serves as approximation for GCN as the authors claim that using entire graph will provide better accuracy. As a result, the vanilla GCN is expected to achieve better accuracy. It would be better if the authors could provide more intuition and explanations.\n3. The evaluation does not take the graph preprocessing into consideration. The authors should report the time and memory taken to carry out the preprocessing steps as well.\n4. For most real-world large-scale industry networks, it is hard to fit the graph into memory. Though the GCN training part could run in distributed way, it is not clear how to efficiently build the clique tree in similar method.\n5. Given the main purpose of the algorithm is to reduce memory usage for large-scale networks, it is expected to see experiments on larger graphs where the large memory footprint becomes a real issue.\n\nDetailed comments:\n1. The description in Section 2.2 is not very clear. It would be better if the authors could provide a more detailed introduction to clique tree and the algorithms used.\n2. For the Chordal-GCN in algorithm 1, for epoch 2 onwards, do we also add consistent-loss when training leaves as well?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1302/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1302/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Chordal-GCN: Exploiting sparsity in training large-scale graph convolutional networks", "authors": ["Xin Jiang*", "Kewei Cheng*", "Song Jiang*", "Yizhou Sun"], "authorids": ["jiangxjames@ucla.edu", "viviancheng@cs.ucla.edu", "songjiang@cs.ucla.edu", "yzsun@cs.ucla.edu"], "keywords": ["graph convolutional network", "semi-supervised learning"], "abstract": "Despite the impressive success of graph convolutional networks (GCNs) on numerous applications, training on large-scale sparse networks remains challenging. Current algorithms require large memory space for storing GCN outputs as well as all the intermediate embeddings. Besides, most of these algorithms involves either random sampling or an approximation of the adjacency matrix, which might unfortunately lose important structure information. In this paper, we propose Chordal-GCN for semi-supervised node classification. The proposed model utilizes the exact graph structure (i.e., without sampling or approximation), while requires limited memory resources compared with the original GCN. Moreover, it leverages the sparsity pattern as well as the clustering structure of the graph. The proposed model first decomposes a large-scale sparse network into several small dense subgraphs (called cliques), and constructs a clique tree. By traversing the tree, GCN training is performed clique by clique, and connections between cliques are exploited via the tree hierarchy. Furthermore, we implement Chordal-GCN on large-scale datasets and demonstrate superior performance.\n", "pdf": "/pdf/1222fba0549d706b48143f965a3f266916f73bcb.pdf", "paperhash": "jiang|chordalgcn_exploiting_sparsity_in_training_largescale_graph_convolutional_networks", "TL;DR": "Chordal-GCN is a scalable graph neural network which exploits the exact graph structure (i.e., without approximation or sampling) and requires limited memory usage.", "code": "https://www.dropbox.com/s/0vby5gbu9qkbigr/chordal-gcn.zip?dl=0", "original_pdf": "/attachment/327700c18fda20197e0303f4e3f8784dd34392be.pdf", "_bibtex": "@misc{\njiang*2020chordalgcn,\ntitle={Chordal-{\\{}GCN{\\}}: Exploiting sparsity in training large-scale graph convolutional networks},\nauthor={Xin Jiang* and Kewei Cheng* and Song Jiang* and Yizhou Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=rJl05AVtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJl05AVtwB", "replyto": "rJl05AVtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1302/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1302/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575635778646, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1302/Reviewers"], "noninvitees": [], "tcdate": 1570237720485, "tmdate": 1575635778660, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1302/-/Official_Review"}}}, {"id": "B1e8GSdatH", "original": null, "number": 3, "cdate": 1571812622413, "ddate": null, "tcdate": 1571812622413, "tmdate": 1572972316718, "tddate": null, "forum": "rJl05AVtwB", "replyto": "rJl05AVtwB", "invitation": "ICLR.cc/2020/Conference/Paper1302/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose Chordal-GCN which is based on the chordal decomposition method post-ordered clique tree and propagates the features based on the order within each subgraph in order to reduce memory usage. The authors show that Chordal-GCN outperforms GCN [1] on all four datasets and argue that Chordal-GCN reduces memory usage.\nThe idea of using Chordal graphs to GCN is novel and interesting. However, my main concern lies in the experiment results.\n\n1) To my best knowledge, the proposed Chordal- match SOTA results on Cora, Citeceer, and Pubmed. However, since these datasets are small and easy to run, I would like to see the mean and standard deviation of the accuracy of all models you ran. Can you also provide the results of the commonly used \"random split setting\"[1]?\n\n2) What is the epoch time of the Chordal-GCN? Can you also report it in Table 2? Without including the pre-processing time, we don't know the overall training time of the method.\n\n3) Given that the main concern is the memory usage, the authors should compare to a strong baseline, SGC [2], which is a linear classifier trained on top of propagated features with memory/space complexity O(d) when using mini-batch training. This is much smaller than the proposed method O(Lc_2d + Ld^2).\nAlso, SGC is at least two magnitudes faster to train (2.7s vs 0.987*410=367.8s + unknown pre-processing time) and more accurate (94.9 vs 94.2) than the proposed Chordal-GCN on the largest Reddit dataset. The authors emphasize that the proposed method is scalable. Please compare it to SGC in Table 2. \nNevertheless, there is some chance that the authors can apply the same method to SGC and speed it up further as long as the preprocessing time is relatively small.\n\n4) Based on Table 2, Cluster-GCN uses less memory and is more accurate and faster to train than Chordal-GCN. Can you justify why people should use the proposed method instead?\n\n5) There are some missing citations. These papers [3,4,5,6] achieved previous SOTA results and should be included in the Tables. \n\nReferences:\n[1] Kipf and Welling: Semi-Supervised Classification with Graph Convolutional Networks (ICLR 2017)\n[2] Wu et al.: Simplifying Graph Convolutional Networks (ICML 2019)\n[3] Klicpera et al.: Predict then Propagate: Graph Neural Networks meet Personalized PageRank (ICLR 2019)\n[4] Gao and Ji: Graph U-Nets (ICML 2019)\n[5] Zhang et al.: GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (UAI 2018) \n[6] Fey: Just Jump: Dynamic Neighborhood Aggregation in Graph Neural Networks (ICLR-W 2019)"}, "signatures": ["ICLR.cc/2020/Conference/Paper1302/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1302/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Chordal-GCN: Exploiting sparsity in training large-scale graph convolutional networks", "authors": ["Xin Jiang*", "Kewei Cheng*", "Song Jiang*", "Yizhou Sun"], "authorids": ["jiangxjames@ucla.edu", "viviancheng@cs.ucla.edu", "songjiang@cs.ucla.edu", "yzsun@cs.ucla.edu"], "keywords": ["graph convolutional network", "semi-supervised learning"], "abstract": "Despite the impressive success of graph convolutional networks (GCNs) on numerous applications, training on large-scale sparse networks remains challenging. Current algorithms require large memory space for storing GCN outputs as well as all the intermediate embeddings. Besides, most of these algorithms involves either random sampling or an approximation of the adjacency matrix, which might unfortunately lose important structure information. In this paper, we propose Chordal-GCN for semi-supervised node classification. The proposed model utilizes the exact graph structure (i.e., without sampling or approximation), while requires limited memory resources compared with the original GCN. Moreover, it leverages the sparsity pattern as well as the clustering structure of the graph. The proposed model first decomposes a large-scale sparse network into several small dense subgraphs (called cliques), and constructs a clique tree. By traversing the tree, GCN training is performed clique by clique, and connections between cliques are exploited via the tree hierarchy. Furthermore, we implement Chordal-GCN on large-scale datasets and demonstrate superior performance.\n", "pdf": "/pdf/1222fba0549d706b48143f965a3f266916f73bcb.pdf", "paperhash": "jiang|chordalgcn_exploiting_sparsity_in_training_largescale_graph_convolutional_networks", "TL;DR": "Chordal-GCN is a scalable graph neural network which exploits the exact graph structure (i.e., without approximation or sampling) and requires limited memory usage.", "code": "https://www.dropbox.com/s/0vby5gbu9qkbigr/chordal-gcn.zip?dl=0", "original_pdf": "/attachment/327700c18fda20197e0303f4e3f8784dd34392be.pdf", "_bibtex": "@misc{\njiang*2020chordalgcn,\ntitle={Chordal-{\\{}GCN{\\}}: Exploiting sparsity in training large-scale graph convolutional networks},\nauthor={Xin Jiang* and Kewei Cheng* and Song Jiang* and Yizhou Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=rJl05AVtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJl05AVtwB", "replyto": "rJl05AVtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1302/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1302/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575635778646, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1302/Reviewers"], "noninvitees": [], "tcdate": 1570237720485, "tmdate": 1575635778660, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1302/-/Official_Review"}}}, {"id": "Sye40EdLKH", "original": null, "number": 1, "cdate": 1571353804159, "ddate": null, "tcdate": 1571353804159, "tmdate": 1571353804159, "tddate": null, "forum": "rJl05AVtwB", "replyto": "r1xfi7GlFH", "invitation": "ICLR.cc/2020/Conference/Paper1302/-/Official_Comment", "content": {"comment": "Thank you for your interest and comments. We answer your questions one by one below.\n\n(1) Ans: The above link is the code with hyperparameters tuned for Cora dataset, and it follows Kipf's Pytorch implementation [1]. In order to test on other datasets, please follow the hyperparameter setup reported in Table 4.\n \nFor Cora dataset, our provided code and the result in Table 2 do use 300 training nodes, not 140. The number 140 in the last column of Table 3 is a typo. We appreciate it a lot for your careful proofreading. We have re-run our code on Cora using the common label rate. The following result still shows the superior performance of Chordal-GCN compared with GCN, which is consistent with the results in our paper.\n \n \tLabel rate   Chordal-GCN   GCN\nCora\t140          81.20 \t78.36\nCora\t300          85.51 \t82.22\n \nWe have carefully checked other results: we follow the experimental setting for other datasets in Table 3 and we can reproduce the results in Table 2.\n\n(2) Ans: Even though using fixed data split is a convention, we think the trained model will be biased to the current test set. So we follow the random held-out strategy and randomly split the dataset for training and test multiple times. This random split strategy is used in all baseline models, and we think this is a fair and consistent setting.\n\n(3) Ans: Accuracy is exactly the same as micro-F1 score in any single-label classification task, with the only exception that some predicted labels are out of range (i.e., the class is an integer between 1 and 10 but one prediction is 11). This can be easily shown as follows: Assume there are n nodes and k classes, and then\n \nprecision = (TP_1+...+TP_k)/(TP_1+...+TP_k+FP_1+...+FP_k) = (TP_1+...+TP_k)/N = accuracy\nrecall\t= (TP_1+...+TP_k)/(TP_1+...+TP_k+FN_1+...+FN_k) = (TP_1+...+TP_k)/N = accuracy\nF1    \t= (2 x precision x recall) / (precision + recall) = accuracy.\n \nSince all the datasets used in our paper are single-labeled, we follow the convention to report the micro-F1 score.\n\n(4) Ans: The data format used in the uploaded code follows Kipf's implementation [1], and it is widely used in GCN related research.\n \nFor other datasets, please follow the hyperparameters reported in Table 4: the merging thresholds are different for different datasets.\n\n(5) Ans: The reported result for Reddit is consistent with Cluster-GCN [6]. The authors' code [3] does not provide the implementation for other datasets. So we write our own data loader for Cora, Citeseer, and Pubmed datasets. The GCN structure is different from [6], and we don\u2019t know the details about other hyperparameters used for these three datasets. These two reasons might explain the inconsistency of the results in our paper and [6].\n\n(6) Ans: The whole adjacency matrix is indeed used in the testing procedure, and this is the convention used in the original GCN [1,2,4] as well as its variants [5,6]. \n\nOriginal GCN can provide prediction for unseen nodes. The prediction will be accurate only if the network does not change too much; otherwise, everything has to be retrained. More importantly, such prediction still needs to use the entire graph to compute the latent representation for the new node and thus the predicted label.\n\nAlthough Chordal-GCN is proposed for large-scale training, it can also be used for online prediction. Since online inference occurs in the testing procedure and does not affect the training process, it won\u2019t be the memory bottleneck for Chordal-GCN.\n\n[1] https://github.com/tkipf/pygcn\n[2] https://github.com/tkipf/gcn\n[3] https://github.com/google-research/google-research/tree/master/cluster_gcn\n[4] T. N. Kipf, M. Welling, Semi-Supervised Classification with Graph Convolutional Networks (ICLR 2017)\n[5] J. Chen, J. Zhu, L. Son, Stochastic Training of Graph Convolutional Networks with Variance Reduction (ICML 2018)\n[6] W.-L. Chiang et al., Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks (KDD 2019)\n[7] J. Chen, T. Ma, C. Xiao, FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling (ICLR 2018)\n", "title": "Thank you for your interest and comments."}, "signatures": ["ICLR.cc/2020/Conference/Paper1302/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper1302/Reviewers", "ICLR.cc/2020/Conference/Paper1302/Area_Chairs"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1302/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Chordal-GCN: Exploiting sparsity in training large-scale graph convolutional networks", "authors": ["Xin Jiang*", "Kewei Cheng*", "Song Jiang*", "Yizhou Sun"], "authorids": ["jiangxjames@ucla.edu", "viviancheng@cs.ucla.edu", "songjiang@cs.ucla.edu", "yzsun@cs.ucla.edu"], "keywords": ["graph convolutional network", "semi-supervised learning"], "abstract": "Despite the impressive success of graph convolutional networks (GCNs) on numerous applications, training on large-scale sparse networks remains challenging. Current algorithms require large memory space for storing GCN outputs as well as all the intermediate embeddings. Besides, most of these algorithms involves either random sampling or an approximation of the adjacency matrix, which might unfortunately lose important structure information. In this paper, we propose Chordal-GCN for semi-supervised node classification. The proposed model utilizes the exact graph structure (i.e., without sampling or approximation), while requires limited memory resources compared with the original GCN. Moreover, it leverages the sparsity pattern as well as the clustering structure of the graph. The proposed model first decomposes a large-scale sparse network into several small dense subgraphs (called cliques), and constructs a clique tree. By traversing the tree, GCN training is performed clique by clique, and connections between cliques are exploited via the tree hierarchy. Furthermore, we implement Chordal-GCN on large-scale datasets and demonstrate superior performance.\n", "pdf": "/pdf/1222fba0549d706b48143f965a3f266916f73bcb.pdf", "paperhash": "jiang|chordalgcn_exploiting_sparsity_in_training_largescale_graph_convolutional_networks", "TL;DR": "Chordal-GCN is a scalable graph neural network which exploits the exact graph structure (i.e., without approximation or sampling) and requires limited memory usage.", "code": "https://www.dropbox.com/s/0vby5gbu9qkbigr/chordal-gcn.zip?dl=0", "original_pdf": "/attachment/327700c18fda20197e0303f4e3f8784dd34392be.pdf", "_bibtex": "@misc{\njiang*2020chordalgcn,\ntitle={Chordal-{\\{}GCN{\\}}: Exploiting sparsity in training large-scale graph convolutional networks},\nauthor={Xin Jiang* and Kewei Cheng* and Song Jiang* and Yizhou Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=rJl05AVtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJl05AVtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1302/Authors", "ICLR.cc/2020/Conference/Paper1302/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1302/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1302/Reviewers", "ICLR.cc/2020/Conference/Paper1302/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1302/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1302/Authors|ICLR.cc/2020/Conference/Paper1302/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158088, "tmdate": 1576860553070, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1302/Authors", "ICLR.cc/2020/Conference/Paper1302/Reviewers", "ICLR.cc/2020/Conference/Paper1302/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1302/-/Official_Comment"}}}, {"id": "r1xfi7GlFH", "original": null, "number": 1, "cdate": 1570935706008, "ddate": null, "tcdate": 1570935706008, "tmdate": 1570935706008, "tddate": null, "forum": "rJl05AVtwB", "replyto": "rJl05AVtwB", "invitation": "ICLR.cc/2020/Conference/Paper1302/-/Public_Comment", "content": {"comment": "I have carefully reviewed the code provided, and found something might be wrong:\n\n(1)\tIn the load_data function in utils.py, the authors randomly select 300 nodes for training, 200 nodes for validation and 1000 nodes for testing. This is totally different from the settings in table 3, which is the same as in the original papers of the baselines. The author claims that \u201cFor the baselines, I use the implementations provided by the authors, and follow the default parameter settings in these models\u201d in section 4.1. So, for example, according to table 3, 140 constant labeled nodes in the Cora dataset are used to train ordinary GCN. However, according to the code provided by the authors, 300 randomly labeled nodes are used to train Chordal-GCN, which means the authors used more supervised information. So, I don\u2019t think results in table 2 can give a fair comparison.\nSo, what is the performance of Chordal-GCN under a fair experimental setting?\nI revised the code of the authors, and randomly selected only 140 labeled nodes for training of Chordal-GCN. I ran the code 10 times, and the averaged accuracy is 76.9, which is much smaller than that reported in the paper. Then, I randomly selected 300 labeled nodes for training of the original GCN, and the averaged accuracy in 10 times is 84.6, which is much larger than that reported in the paper. These results are totally different from the results in table 2.\n(2)\tAccording to the code, in each time of training, the training, validation and testing nodes are randomly selected. So, testing samples are different in each time of running the code. Tough the authors claim \u201cwe conduct training for 10 times and take the mean of the evaluation results\u201d, I don\u2019t think the results are meaningful.\n(3)\tThe metric used in the paper is f1-score, while the evaluation used in the code is accuracy. This is confusing.\n(4)\tIn the load_data function, the authors claim \u201ccora only for now\u201d. The authors gave a new data format for the Cora dataset, but there lacks the corresponding new format for the other three datasets. And the load_data function in the code can only read the new data format. Considering the original Cora, Citeseer and Pubmed datsets share the same data format and are easy to use, I wonder why the authors presented a new format. And if the new format of other datasets is not ready, how was the experiments conducted?\nSo, I wrote a new load_data function for reading the original format of Cora, Citeseer and Pubmed. When conducting experiments on Cora, I can obtain the same results with the two load_data functions. However, when conducting experiments on Pubmed, the accuracy is only ~0.5.\n(5)\tThe reported results of Cluster-GCN are much lowers than those in the original paper.\n(6)\tAccording to the code, during the testing procedure, the whole adjacency matrix is still used for the inference of final prediction. This doesn\u2019t match the propose of the paper when conducting large-scale online prediction. And if we perform inference on each batch, I believe the performance of Chordal-GCN will be even l`ower.\n\nIn summary, I think the experimental results in this paper are hardly convincing.\n", "title": "Experimental results seem wrong in table 2."}, "signatures": ["~Qiang_Liu7"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Qiang_Liu7", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Chordal-GCN: Exploiting sparsity in training large-scale graph convolutional networks", "authors": ["Xin Jiang*", "Kewei Cheng*", "Song Jiang*", "Yizhou Sun"], "authorids": ["jiangxjames@ucla.edu", "viviancheng@cs.ucla.edu", "songjiang@cs.ucla.edu", "yzsun@cs.ucla.edu"], "keywords": ["graph convolutional network", "semi-supervised learning"], "abstract": "Despite the impressive success of graph convolutional networks (GCNs) on numerous applications, training on large-scale sparse networks remains challenging. Current algorithms require large memory space for storing GCN outputs as well as all the intermediate embeddings. Besides, most of these algorithms involves either random sampling or an approximation of the adjacency matrix, which might unfortunately lose important structure information. In this paper, we propose Chordal-GCN for semi-supervised node classification. The proposed model utilizes the exact graph structure (i.e., without sampling or approximation), while requires limited memory resources compared with the original GCN. Moreover, it leverages the sparsity pattern as well as the clustering structure of the graph. The proposed model first decomposes a large-scale sparse network into several small dense subgraphs (called cliques), and constructs a clique tree. By traversing the tree, GCN training is performed clique by clique, and connections between cliques are exploited via the tree hierarchy. Furthermore, we implement Chordal-GCN on large-scale datasets and demonstrate superior performance.\n", "pdf": "/pdf/1222fba0549d706b48143f965a3f266916f73bcb.pdf", "paperhash": "jiang|chordalgcn_exploiting_sparsity_in_training_largescale_graph_convolutional_networks", "TL;DR": "Chordal-GCN is a scalable graph neural network which exploits the exact graph structure (i.e., without approximation or sampling) and requires limited memory usage.", "code": "https://www.dropbox.com/s/0vby5gbu9qkbigr/chordal-gcn.zip?dl=0", "original_pdf": "/attachment/327700c18fda20197e0303f4e3f8784dd34392be.pdf", "_bibtex": "@misc{\njiang*2020chordalgcn,\ntitle={Chordal-{\\{}GCN{\\}}: Exploiting sparsity in training large-scale graph convolutional networks},\nauthor={Xin Jiang* and Kewei Cheng* and Song Jiang* and Yizhou Sun},\nyear={2020},\nurl={https://openreview.net/forum?id=rJl05AVtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJl05AVtwB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504196735, "tmdate": 1576860586263, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1302/Authors", "ICLR.cc/2020/Conference/Paper1302/Reviewers", "ICLR.cc/2020/Conference/Paper1302/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1302/-/Public_Comment"}}}], "count": 10}