{"notes": [{"id": "Bye5OiR5F7", "original": "SJgL3pKFFQ", "number": 377, "cdate": 1538087793639, "ddate": null, "tcdate": 1538087793639, "tmdate": 1545355438566, "tddate": null, "forum": "Bye5OiR5F7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Wasserstein proximal of GANs", "abstract": "We introduce a new method for training GANs by applying the Wasserstein-2 metric proximal on the generators. \nThe approach is based on the gradient operator induced by optimal transport, which connects the geometry of sample space and parameter space in implicit deep generative models. From this theory, we obtain an easy-to-implement regularizer for the parameter updates. Our experiments demonstrate that this method improves the speed and stability in training GANs in terms of wall-clock time and Fr\\'echet Inception Distance (FID) learning curves. ", "keywords": ["Optimal transport", "Wasserstein gradient", "Generative adversarial network", "Unsupervised learning"], "authorids": ["atlin@math.ucla.edu", "wcli@math.ucla.edu", "sjo@math.ucla.edu", "montufar@math.ucla.edu"], "authors": ["Alex Tong Lin", "Wuchen Li", "Stanley Osher", "Guido Montufar"], "TL;DR": "We propose the Wasserstein proximal method for training GANs. ", "pdf": "/pdf/9df0392be5f34787936b348563f3e8e10c62af7c.pdf", "paperhash": "lin|wasserstein_proximal_of_gans", "_bibtex": "@misc{\nlin2019wasserstein,\ntitle={Wasserstein proximal of {GAN}s},\nauthor={Alex Tong Lin and Wuchen Li and Stanley Osher and Guido Montufar},\nyear={2019},\nurl={https://openreview.net/forum?id=Bye5OiR5F7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "H1l7f-e8l4", "original": null, "number": 1, "cdate": 1545105675448, "ddate": null, "tcdate": 1545105675448, "tmdate": 1545354478616, "tddate": null, "forum": "Bye5OiR5F7", "replyto": "Bye5OiR5F7", "invitation": "ICLR.cc/2019/Conference/-/Paper377/Meta_Review", "content": {"metareview": "Both R3 and R1 argue for rejection, while R2 argues for a weak accept. Given that we have to reject borderline paper, the AC concludes with \"revise and resubmit\".", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Reject"}, "signatures": ["ICLR.cc/2019/Conference/Paper377/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper377/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein proximal of GANs", "abstract": "We introduce a new method for training GANs by applying the Wasserstein-2 metric proximal on the generators. \nThe approach is based on the gradient operator induced by optimal transport, which connects the geometry of sample space and parameter space in implicit deep generative models. From this theory, we obtain an easy-to-implement regularizer for the parameter updates. Our experiments demonstrate that this method improves the speed and stability in training GANs in terms of wall-clock time and Fr\\'echet Inception Distance (FID) learning curves. ", "keywords": ["Optimal transport", "Wasserstein gradient", "Generative adversarial network", "Unsupervised learning"], "authorids": ["atlin@math.ucla.edu", "wcli@math.ucla.edu", "sjo@math.ucla.edu", "montufar@math.ucla.edu"], "authors": ["Alex Tong Lin", "Wuchen Li", "Stanley Osher", "Guido Montufar"], "TL;DR": "We propose the Wasserstein proximal method for training GANs. ", "pdf": "/pdf/9df0392be5f34787936b348563f3e8e10c62af7c.pdf", "paperhash": "lin|wasserstein_proximal_of_gans", "_bibtex": "@misc{\nlin2019wasserstein,\ntitle={Wasserstein proximal of {GAN}s},\nauthor={Alex Tong Lin and Wuchen Li and Stanley Osher and Guido Montufar},\nyear={2019},\nurl={https://openreview.net/forum?id=Bye5OiR5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper377/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353237972, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bye5OiR5F7", "replyto": "Bye5OiR5F7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper377/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper377/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper377/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353237972}}}, {"id": "SygKmSa30Q", "original": null, "number": 6, "cdate": 1543456032987, "ddate": null, "tcdate": 1543456032987, "tmdate": 1543456032987, "tddate": null, "forum": "Bye5OiR5F7", "replyto": "ryebQIVYA7", "invitation": "ICLR.cc/2019/Conference/-/Paper377/Official_Comment", "content": {"title": "Our reply to a Clarifying Question", "comment": "Thanks for your reply! You are correct. While the proximal in itself is not an approximation, we indeed are making approximations that serve the purpose of obtaining a robust scheme which is as simple as possible. \n\nTo further clarify our comment: \n\nThe proximal operator is a way of formulating a sequential optimization method. This is not an approximation. In the proximal formulation, each parameter update is expressed implicitly as the minimizer of the original objective function plus a penalty on the size of the step. In the case of the Wasserstein proximal, the size of the step is measured by means of the Wasserstein metric. \n\nOur motivation for using the proximal formulation of the Wasserstein gradient is twofold: \n1) it allows us to compute the parameter updates efficiently. \n2) it is stable and can handle non-smooth loss functions (the Wasserstein-1 loss in WGANs is non-smooth). \n\nBy working with the constrained Wasserstein distance, we can obtain a tractable (finite dimensional) version of the proximal. The SBE method is obtained by second-order expansion of the proximal penalty term d(theta,theta_k). This is an approximation as much as any finite step size gradient method is an approximation of the infinitesimal gradient flow. \n\nWe do introduce an approximation when we derive SBE (see above) and RWP (specifically, we drop the gradient constraint on \\Phi). Our experiments show that RWP offers a robust easy to implement optimization method, which can obtain faster and more stable convergence in state-of-the-art GANs. "}, "signatures": ["ICLR.cc/2019/Conference/Paper377/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper377/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper377/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein proximal of GANs", "abstract": "We introduce a new method for training GANs by applying the Wasserstein-2 metric proximal on the generators. \nThe approach is based on the gradient operator induced by optimal transport, which connects the geometry of sample space and parameter space in implicit deep generative models. From this theory, we obtain an easy-to-implement regularizer for the parameter updates. Our experiments demonstrate that this method improves the speed and stability in training GANs in terms of wall-clock time and Fr\\'echet Inception Distance (FID) learning curves. ", "keywords": ["Optimal transport", "Wasserstein gradient", "Generative adversarial network", "Unsupervised learning"], "authorids": ["atlin@math.ucla.edu", "wcli@math.ucla.edu", "sjo@math.ucla.edu", "montufar@math.ucla.edu"], "authors": ["Alex Tong Lin", "Wuchen Li", "Stanley Osher", "Guido Montufar"], "TL;DR": "We propose the Wasserstein proximal method for training GANs. ", "pdf": "/pdf/9df0392be5f34787936b348563f3e8e10c62af7c.pdf", "paperhash": "lin|wasserstein_proximal_of_gans", "_bibtex": "@misc{\nlin2019wasserstein,\ntitle={Wasserstein proximal of {GAN}s},\nauthor={Alex Tong Lin and Wuchen Li and Stanley Osher and Guido Montufar},\nyear={2019},\nurl={https://openreview.net/forum?id=Bye5OiR5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper377/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623081, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bye5OiR5F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper377/Authors", "ICLR.cc/2019/Conference/Paper377/Reviewers", "ICLR.cc/2019/Conference/Paper377/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper377/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper377/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper377/Authors|ICLR.cc/2019/Conference/Paper377/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper377/Reviewers", "ICLR.cc/2019/Conference/Paper377/Authors", "ICLR.cc/2019/Conference/Paper377/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623081}}}, {"id": "r1xrnt76aX", "original": null, "number": 4, "cdate": 1542433197147, "ddate": null, "tcdate": 1542433197147, "tmdate": 1542433197147, "tddate": null, "forum": "Bye5OiR5F7", "replyto": "r1lL5EbTnm", "invitation": "ICLR.cc/2019/Conference/-/Paper377/Official_Comment", "content": {"title": "Response to AnonReviewer3 continued", "comment": "3. Q:*** the \"semi-backward Euler method\" is introduced without any context. The fact that it is presented as a proposition using qualitative qualifiers such as \"sufficient regularity\" is suspicious. \n\nAnswer: The semi-backward Euler method is proposed as an effective approximation to the Wasserstein proximal operator. The motivation for this is that solving the backward method involves solving the entire path for the constrained metric. This can be hard in numerics. The time reparameterization and semi-backward Euler idea allow us to derive a simple computational method. It just introduces a simple primal-dual minimization for the computation of the gradient operator. These details are available in any book on numerical methods for differential equations, including the backward Euler and semi-backward Euler method. \n\nSufficient regularity is for the rigorous mathematical statement. For the simplicity of presentation, we omitted the details. The current space of Phi requires knowledge of the generator, in which (nabla Phi)\\in L^2(\\rho(theta_k)). In later computations, we apply neural networks to solve the problem. This becomes a standard finite dimensional maximation. As long as the network model of Phi has a well defined (nabla Phi)^2, the inf-supremum problem is well posed. \n\n\nFurther comments:\n\nWe next comment on AnonReviewer3\u2019s comments on the organization, description of the paper, and judgement. \n\nOrganization: AnonReviewer3 thinks that the paper is ``head over heels\u2019\u2019. Having first motivation and then a simplified method, or first a simplified method and then motivation, is really open for debate. Yes, we could have presented a simple method first, and then the motivation. We think the paper is organized in a way that most readers should be able to navigate it with relative ease. It is also natural to first formulate a mathematical problem and general method, to then explore avenues for obtaining effective practical simplifications. The mathematical derivation provides intuition, motivation, and allows the reader to track the approximations. It also provides practical points of departure for developing further methods and alternative simplifications. \n\nJudgement: \u201cI believe that a large pool of readers at ICLR will be extremely disappointed and frustrated to see all of this relatively arduous technical presentation produce such a simple result which, in essence, has absolutely nothing to do with the Wasserstein distance, nor with a \"Wasserstein natural gradient\".\u201d\n\nOne can also take the standpoint that a good paper should provide a theoretically sound and well-motivated derivation which ultimately leads to a simple and practical algorithm, which is precisely what we are doing. \n\nWe are stunned about AnonReviewer3 claim that the method ``has absolutely nothing to do with the Wasserstein distance, nor with a Wasserstein natural gradient\u2019\u2019. This is a gross mischaracterization of the paper. \n\nIt is disappointing that AnonReviewer3 claims ``The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature\u2019\u2019, when his or her comments indicate quite the contrary: \u201cThis metric defines a Riemannian metric between two parameters, by considering the resulting vector field that solves this equation.\u201d\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper377/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper377/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper377/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein proximal of GANs", "abstract": "We introduce a new method for training GANs by applying the Wasserstein-2 metric proximal on the generators. \nThe approach is based on the gradient operator induced by optimal transport, which connects the geometry of sample space and parameter space in implicit deep generative models. From this theory, we obtain an easy-to-implement regularizer for the parameter updates. Our experiments demonstrate that this method improves the speed and stability in training GANs in terms of wall-clock time and Fr\\'echet Inception Distance (FID) learning curves. ", "keywords": ["Optimal transport", "Wasserstein gradient", "Generative adversarial network", "Unsupervised learning"], "authorids": ["atlin@math.ucla.edu", "wcli@math.ucla.edu", "sjo@math.ucla.edu", "montufar@math.ucla.edu"], "authors": ["Alex Tong Lin", "Wuchen Li", "Stanley Osher", "Guido Montufar"], "TL;DR": "We propose the Wasserstein proximal method for training GANs. ", "pdf": "/pdf/9df0392be5f34787936b348563f3e8e10c62af7c.pdf", "paperhash": "lin|wasserstein_proximal_of_gans", "_bibtex": "@misc{\nlin2019wasserstein,\ntitle={Wasserstein proximal of {GAN}s},\nauthor={Alex Tong Lin and Wuchen Li and Stanley Osher and Guido Montufar},\nyear={2019},\nurl={https://openreview.net/forum?id=Bye5OiR5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper377/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623081, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bye5OiR5F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper377/Authors", "ICLR.cc/2019/Conference/Paper377/Reviewers", "ICLR.cc/2019/Conference/Paper377/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper377/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper377/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper377/Authors|ICLR.cc/2019/Conference/Paper377/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper377/Reviewers", "ICLR.cc/2019/Conference/Paper377/Authors", "ICLR.cc/2019/Conference/Paper377/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623081}}}, {"id": "rye0fY76p7", "original": null, "number": 3, "cdate": 1542433046008, "ddate": null, "tcdate": 1542433046008, "tmdate": 1542433046008, "tddate": null, "forum": "Bye5OiR5F7", "replyto": "r1lL5EbTnm", "invitation": "ICLR.cc/2019/Conference/-/Paper377/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Summary of the response: AnonReviewer3 misunderstands important parts of the paper. \n\nThe reviewer misinterprets the relations between distance and the gradient operator. We will explain these relations below. We first present the detailed comments to reviewer 3\u2019s questions. \n\n1. Q: *** \"Wasserstein-2 distance on the full density set\": what do you mean exactly? that d_W(\\theta_0,\\theta_1) \\ne W(p_{\\theta_0},p_{\\theta_1})? Could you elaborate where this analogy breaks down?\n\nAnswer: Distance on the full density set is the distance measured when we are free to move without any constraints on the tangent direction. When we have a parametrized set of densities (e.g., a neural network), the Wasserstein-2 induced metric is constrained, as we can only move along the set of densities from our parameterized set. \n\nFor a visual example, imagine measuring the distance between two points in 3D by the length of a string spanned between the two points vs. measuring the distance between two points on a sphere in 3D by the length of a string on the sphere connecting the two points. The two distances are different. \n\nWhen and how the constrained Wasserstein metric is different from the one in the full density set has been studied in Theorem 7 and Proposition 8 of [arXiv:1803:07033]. It is mainly based on the derivation of the second fundamental formula in the Wasserstein geometry. The proof is as follows: One checks that the constrained metric tensor coincides with the one for the unconstrained set. It is to show that the second fundamental formula equals zero, for a given probability model, e.g. following the derived second fundamental formula, one can check that the Wasserstein metric constrained in the mixtures of Gaussians are not geodetically complete. \n\nThere are some cases where the distance in the full set equals the distance in a constrained set (known as the totally geodesic submanifold). For Wasserstein-2 metric, it is well known that the set of Gaussian distributions has this property. For details, we refer the reader to [arXiv:0801.2250]. One can check that it simply satisfies Proposition 8 of [arXiv:1803:07033], in which the second fundamental form equals zero, which proves that Gaussian measures are totally geodesic submanifold. \n\nWe are happy to add comments about this in the revised paper to clarify these relations. \n\n2.Q: *** It is not clear to me why the dependency of \\Phi in t has disappeared in Theorem 2. It is not clear either in your statement whether \\Phi is optimal at all for the problem in Theorem 1. \n\nAnswer: These two questions relate to the definition of metric structure. We address them in turn.\n\na) The metric introduces a norm in the tangent space. The metric norm is defined at each point, which should not depend on time. The gradient operator is also defined at each point, and should not depend on the time either. \nFor example, for any metric function of Theta defined by the action functional \n         d(theta_0, theta_1)^2 = inf_{theta(t)} {int_0^1 dot theta(s)^T G(theta(s)) dot theta(s) ds }.\nThe metric at theta means    \n             (a,b)_theta = a^T  G(theta) b, where a, b are tangent vectors at theta. \nFor any theta in Theta, the Riemannian gradient is defined by \n                           Grad f(theta)= G(theta)^{-1} nabla_theta f(theta). \nIn the context of the Wasserstein-2 metric, the metric tensor is the inverse of the weighted Laplacian operator. One way for representing it is through Phi, which is the dual variable in the tangent space. According to the above-mentioned definitions, Phi does not depend on time. We refer AnonReviewer3 to [arXiv: 1803.06360] for detailed explanations.\n\nb) Kindly note that Theorems 1 and 2 describe the constrained metric. This is a new optimal transport metric in the parameter space (of the generators). The constrained metric structure does not break the metric tensor structure. It just constrains to the direction that is feasible on the parameter space. We refer AnonReviewer3 to [S Amari. Natural Gradient Works Efficiently in Learning, 1998]. All the proofs locally in time of the unconstrained setting still works here. Theorem 1 pulls back the optimal transport metric tensor to the parameter space. For more related discussions, we refer AnonReviewer3 to [arXiv:1803:07033]."}, "signatures": ["ICLR.cc/2019/Conference/Paper377/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper377/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper377/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein proximal of GANs", "abstract": "We introduce a new method for training GANs by applying the Wasserstein-2 metric proximal on the generators. \nThe approach is based on the gradient operator induced by optimal transport, which connects the geometry of sample space and parameter space in implicit deep generative models. From this theory, we obtain an easy-to-implement regularizer for the parameter updates. Our experiments demonstrate that this method improves the speed and stability in training GANs in terms of wall-clock time and Fr\\'echet Inception Distance (FID) learning curves. ", "keywords": ["Optimal transport", "Wasserstein gradient", "Generative adversarial network", "Unsupervised learning"], "authorids": ["atlin@math.ucla.edu", "wcli@math.ucla.edu", "sjo@math.ucla.edu", "montufar@math.ucla.edu"], "authors": ["Alex Tong Lin", "Wuchen Li", "Stanley Osher", "Guido Montufar"], "TL;DR": "We propose the Wasserstein proximal method for training GANs. ", "pdf": "/pdf/9df0392be5f34787936b348563f3e8e10c62af7c.pdf", "paperhash": "lin|wasserstein_proximal_of_gans", "_bibtex": "@misc{\nlin2019wasserstein,\ntitle={Wasserstein proximal of {GAN}s},\nauthor={Alex Tong Lin and Wuchen Li and Stanley Osher and Guido Montufar},\nyear={2019},\nurl={https://openreview.net/forum?id=Bye5OiR5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper377/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623081, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bye5OiR5F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper377/Authors", "ICLR.cc/2019/Conference/Paper377/Reviewers", "ICLR.cc/2019/Conference/Paper377/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper377/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper377/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper377/Authors|ICLR.cc/2019/Conference/Paper377/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper377/Reviewers", "ICLR.cc/2019/Conference/Paper377/Authors", "ICLR.cc/2019/Conference/Paper377/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623081}}}, {"id": "BJx2fwQpTX", "original": null, "number": 2, "cdate": 1542432531766, "ddate": null, "tcdate": 1542432531766, "tmdate": 1542432531766, "tddate": null, "forum": "Bye5OiR5F7", "replyto": "ByxW6xld2X", "invitation": "ICLR.cc/2019/Conference/-/Paper377/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Thank you for your comments! \n\nSummary of the response: Proximal is not an approximation. RWP is an easy to implement method with a rigorous mathematical motivation. RWP is different from the naive proximal. Natural gradients define distances based on the actual arguments of the objective function and are natural in this sense. Our proximal Wasserstein natural gradient solves the two major bottlenecks for using natural gradients in practice, namely tractability, and stability. Experiments show that our method is faster and more stable than state-of-the-art methods. \n\nIn terms of the cons: \nWe agree that the end result is simple, which was also our intention. However, we do not agree that it can be viewed as a naive application of the proximal operator. A naive implementation of a proximal operator would penalize the size of the parameter update, i.e., \\|theta - theta_{k-1}\\|^2, but our method penalizes the expected value of the distance between generator samples, i.e., \\|g_\\theta(z) - g_{\\theta^{k-1}}(z)\\|^2. \n\nWe do agree that we have many theoretical derivations, and the resulting application is simple. Indeed, in the paper, we seek for a simple and easy to implement method. We humbly argue that, even if the end result is simple, the theoretical derivations give a mathematically sound motivation, interpretation, and a basis to continue working on related methods. In particular, we formulate the Wasserstein proximal and the Wasserstein Semi-backward Euler methods, both of which can be used as a starting point for practical optimization methods for GANs. \n\nIn regard to the natural geometry of parameter space: \nWe agree that the natural choice of a geometry for parameter space is not obvious. However, it is natural to define an optimization method that is independent of the parametrization, which is the case for the Wasserstein gradient but is not the case for the Euclidean gradient. Natural gradients define the geometry on parameter space based on the geometry of the functions that these parameters represent. Since the objective functions that we are optimizing depend on these functions, but not on the parameter, the natural gradients are the natural choice. The Fisher natural gradient has been observed to perform better than the Euclidean gradient in numerous statistics and machine learning applications. \n\nThere are two main reasons why natural gradients are not commonly used in the place of regular gradients: 1) The computation is expensive. 2) The numerics might be unstable, especially when the step size is large. In both regards, our proposed method performs well: 1) Our RWP has no significant additional cost in computation over the regular gradient. 2) Our experiments show that the method is stable and even delivers a more stable convergence than regular gradients. In summary, by using the proximal formulation of the natural Wasserstein gradient, we address the two major obstacles in using natural gradients. \n\nIn Section 2.3 we use a toy example to illustrate the differences between Wasserstein and standard gradients. For this example, we show that for any fixed step size, the Wasserstein proximal is better than the Euclidean proximal, which means that the Wasserstein gradient flow is better than the Euclidean gradient flow. \n\nThe SBE and RWP are two ways of implementing the Wasserstein proximal. SBE is a precise method, although the parameter update is defined implicitly and requires solving an additional optimization problem. On the other hand, RWP tries to formulate a method that is as simple as possible, even if it is not as accurate. \n\nWGANs try to optimize the Wasserstein-1 loss, but currently there are no known practical means of computing the Wasserstein-1 loss. In the literature, the performance of trained GANs has been evaluated by means of visual inspection of the samples, and, more recently, more quantitative methods have been proposed. FID is currently considered the most informative one. This is why we focus on FID. Finding good measures for the performance of a trained GAN is a pressing and active area of investigation in the context of GANs. Addressing this is in any case beyond the scope of this paper. \n\nThanks again for your helpful comments.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper377/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper377/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper377/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein proximal of GANs", "abstract": "We introduce a new method for training GANs by applying the Wasserstein-2 metric proximal on the generators. \nThe approach is based on the gradient operator induced by optimal transport, which connects the geometry of sample space and parameter space in implicit deep generative models. From this theory, we obtain an easy-to-implement regularizer for the parameter updates. Our experiments demonstrate that this method improves the speed and stability in training GANs in terms of wall-clock time and Fr\\'echet Inception Distance (FID) learning curves. ", "keywords": ["Optimal transport", "Wasserstein gradient", "Generative adversarial network", "Unsupervised learning"], "authorids": ["atlin@math.ucla.edu", "wcli@math.ucla.edu", "sjo@math.ucla.edu", "montufar@math.ucla.edu"], "authors": ["Alex Tong Lin", "Wuchen Li", "Stanley Osher", "Guido Montufar"], "TL;DR": "We propose the Wasserstein proximal method for training GANs. ", "pdf": "/pdf/9df0392be5f34787936b348563f3e8e10c62af7c.pdf", "paperhash": "lin|wasserstein_proximal_of_gans", "_bibtex": "@misc{\nlin2019wasserstein,\ntitle={Wasserstein proximal of {GAN}s},\nauthor={Alex Tong Lin and Wuchen Li and Stanley Osher and Guido Montufar},\nyear={2019},\nurl={https://openreview.net/forum?id=Bye5OiR5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper377/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623081, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bye5OiR5F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper377/Authors", "ICLR.cc/2019/Conference/Paper377/Reviewers", "ICLR.cc/2019/Conference/Paper377/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper377/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper377/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper377/Authors|ICLR.cc/2019/Conference/Paper377/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper377/Reviewers", "ICLR.cc/2019/Conference/Paper377/Authors", "ICLR.cc/2019/Conference/Paper377/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623081}}}, {"id": "SyeU6ImapX", "original": null, "number": 1, "cdate": 1542432446472, "ddate": null, "tcdate": 1542432446472, "tmdate": 1542432446472, "tddate": null, "forum": "Bye5OiR5F7", "replyto": "B1xtf8_wh7", "invitation": "ICLR.cc/2019/Conference/-/Paper377/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "Thank you for your comments! \n\nSummary of the response: RWP is an easy to implement method with a rigorous mathematical motivation. It can be interpreted as a stochastic version of SBE. Proximal is not an approximation. The proximal update is defined implicitly as the optimizer of a subproblem, which is why we have several generator updates per outer iteration. Experiments show that our method is faster and more stable than state-of-the-art methods. \n\nIn terms of what RWP corresponds to: \nRWP can be seen as a stochastic version of the Semi-Backward Euler approach. To see this, note that if we write the loss function as Loss(G) + E[Phi(G_theta) \u2013 Phi(G_theta_k-1) - (\u00bd) grad(Phi)(G_theta_k-1)^2], and constrain Phi(x) = a^Tx + b, and to one sample, then solving for the parameters a and b (setting the gradient equal to zero), we obtain RWP for each single patch. We will make this clearer in the paper. \n\nIn terms of approximations in the proximal method: \nThe proximal operator is a way of formulating a sequential optimization method. This is not an approximation. In the proximal formulation, each parameter update is expressed implicitly as the minimizer of the original objective function plus a penalty on the size of the step. In the case of the Wasserstein proximal, the size of the step is measured by means of the Wasserstein metric. \n\nOur motivation for using the proximal formulation of the Wasserstein gradient is twofold: \n1) it allows us to compute the parameter updates efficiently. \n2) it is stable and can handle non-smooth loss functions (the Wasserstein-1 loss in WGANs is non-smooth). \n\nBy working with the constrained Wasserstein distance, we can obtain a tractable (finite dimensional) version of the proximal. The SBE method is obtained by second-order expansion of the proximal penalty term d(theta,theta_k). This is an approximation as much as any finite step size gradient method is an approximation of the infinitesimal gradient flow. \n\nWe do introduce an approximation when we derive SBE (see above) and RWP (specifically, we drop the gradient constraint on \\Phi). This serves the purpose of obtaining a scheme that is as simple as possible. Many practical algorithms follow the principle of simplicity and often work more robustly than other theoretically more accurate methods. Our experiments are in line with this idea and show that RWP offers a robust easy to implement optimization method, which can obtain faster and more stable convergence in state-of-the-art GANs. \n\nIn terms of the outer iterations: \nYes, many approaches to GAN training advocate updating the generator once every outer iteration. However, note that our inner iteration is solving for the proximal update. The proximal step is defined implicitly as the minimizer of a subproblem. In order to obtain the update, we run a short optimization loop, which is the sequence of generator updates per outer iteration. We have found that updating the generator multiple times in every outer iteration leads to better results. We think that this is a valuable observation. \n\nIn the experiments: \nWe do believe the results are significant, as we are comparing wallclock time. This can be especially seen in DRAGAN. Even in CIFAR10 with WGAN-GP, if we look at the average lines we observe that RWP is about 20% faster at reaching the same final FID value. And it also achieves a lower FID at the same time. We agree that more hyperparameter tuning will probably give even better results. \n\nIn terms of our illustration with the toy example: We are happy to add more intuitions and illustrations to convey a clearer picture of what the proximal method does in comparison to the gradient method. \n\nFor the stability results: \nYes, it is the curves, namely when we trained WGAN-GP for one million iterations with and without RWP regularization. We find the curves are less oscillatory with RWP. \n\nThanks again for your comments.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper377/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper377/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper377/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein proximal of GANs", "abstract": "We introduce a new method for training GANs by applying the Wasserstein-2 metric proximal on the generators. \nThe approach is based on the gradient operator induced by optimal transport, which connects the geometry of sample space and parameter space in implicit deep generative models. From this theory, we obtain an easy-to-implement regularizer for the parameter updates. Our experiments demonstrate that this method improves the speed and stability in training GANs in terms of wall-clock time and Fr\\'echet Inception Distance (FID) learning curves. ", "keywords": ["Optimal transport", "Wasserstein gradient", "Generative adversarial network", "Unsupervised learning"], "authorids": ["atlin@math.ucla.edu", "wcli@math.ucla.edu", "sjo@math.ucla.edu", "montufar@math.ucla.edu"], "authors": ["Alex Tong Lin", "Wuchen Li", "Stanley Osher", "Guido Montufar"], "TL;DR": "We propose the Wasserstein proximal method for training GANs. ", "pdf": "/pdf/9df0392be5f34787936b348563f3e8e10c62af7c.pdf", "paperhash": "lin|wasserstein_proximal_of_gans", "_bibtex": "@misc{\nlin2019wasserstein,\ntitle={Wasserstein proximal of {GAN}s},\nauthor={Alex Tong Lin and Wuchen Li and Stanley Osher and Guido Montufar},\nyear={2019},\nurl={https://openreview.net/forum?id=Bye5OiR5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper377/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623081, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bye5OiR5F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper377/Authors", "ICLR.cc/2019/Conference/Paper377/Reviewers", "ICLR.cc/2019/Conference/Paper377/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper377/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper377/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper377/Authors|ICLR.cc/2019/Conference/Paper377/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper377/Reviewers", "ICLR.cc/2019/Conference/Paper377/Authors", "ICLR.cc/2019/Conference/Paper377/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623081}}}, {"id": "r1lL5EbTnm", "original": null, "number": 3, "cdate": 1541375117879, "ddate": null, "tcdate": 1541375117879, "tmdate": 1541534046467, "tddate": null, "forum": "Bye5OiR5F7", "replyto": "Bye5OiR5F7", "invitation": "ICLR.cc/2019/Conference/-/Paper377/Official_Review", "content": {"title": "ultimately, I am not sure there is anything \"Wasserstein\" going on in this new GAN algorithm.", "review": "The authors propose a new GAN procedure. It's maybe easier to reverse-engineer it from the simplest of all places, that is p.16 in the appendix which makes explicit the difference between this GAN and the original one: the update in the generator is carried out l times and takes into account points generated in the previous iteration. \n\nTo get there, the authors take the following road: they exploit the celebrated Benamou-Brenier formulation of the W2 distance between probability measures, which involves integrating over a vector field parameterized in time. The W2 distance which is studied here is not exactly that corresponding to the measures associated with these two parameters, but instead an adaptation of BB to parameterized measures (\"constrained\"). This metric defines a Riemannian metric between two parameters, by considering the resulting vector field that solve this equation (I guess evaluated at time 0). The authors propose to use the natural gradient associated with that Riemannian metric (Theorem 2). Using exactly that natural gradient would involve solving an optimal transport problem (compute the optimal displacement field) and inverting the corresponding operator. The authors mention that, equivalently, a JKO type step could also be considered to obtain an update for \\theta. The authors propose two distinct approximations, a \"semi-backward Euler formulation\", and, next, a simplification of the d_W, which, exploiting the fact that one of the parameterized measures is the push foward of a Gaussian, simplifies to a simpler problem (Prop. 4). That problem introduces a new type of constraint (Gradient constraint) which is yet again simplified.\n\nIn the end, the metric considered on the parameter space is fairly trivial and boils down to the r.h.s. of equation 4. It's essentially an expected squared distance between the new and the old parameter under a Gaussian prior for the encoder.  This yields back the simplification laid out in p.16.\n\nI think the paper is head over heels. It can be caricatured as extreme obfuscation for a very simple modification of the basic GAN algorithm. Although I am *not* claiming this is the intention of the authors, and can very well believe that they found it interesting that so many successive simplifications would yield such a simple modification, I believe that a large pool of readers at ICLR will be extremely disappointed and frustrated to see all of this relatively arduous technical presentation produce such a simple result which, in essence, has absolutely nothing to do with the Wasserstein distance, nor with a \"Wasserstein natural gradient\".\n\nother comments::\n\n*** \"Wasserstein-2 distance on the full density set\": what do you mean exactly? that d_W(\\theta_0,\\theta_1) \\ne W(p_{\\theta_0},p_{\\theta_1})? Could you elaborate where this analogy breaks down? \n\n*** It is not clear to me why the dependency of \\Phi in t has disappeared in Theorem 2. It is not clear either in your statement whether \\Phi is optimal at all for the problem in Theorem 1.\n\n*** the \"semi-backward Euler method\" is introduced without any context. The fact that it is presented as a proposition using qualitative qualifiers such as \"sufficient regularity\" is suspicious. ", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper377/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein proximal of GANs", "abstract": "We introduce a new method for training GANs by applying the Wasserstein-2 metric proximal on the generators. \nThe approach is based on the gradient operator induced by optimal transport, which connects the geometry of sample space and parameter space in implicit deep generative models. From this theory, we obtain an easy-to-implement regularizer for the parameter updates. Our experiments demonstrate that this method improves the speed and stability in training GANs in terms of wall-clock time and Fr\\'echet Inception Distance (FID) learning curves. ", "keywords": ["Optimal transport", "Wasserstein gradient", "Generative adversarial network", "Unsupervised learning"], "authorids": ["atlin@math.ucla.edu", "wcli@math.ucla.edu", "sjo@math.ucla.edu", "montufar@math.ucla.edu"], "authors": ["Alex Tong Lin", "Wuchen Li", "Stanley Osher", "Guido Montufar"], "TL;DR": "We propose the Wasserstein proximal method for training GANs. ", "pdf": "/pdf/9df0392be5f34787936b348563f3e8e10c62af7c.pdf", "paperhash": "lin|wasserstein_proximal_of_gans", "_bibtex": "@misc{\nlin2019wasserstein,\ntitle={Wasserstein proximal of {GAN}s},\nauthor={Alex Tong Lin and Wuchen Li and Stanley Osher and Guido Montufar},\nyear={2019},\nurl={https://openreview.net/forum?id=Bye5OiR5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper377/Official_Review", "cdate": 1542234475181, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Bye5OiR5F7", "replyto": "Bye5OiR5F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper377/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335710573, "tmdate": 1552335710573, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper377/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ByxW6xld2X", "original": null, "number": 2, "cdate": 1541042360914, "ddate": null, "tcdate": 1541042360914, "tmdate": 1541534046225, "tddate": null, "forum": "Bye5OiR5F7", "replyto": "Bye5OiR5F7", "invitation": "ICLR.cc/2019/Conference/-/Paper377/Official_Review", "content": {"title": "Providing an easy-to-implement drop-in regularizer framework, which may simply be viewed as a naive application of the proximal operator.", "review": "\n[summary]\nThis paper considers natural gradient learning in GAN learning, where the Riemannian structure induced by the Wasserstein-2 distance is employed. More concretely, the constrained Wasserstein-2 metric $d_W$, the geodesic distance on the parameter space induced by the Wasserstein-2 distance in the ambient space, is introduced (Theorem 1). The natural gradient on the parameter space with respect to the constrained Wasserstein-2 metric is then derived (Theorem 2). Since direct evaluation of $G^{-1}$ poses difficulty, the authors go on to considering a backward scheme using the proximal operator (3), yielding:\n(i) The Semi-Backward Euler method is proposed via a second-order Taylor approximation of the proximal operator $d_W^2$ (Proposition 3).\n(ii) From an alternative formulation for $d_W$ (Proposition 4), the authors propose dropping the gradient constraint to define a relaxed Wasserstein metric $d$, yielding a simple proximal operator given by the expected squared Euclidean distance in the sample space used as a regularizer (equation (4)). The resulting algorithm is termed the Relaxed Wasserstein Proximal (RWP) algorithm.\n\n[pros]\nThe proposal provides an easy-to-implement drop-in regularizer framework, so that it can straightforwardly be combined with various generator update schemes.\n\n[cons]\nDespite all the theoretical arguments given to justify the proposal, the resulting proposal may simply be viewed as a naive application of the proximal operator.\n\n[Quality]\nSee [Detailed comments] section below.\n\n[Clarity]\nThis paper is basically clearly written.\n\n[Originality]\nProviding justification to the proximal operator approach in GAN learning via natural gradient with respect to the Riemannian structure seems original.\n\n[Significance]\nSee [Detailed comments] section below.\n\n[Detailed comments]\nTo the parameter space $\\Theta\\subset\\mathbb{R}^d$, one can consider introducing several different Riemannian structures, including the conventional Euclidean structure and that induced by the Wasserstein-2 metric. Which Riemannian structure among all these possibilities would be natural and efficient in GAN training would not be evident, and this paper discusses this issue only in the very special single instance in Section 2.3. A more thorough argument supporting superiority of the Riemannian structure induced by the Wasserstein-2 metric would thus be needed in order to justify the proposed approach.\n\nIn relation to this, the result of comparison between WGAN-GP with and without SBE shown in Figure 5 is embarrassing to me, since it might suggest that the proposed framework aiming at performing Wasserstein natural gradient is not so efficient if combined with WGAN-GP. The natural gradient is expected to be efficient when the underlying coordinate system is non-orthonormal (Amari, 1998). Starting with the gradient descent iteration derived from the backward Euler method in (3), which is computationally hard, the argument in this paper goes on to propose two methods: the Semi-Backward Euler method via a second-order Taylor approximation to the backward Euler scheme (Proposition 3), and RWP in (4) via approximation (dropping of the gradient constraint and finite-difference approximation in the integral with respect to $t$) of an alternative simpler formulation for the Wasserstein metric (Proposition 4). These two methods involve different approximations to the Semi-Backward Euler, and one would like to know why the approximations in the latter method is better in performance than those in the former. Discussion on this point is however missing in this paper.\n\nIn Section 3, it would have been better if the performance be compared not only in terms of FID but also the loss considered (i.e., Wasserstein-1), since the latter is exactly what the algorithms are trying to optimize.\n\nMinor points:\n\nPage 4: The line just after equation (4) should be moved to the position following the equation giving $d(\\theta_0,\\theta_1)^2$.\n\nIn the reference list, the NIPS paper by Gulrajani et al. appears twice.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper377/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein proximal of GANs", "abstract": "We introduce a new method for training GANs by applying the Wasserstein-2 metric proximal on the generators. \nThe approach is based on the gradient operator induced by optimal transport, which connects the geometry of sample space and parameter space in implicit deep generative models. From this theory, we obtain an easy-to-implement regularizer for the parameter updates. Our experiments demonstrate that this method improves the speed and stability in training GANs in terms of wall-clock time and Fr\\'echet Inception Distance (FID) learning curves. ", "keywords": ["Optimal transport", "Wasserstein gradient", "Generative adversarial network", "Unsupervised learning"], "authorids": ["atlin@math.ucla.edu", "wcli@math.ucla.edu", "sjo@math.ucla.edu", "montufar@math.ucla.edu"], "authors": ["Alex Tong Lin", "Wuchen Li", "Stanley Osher", "Guido Montufar"], "TL;DR": "We propose the Wasserstein proximal method for training GANs. ", "pdf": "/pdf/9df0392be5f34787936b348563f3e8e10c62af7c.pdf", "paperhash": "lin|wasserstein_proximal_of_gans", "_bibtex": "@misc{\nlin2019wasserstein,\ntitle={Wasserstein proximal of {GAN}s},\nauthor={Alex Tong Lin and Wuchen Li and Stanley Osher and Guido Montufar},\nyear={2019},\nurl={https://openreview.net/forum?id=Bye5OiR5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper377/Official_Review", "cdate": 1542234475181, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Bye5OiR5F7", "replyto": "Bye5OiR5F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper377/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335710573, "tmdate": 1552335710573, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper377/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1xtf8_wh7", "original": null, "number": 1, "cdate": 1541010961171, "ddate": null, "tcdate": 1541010961171, "tmdate": 1541534045976, "tddate": null, "forum": "Bye5OiR5F7", "replyto": "Bye5OiR5F7", "invitation": "ICLR.cc/2019/Conference/-/Paper377/Official_Review", "content": {"title": "An interesting paper but need more work in the context of GANs", "review": "The paper intends to utilize natural gradient induced by Wasserstein-2 distance to train the generator in GAN. Starting from the dynamical formulation of optimal transport, the authors propose the Wasserstein proximal operator as a regularization, which is simple in form and fast to compute. The proximal operator is added to training the generator, unlike most other regularizations that focus on the discriminator. This is an interesting direction. \n\nThe motivation is clear but by so many steps of approximation and relaxation, the authors didn\u2019t address what is the final regularization actually corresponding to? Personally I am not convinced that theoretically the proposed training method is better than the standard SGD. The illustration example in the paper is not very helpful as it didn\u2019t show how the proposed proximal operator works. The proximal operator serves as a regularization and it introduces some error, I would like to know how does this carry over to the whole training procedure. \n\nIn GAN, the optimal discriminator depends on the current generator. Many approaches to GAN training (i.e. WGAN-GP) advocates to update the generator once in every \u201couter-iteration\u201d. I am not sure how the proposed approach fit in those training schemes.\n\nIn the simulation, the difference is not very significant, especially in FID vs iteration number. This could be due to parameter tuning in standard WGAN-GP. I encourage more simulation studies and take more GAN structures into consideration. \n\nLastly, the stability mentioned in the paper lacks a formal definition. Is it the variance of the curves? Is it how robust the model is against outer iterations?", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper377/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wasserstein proximal of GANs", "abstract": "We introduce a new method for training GANs by applying the Wasserstein-2 metric proximal on the generators. \nThe approach is based on the gradient operator induced by optimal transport, which connects the geometry of sample space and parameter space in implicit deep generative models. From this theory, we obtain an easy-to-implement regularizer for the parameter updates. Our experiments demonstrate that this method improves the speed and stability in training GANs in terms of wall-clock time and Fr\\'echet Inception Distance (FID) learning curves. ", "keywords": ["Optimal transport", "Wasserstein gradient", "Generative adversarial network", "Unsupervised learning"], "authorids": ["atlin@math.ucla.edu", "wcli@math.ucla.edu", "sjo@math.ucla.edu", "montufar@math.ucla.edu"], "authors": ["Alex Tong Lin", "Wuchen Li", "Stanley Osher", "Guido Montufar"], "TL;DR": "We propose the Wasserstein proximal method for training GANs. ", "pdf": "/pdf/9df0392be5f34787936b348563f3e8e10c62af7c.pdf", "paperhash": "lin|wasserstein_proximal_of_gans", "_bibtex": "@misc{\nlin2019wasserstein,\ntitle={Wasserstein proximal of {GAN}s},\nauthor={Alex Tong Lin and Wuchen Li and Stanley Osher and Guido Montufar},\nyear={2019},\nurl={https://openreview.net/forum?id=Bye5OiR5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper377/Official_Review", "cdate": 1542234475181, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Bye5OiR5F7", "replyto": "Bye5OiR5F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper377/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335710573, "tmdate": 1552335710573, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper377/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}