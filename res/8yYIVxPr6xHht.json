{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392165720000, "tcdate": 1392165720000, "number": 2, "id": "Wb6A9kK0nsWYL", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "8yYIVxPr6xHht", "replyto": "8yYIVxPr6xHht", "signatures": ["anonymous reviewer 22fb"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Deep learning for class-generic object detection", "review": "This is an interesting paper that shows that using a pretrained object classification network's weights to initialize an object localization network leads to improved performance of the latter. \r\n\r\nOf course the comparisons are made for a fixed (but unspecified) architecture. It seems that the architecture chosen is that of an object classifier, and it is not clear at all that this is a good architecture for an object localizer. (In particular pooling is sensible for the former but makes less sense for the latter). Thus it's not really clear that the gains are that useful - perhaps it would just be better to design a network for the task in hand. That is not addressed in this paper at all.\r\n\r\nThe writing itself is clear, but there are a several significant omissions in the paper.  \r\n\r\nTable 1 caption is unclear, and seems broken.\r\n\r\nSection 4 'multiple GPUs' could be elucidated a little more? \r\n\r\nYou say the output for your bounding box training is discretized, but don't say how. It's not even clear if you output one-hot in the cross-product space or discretize each dimension separately. \r\nWhile your main result is a relative improvement, this is still a fundamental omission that must be rectified. It's hard to interpret your results without knowing the resolution or the scoring system that you use. \r\n\r\nYou don't explain the parameters of the Gaussian used for the labels, nor what you mean by 'multiple bounding boxes' - You've not described that possibility.\r\nAre 7% of objects labelled with bounding boxes, or only 7% of images? \r\nWhere you have multiple objects in an image are you guaranteed to have the bounding boxes for all of them if you have any of them? (I presume not, since 'what is an object' is ambiguous - but you don't discuss this ambiguity).\r\n\r\nFigure 1 legend says 'without bounding boxes' but I presume you mean 'excluding the held-out set' - you train with the other 900 classes' bounding boxes? \r\n\r\nYou don't discuss the details of training at all, (not even the number of nodes in the hidden layers or convolution windows, never mind learning rates etc) nor make it clear just how much is the same as the other references (Coates / Krizhevsky) you cite. ('Similar to' Krizhevsky is all you say)\r\nIn particular you need to explain how you transition from pretraining to 'training' - initialization of the softmax weights is the same as starting from scratch? What about: Learning rate / momentum & their schedules ? \r\n\r\nBibliography is broken everywhere e.g. with commas separating first and last names as well as authors' names from each other. \r\nFirst reference has no source. \r\nJunk like this: 'In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. .... 2009'\r\n\r\nThe first page indicates that this paper is in Proc ICML and in JMLR. I presume this is just a LaTeX oversight?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep learning for class-generic object detection", "decision": "submitted, no decision", "abstract": "We investigate the use of deep neural networks for the task of class-generic object detection. We show that neural networks originally designed for image recognition can be trained to detect objects within images, regardless of their class, including objects for which no bounding box labels have been provided. In addition, we show that bounding box labels yield a 1% performance increase on the ImageNet recognition challenge.", "pdf": "https://arxiv.org/abs/1312.6885", "paperhash": "huval|deep_learning_for_classgeneric_object_detection", "keywords": [], "conflicts": [], "authors": ["Brody Huval", "Adam Coates", "Andrew Ng"], "authorids": ["brodyh@stanford.edu", "acoates@cs.stanford.edu", "ang@cs.stanford.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390860900000, "tcdate": 1390860900000, "number": 1, "id": "ZZw4ZaGzdyt0c", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "8yYIVxPr6xHht", "replyto": "8yYIVxPr6xHht", "signatures": ["anonymous reviewer 7d29"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Deep learning for class-generic object detection", "review": "This abstract investigates the idea of learning an object detection model that does not depend on the class, hence being able to generalize to any number of classes, including classes unknown at training time. The idea is compelling but the paper is short on details and results. We don't know how many bounding boxes are used in the softmax, we don't have the details about the Gaussian used to smooth the targets of the softmax (how picky is it?); The paper does not compare to similar approaches (like Szegedy et al 2013). I feel like this is an interesting idea relevant for the workshop track and hope it will be improved later with more details."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep learning for class-generic object detection", "decision": "submitted, no decision", "abstract": "We investigate the use of deep neural networks for the task of class-generic object detection. We show that neural networks originally designed for image recognition can be trained to detect objects within images, regardless of their class, including objects for which no bounding box labels have been provided. In addition, we show that bounding box labels yield a 1% performance increase on the ImageNet recognition challenge.", "pdf": "https://arxiv.org/abs/1312.6885", "paperhash": "huval|deep_learning_for_classgeneric_object_detection", "keywords": [], "conflicts": [], "authors": ["Brody Huval", "Adam Coates", "Andrew Ng"], "authorids": ["brodyh@stanford.edu", "acoates@cs.stanford.edu", "ang@cs.stanford.edu"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387954020000, "tcdate": 1387954020000, "number": 17, "id": "8yYIVxPr6xHht", "invitation": "ICLR.cc/2014/workshop/-/submission", "forum": "8yYIVxPr6xHht", "signatures": ["brodyh@stanford.edu"], "readers": ["everyone"], "content": {"title": "Deep learning for class-generic object detection", "decision": "submitted, no decision", "abstract": "We investigate the use of deep neural networks for the task of class-generic object detection. We show that neural networks originally designed for image recognition can be trained to detect objects within images, regardless of their class, including objects for which no bounding box labels have been provided. In addition, we show that bounding box labels yield a 1% performance increase on the ImageNet recognition challenge.", "pdf": "https://arxiv.org/abs/1312.6885", "paperhash": "huval|deep_learning_for_classgeneric_object_detection", "keywords": [], "conflicts": [], "authors": ["Brody Huval", "Adam Coates", "Andrew Ng"], "authorids": ["brodyh@stanford.edu", "acoates@cs.stanford.edu", "ang@cs.stanford.edu"]}, "writers": [], "details": {"replyCount": 2, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357014, "id": "ICLR.cc/2014/workshop/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357014}}}], "count": 3}