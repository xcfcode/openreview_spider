{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1364262660000, "tcdate": 1364262660000, "number": 6, "id": "0mPCmj67CX0Ti", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "jbLdjjxPd-b2l", "replyto": "jbLdjjxPd-b2l", "signatures": ["anonymous reviewer 1939"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "As the previous reviewer states, there are very large improvements in the paper.  Clarity and mathematical precision are both greatly increased, and reading it now gives useful insight into the relationship between different perspectives and definitions of the natural gradient, and Hessian based methods.  Note, I did not check the math in Section 7 upon this rereading.\r\n\r\nIt's misleading to suggest that the author's derivation in terms of minimizing the objective on a fixed-KL divergence shell around the current location (approximated as a fixed value of the second order expansion of the Fisher information) is novel.  This is something that Amari also did (see for instance the proof of Theorem 1 on page 4 in Amari, S.-I. (1998). Natural Gradient Works Efficiently in Learning. Neural Computation, 10(2), 251\u2013276. doi:10.1162/089976698300017746.  This claim should be removed.\r\n\r\nIt could still use an editing pass, and especially improvements in the figure captions, but these are nits as opposed to show-stoppers (see specific comments below).  This is a much nicer paper.  My only significant remaining concerns are in terms of the Lagrange-multiplier derivation, and in terms of precedent setting.  It would be that it's a dangerous precedent to set (and promises to make much more work for future reviewers!) to base acceptance decisions on rewritten manuscripts that differ significantly from the version initially submitted.  So -- totally an editorial decision.\r\n\r\np. 2, footnote 2 -- 3rd expression should still start with sum_z\r\n\r\n'emphesis' -> emphasize'\r\n\r\n'to speed up' -> 'to speed up computations'\r\n\r\n'train error' -> 'training error'\r\n\r\nFigure 2 -- label panes (a) and (b) and reference as such.  'KL, different training minibatch' appears to be missing from Figure.  In latex, use ` for open quote and ' for close quote.  capitalize kl.  So, for instance, `KL, unlabeled'\r\n\r\nFigure 3 -- Caption has significant differences from figure\r\n\r\nin most places where it occurs, should refer to 'the natural gradient' rather than 'natural gradient'\r\n\r\n'equation (24) from section 3' -- there is no equation 24 in section 3.  Equation and Section should be capitalized."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Natural Gradient Revisited", "decision": "conferencePoster-iclr2013-workshop", "abstract": "The aim of this paper is two-folded. First we intend to show that Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of Natural Gradient Descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive Natural Gradient from basic principles, contrasting the difference between the two version of the algorithm that are in the literature.", "pdf": "https://arxiv.org/abs/1301.3584v4", "paperhash": "pascanu|natural_gradient_revisited", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1364251020000, "tcdate": 1364251020000, "number": 4, "id": "_MfuTMZ4u7mWN", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "jbLdjjxPd-b2l", "replyto": "jbLdjjxPd-b2l", "signatures": ["anonymous reviewer 6a77"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Clearly, the revised paper is much better than the initial paper to the extent that it should be considered a different paper that shares its title with the initial paper. The ICLR committee will have to make a policy decision about this.\r\n\r\nThe revised paper is poorly summarized by it abstract because it does not show things in the same order as the abstract.  The paper contains the following:\r\n\r\n*  A derivation of natural gradient that does not depend on information geometry.  This derivation is in fact well known (and therefore not new.)\r\n\r\n* A clear discussion of which distribution should be used to compute the natural gradient Riemannian tensor (equation 8).  This is not new, but this is explained nicely and clearly.\r\n\r\n* An illustration of what happens when one mixes these distributions. This is not surprising, but nicely illustrates the point that many so-called 'natural gradient' algorithms are not the same as Amari's natural gradient.   \r\n\r\n* A more specific discussion of the difference between LeRoux 'natural gradient' and the real natural gradient with useful intuitions.  This is a good clarification.\r\n\r\n* A more specific discussion of how many second order algorithms using the Gauss-Newton approximation are related to some so-called natural gradient algorithms which are not the true natural gradient.  Things get confusing because the authors seem committed to calling all these algorithms 'natural gradient' despite their own evidence.\r\n\r\nIn conclusion, although novelty is limited, the paper disambiguates some of the confusion surrounding natural gradient. I simply wish the authors took their own hint and simply proposed banning the words 'natural gradient' to describe things that are not Amari's natural gradient but are simply inspired by it."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Natural Gradient Revisited", "decision": "conferencePoster-iclr2013-workshop", "abstract": "The aim of this paper is two-folded. First we intend to show that Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of Natural Gradient Descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive Natural Gradient from basic principles, contrasting the difference between the two version of the algorithm that are in the literature.", "pdf": "https://arxiv.org/abs/1301.3584v4", "paperhash": "pascanu|natural_gradient_revisited", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363291260000, "tcdate": 1363291260000, "number": 5, "id": "iPpSPn9bTwn4Y", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "jbLdjjxPd-b2l", "replyto": "jbLdjjxPd-b2l", "signatures": ["anonymous reviewer 6f71"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "I read the updated version of the paper. I has indeed been improved substantially, and my concerns were addressed. It should clearly be accepted in its current form."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Natural Gradient Revisited", "decision": "conferencePoster-iclr2013-workshop", "abstract": "The aim of this paper is two-folded. First we intend to show that Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of Natural Gradient Descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive Natural Gradient from basic principles, contrasting the difference between the two version of the algorithm that are in the literature.", "pdf": "https://arxiv.org/abs/1301.3584v4", "paperhash": "pascanu|natural_gradient_revisited", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363288920000, "tcdate": 1363288920000, "number": 1, "id": "XXo-vXWa-ZvQL", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "jbLdjjxPd-b2l", "replyto": "jbLdjjxPd-b2l", "signatures": ["Razvan Pascanu, Yoshua Bengio"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "The revised arxiv paper is available now, and we replied to the reviewers comments."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Natural Gradient Revisited", "decision": "conferencePoster-iclr2013-workshop", "abstract": "The aim of this paper is two-folded. First we intend to show that Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of Natural Gradient Descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive Natural Gradient from basic principles, contrasting the difference between the two version of the algorithm that are in the literature.", "pdf": "https://arxiv.org/abs/1301.3584v4", "paperhash": "pascanu|natural_gradient_revisited", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363217040000, "tcdate": 1363217040000, "number": 1, "id": "j5Y_3gJAHK3nP", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "jbLdjjxPd-b2l", "replyto": "26sD6qgwF8Vob", "signatures": ["Razvan Pascanu, Yoshua Bengio"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "We've made drastic changes to the paper, which should be visible starting Thu, 14 Mar 2013 00:00:00 GMT. We made the paper available also at http://www-etud.iro.umontreal.ca/~pascanur/papers/ICLR_natural_gradient.pdf\r\n\r\n* Regarding the relationship between Hessian-Free and natural\r\ngradient,  it stems from the fact that algebraic manipulations of the \r\nextended Gauss-Newton approximation of the Hessian result in the natural gradient metric. Due to space limit (the paper is quite lengthly) we do not provide all intermediate steps in this algebraic manipulation, but we do provide all the crucial ones. Both natural gradient and Hessian Free have the same form (the gradient is multiplied by the inverse of a matrix before being subtracted from theta, potentially times some scalar learning rate). \r\nTherefore showing that both methods use the same matrix is sufficient \r\nto show that HF can be interpreted as natural gradient. \r\n\r\n* The degeneracy of theta were meant to suggest only that we are dealing with a lower dimensional manifold. We completely agree however that the text was confusing and it was completely re-written to avoid that potential confusion. In the re-write we've removed this detail as it is not crucial for the paper. \r\n\r\n* The relations at the end of page 2 do hold in general, as the expectation is taken over z (a detail that we specify now). We are not using a fully Bayesian framework, i.e. theta is not a random variable in the text. \r\n\r\n* Equation 15 was corrected. When computing the Hessian, we compute the derivatives with respect to `r`."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Natural Gradient Revisited", "decision": "conferencePoster-iclr2013-workshop", "abstract": "The aim of this paper is two-folded. First we intend to show that Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of Natural Gradient Descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive Natural Gradient from basic principles, contrasting the difference between the two version of the algorithm that are in the literature.", "pdf": "https://arxiv.org/abs/1301.3584v4", "paperhash": "pascanu|natural_gradient_revisited", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363216920000, "tcdate": 1363216920000, "number": 1, "id": "37JmPPz9dT39G", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "jbLdjjxPd-b2l", "replyto": "uEQsuu1xiBueM", "signatures": ["Razvan Pascanu, Yoshua Bengio"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "We've made drastic changes to the paper, which should be visible starting Thu, 14 Mar 2013 00:00:00 GMT. We made the paper available also at http://www-etud.iro.umontreal.ca/~pascanur/papers/ICLR_natural_gradient.pdf\r\n\r\n* Regarding the title, we have changed it to 'Revisiting Natural Gradient for Deep Networks', to better reflect the scope of the paper. We would like to thank you for the pointers on natural gradient for reinforcement learning and stochastic search (they have been incorporated in the new version).\r\n\r\n* We have provided more details of our derivation in section 2. We've added a plot describing path taken by different learning algorithms in parameter space (natural gradient, gradient descent, Newton's method, Le Roux's natural gradient). The new plot can be found at the beginning of section 4. If you have another suggestion of drawing that would help to illustrate what is going on, we would be pleased to know about it.\r\n\r\n* As you suggested, we have fixed the statement about the confusion between the two version of natural gradients in the past literature.\r\n\r\n* We have slightly rephrased our arguments in section 6 to better point out our intuitions. Indeed the approximations involved in the algorithms have an important role in its behaviour and we clarified this.\r\nHowever we were trying to say something different. We rephrased our argument as follows : robustness comes from the fact that we move in the direction of low variance for (d p(y|x))/(d theta) (the Fisher Information matrix is the uncentered covariance of these gradients). These are not the gradients of the error we minimize, but we argue that directions of high variance for these gradients, are directions of high variance for the gradients of the error function as well. Our reasoning is as follows. If moving in a direction `delta` would cause a large change in the gradient dL/dtheta, where L is the cost, this means that L(theta+delta) has to be 'very' different for different inputs.\r\nBut since L(theta) is just the evaluation of p(y|x) for particular values of \r\ny for given x, this means that if L varies with x so does p(y|x).\r\nThis means `delta` has to be a direction of high variance for the metric. This is true even if you move with infinitesimal speed, as it is more about picking the direction of low variance. This formulation is based on the the same argument you provided yourself regarding our early-overfitting experiment. Note that large variations in p(y|x) should be reflected in large curvature of the KL, as it indicates that p changes quickly. We originally formulated the argument around this large changes of p. We agree however that our original argument could have been clearer and more complete, and we hope it is clearer in the new version. \r\n\r\n* Regarding the use of unlabeled data, we added the proposed citation.\r\n\r\n* We have provided both pseudo code in the appendix, and we have made the code available at git@github.com:pascanur/natgrad.git\r\n\r\n* Regarding the early-overfitting experiment, we agree with the reviewer that natural gradient reduces variance overall.  In terms of relative variance it seems that it does not make a big difference. We however emphasize that reducing overall variance is important, as it makes learning overall less sensitive to the order of training examples (which in some sense is related to the original problem, in the sense that it also reduces the importance of the early examples to the beheviour of the trained model). We agree that the original focus of the section was sligthly off, and we changed it to addressing the sensitivity of the model to the examples it sees during training. Thanks again for the comment.\r\n\r\n* We did use a grid search for the other experiments, and used for each\r\nalgorithm the point on the grid that had the smallest validation error (a detail that we explicitly say in the paper). \r\n\r\nWe are however in the process of improving those results by extending our grid."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Natural Gradient Revisited", "decision": "conferencePoster-iclr2013-workshop", "abstract": "The aim of this paper is two-folded. First we intend to show that Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of Natural Gradient Descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive Natural Gradient from basic principles, contrasting the difference between the two version of the algorithm that are in the literature.", "pdf": "https://arxiv.org/abs/1301.3584v4", "paperhash": "pascanu|natural_gradient_revisited", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363216800000, "tcdate": 1363216800000, "number": 1, "id": "LuEnLatTnvu1A", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "jbLdjjxPd-b2l", "replyto": "ttBP0QO8pKtvq", "signatures": ["Razvan Pascanu"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "We've made drastic changes to the paper, which should be visible starting Thu, 14 Mar 2013 00:00:00 GMT.  We made the paper available also at http://www-etud.iro.umontreal.ca/~pascanur/papers/ICLR_natural_gradient.pdf\r\n\r\n* Regarding the differences between equation (1) and equation (7), it comes from moving from p(z) to the conditional p(y|x). This is emphasized in the text introducing equation (7), explaining in more details how one goes from (1) to (7).\r\n\r\n* Regarding the final arguments and overall presentation of the arguments in the paper, we have reworked the overall writeup of the paper in a way that you will hopefully find satisfactory."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Natural Gradient Revisited", "decision": "conferencePoster-iclr2013-workshop", "abstract": "The aim of this paper is two-folded. First we intend to show that Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of Natural Gradient Descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive Natural Gradient from basic principles, contrasting the difference between the two version of the algorithm that are in the literature.", "pdf": "https://arxiv.org/abs/1301.3584v4", "paperhash": "pascanu|natural_gradient_revisited", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363216740000, "tcdate": 1363216740000, "number": 2, "id": "aaN5bD_cRqbLk", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "jbLdjjxPd-b2l", "replyto": "jbLdjjxPd-b2l", "signatures": ["Razvan Pascanu"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We would like to thank all the reviewers for their feedback and insights. We had submitted a new version of the paper (it should appear on arxiv on Thu, 14 Mar 2013 00:00:00 GMT, though it can be retrieved now from \r\n\r\nhttp://www-etud.iro.umontreal.ca/~pascanur/papers/ICLR_natural_gradient.pdf\r\n\r\n We kindly ask the reviewers to look at. The new paper \r\ncontains drastic changes that we believe will improve the \r\nquality of the paper. In a few bullet points the changes are: \r\n\r\n* The title of the paper was changed to reflect our focus on natural\r\ngradient for deep neural networks\r\n* The wording and structure of the paper was slightly changed to better\r\nreflect the final conclusions\r\n* We improved notation, providing more details where they were missing\r\n* Additional plots were added as empirical proof to some of our hypotheses\r\n* We've added both the pseudo-code as well as link to a Theano-based\r\nimplementation of the algorithm"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Natural Gradient Revisited", "decision": "conferencePoster-iclr2013-workshop", "abstract": "The aim of this paper is two-folded. First we intend to show that Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of Natural Gradient Descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive Natural Gradient from basic principles, contrasting the difference between the two version of the algorithm that are in the literature.", "pdf": "https://arxiv.org/abs/1301.3584v4", "paperhash": "pascanu|natural_gradient_revisited", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362404760000, "tcdate": 1362404760000, "number": 8, "id": "26sD6qgwF8Vob", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "jbLdjjxPd-b2l", "replyto": "jbLdjjxPd-b2l", "signatures": ["anonymous reviewer 1939"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Natural Gradient Revisited", "review": "This paper attempts to reconcile several definitions of the natural gradient, and to connect the Gauss-Newton approximation of the Hessian used in Hessian free optimization to the metric used in natural gradient descent.  Understanding the geometry of objective functions, and the geometry of the space they live in, is crucial for model training, and is arguably the greatest bottleneck in training deep or otherwise complex models.  However, this paper makes a confused presentation of the underlying ideas, and does not succeed in clearly tying them together.\r\n\r\nMore specific comments:\r\n\r\nIn the second (and third) paragraph of section 2, the natural gradient is discussed as if it stems from degeneracies in theta, where multiple theta values correspond to the same distribution p.  This is inaccurate.  Degeneracies in theta have nothing to do with the natural gradient.  This may stem from a misinterpretation of the role of symmetries in natural gradient derivations?  Symmetries are frequently used in the derivation of the natural gradient, in that the metric is frequently chosen such that it is invariant to symmetries in the parameter space.  However, the metric being invariant to symmetries does not mean that p is similarly invariant, and there are natural gradient applications where symmetries aren't used at all.  (You might find The Natural Gradient by Analogy to Signal Whitening, Sohl-Dickstein, http://arxiv.org/abs/1205.1828 a more straightforward introduction to the natural gradient.)\r\n\r\nAt the end of page 2, between equations 2 and 3, you introduce relations which certainly don't hold in general.  At the least you should give the assumptions you're using.  (also, notationally, it's not clear what you're taking the expectation over -- z? theta?)\r\n\r\nEquation 15 doesn't make sense.  As written, the matrices are the wrong shape.  Should the inner second derivative be in terms of r instead of theta?\r\n\r\nThe text has minor English difficulties, and could benefit from a grammar and word choice editing pass.  I stopped marking these pretty early on, but here are some specific suggested edits:\r\n'two-folded' -> 'two-fold'\r\n'framework of natural gradient' -> 'framework of the natural gradient'\r\n'gradient protects about' -> 'gradient protects against'\r\n'worrysome' -> 'worrisome'\r\n'even though is called the same' -> 'despite the shared name'\r\n'differ' -> 'defer'\r\n'get map' -> 'get mapped'"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Natural Gradient Revisited", "decision": "conferencePoster-iclr2013-workshop", "abstract": "The aim of this paper is two-folded. First we intend to show that Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of Natural Gradient Descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive Natural Gradient from basic principles, contrasting the difference between the two version of the algorithm that are in the literature.", "pdf": "https://arxiv.org/abs/1301.3584v4", "paperhash": "pascanu|natural_gradient_revisited", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362372600000, "tcdate": 1362372600000, "number": 3, "id": "uEQsuu1xiBueM", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "jbLdjjxPd-b2l", "replyto": "jbLdjjxPd-b2l", "signatures": ["anonymous reviewer 6f71"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Natural Gradient Revisited", "review": "Summary\r\n\r\nThe paper reviews the concept of natural gradient, re-derives it in the context of neural network training, compares a number of natural gradient-based algorithms and discusses their differences. The paper's aims are highly relevant to the state of the field, and it contains numerous valuable insights. Precisely because of its topic's importance, however, I deplore its lack of maturity, especially in terms of experimental results and literature overview. \r\n\r\n\r\nComments\r\n\r\n-- The title raises the expectation of a review-style paper with a broad literature overview on the topic, but that aspect is underdeveloped. A paper such as this would be a perfect opportunity to relate natural gradient-related work in neural networks to closely related approaches in reinforcement learning [1,2] and stochastic search [3].\r\n\r\n-- The discussion in section 2 is correct and useful, but would benefit enormously from an illustrative figure that clarifies the relation between parameter space and distribution manifold, and how gradient directions differ in both. The last sentence (Lagrange method) is also breezing over a number of details that would benefit from a more explicit treatment.\r\n\r\n-- There is a recurring claim that gradient-covariances are 'usually confused' with Fisher matrices. While there are indeed a few authors who did fall victim to this, it is not a belief held by many researchers working on natural gradients, please reformulate.\r\n\r\n-- The information-geometric manifold is generally highly curved, which means that results that hold for infinitesimal step-sizes do not generally apply to realistic gradient algorithms with large finite steps. Indeed, [4] introduces an information-geometric 'flow' and contrasts it with its finite-step approximations. It is important to distinguish the effect of the natural gradient itself from the artifacts of finite-step approximations, indeed the asymptotic behavior can differ, see [5]. A number of arguments in section 6 could be revised in this light.\r\n\r\n-- The idea of using more data to estimate the Fisher information matrix (because if does not need to be labeled), compared to the data necessary for the steepest gradient itself, is promising for semi-supervised neural network training. It was previously was presented in [3], in a slightly different context with infinitely many unlabeled samples.\r\n\r\n-- The new variants of natural gradient descent should be given in pseudocode in the appendix, and if possible even with a reference open-source implementation in the Theano framework.\r\n\r\n-- The experiment presented in Figure 2 is very interesting, although I disagree with the conclusions that are derived from it: the variance is qualitatively the same for both algorithms, just rescaled by roughly a factor 4. So, relatively speaking, the influence of early samples is still equally strong, only the generic variability of the natural gradient is reduced: plausibly by the effect that the Fisher-preconditioning reduces step-sizes in directions of high variance.\r\n\r\n-- The other experiments, which focus on test-set performance, have a major flaw: it appears each algorithm variant was run exactly once on each dataset, which makes it very difficult to judge whether the results are significant. Also, the effect of hyper-parameter-tuning on those results is left vague.\r\n\r\n\r\n\r\nMinor points/typos\r\n-- Generally, structure the text such that equations are presented before they are referred to, this makes for a more linear reading flow.\r\n-- variable n is undefined\r\n-- clarify which spaces the variables x, z, t, theta live in.\r\n-- 'three most typical'\r\n-- 'different parametrizations of the model'\r\n-- 'similar derivations'\r\n-- 'plateaus' \r\n-- axes of figures could be homogenized.\r\n\r\nReferences\r\n[1] 'Natural policy gradient', Kakade, NIPS 2002.\r\n[2] 'Natural Actor-Critic', Peters and Schaal, Neurocomputing 2008.\r\n[3] 'Stochastic Search using the Natural Gradient', Sun et al, ICML 2009.\r\n[4] 'Information-Geometric Optimization Algorithms: A Unifying Picture via Invariance Principles', Arnold et al, Arxiv 2011.\r\n[5] 'Natural Evolution Strategies Converge on Sphere Functions', Schaul, GECCO 2012."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Natural Gradient Revisited", "decision": "conferencePoster-iclr2013-workshop", "abstract": "The aim of this paper is two-folded. First we intend to show that Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of Natural Gradient Descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive Natural Gradient from basic principles, contrasting the difference between the two version of the algorithm that are in the literature.", "pdf": "https://arxiv.org/abs/1301.3584v4", "paperhash": "pascanu|natural_gradient_revisited", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362084780000, "tcdate": 1362084780000, "number": 9, "id": "wiYbiqRc-GqXO", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "jbLdjjxPd-b2l", "replyto": "jbLdjjxPd-b2l", "signatures": ["Razvan Pascanu, Yoshua Bengio"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Thank you for your comments. We will soon push a revision to fix all the grammar and language mistakes you pointed out.\r\n\r\nRegarding equation (1) and equation (7), mathbf{G} represents the Fisher Information Matrix form of the metric resulting when you consider respectively p(x) vs p(y|x). Equation (1) is introduced in section 1, which presents the generic case of a family of distributions p_{\theta}(x). From section 3 onwards we adapt these equations specifically to neural networks, where, from a probabilistic point of view, we are dealing with conditional probabilities p(y|x).\r\n\r\nCould you please be more specific regarding the elements of the paper that you found confusing?\r\nWe would like to reformulate the conclusion to make our contributions clearer. The novel points we are trying to make are:\r\n\r\n(1) Hessian Free optimization and Krylov Subspace Descent, as long as they use the Gauss-Newton approximation of the Hessian, can be understood as Natural Gradient, because the Gauss-Newton matrix matches the metric of Natural Gradient (and the rest of the pipeline is the same).\r\n(2) Possibly due to the regularization effect discussed in (6), we hypothesize and support with empirical results that Natural Gradient helps dealing with the early overfitting problem introduced by Erhan et al.  This early overfitting problem might be a serious issue when trying to scale neural networks to large models with very large datasets.\r\n(3) We make the observation that since the targets get integrated out when computing the metric of Natural Gradient, one can use unlabeled data to improve the accuracy of this metric that dictates the speed with which we move in parameter space.\r\n(4) Natural Gradient introduced by Nicolas Le Roux et al has a fundamental difference with Amari's. It is not just a different justification, but a different algorithm that might behave differently in practice.\r\n(5) Natural Gradient is different from a second order method because while one uses second order information, it is not the second order information of the error function, but of the KL divergence (which is quite different). For e.g. it is always positive definite by construction, while the curvature is not. Also, when considering the curvature of the KL, is not the curvature of the same surface throughout learning. At each step we have a different KL divergence and hence a different surface, while for second order methods the error surface stays constant through out learning. The second distinction is that Natural Gradient is naturally suited for online learning, provided that we have sufficient statistics to estimate the KL divergence (the metric). Theoretically, second order methods are meant to be batch methods (because the Hessian is supposed over the whole dataset) where the Natural Gradient metric only depends on the model.\r\n(6) The standard understanding of Natural Gradient is that by imposing the KL divergence between p_{theta}(y|x) and p_{theta+delta}(y|x) to be constant it ensures that some amount of progress is done at every step and hence it converges faster. We add that it also ensures that you do not move too far in some direction (which would make the KL change quickly), hence acting as a regularizer.\r\n\r\nRegarding the paper not being formal enough we often find that a dry mathematical treatment of the problem does not help improving the understanding or eliminating confusions.  We believe that we were formal enough when showing the equivalence between the generalized Gauss-Newton and Amari's metric. Point (6) of our conclusion is a hypothesis which we validate empirically and we do not have a formal treatment for it."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Natural Gradient Revisited", "decision": "conferencePoster-iclr2013-workshop", "abstract": "The aim of this paper is two-folded. First we intend to show that Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of Natural Gradient Descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive Natural Gradient from basic principles, contrasting the difference between the two version of the algorithm that are in the literature.", "pdf": "https://arxiv.org/abs/1301.3584v4", "paperhash": "pascanu|natural_gradient_revisited", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1361998920000, "tcdate": 1361998920000, "number": 7, "id": "ttBP0QO8pKtvq", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "jbLdjjxPd-b2l", "replyto": "jbLdjjxPd-b2l", "signatures": ["anonymous reviewer 6a77"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Natural Gradient Revisited", "review": "GENERAL COMMENTS\r\nThe paper promises to establish the relation between Amari's natural gradient and many methods that are called Natural Gradient or can be related to Natural Gradient because they use Gauss-Newton approximations of the Hessian.  The problem is that I find the paper misleading.  In particular the G of equation (1) is not the same as the G of equation (7).  The author certainly points out that the crux of the matter is to understand which distribution is used to approximate the Fisher information matrix, but the final argument is a mess.  This should be done a lot more rigorously (and a lot less informally.)  As the paper stands, it only increases the level of confusion.\r\n\r\nSPECIFIC COMMENTS\r\n* (ichi Amari, 1997) - > (Amari, 1997)\r\n* differ -> defer\r\n* Due to this surjection:  A surjection is something else!\r\n* Equation (1):  please make clear that the expectation is an expectation on z distributed according p_theta  (not the ground truth nor the empirical distribution). Equation (7) then appears to be a mix of both.\r\n* becomes the conditional p_\theta(t|x) where q(x) represents:     where is q in p_\theta(t|x)"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Natural Gradient Revisited", "decision": "conferencePoster-iclr2013-workshop", "abstract": "The aim of this paper is two-folded. First we intend to show that Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of Natural Gradient Descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive Natural Gradient from basic principles, contrasting the difference between the two version of the algorithm that are in the literature.", "pdf": "https://arxiv.org/abs/1301.3584v4", "paperhash": "pascanu|natural_gradient_revisited", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358454600000, "tcdate": 1358454600000, "number": 54, "id": "jbLdjjxPd-b2l", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "jbLdjjxPd-b2l", "signatures": ["r.pascanu@gmail.com"], "readers": ["everyone"], "content": {"title": "Natural Gradient Revisited", "decision": "conferencePoster-iclr2013-workshop", "abstract": "The aim of this paper is two-folded. First we intend to show that Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of Natural Gradient Descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive Natural Gradient from basic principles, contrasting the difference between the two version of the algorithm that are in the literature.", "pdf": "https://arxiv.org/abs/1301.3584v4", "paperhash": "pascanu|natural_gradient_revisited", "keywords": [], "conflicts": [], "authors": ["Razvan Pascanu", "Yoshua Bengio"], "authorids": ["r.pascanu@gmail.com", "yoshua.bengio@gmail.com"]}, "writers": [], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 13}