{"notes": [{"id": "HJlWIANtPH", "original": "rkeqF08_DH", "number": 1129, "cdate": 1569439305348, "ddate": null, "tcdate": 1569439305348, "tmdate": 1577168228483, "tddate": null, "forum": "HJlWIANtPH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["zhangxiyuan@zju.edu.cn", "yuanyang@tsinghua.edu.cn", "indyk@mit.edu"], "title": "Neural Embeddings for Nearest Neighbor Search Under Edit Distance", "authors": ["Xiyuan Zhang", "Yang Yuan", "Piotr Indyk"], "pdf": "/pdf/7f10042dfc3aaf2cc0fd1847c4b9ac2f19a02dd2.pdf", "TL;DR": "We propose a learning-based edit distance embedding method, which improves over prior data-independent approaches.", "abstract": "The edit distance between two sequences is an important metric with many applications. The drawback, however, is the high computational cost of many basic problems involving this notion, such as the nearest neighbor search. A natural approach to overcoming this issue is to embed the sequences into a vector space such that the geometric distance in the target space approximates the edit distance in the original space. However, the known edit distance embedding algorithms, such as Chakraborty et al.(2016), construct embeddings that are data-independent, i.e., do not exploit any structure of embedded sets of strings. In this paper we propose an alternative approach, which learns the embedding function according to the data distribution. Our experiments show that the new algorithm has much better empirical performance than prior data-independent methods. ", "keywords": ["Embedding", "Edit Distance", "Nearest Neighbor Search", "Learning-Augmented Algorithm"], "paperhash": "zhang|neural_embeddings_for_nearest_neighbor_search_under_edit_distance", "original_pdf": "/attachment/020fef6aebc4baeb078f4c753aaead88fef71538.pdf", "_bibtex": "@misc{\nzhang2020neural,\ntitle={Neural Embeddings for Nearest Neighbor Search Under Edit Distance},\nauthor={Xiyuan Zhang and Yang Yuan and Piotr Indyk},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlWIANtPH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "8J2VNxGK1V", "original": null, "number": 1, "cdate": 1576798715269, "ddate": null, "tcdate": 1576798715269, "tmdate": 1576800921257, "tddate": null, "forum": "HJlWIANtPH", "replyto": "HJlWIANtPH", "invitation": "ICLR.cc/2020/Conference/Paper1129/-/Decision", "content": {"decision": "Reject", "comment": "This paper presents an approach to improving the calculation of embeddings for nearest-neighbor search with respect to edit distance.\n\nReading the reviews, it seems that the paper is greatly improved over its previous version, but still has significant clarity issues. Given that these issues remain even after one major revision, I would suggest that the paper not be accepted for this ICLR, but that the authors carefully revise the paper for clarity and submit to a following submission opportunity. It may help to share the paper with others who are not familiar with the research until they can read it once and understand the method well.\n\nI have quoted Reviewer 3 below in the author discussion, where there are some additional clarity issues that may help being resolved:\n\n----------\n\nSome specifics are clear now with their new edition. \n* The [relationship between] cgk' & cgk not as clear as it could be. For example the algorithms are designed for bits. So one should assume that they are applying it on the bits of the characters. But this should be clarified in the manuscript.\n* Also still backpropagating through f' is not clear to me.\n* And in the text for inference they still say: \"We randomly select 100 queries and use the remainder of the dataset as the base set\" which should be \"the remainder excluding the training set\" or \"including?\".", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhangxiyuan@zju.edu.cn", "yuanyang@tsinghua.edu.cn", "indyk@mit.edu"], "title": "Neural Embeddings for Nearest Neighbor Search Under Edit Distance", "authors": ["Xiyuan Zhang", "Yang Yuan", "Piotr Indyk"], "pdf": "/pdf/7f10042dfc3aaf2cc0fd1847c4b9ac2f19a02dd2.pdf", "TL;DR": "We propose a learning-based edit distance embedding method, which improves over prior data-independent approaches.", "abstract": "The edit distance between two sequences is an important metric with many applications. The drawback, however, is the high computational cost of many basic problems involving this notion, such as the nearest neighbor search. A natural approach to overcoming this issue is to embed the sequences into a vector space such that the geometric distance in the target space approximates the edit distance in the original space. However, the known edit distance embedding algorithms, such as Chakraborty et al.(2016), construct embeddings that are data-independent, i.e., do not exploit any structure of embedded sets of strings. In this paper we propose an alternative approach, which learns the embedding function according to the data distribution. Our experiments show that the new algorithm has much better empirical performance than prior data-independent methods. ", "keywords": ["Embedding", "Edit Distance", "Nearest Neighbor Search", "Learning-Augmented Algorithm"], "paperhash": "zhang|neural_embeddings_for_nearest_neighbor_search_under_edit_distance", "original_pdf": "/attachment/020fef6aebc4baeb078f4c753aaead88fef71538.pdf", "_bibtex": "@misc{\nzhang2020neural,\ntitle={Neural Embeddings for Nearest Neighbor Search Under Edit Distance},\nauthor={Xiyuan Zhang and Yang Yuan and Piotr Indyk},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlWIANtPH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJlWIANtPH", "replyto": "HJlWIANtPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795717771, "tmdate": 1576800268142, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1129/-/Decision"}}}, {"id": "BJe0VGqnoS", "original": null, "number": 7, "cdate": 1573851701897, "ddate": null, "tcdate": 1573851701897, "tmdate": 1573851701897, "tddate": null, "forum": "HJlWIANtPH", "replyto": "BJeozMdnsS", "invitation": "ICLR.cc/2020/Conference/Paper1129/-/Official_Comment", "content": {"title": "your comment", "comment": "Thank you for your response! The question you ask (about \"rounding\") is very interesting but it appears to be quite non-trivial. We will investigate it over the next few weeks. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1129/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1129/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhangxiyuan@zju.edu.cn", "yuanyang@tsinghua.edu.cn", "indyk@mit.edu"], "title": "Neural Embeddings for Nearest Neighbor Search Under Edit Distance", "authors": ["Xiyuan Zhang", "Yang Yuan", "Piotr Indyk"], "pdf": "/pdf/7f10042dfc3aaf2cc0fd1847c4b9ac2f19a02dd2.pdf", "TL;DR": "We propose a learning-based edit distance embedding method, which improves over prior data-independent approaches.", "abstract": "The edit distance between two sequences is an important metric with many applications. The drawback, however, is the high computational cost of many basic problems involving this notion, such as the nearest neighbor search. A natural approach to overcoming this issue is to embed the sequences into a vector space such that the geometric distance in the target space approximates the edit distance in the original space. However, the known edit distance embedding algorithms, such as Chakraborty et al.(2016), construct embeddings that are data-independent, i.e., do not exploit any structure of embedded sets of strings. In this paper we propose an alternative approach, which learns the embedding function according to the data distribution. Our experiments show that the new algorithm has much better empirical performance than prior data-independent methods. ", "keywords": ["Embedding", "Edit Distance", "Nearest Neighbor Search", "Learning-Augmented Algorithm"], "paperhash": "zhang|neural_embeddings_for_nearest_neighbor_search_under_edit_distance", "original_pdf": "/attachment/020fef6aebc4baeb078f4c753aaead88fef71538.pdf", "_bibtex": "@misc{\nzhang2020neural,\ntitle={Neural Embeddings for Nearest Neighbor Search Under Edit Distance},\nauthor={Xiyuan Zhang and Yang Yuan and Piotr Indyk},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlWIANtPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlWIANtPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1129/Authors", "ICLR.cc/2020/Conference/Paper1129/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1129/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1129/Reviewers", "ICLR.cc/2020/Conference/Paper1129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1129/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1129/Authors|ICLR.cc/2020/Conference/Paper1129/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160793, "tmdate": 1576860555857, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1129/Authors", "ICLR.cc/2020/Conference/Paper1129/Reviewers", "ICLR.cc/2020/Conference/Paper1129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1129/-/Official_Comment"}}}, {"id": "BJeozMdnsS", "original": null, "number": 6, "cdate": 1573843475516, "ddate": null, "tcdate": 1573843475516, "tmdate": 1573843475516, "tddate": null, "forum": "HJlWIANtPH", "replyto": "S1gaimrBiB", "invitation": "ICLR.cc/2020/Conference/Paper1129/-/Official_Comment", "content": {"title": "RE: Author response", "comment": "I have read the other reviews and authors' responses; I also briefly looked into the updated paper. These have clarified a number of important details, and I have updated my score to \"weak accept\" as a result.\n\nTo me, there is still a bit of a disconnect between developing the CGK' algorithm, proving various decoding properties about the binary vectors it creates, and then learning continuous vectors where the proofs no longer hold. Would it be possible to show some sort of approximate \"rounding\" result (e.g., if we round the continuous vectors to binary ones, we recover the original sequence with some bounded likelihood)?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1129/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1129/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhangxiyuan@zju.edu.cn", "yuanyang@tsinghua.edu.cn", "indyk@mit.edu"], "title": "Neural Embeddings for Nearest Neighbor Search Under Edit Distance", "authors": ["Xiyuan Zhang", "Yang Yuan", "Piotr Indyk"], "pdf": "/pdf/7f10042dfc3aaf2cc0fd1847c4b9ac2f19a02dd2.pdf", "TL;DR": "We propose a learning-based edit distance embedding method, which improves over prior data-independent approaches.", "abstract": "The edit distance between two sequences is an important metric with many applications. The drawback, however, is the high computational cost of many basic problems involving this notion, such as the nearest neighbor search. A natural approach to overcoming this issue is to embed the sequences into a vector space such that the geometric distance in the target space approximates the edit distance in the original space. However, the known edit distance embedding algorithms, such as Chakraborty et al.(2016), construct embeddings that are data-independent, i.e., do not exploit any structure of embedded sets of strings. In this paper we propose an alternative approach, which learns the embedding function according to the data distribution. Our experiments show that the new algorithm has much better empirical performance than prior data-independent methods. ", "keywords": ["Embedding", "Edit Distance", "Nearest Neighbor Search", "Learning-Augmented Algorithm"], "paperhash": "zhang|neural_embeddings_for_nearest_neighbor_search_under_edit_distance", "original_pdf": "/attachment/020fef6aebc4baeb078f4c753aaead88fef71538.pdf", "_bibtex": "@misc{\nzhang2020neural,\ntitle={Neural Embeddings for Nearest Neighbor Search Under Edit Distance},\nauthor={Xiyuan Zhang and Yang Yuan and Piotr Indyk},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlWIANtPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlWIANtPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1129/Authors", "ICLR.cc/2020/Conference/Paper1129/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1129/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1129/Reviewers", "ICLR.cc/2020/Conference/Paper1129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1129/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1129/Authors|ICLR.cc/2020/Conference/Paper1129/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160793, "tmdate": 1576860555857, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1129/Authors", "ICLR.cc/2020/Conference/Paper1129/Reviewers", "ICLR.cc/2020/Conference/Paper1129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1129/-/Official_Comment"}}}, {"id": "SJgPAR6hYr", "original": null, "number": 2, "cdate": 1571770062991, "ddate": null, "tcdate": 1571770062991, "tmdate": 1573843180536, "tddate": null, "forum": "HJlWIANtPH", "replyto": "HJlWIANtPH", "invitation": "ICLR.cc/2020/Conference/Paper1129/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "In this paper, the authors propose an approach for learning embeddings for strings using a three-phase approach. Each phase uses a different neural network to learn embeddings for strings such that the distance between strings in embedding space approximates the edit distance between the strings. A modest set of empirical results suggests that the proposed approach outperforms hand-crafted embeddings.\n\nI found the presentation in this paper very disjointed and difficult to follow. While I believe (my interpretation of) the basic idea of this paper is interesting, I believe the current presentation significantly hinders readers from following the authors\u2019 intentions.\n\nComments\n\nThe description of how the network structure and weights are \u201cinitialized\u201d across the different phases is not clear. Different notation (f_1, f_2, f_3) is used for each network, but in reality, this is just the same network. However, the writing makes this very difficult to notice.\n\nThe authors introduce the CGK and CGK\u2019 embedding algorithms, and then proceed to prove various properties about them. However, it is not clear to me how these theoretical properties are used by the neural network. From what I can tell, CGK\u2019 is an alternative to CGK which reduces the output size relative to CGK (from 3n to at most 2n) while still ensuring exact reconstruction of the input. (I did not verify the proof in detail.) The authors then claim that this is helpful in the current context because it ensures the network parameters can be easily optimized. It is not clear to me what this means. (I guess that somehow using \u201cCGK\u2019 distance\u201d makes training the model easier than using \u201cCGK distance\u201d.) Additionally, the experiments do not verify this claim empirically. So it is unclear whether using \u201cCGK\u2019 distance\u201d helps in the context of learning embeddings.\n\nIt is really unclear to me whether the neural network outputs a continuous or a binary vector. In particular, Equations 5 - 8 all suggest that Hamming loss is defined on the outputs of the various neural networks (f_1, f_2, and f_3). The paper also refers to bits in the output of f_3. Later on, though, the paper mentions that the neural embedded strings are continuous vectors. While this could just be typos or inconsistent notation, considering that other parts of the paper do rely on binary representations, this makes the presentation very confusing.\n\nIt is unclear to me whether the can be (approximately) reconstructed from the embeddings. It seems that Theorem 1 suggests that the binary outputs of CGK\u2019 can be decoded, but I cannot tell whether that extends to the embeddings.\n\nIt is unclear to me how positives and negatives are sampled for training in Phase 2, and also whether that impacts training.\n\nThe experimental results should include some measure of variance based on different train and/or test splits.\n\nIt seems as though the three phases could be rolled into a single multi-task learning problem in which the network is trained during a single phase.\n\nTypos, etc.\n\nThe references are not consistently formatted.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1129/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1129/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhangxiyuan@zju.edu.cn", "yuanyang@tsinghua.edu.cn", "indyk@mit.edu"], "title": "Neural Embeddings for Nearest Neighbor Search Under Edit Distance", "authors": ["Xiyuan Zhang", "Yang Yuan", "Piotr Indyk"], "pdf": "/pdf/7f10042dfc3aaf2cc0fd1847c4b9ac2f19a02dd2.pdf", "TL;DR": "We propose a learning-based edit distance embedding method, which improves over prior data-independent approaches.", "abstract": "The edit distance between two sequences is an important metric with many applications. The drawback, however, is the high computational cost of many basic problems involving this notion, such as the nearest neighbor search. A natural approach to overcoming this issue is to embed the sequences into a vector space such that the geometric distance in the target space approximates the edit distance in the original space. However, the known edit distance embedding algorithms, such as Chakraborty et al.(2016), construct embeddings that are data-independent, i.e., do not exploit any structure of embedded sets of strings. In this paper we propose an alternative approach, which learns the embedding function according to the data distribution. Our experiments show that the new algorithm has much better empirical performance than prior data-independent methods. ", "keywords": ["Embedding", "Edit Distance", "Nearest Neighbor Search", "Learning-Augmented Algorithm"], "paperhash": "zhang|neural_embeddings_for_nearest_neighbor_search_under_edit_distance", "original_pdf": "/attachment/020fef6aebc4baeb078f4c753aaead88fef71538.pdf", "_bibtex": "@misc{\nzhang2020neural,\ntitle={Neural Embeddings for Nearest Neighbor Search Under Edit Distance},\nauthor={Xiyuan Zhang and Yang Yuan and Piotr Indyk},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlWIANtPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJlWIANtPH", "replyto": "HJlWIANtPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1129/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1129/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575412036806, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1129/Reviewers"], "noninvitees": [], "tcdate": 1570237741941, "tmdate": 1575412036820, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1129/-/Official_Review"}}}, {"id": "HJxlNfsFsB", "original": null, "number": 5, "cdate": 1573659176322, "ddate": null, "tcdate": 1573659176322, "tmdate": 1573659176322, "tddate": null, "forum": "HJlWIANtPH", "replyto": "SJgPAR6hYr", "invitation": "ICLR.cc/2020/Conference/Paper1129/-/Official_Comment", "content": {"title": "Further Update of Variance Based on Different Train and/or Test Splits", "comment": "We have further added the standard deviation of Table 2 and Table 5 by testing with five different query sets, each of which contains 100 randomly selected strings. Thanks!"}, "signatures": ["ICLR.cc/2020/Conference/Paper1129/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1129/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhangxiyuan@zju.edu.cn", "yuanyang@tsinghua.edu.cn", "indyk@mit.edu"], "title": "Neural Embeddings for Nearest Neighbor Search Under Edit Distance", "authors": ["Xiyuan Zhang", "Yang Yuan", "Piotr Indyk"], "pdf": "/pdf/7f10042dfc3aaf2cc0fd1847c4b9ac2f19a02dd2.pdf", "TL;DR": "We propose a learning-based edit distance embedding method, which improves over prior data-independent approaches.", "abstract": "The edit distance between two sequences is an important metric with many applications. The drawback, however, is the high computational cost of many basic problems involving this notion, such as the nearest neighbor search. A natural approach to overcoming this issue is to embed the sequences into a vector space such that the geometric distance in the target space approximates the edit distance in the original space. However, the known edit distance embedding algorithms, such as Chakraborty et al.(2016), construct embeddings that are data-independent, i.e., do not exploit any structure of embedded sets of strings. In this paper we propose an alternative approach, which learns the embedding function according to the data distribution. Our experiments show that the new algorithm has much better empirical performance than prior data-independent methods. ", "keywords": ["Embedding", "Edit Distance", "Nearest Neighbor Search", "Learning-Augmented Algorithm"], "paperhash": "zhang|neural_embeddings_for_nearest_neighbor_search_under_edit_distance", "original_pdf": "/attachment/020fef6aebc4baeb078f4c753aaead88fef71538.pdf", "_bibtex": "@misc{\nzhang2020neural,\ntitle={Neural Embeddings for Nearest Neighbor Search Under Edit Distance},\nauthor={Xiyuan Zhang and Yang Yuan and Piotr Indyk},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlWIANtPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlWIANtPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1129/Authors", "ICLR.cc/2020/Conference/Paper1129/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1129/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1129/Reviewers", "ICLR.cc/2020/Conference/Paper1129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1129/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1129/Authors|ICLR.cc/2020/Conference/Paper1129/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160793, "tmdate": 1576860555857, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1129/Authors", "ICLR.cc/2020/Conference/Paper1129/Reviewers", "ICLR.cc/2020/Conference/Paper1129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1129/-/Official_Comment"}}}, {"id": "SygCENHHjH", "original": null, "number": 4, "cdate": 1573372981857, "ddate": null, "tcdate": 1573372981857, "tmdate": 1573373651523, "tddate": null, "forum": "HJlWIANtPH", "replyto": "rJggF6khFS", "invitation": "ICLR.cc/2020/Conference/Paper1129/-/Official_Comment", "content": {"title": "Response to Official Review #1", "comment": "Thanks for your supportive feedback! We have updated our paper accordingly.\n\nQ: Why use 3 phases, not just a single phase? \nA: We have tried to train phase 1 and phase 2 together as a multi-task problem, but the empirical results were not as good. This is because phase 1 first scales and approximates the edit distance then phase 2 further captures the relative similarity, so the order of phase 1 and phase 2 matters. Besides, phase 3 cannot be jointly trained with phase 1 and phase 2, because in phase 3 we are also optimizing the CGK\u2019 algorithm, which uses approximation to the gradients (see Algorithm 4). This phase is more like a fine tuning step. \nMoreover, even if all phases can be trained together, one needs to tune the final loss function that combines all the losses together. Our current three phases training avoids this tuning problem because we just train the network until convergence for each phase, without using any hyperparameters. So it should be easier (rather than harder) for the readers to use, compared with single phase training. \n\nQ: Table 5 and text in Sec 5.4 are not matched?\nA: Thank you for pointing it out. Here we only show the top-1 recall table (Table 5). We also evaluate on top-10, top-50 and top-100 and have not listed these tables due to limited space. \n \nQ: Can the cost to Phase 3 properly explained? Could there be a way to decide if/when we require phase 3? \nA: Phase 3 essentially involves padding additional characters into the embedding, so that misaligned strings will become aligned. Consider  \u2018ACTCGC\u2019 and \u2018CACTCG\u2019. \u2018CACTCG\u2019 can be edited by shifting one character to the right from \u2018ACTCGC\u2019, so their edit distance is two. However,  these two strings differ on every position, so it is hard for neural network using phase 1 and 2 to find a good embedding such that after embedding these two strings are close to each other. Phase 3 solves this problem by padding. However, in most cases, the two strings are already aligned, then Phase 3 incurs cost because it may add more paddings and make the embedding worse. \nEmpirically, we observe that phase 1+2 outperforms phase 1+2+3 for most datasets, except for hard cases. But adding phase 3 does not incur much cost (minor cost), so we recommend always adding phase 3, unless the users are extremely performance driven. \n \nQ: Why is a 2 layer GRU necessary and/or sufficient?\nA: Based on our experiments, 1-layer GRU gives inferior performance while 3-layer GRU gives similar results as 2-layer GRU. The network design intuition is mainly from the field of natural language processing. The sequence of characters in our problem resembles a sentence in NLP task, while each character in the sequence resembles a word in NLP. It is observed that if two sentences have similar meaning then GRU will learn similar embeddings for them. Therefore, we believe GRU (or other NLP network structures) is suitable for our task. \n\nThank you for reading our responses. If you have further questions, we are happy to discuss."}, "signatures": ["ICLR.cc/2020/Conference/Paper1129/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1129/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhangxiyuan@zju.edu.cn", "yuanyang@tsinghua.edu.cn", "indyk@mit.edu"], "title": "Neural Embeddings for Nearest Neighbor Search Under Edit Distance", "authors": ["Xiyuan Zhang", "Yang Yuan", "Piotr Indyk"], "pdf": "/pdf/7f10042dfc3aaf2cc0fd1847c4b9ac2f19a02dd2.pdf", "TL;DR": "We propose a learning-based edit distance embedding method, which improves over prior data-independent approaches.", "abstract": "The edit distance between two sequences is an important metric with many applications. The drawback, however, is the high computational cost of many basic problems involving this notion, such as the nearest neighbor search. A natural approach to overcoming this issue is to embed the sequences into a vector space such that the geometric distance in the target space approximates the edit distance in the original space. However, the known edit distance embedding algorithms, such as Chakraborty et al.(2016), construct embeddings that are data-independent, i.e., do not exploit any structure of embedded sets of strings. In this paper we propose an alternative approach, which learns the embedding function according to the data distribution. Our experiments show that the new algorithm has much better empirical performance than prior data-independent methods. ", "keywords": ["Embedding", "Edit Distance", "Nearest Neighbor Search", "Learning-Augmented Algorithm"], "paperhash": "zhang|neural_embeddings_for_nearest_neighbor_search_under_edit_distance", "original_pdf": "/attachment/020fef6aebc4baeb078f4c753aaead88fef71538.pdf", "_bibtex": "@misc{\nzhang2020neural,\ntitle={Neural Embeddings for Nearest Neighbor Search Under Edit Distance},\nauthor={Xiyuan Zhang and Yang Yuan and Piotr Indyk},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlWIANtPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlWIANtPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1129/Authors", "ICLR.cc/2020/Conference/Paper1129/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1129/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1129/Reviewers", "ICLR.cc/2020/Conference/Paper1129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1129/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1129/Authors|ICLR.cc/2020/Conference/Paper1129/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160793, "tmdate": 1576860555857, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1129/Authors", "ICLR.cc/2020/Conference/Paper1129/Reviewers", "ICLR.cc/2020/Conference/Paper1129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1129/-/Official_Comment"}}}, {"id": "BkerHGrSjH", "original": null, "number": 2, "cdate": 1573372477394, "ddate": null, "tcdate": 1573372477394, "tmdate": 1573373628802, "tddate": null, "forum": "HJlWIANtPH", "replyto": "BJljrxLpKr", "invitation": "ICLR.cc/2020/Conference/Paper1129/-/Official_Comment", "content": {"title": "Response to Official Review #3", "comment": "Thank you for your detailed comments and suggestions! We have updated our paper accordingly.\n\nQ: Are you memorizing the dataset?\nA: No, we are not memorizing the dataset, sorry for the confusion! In our experiment, we use less than 2% data points in the whole dataset for training. We have updated the paper in Section 5.1 to make it clear. \n\nQ: Why use absolute loss instead of least square loss? \nA: In our implementation, we have adopted learning rate decay so the learning rate becomes smaller with epochs increasing, which solves the lr/2 distance problem. Empirically, we observe that absolute loss has similar performance compared with least square loss. But you are right, if one uses fixed learning rate, it makes more sense to use least square loss. \n\nQ: How did you sample positive and negative data points? \nA: We randomly sample the positive pairs from top-5 edit distance and sample negative pairs as the top-30 distance in the embedded space. This approach is fairly standard for triplet loss, see e.g. [1, 2].\n[1] Distance metric Learning for large margin nearest neighbor classification.\n[2] Spreading vector for similarity search \n \nQ: Why during phase 2, the phase loss is stopped?\nA: We have tried to train phase 1 and phase 2 together as a multi-task problem, but the empirical results were not as good. This is because phase 1 first scales and approximates the edit distance then phase 2 further captures the relative similarity, so the order of phase 1 and phase 2 matters. We have also tried to first train phase 1 then eq5 + eq6 together, but the results were also not as good, potentially because it\u2019s not easy to tune the final loss function that combines different losses together.\n \nQ: How are you optimizing? \nA: We use Adam optimizer and 4:1 train: validation split for cross-validation and tune the hyper-parameters.\n\nQ: Is there a typo in eq7?\nA: Thank you for pointing it out. This is not a typo, but the description is indeed confusing. We have updated it in the new version, and hopefully it\u2019s now better.  \n \nQ: related work: LSDE: Levenshtein Space Deep Embedding for Query-by-string Word Spotting\nA: Thanks for pointing out this related work. We have cited it in our paper. \n\nThank you for reading our responses. If you have further questions, we are happy to discuss."}, "signatures": ["ICLR.cc/2020/Conference/Paper1129/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1129/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhangxiyuan@zju.edu.cn", "yuanyang@tsinghua.edu.cn", "indyk@mit.edu"], "title": "Neural Embeddings for Nearest Neighbor Search Under Edit Distance", "authors": ["Xiyuan Zhang", "Yang Yuan", "Piotr Indyk"], "pdf": "/pdf/7f10042dfc3aaf2cc0fd1847c4b9ac2f19a02dd2.pdf", "TL;DR": "We propose a learning-based edit distance embedding method, which improves over prior data-independent approaches.", "abstract": "The edit distance between two sequences is an important metric with many applications. The drawback, however, is the high computational cost of many basic problems involving this notion, such as the nearest neighbor search. A natural approach to overcoming this issue is to embed the sequences into a vector space such that the geometric distance in the target space approximates the edit distance in the original space. However, the known edit distance embedding algorithms, such as Chakraborty et al.(2016), construct embeddings that are data-independent, i.e., do not exploit any structure of embedded sets of strings. In this paper we propose an alternative approach, which learns the embedding function according to the data distribution. Our experiments show that the new algorithm has much better empirical performance than prior data-independent methods. ", "keywords": ["Embedding", "Edit Distance", "Nearest Neighbor Search", "Learning-Augmented Algorithm"], "paperhash": "zhang|neural_embeddings_for_nearest_neighbor_search_under_edit_distance", "original_pdf": "/attachment/020fef6aebc4baeb078f4c753aaead88fef71538.pdf", "_bibtex": "@misc{\nzhang2020neural,\ntitle={Neural Embeddings for Nearest Neighbor Search Under Edit Distance},\nauthor={Xiyuan Zhang and Yang Yuan and Piotr Indyk},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlWIANtPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlWIANtPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1129/Authors", "ICLR.cc/2020/Conference/Paper1129/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1129/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1129/Reviewers", "ICLR.cc/2020/Conference/Paper1129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1129/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1129/Authors|ICLR.cc/2020/Conference/Paper1129/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160793, "tmdate": 1576860555857, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1129/Authors", "ICLR.cc/2020/Conference/Paper1129/Reviewers", "ICLR.cc/2020/Conference/Paper1129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1129/-/Official_Comment"}}}, {"id": "S1gaimrBiB", "original": null, "number": 3, "cdate": 1573372836806, "ddate": null, "tcdate": 1573372836806, "tmdate": 1573372836806, "tddate": null, "forum": "HJlWIANtPH", "replyto": "SJgPAR6hYr", "invitation": "ICLR.cc/2020/Conference/Paper1129/-/Official_Comment", "content": {"title": "Response to Official Review #2", "comment": "Thank you for your detailed comments and suggestions! We have updated our paper accordingly. \n\nQ: Three phases are optimizing the same network, which is not explicitly mentioned \nA: Sorry for the confusion. We have updated the description in the introduction, and explicitly mention that we are optimizing the same network. Moreover, each phase is initialized using the final weights from the previous phase, and the first phase is using the default random initialization from PyTorch.\n\nQ: How theoretical properties of CGK/CGK\u2019 are used by the neural network? And why is CGK\u2019 better than CGK?\nA: CGK\u2019 is for hard examples. Consider \u2018ACTCGC\u2019 and \u2018CACTCG\u2019. \u2018CACTCG\u2019 can be edited by shifting one character to the right from \u2018ACTCGC\u2019, so their edit distance is two. However,  these two strings differ at every position, so it\u2019s hard for neural network that uses just phases 1 and 2 to find a good embedding such that after embedding these two strings are close to each other. However, CGK/CGK\u2019 easily tackles this situation since it may randomly insert some characters into the string (see Algorithm 1), so that the two strings will be matched afterward. This is the main intuition behind our proof. We hope to use this alignment property so we apply CGK\u2019 into our embedding, and optimize its parameters. Notice that CGK\u2019 is better than CGK in the sense that CGK may repeat each character multiple times, but CGK\u2019 only does it at most twice. Therefore, we only need to compare the two cases for each character, and use that information for optimization (see Algorithm 4). \n \nQ: In your experiments, did you verify that using CGK\u2019 indeed helps?\nA: Phase 3 is the optimized version of CGK\u2019 and empirically, experiments on synthetic hard case data set illustrate the improvement of utilizing phase 3. See Appendix A.3 for details. \n \nQ: The network outputs continuous or binary vector?\nA: Sorry for the confusion. The network outputs a continuous vector. We have fixed the confusing parts in the paper. And indeed, the analysis of CGK and CGK\u2019 are based on binary vectors, but our optimized CGK\u2019 (see Algorithm 3) can output continuous vectors. \n \nQ: Reconstruction of the embedding?\nA: Binary outputs of CGK\u2019 can be decoded, but outputs of embeddings as a whole cannot be reconstructed. \n \nQ: How did you sample positive and negative data points? \nA: We randomly sample the positive pairs from top-5 edit distance and sample negative pairs as the top-30 distance in the embedded space. This approach is fairly standard for triplet loss, see e.g. [1, 2].\n[1] Distance metric Learning for large margin nearest neighbor classification.\n[2] Spreading vector for similarity search \n \nQ: Can you include some measure of variance based on different train and/or test splits\nA: Sure. We have tried different test queries and due to time constraint, we have only updated the results of Neural Phase 1+2+3 in Table 5. We will update other results as well in the next version of our paper. \n \nQ: Why use 3 phases, not just a single phase? \nA: We have tried to train phase 1 and phase 2 together as a multi-task problem, but the empirical results we obtained were not as good. This is because phase 1 first scales and approximates the edit distance, and then phase 2 further captures the relative similarity, so the order of phase 1 and phase 2 seems to matter. Furthermore, phase 3 cannot be jointly trained with phase 1 and phase 2, because in phase 3 we are also optimizing the CGK\u2019 algorithm, which uses approximation to the gradients (see Algorithm 4). This phase is more like a fine tuning step. \nMoreover, even if all phases can be trained together, one needs to tune the final loss function that combines all the losses together. Our current three phases training avoids this tuning problem because we just train the network until convergence for each phase, without using any hyperparameters. \n\nQ: Reference format not consistent\nA: Thanks for pointing them out! We have adjusted the format.\n\nThank you for reading our responses. If you have further questions, we are happy to discuss."}, "signatures": ["ICLR.cc/2020/Conference/Paper1129/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1129/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhangxiyuan@zju.edu.cn", "yuanyang@tsinghua.edu.cn", "indyk@mit.edu"], "title": "Neural Embeddings for Nearest Neighbor Search Under Edit Distance", "authors": ["Xiyuan Zhang", "Yang Yuan", "Piotr Indyk"], "pdf": "/pdf/7f10042dfc3aaf2cc0fd1847c4b9ac2f19a02dd2.pdf", "TL;DR": "We propose a learning-based edit distance embedding method, which improves over prior data-independent approaches.", "abstract": "The edit distance between two sequences is an important metric with many applications. The drawback, however, is the high computational cost of many basic problems involving this notion, such as the nearest neighbor search. A natural approach to overcoming this issue is to embed the sequences into a vector space such that the geometric distance in the target space approximates the edit distance in the original space. However, the known edit distance embedding algorithms, such as Chakraborty et al.(2016), construct embeddings that are data-independent, i.e., do not exploit any structure of embedded sets of strings. In this paper we propose an alternative approach, which learns the embedding function according to the data distribution. Our experiments show that the new algorithm has much better empirical performance than prior data-independent methods. ", "keywords": ["Embedding", "Edit Distance", "Nearest Neighbor Search", "Learning-Augmented Algorithm"], "paperhash": "zhang|neural_embeddings_for_nearest_neighbor_search_under_edit_distance", "original_pdf": "/attachment/020fef6aebc4baeb078f4c753aaead88fef71538.pdf", "_bibtex": "@misc{\nzhang2020neural,\ntitle={Neural Embeddings for Nearest Neighbor Search Under Edit Distance},\nauthor={Xiyuan Zhang and Yang Yuan and Piotr Indyk},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlWIANtPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlWIANtPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1129/Authors", "ICLR.cc/2020/Conference/Paper1129/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1129/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1129/Reviewers", "ICLR.cc/2020/Conference/Paper1129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1129/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1129/Authors|ICLR.cc/2020/Conference/Paper1129/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160793, "tmdate": 1576860555857, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1129/Authors", "ICLR.cc/2020/Conference/Paper1129/Reviewers", "ICLR.cc/2020/Conference/Paper1129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1129/-/Official_Comment"}}}, {"id": "rJggF6khFS", "original": null, "number": 1, "cdate": 1571712375635, "ddate": null, "tcdate": 1571712375635, "tmdate": 1572972509057, "tddate": null, "forum": "HJlWIANtPH", "replyto": "HJlWIANtPH", "invitation": "ICLR.cc/2020/Conference/Paper1129/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "\nThe paper proposes a scheme to learn representations for sequences\nusing neural networks such that the Hamming distance in the embedded\nspace is close to the edit distance in the original representation of\nthe sequences. This is achieved through a 3-phase algorithm which are\nclearly motivated and technically sound. The empirical results clearly\ndemonstrate the gains from the representation learning on multiple\ndata sets and scenarios. \n\nGiven the clarity, technical soundness and the improved empirical\nperformance, I am leaning towards an accept. However, there are a\ncouple of open questions that, if addressed, would help me better\nunderstand the contributions:\n\n- The main concern is the need for the separated out phases instead\n  of directly minimizing some combinations of these different\n  terms. Is there any inherent reason why the loss function cannot be\n  combined to just train a single neural network in an end-to-end\n  manner instead of this phase-wise manner. The presence of these\n  distinct phases require the reader to figure out when to switch\n  phases, and how dependent the downstream performance is to the\n  phase change decision. \n- As expected, there is a cost to Phase 3. The text is Sec. 5.4 does\n  not appear to match up with the Table 5. The table indicates that\n  removing phase 3 actually improves performance in all but the top-10\n  and top-200 case. The text says something different. Please clarify.\n- Moreover, if there is a cost to Phase 3, can this be properly\n  explained? If there is a cost to it, could there be a way to decide\n  if/when we require the phase 3?\n\n\nMinor:\n\n- It would also be important to understand the effect of the network\n  architecture on the downstream performance. Why is a 2 layer GRU\n  necessary and/or sufficient? Some intuition regarding this would be\n  very useful and can indicate the ease of applicability of the\n  proposed scheme across different data sets.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1129/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1129/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhangxiyuan@zju.edu.cn", "yuanyang@tsinghua.edu.cn", "indyk@mit.edu"], "title": "Neural Embeddings for Nearest Neighbor Search Under Edit Distance", "authors": ["Xiyuan Zhang", "Yang Yuan", "Piotr Indyk"], "pdf": "/pdf/7f10042dfc3aaf2cc0fd1847c4b9ac2f19a02dd2.pdf", "TL;DR": "We propose a learning-based edit distance embedding method, which improves over prior data-independent approaches.", "abstract": "The edit distance between two sequences is an important metric with many applications. The drawback, however, is the high computational cost of many basic problems involving this notion, such as the nearest neighbor search. A natural approach to overcoming this issue is to embed the sequences into a vector space such that the geometric distance in the target space approximates the edit distance in the original space. However, the known edit distance embedding algorithms, such as Chakraborty et al.(2016), construct embeddings that are data-independent, i.e., do not exploit any structure of embedded sets of strings. In this paper we propose an alternative approach, which learns the embedding function according to the data distribution. Our experiments show that the new algorithm has much better empirical performance than prior data-independent methods. ", "keywords": ["Embedding", "Edit Distance", "Nearest Neighbor Search", "Learning-Augmented Algorithm"], "paperhash": "zhang|neural_embeddings_for_nearest_neighbor_search_under_edit_distance", "original_pdf": "/attachment/020fef6aebc4baeb078f4c753aaead88fef71538.pdf", "_bibtex": "@misc{\nzhang2020neural,\ntitle={Neural Embeddings for Nearest Neighbor Search Under Edit Distance},\nauthor={Xiyuan Zhang and Yang Yuan and Piotr Indyk},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlWIANtPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJlWIANtPH", "replyto": "HJlWIANtPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1129/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1129/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575412036806, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1129/Reviewers"], "noninvitees": [], "tcdate": 1570237741941, "tmdate": 1575412036820, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1129/-/Official_Review"}}}, {"id": "BJljrxLpKr", "original": null, "number": 3, "cdate": 1571803202798, "ddate": null, "tcdate": 1571803202798, "tmdate": 1572972508970, "tddate": null, "forum": "HJlWIANtPH", "replyto": "HJlWIANtPH", "invitation": "ICLR.cc/2020/Conference/Paper1129/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Authors propose a three phase learning schedule to find embedding vectors for sequences. The goal is to have the euclidean distance of embedding vectors mimic the edit distance of input sequences. Given such embedding one can perform faster approximate nearest neighbor search in compare to calculating pairwise edit distances.\nThey use a RNN to output a real value per sequence step. At first they pretrain with the absolute difference of euclidean distance and edit distance. Then they fine tune with a triplet loss, such that the difference in euclidean distances be larger than the difference in edit distances. Finally they modify the embeddings with a stochastic, differentiable algorithm such that they can get guarantees for generalization.\n\nUnfortunately, the manuscript is not well written. There is a high chance of misunderstandings. What I gather from the experiment section is that their model is trained on the whole corpus. During training repeatedly trains with absolute loss on pairwise edit distances. During inference random 100 of those same sequences that has been trained on are selected to compare with the rest. If this is true, I fail to grasp the point of this paper. Since during training you have effectively calculated all the pairwise edit distances. There is no generalization happening. This paper has effectively memorized the edit distances of some sequences. \n\nIt seems that only phase 3 (cgk') is designed to have any accuracy on unseen sequences, and experiments show that it underperforms the original cgk.\n\nIf this is not true and indeed they are training for example on one half of the corpus and the 100 query + base are unseen during training I am willing to increase my score. Given the added clarification in the paper.\n\nAgain assuming that this is not just memorization:\n\nWhy eq 5 (regression loss) is the absolute value? It means that you will never get closer than lr/2 to the optimal point, where as with a least squares loss your gradients get smaller when you get closer.\n\nHow are the negative sample, positive samples selected for an anchor? Are they just two random points, is there any importance sampling happening?\n\nWhy during phase 2, the phase 1 loss is stopped? There is no intuition, justification in the paper. Why the loss is not eq 5 + eq 6 during the whole training?\n\nHow are you optimizing? SGD I assume? How are you selecting hyper-parameters, such as learning rate? Is there any validation set?\n\nIs there a typo in eq 7? The text says \"we calculate the absolute loss as in Phase 1 to optimize our embedding network f3\" but eq 7 is on f'(3). Are you backpropagating through algorithm 3 toward embeddings or just toward thres as in algorithm 4?\n\nCurrently, given the poor quality of the write up, the merit of the idea and the experiments is not clear.\n\nRelated work: LSDE: Levenshtein Space Deep Embedding for Query-by-string Word Spotting"}, "signatures": ["ICLR.cc/2020/Conference/Paper1129/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1129/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhangxiyuan@zju.edu.cn", "yuanyang@tsinghua.edu.cn", "indyk@mit.edu"], "title": "Neural Embeddings for Nearest Neighbor Search Under Edit Distance", "authors": ["Xiyuan Zhang", "Yang Yuan", "Piotr Indyk"], "pdf": "/pdf/7f10042dfc3aaf2cc0fd1847c4b9ac2f19a02dd2.pdf", "TL;DR": "We propose a learning-based edit distance embedding method, which improves over prior data-independent approaches.", "abstract": "The edit distance between two sequences is an important metric with many applications. The drawback, however, is the high computational cost of many basic problems involving this notion, such as the nearest neighbor search. A natural approach to overcoming this issue is to embed the sequences into a vector space such that the geometric distance in the target space approximates the edit distance in the original space. However, the known edit distance embedding algorithms, such as Chakraborty et al.(2016), construct embeddings that are data-independent, i.e., do not exploit any structure of embedded sets of strings. In this paper we propose an alternative approach, which learns the embedding function according to the data distribution. Our experiments show that the new algorithm has much better empirical performance than prior data-independent methods. ", "keywords": ["Embedding", "Edit Distance", "Nearest Neighbor Search", "Learning-Augmented Algorithm"], "paperhash": "zhang|neural_embeddings_for_nearest_neighbor_search_under_edit_distance", "original_pdf": "/attachment/020fef6aebc4baeb078f4c753aaead88fef71538.pdf", "_bibtex": "@misc{\nzhang2020neural,\ntitle={Neural Embeddings for Nearest Neighbor Search Under Edit Distance},\nauthor={Xiyuan Zhang and Yang Yuan and Piotr Indyk},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlWIANtPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJlWIANtPH", "replyto": "HJlWIANtPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1129/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1129/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575412036806, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1129/Reviewers"], "noninvitees": [], "tcdate": 1570237741941, "tmdate": 1575412036820, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1129/-/Official_Review"}}}, {"id": "BkxL1N7e5S", "original": null, "number": 1, "cdate": 1571988445763, "ddate": null, "tcdate": 1571988445763, "tmdate": 1571988445763, "tddate": null, "forum": "HJlWIANtPH", "replyto": "HklJ9PRk9r", "invitation": "ICLR.cc/2020/Conference/Paper1129/-/Official_Comment", "content": {"title": "Anonymous Code Available", "comment": "Thanks for your interest in our work! Code is available here https://drive.google.com/drive/folders/1FuyzFq7vIh9biTY3YFRgFRgfRCehng5C?usp=sharing"}, "signatures": ["ICLR.cc/2020/Conference/Paper1129/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1129/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhangxiyuan@zju.edu.cn", "yuanyang@tsinghua.edu.cn", "indyk@mit.edu"], "title": "Neural Embeddings for Nearest Neighbor Search Under Edit Distance", "authors": ["Xiyuan Zhang", "Yang Yuan", "Piotr Indyk"], "pdf": "/pdf/7f10042dfc3aaf2cc0fd1847c4b9ac2f19a02dd2.pdf", "TL;DR": "We propose a learning-based edit distance embedding method, which improves over prior data-independent approaches.", "abstract": "The edit distance between two sequences is an important metric with many applications. The drawback, however, is the high computational cost of many basic problems involving this notion, such as the nearest neighbor search. A natural approach to overcoming this issue is to embed the sequences into a vector space such that the geometric distance in the target space approximates the edit distance in the original space. However, the known edit distance embedding algorithms, such as Chakraborty et al.(2016), construct embeddings that are data-independent, i.e., do not exploit any structure of embedded sets of strings. In this paper we propose an alternative approach, which learns the embedding function according to the data distribution. Our experiments show that the new algorithm has much better empirical performance than prior data-independent methods. ", "keywords": ["Embedding", "Edit Distance", "Nearest Neighbor Search", "Learning-Augmented Algorithm"], "paperhash": "zhang|neural_embeddings_for_nearest_neighbor_search_under_edit_distance", "original_pdf": "/attachment/020fef6aebc4baeb078f4c753aaead88fef71538.pdf", "_bibtex": "@misc{\nzhang2020neural,\ntitle={Neural Embeddings for Nearest Neighbor Search Under Edit Distance},\nauthor={Xiyuan Zhang and Yang Yuan and Piotr Indyk},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlWIANtPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlWIANtPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1129/Authors", "ICLR.cc/2020/Conference/Paper1129/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1129/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1129/Reviewers", "ICLR.cc/2020/Conference/Paper1129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1129/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1129/Authors|ICLR.cc/2020/Conference/Paper1129/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160793, "tmdate": 1576860555857, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1129/Authors", "ICLR.cc/2020/Conference/Paper1129/Reviewers", "ICLR.cc/2020/Conference/Paper1129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1129/-/Official_Comment"}}}, {"id": "HklJ9PRk9r", "original": null, "number": 1, "cdate": 1571968903474, "ddate": null, "tcdate": 1571968903474, "tmdate": 1571980705497, "tddate": null, "forum": "HJlWIANtPH", "replyto": "HJlWIANtPH", "invitation": "ICLR.cc/2020/Conference/Paper1129/-/Public_Comment", "content": {"title": "Could you please provide source code?", "comment": "Hi, it's an interesting idea. Is it possible to share the source code using an anonymous link(e.g. putting the source code on gofile)? Thanks."}, "signatures": ["~Xinyan_Dai1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Xinyan_Dai1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhangxiyuan@zju.edu.cn", "yuanyang@tsinghua.edu.cn", "indyk@mit.edu"], "title": "Neural Embeddings for Nearest Neighbor Search Under Edit Distance", "authors": ["Xiyuan Zhang", "Yang Yuan", "Piotr Indyk"], "pdf": "/pdf/7f10042dfc3aaf2cc0fd1847c4b9ac2f19a02dd2.pdf", "TL;DR": "We propose a learning-based edit distance embedding method, which improves over prior data-independent approaches.", "abstract": "The edit distance between two sequences is an important metric with many applications. The drawback, however, is the high computational cost of many basic problems involving this notion, such as the nearest neighbor search. A natural approach to overcoming this issue is to embed the sequences into a vector space such that the geometric distance in the target space approximates the edit distance in the original space. However, the known edit distance embedding algorithms, such as Chakraborty et al.(2016), construct embeddings that are data-independent, i.e., do not exploit any structure of embedded sets of strings. In this paper we propose an alternative approach, which learns the embedding function according to the data distribution. Our experiments show that the new algorithm has much better empirical performance than prior data-independent methods. ", "keywords": ["Embedding", "Edit Distance", "Nearest Neighbor Search", "Learning-Augmented Algorithm"], "paperhash": "zhang|neural_embeddings_for_nearest_neighbor_search_under_edit_distance", "original_pdf": "/attachment/020fef6aebc4baeb078f4c753aaead88fef71538.pdf", "_bibtex": "@misc{\nzhang2020neural,\ntitle={Neural Embeddings for Nearest Neighbor Search Under Edit Distance},\nauthor={Xiyuan Zhang and Yang Yuan and Piotr Indyk},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlWIANtPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlWIANtPH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504199167, "tmdate": 1576860589009, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1129/Authors", "ICLR.cc/2020/Conference/Paper1129/Reviewers", "ICLR.cc/2020/Conference/Paper1129/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1129/-/Public_Comment"}}}], "count": 13}