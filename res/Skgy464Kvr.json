{"notes": [{"id": "Skgy464Kvr", "original": "H1eezu-Pvr", "number": 471, "cdate": 1569439015216, "ddate": null, "tcdate": 1569439015216, "tmdate": 1583912024765, "tddate": null, "forum": "Skgy464Kvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions", "authors": ["Yao Qin", "Nicholas Frosst", "Sara Sabour", "Colin Raffel", "Garrison Cottrell", "Geoffrey Hinton"], "authorids": ["yaq007@eng.ucsd.edu", "frosst@google.com", "sasabour@google.com", "craffel@google.com", "gary@eng.ucsd.edu", "geoffhinton@google.com"], "keywords": ["Adversarial Examples", "Detection of adversarial attacks"], "abstract": "Adversarial examples raise questions about whether neural network models are sensitive to the same visual features as humans. In this paper, we first detect adversarial examples or otherwise corrupted images based on a class-conditional reconstruction of the input. To specifically attack our detection mechanism, we propose the Reconstructive Attack which seeks both to cause a misclassification and a low reconstruction error. This reconstructive attack produces undetected adversarial examples but with much smaller success rate. Among all these attacks, we find that CapsNets always perform better than convolutional networks. Then, we diagnose the adversarial examples for CapsNets and find that the success of the reconstructive attack is highly related to the visual similarity between the source and target class. Additionally, the resulting perturbations can cause the input image to appear visually more like the target class and hence become non-adversarial. This suggests that CapsNets use features that are more aligned with human perception and have the potential to address the central issue raised by adversarial examples.", "pdf": "/pdf/7b2f6965891fd2117fa2b90b0a0951f3e06c79cd.pdf", "paperhash": "qin|detecting_and_diagnosing_adversarial_images_with_classconditional_capsule_reconstructions", "_bibtex": "@inproceedings{\nQin2020Detecting,\ntitle={Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions},\nauthor={Yao Qin and Nicholas Frosst and Sara Sabour and Colin Raffel and Garrison Cottrell and Geoffrey Hinton},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgy464Kvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/590fc920ad9801a3b607114e45acea5fa0afef3b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "d-0NsuthcE", "original": null, "number": 1, "cdate": 1576798697453, "ddate": null, "tcdate": 1576798697453, "tmdate": 1576800938308, "tddate": null, "forum": "Skgy464Kvr", "replyto": "Skgy464Kvr", "invitation": "ICLR.cc/2020/Conference/Paper471/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper presents a mechanism for capsule networks to defend against adversarial examples, and a new attack, the reconstruction attack. The differing success of this attacks on capsnets and convnets is used to argue that capsnets find features that are more similar to what humans use.\n\nReviewers generally like the paper, but took instance with the strength of the claim (about the usefulness of the examples) and argued that the paper might not be as novel as it claims.\n\nStill, this seems like a valuable contribution that should be published.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions", "authors": ["Yao Qin", "Nicholas Frosst", "Sara Sabour", "Colin Raffel", "Garrison Cottrell", "Geoffrey Hinton"], "authorids": ["yaq007@eng.ucsd.edu", "frosst@google.com", "sasabour@google.com", "craffel@google.com", "gary@eng.ucsd.edu", "geoffhinton@google.com"], "keywords": ["Adversarial Examples", "Detection of adversarial attacks"], "abstract": "Adversarial examples raise questions about whether neural network models are sensitive to the same visual features as humans. In this paper, we first detect adversarial examples or otherwise corrupted images based on a class-conditional reconstruction of the input. To specifically attack our detection mechanism, we propose the Reconstructive Attack which seeks both to cause a misclassification and a low reconstruction error. This reconstructive attack produces undetected adversarial examples but with much smaller success rate. Among all these attacks, we find that CapsNets always perform better than convolutional networks. Then, we diagnose the adversarial examples for CapsNets and find that the success of the reconstructive attack is highly related to the visual similarity between the source and target class. Additionally, the resulting perturbations can cause the input image to appear visually more like the target class and hence become non-adversarial. This suggests that CapsNets use features that are more aligned with human perception and have the potential to address the central issue raised by adversarial examples.", "pdf": "/pdf/7b2f6965891fd2117fa2b90b0a0951f3e06c79cd.pdf", "paperhash": "qin|detecting_and_diagnosing_adversarial_images_with_classconditional_capsule_reconstructions", "_bibtex": "@inproceedings{\nQin2020Detecting,\ntitle={Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions},\nauthor={Yao Qin and Nicholas Frosst and Sara Sabour and Colin Raffel and Garrison Cottrell and Geoffrey Hinton},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgy464Kvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/590fc920ad9801a3b607114e45acea5fa0afef3b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Skgy464Kvr", "replyto": "Skgy464Kvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795715914, "tmdate": 1576800265935, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper471/-/Decision"}}}, {"id": "ByeNI910FS", "original": null, "number": 1, "cdate": 1571842635609, "ddate": null, "tcdate": 1571842635609, "tmdate": 1574734716655, "tddate": null, "forum": "Skgy464Kvr", "replyto": "Skgy464Kvr", "invitation": "ICLR.cc/2020/Conference/Paper471/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "This paper proposed a new defense method for capsule networks. For both white-box and black-box settings, the proposed CapNets has shown superior performance than two variants of CNNs. The visualizations of adversarial examples generated by the CapNets are more aligned with the human perception which is very insightful. On the corrupted MNIST dataset, the results show the proposed defense method can also be used well as an out-of-distribution detector. Overall the paper is clearly written and easy to follow.\n\n\n\nHere I have some concerns:\n\nThe major one is the limited comparisons. For the defense of CNNs, the author only implemented two types of the strategy, which is derived from the characteristics of the capsule network. However, there are also other a lot of defense methods for CNNs which are designed according to the characteristics of themselves. I think the author should also compare the defense performance with those methods to hold the strong claims that CpasNets always perform better than convolutional networks. Also, the experiments performed on Cifar-10 is very limited.\n\n\n\nThe adversarial examples generated by the CapsNets shown in Figure 3 and Figure 11 are indeed changing the number shape which is aligned with human perception. However, some studies for CNNs has also found similar results on MNIST(Towards Deep Learning Models Resistant to Adversarial Attacks) and CIFAR-10(RANDOM MASK: Towards Robust Convolutional Neural Networks). The author should provide more visualizations on other datasets such as CIFAR-10 to support the contribution that the features captured by CapsNets are more aligned with human perception than CNNs.\n\n=========================================================\nAfter Rebuttal:\n\nI thank the author for the response. \n\nI still think the argument that CpasNets always perform better than convolutional networks is an overstatement since you only performed a few defense methods. A milder one is more suitable.\n\nAlso, the high variance of the Capsule Network in Figure 10 can tell something but it is not enough. Could you find similar visible semantic changes in CIFAR-10 dataset as Figure 5?  If yes, you should also list some results. The mentioned CNN works could find similar phenomena on both MNIST and CIFAR-10 dataset. I do not see the strong evidence for the argument that features captured by CapsNets are more aligned with human perception than CNNs.\n\nTo sum up, I think some arguments are overstatements. But this is a good work to analyze the robustness of Capsule Net, I would like to rate 6.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper471/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper471/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions", "authors": ["Yao Qin", "Nicholas Frosst", "Sara Sabour", "Colin Raffel", "Garrison Cottrell", "Geoffrey Hinton"], "authorids": ["yaq007@eng.ucsd.edu", "frosst@google.com", "sasabour@google.com", "craffel@google.com", "gary@eng.ucsd.edu", "geoffhinton@google.com"], "keywords": ["Adversarial Examples", "Detection of adversarial attacks"], "abstract": "Adversarial examples raise questions about whether neural network models are sensitive to the same visual features as humans. In this paper, we first detect adversarial examples or otherwise corrupted images based on a class-conditional reconstruction of the input. To specifically attack our detection mechanism, we propose the Reconstructive Attack which seeks both to cause a misclassification and a low reconstruction error. This reconstructive attack produces undetected adversarial examples but with much smaller success rate. Among all these attacks, we find that CapsNets always perform better than convolutional networks. Then, we diagnose the adversarial examples for CapsNets and find that the success of the reconstructive attack is highly related to the visual similarity between the source and target class. Additionally, the resulting perturbations can cause the input image to appear visually more like the target class and hence become non-adversarial. This suggests that CapsNets use features that are more aligned with human perception and have the potential to address the central issue raised by adversarial examples.", "pdf": "/pdf/7b2f6965891fd2117fa2b90b0a0951f3e06c79cd.pdf", "paperhash": "qin|detecting_and_diagnosing_adversarial_images_with_classconditional_capsule_reconstructions", "_bibtex": "@inproceedings{\nQin2020Detecting,\ntitle={Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions},\nauthor={Yao Qin and Nicholas Frosst and Sara Sabour and Colin Raffel and Garrison Cottrell and Geoffrey Hinton},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgy464Kvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/590fc920ad9801a3b607114e45acea5fa0afef3b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Skgy464Kvr", "replyto": "Skgy464Kvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper471/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper471/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576283628493, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper471/Reviewers"], "noninvitees": [], "tcdate": 1570237751646, "tmdate": 1576283628507, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper471/-/Official_Review"}}}, {"id": "rylNjbVcjB", "original": null, "number": 4, "cdate": 1573695899715, "ddate": null, "tcdate": 1573695899715, "tmdate": 1573719351282, "tddate": null, "forum": "Skgy464Kvr", "replyto": "ByeNI910FS", "invitation": "ICLR.cc/2020/Conference/Paper471/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thanks very much for your suggestions and we believe that most of your concerns are about the performance of our model on the CIFAR-10 dataset. Therefore, we take your advice to add more experiments on the CIFAR-10 dataset in Section 5.4 in the paper to validate the effectiveness of our methods. \n\nFirst, we compare our models to state-of-the-art CNN-based models against defense-aware attacks (the defense-aware attacks are the most important attack in evaluating the performance of detection methods) and the results are shown in Table 4. We can see that our deflecting model based on CapsNets has a much better detection performance against defense-aware attacks. We should emphasize that most of the existing detection models based on CNNs are further fully attacked by defense-aware attacks. Our detection method based on CapsNets can still have a good detection performance against defense-aware attacks. \n\nIn addition, we also plot the white-box and black-box Undetected Rate vs False Positive Rate of the defense-aware R-PGD attack by sweeping the threshold of the l2 reconstruction error from [0, 30] ( Please see Figure 3 (left)). We can see that the black-box R-PGD has a much smaller undetected rate compared to the white-box R-PGD, which implies that the CapsNet greatly reduces the attack transferability. \n\nFurthermore, we include a comparison among our CapsNet with class-conditional reconstruction, \u201cDeepCaps\u201d proposed in [1] and \u201cCapsNet All\u201d using all the capsule information rather than the winning-capsule information on the  CIFAR-10 dataset. The classification performance of the three models on the clean test set is very similar, around 92.5%. The undetected rate of the defense-aware R-PGD versus the False Positive Rate is displayed in Figure 3 (right). We can see that \n1) our CapsNet with class-conditional information has the smallest Undetected Rate among the three models. \n2) The CapsNet using all the capsule information has the worst detection performance. This is also consistent with CNN-based models on the other three datasets. As shown in Figure 2, CNN with class-conditional information CNN+CR always has a better detection performance compared to CNN without class-conditional reconstruction CNN+R. \n\nFinally, we also add the visualization of the adversarial success rates for each source/target pair for adversarial attacks on CIFAR-10 with a l_infinity bound 8/255 in Figure 10 in the Appendix. Similarly, we can see that there is a higher variance for the CapsNet model than CNN models. Therefore, we claim that these visualizations on the Fashion-MNIST, SVHN and CIFAR-10 datasets suggest that the features captured by CapsNets are more aligned to human perception compared to CNNs.\n\n[1] Rajasegaran, Jathushan, et al. \"DeepCaps: Going Deeper with Capsule Networks.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper471/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper471/Authors", "ICLR.cc/2020/Conference/Paper471/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper471/Reviewers", "ICLR.cc/2020/Conference/Paper471/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper471/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions", "authors": ["Yao Qin", "Nicholas Frosst", "Sara Sabour", "Colin Raffel", "Garrison Cottrell", "Geoffrey Hinton"], "authorids": ["yaq007@eng.ucsd.edu", "frosst@google.com", "sasabour@google.com", "craffel@google.com", "gary@eng.ucsd.edu", "geoffhinton@google.com"], "keywords": ["Adversarial Examples", "Detection of adversarial attacks"], "abstract": "Adversarial examples raise questions about whether neural network models are sensitive to the same visual features as humans. In this paper, we first detect adversarial examples or otherwise corrupted images based on a class-conditional reconstruction of the input. To specifically attack our detection mechanism, we propose the Reconstructive Attack which seeks both to cause a misclassification and a low reconstruction error. This reconstructive attack produces undetected adversarial examples but with much smaller success rate. Among all these attacks, we find that CapsNets always perform better than convolutional networks. Then, we diagnose the adversarial examples for CapsNets and find that the success of the reconstructive attack is highly related to the visual similarity between the source and target class. Additionally, the resulting perturbations can cause the input image to appear visually more like the target class and hence become non-adversarial. This suggests that CapsNets use features that are more aligned with human perception and have the potential to address the central issue raised by adversarial examples.", "pdf": "/pdf/7b2f6965891fd2117fa2b90b0a0951f3e06c79cd.pdf", "paperhash": "qin|detecting_and_diagnosing_adversarial_images_with_classconditional_capsule_reconstructions", "_bibtex": "@inproceedings{\nQin2020Detecting,\ntitle={Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions},\nauthor={Yao Qin and Nicholas Frosst and Sara Sabour and Colin Raffel and Garrison Cottrell and Geoffrey Hinton},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgy464Kvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/590fc920ad9801a3b607114e45acea5fa0afef3b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skgy464Kvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper471/Authors", "ICLR.cc/2020/Conference/Paper471/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper471/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper471/Reviewers", "ICLR.cc/2020/Conference/Paper471/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper471/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper471/Authors|ICLR.cc/2020/Conference/Paper471/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170997, "tmdate": 1576860542194, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper471/Authors", "ICLR.cc/2020/Conference/Paper471/Reviewers", "ICLR.cc/2020/Conference/Paper471/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper471/-/Official_Comment"}}}, {"id": "Skl8EWE5or", "original": null, "number": 3, "cdate": 1573695790494, "ddate": null, "tcdate": 1573695790494, "tmdate": 1573719248533, "tddate": null, "forum": "Skgy464Kvr", "replyto": "rkeJjZa0tB", "invitation": "ICLR.cc/2020/Conference/Paper471/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thanks very much for your very positive feedback and support for our work. The answers to your questions are listed below:\n\n1. We use the simple sum operation on each neuron group to serve as the logit because 1) the sum operation has already enabled the CNN classifier to have a comparable prediction performance to CapsNets.  2) We control the number of parameters in CapsNets and CNNs to be exactly the same. Therefore, the simple sum operation will not incorporate extra parameters compared to CapsNets. Since our target is to compare the performance between CapsNet and CNN-based networks against adversarial examples, we make sure the classification performance and the number of parameters for different network architectures are similar for a fair comparison. 3) In addition, we hypothesize the linear relation of each neuron in each neuron group could be learned by the previous layer to some extent. Therefore, we do not think a linear combination would incorporate a big difference in the results reported in our work.\n\n2. Thanks very much for your suggestion to draw the AUC curve. We added a plot of the Undetected Rate of the defense-aware R-PGD attack vs the False Positive Rate curve in Figure 2 by changing the threshold of the l2 reconstruction error to measure the performance of different detection methods. We should note that the standard AUC curve plots the True Positive Rate vs False Positive Rate. Since the success rate for the adversarial examples is close to 100%, the AUC curve is enough to measure detection performance. However, in our CapsNets-based model, the success rate (bounded by a small l_infinity norm) is far from 100%. A weak attack that has a small success rate may have a substantially smaller True Positive Rate compared to a stronger attack (The smaller the True Positive Rate is, the stronger the attack is). In this case, the AUC curve is not sufficient to compare different attacks and detection methods. Therefore, we added a plot of the Undetected Rate of the defense-aware R-PGD attack vs the False Positive Rate curve in Figure 2 by changing the threshold of the l2 reconstruction error from [0, 20] for MNIST, [0, 40]  for Fashion-MNIST and [0, 300] for SVHN. Similarly, we can see that CapsNets perform much better than CNN-based networks. That is: at the same False Positive Rate, the defense aware R-PGD attack against CapsNets always has the smallest Undetected Rate (smaller is better for the detection method).\n\n\n3. Thanks very much for pointing out the class-conditional information. We add a section \u201cClass-conditional Information\u201d in Section 5.4, where we compare our CapsNet with \u201cDeepCaps\u201d[1] and \u201cCapsNet All\u201d. The classification performance of the three models on the clean test set is very similar, around 92.5%. The undetected rate of the defense-aware R-PGD versus the False Positive Rate is displayed in Figure 3 (right). We can see that \n1) our CapsNet with class-conditional information has the smallest Undetected Rate among the three models. \n2) The CapsNet using all the capsule information has the worst detection performance. This is also consistent with CNN-based models on the other three datasets. As shown in Figure 2, CNN with class-conditional information CNN+CR always has a better detection performance compared to CNN without class-conditional reconstruction CNN+R. \n3)  DeepCaps has a little worse detection performance compared to our CapsNet. \nTherefore, we conclude that the class-conditional information is necessary to achieve robustness to adversarial attacks. We include these analyses in Section 5.4.\n\n[1] Rajasegaran, Jathushan, et al. \"DeepCaps: Going Deeper with Capsule Networks.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper471/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper471/Authors", "ICLR.cc/2020/Conference/Paper471/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper471/Reviewers", "ICLR.cc/2020/Conference/Paper471/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper471/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions", "authors": ["Yao Qin", "Nicholas Frosst", "Sara Sabour", "Colin Raffel", "Garrison Cottrell", "Geoffrey Hinton"], "authorids": ["yaq007@eng.ucsd.edu", "frosst@google.com", "sasabour@google.com", "craffel@google.com", "gary@eng.ucsd.edu", "geoffhinton@google.com"], "keywords": ["Adversarial Examples", "Detection of adversarial attacks"], "abstract": "Adversarial examples raise questions about whether neural network models are sensitive to the same visual features as humans. In this paper, we first detect adversarial examples or otherwise corrupted images based on a class-conditional reconstruction of the input. To specifically attack our detection mechanism, we propose the Reconstructive Attack which seeks both to cause a misclassification and a low reconstruction error. This reconstructive attack produces undetected adversarial examples but with much smaller success rate. Among all these attacks, we find that CapsNets always perform better than convolutional networks. Then, we diagnose the adversarial examples for CapsNets and find that the success of the reconstructive attack is highly related to the visual similarity between the source and target class. Additionally, the resulting perturbations can cause the input image to appear visually more like the target class and hence become non-adversarial. This suggests that CapsNets use features that are more aligned with human perception and have the potential to address the central issue raised by adversarial examples.", "pdf": "/pdf/7b2f6965891fd2117fa2b90b0a0951f3e06c79cd.pdf", "paperhash": "qin|detecting_and_diagnosing_adversarial_images_with_classconditional_capsule_reconstructions", "_bibtex": "@inproceedings{\nQin2020Detecting,\ntitle={Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions},\nauthor={Yao Qin and Nicholas Frosst and Sara Sabour and Colin Raffel and Garrison Cottrell and Geoffrey Hinton},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgy464Kvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/590fc920ad9801a3b607114e45acea5fa0afef3b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skgy464Kvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper471/Authors", "ICLR.cc/2020/Conference/Paper471/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper471/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper471/Reviewers", "ICLR.cc/2020/Conference/Paper471/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper471/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper471/Authors|ICLR.cc/2020/Conference/Paper471/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170997, "tmdate": 1576860542194, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper471/Authors", "ICLR.cc/2020/Conference/Paper471/Reviewers", "ICLR.cc/2020/Conference/Paper471/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper471/-/Official_Comment"}}}, {"id": "ryl7blVcsH", "original": null, "number": 1, "cdate": 1573695482538, "ddate": null, "tcdate": 1573695482538, "tmdate": 1573719222781, "tddate": null, "forum": "Skgy464Kvr", "replyto": "Skgy464Kvr", "invitation": "ICLR.cc/2020/Conference/Paper471/-/Official_Comment", "content": {"title": "General Response", "comment": "We thank all the reviewers for their constructive and positive feedback and take the reviewer\u2019s suggestions including more experiments and adding the corresponding analysis in the paper. They are mainly:\n\n1) We plot the undetected rate versus the False Positive Rate by changing the threshold on four datasets, see Figure 2.\n2) We include the comparison with other state-of-the-art CNN based models on CIFAR-10 dataset, see Table 4.\n3) We add the visualization of the success rate between source/target pairs on the CIFAR-10 dataset, see Figure 10 in the Appendix.\n4) We compare our CapsNet with two other CapsNet variants on CIFAR-10 dataset: one is a CapsNet without masking mechanism and another one is a CapsNet with class-independent reconstruction[1], see Figure 3 (right).\n\nPlease feel free to let us know if you have other questions and thanks again for your helpful suggestions.\n\n[1] Rajasegaran, Jathushan, et al. \"DeepCaps: Going Deeper with Capsule Networks.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper471/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper471/Authors", "ICLR.cc/2020/Conference/Paper471/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper471/Reviewers", "ICLR.cc/2020/Conference/Paper471/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper471/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions", "authors": ["Yao Qin", "Nicholas Frosst", "Sara Sabour", "Colin Raffel", "Garrison Cottrell", "Geoffrey Hinton"], "authorids": ["yaq007@eng.ucsd.edu", "frosst@google.com", "sasabour@google.com", "craffel@google.com", "gary@eng.ucsd.edu", "geoffhinton@google.com"], "keywords": ["Adversarial Examples", "Detection of adversarial attacks"], "abstract": "Adversarial examples raise questions about whether neural network models are sensitive to the same visual features as humans. In this paper, we first detect adversarial examples or otherwise corrupted images based on a class-conditional reconstruction of the input. To specifically attack our detection mechanism, we propose the Reconstructive Attack which seeks both to cause a misclassification and a low reconstruction error. This reconstructive attack produces undetected adversarial examples but with much smaller success rate. Among all these attacks, we find that CapsNets always perform better than convolutional networks. Then, we diagnose the adversarial examples for CapsNets and find that the success of the reconstructive attack is highly related to the visual similarity between the source and target class. Additionally, the resulting perturbations can cause the input image to appear visually more like the target class and hence become non-adversarial. This suggests that CapsNets use features that are more aligned with human perception and have the potential to address the central issue raised by adversarial examples.", "pdf": "/pdf/7b2f6965891fd2117fa2b90b0a0951f3e06c79cd.pdf", "paperhash": "qin|detecting_and_diagnosing_adversarial_images_with_classconditional_capsule_reconstructions", "_bibtex": "@inproceedings{\nQin2020Detecting,\ntitle={Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions},\nauthor={Yao Qin and Nicholas Frosst and Sara Sabour and Colin Raffel and Garrison Cottrell and Geoffrey Hinton},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgy464Kvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/590fc920ad9801a3b607114e45acea5fa0afef3b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skgy464Kvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper471/Authors", "ICLR.cc/2020/Conference/Paper471/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper471/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper471/Reviewers", "ICLR.cc/2020/Conference/Paper471/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper471/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper471/Authors|ICLR.cc/2020/Conference/Paper471/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170997, "tmdate": 1576860542194, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper471/Authors", "ICLR.cc/2020/Conference/Paper471/Reviewers", "ICLR.cc/2020/Conference/Paper471/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper471/-/Official_Comment"}}}, {"id": "SJg7sxEcjS", "original": null, "number": 2, "cdate": 1573695643236, "ddate": null, "tcdate": 1573695643236, "tmdate": 1573695643236, "tddate": null, "forum": "Skgy464Kvr", "replyto": "SJxlSj5VcS", "invitation": "ICLR.cc/2020/Conference/Paper471/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We really appreciate the reviewer\u2019s positive feedback and helpful suggestions. We answer your questions as follows:\n\n1. We agree with the reviewer\u2019s opinion that our current method has not been scaled to a large-scale real-word dataset such as ImageNet. However, we should note that Capsule Networks have never been scaled to a large dataset like ImageNet. Our work successfully scaled CapsNets to the CIFAR-10 dataset with state-of-art classification accuracy of 92.2% on the clean test set and we found that our methods generalize very well from MNIST, SVHN to CIFAR-10, which suggests it is very promising to make our detection methods generalize to even larger datasets once CapsNets are scaled to ImageNet. We also appreciate the reviewer\u2019s suggestion of an even better similarity metric rather than l_2 distance. We believe these are promising avenues for future work and we add them to Section 7 in the paper.\n\n2. To address the concern that \u201cit is not fully convincing that features learned by CapsNets are superior to features learned by CNN baselines\u201d: \n(1) Our intention is not to claim that the learned features are superior. Instead, what we conclude is that the experimental results on MNIST, Fashion-MNIST, SVHN and CIFAR10 \u201csuggest\u201d that features learned by CapsNet may be more robust to adversarial attack and may be more aligned to human perception. \n(2) Second, the network architecture (eg. number of layers) and the hyper-parameters related to adversarial optimization are different for each dataset. \n(3) Lastly, apart from the visualization on the Fashion-MNIST and SVHN datasets displayed in Section 6 in the paper, we also add the visualization of  the adversarial success rates for each source/target pair for adversarial attacks on CIFAR10 with an l_infinity bound of 8/255 in Figure 10 in Appendix. Similarly, we can see that there is a higher variance for the CapsNet model than CNN models.\n\n3.  We totally agree with the reviewer that the proposed method is not specific to generative models using class labels for conditioning. We believe it is very promising for our method to generalize to other conditions  (e.g., image-to-image translation) as well. We add this as a promising and interesting future work in Section 7.\n\n4.  For our proposed defense-aware R-PGD attack, we did not observe a significant difference compared to other existing attacks. However, we observe that the attack transferability against CapsNets will be greatly reduced compared to CNN based networks. \n\n5.  We take the reviewer\u2019s advice to draw the AUC curve to measure the performance of different detection methods in Figure 2. We should note that the standard AUC curve plots the True Positive Rate vs False Positive Rate. Since the success rate for standard adversarial examples is close to 100%, the AUC curve is enough to measure detection performance. However, in our CapsNets-based model, the success rate (bounded by a small l_infinity norm) is far lower than 100%. A weak attack that has a small success rate may have a substantially smaller True Positive Rate compared to a stronger attack (The smaller the True Positive Rate is, the stronger the attack is). In this case, the AUC curve is not sufficient to compare different attacks and detection methods. Therefore, we have added a plot of the Undetected Rate of the defense-aware R-PGD attack vs the False Positive Rate curve in Figure 2 by changing the threshold of the l2 reconstruction error from [0, 20] for MNIST, [0, 40]  for Fashion-MNIST and [0, 300] for SVHN. Similarly, we can see that CapsNets perform much better than CNN-based networks. That is: at the same False Positive Rate, the defense aware R-PGD attack against CapsNets always has the smallest Undetected Rate (smaller is better for the detection method).\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper471/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper471/Authors", "ICLR.cc/2020/Conference/Paper471/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper471/Reviewers", "ICLR.cc/2020/Conference/Paper471/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper471/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions", "authors": ["Yao Qin", "Nicholas Frosst", "Sara Sabour", "Colin Raffel", "Garrison Cottrell", "Geoffrey Hinton"], "authorids": ["yaq007@eng.ucsd.edu", "frosst@google.com", "sasabour@google.com", "craffel@google.com", "gary@eng.ucsd.edu", "geoffhinton@google.com"], "keywords": ["Adversarial Examples", "Detection of adversarial attacks"], "abstract": "Adversarial examples raise questions about whether neural network models are sensitive to the same visual features as humans. In this paper, we first detect adversarial examples or otherwise corrupted images based on a class-conditional reconstruction of the input. To specifically attack our detection mechanism, we propose the Reconstructive Attack which seeks both to cause a misclassification and a low reconstruction error. This reconstructive attack produces undetected adversarial examples but with much smaller success rate. Among all these attacks, we find that CapsNets always perform better than convolutional networks. Then, we diagnose the adversarial examples for CapsNets and find that the success of the reconstructive attack is highly related to the visual similarity between the source and target class. Additionally, the resulting perturbations can cause the input image to appear visually more like the target class and hence become non-adversarial. This suggests that CapsNets use features that are more aligned with human perception and have the potential to address the central issue raised by adversarial examples.", "pdf": "/pdf/7b2f6965891fd2117fa2b90b0a0951f3e06c79cd.pdf", "paperhash": "qin|detecting_and_diagnosing_adversarial_images_with_classconditional_capsule_reconstructions", "_bibtex": "@inproceedings{\nQin2020Detecting,\ntitle={Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions},\nauthor={Yao Qin and Nicholas Frosst and Sara Sabour and Colin Raffel and Garrison Cottrell and Geoffrey Hinton},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgy464Kvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/590fc920ad9801a3b607114e45acea5fa0afef3b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skgy464Kvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper471/Authors", "ICLR.cc/2020/Conference/Paper471/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper471/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper471/Reviewers", "ICLR.cc/2020/Conference/Paper471/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper471/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper471/Authors|ICLR.cc/2020/Conference/Paper471/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170997, "tmdate": 1576860542194, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper471/Authors", "ICLR.cc/2020/Conference/Paper471/Reviewers", "ICLR.cc/2020/Conference/Paper471/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper471/-/Official_Comment"}}}, {"id": "rkeJjZa0tB", "original": null, "number": 2, "cdate": 1571897751389, "ddate": null, "tcdate": 1571897751389, "tmdate": 1572972591198, "tddate": null, "forum": "Skgy464Kvr", "replyto": "Skgy464Kvr", "invitation": "ICLR.cc/2020/Conference/Paper471/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work proposes to detect adversarial examples or otherwise corrupted images with the reconstruction network, which is used to regularise CapsNets. To further confirm the effectiveness of their detection method, they propose the Reconstructive Attack, which seeks both to cause a misclassification and a low reconstruction error. The comprehensive experiments are conducted.\n\nAlthough the idea is not very novel, the paper makes enough contributions to get accepted. Especially, Section 6 diagnoses the adversarial examples for CapsNets and shows the relationship between the success of the attack and the visual similarity between the source and target class.\n\nWe have the following question for authors about this work:\n\n1. Baseline models: This work creates the baseline model CNN+CR, by dividing the penultimate hidden layer of a CNN into groups corresponding to each class. The sum of each neuron group serves as the logit for that particular class. Why is the sum operation used to create the logit? The sum operation is pretty rare the existing CNN architectures. A linear combination may be a better choice?\n\n2. Detection Threshold: this paper empirically chooses the 95th percentile of validation distances as the threshold to detect adversary samples. Why not report the Area Under Curve (AOC) score? It is a more comprehensive evaluation metric for such a problem.\n\n3. Class-conditional information: This paper follows the architecture in Sabour et al. (2017) where the reconstruction is class-conditional. The work DeepCaps [1] shows that the reconstruction without class-conditional information leads to the better disentanglement of instantiation parameters. Is the class-conditional information necessary to achieve the robustness to adversary attacks?\n[1] Rajasegaran, Jathushan, et al. \"DeepCaps: Going Deeper with Capsule Networks.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019."}, "signatures": ["ICLR.cc/2020/Conference/Paper471/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper471/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions", "authors": ["Yao Qin", "Nicholas Frosst", "Sara Sabour", "Colin Raffel", "Garrison Cottrell", "Geoffrey Hinton"], "authorids": ["yaq007@eng.ucsd.edu", "frosst@google.com", "sasabour@google.com", "craffel@google.com", "gary@eng.ucsd.edu", "geoffhinton@google.com"], "keywords": ["Adversarial Examples", "Detection of adversarial attacks"], "abstract": "Adversarial examples raise questions about whether neural network models are sensitive to the same visual features as humans. In this paper, we first detect adversarial examples or otherwise corrupted images based on a class-conditional reconstruction of the input. To specifically attack our detection mechanism, we propose the Reconstructive Attack which seeks both to cause a misclassification and a low reconstruction error. This reconstructive attack produces undetected adversarial examples but with much smaller success rate. Among all these attacks, we find that CapsNets always perform better than convolutional networks. Then, we diagnose the adversarial examples for CapsNets and find that the success of the reconstructive attack is highly related to the visual similarity between the source and target class. Additionally, the resulting perturbations can cause the input image to appear visually more like the target class and hence become non-adversarial. This suggests that CapsNets use features that are more aligned with human perception and have the potential to address the central issue raised by adversarial examples.", "pdf": "/pdf/7b2f6965891fd2117fa2b90b0a0951f3e06c79cd.pdf", "paperhash": "qin|detecting_and_diagnosing_adversarial_images_with_classconditional_capsule_reconstructions", "_bibtex": "@inproceedings{\nQin2020Detecting,\ntitle={Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions},\nauthor={Yao Qin and Nicholas Frosst and Sara Sabour and Colin Raffel and Garrison Cottrell and Geoffrey Hinton},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgy464Kvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/590fc920ad9801a3b607114e45acea5fa0afef3b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Skgy464Kvr", "replyto": "Skgy464Kvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper471/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper471/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576283628493, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper471/Reviewers"], "noninvitees": [], "tcdate": 1570237751646, "tmdate": 1576283628507, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper471/-/Official_Review"}}}, {"id": "SJxlSj5VcS", "original": null, "number": 3, "cdate": 1572281144212, "ddate": null, "tcdate": 1572281144212, "tmdate": 1572972591157, "tddate": null, "forum": "Skgy464Kvr", "replyto": "Skgy464Kvr", "invitation": "ICLR.cc/2020/Conference/Paper471/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the problem of detecting and generating adversarial images using class-conditional capsule networks. Specifically, this paper first introduced a novel method that detects adversarial examples by class-conditional image reconstruction. Motivated by this defense method, this paper further proposed a novel reconstructive attack that minimizes both classification and reconstruction loss. Experimental evaluations are conducted on MNIST, FashionMNIST, SVHN, and CIFAR-10 dataset. Results demonstrate the effectiveness of the proposed defense and the novel reconstructive attack method.\n\nOverall, this paper is well-motivated and presentation is clear. It proposed a smart way of using the class-conditional generative model to improve the adversarial robustness. Please address the following questions.\n\n(1) Reviewer\u2019s major concern is that this method is not very scalable to large-scale real-world datasets such as ImageNet and SUN database. First, training a class-conditional generative model on MNIST is relatively easy compared to ImageNet. The generative model could potentially create image artifacts on higher resolution images. \n\n-- SUN Database: Large-scale Scene Recognition from Abbey to Zoo. Xiao et al. In CVPR 2010.\n\nSecond, the proposed proxy based on l_2 image distance might not be effective at all for higher-resolution images.\n\n-- A note on the evaluation of generative models, Theis et al. In ICLR 2016.\n-- The Unreasonable Effectiveness of Deep Features as a Perceptual Metric, Zhang et al. In CVPR 2018.\n\n(2) Reviewer is not fully convinced by the argument that features learned by CapsNets are superior to features learned by CNN baselines. To draw such conclusion, it is necessary to conduct systematic experiments with different CapsNets and CNNs architectures (e.g., number of layers) and other hyper-parameters related to the adversarial optimization.\n\n(3) It looks like the proposed method is not specific to generative models use class labels as condition. reviewer is curious whether the method generalizes to other conditions (e.g., image-to-image translation) as well.\n\n-- Learning Structured Output Representation using Deep Conditional Generative Models, Sohn et al. In NIPS 2015.\n-- Image-to-Image Translation with Conditional Adversarial Nets, Isola et al. In CVPR 2017.\n-- Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks, Zhu et al. In ICCV 2017.\n-- Semantic Image Synthesis with Spatially-Adaptive Normalization, Park et al. In CVPR 2019.\n\n(4) Can you possibly comment on the attack transferability compared to other existing attacks evaluated in this paper?\n\n(5) Detection threshold: Can you possibly draw the AOC curve or report the Area Under Curve (AOC) score?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper471/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper471/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions", "authors": ["Yao Qin", "Nicholas Frosst", "Sara Sabour", "Colin Raffel", "Garrison Cottrell", "Geoffrey Hinton"], "authorids": ["yaq007@eng.ucsd.edu", "frosst@google.com", "sasabour@google.com", "craffel@google.com", "gary@eng.ucsd.edu", "geoffhinton@google.com"], "keywords": ["Adversarial Examples", "Detection of adversarial attacks"], "abstract": "Adversarial examples raise questions about whether neural network models are sensitive to the same visual features as humans. In this paper, we first detect adversarial examples or otherwise corrupted images based on a class-conditional reconstruction of the input. To specifically attack our detection mechanism, we propose the Reconstructive Attack which seeks both to cause a misclassification and a low reconstruction error. This reconstructive attack produces undetected adversarial examples but with much smaller success rate. Among all these attacks, we find that CapsNets always perform better than convolutional networks. Then, we diagnose the adversarial examples for CapsNets and find that the success of the reconstructive attack is highly related to the visual similarity between the source and target class. Additionally, the resulting perturbations can cause the input image to appear visually more like the target class and hence become non-adversarial. This suggests that CapsNets use features that are more aligned with human perception and have the potential to address the central issue raised by adversarial examples.", "pdf": "/pdf/7b2f6965891fd2117fa2b90b0a0951f3e06c79cd.pdf", "paperhash": "qin|detecting_and_diagnosing_adversarial_images_with_classconditional_capsule_reconstructions", "_bibtex": "@inproceedings{\nQin2020Detecting,\ntitle={Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions},\nauthor={Yao Qin and Nicholas Frosst and Sara Sabour and Colin Raffel and Garrison Cottrell and Geoffrey Hinton},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skgy464Kvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/590fc920ad9801a3b607114e45acea5fa0afef3b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Skgy464Kvr", "replyto": "Skgy464Kvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper471/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper471/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576283628493, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper471/Reviewers"], "noninvitees": [], "tcdate": 1570237751646, "tmdate": 1576283628507, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper471/-/Official_Review"}}}], "count": 9}