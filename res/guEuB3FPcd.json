{"notes": [{"id": "guEuB3FPcd", "original": "94X0OzI6px5", "number": 3482, "cdate": 1601308386429, "ddate": null, "tcdate": 1601308386429, "tmdate": 1614985661049, "tddate": null, "forum": "guEuB3FPcd", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "AlgebraNets", "authorids": ["~Jordan_Hoffmann1", "~Simon_Schmitt1", "~Simon_Osindero1", "~Karen_Simonyan1", "~Erich_Elsen1"], "authors": ["Jordan Hoffmann", "Simon Schmitt", "Simon Osindero", "Karen Simonyan", "Erich Elsen"], "keywords": ["Sparsity", "Pruning", "Efficiency", "Mathematics"], "abstract": "Neural networks have historically been built layerwise from the set of functions in ${f: \\mathbb{R}^n \\to \\mathbb{R}^m }$, i.e. with activations and weights/parameters represented by real numbers, $\\mathbb{R}$. Our work considers a richer set of objects for activations and weights, and undertakes a comprehensive study of alternative algebras as number representations by studying their performance on two challenging problems: large-scale image classification using the ImageNet dataset and language modeling using the enwiki8 and WikiText-103 datasets. We denote this broader class of models as AlgebraNets. Our findings indicate that the conclusions of prior work, which explored neural networks constructed from $\\mathbb{C}$ (complex numbers) and $\\mathbb{H}$ (quaternions) on smaller datasets, do not always transfer to these challenging settings.  However, our results demonstrate that there are alternative algebras which deliver better parameter and computational efficiency compared with $\\mathbb{R}$. We consider $\\mathbb{C}$, $\\mathbb{H}$, $M_{2}(\\mathbb{R})$ (the set of $2\\times2$ real-valued matrices), $M_{2}(\\mathbb{C})$, $M_{3}(\\mathbb{R})$, $M_{4}(\\mathbb{R})$, dual numbers and the $\\mathbb{R}^3$ cross product.  Additionally, we note that multiplication in these algebras has higher compute density than real multiplication, a useful property in situations with inherently limited parameter reuse such as auto-regressive inference and sparse neural networks. We therefore investigate how to induce sparsity within AlgebraNets. We hope that our strong results on large-scale, practical benchmarks will spur further exploration of these unconventional architectures which challenge the default choice of using real numbers for neural network weights and activations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hoffmann|algebranets", "one-sentence_summary": "We investigate a wide range of alternatives to real-valued weights in neural networks and find promising alternatives. ", "supplementary_material": "/attachment/2873be7fd30024a18ed0260327bb37527ed3d60f.zip", "pdf": "/pdf/a29db3f04aadec5e862ecd2dc316a628e12fd6f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iaVGYcsIw", "_bibtex": "@misc{\nhoffmann2021algebranets,\ntitle={AlgebraNets},\nauthor={Jordan Hoffmann and Simon Schmitt and Simon Osindero and Karen Simonyan and Erich Elsen},\nyear={2021},\nurl={https://openreview.net/forum?id=guEuB3FPcd}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 19, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "kzKLnXf-M74", "original": null, "number": 1, "cdate": 1610040500801, "ddate": null, "tcdate": 1610040500801, "tmdate": 1610474107497, "tddate": null, "forum": "guEuB3FPcd", "replyto": "guEuB3FPcd", "invitation": "ICLR.cc/2021/Conference/Paper3482/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper proposes to deep neural network models with elements of the weight from algebras, and considers a wide range of algebras and large scale promising experiments. The paper raised a heated discussion.\n\nPros: \n\n- Using algebras, one can hope for more efficient architectures \n\n- Numerical experiments on a wide range of problems\n\nCons:\n\n - The theoretical grounding provided in the current version of the paper is not sufficient. The study is empirical (nothing wrong about it), but there is no clear understanding/explanation of why particular choice is better than another, and also why it works in the particular setup. \n\n- The title does not reflect the content of the paper. It is too broad, and also in some sense \u201cprovocative\u201d. The reader expects something much more significant from it.\n\n- Experiment setup: the resulting flops/accuracy figure (main result, Figure 1) does not contain error bars.  I.e., the accuracies should be averaged over several random seeds in order to guarantee the resulting metrics. Also, this figure does not show a clear advantage over the ResNet-50 baseline."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AlgebraNets", "authorids": ["~Jordan_Hoffmann1", "~Simon_Schmitt1", "~Simon_Osindero1", "~Karen_Simonyan1", "~Erich_Elsen1"], "authors": ["Jordan Hoffmann", "Simon Schmitt", "Simon Osindero", "Karen Simonyan", "Erich Elsen"], "keywords": ["Sparsity", "Pruning", "Efficiency", "Mathematics"], "abstract": "Neural networks have historically been built layerwise from the set of functions in ${f: \\mathbb{R}^n \\to \\mathbb{R}^m }$, i.e. with activations and weights/parameters represented by real numbers, $\\mathbb{R}$. Our work considers a richer set of objects for activations and weights, and undertakes a comprehensive study of alternative algebras as number representations by studying their performance on two challenging problems: large-scale image classification using the ImageNet dataset and language modeling using the enwiki8 and WikiText-103 datasets. We denote this broader class of models as AlgebraNets. Our findings indicate that the conclusions of prior work, which explored neural networks constructed from $\\mathbb{C}$ (complex numbers) and $\\mathbb{H}$ (quaternions) on smaller datasets, do not always transfer to these challenging settings.  However, our results demonstrate that there are alternative algebras which deliver better parameter and computational efficiency compared with $\\mathbb{R}$. We consider $\\mathbb{C}$, $\\mathbb{H}$, $M_{2}(\\mathbb{R})$ (the set of $2\\times2$ real-valued matrices), $M_{2}(\\mathbb{C})$, $M_{3}(\\mathbb{R})$, $M_{4}(\\mathbb{R})$, dual numbers and the $\\mathbb{R}^3$ cross product.  Additionally, we note that multiplication in these algebras has higher compute density than real multiplication, a useful property in situations with inherently limited parameter reuse such as auto-regressive inference and sparse neural networks. We therefore investigate how to induce sparsity within AlgebraNets. We hope that our strong results on large-scale, practical benchmarks will spur further exploration of these unconventional architectures which challenge the default choice of using real numbers for neural network weights and activations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hoffmann|algebranets", "one-sentence_summary": "We investigate a wide range of alternatives to real-valued weights in neural networks and find promising alternatives. ", "supplementary_material": "/attachment/2873be7fd30024a18ed0260327bb37527ed3d60f.zip", "pdf": "/pdf/a29db3f04aadec5e862ecd2dc316a628e12fd6f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iaVGYcsIw", "_bibtex": "@misc{\nhoffmann2021algebranets,\ntitle={AlgebraNets},\nauthor={Jordan Hoffmann and Simon Schmitt and Simon Osindero and Karen Simonyan and Erich Elsen},\nyear={2021},\nurl={https://openreview.net/forum?id=guEuB3FPcd}\n}"}, "tags": [], "invitation": {"reply": {"forum": "guEuB3FPcd", "replyto": "guEuB3FPcd", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040500787, "tmdate": 1610474107482, "id": "ICLR.cc/2021/Conference/Paper3482/-/Decision"}}}, {"id": "b13emmFhpZs", "original": null, "number": 1, "cdate": 1603598926327, "ddate": null, "tcdate": 1603598926327, "tmdate": 1606774400911, "tddate": null, "forum": "guEuB3FPcd", "replyto": "guEuB3FPcd", "invitation": "ICLR.cc/2021/Conference/Paper3482/-/Official_Review", "content": {"title": "Interesting study of replacing the traditional real-valued algebra with other associative algebras", "review": "The paper proposes an interesting kind of networks, AlgebraNets, which is a general paradigm of replacing the commonly used real-valued algebra with other associative algebras. This paper considers C, H, M2(R) (the set of 2 \u00d7 2 real-valued matrices), M2(C), M3(R), M4(R), dual numbers, and the R3 cross product, and investigates the sparsity within AlgebraNets. \n\nThe work in the paper is interesting and this paper is generally written well. However, there are a few issues/comments with the work:\n\n1.The citation of the references in the main body of this paper is not easy to read. It will be better to replace the format \u201cauthor(s) (year)\u201d with the format \u201c(author(s), year)\u201d ;\n\n2.Some figures and tables do not appear near the discussion, for example, Figure 1 is shown on Page but it is discussed until page 5, which makes it difficult to read;\n\n3.In Figure 1, the subfigure in the second row and first column, it seems that the performance of model with H and whitening the best stable performance.  The subfigure in the second row and second column, it can be seen that the model with H  is not better than the baseline model;\n\n4.There are many inconsistencies in the format of the reference, for example,\n\n1)In some places the author's name is abbreviated, while in others it is not. References \u201cC. J. Gaudet and A. S. Maida. Deep quaternion networks. In 2018 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138, 2018. \u201d and \u201cGeoffrey E. Hinton, Sara Sabour, and Nicholas Frosst. Matrix capsules with em routing. In ICLR, 2018. \u201d;\n\n2)In some places the conference\u2019s name is abbreviated with the link, while in others it is not. References \u201cSiddhant M. Jayakumar, Wojciech M. Czarnecki, Jacob Menick, Jonathan Schwarz, Jack Rae, Simon Osindero, Yee Whye Teh, Tim Harley, and Razvan Pascanu. Multiplicative interactions and where to find them. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=rylnK6VtDH.\u201d and \u201cGeoffrey E. Hinton, Sara Sabour, and Nicholas Frosst. Matrix capsules with em routing. In ICLR, 2018. \u201d.\n\nPlease check carefully and correct the inconsistencies.\n\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nThe paper replaces the traditional real-valued algebra with other associative algebras and shows its parameter and FLOP efficiency.  In the beginning, \"I think it is an interesting piece of work, and it may be helpful to develop the basic structural design of neural networks. \". However, after getting the response from the author(s), I more doubt the significance of the work in this paper: although many types of models have been proposed in this paper, the improvement over the baseline models is limited. I did not lower the grade on this paper since I thought it would be interesting and important (if effective) to extend the traditional real number field to more complex algebraic structures.\n\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper3482/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AlgebraNets", "authorids": ["~Jordan_Hoffmann1", "~Simon_Schmitt1", "~Simon_Osindero1", "~Karen_Simonyan1", "~Erich_Elsen1"], "authors": ["Jordan Hoffmann", "Simon Schmitt", "Simon Osindero", "Karen Simonyan", "Erich Elsen"], "keywords": ["Sparsity", "Pruning", "Efficiency", "Mathematics"], "abstract": "Neural networks have historically been built layerwise from the set of functions in ${f: \\mathbb{R}^n \\to \\mathbb{R}^m }$, i.e. with activations and weights/parameters represented by real numbers, $\\mathbb{R}$. Our work considers a richer set of objects for activations and weights, and undertakes a comprehensive study of alternative algebras as number representations by studying their performance on two challenging problems: large-scale image classification using the ImageNet dataset and language modeling using the enwiki8 and WikiText-103 datasets. We denote this broader class of models as AlgebraNets. Our findings indicate that the conclusions of prior work, which explored neural networks constructed from $\\mathbb{C}$ (complex numbers) and $\\mathbb{H}$ (quaternions) on smaller datasets, do not always transfer to these challenging settings.  However, our results demonstrate that there are alternative algebras which deliver better parameter and computational efficiency compared with $\\mathbb{R}$. We consider $\\mathbb{C}$, $\\mathbb{H}$, $M_{2}(\\mathbb{R})$ (the set of $2\\times2$ real-valued matrices), $M_{2}(\\mathbb{C})$, $M_{3}(\\mathbb{R})$, $M_{4}(\\mathbb{R})$, dual numbers and the $\\mathbb{R}^3$ cross product.  Additionally, we note that multiplication in these algebras has higher compute density than real multiplication, a useful property in situations with inherently limited parameter reuse such as auto-regressive inference and sparse neural networks. We therefore investigate how to induce sparsity within AlgebraNets. We hope that our strong results on large-scale, practical benchmarks will spur further exploration of these unconventional architectures which challenge the default choice of using real numbers for neural network weights and activations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hoffmann|algebranets", "one-sentence_summary": "We investigate a wide range of alternatives to real-valued weights in neural networks and find promising alternatives. ", "supplementary_material": "/attachment/2873be7fd30024a18ed0260327bb37527ed3d60f.zip", "pdf": "/pdf/a29db3f04aadec5e862ecd2dc316a628e12fd6f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iaVGYcsIw", "_bibtex": "@misc{\nhoffmann2021algebranets,\ntitle={AlgebraNets},\nauthor={Jordan Hoffmann and Simon Schmitt and Simon Osindero and Karen Simonyan and Erich Elsen},\nyear={2021},\nurl={https://openreview.net/forum?id=guEuB3FPcd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "guEuB3FPcd", "replyto": "guEuB3FPcd", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3482/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075008, "tmdate": 1606915800342, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3482/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3482/-/Official_Review"}}}, {"id": "VF52BomZ2Nb", "original": null, "number": 20, "cdate": 1605984894780, "ddate": null, "tcdate": 1605984894780, "tmdate": 1606158262349, "tddate": null, "forum": "guEuB3FPcd", "replyto": "9PElhsrS11z", "invitation": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment", "content": {"title": "You do not understand.", "comment": "If you do not understand them, try to discuss them in detail with your mentors. It is OK to reply/respond that you have better logics while the reviewer may not fully appreciate your work, but do not try to dispute without fully understanding the comments."}, "signatures": ["ICLR.cc/2021/Conference/Paper3482/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AlgebraNets", "authorids": ["~Jordan_Hoffmann1", "~Simon_Schmitt1", "~Simon_Osindero1", "~Karen_Simonyan1", "~Erich_Elsen1"], "authors": ["Jordan Hoffmann", "Simon Schmitt", "Simon Osindero", "Karen Simonyan", "Erich Elsen"], "keywords": ["Sparsity", "Pruning", "Efficiency", "Mathematics"], "abstract": "Neural networks have historically been built layerwise from the set of functions in ${f: \\mathbb{R}^n \\to \\mathbb{R}^m }$, i.e. with activations and weights/parameters represented by real numbers, $\\mathbb{R}$. Our work considers a richer set of objects for activations and weights, and undertakes a comprehensive study of alternative algebras as number representations by studying their performance on two challenging problems: large-scale image classification using the ImageNet dataset and language modeling using the enwiki8 and WikiText-103 datasets. We denote this broader class of models as AlgebraNets. Our findings indicate that the conclusions of prior work, which explored neural networks constructed from $\\mathbb{C}$ (complex numbers) and $\\mathbb{H}$ (quaternions) on smaller datasets, do not always transfer to these challenging settings.  However, our results demonstrate that there are alternative algebras which deliver better parameter and computational efficiency compared with $\\mathbb{R}$. We consider $\\mathbb{C}$, $\\mathbb{H}$, $M_{2}(\\mathbb{R})$ (the set of $2\\times2$ real-valued matrices), $M_{2}(\\mathbb{C})$, $M_{3}(\\mathbb{R})$, $M_{4}(\\mathbb{R})$, dual numbers and the $\\mathbb{R}^3$ cross product.  Additionally, we note that multiplication in these algebras has higher compute density than real multiplication, a useful property in situations with inherently limited parameter reuse such as auto-regressive inference and sparse neural networks. We therefore investigate how to induce sparsity within AlgebraNets. We hope that our strong results on large-scale, practical benchmarks will spur further exploration of these unconventional architectures which challenge the default choice of using real numbers for neural network weights and activations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hoffmann|algebranets", "one-sentence_summary": "We investigate a wide range of alternatives to real-valued weights in neural networks and find promising alternatives. ", "supplementary_material": "/attachment/2873be7fd30024a18ed0260327bb37527ed3d60f.zip", "pdf": "/pdf/a29db3f04aadec5e862ecd2dc316a628e12fd6f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iaVGYcsIw", "_bibtex": "@misc{\nhoffmann2021algebranets,\ntitle={AlgebraNets},\nauthor={Jordan Hoffmann and Simon Schmitt and Simon Osindero and Karen Simonyan and Erich Elsen},\nyear={2021},\nurl={https://openreview.net/forum?id=guEuB3FPcd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "guEuB3FPcd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3482/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3482/Authors|ICLR.cc/2021/Conference/Paper3482/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837068, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment"}}}, {"id": "XLutTBTn4O", "original": null, "number": 19, "cdate": 1605984371465, "ddate": null, "tcdate": 1605984371465, "tmdate": 1606158157886, "tddate": null, "forum": "guEuB3FPcd", "replyto": "x_NtIWstj3y", "invitation": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment", "content": {"title": "Would like to reconsider if you can address the following concerns.", "comment": "1.  Please point out tasks that may have clear need to switch to the proposed algebra sets.  I am NOT saying that \"the provided tasks in the paper are not important\". I am questioning that in a top AI conference, what kind of readers would benefit from such an engineering switch, proposed by this paper.\n\n     I notice the authors questions the review comments. But actually, the meaning and question is like the above.\n\n     I wrote several sentences to encourage the authors to provide responses:\n\n    \"Would the authors be able to justify this?\"  \"The concern is not \"What tasks would you like to see?\" but why engineers need such a switch.\"\n\n    \"I would hope the authors clarify their methodology, and then present the advantages obtained in the experiments.\"  \n \n    \"I would simply ask the authors to respond to a direct question: how would you like the community to appreciate your work?\"\n\n     \"ImageNet is not that challenging and there may be no clear need to switch to complex numbers\".  For example, if the authors believe ImageNet is your choice, please give more clear reasons for asking many engineers/readers to try your approach. Please be advised, if ICLR accepts this paper, many readers will be interested and would like to try it out. If you can clearly justify it, I would be happy to re-evaluate. \n\n     You previous response \"We are surprised that you do not think ImageNet is not a suitable task for demonstrating this method: it is a challenging task and is widely used as a way to benchmark state-of-the-art methods. \" does not address my above concern. \n     \n\n2.  A second concern about your experiments \"I would hope the authors clarify their methodology, and then present the advantages obtained in the experiments.\"   \n\n    You mentioned some recent and close work use similar performance metrics.  However, the question is would you please \"clarify the experiment methodology\".  Mentioning recent work is good, but does not FULLY address my concern.\n\n3.  Which category would you claim your work to be \"Doing something new, doing something important, doing something new and important\"?\n\n4. Please try to point out the scientific values behind. It can be simple but effective. For example, when I read the manuscript, one can hardly believe \"2x2 matrix rings)... work better than anything ....\" this claim is rather problematic.  Try to provide answers and convince the reviewer and future readers, using two or three sentences (clear logics).\n\n5. This exaggerated claim raised concerns about the rigorous of the methodology of this work.\n\n    Your response can justify that your work has not such exaggeration.  Try to provide response that address such a concern, if future readers also challenge such a exaggeration.\n\n\nAll the above concerns (and some other in my comments), are asking for your clarifications.  \n\nIf you do not understand them, try to discuss them in detail with your mentors.  It is OK to reply/respond that you have better logics while the reviewer may not fully appreciate your work, but do not try to dispute without fully understanding the comments."}, "signatures": ["ICLR.cc/2021/Conference/Paper3482/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AlgebraNets", "authorids": ["~Jordan_Hoffmann1", "~Simon_Schmitt1", "~Simon_Osindero1", "~Karen_Simonyan1", "~Erich_Elsen1"], "authors": ["Jordan Hoffmann", "Simon Schmitt", "Simon Osindero", "Karen Simonyan", "Erich Elsen"], "keywords": ["Sparsity", "Pruning", "Efficiency", "Mathematics"], "abstract": "Neural networks have historically been built layerwise from the set of functions in ${f: \\mathbb{R}^n \\to \\mathbb{R}^m }$, i.e. with activations and weights/parameters represented by real numbers, $\\mathbb{R}$. Our work considers a richer set of objects for activations and weights, and undertakes a comprehensive study of alternative algebras as number representations by studying their performance on two challenging problems: large-scale image classification using the ImageNet dataset and language modeling using the enwiki8 and WikiText-103 datasets. We denote this broader class of models as AlgebraNets. Our findings indicate that the conclusions of prior work, which explored neural networks constructed from $\\mathbb{C}$ (complex numbers) and $\\mathbb{H}$ (quaternions) on smaller datasets, do not always transfer to these challenging settings.  However, our results demonstrate that there are alternative algebras which deliver better parameter and computational efficiency compared with $\\mathbb{R}$. We consider $\\mathbb{C}$, $\\mathbb{H}$, $M_{2}(\\mathbb{R})$ (the set of $2\\times2$ real-valued matrices), $M_{2}(\\mathbb{C})$, $M_{3}(\\mathbb{R})$, $M_{4}(\\mathbb{R})$, dual numbers and the $\\mathbb{R}^3$ cross product.  Additionally, we note that multiplication in these algebras has higher compute density than real multiplication, a useful property in situations with inherently limited parameter reuse such as auto-regressive inference and sparse neural networks. We therefore investigate how to induce sparsity within AlgebraNets. We hope that our strong results on large-scale, practical benchmarks will spur further exploration of these unconventional architectures which challenge the default choice of using real numbers for neural network weights and activations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hoffmann|algebranets", "one-sentence_summary": "We investigate a wide range of alternatives to real-valued weights in neural networks and find promising alternatives. ", "supplementary_material": "/attachment/2873be7fd30024a18ed0260327bb37527ed3d60f.zip", "pdf": "/pdf/a29db3f04aadec5e862ecd2dc316a628e12fd6f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iaVGYcsIw", "_bibtex": "@misc{\nhoffmann2021algebranets,\ntitle={AlgebraNets},\nauthor={Jordan Hoffmann and Simon Schmitt and Simon Osindero and Karen Simonyan and Erich Elsen},\nyear={2021},\nurl={https://openreview.net/forum?id=guEuB3FPcd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "guEuB3FPcd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3482/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3482/Authors|ICLR.cc/2021/Conference/Paper3482/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837068, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment"}}}, {"id": "x_NtIWstj3y", "original": null, "number": 16, "cdate": 1605971391504, "ddate": null, "tcdate": 1605971391504, "tmdate": 1605971391504, "tddate": null, "forum": "guEuB3FPcd", "replyto": "kYdrqZBz_b", "invitation": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment", "content": {"title": "Alternative Title?", "comment": "Because the reviewer has such strong concerns about the title, we wonder if changing the title would allow the reviewer to reconsider their opinion on the rest of the paper?"}, "signatures": ["ICLR.cc/2021/Conference/Paper3482/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AlgebraNets", "authorids": ["~Jordan_Hoffmann1", "~Simon_Schmitt1", "~Simon_Osindero1", "~Karen_Simonyan1", "~Erich_Elsen1"], "authors": ["Jordan Hoffmann", "Simon Schmitt", "Simon Osindero", "Karen Simonyan", "Erich Elsen"], "keywords": ["Sparsity", "Pruning", "Efficiency", "Mathematics"], "abstract": "Neural networks have historically been built layerwise from the set of functions in ${f: \\mathbb{R}^n \\to \\mathbb{R}^m }$, i.e. with activations and weights/parameters represented by real numbers, $\\mathbb{R}$. Our work considers a richer set of objects for activations and weights, and undertakes a comprehensive study of alternative algebras as number representations by studying their performance on two challenging problems: large-scale image classification using the ImageNet dataset and language modeling using the enwiki8 and WikiText-103 datasets. We denote this broader class of models as AlgebraNets. Our findings indicate that the conclusions of prior work, which explored neural networks constructed from $\\mathbb{C}$ (complex numbers) and $\\mathbb{H}$ (quaternions) on smaller datasets, do not always transfer to these challenging settings.  However, our results demonstrate that there are alternative algebras which deliver better parameter and computational efficiency compared with $\\mathbb{R}$. We consider $\\mathbb{C}$, $\\mathbb{H}$, $M_{2}(\\mathbb{R})$ (the set of $2\\times2$ real-valued matrices), $M_{2}(\\mathbb{C})$, $M_{3}(\\mathbb{R})$, $M_{4}(\\mathbb{R})$, dual numbers and the $\\mathbb{R}^3$ cross product.  Additionally, we note that multiplication in these algebras has higher compute density than real multiplication, a useful property in situations with inherently limited parameter reuse such as auto-regressive inference and sparse neural networks. We therefore investigate how to induce sparsity within AlgebraNets. We hope that our strong results on large-scale, practical benchmarks will spur further exploration of these unconventional architectures which challenge the default choice of using real numbers for neural network weights and activations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hoffmann|algebranets", "one-sentence_summary": "We investigate a wide range of alternatives to real-valued weights in neural networks and find promising alternatives. ", "supplementary_material": "/attachment/2873be7fd30024a18ed0260327bb37527ed3d60f.zip", "pdf": "/pdf/a29db3f04aadec5e862ecd2dc316a628e12fd6f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iaVGYcsIw", "_bibtex": "@misc{\nhoffmann2021algebranets,\ntitle={AlgebraNets},\nauthor={Jordan Hoffmann and Simon Schmitt and Simon Osindero and Karen Simonyan and Erich Elsen},\nyear={2021},\nurl={https://openreview.net/forum?id=guEuB3FPcd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "guEuB3FPcd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3482/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3482/Authors|ICLR.cc/2021/Conference/Paper3482/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837068, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment"}}}, {"id": "kYdrqZBz_b", "original": null, "number": 3, "cdate": 1604032652222, "ddate": null, "tcdate": 1604032652222, "tmdate": 1605906498377, "tddate": null, "forum": "guEuB3FPcd", "replyto": "guEuB3FPcd", "invitation": "ICLR.cc/2021/Conference/Paper3482/-/Official_Review", "content": {"title": "Huge title without convincing contribution", "review": "In this paper, the authors propose the usage of complex numbers in deep neural networks. Would be good to know that complex numbers, n x n matrices, quaternions, diagonal matrices, etc. all can be used in neural networks. The authors also claims benchmark performance in large-scale image classification and language modeling.\n\nHowever, this work cannot be appreciated due to the following aspects:\n1. A first question is \"Why it is necessary?\"  Interestingly, the authors already included Section 2.1 Why Algebras?   However,  I am not convinced at all.  A good answer may take either of the two forms: A). simply a math step showing great potential behind;  2) large-scale neural networks that have engineering advantages.  It seems that the authors took the second approach, however, ImageNet is not that challenging and there may be no clear need to switch to complex numbers.  Would the authors be able to justify this?\n\n2. Then, the authors directly go to evaluations. The figures seem to show good advantages.  However, could you please justify your x,y-axis?  The reported results look high biased. As a reviewer, I have to doubt that the authors may have selectively present their results. \n\n    A good research paper on such a big topic, should give clear methodology first, right?  If the methodology is questionable, such good results may become noise to the community.\n    I would hope the authors clarify their methodology, and then present that advantages obtained in the experiments.\n\n3. As a top AI conference, I believe that we are looking for intellectual contributions.\n    This paper is working on a huge title, which is attractive. However, when I try to identify the intellectual contributions (can be theory, algorithm, engineering, applications), I am not convinced at all.  I know such a topic is not easy to handle. I would simple ask the authors to respond to a direct question: how would like the community to appreciate your work?\n\nNOTE: a lot of disputes are around \"the huge title 'AlgebraNets'\". However, I did not receive justification response from the authors. A possible reason may be the authors are not aware of how big the topic it is, and were so attractive/confident in the current experimental improvements (which is also very appreciated).\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3482/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AlgebraNets", "authorids": ["~Jordan_Hoffmann1", "~Simon_Schmitt1", "~Simon_Osindero1", "~Karen_Simonyan1", "~Erich_Elsen1"], "authors": ["Jordan Hoffmann", "Simon Schmitt", "Simon Osindero", "Karen Simonyan", "Erich Elsen"], "keywords": ["Sparsity", "Pruning", "Efficiency", "Mathematics"], "abstract": "Neural networks have historically been built layerwise from the set of functions in ${f: \\mathbb{R}^n \\to \\mathbb{R}^m }$, i.e. with activations and weights/parameters represented by real numbers, $\\mathbb{R}$. Our work considers a richer set of objects for activations and weights, and undertakes a comprehensive study of alternative algebras as number representations by studying their performance on two challenging problems: large-scale image classification using the ImageNet dataset and language modeling using the enwiki8 and WikiText-103 datasets. We denote this broader class of models as AlgebraNets. Our findings indicate that the conclusions of prior work, which explored neural networks constructed from $\\mathbb{C}$ (complex numbers) and $\\mathbb{H}$ (quaternions) on smaller datasets, do not always transfer to these challenging settings.  However, our results demonstrate that there are alternative algebras which deliver better parameter and computational efficiency compared with $\\mathbb{R}$. We consider $\\mathbb{C}$, $\\mathbb{H}$, $M_{2}(\\mathbb{R})$ (the set of $2\\times2$ real-valued matrices), $M_{2}(\\mathbb{C})$, $M_{3}(\\mathbb{R})$, $M_{4}(\\mathbb{R})$, dual numbers and the $\\mathbb{R}^3$ cross product.  Additionally, we note that multiplication in these algebras has higher compute density than real multiplication, a useful property in situations with inherently limited parameter reuse such as auto-regressive inference and sparse neural networks. We therefore investigate how to induce sparsity within AlgebraNets. We hope that our strong results on large-scale, practical benchmarks will spur further exploration of these unconventional architectures which challenge the default choice of using real numbers for neural network weights and activations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hoffmann|algebranets", "one-sentence_summary": "We investigate a wide range of alternatives to real-valued weights in neural networks and find promising alternatives. ", "supplementary_material": "/attachment/2873be7fd30024a18ed0260327bb37527ed3d60f.zip", "pdf": "/pdf/a29db3f04aadec5e862ecd2dc316a628e12fd6f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iaVGYcsIw", "_bibtex": "@misc{\nhoffmann2021algebranets,\ntitle={AlgebraNets},\nauthor={Jordan Hoffmann and Simon Schmitt and Simon Osindero and Karen Simonyan and Erich Elsen},\nyear={2021},\nurl={https://openreview.net/forum?id=guEuB3FPcd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "guEuB3FPcd", "replyto": "guEuB3FPcd", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3482/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075008, "tmdate": 1606915800342, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3482/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3482/-/Official_Review"}}}, {"id": "beqnp3pzIr", "original": null, "number": 15, "cdate": 1605895565388, "ddate": null, "tcdate": 1605895565388, "tmdate": 1605895565388, "tddate": null, "forum": "guEuB3FPcd", "replyto": "b13emmFhpZs", "invitation": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment", "content": {"title": "Anything more we can address? ", "comment": "As the discussion period ends soon, we were wondering if there are any more concerns we could address to help increase your score of our work?"}, "signatures": ["ICLR.cc/2021/Conference/Paper3482/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AlgebraNets", "authorids": ["~Jordan_Hoffmann1", "~Simon_Schmitt1", "~Simon_Osindero1", "~Karen_Simonyan1", "~Erich_Elsen1"], "authors": ["Jordan Hoffmann", "Simon Schmitt", "Simon Osindero", "Karen Simonyan", "Erich Elsen"], "keywords": ["Sparsity", "Pruning", "Efficiency", "Mathematics"], "abstract": "Neural networks have historically been built layerwise from the set of functions in ${f: \\mathbb{R}^n \\to \\mathbb{R}^m }$, i.e. with activations and weights/parameters represented by real numbers, $\\mathbb{R}$. Our work considers a richer set of objects for activations and weights, and undertakes a comprehensive study of alternative algebras as number representations by studying their performance on two challenging problems: large-scale image classification using the ImageNet dataset and language modeling using the enwiki8 and WikiText-103 datasets. We denote this broader class of models as AlgebraNets. Our findings indicate that the conclusions of prior work, which explored neural networks constructed from $\\mathbb{C}$ (complex numbers) and $\\mathbb{H}$ (quaternions) on smaller datasets, do not always transfer to these challenging settings.  However, our results demonstrate that there are alternative algebras which deliver better parameter and computational efficiency compared with $\\mathbb{R}$. We consider $\\mathbb{C}$, $\\mathbb{H}$, $M_{2}(\\mathbb{R})$ (the set of $2\\times2$ real-valued matrices), $M_{2}(\\mathbb{C})$, $M_{3}(\\mathbb{R})$, $M_{4}(\\mathbb{R})$, dual numbers and the $\\mathbb{R}^3$ cross product.  Additionally, we note that multiplication in these algebras has higher compute density than real multiplication, a useful property in situations with inherently limited parameter reuse such as auto-regressive inference and sparse neural networks. We therefore investigate how to induce sparsity within AlgebraNets. We hope that our strong results on large-scale, practical benchmarks will spur further exploration of these unconventional architectures which challenge the default choice of using real numbers for neural network weights and activations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hoffmann|algebranets", "one-sentence_summary": "We investigate a wide range of alternatives to real-valued weights in neural networks and find promising alternatives. ", "supplementary_material": "/attachment/2873be7fd30024a18ed0260327bb37527ed3d60f.zip", "pdf": "/pdf/a29db3f04aadec5e862ecd2dc316a628e12fd6f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iaVGYcsIw", "_bibtex": "@misc{\nhoffmann2021algebranets,\ntitle={AlgebraNets},\nauthor={Jordan Hoffmann and Simon Schmitt and Simon Osindero and Karen Simonyan and Erich Elsen},\nyear={2021},\nurl={https://openreview.net/forum?id=guEuB3FPcd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "guEuB3FPcd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3482/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3482/Authors|ICLR.cc/2021/Conference/Paper3482/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837068, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment"}}}, {"id": "ULa8RnY-aUC", "original": null, "number": 14, "cdate": 1605565805533, "ddate": null, "tcdate": 1605565805533, "tmdate": 1605565805533, "tddate": null, "forum": "guEuB3FPcd", "replyto": "_sCd8EyDKe", "invitation": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment", "content": {"title": "Re: concern of new insights", "comment": "There seems to be quite ab bit of discussion regarding the title, and if calling it \"AlegbraNets\" might have overpromised and underdelivered; I understand the other reviewers' concerns. However, upon re-examining the paper, I believe there may be enough merit to warrant acceptance. I agree with other reviewers' that this work may not be entirely novel (which I point out in my review as well); however, I see this is as a valuable contribution for the following reasons:\n\n1. To my knowledge, one of the first publications to empirically show the value of other algebras on established datasets (ImageNet, enwiki-8) and respective near-SOTA model architectures such as transformer-xl. Deep complex networks motivates the line of research, but I would consider CIFAR-10/100 to be more of toy examples. These results, if broadly disseminated, has the potential to encourage subsequent contributions. I see the leap from examples to larger-scale results as impactful. If there are other publications that establish similar results, please share and I can certainly be convinced otherwise.\n\n2. On a similar note, a search (and exploration) of algebras beyond complex numbers is valuable and their recommendation of using 2x2 matrix rings as optimal under a computational notion is promising.\n\n3. Upon seeing the authors' response on memory footprint, I do see there is a tradeoff between the computation and memory footprint, making it a design choice. Something that would strengthen the paper a bit  more is if they can define a cost model based on standard hardware (e.g. GPU/TPU) and show how using a 2x2 matrix algebra is conclusively better than real numbers.\n\nOverall, my vote of confidence is for the empirical results on widely adopted convolutional models, transformer-xl etc., the ease of usage, etc. This could be one of the papers that spur the paradigm shift from real numbers to other algebras in the SOTA spectrum of models. Unless there are prior results I am unaware of that have shown similar results\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3482/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AlgebraNets", "authorids": ["~Jordan_Hoffmann1", "~Simon_Schmitt1", "~Simon_Osindero1", "~Karen_Simonyan1", "~Erich_Elsen1"], "authors": ["Jordan Hoffmann", "Simon Schmitt", "Simon Osindero", "Karen Simonyan", "Erich Elsen"], "keywords": ["Sparsity", "Pruning", "Efficiency", "Mathematics"], "abstract": "Neural networks have historically been built layerwise from the set of functions in ${f: \\mathbb{R}^n \\to \\mathbb{R}^m }$, i.e. with activations and weights/parameters represented by real numbers, $\\mathbb{R}$. Our work considers a richer set of objects for activations and weights, and undertakes a comprehensive study of alternative algebras as number representations by studying their performance on two challenging problems: large-scale image classification using the ImageNet dataset and language modeling using the enwiki8 and WikiText-103 datasets. We denote this broader class of models as AlgebraNets. Our findings indicate that the conclusions of prior work, which explored neural networks constructed from $\\mathbb{C}$ (complex numbers) and $\\mathbb{H}$ (quaternions) on smaller datasets, do not always transfer to these challenging settings.  However, our results demonstrate that there are alternative algebras which deliver better parameter and computational efficiency compared with $\\mathbb{R}$. We consider $\\mathbb{C}$, $\\mathbb{H}$, $M_{2}(\\mathbb{R})$ (the set of $2\\times2$ real-valued matrices), $M_{2}(\\mathbb{C})$, $M_{3}(\\mathbb{R})$, $M_{4}(\\mathbb{R})$, dual numbers and the $\\mathbb{R}^3$ cross product.  Additionally, we note that multiplication in these algebras has higher compute density than real multiplication, a useful property in situations with inherently limited parameter reuse such as auto-regressive inference and sparse neural networks. We therefore investigate how to induce sparsity within AlgebraNets. We hope that our strong results on large-scale, practical benchmarks will spur further exploration of these unconventional architectures which challenge the default choice of using real numbers for neural network weights and activations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hoffmann|algebranets", "one-sentence_summary": "We investigate a wide range of alternatives to real-valued weights in neural networks and find promising alternatives. ", "supplementary_material": "/attachment/2873be7fd30024a18ed0260327bb37527ed3d60f.zip", "pdf": "/pdf/a29db3f04aadec5e862ecd2dc316a628e12fd6f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iaVGYcsIw", "_bibtex": "@misc{\nhoffmann2021algebranets,\ntitle={AlgebraNets},\nauthor={Jordan Hoffmann and Simon Schmitt and Simon Osindero and Karen Simonyan and Erich Elsen},\nyear={2021},\nurl={https://openreview.net/forum?id=guEuB3FPcd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "guEuB3FPcd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3482/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3482/Authors|ICLR.cc/2021/Conference/Paper3482/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837068, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment"}}}, {"id": "PEdsqqhMCw", "original": null, "number": 13, "cdate": 1605547065403, "ddate": null, "tcdate": 1605547065403, "tmdate": 1605547065403, "tddate": null, "forum": "guEuB3FPcd", "replyto": "guEuB3FPcd", "invitation": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment", "content": {"title": "Updated Version", "comment": "We would like to thank the reviewers for the comments. We have updated the manuscript. We have moved the figures such that they appear closer to where they are referenced. We updated the citation style, as asked for. We also standardised the presentation of citations. In the supplement, we added a discussion of the change in activation memory. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3482/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AlgebraNets", "authorids": ["~Jordan_Hoffmann1", "~Simon_Schmitt1", "~Simon_Osindero1", "~Karen_Simonyan1", "~Erich_Elsen1"], "authors": ["Jordan Hoffmann", "Simon Schmitt", "Simon Osindero", "Karen Simonyan", "Erich Elsen"], "keywords": ["Sparsity", "Pruning", "Efficiency", "Mathematics"], "abstract": "Neural networks have historically been built layerwise from the set of functions in ${f: \\mathbb{R}^n \\to \\mathbb{R}^m }$, i.e. with activations and weights/parameters represented by real numbers, $\\mathbb{R}$. Our work considers a richer set of objects for activations and weights, and undertakes a comprehensive study of alternative algebras as number representations by studying their performance on two challenging problems: large-scale image classification using the ImageNet dataset and language modeling using the enwiki8 and WikiText-103 datasets. We denote this broader class of models as AlgebraNets. Our findings indicate that the conclusions of prior work, which explored neural networks constructed from $\\mathbb{C}$ (complex numbers) and $\\mathbb{H}$ (quaternions) on smaller datasets, do not always transfer to these challenging settings.  However, our results demonstrate that there are alternative algebras which deliver better parameter and computational efficiency compared with $\\mathbb{R}$. We consider $\\mathbb{C}$, $\\mathbb{H}$, $M_{2}(\\mathbb{R})$ (the set of $2\\times2$ real-valued matrices), $M_{2}(\\mathbb{C})$, $M_{3}(\\mathbb{R})$, $M_{4}(\\mathbb{R})$, dual numbers and the $\\mathbb{R}^3$ cross product.  Additionally, we note that multiplication in these algebras has higher compute density than real multiplication, a useful property in situations with inherently limited parameter reuse such as auto-regressive inference and sparse neural networks. We therefore investigate how to induce sparsity within AlgebraNets. We hope that our strong results on large-scale, practical benchmarks will spur further exploration of these unconventional architectures which challenge the default choice of using real numbers for neural network weights and activations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hoffmann|algebranets", "one-sentence_summary": "We investigate a wide range of alternatives to real-valued weights in neural networks and find promising alternatives. ", "supplementary_material": "/attachment/2873be7fd30024a18ed0260327bb37527ed3d60f.zip", "pdf": "/pdf/a29db3f04aadec5e862ecd2dc316a628e12fd6f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iaVGYcsIw", "_bibtex": "@misc{\nhoffmann2021algebranets,\ntitle={AlgebraNets},\nauthor={Jordan Hoffmann and Simon Schmitt and Simon Osindero and Karen Simonyan and Erich Elsen},\nyear={2021},\nurl={https://openreview.net/forum?id=guEuB3FPcd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "guEuB3FPcd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3482/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3482/Authors|ICLR.cc/2021/Conference/Paper3482/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837068, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment"}}}, {"id": "9PElhsrS11z", "original": null, "number": 12, "cdate": 1605532738740, "ddate": null, "tcdate": 1605532738740, "tmdate": 1605532738740, "tddate": null, "forum": "guEuB3FPcd", "replyto": "KHqXw_Vu8sh", "invitation": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment", "content": {"title": "Response", "comment": "The reviewer seems to believe that using the algebras in this paper would be a large engineering challenge.   We note that it is fairly simple to implement these networks in Tensorflow or Pytorch, just as we\u2019ve shown in the appendix for JAX.  This does leave some performance on the table, but this is a natural progression for almost all new ideas. First it is demonstrated that they work, then later maximum performance implementations are created.  The initial implementations of real-valued convolutions were far from optimal as one example.\n\nIt is also odd to claim that \u201csome improvements on well-studied datasets are not enough\u201d when probably a majority of all papers accepted will do exactly this \u2014 show improvements on well-studied datasets.  Indeed, one should be skeptical of claims on poorly studied datasets, it is far harder to show gains on well studied datasets.  We strongly disagree with the implication that because \"we already have very effective neural networks\", research on _more_ effective techniques is not necessary. \n\nThe reviewer repeatedly claims that \u201cImageNet \u2026 is not convincing enough\u201d and \u201cexperiments and claims are from two tasks (on two datasets) which are not enough\u201d, but then when we ask directly for which additional tasks would be useful to include, they instead say \u201cThe concern is not \u2018What tasks would you like to see?\u2019 but why engineers need such a switch.\u201d  If the reviewer could clarify their position, the authors would find it most helpful.  And for what it\u2019s worth, we would like to clarify that we believe we have three tasks and four datasets.  Image classification, character level and word level language modeling are the tasks, and ImageNet, CIFAR-10 (appendix), enwik8 and WikiText-103 are the datasets"}, "signatures": ["ICLR.cc/2021/Conference/Paper3482/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AlgebraNets", "authorids": ["~Jordan_Hoffmann1", "~Simon_Schmitt1", "~Simon_Osindero1", "~Karen_Simonyan1", "~Erich_Elsen1"], "authors": ["Jordan Hoffmann", "Simon Schmitt", "Simon Osindero", "Karen Simonyan", "Erich Elsen"], "keywords": ["Sparsity", "Pruning", "Efficiency", "Mathematics"], "abstract": "Neural networks have historically been built layerwise from the set of functions in ${f: \\mathbb{R}^n \\to \\mathbb{R}^m }$, i.e. with activations and weights/parameters represented by real numbers, $\\mathbb{R}$. Our work considers a richer set of objects for activations and weights, and undertakes a comprehensive study of alternative algebras as number representations by studying their performance on two challenging problems: large-scale image classification using the ImageNet dataset and language modeling using the enwiki8 and WikiText-103 datasets. We denote this broader class of models as AlgebraNets. Our findings indicate that the conclusions of prior work, which explored neural networks constructed from $\\mathbb{C}$ (complex numbers) and $\\mathbb{H}$ (quaternions) on smaller datasets, do not always transfer to these challenging settings.  However, our results demonstrate that there are alternative algebras which deliver better parameter and computational efficiency compared with $\\mathbb{R}$. We consider $\\mathbb{C}$, $\\mathbb{H}$, $M_{2}(\\mathbb{R})$ (the set of $2\\times2$ real-valued matrices), $M_{2}(\\mathbb{C})$, $M_{3}(\\mathbb{R})$, $M_{4}(\\mathbb{R})$, dual numbers and the $\\mathbb{R}^3$ cross product.  Additionally, we note that multiplication in these algebras has higher compute density than real multiplication, a useful property in situations with inherently limited parameter reuse such as auto-regressive inference and sparse neural networks. We therefore investigate how to induce sparsity within AlgebraNets. We hope that our strong results on large-scale, practical benchmarks will spur further exploration of these unconventional architectures which challenge the default choice of using real numbers for neural network weights and activations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hoffmann|algebranets", "one-sentence_summary": "We investigate a wide range of alternatives to real-valued weights in neural networks and find promising alternatives. ", "supplementary_material": "/attachment/2873be7fd30024a18ed0260327bb37527ed3d60f.zip", "pdf": "/pdf/a29db3f04aadec5e862ecd2dc316a628e12fd6f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iaVGYcsIw", "_bibtex": "@misc{\nhoffmann2021algebranets,\ntitle={AlgebraNets},\nauthor={Jordan Hoffmann and Simon Schmitt and Simon Osindero and Karen Simonyan and Erich Elsen},\nyear={2021},\nurl={https://openreview.net/forum?id=guEuB3FPcd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "guEuB3FPcd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3482/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3482/Authors|ICLR.cc/2021/Conference/Paper3482/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837068, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment"}}}, {"id": "FxUQ8wGlRce", "original": null, "number": 4, "cdate": 1605374551316, "ddate": null, "tcdate": 1605374551316, "tmdate": 1605482699658, "tddate": null, "forum": "guEuB3FPcd", "replyto": "_sCd8EyDKe", "invitation": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment", "content": {"title": "We feel there are many new insights and contributions.", "comment": "Deep Complex Networks is an interesting paper that highlighted some of the potential of investigating these alternate algebras. However, they only investigate a single algebra (complex numbers) and do not recognize the increased compute density of algebra nor explore pruning or sparsity inducing methods that would greatly benefit from this increased compute density on modern hardware. Additionally, while their proposal is parameter efficient, it is not FLOP efficient due to the computationally expensive whitening procedure. \n\nWe test a large number of algebras and find an algebra (2x2 matrix rings) that actually work better than anything that has previously been looked at, in terms of performance per FLOP. Additionally, we show that we do not need some of the complexities discussed in earlier work exploring these algebras: specific initialisation schemes, for example, do not seem to matter as much.\n\nLastly, we find some crucial differences in terms of the efficacy of these algebras in testing at scale. Using ImageNet instead of CIFAR-10 one does not recover the same performance per-parameter. To further test this regime, we also use the more computationally efficient MobileNet. Finally, we test the most promising algebras on a variety of different domains as well.\n\nDeep Complex Networks was an exciting work but we think we have made a series of new contributions that are important to anyone interested in complex networks or other algebras.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3482/Authors"], "readers": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs", "ICLR.cc/2021/Conference/Paper3482/Reviewers", "ICLR.cc/2021/Conference/Paper3482/Authors", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AlgebraNets", "authorids": ["~Jordan_Hoffmann1", "~Simon_Schmitt1", "~Simon_Osindero1", "~Karen_Simonyan1", "~Erich_Elsen1"], "authors": ["Jordan Hoffmann", "Simon Schmitt", "Simon Osindero", "Karen Simonyan", "Erich Elsen"], "keywords": ["Sparsity", "Pruning", "Efficiency", "Mathematics"], "abstract": "Neural networks have historically been built layerwise from the set of functions in ${f: \\mathbb{R}^n \\to \\mathbb{R}^m }$, i.e. with activations and weights/parameters represented by real numbers, $\\mathbb{R}$. Our work considers a richer set of objects for activations and weights, and undertakes a comprehensive study of alternative algebras as number representations by studying their performance on two challenging problems: large-scale image classification using the ImageNet dataset and language modeling using the enwiki8 and WikiText-103 datasets. We denote this broader class of models as AlgebraNets. Our findings indicate that the conclusions of prior work, which explored neural networks constructed from $\\mathbb{C}$ (complex numbers) and $\\mathbb{H}$ (quaternions) on smaller datasets, do not always transfer to these challenging settings.  However, our results demonstrate that there are alternative algebras which deliver better parameter and computational efficiency compared with $\\mathbb{R}$. We consider $\\mathbb{C}$, $\\mathbb{H}$, $M_{2}(\\mathbb{R})$ (the set of $2\\times2$ real-valued matrices), $M_{2}(\\mathbb{C})$, $M_{3}(\\mathbb{R})$, $M_{4}(\\mathbb{R})$, dual numbers and the $\\mathbb{R}^3$ cross product.  Additionally, we note that multiplication in these algebras has higher compute density than real multiplication, a useful property in situations with inherently limited parameter reuse such as auto-regressive inference and sparse neural networks. We therefore investigate how to induce sparsity within AlgebraNets. We hope that our strong results on large-scale, practical benchmarks will spur further exploration of these unconventional architectures which challenge the default choice of using real numbers for neural network weights and activations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hoffmann|algebranets", "one-sentence_summary": "We investigate a wide range of alternatives to real-valued weights in neural networks and find promising alternatives. ", "supplementary_material": "/attachment/2873be7fd30024a18ed0260327bb37527ed3d60f.zip", "pdf": "/pdf/a29db3f04aadec5e862ecd2dc316a628e12fd6f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iaVGYcsIw", "_bibtex": "@misc{\nhoffmann2021algebranets,\ntitle={AlgebraNets},\nauthor={Jordan Hoffmann and Simon Schmitt and Simon Osindero and Karen Simonyan and Erich Elsen},\nyear={2021},\nurl={https://openreview.net/forum?id=guEuB3FPcd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "guEuB3FPcd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3482/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3482/Authors|ICLR.cc/2021/Conference/Paper3482/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837068, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment"}}}, {"id": "AHl56e811mE", "original": null, "number": 11, "cdate": 1605393806007, "ddate": null, "tcdate": 1605393806007, "tmdate": 1605393806007, "tddate": null, "forum": "guEuB3FPcd", "replyto": "fNwPsjdj4Kr", "invitation": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment", "content": {"title": "Response to Item 3", "comment": "Sorry for not responding to the third point-- in Figure 1, we are showing performance per parameter (left) and performance per FLOP (right). This is important because while an algebra may be able to reduce the parameter counts, there may be an increased FLOP cost, especially due to a procedure like whitening. We note that in earlier work (Deep Complex Networks and Deep Quaternion Networks) the cost of whitening was largely ignored, since results focused on parameter efficiency. In terms of performance-per-parameter, many algebras are actually more performant than the baseline. However, in terms of FLOPs, only M_2(R) is able to match the baseline performance. It is important to note, however, that these algebras have added benefits: specifically, the higher compute density. Aside from being important for sparsity, with proper kernels (or even hardware!) the performance gap may be negligible."}, "signatures": ["ICLR.cc/2021/Conference/Paper3482/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AlgebraNets", "authorids": ["~Jordan_Hoffmann1", "~Simon_Schmitt1", "~Simon_Osindero1", "~Karen_Simonyan1", "~Erich_Elsen1"], "authors": ["Jordan Hoffmann", "Simon Schmitt", "Simon Osindero", "Karen Simonyan", "Erich Elsen"], "keywords": ["Sparsity", "Pruning", "Efficiency", "Mathematics"], "abstract": "Neural networks have historically been built layerwise from the set of functions in ${f: \\mathbb{R}^n \\to \\mathbb{R}^m }$, i.e. with activations and weights/parameters represented by real numbers, $\\mathbb{R}$. Our work considers a richer set of objects for activations and weights, and undertakes a comprehensive study of alternative algebras as number representations by studying their performance on two challenging problems: large-scale image classification using the ImageNet dataset and language modeling using the enwiki8 and WikiText-103 datasets. We denote this broader class of models as AlgebraNets. Our findings indicate that the conclusions of prior work, which explored neural networks constructed from $\\mathbb{C}$ (complex numbers) and $\\mathbb{H}$ (quaternions) on smaller datasets, do not always transfer to these challenging settings.  However, our results demonstrate that there are alternative algebras which deliver better parameter and computational efficiency compared with $\\mathbb{R}$. We consider $\\mathbb{C}$, $\\mathbb{H}$, $M_{2}(\\mathbb{R})$ (the set of $2\\times2$ real-valued matrices), $M_{2}(\\mathbb{C})$, $M_{3}(\\mathbb{R})$, $M_{4}(\\mathbb{R})$, dual numbers and the $\\mathbb{R}^3$ cross product.  Additionally, we note that multiplication in these algebras has higher compute density than real multiplication, a useful property in situations with inherently limited parameter reuse such as auto-regressive inference and sparse neural networks. We therefore investigate how to induce sparsity within AlgebraNets. We hope that our strong results on large-scale, practical benchmarks will spur further exploration of these unconventional architectures which challenge the default choice of using real numbers for neural network weights and activations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hoffmann|algebranets", "one-sentence_summary": "We investigate a wide range of alternatives to real-valued weights in neural networks and find promising alternatives. ", "supplementary_material": "/attachment/2873be7fd30024a18ed0260327bb37527ed3d60f.zip", "pdf": "/pdf/a29db3f04aadec5e862ecd2dc316a628e12fd6f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iaVGYcsIw", "_bibtex": "@misc{\nhoffmann2021algebranets,\ntitle={AlgebraNets},\nauthor={Jordan Hoffmann and Simon Schmitt and Simon Osindero and Karen Simonyan and Erich Elsen},\nyear={2021},\nurl={https://openreview.net/forum?id=guEuB3FPcd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "guEuB3FPcd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3482/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3482/Authors|ICLR.cc/2021/Conference/Paper3482/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837068, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment"}}}, {"id": "fNwPsjdj4Kr", "original": null, "number": 10, "cdate": 1605391349252, "ddate": null, "tcdate": 1605391349252, "tmdate": 1605391349252, "tddate": null, "forum": "guEuB3FPcd", "replyto": "jwaCE4SlSJG", "invitation": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment", "content": {"title": "Please clarify my question ", "comment": "Thanks for your response. \n\nSure, there are. Because you don't respond to my above item 3. If the performance could not be improved, what is the meaning of your complex model? "}, "signatures": ["ICLR.cc/2021/Conference/Paper3482/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AlgebraNets", "authorids": ["~Jordan_Hoffmann1", "~Simon_Schmitt1", "~Simon_Osindero1", "~Karen_Simonyan1", "~Erich_Elsen1"], "authors": ["Jordan Hoffmann", "Simon Schmitt", "Simon Osindero", "Karen Simonyan", "Erich Elsen"], "keywords": ["Sparsity", "Pruning", "Efficiency", "Mathematics"], "abstract": "Neural networks have historically been built layerwise from the set of functions in ${f: \\mathbb{R}^n \\to \\mathbb{R}^m }$, i.e. with activations and weights/parameters represented by real numbers, $\\mathbb{R}$. Our work considers a richer set of objects for activations and weights, and undertakes a comprehensive study of alternative algebras as number representations by studying their performance on two challenging problems: large-scale image classification using the ImageNet dataset and language modeling using the enwiki8 and WikiText-103 datasets. We denote this broader class of models as AlgebraNets. Our findings indicate that the conclusions of prior work, which explored neural networks constructed from $\\mathbb{C}$ (complex numbers) and $\\mathbb{H}$ (quaternions) on smaller datasets, do not always transfer to these challenging settings.  However, our results demonstrate that there are alternative algebras which deliver better parameter and computational efficiency compared with $\\mathbb{R}$. We consider $\\mathbb{C}$, $\\mathbb{H}$, $M_{2}(\\mathbb{R})$ (the set of $2\\times2$ real-valued matrices), $M_{2}(\\mathbb{C})$, $M_{3}(\\mathbb{R})$, $M_{4}(\\mathbb{R})$, dual numbers and the $\\mathbb{R}^3$ cross product.  Additionally, we note that multiplication in these algebras has higher compute density than real multiplication, a useful property in situations with inherently limited parameter reuse such as auto-regressive inference and sparse neural networks. We therefore investigate how to induce sparsity within AlgebraNets. We hope that our strong results on large-scale, practical benchmarks will spur further exploration of these unconventional architectures which challenge the default choice of using real numbers for neural network weights and activations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hoffmann|algebranets", "one-sentence_summary": "We investigate a wide range of alternatives to real-valued weights in neural networks and find promising alternatives. ", "supplementary_material": "/attachment/2873be7fd30024a18ed0260327bb37527ed3d60f.zip", "pdf": "/pdf/a29db3f04aadec5e862ecd2dc316a628e12fd6f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iaVGYcsIw", "_bibtex": "@misc{\nhoffmann2021algebranets,\ntitle={AlgebraNets},\nauthor={Jordan Hoffmann and Simon Schmitt and Simon Osindero and Karen Simonyan and Erich Elsen},\nyear={2021},\nurl={https://openreview.net/forum?id=guEuB3FPcd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "guEuB3FPcd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3482/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3482/Authors|ICLR.cc/2021/Conference/Paper3482/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837068, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment"}}}, {"id": "KHqXw_Vu8sh", "original": null, "number": 7, "cdate": 1605376992765, "ddate": null, "tcdate": 1605376992765, "tmdate": 1605380962483, "tddate": null, "forum": "guEuB3FPcd", "replyto": "P9MLsJMrRQW", "invitation": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment", "content": {"title": "Still huge title without convincing contributions", "comment": "1. \"Do you mean why are more efficient neural networks necessary? Or why are different algebras necessary for more efficient networks? \"\n     The target project aims to improve efficiency by complex algebras, I am interested why go for it?  yes, there is performance gain, also there is overhead, and not compatible in TensorFlow/PyTorch (as is used by the wide community).  Why it is necessary to go for it?  It is OK to be a small group of researchers or a particular industrial product.   As in the following points, I think claiming ImgeNet should go for it is not convincing enough.\n\n2. Do not be \"surprised that you do not think ImageNet is not a suitable task for demonstrating this method: it is a challenging task and is widely used as a way to benchmark state-of-the-art methods. We also have results on enwik8 and wikitext-103 language modeling\"\n     The reasons are we already have very effective neural networks. I do not see clear reason why we urgent engineers switch to much more complex algebras.  If the reviewers vote for an acceptance of \"AlgebraNets\" to ICLR, some improvements on well-studied datasets are not enough to justify why ICLR accepts such a big title.\n     The concern is not \"What tasks would you like to see?\"  but why engineers need such a switch.\n\n    Another very direct question would be: the compared schemes in ImageNet were targeting at improving accuracy, now your results claiming better computation efficiency. Actually, more fair comparison would be those compression schemes (EfficientNets, MobileNets (included), complex-valued nets), right?   The current presentation of the evaluation methodology is not convincing.\n\n2. \"We feel there are a series of important contributions in the work: a.)   b) c) and d)\" \n     Those claims are interesting, but are far from a support of \"AlgebraNets\".  The experiments and claims are from two tasks (on two datasets), which are not enough. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3482/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AlgebraNets", "authorids": ["~Jordan_Hoffmann1", "~Simon_Schmitt1", "~Simon_Osindero1", "~Karen_Simonyan1", "~Erich_Elsen1"], "authors": ["Jordan Hoffmann", "Simon Schmitt", "Simon Osindero", "Karen Simonyan", "Erich Elsen"], "keywords": ["Sparsity", "Pruning", "Efficiency", "Mathematics"], "abstract": "Neural networks have historically been built layerwise from the set of functions in ${f: \\mathbb{R}^n \\to \\mathbb{R}^m }$, i.e. with activations and weights/parameters represented by real numbers, $\\mathbb{R}$. Our work considers a richer set of objects for activations and weights, and undertakes a comprehensive study of alternative algebras as number representations by studying their performance on two challenging problems: large-scale image classification using the ImageNet dataset and language modeling using the enwiki8 and WikiText-103 datasets. We denote this broader class of models as AlgebraNets. Our findings indicate that the conclusions of prior work, which explored neural networks constructed from $\\mathbb{C}$ (complex numbers) and $\\mathbb{H}$ (quaternions) on smaller datasets, do not always transfer to these challenging settings.  However, our results demonstrate that there are alternative algebras which deliver better parameter and computational efficiency compared with $\\mathbb{R}$. We consider $\\mathbb{C}$, $\\mathbb{H}$, $M_{2}(\\mathbb{R})$ (the set of $2\\times2$ real-valued matrices), $M_{2}(\\mathbb{C})$, $M_{3}(\\mathbb{R})$, $M_{4}(\\mathbb{R})$, dual numbers and the $\\mathbb{R}^3$ cross product.  Additionally, we note that multiplication in these algebras has higher compute density than real multiplication, a useful property in situations with inherently limited parameter reuse such as auto-regressive inference and sparse neural networks. We therefore investigate how to induce sparsity within AlgebraNets. We hope that our strong results on large-scale, practical benchmarks will spur further exploration of these unconventional architectures which challenge the default choice of using real numbers for neural network weights and activations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hoffmann|algebranets", "one-sentence_summary": "We investigate a wide range of alternatives to real-valued weights in neural networks and find promising alternatives. ", "supplementary_material": "/attachment/2873be7fd30024a18ed0260327bb37527ed3d60f.zip", "pdf": "/pdf/a29db3f04aadec5e862ecd2dc316a628e12fd6f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iaVGYcsIw", "_bibtex": "@misc{\nhoffmann2021algebranets,\ntitle={AlgebraNets},\nauthor={Jordan Hoffmann and Simon Schmitt and Simon Osindero and Karen Simonyan and Erich Elsen},\nyear={2021},\nurl={https://openreview.net/forum?id=guEuB3FPcd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "guEuB3FPcd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3482/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3482/Authors|ICLR.cc/2021/Conference/Paper3482/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837068, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment"}}}, {"id": "_sCd8EyDKe", "original": null, "number": 3, "cdate": 1605314566723, "ddate": null, "tcdate": 1605314566723, "tmdate": 1605379410515, "tddate": null, "forum": "guEuB3FPcd", "replyto": "k3b56qlIUhz", "invitation": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment", "content": {"title": "A concern of any new insights over \"Deep Complex Networks\" ICLR 2018.", "comment": "Dear Reviewer,\n   I have a concern: whether this work provides any new insights over \"Deep Complex Networks\" ICLR 2018. \nhttps://openreview.net/forum?id=H1T2hmZAb\n\n   I am not sure whether there is enough value to support this work appear in a top AI conference.  Would like to hear your opinions."}, "signatures": ["ICLR.cc/2021/Conference/Paper3482/AnonReviewer3"], "readers": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs", "ICLR.cc/2021/Conference/Paper3482/Reviewers", "ICLR.cc/2021/Conference/Paper3482/Authors", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AlgebraNets", "authorids": ["~Jordan_Hoffmann1", "~Simon_Schmitt1", "~Simon_Osindero1", "~Karen_Simonyan1", "~Erich_Elsen1"], "authors": ["Jordan Hoffmann", "Simon Schmitt", "Simon Osindero", "Karen Simonyan", "Erich Elsen"], "keywords": ["Sparsity", "Pruning", "Efficiency", "Mathematics"], "abstract": "Neural networks have historically been built layerwise from the set of functions in ${f: \\mathbb{R}^n \\to \\mathbb{R}^m }$, i.e. with activations and weights/parameters represented by real numbers, $\\mathbb{R}$. Our work considers a richer set of objects for activations and weights, and undertakes a comprehensive study of alternative algebras as number representations by studying their performance on two challenging problems: large-scale image classification using the ImageNet dataset and language modeling using the enwiki8 and WikiText-103 datasets. We denote this broader class of models as AlgebraNets. Our findings indicate that the conclusions of prior work, which explored neural networks constructed from $\\mathbb{C}$ (complex numbers) and $\\mathbb{H}$ (quaternions) on smaller datasets, do not always transfer to these challenging settings.  However, our results demonstrate that there are alternative algebras which deliver better parameter and computational efficiency compared with $\\mathbb{R}$. We consider $\\mathbb{C}$, $\\mathbb{H}$, $M_{2}(\\mathbb{R})$ (the set of $2\\times2$ real-valued matrices), $M_{2}(\\mathbb{C})$, $M_{3}(\\mathbb{R})$, $M_{4}(\\mathbb{R})$, dual numbers and the $\\mathbb{R}^3$ cross product.  Additionally, we note that multiplication in these algebras has higher compute density than real multiplication, a useful property in situations with inherently limited parameter reuse such as auto-regressive inference and sparse neural networks. We therefore investigate how to induce sparsity within AlgebraNets. We hope that our strong results on large-scale, practical benchmarks will spur further exploration of these unconventional architectures which challenge the default choice of using real numbers for neural network weights and activations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hoffmann|algebranets", "one-sentence_summary": "We investigate a wide range of alternatives to real-valued weights in neural networks and find promising alternatives. ", "supplementary_material": "/attachment/2873be7fd30024a18ed0260327bb37527ed3d60f.zip", "pdf": "/pdf/a29db3f04aadec5e862ecd2dc316a628e12fd6f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iaVGYcsIw", "_bibtex": "@misc{\nhoffmann2021algebranets,\ntitle={AlgebraNets},\nauthor={Jordan Hoffmann and Simon Schmitt and Simon Osindero and Karen Simonyan and Erich Elsen},\nyear={2021},\nurl={https://openreview.net/forum?id=guEuB3FPcd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "guEuB3FPcd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3482/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3482/Authors|ICLR.cc/2021/Conference/Paper3482/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837068, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment"}}}, {"id": "jwaCE4SlSJG", "original": null, "number": 9, "cdate": 1605377224178, "ddate": null, "tcdate": 1605377224178, "tmdate": 1605377224178, "tddate": null, "forum": "guEuB3FPcd", "replyto": "b13emmFhpZs", "invitation": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment", "content": {"title": "Thank you for the review", "comment": "Thank you for your careful read of our manuscript. We appreciate the comments very much, and will update the paper with the changes to the references and try to better stagger the introduction and reference to the different figures in the manuscript. \n\nAre there technical issues we can address/clarify/improve that would help improve the perception of our work? \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3482/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AlgebraNets", "authorids": ["~Jordan_Hoffmann1", "~Simon_Schmitt1", "~Simon_Osindero1", "~Karen_Simonyan1", "~Erich_Elsen1"], "authors": ["Jordan Hoffmann", "Simon Schmitt", "Simon Osindero", "Karen Simonyan", "Erich Elsen"], "keywords": ["Sparsity", "Pruning", "Efficiency", "Mathematics"], "abstract": "Neural networks have historically been built layerwise from the set of functions in ${f: \\mathbb{R}^n \\to \\mathbb{R}^m }$, i.e. with activations and weights/parameters represented by real numbers, $\\mathbb{R}$. Our work considers a richer set of objects for activations and weights, and undertakes a comprehensive study of alternative algebras as number representations by studying their performance on two challenging problems: large-scale image classification using the ImageNet dataset and language modeling using the enwiki8 and WikiText-103 datasets. We denote this broader class of models as AlgebraNets. Our findings indicate that the conclusions of prior work, which explored neural networks constructed from $\\mathbb{C}$ (complex numbers) and $\\mathbb{H}$ (quaternions) on smaller datasets, do not always transfer to these challenging settings.  However, our results demonstrate that there are alternative algebras which deliver better parameter and computational efficiency compared with $\\mathbb{R}$. We consider $\\mathbb{C}$, $\\mathbb{H}$, $M_{2}(\\mathbb{R})$ (the set of $2\\times2$ real-valued matrices), $M_{2}(\\mathbb{C})$, $M_{3}(\\mathbb{R})$, $M_{4}(\\mathbb{R})$, dual numbers and the $\\mathbb{R}^3$ cross product.  Additionally, we note that multiplication in these algebras has higher compute density than real multiplication, a useful property in situations with inherently limited parameter reuse such as auto-regressive inference and sparse neural networks. We therefore investigate how to induce sparsity within AlgebraNets. We hope that our strong results on large-scale, practical benchmarks will spur further exploration of these unconventional architectures which challenge the default choice of using real numbers for neural network weights and activations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hoffmann|algebranets", "one-sentence_summary": "We investigate a wide range of alternatives to real-valued weights in neural networks and find promising alternatives. ", "supplementary_material": "/attachment/2873be7fd30024a18ed0260327bb37527ed3d60f.zip", "pdf": "/pdf/a29db3f04aadec5e862ecd2dc316a628e12fd6f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iaVGYcsIw", "_bibtex": "@misc{\nhoffmann2021algebranets,\ntitle={AlgebraNets},\nauthor={Jordan Hoffmann and Simon Schmitt and Simon Osindero and Karen Simonyan and Erich Elsen},\nyear={2021},\nurl={https://openreview.net/forum?id=guEuB3FPcd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "guEuB3FPcd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3482/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3482/Authors|ICLR.cc/2021/Conference/Paper3482/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837068, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment"}}}, {"id": "EQScenKpK1t", "original": null, "number": 8, "cdate": 1605377037259, "ddate": null, "tcdate": 1605377037259, "tmdate": 1605377037259, "tddate": null, "forum": "guEuB3FPcd", "replyto": "k3b56qlIUhz", "invitation": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment", "content": {"title": "Thank you for your review", "comment": "Thanks for the review. We have tried to address your section of cons below, and will update the text in the next few days to reflect these changes. We, of course, thank you for listing the pros of our work, we agree that the exploration of alternate algebras is both useful and impactful! \n\n> The authors motivate this work with computational efficiency; however, I did not find any discussion or comments on the total memory footprint. Do any of the algebras require us to keep track of partial computations/intermediate steps - subsequently increasing the total memory footprint? In the case of vision examples, which are dominated by the activations, what are the implications? If the memory footprint is indeed not consistent with a real-valued algebra, then are we trading model/input size for fewer parameters/efficient computation?\n\nThere are two issues that could increase the memory footprint.  The first is that regardless of implementation, the number of activations will be larger by a factor of about 1.3 (empirically) for the M2R networks when matching the performance of the real network.\n\nThe second issue is that with our current implementation, there are indeed intermediate feature maps that could increase the memory usage.  For M2R there are 8 convolutions of size C/4, which means the memory usage would approximately be doubled.  However, we note that if the appropriate kernels were written to perform the algebra calculation at the lowest level, then this doubling overhead would not exist. \n\nWe will update the text saying that this is a possible concern and point out mitigating strategies.\n\n\n> Are certain algebras more amenable to specific hardware architectures? If so, a brief discussion would enhance the paper overall.\n\n\nThe matrix algebras would map nicely to the currently popular systolic arrays common on accelerators such as GPUs and TPUs.  Although the arrays on current GPUs and TPUs are bigger than sizes considered here, it is possible that future hardware could move to smaller arrays.  Having a larger number of smaller systolic arrays would map nicely to sparse algebra networks.  It would also be possible to build specific algebra multipliers at the hardware level for any algebra.\n\nThese algebras would also accelerate inference cases that would otherwise have a batch size 1 and be completely bandwidth limited, by increasing the compute density even in this case.\n\nWe agree this is an interesting direction and will add a section to the appendix that emphasises this further.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3482/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AlgebraNets", "authorids": ["~Jordan_Hoffmann1", "~Simon_Schmitt1", "~Simon_Osindero1", "~Karen_Simonyan1", "~Erich_Elsen1"], "authors": ["Jordan Hoffmann", "Simon Schmitt", "Simon Osindero", "Karen Simonyan", "Erich Elsen"], "keywords": ["Sparsity", "Pruning", "Efficiency", "Mathematics"], "abstract": "Neural networks have historically been built layerwise from the set of functions in ${f: \\mathbb{R}^n \\to \\mathbb{R}^m }$, i.e. with activations and weights/parameters represented by real numbers, $\\mathbb{R}$. Our work considers a richer set of objects for activations and weights, and undertakes a comprehensive study of alternative algebras as number representations by studying their performance on two challenging problems: large-scale image classification using the ImageNet dataset and language modeling using the enwiki8 and WikiText-103 datasets. We denote this broader class of models as AlgebraNets. Our findings indicate that the conclusions of prior work, which explored neural networks constructed from $\\mathbb{C}$ (complex numbers) and $\\mathbb{H}$ (quaternions) on smaller datasets, do not always transfer to these challenging settings.  However, our results demonstrate that there are alternative algebras which deliver better parameter and computational efficiency compared with $\\mathbb{R}$. We consider $\\mathbb{C}$, $\\mathbb{H}$, $M_{2}(\\mathbb{R})$ (the set of $2\\times2$ real-valued matrices), $M_{2}(\\mathbb{C})$, $M_{3}(\\mathbb{R})$, $M_{4}(\\mathbb{R})$, dual numbers and the $\\mathbb{R}^3$ cross product.  Additionally, we note that multiplication in these algebras has higher compute density than real multiplication, a useful property in situations with inherently limited parameter reuse such as auto-regressive inference and sparse neural networks. We therefore investigate how to induce sparsity within AlgebraNets. We hope that our strong results on large-scale, practical benchmarks will spur further exploration of these unconventional architectures which challenge the default choice of using real numbers for neural network weights and activations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hoffmann|algebranets", "one-sentence_summary": "We investigate a wide range of alternatives to real-valued weights in neural networks and find promising alternatives. ", "supplementary_material": "/attachment/2873be7fd30024a18ed0260327bb37527ed3d60f.zip", "pdf": "/pdf/a29db3f04aadec5e862ecd2dc316a628e12fd6f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iaVGYcsIw", "_bibtex": "@misc{\nhoffmann2021algebranets,\ntitle={AlgebraNets},\nauthor={Jordan Hoffmann and Simon Schmitt and Simon Osindero and Karen Simonyan and Erich Elsen},\nyear={2021},\nurl={https://openreview.net/forum?id=guEuB3FPcd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "guEuB3FPcd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3482/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3482/Authors|ICLR.cc/2021/Conference/Paper3482/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837068, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment"}}}, {"id": "P9MLsJMrRQW", "original": null, "number": 5, "cdate": 1605374822548, "ddate": null, "tcdate": 1605374822548, "tmdate": 1605374822548, "tddate": null, "forum": "guEuB3FPcd", "replyto": "kYdrqZBz_b", "invitation": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment", "content": {"title": "Thank you for your review", "comment": "Thank you for your review. Below, we\u2019ve tried to answer your questions, but firstly here is our motivation for this work, which we hope will help frame both the manuscript and our response.\na) We wanted to search for more efficient alternatives to real numbers to use in neural networks.  This was the goal from the beginning.  We were especially interested if we could combine the higher compute density of algebras with sparsity.\nb) There had been some prior work showing complex numbers and quaternions were more parameter efficient but nothing about FLOPs. FLOPs are correlated with runtime and often at least as important as parameter efficiency, especially in vision models.\nc) We noticed the prior work on complex and quaternions used very FLOP expensive whitening and special initialization.\nd) We chose to investigate those algebras and many more on both a parameter and FLOP efficiency basis. Furthermore, we made preliminary steps towards testing sparsity inducing techniques and these algebras.\n\nIn response to your specific queries: \n* Do you mean why are more efficient neural networks necessary?  Or why are different algebras necessary for more efficient networks?  They are one approach to finding more efficient architectures, but certainly not the only one.\nWe are surprised that you do not think ImageNet is not a suitable task for demonstrating this method: it is a challenging task and is widely used as a way to benchmark state-of-the-art methods. We also have results on enwik8 and wikitext-103 language modeling which, while certainly not large by GPT-3 standards, has been considered a standard language modeling benchmark in the literature.  What tasks would you like to see?\n\n* Thanks for commenting that the results look promising. We choose our axes based upon the standards in other work on efficiency in neural networks.  For example: EfficientNet (M. Tan, et al 2019) show results as FLOPs/parameters vs top-1 accuracy. Similarly, MobileNet (A.G. Howard et al 2017) and MobileNet v2 (M. Sandler et al 2018) also present the same axes. Many pruning papers also use these same axes, for example, \u201cWhat is the State of Neural Network Pruning?\u201d from D. Blalock et al 2020 and \u201cThe State of Sparsity in Deep Neural Networks\u201d from T. Gale et al 2019.  We thank the reviewer for commenting that they found some of the methodology unclear -- are there certain aspects that you found to be particularly confusing?  \n\n* We feel there are a series of important contributions in the work:\n a.) We find some complexities from prior works are not needed.  For example, we do not need special initializations for good performance. \nb.) Clear demonstration of which algebras are more efficient both in terms of parameters and FLOPs in a modern regime across multiple domains. \nc.) We discover that M_2R is better than all algebras that have been previously considered in terms of performance per FLOP while still offering a substantial parameter reduction. \nd.) Showing that M_2R networks can be made sparse and will be better than normal sparsity due to the higher compute density of the algebra.\n\nWe hope that this helps address your concerns.  Please do let us know if there is anything more we can clarify. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3482/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AlgebraNets", "authorids": ["~Jordan_Hoffmann1", "~Simon_Schmitt1", "~Simon_Osindero1", "~Karen_Simonyan1", "~Erich_Elsen1"], "authors": ["Jordan Hoffmann", "Simon Schmitt", "Simon Osindero", "Karen Simonyan", "Erich Elsen"], "keywords": ["Sparsity", "Pruning", "Efficiency", "Mathematics"], "abstract": "Neural networks have historically been built layerwise from the set of functions in ${f: \\mathbb{R}^n \\to \\mathbb{R}^m }$, i.e. with activations and weights/parameters represented by real numbers, $\\mathbb{R}$. Our work considers a richer set of objects for activations and weights, and undertakes a comprehensive study of alternative algebras as number representations by studying their performance on two challenging problems: large-scale image classification using the ImageNet dataset and language modeling using the enwiki8 and WikiText-103 datasets. We denote this broader class of models as AlgebraNets. Our findings indicate that the conclusions of prior work, which explored neural networks constructed from $\\mathbb{C}$ (complex numbers) and $\\mathbb{H}$ (quaternions) on smaller datasets, do not always transfer to these challenging settings.  However, our results demonstrate that there are alternative algebras which deliver better parameter and computational efficiency compared with $\\mathbb{R}$. We consider $\\mathbb{C}$, $\\mathbb{H}$, $M_{2}(\\mathbb{R})$ (the set of $2\\times2$ real-valued matrices), $M_{2}(\\mathbb{C})$, $M_{3}(\\mathbb{R})$, $M_{4}(\\mathbb{R})$, dual numbers and the $\\mathbb{R}^3$ cross product.  Additionally, we note that multiplication in these algebras has higher compute density than real multiplication, a useful property in situations with inherently limited parameter reuse such as auto-regressive inference and sparse neural networks. We therefore investigate how to induce sparsity within AlgebraNets. We hope that our strong results on large-scale, practical benchmarks will spur further exploration of these unconventional architectures which challenge the default choice of using real numbers for neural network weights and activations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hoffmann|algebranets", "one-sentence_summary": "We investigate a wide range of alternatives to real-valued weights in neural networks and find promising alternatives. ", "supplementary_material": "/attachment/2873be7fd30024a18ed0260327bb37527ed3d60f.zip", "pdf": "/pdf/a29db3f04aadec5e862ecd2dc316a628e12fd6f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iaVGYcsIw", "_bibtex": "@misc{\nhoffmann2021algebranets,\ntitle={AlgebraNets},\nauthor={Jordan Hoffmann and Simon Schmitt and Simon Osindero and Karen Simonyan and Erich Elsen},\nyear={2021},\nurl={https://openreview.net/forum?id=guEuB3FPcd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "guEuB3FPcd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3482/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3482/Authors|ICLR.cc/2021/Conference/Paper3482/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837068, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3482/-/Official_Comment"}}}, {"id": "k3b56qlIUhz", "original": null, "number": 2, "cdate": 1603847008646, "ddate": null, "tcdate": 1603847008646, "tmdate": 1605023992128, "tddate": null, "forum": "guEuB3FPcd", "replyto": "guEuB3FPcd", "invitation": "ICLR.cc/2021/Conference/Paper3482/-/Official_Review", "content": {"title": "Impactful paper with strong empirical results", "review": "## Summary\nThe authors propose AlgebraNets - a previously explored approach to replace real-valued algebra in deep learning models with other associative algebras that include 2x2 matrices over real and complex numbers. They provide a comprehensive overview of prior methods in this direction and motivate their work with potential for both parameter and computational efficiency, and suggest that the latter is typically overlooked in prior literature. The paper is very well-written and follows a nice narrative, and the claims are mostly backed empirically with experimental results. \n## Pros \n* Empirically justified with experiments on state-of-the-art benchmarks in both computer vision and NLP. \n* Establishes that exploring other algebras is not just an exercise for mathematical curiosity but also practical, and encourages deep learning practitioners to extend the results. \n* Perhaps the most useful aspect is that the experiments fit well into a standard deep learning framework \u2013 with conventional operations, initialization, etc. That is, the algebras do not require significant custom ops/modifications to achieve state-of-the-art results. \n* Shows multiplicative efficiency (parameter count and FLOPs) in many cases \n## Cons \n* The authors motivate this work with computational efficiency; however, I did not find any discussion or comments on the total memory footprint. Do any of the algebras require us to keep track of partial computations/intermediate steps - subsequently increasing the total memory footprint? In the case of vision examples, which are dominated by the activations, what are the implications? If the memory footprint is indeed not consistent with a real-valued algebra, then are we trading model/input size for fewer parameters/efficient computation?\n* An intuitive justification of the algebras used in these experiments, along with insight for future algebras might be a nice addition, although I wouldn't consider it a con.\n* Are certain algebras more amenable to specific hardware architectures? If so, a brief discussion would enhance the paper overall.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3482/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3482/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AlgebraNets", "authorids": ["~Jordan_Hoffmann1", "~Simon_Schmitt1", "~Simon_Osindero1", "~Karen_Simonyan1", "~Erich_Elsen1"], "authors": ["Jordan Hoffmann", "Simon Schmitt", "Simon Osindero", "Karen Simonyan", "Erich Elsen"], "keywords": ["Sparsity", "Pruning", "Efficiency", "Mathematics"], "abstract": "Neural networks have historically been built layerwise from the set of functions in ${f: \\mathbb{R}^n \\to \\mathbb{R}^m }$, i.e. with activations and weights/parameters represented by real numbers, $\\mathbb{R}$. Our work considers a richer set of objects for activations and weights, and undertakes a comprehensive study of alternative algebras as number representations by studying their performance on two challenging problems: large-scale image classification using the ImageNet dataset and language modeling using the enwiki8 and WikiText-103 datasets. We denote this broader class of models as AlgebraNets. Our findings indicate that the conclusions of prior work, which explored neural networks constructed from $\\mathbb{C}$ (complex numbers) and $\\mathbb{H}$ (quaternions) on smaller datasets, do not always transfer to these challenging settings.  However, our results demonstrate that there are alternative algebras which deliver better parameter and computational efficiency compared with $\\mathbb{R}$. We consider $\\mathbb{C}$, $\\mathbb{H}$, $M_{2}(\\mathbb{R})$ (the set of $2\\times2$ real-valued matrices), $M_{2}(\\mathbb{C})$, $M_{3}(\\mathbb{R})$, $M_{4}(\\mathbb{R})$, dual numbers and the $\\mathbb{R}^3$ cross product.  Additionally, we note that multiplication in these algebras has higher compute density than real multiplication, a useful property in situations with inherently limited parameter reuse such as auto-regressive inference and sparse neural networks. We therefore investigate how to induce sparsity within AlgebraNets. We hope that our strong results on large-scale, practical benchmarks will spur further exploration of these unconventional architectures which challenge the default choice of using real numbers for neural network weights and activations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hoffmann|algebranets", "one-sentence_summary": "We investigate a wide range of alternatives to real-valued weights in neural networks and find promising alternatives. ", "supplementary_material": "/attachment/2873be7fd30024a18ed0260327bb37527ed3d60f.zip", "pdf": "/pdf/a29db3f04aadec5e862ecd2dc316a628e12fd6f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iaVGYcsIw", "_bibtex": "@misc{\nhoffmann2021algebranets,\ntitle={AlgebraNets},\nauthor={Jordan Hoffmann and Simon Schmitt and Simon Osindero and Karen Simonyan and Erich Elsen},\nyear={2021},\nurl={https://openreview.net/forum?id=guEuB3FPcd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "guEuB3FPcd", "replyto": "guEuB3FPcd", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3482/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075008, "tmdate": 1606915800342, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3482/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3482/-/Official_Review"}}}], "count": 20}