{"notes": [{"id": "BJemQ209FQ", "original": "S1gzuKpctX", "number": 1342, "cdate": 1538087962748, "ddate": null, "tcdate": 1538087962748, "tmdate": 1550878194423, "tddate": null, "forum": "BJemQ209FQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Learning to Navigate the Web", "abstract": "Learning in environments with large state and action spaces, and sparse rewards, can hinder a Reinforcement Learning (RL) agent\u2019s learning through trial-and-error. For instance, following natural language instructions on the Web (such as booking a flight ticket) leads to RL settings where input vocabulary and number of actionable elements on a page can grow very large. Even though recent approaches improve the success rate on relatively simple environments with the help of human demonstrations to guide the exploration, they still fail in environments where the set of possible instructions can reach millions. We approach the aforementioned problems from a different perspective and propose guided RL approaches that can generate unbounded amount of experience for an agent to learn from. Instead of learning from a complicated instruction with a large vocabulary, we decompose it into multiple sub-instructions and schedule a curriculum in which an agent is tasked with a gradually increasing subset of these relatively easier sub-instructions. In addition, when the expert demonstrations are not available, we propose a novel meta-learning framework that generates new instruction following tasks and trains the agent more effectively. We train DQN, deep reinforcement learning agent, with Q-value function approximated with a novel QWeb neural network architecture on these smaller, synthetic instructions. We evaluate the ability of our agent to generalize to new instructions onWorld of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions. The QWeb agent outperforms the baseline without using any human demonstration achieving 100% success rate on several difficult environments.", "keywords": ["navigating web pages", "reinforcement learning", "q learning", "curriculum learning", "meta training"], "authorids": ["izzeddingur@gmail.com", "rueckert@google.com", "sandrafaust@google.com", "dilek@ieee.org"], "authors": ["Izzeddin Gur", "Ulrich Rueckert", "Aleksandra Faust", "Dilek Hakkani-Tur"], "TL;DR": "We train reinforcement learning policies using reward augmentation, curriculum learning, and meta-learning  to successfully navigate web pages.", "pdf": "/pdf/1ebd2cc00d943a087baca34ea2d616ac522de5d5.pdf", "paperhash": "gur|learning_to_navigate_the_web", "_bibtex": "@inproceedings{\ngur2018learning,\ntitle={Learning to Navigate the Web},\nauthor={Izzeddin Gur and Ulrich Rueckert and Aleksandra Faust and Dilek Hakkani-Tur},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJemQ209FQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1lYxf1bl4", "original": null, "number": 1, "cdate": 1544774129286, "ddate": null, "tcdate": 1544774129286, "tmdate": 1545354482914, "tddate": null, "forum": "BJemQ209FQ", "replyto": "BJemQ209FQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1342/Meta_Review", "content": {"metareview": "All reviewers (including those with substantial expertise in RL) were solid in their praise for this paper that is also tackling an interesting application that is much less well studied but deserves attention.\n\n", "confidence": "3: The area chair is somewhat confident", "recommendation": "Accept (Poster)", "title": " Important topic, solid contribution"}, "signatures": ["ICLR.cc/2019/Conference/Paper1342/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1342/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Navigate the Web", "abstract": "Learning in environments with large state and action spaces, and sparse rewards, can hinder a Reinforcement Learning (RL) agent\u2019s learning through trial-and-error. For instance, following natural language instructions on the Web (such as booking a flight ticket) leads to RL settings where input vocabulary and number of actionable elements on a page can grow very large. Even though recent approaches improve the success rate on relatively simple environments with the help of human demonstrations to guide the exploration, they still fail in environments where the set of possible instructions can reach millions. We approach the aforementioned problems from a different perspective and propose guided RL approaches that can generate unbounded amount of experience for an agent to learn from. Instead of learning from a complicated instruction with a large vocabulary, we decompose it into multiple sub-instructions and schedule a curriculum in which an agent is tasked with a gradually increasing subset of these relatively easier sub-instructions. In addition, when the expert demonstrations are not available, we propose a novel meta-learning framework that generates new instruction following tasks and trains the agent more effectively. We train DQN, deep reinforcement learning agent, with Q-value function approximated with a novel QWeb neural network architecture on these smaller, synthetic instructions. We evaluate the ability of our agent to generalize to new instructions onWorld of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions. The QWeb agent outperforms the baseline without using any human demonstration achieving 100% success rate on several difficult environments.", "keywords": ["navigating web pages", "reinforcement learning", "q learning", "curriculum learning", "meta training"], "authorids": ["izzeddingur@gmail.com", "rueckert@google.com", "sandrafaust@google.com", "dilek@ieee.org"], "authors": ["Izzeddin Gur", "Ulrich Rueckert", "Aleksandra Faust", "Dilek Hakkani-Tur"], "TL;DR": "We train reinforcement learning policies using reward augmentation, curriculum learning, and meta-learning  to successfully navigate web pages.", "pdf": "/pdf/1ebd2cc00d943a087baca34ea2d616ac522de5d5.pdf", "paperhash": "gur|learning_to_navigate_the_web", "_bibtex": "@inproceedings{\ngur2018learning,\ntitle={Learning to Navigate the Web},\nauthor={Izzeddin Gur and Ulrich Rueckert and Aleksandra Faust and Dilek Hakkani-Tur},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJemQ209FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1342/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352872946, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJemQ209FQ", "replyto": "BJemQ209FQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1342/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1342/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1342/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352872946}}}, {"id": "SylnSTaghX", "original": null, "number": 2, "cdate": 1540574532323, "ddate": null, "tcdate": 1540574532323, "tmdate": 1543368896084, "tddate": null, "forum": "BJemQ209FQ", "replyto": "BJemQ209FQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1342/Official_Review", "content": {"title": "solid experiments, needs clarity improvement", "review": "UPDATE:\n\nThank you to the authors for a comprehensive response.  I have increased my score based on these changes.  I apologize for the misunderstanding about ArXiV papers and indeed the authors are correct on that point.  Thank you as well for reporting the learning speeds.  As you mentioned, they confirm our intuitions and complete the picture of the algorithm\u2019s behavior.  The addition of pseudo-code does make the paper and algorithm easier to follow.  Thank you for adding it.  The rewritten section 5 is indeed much easier to follow and makes the coordination between the agents clear.  Seeing that the instructor is a fixed policy resolves the game theoretic issue form the original review.\n\n\nSummary:\n\nThe paper proposes a deep reinforcement learning approach to filling out web forms, called QWeb.  In addition to both deep and shallow embeddings of the states, the authors evaluate various methods for improving the learning system, including reward shaping, introducing subgoals, and even a meta-learning algorithm that is used as an instructor.  These variations are tested in several environments and basic QWeb is shown to outperform the baselines and many of the adaptations perform even better than that in more complex domains.\n\nReview:\n\nOverall, the problem the paper considers is important and their results seem significant.  The authors have derived a novel architecture and are the first to tackle the problem of filling in web forms at this scale with an autonomous learning agent rather than one that is taught mostly by demonstration.  \n\nThe related work section is very well written with topical references to recent results and solid differentiations to the new algorithm.  However, I see many references in the paper are not from peer reviewed conferences or journals.  Unless absolutely necessary, such papers should not be cited because they have not been properly peer reviewed.  If the papers cited have actually been in a conference or journal, please add the correct attribution.\n\nThe experiments seem well conducted.  I liked that each new addition to the algorithm was tested incrementally in Figure 7 to give a realistic view of the gains introduced by each change.  I also thought the earlier comparisons to the baselines were well done and I liked that they were done against modern cutting-edge LfD demonstrations.  The only thing I would have liked to seen beyond these results are actual learning curves showing, after X iterations, what percentage of the tasks could be completed.  I suspect that in many domains the baseline LfD techniques are learning much faster since learning from teachers tends to be more targeted and sample efficient.  Learning curves would show us whether or not this is the case. \n\nThe weakest part of the paper was the description of the instructor network and the Meta-training in general.  This portion seemed ill-described and largely speculative, despite the promising results in Figure 7.  In particular, Section 5 is very unclear on how exactly the Meta-Learning works.  Pseudocode is definitely needed in this portion well beyond the quick descriptions in Figure 4 and 5, which I could not understand, despite multiple readings.  I suggest eliminating those figures and providing concrete pseudo\u2014code describing the meta learning and also addressing the following open questions in the text:\n\u2022\tWhy is a rule based randomized policy good to learn from?  How is this different from learning from demonstration in the baselines?\n\u2022\tHow is a \u201cfine grained signal\u201d generated?  What does that mean?  Is it a reward?\n\u2022\tIn Section 5.1, are there two RL agents, an instructor and a learner with different reward functions?  If so, isn\u2019t this becoming game theoretic and is this likely to converge in most scenarios?\n\u2022\tWhat does Q_D^I actually represent?  Why is maximizing these values a good thing?\n\nThere are a few grammatical mistakes in the paper including:\n\nAbstract \u2013 simpler environments -> simple environments\nAbstract- with gradually increasing -> with a gradually increasing\nPage 2 \u2013 generate unbounded -> generate an unbounded\nPage 7 \u2013 correct value -> correct values\nPage 9 \u2013 episode length -> episode lengths\n\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1342/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Learning to Navigate the Web", "abstract": "Learning in environments with large state and action spaces, and sparse rewards, can hinder a Reinforcement Learning (RL) agent\u2019s learning through trial-and-error. For instance, following natural language instructions on the Web (such as booking a flight ticket) leads to RL settings where input vocabulary and number of actionable elements on a page can grow very large. Even though recent approaches improve the success rate on relatively simple environments with the help of human demonstrations to guide the exploration, they still fail in environments where the set of possible instructions can reach millions. We approach the aforementioned problems from a different perspective and propose guided RL approaches that can generate unbounded amount of experience for an agent to learn from. Instead of learning from a complicated instruction with a large vocabulary, we decompose it into multiple sub-instructions and schedule a curriculum in which an agent is tasked with a gradually increasing subset of these relatively easier sub-instructions. In addition, when the expert demonstrations are not available, we propose a novel meta-learning framework that generates new instruction following tasks and trains the agent more effectively. We train DQN, deep reinforcement learning agent, with Q-value function approximated with a novel QWeb neural network architecture on these smaller, synthetic instructions. We evaluate the ability of our agent to generalize to new instructions onWorld of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions. The QWeb agent outperforms the baseline without using any human demonstration achieving 100% success rate on several difficult environments.", "keywords": ["navigating web pages", "reinforcement learning", "q learning", "curriculum learning", "meta training"], "authorids": ["izzeddingur@gmail.com", "rueckert@google.com", "sandrafaust@google.com", "dilek@ieee.org"], "authors": ["Izzeddin Gur", "Ulrich Rueckert", "Aleksandra Faust", "Dilek Hakkani-Tur"], "TL;DR": "We train reinforcement learning policies using reward augmentation, curriculum learning, and meta-learning  to successfully navigate web pages.", "pdf": "/pdf/1ebd2cc00d943a087baca34ea2d616ac522de5d5.pdf", "paperhash": "gur|learning_to_navigate_the_web", "_bibtex": "@inproceedings{\ngur2018learning,\ntitle={Learning to Navigate the Web},\nauthor={Izzeddin Gur and Ulrich Rueckert and Aleksandra Faust and Dilek Hakkani-Tur},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJemQ209FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1342/Official_Review", "cdate": 1542234250753, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJemQ209FQ", "replyto": "BJemQ209FQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1342/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335926120, "tmdate": 1552335926120, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1342/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Bklaz-qtRm", "original": null, "number": 4, "cdate": 1543246100935, "ddate": null, "tcdate": 1543246100935, "tmdate": 1543246100935, "tddate": null, "forum": "BJemQ209FQ", "replyto": "HkxIVs6PsQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1342/Official_Comment", "content": {"title": "Thank you for the comments and questions.", "comment": "We thank the reviewer for the comments and questions. Below are our responses.\n\n> \u201cIn the first set of experiments it is clear the improved performance of QWeb over Shi17 and Liu18, however, it is not clear why QWeb is not able to learn in the social-media-all problem. The authors tested only one of the possible variants (AR) of the proposed approach with good performance.\u201d \n\nThe main reason is that in social-media-all environment, the size of the vocabulary is more than 7000 and task length is 12 which are both considerably larger compared to other environments. Another reason is that the QWeb can not learn the correct action by focusing on a single node; it needs to incorporate siblings of a node in the DOM tree to generate the correct action. Without adding shallow encoding (SE) and one of the proposed approaches (such as AR), QWeb is not able to train purely from trial-and-error as the number of successful episodes is very small. \n\nWe updated the Section 6.1 of the paper with these explanations and we plan to conduct more experiments in future work.\n\n> \u201cIt is not clear in the book-flight-form environment, why the QWeb+SE+AR obtained 100% success while the MetaQWeb, which includes one of main components in this paper, has a lower performance.\u201d\n\nThe main reason of the performance difference between the QWeb+SE+AR and the MetaQWeb can be explained by the difference between the generated experience that these models learn from. In training QWeb+SE+AR, we use the original and clean instructions that the environment sets at the beginning of an episode. MetaQWeb, however, is trained with the instructions that instructor agent generates. These instructions are sometimes incorrect (as indicated by the error rate of INET : $4\\%$) and might not reflect the goal accurately. These noisy experiences hurt the performance of MetaQWeb slightly and causes the $1\\%$ drop in performance. \n\nWe updated the Section 6.2 with this explanation.\n\n> \u201cThe proposed method uses a large number of components/methods, but it is not clear the relevance of each of them. The papers reads like, \"I have a very complex problem to solve so I try all the methods that I think will be useful\". The paper will benefit from an individual assessment of the different components.\u201d\n\nThank you for the comment. We have revised the Introduction, and Sections 4 and 5 to clarify the differences between the methods and contributions. Below is the summary that hopefully brings more clarity to the reasoning before the approaches.\n\nWe aim to solve the web navigation tasks in two situations, when the expert demonstrations are available and when they are not. When the expert demonstrations are available, we need to make several improvements to the training to outperform the baselines. These improvements are: better neural network architecture (QWeb), and more dense rewards. We get the more dense rewards by using the reward potentials and setting up a curriculum over the given demonstrations. \n\nIn the second case, when the expert demonstrations are not available. In that situation, we use the meta-trainer to generate new demonstrations. \n\n> \u201dThe authors should include a section of conclusions and future work.\u201d\nThank you for point it out. The section is added to the paper.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1342/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1342/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1342/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Navigate the Web", "abstract": "Learning in environments with large state and action spaces, and sparse rewards, can hinder a Reinforcement Learning (RL) agent\u2019s learning through trial-and-error. For instance, following natural language instructions on the Web (such as booking a flight ticket) leads to RL settings where input vocabulary and number of actionable elements on a page can grow very large. Even though recent approaches improve the success rate on relatively simple environments with the help of human demonstrations to guide the exploration, they still fail in environments where the set of possible instructions can reach millions. We approach the aforementioned problems from a different perspective and propose guided RL approaches that can generate unbounded amount of experience for an agent to learn from. Instead of learning from a complicated instruction with a large vocabulary, we decompose it into multiple sub-instructions and schedule a curriculum in which an agent is tasked with a gradually increasing subset of these relatively easier sub-instructions. In addition, when the expert demonstrations are not available, we propose a novel meta-learning framework that generates new instruction following tasks and trains the agent more effectively. We train DQN, deep reinforcement learning agent, with Q-value function approximated with a novel QWeb neural network architecture on these smaller, synthetic instructions. We evaluate the ability of our agent to generalize to new instructions onWorld of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions. The QWeb agent outperforms the baseline without using any human demonstration achieving 100% success rate on several difficult environments.", "keywords": ["navigating web pages", "reinforcement learning", "q learning", "curriculum learning", "meta training"], "authorids": ["izzeddingur@gmail.com", "rueckert@google.com", "sandrafaust@google.com", "dilek@ieee.org"], "authors": ["Izzeddin Gur", "Ulrich Rueckert", "Aleksandra Faust", "Dilek Hakkani-Tur"], "TL;DR": "We train reinforcement learning policies using reward augmentation, curriculum learning, and meta-learning  to successfully navigate web pages.", "pdf": "/pdf/1ebd2cc00d943a087baca34ea2d616ac522de5d5.pdf", "paperhash": "gur|learning_to_navigate_the_web", "_bibtex": "@inproceedings{\ngur2018learning,\ntitle={Learning to Navigate the Web},\nauthor={Izzeddin Gur and Ulrich Rueckert and Aleksandra Faust and Dilek Hakkani-Tur},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJemQ209FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1342/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610811, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJemQ209FQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1342/Authors", "ICLR.cc/2019/Conference/Paper1342/Reviewers", "ICLR.cc/2019/Conference/Paper1342/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1342/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1342/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1342/Authors|ICLR.cc/2019/Conference/Paper1342/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1342/Reviewers", "ICLR.cc/2019/Conference/Paper1342/Authors", "ICLR.cc/2019/Conference/Paper1342/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610811}}}, {"id": "SJeSplqYR7", "original": null, "number": 3, "cdate": 1543246012555, "ddate": null, "tcdate": 1543246012555, "tmdate": 1543246012555, "tddate": null, "forum": "BJemQ209FQ", "replyto": "HJlCjlqKAX", "invitation": "ICLR.cc/2019/Conference/-/Paper1342/Official_Comment", "content": {"title": "Thank you for the insightful comments (2/2).", "comment": "> \u201cIn Section 5.1, are there two RL agents, an instructor and a learner with different reward functions?  If so, isn\u2019t this becoming game theoretic and is this likely to converge in most scenarios?\u201d\n\nThere are two different RL agents : instructor agent (INET) and navigator agent (QWeb). These are trained in two phases : (i) we first train INET (a DQN agent with Q value function defined at the end of Section 5.1) using the instruction generation environment that we described in Section 5.1, (ii) next, parameters of the INET agent is fixed and we train QWeb using the instruction and goal pairs that the meta-trainer generates by running INET at the beginning of each episode. Hence, we avoid the problems that could have arised by jointly training two different RL agents with different objectives.\n\n> \u201cWhat does Q_D^I actually represent?  Why is maximizing these values a good thing?\u201d\nQ_D^I is the Q value function that we used to train instructor agent (INET) as we described in Section 5.1.\n\n> \u201cThere are a few grammatical mistakes in the paper including.\u201d\nThank you for pointing it out. We updated in the paper, and will make another pass for the final version if accepted."}, "signatures": ["ICLR.cc/2019/Conference/Paper1342/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1342/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1342/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Navigate the Web", "abstract": "Learning in environments with large state and action spaces, and sparse rewards, can hinder a Reinforcement Learning (RL) agent\u2019s learning through trial-and-error. For instance, following natural language instructions on the Web (such as booking a flight ticket) leads to RL settings where input vocabulary and number of actionable elements on a page can grow very large. Even though recent approaches improve the success rate on relatively simple environments with the help of human demonstrations to guide the exploration, they still fail in environments where the set of possible instructions can reach millions. We approach the aforementioned problems from a different perspective and propose guided RL approaches that can generate unbounded amount of experience for an agent to learn from. Instead of learning from a complicated instruction with a large vocabulary, we decompose it into multiple sub-instructions and schedule a curriculum in which an agent is tasked with a gradually increasing subset of these relatively easier sub-instructions. In addition, when the expert demonstrations are not available, we propose a novel meta-learning framework that generates new instruction following tasks and trains the agent more effectively. We train DQN, deep reinforcement learning agent, with Q-value function approximated with a novel QWeb neural network architecture on these smaller, synthetic instructions. We evaluate the ability of our agent to generalize to new instructions onWorld of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions. The QWeb agent outperforms the baseline without using any human demonstration achieving 100% success rate on several difficult environments.", "keywords": ["navigating web pages", "reinforcement learning", "q learning", "curriculum learning", "meta training"], "authorids": ["izzeddingur@gmail.com", "rueckert@google.com", "sandrafaust@google.com", "dilek@ieee.org"], "authors": ["Izzeddin Gur", "Ulrich Rueckert", "Aleksandra Faust", "Dilek Hakkani-Tur"], "TL;DR": "We train reinforcement learning policies using reward augmentation, curriculum learning, and meta-learning  to successfully navigate web pages.", "pdf": "/pdf/1ebd2cc00d943a087baca34ea2d616ac522de5d5.pdf", "paperhash": "gur|learning_to_navigate_the_web", "_bibtex": "@inproceedings{\ngur2018learning,\ntitle={Learning to Navigate the Web},\nauthor={Izzeddin Gur and Ulrich Rueckert and Aleksandra Faust and Dilek Hakkani-Tur},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJemQ209FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1342/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610811, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJemQ209FQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1342/Authors", "ICLR.cc/2019/Conference/Paper1342/Reviewers", "ICLR.cc/2019/Conference/Paper1342/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1342/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1342/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1342/Authors|ICLR.cc/2019/Conference/Paper1342/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1342/Reviewers", "ICLR.cc/2019/Conference/Paper1342/Authors", "ICLR.cc/2019/Conference/Paper1342/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610811}}}, {"id": "HJlCjlqKAX", "original": null, "number": 2, "cdate": 1543245990485, "ddate": null, "tcdate": 1543245990485, "tmdate": 1543245990485, "tddate": null, "forum": "BJemQ209FQ", "replyto": "SylnSTaghX", "invitation": "ICLR.cc/2019/Conference/-/Paper1342/Official_Comment", "content": {"title": "Thank you for the insightful comments (1/2).", "comment": "We thank the reviewer for the insightful comments. Below are our responses.\n\n> \u201cHowever, I see many references in the paper are not from peer reviewed conferences or journals.  Unless absolutely necessary, such papers should not be cited because they have not been properly peer reviewed.\u201d\n\nThank you for pointing that out. We updated the references where the archival versions became available, and will do so again before camera-ready if accepted. At the same time, we also wanted to kindly point out that ICLR reviewer guidelines consider publication on Arxiv as prior work that should be properly cited: https://iclr.cc/Conferences/2019/Reviewer_Guidelines\n\n\n> \u201cThe only thing I would have liked to seen beyond these results are actual learning curves showing, after X iterations, what percentage of the tasks could be completed.  I suspect that in many domains the baseline LfD techniques are learning much faster since learning from teachers tends to be more targeted and sample efficient.  Learning curves would show us whether or not this is the case. \u201c\n\nWe collected the number of steps (k=1000) needed to reach the top performance :\n___________________________________________________\n| environment \\ method  |  QWeb   |  LIU18 | \n--------------------------------------------------------------\n|  click-pie                           | 175k      |  13k      |\n|  login-user                       |  96k       |  < 1k     |\n|  click-dialog                     |  5k         |  < 1k     |\n|  enter-password             |  3k         |  < 1k     |\n--------------------------------------------------------------\n\nThese numbers reflect the reviewer\u2019s intuition that LfD techniques learn faster, however, with a drop in success rate for some environments. We updated the experimental results in Section 6.1 with these results.\n\n> \u201dThe weakest part of the paper was the description of the instructor network and the Meta-training in general.  This portion seemed ill-described and largely speculative, despite the promising results in Figure 7.  In particular, Section 5 is very unclear on how exactly the Meta-Learning works.  Pseudocode is definitely needed in this portion well beyond the quick descriptions in Figure 4 and 5, which I could not understand, despite multiple readings.  I suggest eliminating those figures and providing concrete pseudo\u2014code describing the meta learning and also addressing the following open questions in the text:\u201d\n\nThank you so much for the suggestion. We added the Algorithms 1, 2, 3 for the curriculum learning, DQN training, and meta learning. We removed Figure 5, and put Figures 3 and 4 side-by-side, since they both depict neural network architecture. We have also rewrote the Section 5. We are hoping that the changes are improving the clarity.\n\n> \u201cWhy is a rule based randomized policy good to learn from?  How is this different from learning from demonstration in the baselines?\u201d\nWhen the expert demonstrations are not available, we can use any policy (random or rule-based) and pretend that the policy is following some, to us known, instruction. The instructor agent learns to recover that hidden instructions, in effect creating new demonstrations. Once the instructor is trained to recover the instructions for a given policy, we generate new instruction / goal paths so that we can train QWeb. The choice of policy is arbitrary, and it was a design choice to select a simple, rule-based policy that visits each DOM element in web navigation environments.\n\nOur meta-training approach has two main advantages over learning from demonstrations:\nBy learning to generate new instruction following tasks, we can generate unbounded amount of episodes for any environment where collecting large amount of episodes for each environment is costly.\nSimilar to our curriculum generation with simulated goals approach, generated goal states are allowed to be incomplete. For example, if we constrain our rule based policy to run only a small number of steps, generated goal state could be incomplete and some DOM elements in the web page could be unvisited. In this case, QWeb can still leverage these experiences while also learning from the original instructions and sparse rewards that the environment generates.\n\nPaper\u2019s introduction, and Section 5 are updated to clarify the role and selection of the rule-based policy, and advantages over the baselines.\n\n> \u201cHow is a \u201cfine grained signal\u201d generated?  What does that mean?  Is it a reward?\u201d\nThank you for pointing it out. Yes, it is a dense reward. We updated the paper to use more commonly used term: dense reward.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1342/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1342/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1342/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Navigate the Web", "abstract": "Learning in environments with large state and action spaces, and sparse rewards, can hinder a Reinforcement Learning (RL) agent\u2019s learning through trial-and-error. For instance, following natural language instructions on the Web (such as booking a flight ticket) leads to RL settings where input vocabulary and number of actionable elements on a page can grow very large. Even though recent approaches improve the success rate on relatively simple environments with the help of human demonstrations to guide the exploration, they still fail in environments where the set of possible instructions can reach millions. We approach the aforementioned problems from a different perspective and propose guided RL approaches that can generate unbounded amount of experience for an agent to learn from. Instead of learning from a complicated instruction with a large vocabulary, we decompose it into multiple sub-instructions and schedule a curriculum in which an agent is tasked with a gradually increasing subset of these relatively easier sub-instructions. In addition, when the expert demonstrations are not available, we propose a novel meta-learning framework that generates new instruction following tasks and trains the agent more effectively. We train DQN, deep reinforcement learning agent, with Q-value function approximated with a novel QWeb neural network architecture on these smaller, synthetic instructions. We evaluate the ability of our agent to generalize to new instructions onWorld of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions. The QWeb agent outperforms the baseline without using any human demonstration achieving 100% success rate on several difficult environments.", "keywords": ["navigating web pages", "reinforcement learning", "q learning", "curriculum learning", "meta training"], "authorids": ["izzeddingur@gmail.com", "rueckert@google.com", "sandrafaust@google.com", "dilek@ieee.org"], "authors": ["Izzeddin Gur", "Ulrich Rueckert", "Aleksandra Faust", "Dilek Hakkani-Tur"], "TL;DR": "We train reinforcement learning policies using reward augmentation, curriculum learning, and meta-learning  to successfully navigate web pages.", "pdf": "/pdf/1ebd2cc00d943a087baca34ea2d616ac522de5d5.pdf", "paperhash": "gur|learning_to_navigate_the_web", "_bibtex": "@inproceedings{\ngur2018learning,\ntitle={Learning to Navigate the Web},\nauthor={Izzeddin Gur and Ulrich Rueckert and Aleksandra Faust and Dilek Hakkani-Tur},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJemQ209FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1342/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610811, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJemQ209FQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1342/Authors", "ICLR.cc/2019/Conference/Paper1342/Reviewers", "ICLR.cc/2019/Conference/Paper1342/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1342/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1342/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1342/Authors|ICLR.cc/2019/Conference/Paper1342/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1342/Reviewers", "ICLR.cc/2019/Conference/Paper1342/Authors", "ICLR.cc/2019/Conference/Paper1342/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610811}}}, {"id": "BJexNkcF0Q", "original": null, "number": 1, "cdate": 1543245607955, "ddate": null, "tcdate": 1543245607955, "tmdate": 1543245607955, "tddate": null, "forum": "BJemQ209FQ", "replyto": "ryejZrK9h7", "invitation": "ICLR.cc/2019/Conference/-/Paper1342/Official_Comment", "content": {"title": "Thank you for the kind words and questions that help us improve the paper.", "comment": "We thank the reviewer for the kind words and questions that help us improve the paper. We detail our responses below.\n\n> \u201cThere are a few notations used without definition, for example DOM tree, Potential (in equation (4))\u201d\n\nWe updated in the paper.\nOn Page 3, line 3: \u201cthe Document Object Model (DOM) trees, a hierarchy of web elements in a web page.\u201d\n\nSection 4.2:\u201cwe define a potential function ($Potential(s, g)$) that counts the number of matching DOM elements between a given state (s) and the goal state (g); normalized by the number of DOM elements in the goal state. Potential based reward is then computed as the scaled difference between two potentials for the next state and current state\u201d\n\n> \u201cSome justification regarding the the Q value function specified in (1) might be helpful, otherwise it looks very adhoc\u201d.\n\nOur Q value function in Eq. (1) is motivated by the design of our composite actions (click(e) and type(e, y)) and the nature of web pages in general. A DOM element (e) in a web page mostly identifies which composite action to select, e.g., a text box such as destination airport is typed with the name of an airport code while a date picker is clicked. This motivates the dependency graph that we sketched in Figure 2. We define our Q value function for each composite action based on this dependency graph via a separate value function to model each node in the graph given its dependencies. We also also added this motivation to Section 3.\n\n\n> \u201cAlthough using both shallow encoding and augmented reward lead to good empirical results, it might be useful to give more insights, for example, sample size limit cause overfitting for deep models?\u201d\n\nWe would like to give more insights into overfitting of deep models without and with augmented rewards. Without augmented rewards, the Q function overfits very early to the minimum Q value possible since the majority of the episodes are unsuccessful and the reward is highly unbalanced towards negative. Escaping this bad minima via purely random exploration is difficult especially in environments that require longer episodes. We observe that in majority of these cases the policy converges to terminating the episode as early as possible to get the least step penalty. With augmented rewards, Q function recovers from these cases very quickly and gradually learns from more successful episodes. We also added these insights into Section 6.1.\n\n\n> \u201cWhat are the sizes of action state and action spaces?\u201d\n\nOur action and state spaces are mainly defined by the number of DOM elements in web pages and number of fields in the instructions. For example, in flight-booking-form environment, the number of DOM elements is capped at 100, number of fields is 3, and there are two types of actions (click or type). Hence, the number of possible actions would reach 600 and number of possible variables in a state reach 300. These numbers, however, do not reflect the possible \u201crealization\u201d of a DOM element or a field; they just reflect a sketch. For example, \u201cfrom\u201d field can take a value from 700 possible airports or \u201cdestination\u201d input DOM element can be repetitively typed with any value from the instruction. These greatly increase the space of both states and actions. We added this description into the Section 6.1.\n\n\n> \u201cThe conclusion part is missing.\u201d\nThank you for pointing that out. We added the conclusion in the paper.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1342/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1342/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1342/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Navigate the Web", "abstract": "Learning in environments with large state and action spaces, and sparse rewards, can hinder a Reinforcement Learning (RL) agent\u2019s learning through trial-and-error. For instance, following natural language instructions on the Web (such as booking a flight ticket) leads to RL settings where input vocabulary and number of actionable elements on a page can grow very large. Even though recent approaches improve the success rate on relatively simple environments with the help of human demonstrations to guide the exploration, they still fail in environments where the set of possible instructions can reach millions. We approach the aforementioned problems from a different perspective and propose guided RL approaches that can generate unbounded amount of experience for an agent to learn from. Instead of learning from a complicated instruction with a large vocabulary, we decompose it into multiple sub-instructions and schedule a curriculum in which an agent is tasked with a gradually increasing subset of these relatively easier sub-instructions. In addition, when the expert demonstrations are not available, we propose a novel meta-learning framework that generates new instruction following tasks and trains the agent more effectively. We train DQN, deep reinforcement learning agent, with Q-value function approximated with a novel QWeb neural network architecture on these smaller, synthetic instructions. We evaluate the ability of our agent to generalize to new instructions onWorld of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions. The QWeb agent outperforms the baseline without using any human demonstration achieving 100% success rate on several difficult environments.", "keywords": ["navigating web pages", "reinforcement learning", "q learning", "curriculum learning", "meta training"], "authorids": ["izzeddingur@gmail.com", "rueckert@google.com", "sandrafaust@google.com", "dilek@ieee.org"], "authors": ["Izzeddin Gur", "Ulrich Rueckert", "Aleksandra Faust", "Dilek Hakkani-Tur"], "TL;DR": "We train reinforcement learning policies using reward augmentation, curriculum learning, and meta-learning  to successfully navigate web pages.", "pdf": "/pdf/1ebd2cc00d943a087baca34ea2d616ac522de5d5.pdf", "paperhash": "gur|learning_to_navigate_the_web", "_bibtex": "@inproceedings{\ngur2018learning,\ntitle={Learning to Navigate the Web},\nauthor={Izzeddin Gur and Ulrich Rueckert and Aleksandra Faust and Dilek Hakkani-Tur},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJemQ209FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1342/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610811, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJemQ209FQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1342/Authors", "ICLR.cc/2019/Conference/Paper1342/Reviewers", "ICLR.cc/2019/Conference/Paper1342/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1342/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1342/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1342/Authors|ICLR.cc/2019/Conference/Paper1342/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1342/Reviewers", "ICLR.cc/2019/Conference/Paper1342/Authors", "ICLR.cc/2019/Conference/Paper1342/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610811}}}, {"id": "ryejZrK9h7", "original": null, "number": 3, "cdate": 1541211395108, "ddate": null, "tcdate": 1541211395108, "tmdate": 1541533214016, "tddate": null, "forum": "BJemQ209FQ", "replyto": "BJemQ209FQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1342/Official_Review", "content": {"title": "a good RL application paper for dealing with large action and state spaces", "review": "This paper developed a curriculum learning method for training an RL agent to navigate a web. It is based on the idea of decomposing an instruction in to multiple sub-instructions, which is equivalent to decompose the original task into multiple easy to solve sub-tasks. The paper is well motivated and easily accessible. The problem tackled in this work is an interesting application of RL dealing with large action and state spaces. It also demonstrates superior performance over the state of the art methods on the same domains\n\nHere are the comments for improving this manuscript:\n  \nThere are a few notations used without definition, for example DOM tree, Potential (in equation (4))\n\nSome justification regarding the the Q value function specified in (1) might be helpful, otherwise it looks very adhoc.\n\nAlthough using both shallow encoding and augmented reward lead to good empirical results, it might be useful to give more insights, for example, sample size limit cause overfitting for deep models?\n\nWhat are the sizes of action state and action spaces?\n\nThe conclusion part is missing.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1342/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Navigate the Web", "abstract": "Learning in environments with large state and action spaces, and sparse rewards, can hinder a Reinforcement Learning (RL) agent\u2019s learning through trial-and-error. For instance, following natural language instructions on the Web (such as booking a flight ticket) leads to RL settings where input vocabulary and number of actionable elements on a page can grow very large. Even though recent approaches improve the success rate on relatively simple environments with the help of human demonstrations to guide the exploration, they still fail in environments where the set of possible instructions can reach millions. We approach the aforementioned problems from a different perspective and propose guided RL approaches that can generate unbounded amount of experience for an agent to learn from. Instead of learning from a complicated instruction with a large vocabulary, we decompose it into multiple sub-instructions and schedule a curriculum in which an agent is tasked with a gradually increasing subset of these relatively easier sub-instructions. In addition, when the expert demonstrations are not available, we propose a novel meta-learning framework that generates new instruction following tasks and trains the agent more effectively. We train DQN, deep reinforcement learning agent, with Q-value function approximated with a novel QWeb neural network architecture on these smaller, synthetic instructions. We evaluate the ability of our agent to generalize to new instructions onWorld of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions. The QWeb agent outperforms the baseline without using any human demonstration achieving 100% success rate on several difficult environments.", "keywords": ["navigating web pages", "reinforcement learning", "q learning", "curriculum learning", "meta training"], "authorids": ["izzeddingur@gmail.com", "rueckert@google.com", "sandrafaust@google.com", "dilek@ieee.org"], "authors": ["Izzeddin Gur", "Ulrich Rueckert", "Aleksandra Faust", "Dilek Hakkani-Tur"], "TL;DR": "We train reinforcement learning policies using reward augmentation, curriculum learning, and meta-learning  to successfully navigate web pages.", "pdf": "/pdf/1ebd2cc00d943a087baca34ea2d616ac522de5d5.pdf", "paperhash": "gur|learning_to_navigate_the_web", "_bibtex": "@inproceedings{\ngur2018learning,\ntitle={Learning to Navigate the Web},\nauthor={Izzeddin Gur and Ulrich Rueckert and Aleksandra Faust and Dilek Hakkani-Tur},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJemQ209FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1342/Official_Review", "cdate": 1542234250753, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJemQ209FQ", "replyto": "BJemQ209FQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1342/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335926120, "tmdate": 1552335926120, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1342/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkxIVs6PsQ", "original": null, "number": 1, "cdate": 1539984173896, "ddate": null, "tcdate": 1539984173896, "tmdate": 1541533213570, "tddate": null, "forum": "BJemQ209FQ", "replyto": "BJemQ209FQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1342/Official_Review", "content": {"title": "A novel proposal addressing a complex problem with a large number of components but without a clear analysis of their relevance", "review": "The paper propose a framework to deal with large state and action\nspaces with sparse rewards in reinforcement learning. In particular,\nthey propose to use a meta-learner to generate experience to the agent\nand to decompose the learning task into simpler sub-tasks. The authors\ntrain a DQN with a novel architecture to navigate the Web.\nIn addition the authors propose to use several strategies: shallow\nencoding (SE), reward shaping (AR) and curriculum learning (CI/CG). \nIt is shown how the proposed method outperforms state-of-the-art\nsystems on several tasks.\n\nIn the first set of experiments it is clear the improved performance\nof QWeb over Shi17 and Liu18, however, it is not clear why QWeb is not\nable to learn in the social-media-all problem. The authors tested only\none of the possible variants (AR) of the proposed approach with good\nperformance. \n\nIt is not clear in the book-flight-form environment, why the\nQWeb+SE+AR obtained 100% success while the MetaQWeb, which includes\none of main components in this paper, has a lower performance.\n\nThe proposed method uses a large number of components/methods, but it\nis not clear the relevance of each of them. The papers reads like, \"I\nhave a very complex problem to solve so I try all the methods that I\nthink will be useful\". The paper will benefit from an individual\nassessment of the different components.\n\nThe authors should include a section of conclusions and future work.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1342/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Navigate the Web", "abstract": "Learning in environments with large state and action spaces, and sparse rewards, can hinder a Reinforcement Learning (RL) agent\u2019s learning through trial-and-error. For instance, following natural language instructions on the Web (such as booking a flight ticket) leads to RL settings where input vocabulary and number of actionable elements on a page can grow very large. Even though recent approaches improve the success rate on relatively simple environments with the help of human demonstrations to guide the exploration, they still fail in environments where the set of possible instructions can reach millions. We approach the aforementioned problems from a different perspective and propose guided RL approaches that can generate unbounded amount of experience for an agent to learn from. Instead of learning from a complicated instruction with a large vocabulary, we decompose it into multiple sub-instructions and schedule a curriculum in which an agent is tasked with a gradually increasing subset of these relatively easier sub-instructions. In addition, when the expert demonstrations are not available, we propose a novel meta-learning framework that generates new instruction following tasks and trains the agent more effectively. We train DQN, deep reinforcement learning agent, with Q-value function approximated with a novel QWeb neural network architecture on these smaller, synthetic instructions. We evaluate the ability of our agent to generalize to new instructions onWorld of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions. The QWeb agent outperforms the baseline without using any human demonstration achieving 100% success rate on several difficult environments.", "keywords": ["navigating web pages", "reinforcement learning", "q learning", "curriculum learning", "meta training"], "authorids": ["izzeddingur@gmail.com", "rueckert@google.com", "sandrafaust@google.com", "dilek@ieee.org"], "authors": ["Izzeddin Gur", "Ulrich Rueckert", "Aleksandra Faust", "Dilek Hakkani-Tur"], "TL;DR": "We train reinforcement learning policies using reward augmentation, curriculum learning, and meta-learning  to successfully navigate web pages.", "pdf": "/pdf/1ebd2cc00d943a087baca34ea2d616ac522de5d5.pdf", "paperhash": "gur|learning_to_navigate_the_web", "_bibtex": "@inproceedings{\ngur2018learning,\ntitle={Learning to Navigate the Web},\nauthor={Izzeddin Gur and Ulrich Rueckert and Aleksandra Faust and Dilek Hakkani-Tur},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BJemQ209FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1342/Official_Review", "cdate": 1542234250753, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJemQ209FQ", "replyto": "BJemQ209FQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1342/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335926120, "tmdate": 1552335926120, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1342/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 9}