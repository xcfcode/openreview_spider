{"notes": [{"id": "I6NRcao1w-X", "original": "yWVbmpyzrfX", "number": 542, "cdate": 1601308066862, "ddate": null, "tcdate": 1601308066862, "tmdate": 1614985695026, "tddate": null, "forum": "I6NRcao1w-X", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Robust Reinforcement Learning using Adversarial Populations", "authorids": ["~Eugene_Vinitsky1", "yuqing_du@berkeley.edu", "~Kanaad_V_Parvate1", "~Kathy_Jang1", "~Pieter_Abbeel2", "~Alexandre_Bayen2"], "authors": ["Eugene Vinitsky", "Yuqing du", "Kanaad V Parvate", "Kathy Jang", "Pieter Abbeel", "Alexandre Bayen"], "keywords": ["Robust Control", "Reinforcement Learning", "Multiagent Systems"], "abstract": "Reinforcement Learning (RL) is an effective tool for controller design but can struggle with issues of robustness, failing catastrophically when the underlying system dynamics are perturbed. The Robust RL formulation tackles this by adding worst-case adversarial noise to the dynamics and constructing the noise distribution as the solution to a zero-sum minimax game. However, existing work on learning solutions to the Robust RL formulation has primarily focused on training a single RL agent against a single adversary. In this work, we demonstrate that using a single adversary does not consistently yield robustness to dynamics variations under standard parametrizations of the adversary; the resulting policy is highly exploitable by new adversaries. We propose a population-based augmentation to the Robust RL formulation in which we randomly initialize a population of adversaries and sample from the population uniformly during training. We empirically validate across a variety of benchmarks that the use of an adversarial population results in a less exploitable, more robust policy.  Finally, we demonstrate that this approach provides comparable robustness and generalization as domain randomization on these benchmarks while avoiding a ubiquitous domain randomization failure mode.", "one-sentence_summary": "We demonstrate that the standard robust RL formulation does not consistently yield robustness but that this can be alleviated using a population of adversaries.", "pdf": "/pdf/1e5388e0a8d4b52ab688222e9a6d499254c63e02.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vinitsky|robust_reinforcement_learning_using_adversarial_populations", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=r3UNEA7hm", "_bibtex": "@misc{\nvinitsky2021robust,\ntitle={Robust Reinforcement Learning using Adversarial Populations},\nauthor={Eugene Vinitsky and Yuqing du and Kanaad V Parvate and Kathy Jang and Pieter Abbeel and Alexandre Bayen},\nyear={2021},\nurl={https://openreview.net/forum?id=I6NRcao1w-X}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 19, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "QJSGvMhxn9G", "original": null, "number": 1, "cdate": 1610040452522, "ddate": null, "tcdate": 1610040452522, "tmdate": 1610474054814, "tddate": null, "forum": "I6NRcao1w-X", "replyto": "I6NRcao1w-X", "invitation": "ICLR.cc/2021/Conference/Paper542/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper studies reinforcement learning in the presence of (adversarial) perturbations in the underlying system dynamics. The main (novel) observation is that \u00a0agents trained against a single policy may overfit \u00a0to that policy and hence will lack robustness to new/unseen policies. The paper proposes a population-based augmentation to the Robust RL formulation in which a population of adversaries are randomly initialized and samples from during training. The authors seek to show that their method generalizes well to unseen policies at test time.\n\nMost reviewers agree that the paper provides a range of solid experimental results (with in-distribution and out-of-distribution tasks) showing robustness and generalization of their methods on several robotics benchmarks while avoiding a ubiquitous domain randomization failure mode. However, all the reviewers (and myself) agree that some of the conceptual claims of the paper may not be precise. For example, some of the reviewers disagree with the authors on finding the (mixed) Nash equilibria. Such general claims are hard to validate (may not even be true) and need theoretical justification. Hence, it is not conceptually clear why using multiple adversaries would not suffer from the same limitations as in the single adversary case.\u00a0 Also, in the discussion phase, the reviewers agreed that the results/claims of the paper (i.e. overfitting to a single adversary and the need for multiple adversaries) are very interesting, but at the same time need to be confirmed by more extensive experiments.  \n\nIndeed, if the above are addressed, the paper would make a strong contribution to the area of RL. \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Reinforcement Learning using Adversarial Populations", "authorids": ["~Eugene_Vinitsky1", "yuqing_du@berkeley.edu", "~Kanaad_V_Parvate1", "~Kathy_Jang1", "~Pieter_Abbeel2", "~Alexandre_Bayen2"], "authors": ["Eugene Vinitsky", "Yuqing du", "Kanaad V Parvate", "Kathy Jang", "Pieter Abbeel", "Alexandre Bayen"], "keywords": ["Robust Control", "Reinforcement Learning", "Multiagent Systems"], "abstract": "Reinforcement Learning (RL) is an effective tool for controller design but can struggle with issues of robustness, failing catastrophically when the underlying system dynamics are perturbed. The Robust RL formulation tackles this by adding worst-case adversarial noise to the dynamics and constructing the noise distribution as the solution to a zero-sum minimax game. However, existing work on learning solutions to the Robust RL formulation has primarily focused on training a single RL agent against a single adversary. In this work, we demonstrate that using a single adversary does not consistently yield robustness to dynamics variations under standard parametrizations of the adversary; the resulting policy is highly exploitable by new adversaries. We propose a population-based augmentation to the Robust RL formulation in which we randomly initialize a population of adversaries and sample from the population uniformly during training. We empirically validate across a variety of benchmarks that the use of an adversarial population results in a less exploitable, more robust policy.  Finally, we demonstrate that this approach provides comparable robustness and generalization as domain randomization on these benchmarks while avoiding a ubiquitous domain randomization failure mode.", "one-sentence_summary": "We demonstrate that the standard robust RL formulation does not consistently yield robustness but that this can be alleviated using a population of adversaries.", "pdf": "/pdf/1e5388e0a8d4b52ab688222e9a6d499254c63e02.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vinitsky|robust_reinforcement_learning_using_adversarial_populations", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=r3UNEA7hm", "_bibtex": "@misc{\nvinitsky2021robust,\ntitle={Robust Reinforcement Learning using Adversarial Populations},\nauthor={Eugene Vinitsky and Yuqing du and Kanaad V Parvate and Kathy Jang and Pieter Abbeel and Alexandre Bayen},\nyear={2021},\nurl={https://openreview.net/forum?id=I6NRcao1w-X}\n}"}, "tags": [], "invitation": {"reply": {"forum": "I6NRcao1w-X", "replyto": "I6NRcao1w-X", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040452508, "tmdate": 1610474054799, "id": "ICLR.cc/2021/Conference/Paper542/-/Decision"}}}, {"id": "5wc1ZUqVmM", "original": null, "number": 1, "cdate": 1603713498005, "ddate": null, "tcdate": 1603713498005, "tmdate": 1607153257025, "tddate": null, "forum": "I6NRcao1w-X", "replyto": "I6NRcao1w-X", "invitation": "ICLR.cc/2021/Conference/Paper542/-/Official_Review", "content": {"title": "Simple and nice idea, but experiments are not convincing enough.", "review": "Summary: This paper proposes to improve robustness in reinforcement learning via a population of diverse adversaries, where previous works mainly focus on the use a single adversary to mitigate the problem that the trained policy could be highly exploitable by the adversary. Specifically, at each iteration, it randomly selects an adversary from the population for rollouts, and it is trained by PPO. Experiments are conducted on 3 MuJoCo environments in comparison with vanilla PPO, domain randomization. \n\nStrong points: Using a population of adversaries to improve robustness in RL is interesting. The idea is simple, and the writing is clear.\n\nConcerns:\nMy major concern is in the experimental evaluation.\na. Results are shown using final performance. I am curious about the learning curve \u2013 how does the method compare against other baselines in terms of sample efficiency? A side-effect using a population is that RAP needs to update n adversaries at each training iteration compared with using a single adversary, and will incur more computation overhead. Could authors fairly compare with other baselines in terms of this and show the learning curve? \n\nb. How *MUCH LONGER* does it take to run RAP compared with other baselines? How much more memory does it take to use n adversaries compared with a single adversary?\n\nc. Could authors compare with a naive extension of the single adversary case in which the single adversary sample n actions? Is the baseline comparable with RAP using n adversaries? \n\nd. I am confused why RAP is built upon an on-policy algorithm. A number of works using population-based methods are built upon off-policy algorithms as agents in the population can share the samples and could be beneficial. Could authors build the method upon off-policy algorithms to further improve the applicability of RAP?\n\ne. For Figure 3, the performance gain over using a single adversary is not significant on HalfCheetah and Ant, and the results is not convincing enough to support the claim. \n\nAs the paper uses the population-based methods, it is also worth discussing its relation with Khadka et al. 2018, etc.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper542/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Reinforcement Learning using Adversarial Populations", "authorids": ["~Eugene_Vinitsky1", "yuqing_du@berkeley.edu", "~Kanaad_V_Parvate1", "~Kathy_Jang1", "~Pieter_Abbeel2", "~Alexandre_Bayen2"], "authors": ["Eugene Vinitsky", "Yuqing du", "Kanaad V Parvate", "Kathy Jang", "Pieter Abbeel", "Alexandre Bayen"], "keywords": ["Robust Control", "Reinforcement Learning", "Multiagent Systems"], "abstract": "Reinforcement Learning (RL) is an effective tool for controller design but can struggle with issues of robustness, failing catastrophically when the underlying system dynamics are perturbed. The Robust RL formulation tackles this by adding worst-case adversarial noise to the dynamics and constructing the noise distribution as the solution to a zero-sum minimax game. However, existing work on learning solutions to the Robust RL formulation has primarily focused on training a single RL agent against a single adversary. In this work, we demonstrate that using a single adversary does not consistently yield robustness to dynamics variations under standard parametrizations of the adversary; the resulting policy is highly exploitable by new adversaries. We propose a population-based augmentation to the Robust RL formulation in which we randomly initialize a population of adversaries and sample from the population uniformly during training. We empirically validate across a variety of benchmarks that the use of an adversarial population results in a less exploitable, more robust policy.  Finally, we demonstrate that this approach provides comparable robustness and generalization as domain randomization on these benchmarks while avoiding a ubiquitous domain randomization failure mode.", "one-sentence_summary": "We demonstrate that the standard robust RL formulation does not consistently yield robustness but that this can be alleviated using a population of adversaries.", "pdf": "/pdf/1e5388e0a8d4b52ab688222e9a6d499254c63e02.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vinitsky|robust_reinforcement_learning_using_adversarial_populations", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=r3UNEA7hm", "_bibtex": "@misc{\nvinitsky2021robust,\ntitle={Robust Reinforcement Learning using Adversarial Populations},\nauthor={Eugene Vinitsky and Yuqing du and Kanaad V Parvate and Kathy Jang and Pieter Abbeel and Alexandre Bayen},\nyear={2021},\nurl={https://openreview.net/forum?id=I6NRcao1w-X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "I6NRcao1w-X", "replyto": "I6NRcao1w-X", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper542/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538140809, "tmdate": 1606915787416, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper542/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper542/-/Official_Review"}}}, {"id": "4Sxhv4hLdA4", "original": null, "number": 17, "cdate": 1606266215414, "ddate": null, "tcdate": 1606266215414, "tmdate": 1606267253316, "tddate": null, "forum": "I6NRcao1w-X", "replyto": "QhiKuJ-3g5", "invitation": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment", "content": {"title": "Solving for Nash", "comment": "Thanks for the response. I think we are still unclear on the precise nature of the reviewers objection. It seems that the reviewer agrees that solving for Nash is an acceptable goal here.\n\nWould it clear things up in the paper if we stated more clearly that there are no guarantees (in some classes of problems) in getting a Nash equilibrium with gradient based methods and that we only are attempting (but not necessarily succeeding) in solving for Nash? We agree that naive gradient descent based methods have no guarantees of getting to Nash. Despite this, using gradient based methods in MARL systems is a standard approach despite the lack of theoretical guarantees. The RARL paper we are contrasting against is trying to solve for Nash and is at least finding a local equilibrium (since training appears to converge) and claiming that this local equilibrium (or the cycle around a local equilibrium in case we don't quite converge) is reasonably close in value to the global equilibrium. We are pointing out that this is not the case, that these local equilibria or cycles are extremely sub-optimal. Would it be helpful to just state that using this solution method is likely to lead to a local equilibrium or cycling behavior due to the use of gradient descent?\n\nWe are trying to understand if the objection is to the language in the paper, to the solution concept, or to the actual underlying algorithm. The above response reads to us like an issue of language that we are happy to amend to be clearer around the theoretical constraints of these approaches. If we've misunderstood, additional clarity would be really useful in helping us to write an improved paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper542/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Reinforcement Learning using Adversarial Populations", "authorids": ["~Eugene_Vinitsky1", "yuqing_du@berkeley.edu", "~Kanaad_V_Parvate1", "~Kathy_Jang1", "~Pieter_Abbeel2", "~Alexandre_Bayen2"], "authors": ["Eugene Vinitsky", "Yuqing du", "Kanaad V Parvate", "Kathy Jang", "Pieter Abbeel", "Alexandre Bayen"], "keywords": ["Robust Control", "Reinforcement Learning", "Multiagent Systems"], "abstract": "Reinforcement Learning (RL) is an effective tool for controller design but can struggle with issues of robustness, failing catastrophically when the underlying system dynamics are perturbed. The Robust RL formulation tackles this by adding worst-case adversarial noise to the dynamics and constructing the noise distribution as the solution to a zero-sum minimax game. However, existing work on learning solutions to the Robust RL formulation has primarily focused on training a single RL agent against a single adversary. In this work, we demonstrate that using a single adversary does not consistently yield robustness to dynamics variations under standard parametrizations of the adversary; the resulting policy is highly exploitable by new adversaries. We propose a population-based augmentation to the Robust RL formulation in which we randomly initialize a population of adversaries and sample from the population uniformly during training. We empirically validate across a variety of benchmarks that the use of an adversarial population results in a less exploitable, more robust policy.  Finally, we demonstrate that this approach provides comparable robustness and generalization as domain randomization on these benchmarks while avoiding a ubiquitous domain randomization failure mode.", "one-sentence_summary": "We demonstrate that the standard robust RL formulation does not consistently yield robustness but that this can be alleviated using a population of adversaries.", "pdf": "/pdf/1e5388e0a8d4b52ab688222e9a6d499254c63e02.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vinitsky|robust_reinforcement_learning_using_adversarial_populations", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=r3UNEA7hm", "_bibtex": "@misc{\nvinitsky2021robust,\ntitle={Robust Reinforcement Learning using Adversarial Populations},\nauthor={Eugene Vinitsky and Yuqing du and Kanaad V Parvate and Kathy Jang and Pieter Abbeel and Alexandre Bayen},\nyear={2021},\nurl={https://openreview.net/forum?id=I6NRcao1w-X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "I6NRcao1w-X", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper542/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper542/Authors|ICLR.cc/2021/Conference/Paper542/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869849, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment"}}}, {"id": "QhiKuJ-3g5", "original": null, "number": 16, "cdate": 1606264740038, "ddate": null, "tcdate": 1606264740038, "tmdate": 1606264740038, "tddate": null, "forum": "I6NRcao1w-X", "replyto": "9BpeBbEExeK", "invitation": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment", "content": {"title": "RE: Motivation is with respect to RARL ", "comment": "Thanks for the response. \n\nI totally agree with the effectiveness of the proposed approach in practice, which provides a better quality approximation to equilibriums (either maximin or Nash) in terms of numerical optimization results.  My concern was about the current overall problem formulation is misleading, where the authors confuse the ideal solution concept that the algorithm aims to achieve and its properties, with what can be realized in practice due to optimization difficulties in using gradient updates to solve equilibrium problems. This paper addresses the latter but incorrectly interprets the former in motivation and explanations throughout the paper. As fixing this would constitute a major revision of the paper's argument, I intend to keep my current decision. "}, "signatures": ["ICLR.cc/2021/Conference/Paper542/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Reinforcement Learning using Adversarial Populations", "authorids": ["~Eugene_Vinitsky1", "yuqing_du@berkeley.edu", "~Kanaad_V_Parvate1", "~Kathy_Jang1", "~Pieter_Abbeel2", "~Alexandre_Bayen2"], "authors": ["Eugene Vinitsky", "Yuqing du", "Kanaad V Parvate", "Kathy Jang", "Pieter Abbeel", "Alexandre Bayen"], "keywords": ["Robust Control", "Reinforcement Learning", "Multiagent Systems"], "abstract": "Reinforcement Learning (RL) is an effective tool for controller design but can struggle with issues of robustness, failing catastrophically when the underlying system dynamics are perturbed. The Robust RL formulation tackles this by adding worst-case adversarial noise to the dynamics and constructing the noise distribution as the solution to a zero-sum minimax game. However, existing work on learning solutions to the Robust RL formulation has primarily focused on training a single RL agent against a single adversary. In this work, we demonstrate that using a single adversary does not consistently yield robustness to dynamics variations under standard parametrizations of the adversary; the resulting policy is highly exploitable by new adversaries. We propose a population-based augmentation to the Robust RL formulation in which we randomly initialize a population of adversaries and sample from the population uniformly during training. We empirically validate across a variety of benchmarks that the use of an adversarial population results in a less exploitable, more robust policy.  Finally, we demonstrate that this approach provides comparable robustness and generalization as domain randomization on these benchmarks while avoiding a ubiquitous domain randomization failure mode.", "one-sentence_summary": "We demonstrate that the standard robust RL formulation does not consistently yield robustness but that this can be alleviated using a population of adversaries.", "pdf": "/pdf/1e5388e0a8d4b52ab688222e9a6d499254c63e02.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vinitsky|robust_reinforcement_learning_using_adversarial_populations", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=r3UNEA7hm", "_bibtex": "@misc{\nvinitsky2021robust,\ntitle={Robust Reinforcement Learning using Adversarial Populations},\nauthor={Eugene Vinitsky and Yuqing du and Kanaad V Parvate and Kathy Jang and Pieter Abbeel and Alexandre Bayen},\nyear={2021},\nurl={https://openreview.net/forum?id=I6NRcao1w-X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "I6NRcao1w-X", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper542/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper542/Authors|ICLR.cc/2021/Conference/Paper542/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869849, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment"}}}, {"id": "9BpeBbEExeK", "original": null, "number": 15, "cdate": 1606258259318, "ddate": null, "tcdate": 1606258259318, "tmdate": 1606263070824, "tddate": null, "forum": "I6NRcao1w-X", "replyto": "oJTo2y13-30", "invitation": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment", "content": {"title": "Motivation is with respect to RARL", "comment": "Dear reviewer,\nThank you for the comments. We would like to add the following clarifications to the discussion.\n\n> However, a Nash equilibrium is only stronger. It's \"both\" minimax and maximin. In other words, my previous comment would still apply. The description in the paper that the learner would overfit to the adversary and another adversary could introduce further perturbation for the learner is simply wrong on the conceptual level.\n\nThe RARL (Pinto et. al.)  framework, which we are building on, explicitly is attempting to solve for Nash (which as we point out in our rebuttal, would have the same value if the solution set includes mixed equilibria). More directly though, since we are contrasting with this setting, we focus on Nash and use similar gradient based techniques rather than bi-level optimization.\n>  In response to the point about \u201coverfitting\u201d being conceptually incorrect: \n\nA unimodal adversary will indeed cause the agent to \u201coverfit\u201d as shown in figure 2: at convergence agents trained against a single adversary **are not robust to adversaries from other seeds** even though those adversaries are technically in the training class.\nFor example, in a gridworld setting where the adversary places the goal state, a sharp unimodal representation would concentrate the goal placement into some particular quadrant of the world. Thus, at any moment in training the agent will have some belief about roughly where the goal should be hard-coded into its policy and will always attempt to go there first. This behavior would also consequently show up in testing settings. While we do not use the gridworld example directly in our paper, our robot vs. wind example has the same flavor.\n> Furthermore, gradient updates are not guaranteed to find Nash equilibriums and solving bilevel optimization in general has much stronger convergence guarantees (since it still minimization or maximization) than finding equilibriums).\n\nLike most RL methods with function approximation in continuous settings, there is no guarantee of finding Nash and converging. This is in general a problem with the entire field and does not apply uniquely to our paper.  We think that work on constructing convergence guarantees is very interesting and worth pursuing and intend to study convergence analysis of population based methods in future work. However, we do not think the absence of convergence guarantees weakens the point we raise about overfitting that we explicitly demonstrate in Figure 2 and supported elsewhere by our experimental results.\n\n> The proposed technique from my perspective is more an optimization heuristic to finding the equilibrium, which is similar to the mixing idea used to find equilibriums in the optimization and the regret minimization literature.\n\nOur technique is explicitly an optimization heuristic that performs well in these settings; we do not claim otherwise in our paper and try to be clear about this. However, this optimization heuristic performs well and would be a stronger baseline in future works than the current baseline of single-adversary RARL that appears in the literature. As we demonstrate, there is no additional computational cost to doing so and significant benefit since single adversary RARL (Fig. 2) does not at all achieve its stated goal.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper542/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Reinforcement Learning using Adversarial Populations", "authorids": ["~Eugene_Vinitsky1", "yuqing_du@berkeley.edu", "~Kanaad_V_Parvate1", "~Kathy_Jang1", "~Pieter_Abbeel2", "~Alexandre_Bayen2"], "authors": ["Eugene Vinitsky", "Yuqing du", "Kanaad V Parvate", "Kathy Jang", "Pieter Abbeel", "Alexandre Bayen"], "keywords": ["Robust Control", "Reinforcement Learning", "Multiagent Systems"], "abstract": "Reinforcement Learning (RL) is an effective tool for controller design but can struggle with issues of robustness, failing catastrophically when the underlying system dynamics are perturbed. The Robust RL formulation tackles this by adding worst-case adversarial noise to the dynamics and constructing the noise distribution as the solution to a zero-sum minimax game. However, existing work on learning solutions to the Robust RL formulation has primarily focused on training a single RL agent against a single adversary. In this work, we demonstrate that using a single adversary does not consistently yield robustness to dynamics variations under standard parametrizations of the adversary; the resulting policy is highly exploitable by new adversaries. We propose a population-based augmentation to the Robust RL formulation in which we randomly initialize a population of adversaries and sample from the population uniformly during training. We empirically validate across a variety of benchmarks that the use of an adversarial population results in a less exploitable, more robust policy.  Finally, we demonstrate that this approach provides comparable robustness and generalization as domain randomization on these benchmarks while avoiding a ubiquitous domain randomization failure mode.", "one-sentence_summary": "We demonstrate that the standard robust RL formulation does not consistently yield robustness but that this can be alleviated using a population of adversaries.", "pdf": "/pdf/1e5388e0a8d4b52ab688222e9a6d499254c63e02.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vinitsky|robust_reinforcement_learning_using_adversarial_populations", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=r3UNEA7hm", "_bibtex": "@misc{\nvinitsky2021robust,\ntitle={Robust Reinforcement Learning using Adversarial Populations},\nauthor={Eugene Vinitsky and Yuqing du and Kanaad V Parvate and Kathy Jang and Pieter Abbeel and Alexandre Bayen},\nyear={2021},\nurl={https://openreview.net/forum?id=I6NRcao1w-X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "I6NRcao1w-X", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper542/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper542/Authors|ICLR.cc/2021/Conference/Paper542/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869849, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment"}}}, {"id": "oJTo2y13-30", "original": null, "number": 14, "cdate": 1606245088676, "ddate": null, "tcdate": 1606245088676, "tmdate": 1606245088676, "tddate": null, "forum": "I6NRcao1w-X", "replyto": "3V1w9WdTcA2", "invitation": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment", "content": {"title": "Still unsolid motivation", "comment": "Thanks for the response. \n\nHowever, a Nash equilibrium is only stronger. It's \"both\" minimax and maximin. In other words, my previous comment would still apply. The description in the paper that the learner would overfit to the adversary and another adversary could introduce further perturbation for the learner is simply wrong on the conceptual level. Furthermore, gradient updates are not guaranteed to find Nash equilibriums and solving bilevel optimization in general has much stronger convergence guarantees (since it still minimization or maximization) than finding equilibriums). Finally, the authors should discuss conditions for the existence of (pure) Nash equilibriums.\n\nThe proposed technique from my perspective is more an optimization heuristic to finding the equilibrium, which is similar to the mixing idea used to find equilibriums in the optimization and the regret minimization literature. This paper needs a more solid theoretical foundation and review relevant literature mentioned above. I think the current handwavy interpretation is misleading to readers. \n\nSince this argument is used all over the paper, I decided to keep my original score."}, "signatures": ["ICLR.cc/2021/Conference/Paper542/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Reinforcement Learning using Adversarial Populations", "authorids": ["~Eugene_Vinitsky1", "yuqing_du@berkeley.edu", "~Kanaad_V_Parvate1", "~Kathy_Jang1", "~Pieter_Abbeel2", "~Alexandre_Bayen2"], "authors": ["Eugene Vinitsky", "Yuqing du", "Kanaad V Parvate", "Kathy Jang", "Pieter Abbeel", "Alexandre Bayen"], "keywords": ["Robust Control", "Reinforcement Learning", "Multiagent Systems"], "abstract": "Reinforcement Learning (RL) is an effective tool for controller design but can struggle with issues of robustness, failing catastrophically when the underlying system dynamics are perturbed. The Robust RL formulation tackles this by adding worst-case adversarial noise to the dynamics and constructing the noise distribution as the solution to a zero-sum minimax game. However, existing work on learning solutions to the Robust RL formulation has primarily focused on training a single RL agent against a single adversary. In this work, we demonstrate that using a single adversary does not consistently yield robustness to dynamics variations under standard parametrizations of the adversary; the resulting policy is highly exploitable by new adversaries. We propose a population-based augmentation to the Robust RL formulation in which we randomly initialize a population of adversaries and sample from the population uniformly during training. We empirically validate across a variety of benchmarks that the use of an adversarial population results in a less exploitable, more robust policy.  Finally, we demonstrate that this approach provides comparable robustness and generalization as domain randomization on these benchmarks while avoiding a ubiquitous domain randomization failure mode.", "one-sentence_summary": "We demonstrate that the standard robust RL formulation does not consistently yield robustness but that this can be alleviated using a population of adversaries.", "pdf": "/pdf/1e5388e0a8d4b52ab688222e9a6d499254c63e02.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vinitsky|robust_reinforcement_learning_using_adversarial_populations", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=r3UNEA7hm", "_bibtex": "@misc{\nvinitsky2021robust,\ntitle={Robust Reinforcement Learning using Adversarial Populations},\nauthor={Eugene Vinitsky and Yuqing du and Kanaad V Parvate and Kathy Jang and Pieter Abbeel and Alexandre Bayen},\nyear={2021},\nurl={https://openreview.net/forum?id=I6NRcao1w-X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "I6NRcao1w-X", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper542/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper542/Authors|ICLR.cc/2021/Conference/Paper542/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869849, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment"}}}, {"id": "w1P3IlRf0mX", "original": null, "number": 13, "cdate": 1606157815523, "ddate": null, "tcdate": 1606157815523, "tmdate": 1606157815523, "tddate": null, "forum": "I6NRcao1w-X", "replyto": "Y_KVkMV00e3", "invitation": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment", "content": {"title": "Rebuttal response: new experiments are solid, improve the argument of the paper", "comment": "> Re: what states are available to train adversarial and regular policies\n\nThanks for clarifying.\n\n> \"We find that once the adversary number increases significantly, the generalization performance starts to degrade.\"\n\nWhile not unexpected, this is a good result and what one would hope to observe.  The paper is certainly made stronger by including this sort of discussion.\n\n> \"We have added an additional set of experiments to the Appendix that we think supports our point that a single adversary is often insufficient.\"\n\nThe cup and bandit experiments are solid, and reinforce the experiments already presented in the main text.  Further details concerning the cup experiment would be good for a future version, as the short discussion made it challenging to understand the task at hand.\n\n> Overall response:\n\nI do agree with some of the reviewers that the formulation here is in some sense incremental.  However, I still feel that the experiments are solid, and that the new experiments show that this algorithm is quite effective, especially when DR fails.  For this reason, I will stick to my original score.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper542/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Reinforcement Learning using Adversarial Populations", "authorids": ["~Eugene_Vinitsky1", "yuqing_du@berkeley.edu", "~Kanaad_V_Parvate1", "~Kathy_Jang1", "~Pieter_Abbeel2", "~Alexandre_Bayen2"], "authors": ["Eugene Vinitsky", "Yuqing du", "Kanaad V Parvate", "Kathy Jang", "Pieter Abbeel", "Alexandre Bayen"], "keywords": ["Robust Control", "Reinforcement Learning", "Multiagent Systems"], "abstract": "Reinforcement Learning (RL) is an effective tool for controller design but can struggle with issues of robustness, failing catastrophically when the underlying system dynamics are perturbed. The Robust RL formulation tackles this by adding worst-case adversarial noise to the dynamics and constructing the noise distribution as the solution to a zero-sum minimax game. However, existing work on learning solutions to the Robust RL formulation has primarily focused on training a single RL agent against a single adversary. In this work, we demonstrate that using a single adversary does not consistently yield robustness to dynamics variations under standard parametrizations of the adversary; the resulting policy is highly exploitable by new adversaries. We propose a population-based augmentation to the Robust RL formulation in which we randomly initialize a population of adversaries and sample from the population uniformly during training. We empirically validate across a variety of benchmarks that the use of an adversarial population results in a less exploitable, more robust policy.  Finally, we demonstrate that this approach provides comparable robustness and generalization as domain randomization on these benchmarks while avoiding a ubiquitous domain randomization failure mode.", "one-sentence_summary": "We demonstrate that the standard robust RL formulation does not consistently yield robustness but that this can be alleviated using a population of adversaries.", "pdf": "/pdf/1e5388e0a8d4b52ab688222e9a6d499254c63e02.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vinitsky|robust_reinforcement_learning_using_adversarial_populations", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=r3UNEA7hm", "_bibtex": "@misc{\nvinitsky2021robust,\ntitle={Robust Reinforcement Learning using Adversarial Populations},\nauthor={Eugene Vinitsky and Yuqing du and Kanaad V Parvate and Kathy Jang and Pieter Abbeel and Alexandre Bayen},\nyear={2021},\nurl={https://openreview.net/forum?id=I6NRcao1w-X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "I6NRcao1w-X", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper542/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper542/Authors|ICLR.cc/2021/Conference/Paper542/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869849, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment"}}}, {"id": "584Irr-P0Li", "original": null, "number": 11, "cdate": 1606071700620, "ddate": null, "tcdate": 1606071700620, "tmdate": 1606071700620, "tddate": null, "forum": "I6NRcao1w-X", "replyto": "I6NRcao1w-X", "invitation": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment", "content": {"title": "After first revision", "comment": "Dear reviewers,\n\nWe sincerely appreciate the time and effort you took to review our paper.\n\nSince the second discussion phase will end soon, please let us know if you have any comments/concerns that we have not addressed to your satisfaction. We will be happy to clarify further and strengthen our paper.\n\nThank you very much!\n\nAuthors\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper542/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper542/Reviewers", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Reinforcement Learning using Adversarial Populations", "authorids": ["~Eugene_Vinitsky1", "yuqing_du@berkeley.edu", "~Kanaad_V_Parvate1", "~Kathy_Jang1", "~Pieter_Abbeel2", "~Alexandre_Bayen2"], "authors": ["Eugene Vinitsky", "Yuqing du", "Kanaad V Parvate", "Kathy Jang", "Pieter Abbeel", "Alexandre Bayen"], "keywords": ["Robust Control", "Reinforcement Learning", "Multiagent Systems"], "abstract": "Reinforcement Learning (RL) is an effective tool for controller design but can struggle with issues of robustness, failing catastrophically when the underlying system dynamics are perturbed. The Robust RL formulation tackles this by adding worst-case adversarial noise to the dynamics and constructing the noise distribution as the solution to a zero-sum minimax game. However, existing work on learning solutions to the Robust RL formulation has primarily focused on training a single RL agent against a single adversary. In this work, we demonstrate that using a single adversary does not consistently yield robustness to dynamics variations under standard parametrizations of the adversary; the resulting policy is highly exploitable by new adversaries. We propose a population-based augmentation to the Robust RL formulation in which we randomly initialize a population of adversaries and sample from the population uniformly during training. We empirically validate across a variety of benchmarks that the use of an adversarial population results in a less exploitable, more robust policy.  Finally, we demonstrate that this approach provides comparable robustness and generalization as domain randomization on these benchmarks while avoiding a ubiquitous domain randomization failure mode.", "one-sentence_summary": "We demonstrate that the standard robust RL formulation does not consistently yield robustness but that this can be alleviated using a population of adversaries.", "pdf": "/pdf/1e5388e0a8d4b52ab688222e9a6d499254c63e02.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vinitsky|robust_reinforcement_learning_using_adversarial_populations", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=r3UNEA7hm", "_bibtex": "@misc{\nvinitsky2021robust,\ntitle={Robust Reinforcement Learning using Adversarial Populations},\nauthor={Eugene Vinitsky and Yuqing du and Kanaad V Parvate and Kathy Jang and Pieter Abbeel and Alexandre Bayen},\nyear={2021},\nurl={https://openreview.net/forum?id=I6NRcao1w-X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "I6NRcao1w-X", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper542/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper542/Authors|ICLR.cc/2021/Conference/Paper542/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869849, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment"}}}, {"id": "vWhPaPbY8OF", "original": null, "number": 7, "cdate": 1605504110274, "ddate": null, "tcdate": 1605504110274, "tmdate": 1606006206886, "tddate": null, "forum": "I6NRcao1w-X", "replyto": "rY8B3gLhFbN", "invitation": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment", "content": {"title": "Rebuttal Part 2", "comment": "> e. For Figure 3, the performance gain over using a single adversary is not significant on HalfCheetah and Ant, and the results is not convincing enough to support the claim.\n\nA. We have added an additional non-walking experiment in continuous control environments and a bandit environment that demonstrate the value of this approach in the Appendix. We believe the challenge is that several of these mujoco environments, in particular ant, are inherently quite stable and are not particularly affected by the perturbations we apply whereas hopper is a bit more unstable due to the single contact point. Thus, there is little performance improvement in robustness when adversaries are added in the first place and not much room for improvement from additional adversaries in the two non-hopper experiments. To address this point directly, we have also added a ball-in-cup task  and as an example that we think makes the failure modes of a single adversary clearer, we have added a 10 dimensional Bernoulli bandit task where each adversary outputs a bandit instance that the agent is allowed to play in. Here the use of a single adversary corresponds to a single bandit instance and the issue of overfitting is clear in that the agent will memorize the single bandit instance whereas the generalization to new instances is better when multiple adversaries are used. This example also makes clear the challenge of domain randomization in high dimensions, namely that most samples are uninformative when sampled uniformly. Consequently, the domain randomization trained policy completely fails to sample arms appropriately on one of our test bandit instances though it looks superficially good on uniformly sampled instances; in contrast the agent trained against adversarially selected bandit instances is robust on our tests. \n\n>As the paper uses the population-based methods, it is also worth discussing its relation with Khadka et al. 2018, etc.\n\nA: We are not sure which paper the reviewer is referencing but we would love to read it and appropriately reference it. Would you mind providing the full reference?\n\nFinally, we have slightly re-written the experiments section of the paper to clarify which tasks are in-distribution vs. out-of-distribution. Playing against tasks generated by our adversary class are in-distribution, whereas playing against mass and friction variations is out-of-distribution. Playing an adversary only provides guarantees on in-distribution performance and Figure 2 now shows that agents trained against a single adversary do not inherit this in-distribution performance guarantee for the reasons identified in the introduction. Additionally, we now test how domain randomization does on these in-distribution tasks and show that domain randomization does not provide robustness on these tasks either.\nAs the discussion of failure modes of domain randomization is interesting, but not the key contribution of the paper, we have moved the section formerly called \"Hypothesis 3\" into the appendix and direct readers there in the experiments section.\nTo summarize, we have made the following changes and additions (highlighted in red in the latest paper version):\n- Compared the performance of agents across different adversary training regimes (agent trained against 1 adv vs. adversaries from the 3 adversary case, domain randomization agents against 1 and 3 adversaries, etc.). This is summarized in Fig. 2.\n- We have added additional experiments on bandit tasks and a Deepmind control ball-in-cup task to demonstrate our method in more varied domains.\n- We have rewritten the experiment section to indicate which tasks are in-distribution (for which we have lower bounded performance guarantees from adversarial training) vs. out-of-distribution (for which performance improvements are plausible but not guaranteed).\n- We have moved the discussion on failure modes of domain randomization to the appendix and added an additional example of a failure mode of domain randomization.\n- Edits to respond to particular writing issues / notation brought up by the reviewers.\n\nWe hope that these changes address the main concerns brought up by the reviewers and are happy to respond to any further questions. "}, "signatures": ["ICLR.cc/2021/Conference/Paper542/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Reinforcement Learning using Adversarial Populations", "authorids": ["~Eugene_Vinitsky1", "yuqing_du@berkeley.edu", "~Kanaad_V_Parvate1", "~Kathy_Jang1", "~Pieter_Abbeel2", "~Alexandre_Bayen2"], "authors": ["Eugene Vinitsky", "Yuqing du", "Kanaad V Parvate", "Kathy Jang", "Pieter Abbeel", "Alexandre Bayen"], "keywords": ["Robust Control", "Reinforcement Learning", "Multiagent Systems"], "abstract": "Reinforcement Learning (RL) is an effective tool for controller design but can struggle with issues of robustness, failing catastrophically when the underlying system dynamics are perturbed. The Robust RL formulation tackles this by adding worst-case adversarial noise to the dynamics and constructing the noise distribution as the solution to a zero-sum minimax game. However, existing work on learning solutions to the Robust RL formulation has primarily focused on training a single RL agent against a single adversary. In this work, we demonstrate that using a single adversary does not consistently yield robustness to dynamics variations under standard parametrizations of the adversary; the resulting policy is highly exploitable by new adversaries. We propose a population-based augmentation to the Robust RL formulation in which we randomly initialize a population of adversaries and sample from the population uniformly during training. We empirically validate across a variety of benchmarks that the use of an adversarial population results in a less exploitable, more robust policy.  Finally, we demonstrate that this approach provides comparable robustness and generalization as domain randomization on these benchmarks while avoiding a ubiquitous domain randomization failure mode.", "one-sentence_summary": "We demonstrate that the standard robust RL formulation does not consistently yield robustness but that this can be alleviated using a population of adversaries.", "pdf": "/pdf/1e5388e0a8d4b52ab688222e9a6d499254c63e02.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vinitsky|robust_reinforcement_learning_using_adversarial_populations", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=r3UNEA7hm", "_bibtex": "@misc{\nvinitsky2021robust,\ntitle={Robust Reinforcement Learning using Adversarial Populations},\nauthor={Eugene Vinitsky and Yuqing du and Kanaad V Parvate and Kathy Jang and Pieter Abbeel and Alexandre Bayen},\nyear={2021},\nurl={https://openreview.net/forum?id=I6NRcao1w-X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "I6NRcao1w-X", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper542/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper542/Authors|ICLR.cc/2021/Conference/Paper542/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869849, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment"}}}, {"id": "ofSjCEwNT3p", "original": null, "number": 9, "cdate": 1606006187358, "ddate": null, "tcdate": 1606006187358, "tmdate": 1606006187358, "tddate": null, "forum": "I6NRcao1w-X", "replyto": "Y_KVkMV00e3", "invitation": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment", "content": {"title": "Summary of changes", "comment": "Finally, we have slightly re-written the experiments section of the paper to clarify which tasks are in-distribution vs. out-of-distribution. Playing against tasks generated by our adversary class are in-distribution, whereas playing against mass and friction variations is out-of-distribution. Playing an adversary only provides guarantees on in-distribution performance and Figure 2 now shows that agents trained against a single adversary do not inherit this in-distribution performance guarantee for the reasons identified in the introduction. Additionally, we now test how domain randomization does on these in-distribution tasks and show that domain randomization does not provide robustness on these tasks either.\nAs the discussion of failure modes of domain randomization is interesting, but not the key contribution of the paper, we have moved the section formerly called \"Hypothesis 3\" into the appendix and direct readers there in the experiments section.\nTo summarize, we have made the following changes and additions (highlighted in red in the latest paper version):\n- Compared the performance of agents across different adversary training regimes (agent trained against 1 adv vs. adversaries from the 3 adversary case, domain randomization agents against 1 and 3 adversaries, etc.). This is summarized in Fig. 2.\n- We have added additional experiments on bandit tasks and a Deepmind control ball-in-cup task to demonstrate our method in more varied domains.\n- We have rewritten the experiment section to indicate which tasks are in-distribution (for which we have lower bounded performance guarantees from adversarial training) vs. out-of-distribution (for which performance improvements are plausible but not guaranteed).\n- We have moved the discussion on failure modes of domain randomization to the appendix and added an additional example of a failure mode of domain randomization.\n- Edits to respond to particular writing issues / notation brought up by the reviewers.\n\nWe hope that these changes address the main concerns brought up by the reviewers and are happy to respond to any further questions. "}, "signatures": ["ICLR.cc/2021/Conference/Paper542/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Reinforcement Learning using Adversarial Populations", "authorids": ["~Eugene_Vinitsky1", "yuqing_du@berkeley.edu", "~Kanaad_V_Parvate1", "~Kathy_Jang1", "~Pieter_Abbeel2", "~Alexandre_Bayen2"], "authors": ["Eugene Vinitsky", "Yuqing du", "Kanaad V Parvate", "Kathy Jang", "Pieter Abbeel", "Alexandre Bayen"], "keywords": ["Robust Control", "Reinforcement Learning", "Multiagent Systems"], "abstract": "Reinforcement Learning (RL) is an effective tool for controller design but can struggle with issues of robustness, failing catastrophically when the underlying system dynamics are perturbed. The Robust RL formulation tackles this by adding worst-case adversarial noise to the dynamics and constructing the noise distribution as the solution to a zero-sum minimax game. However, existing work on learning solutions to the Robust RL formulation has primarily focused on training a single RL agent against a single adversary. In this work, we demonstrate that using a single adversary does not consistently yield robustness to dynamics variations under standard parametrizations of the adversary; the resulting policy is highly exploitable by new adversaries. We propose a population-based augmentation to the Robust RL formulation in which we randomly initialize a population of adversaries and sample from the population uniformly during training. We empirically validate across a variety of benchmarks that the use of an adversarial population results in a less exploitable, more robust policy.  Finally, we demonstrate that this approach provides comparable robustness and generalization as domain randomization on these benchmarks while avoiding a ubiquitous domain randomization failure mode.", "one-sentence_summary": "We demonstrate that the standard robust RL formulation does not consistently yield robustness but that this can be alleviated using a population of adversaries.", "pdf": "/pdf/1e5388e0a8d4b52ab688222e9a6d499254c63e02.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vinitsky|robust_reinforcement_learning_using_adversarial_populations", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=r3UNEA7hm", "_bibtex": "@misc{\nvinitsky2021robust,\ntitle={Robust Reinforcement Learning using Adversarial Populations},\nauthor={Eugene Vinitsky and Yuqing du and Kanaad V Parvate and Kathy Jang and Pieter Abbeel and Alexandre Bayen},\nyear={2021},\nurl={https://openreview.net/forum?id=I6NRcao1w-X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "I6NRcao1w-X", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper542/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper542/Authors|ICLR.cc/2021/Conference/Paper542/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869849, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment"}}}, {"id": "Y_KVkMV00e3", "original": null, "number": 4, "cdate": 1605502531925, "ddate": null, "tcdate": 1605502531925, "tmdate": 1606006166015, "tddate": null, "forum": "I6NRcao1w-X", "replyto": "13vwhyz4HtM", "invitation": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment", "content": {"title": "Notation mistakes corrected, additional experiments added, MDP formulation added, review appreciated!", "comment": "We thank the reviewer for their detailed and thoughtful review, and are happy that the reviewer finds our contribution compelling and interesting. We address some main concerns as follows: \n\n> \u201cI felt the paper was a bit unclear about what states are available to train adversarial and regular policies. That is, it is unclear whether in the trajectories and , the agents/adversaries observe their own actions, the actions of their counterpart (e.g. the agent observing the adversaries action), or both agent and adversary observing the perturbed action \u201d\n\nDue to limited space, we had moved the formulation of the MDP into Appendix A. To clarify, the agent sees its observation and its own prior action which gives it some amount of adaptability to the mismatched dynamics. The adversary only sees the agent state as we want the adversaries to represent a mismatched dynamical system. Allowing the adversary to also see the agent action would be similar to conditioning the adversary directly on the agent policy, which is not often a characteristic of physical dynamical systems. \n\n>Further study should be done as to how many adversaries one needs to provide meaningful levels of robustness. It seems that in different experiments, 1, 3, and 5 adversaries were considered. How should one decided how many adversaries to use?\n\nDue to a lack of space, we are not able to put all the graphs that support this point into the main body of the text. However, we find empirically that three adversaries is usually optimal for the domains we consider and show a sweep from 0 - 11 adversaries for the hopper domain in the Appendix B . We find that once the adversary number increases significantly, the generalization performance starts to degrade. We hypothesize that this is due to the decreasing total number of samples per iteration allotted to each adversary. We note, however, that since the goal of the adversaries is to cover possible failure modes and approximate mixed nash, more than three adversaries may be needed for problems with many failure modes.\n\n> \u201cOne weakness is that only one scenario (e.g. walking within these three environments) was considered. It seems that the claims of the paper could be strengthened if more environments/tasks were considered.\u201d \n\nWe have added an additional set of experiments to the Appendix that we think supports our point that a single adversary is often insufficient. We have added a robotic arm task involving catching a ball in a cup and a Bernoulli bandit experiment in which the adversary takes in a zero vector and outputs the Bernoulli parameter p for each of ten arms. In particular, we would like to highlight that the bandit experiment makes very clear some of the weaknesses of domain randomization, as shown in Figure 15. Namely, that DR performs superficially well on uniformly sampled tasks (here evenly spread corresponds to just sampling the value p from the uniform distribution which is exactly the sampling distribution we use for domain randomization) but can fail catastrophically on hard tasks that are not often surfaced by uniform sampling (here \u201cone good arm\u201d which corresponds to one arm having high bernoulli value p and the others all having very low value). \n\n**Notation**\nWe appreciate the reviewer for catching the mistakes in reward function notation and rollout length. We have corrected these in the new uploaded version (in red) and appreciate the careful readthrough. \nWe chose to bold only the proposed method as domain randomization is technically outside the class of perturbations that our adversaries can construct and is consequently not a fair baseline. We provide domain randomization as an unfair baseline (essentially more of an oracle) to explicitly demonstrate limitations of our approach and show the possibility of improved performance if adversaries were constructed that somehow included the perturbations generated by domain randomization in their class of perturbations. We also wanted to include domain randomization as a way of pointing out limitations in this work, namely, that for the studied domains there are obvious domain randomizations that perform well (albeit with some downsides discussed in the paper). However, we understand that the presentation is unclear and have amended the table to show both best result overall and best within the adversarial methods. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper542/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Reinforcement Learning using Adversarial Populations", "authorids": ["~Eugene_Vinitsky1", "yuqing_du@berkeley.edu", "~Kanaad_V_Parvate1", "~Kathy_Jang1", "~Pieter_Abbeel2", "~Alexandre_Bayen2"], "authors": ["Eugene Vinitsky", "Yuqing du", "Kanaad V Parvate", "Kathy Jang", "Pieter Abbeel", "Alexandre Bayen"], "keywords": ["Robust Control", "Reinforcement Learning", "Multiagent Systems"], "abstract": "Reinforcement Learning (RL) is an effective tool for controller design but can struggle with issues of robustness, failing catastrophically when the underlying system dynamics are perturbed. The Robust RL formulation tackles this by adding worst-case adversarial noise to the dynamics and constructing the noise distribution as the solution to a zero-sum minimax game. However, existing work on learning solutions to the Robust RL formulation has primarily focused on training a single RL agent against a single adversary. In this work, we demonstrate that using a single adversary does not consistently yield robustness to dynamics variations under standard parametrizations of the adversary; the resulting policy is highly exploitable by new adversaries. We propose a population-based augmentation to the Robust RL formulation in which we randomly initialize a population of adversaries and sample from the population uniformly during training. We empirically validate across a variety of benchmarks that the use of an adversarial population results in a less exploitable, more robust policy.  Finally, we demonstrate that this approach provides comparable robustness and generalization as domain randomization on these benchmarks while avoiding a ubiquitous domain randomization failure mode.", "one-sentence_summary": "We demonstrate that the standard robust RL formulation does not consistently yield robustness but that this can be alleviated using a population of adversaries.", "pdf": "/pdf/1e5388e0a8d4b52ab688222e9a6d499254c63e02.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vinitsky|robust_reinforcement_learning_using_adversarial_populations", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=r3UNEA7hm", "_bibtex": "@misc{\nvinitsky2021robust,\ntitle={Robust Reinforcement Learning using Adversarial Populations},\nauthor={Eugene Vinitsky and Yuqing du and Kanaad V Parvate and Kathy Jang and Pieter Abbeel and Alexandre Bayen},\nyear={2021},\nurl={https://openreview.net/forum?id=I6NRcao1w-X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "I6NRcao1w-X", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper542/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper542/Authors|ICLR.cc/2021/Conference/Paper542/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869849, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment"}}}, {"id": "3V1w9WdTcA2", "original": null, "number": 6, "cdate": 1605504017274, "ddate": null, "tcdate": 1605504017274, "tmdate": 1606006144607, "tddate": null, "forum": "I6NRcao1w-X", "replyto": "ltkTgMokWXJ", "invitation": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment", "content": {"title": "Rebuttal Part 2", "comment": ">\u201dIn figure 2, I think a fairer comparison should let agents of both sides face the same adversary\u201d\n\nAs the reviewer suggested, we also ran a more fair comparison of testing agents against the same adversary. We have uploaded a new version of Figure 2 that also includes the agent trained against 1 adversary playing the set of 3 adversaries and vice versa. The results show that the agent trained against 1 adversary performs terribly against the set of 3 adversaries while the reverse is not true. This suggests that the improved performance on the right hand side of Fig 2 (top right) is not due to weaker adversaries but due to a more robust agent.\n\n>\u201dI cannot find the description of the exact architecture used in the paper, but can you try training with a more expressive policy and see if domain randomization still perform worse than the direct training without the adversary?\u201d\n\nTo clarify, all our agent and adversary policies consists of two layers with 64 units with a Tanh non-linearity. We assume that by a more expressive policy the reviewer meant a half-cheetah example in which the agent trained with DR has memory and could potentially identify which (friction, mass) pair it is operating in and subsequently perform optimally in this instance. We have provided additional experiments in Appendix E and argue that a more expressive policy does not alleviate a weakness of domain randomization being an expectation over the uniformly sampled (friction, mass) pairs. As an illustrative example, consider if one of the (friction, mass) pairs yielded an arbitrarily high score (eg. 1e20), and there was some minor cost associated with taking a few steps to identify which instance you were in, the agent would choose to not identify the (friction, mass) pair and focus instead on picking a policy that consistently yielded the high score. While this is an extreme example, we hope it clarifies why domain randomization is not guaranteed to yield robust behavior and do not see a resolution to the problem as a result of a more expressive policy. However, if the reviewer meant something else or if there are further questions and concerns, please let us know. \n\nFinally, we have slightly re-written the experiments section of the paper to clarify which tasks are in-distribution vs. out-of-distribution. Playing against tasks generated by our adversary class are in-distribution, whereas playing against mass and friction variations is out-of-distribution. Playing an adversary only provides guarantees on in-distribution performance and Figure 2 now shows that agents trained against a single adversary do not inherit this in-distribution performance guarantee for the reasons identified in the introduction. Additionally, we now test how domain randomization does on these in-distribution tasks and show that domain randomization does not provide robustness on these tasks either.\nAs the discussion of failure modes of domain randomization is interesting, but not the key contribution of the paper, we have moved the section formerly called \"Hypothesis 3\" into the appendix and direct readers there in the experiments section.\nTo summarize, we have made the following changes and additions (highlighted in red in the latest paper version):\n- Compared the performance of agents across different adversary training regimes (agent trained against 1 adv vs. adversaries from the 3 adversary case, domain randomization agents against 1 and 3 adversaries, etc.). This is summarized in Fig. 2.\n- We have added additional experiments on bandit tasks and a Deepmind control ball-in-cup task to demonstrate our method in more varied domains.\n- We have rewritten the experiment section to indicate which tasks are in-distribution (for which we have lower bounded performance guarantees from adversarial training) vs. out-of-distribution (for which performance improvements are plausible but not guaranteed).\n- We have moved the discussion on failure modes of domain randomization to the appendix and added an additional example of a failure mode of domain randomization.\n- Edits to respond to particular writing issues / notation brought up by the reviewers.\n\nWe hope that these changes address the main concerns brought up by the reviewers and are happy to respond to any further questions. "}, "signatures": ["ICLR.cc/2021/Conference/Paper542/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Reinforcement Learning using Adversarial Populations", "authorids": ["~Eugene_Vinitsky1", "yuqing_du@berkeley.edu", "~Kanaad_V_Parvate1", "~Kathy_Jang1", "~Pieter_Abbeel2", "~Alexandre_Bayen2"], "authors": ["Eugene Vinitsky", "Yuqing du", "Kanaad V Parvate", "Kathy Jang", "Pieter Abbeel", "Alexandre Bayen"], "keywords": ["Robust Control", "Reinforcement Learning", "Multiagent Systems"], "abstract": "Reinforcement Learning (RL) is an effective tool for controller design but can struggle with issues of robustness, failing catastrophically when the underlying system dynamics are perturbed. The Robust RL formulation tackles this by adding worst-case adversarial noise to the dynamics and constructing the noise distribution as the solution to a zero-sum minimax game. However, existing work on learning solutions to the Robust RL formulation has primarily focused on training a single RL agent against a single adversary. In this work, we demonstrate that using a single adversary does not consistently yield robustness to dynamics variations under standard parametrizations of the adversary; the resulting policy is highly exploitable by new adversaries. We propose a population-based augmentation to the Robust RL formulation in which we randomly initialize a population of adversaries and sample from the population uniformly during training. We empirically validate across a variety of benchmarks that the use of an adversarial population results in a less exploitable, more robust policy.  Finally, we demonstrate that this approach provides comparable robustness and generalization as domain randomization on these benchmarks while avoiding a ubiquitous domain randomization failure mode.", "one-sentence_summary": "We demonstrate that the standard robust RL formulation does not consistently yield robustness but that this can be alleviated using a population of adversaries.", "pdf": "/pdf/1e5388e0a8d4b52ab688222e9a6d499254c63e02.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vinitsky|robust_reinforcement_learning_using_adversarial_populations", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=r3UNEA7hm", "_bibtex": "@misc{\nvinitsky2021robust,\ntitle={Robust Reinforcement Learning using Adversarial Populations},\nauthor={Eugene Vinitsky and Yuqing du and Kanaad V Parvate and Kathy Jang and Pieter Abbeel and Alexandre Bayen},\nyear={2021},\nurl={https://openreview.net/forum?id=I6NRcao1w-X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "I6NRcao1w-X", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper542/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper542/Authors|ICLR.cc/2021/Conference/Paper542/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869849, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment"}}}, {"id": "qBMXqL73SFy", "original": null, "number": 2, "cdate": 1605502126387, "ddate": null, "tcdate": 1605502126387, "tmdate": 1606006101722, "tddate": null, "forum": "I6NRcao1w-X", "replyto": "OCTwp4ylrkl", "invitation": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment", "content": {"title": "Main contribution is pointing out flaws in an existing framework", "comment": "We thank the reviewer for their thoughtful feedback and constructive comments and are happy the reviewer found our experimental results solid. We agree with the reviewer that the conceptual novelty of this paper is limited but would like to clarify that conceptual novelty is not the main contribution of our paper. **In our view, the main contribution of this work is pointing out serious limitations in the use of a single adversary in continuous control tasks where the parametrization of the adversary is unimodal.** This parametrization is both used in the original, influential Robust Adversarial Reinforcement Learning framework and also appears as a baseline in many works (see [1], [2] as examples). As the reviewer points out, this is in contrast to the case in robust control where an entire class of structured perturbations is simultaneously considered. We view our work as explicitly drawing this connection and pointing out that constructing an agent that is robust against a single perturbation from the class is not the correct notion of robustness.\n\nWithin the Robust Adversarial Reinforcement Learning framework and related frameworks, it is common to use a single adversary and as both Fig. 2 and our exposition point out, this does not actually yield robustness to perturbations that are within the class of training perturbations. Our contribution is noting the absence of robustness from a single uni-modal adversary and proposing a simple solution that leads to robustness in the domains that we examine. To further this point, the rebuttal version that we have uploaded has additional experiments in the appendix (highlighted in red) demonstrating the need for multiple adversaries in additional, more diverse domains: catching a ball in a cup and a bernoulli bandit task. Seeing as the training mechanism that we are responding to, Robust Adversarial Reinforcement Learning, is an influential work in the robust RL literature, we think that pointing out key flaws in the approach and pointing out that they can be approximately fixed with simple mechanisms is a valuable contribution. \n\n> \"One can augment all the adversary networks as a big network and then the problem formulation is the same as before. If we think each phi_i is a block in this big \"single adversary network\", then what the authors have done can be thought as doing block coordinate descent.\"\n\nThe point the reviewer raises about population based approaches being equivalent to a larger neural network with a categorical variable selecting which subnetwork is currently activated is an interesting point. Essentially, our population based approach is intended to roughly approximate a multi-modal policy but a large network with smaller subnetworks that are activated by some criterion is another way of representing multi-modality. As we discuss in the future work section, we also think it would be valuable to explore direct multi-modal adversary policies rather than the population based approach. However, it can be difficult to train policies that appropriately randomize as naive learning dynamics can cause agents to cycle through pure policies even when the Nash equilibrium is mixed (see \u201cLearning with opponent-learning awareness\u201d by Foerster et al for examples and possible solutions). Since we did not want to add additional training techniques to induce our adversaries to adopt a mixed nash policy directly, we suggest population based training as a simple alternative. \n\n> \u201cFrom this perspective, there is not too much conceptual novelty, and the main contribution of this paper is doing some more detailed study showing how to combine augmentation and adversarial RL.\u201d \n\nWe are unsure what is meant by \u201caugmentation and adversarial RL\u201d as we are not using augmentations in this work. Could the reviewer clarify this statement?\n\nPlease let us know if you have any additional comments or concerns. Thank you! \n\n[1] Ma, Xiaobai, Katherine Driggs-Campbell, and Mykel J. Kochenderfer. \"Improved robustness and safety for autonomous vehicle control with adversarial reinforcement learning.\" 2018 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2018.\n[2] Kumar, Saurabh, et al. \"One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL.\" Advances in Neural Information Processing Systems 33 (2020).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper542/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Reinforcement Learning using Adversarial Populations", "authorids": ["~Eugene_Vinitsky1", "yuqing_du@berkeley.edu", "~Kanaad_V_Parvate1", "~Kathy_Jang1", "~Pieter_Abbeel2", "~Alexandre_Bayen2"], "authors": ["Eugene Vinitsky", "Yuqing du", "Kanaad V Parvate", "Kathy Jang", "Pieter Abbeel", "Alexandre Bayen"], "keywords": ["Robust Control", "Reinforcement Learning", "Multiagent Systems"], "abstract": "Reinforcement Learning (RL) is an effective tool for controller design but can struggle with issues of robustness, failing catastrophically when the underlying system dynamics are perturbed. The Robust RL formulation tackles this by adding worst-case adversarial noise to the dynamics and constructing the noise distribution as the solution to a zero-sum minimax game. However, existing work on learning solutions to the Robust RL formulation has primarily focused on training a single RL agent against a single adversary. In this work, we demonstrate that using a single adversary does not consistently yield robustness to dynamics variations under standard parametrizations of the adversary; the resulting policy is highly exploitable by new adversaries. We propose a population-based augmentation to the Robust RL formulation in which we randomly initialize a population of adversaries and sample from the population uniformly during training. We empirically validate across a variety of benchmarks that the use of an adversarial population results in a less exploitable, more robust policy.  Finally, we demonstrate that this approach provides comparable robustness and generalization as domain randomization on these benchmarks while avoiding a ubiquitous domain randomization failure mode.", "one-sentence_summary": "We demonstrate that the standard robust RL formulation does not consistently yield robustness but that this can be alleviated using a population of adversaries.", "pdf": "/pdf/1e5388e0a8d4b52ab688222e9a6d499254c63e02.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vinitsky|robust_reinforcement_learning_using_adversarial_populations", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=r3UNEA7hm", "_bibtex": "@misc{\nvinitsky2021robust,\ntitle={Robust Reinforcement Learning using Adversarial Populations},\nauthor={Eugene Vinitsky and Yuqing du and Kanaad V Parvate and Kathy Jang and Pieter Abbeel and Alexandre Bayen},\nyear={2021},\nurl={https://openreview.net/forum?id=I6NRcao1w-X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "I6NRcao1w-X", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper542/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper542/Authors|ICLR.cc/2021/Conference/Paper542/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869849, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment"}}}, {"id": "Z7C1EW5zyP2", "original": null, "number": 8, "cdate": 1606006073785, "ddate": null, "tcdate": 1606006073785, "tmdate": 1606006073785, "tddate": null, "forum": "I6NRcao1w-X", "replyto": "qBMXqL73SFy", "invitation": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment", "content": {"title": "Additional changes + summary", "comment": "Finally, we have slightly re-written the experiments section of the paper to clarify which tasks are in-distribution vs. out-of-distribution. Playing against tasks generated by our adversary class are in-distribution, whereas playing against mass and friction variations is out-of-distribution. Playing an adversary only provides guarantees on in-distribution performance and Figure 2 now shows that agents trained against a single adversary do not inherit this in-distribution performance guarantee for the reasons identified in the introduction. Additionally, we now test how domain randomization does on these in-distribution tasks and show that domain randomization does not provide robustness on these tasks either.\nAs the discussion of failure modes of domain randomization is interesting, but not the key contribution of the paper, we have moved the section formerly called \"Hypothesis 3\" into the appendix and direct readers there in the experiments section.\nTo summarize, we have made the following changes and additions (highlighted in red in the latest paper version):\n- Compared the performance of agents across different adversary training regimes (agent trained against 1 adv vs. adversaries from the 3 adversary case, domain randomization agents against 1 and 3 adversaries, etc.). This is summarized in Fig. 2.\n- We have added additional experiments on bandit tasks and a Deepmind control ball-in-cup task to demonstrate our method in more varied domains.\n- We have rewritten the experiment section to indicate which tasks are in-distribution (for which we have lower bounded performance guarantees from adversarial training) vs. out-of-distribution (for which performance improvements are plausible but not guaranteed).\n- We have moved the discussion on failure modes of domain randomization to the appendix and added an additional example of a failure mode of domain randomization.\n- Edits to respond to particular writing issues / notation brought up by the reviewers.\n\nWe hope that these changes address the main concerns brought up by the reviewers and are happy to respond to any further questions. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper542/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Reinforcement Learning using Adversarial Populations", "authorids": ["~Eugene_Vinitsky1", "yuqing_du@berkeley.edu", "~Kanaad_V_Parvate1", "~Kathy_Jang1", "~Pieter_Abbeel2", "~Alexandre_Bayen2"], "authors": ["Eugene Vinitsky", "Yuqing du", "Kanaad V Parvate", "Kathy Jang", "Pieter Abbeel", "Alexandre Bayen"], "keywords": ["Robust Control", "Reinforcement Learning", "Multiagent Systems"], "abstract": "Reinforcement Learning (RL) is an effective tool for controller design but can struggle with issues of robustness, failing catastrophically when the underlying system dynamics are perturbed. The Robust RL formulation tackles this by adding worst-case adversarial noise to the dynamics and constructing the noise distribution as the solution to a zero-sum minimax game. However, existing work on learning solutions to the Robust RL formulation has primarily focused on training a single RL agent against a single adversary. In this work, we demonstrate that using a single adversary does not consistently yield robustness to dynamics variations under standard parametrizations of the adversary; the resulting policy is highly exploitable by new adversaries. We propose a population-based augmentation to the Robust RL formulation in which we randomly initialize a population of adversaries and sample from the population uniformly during training. We empirically validate across a variety of benchmarks that the use of an adversarial population results in a less exploitable, more robust policy.  Finally, we demonstrate that this approach provides comparable robustness and generalization as domain randomization on these benchmarks while avoiding a ubiquitous domain randomization failure mode.", "one-sentence_summary": "We demonstrate that the standard robust RL formulation does not consistently yield robustness but that this can be alleviated using a population of adversaries.", "pdf": "/pdf/1e5388e0a8d4b52ab688222e9a6d499254c63e02.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vinitsky|robust_reinforcement_learning_using_adversarial_populations", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=r3UNEA7hm", "_bibtex": "@misc{\nvinitsky2021robust,\ntitle={Robust Reinforcement Learning using Adversarial Populations},\nauthor={Eugene Vinitsky and Yuqing du and Kanaad V Parvate and Kathy Jang and Pieter Abbeel and Alexandre Bayen},\nyear={2021},\nurl={https://openreview.net/forum?id=I6NRcao1w-X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "I6NRcao1w-X", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper542/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper542/Authors|ICLR.cc/2021/Conference/Paper542/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869849, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment"}}}, {"id": "ltkTgMokWXJ", "original": null, "number": 3, "cdate": 1605502413201, "ddate": null, "tcdate": 1605502413201, "tmdate": 1605549720871, "tddate": null, "forum": "I6NRcao1w-X", "replyto": "DhQXct3DIFZ", "invitation": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment", "content": {"title": "Additional experiments, clarification about training objective", "comment": "We thank the reviewer for their thoughtful feedback and constructive comments and are happy that the reviewer finds our proposed method to be effective and has a nice design. We hope to address the main concerns as follows:\n**Motivation and Explanation**\nWe suspect that our use of notation in equation 1 and 3 has led to some lack of clarity on what training approach we are using to construct robust agents. In particular, our intent is to use multi-agent training to solve for the Nash equilibrium but the notation makes it appear as though we are trying to condition the agent\u2019s policy on the antagonist to solve for a maximin solution. To be clear, the agent and adversary are being trained simultaneously, and neither conditions on either the actions or the policy of the other. We have updated the notation in Eq. (1) and (3) in a way that hopefully makes this clearer. Furthermore, we would like to outline more clearly why we think Nash is a reasonable solution concept here.\n First, in the original Robust Adversarial Reinforcement Learning paper, the authors train their agent by alternating updates i.e. one gradient step for the agent, one gradient step for the adversary and repeating that process over and over. Since they are taking a single small gradient step rather than many, their approach should approach a Nash equilibrium. As this is the paper we are comparing against and trying to point out weaknesses in, it makes sense to use the same solution concept. \nSecond, this is a zero sum game and consequently Nash, maximin, and minimax have the same value. While maximin would require solving a bi-level optimization problem, we can simply solve for Nash and have a guarantee of achieving the same value without the difficulty of bi-level optimization while providing the same robustness guarantees. Additionally, maximin can be viewed as an adversary conditioned on the agent policy. However, we are not looking for perturbations that are conditioned on the agent policy as we are simply trying to generate a perturbed set of dynamical systems. Since, for the continuous control tasks we study, the dynamics mismatch will not be conditioned on the agent policy, we do not want to learn adversaries that are aware of the agent policy.  \n\n> \u201cI think for understanding how well this algorithm works, the authors should test the algorithm on a more tasks with diverse characteristics (e.g. tabular, video games, tabular, traffic)\u201d \n\nAs suggested by the reviewers, we ran the following additional experiments and the results can be found in the appendix:\n- Added experiments in a different domain (multi-armed bandit)\n- Extended control experiments to non-walking tasks (catching a ball in cup)\n\nWe chose to focus on continuous control experiments as the robustness challenges in this domain are primarily due to dynamics robustness. In many of the discrete domains, the robustness challenges often have to do with a lack of robustness to states that are outside the training distribution. For example, in Atari, it is well known that agent performance can be reduced by changing the background image (for example, see [1]). While robustness to state distribution mismatch is also an extremely interesting problem, it requires constructing a different class of adversaries that can apply perturbations to the state space. This is an open challenge and one we leave for future work. However, we hope that the additional continuous control environment in a distinct task and the bandit environments demonstrate that there is room for improvement over the single adversary case. \nIn particular, the Bernoulli bandit example, in which adversaries output the p values (the probability of outputting 1), highlights the conceptual issue with using a single adversary. Here a single adversary corresponds to a single bandit instance and it seems sensible to expect that an optimal bandit sampling strategy will be learnt from interacting with just a single bandit. As an interesting side-note, this example also illustrates another case where domain randomization fails terribly. The test example \u201cone good arm\u201d (see Figure 15) corresponds to one arm with probability 0.9 and 9 arms with Bernoulli probability 0.1. Such an example, in which it is necessary to continue sampling all arms with non-zero probability to have a chance of identifying the good arm, appears rarely in the domain randomization training distribution. Instead, the domain randomization training distribution primarily features easy examples where most arms are reasonably good. Consequently, the domain randomization fails terribly on this adversarial example whereas agents trained against adversarially constructed bandit instances are robust.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper542/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Reinforcement Learning using Adversarial Populations", "authorids": ["~Eugene_Vinitsky1", "yuqing_du@berkeley.edu", "~Kanaad_V_Parvate1", "~Kathy_Jang1", "~Pieter_Abbeel2", "~Alexandre_Bayen2"], "authors": ["Eugene Vinitsky", "Yuqing du", "Kanaad V Parvate", "Kathy Jang", "Pieter Abbeel", "Alexandre Bayen"], "keywords": ["Robust Control", "Reinforcement Learning", "Multiagent Systems"], "abstract": "Reinforcement Learning (RL) is an effective tool for controller design but can struggle with issues of robustness, failing catastrophically when the underlying system dynamics are perturbed. The Robust RL formulation tackles this by adding worst-case adversarial noise to the dynamics and constructing the noise distribution as the solution to a zero-sum minimax game. However, existing work on learning solutions to the Robust RL formulation has primarily focused on training a single RL agent against a single adversary. In this work, we demonstrate that using a single adversary does not consistently yield robustness to dynamics variations under standard parametrizations of the adversary; the resulting policy is highly exploitable by new adversaries. We propose a population-based augmentation to the Robust RL formulation in which we randomly initialize a population of adversaries and sample from the population uniformly during training. We empirically validate across a variety of benchmarks that the use of an adversarial population results in a less exploitable, more robust policy.  Finally, we demonstrate that this approach provides comparable robustness and generalization as domain randomization on these benchmarks while avoiding a ubiquitous domain randomization failure mode.", "one-sentence_summary": "We demonstrate that the standard robust RL formulation does not consistently yield robustness but that this can be alleviated using a population of adversaries.", "pdf": "/pdf/1e5388e0a8d4b52ab688222e9a6d499254c63e02.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vinitsky|robust_reinforcement_learning_using_adversarial_populations", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=r3UNEA7hm", "_bibtex": "@misc{\nvinitsky2021robust,\ntitle={Robust Reinforcement Learning using Adversarial Populations},\nauthor={Eugene Vinitsky and Yuqing du and Kanaad V Parvate and Kathy Jang and Pieter Abbeel and Alexandre Bayen},\nyear={2021},\nurl={https://openreview.net/forum?id=I6NRcao1w-X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "I6NRcao1w-X", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper542/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper542/Authors|ICLR.cc/2021/Conference/Paper542/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869849, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment"}}}, {"id": "rY8B3gLhFbN", "original": null, "number": 5, "cdate": 1605502760955, "ddate": null, "tcdate": 1605502760955, "tmdate": 1605504098951, "tddate": null, "forum": "I6NRcao1w-X", "replyto": "5wc1ZUqVmM", "invitation": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment", "content": {"title": "Additional experiments added, graphs on memory and reward curves added", "comment": "We thank the reviewer for their detailed review and useful feedback and are happy the reviewer finds our method interesting. We believe that the following clarifications and answers to the reviewer\u2019s questions that were raised here will make for a stronger paper. We have addressed the individual questions below and made changes to the paper (marked in red) to help address these issues. In particular, we failed to properly address sample complexity and run-time due to the page limit but are using the extra space to address this.\n>  a. Results are shown using final performance. I am curious about the learning curve \u2013 how does the method compare against other baselines in terms of sample efficiency? A side-effect using a population is that RAP needs to update n adversaries at each training iteration compared with using a single adversary, and will incur more computation overhead. Could authors fairly compare with other baselines in terms of this and show the learning curve?\n\nA: This is a fair point; we initially addressed this in the paper but were limited on space. Fortunately, the usage of RAP incurs no additional run-time cost relative to using a single adversary although it does incur a small memory cost in storing each of the additional adversary policies. As we believe the closest baseline is the original RARL paper, we have added Figure 11 to the appendix to compare the learning curve across number of adversaries (for hopper, cheetah, and ant). While there is some random variation in how quickly the agent learns (likely due to small grid searches over hyperparameters), the overall training speed seems to vary only a little in terms of number of iterations. \nImportantly, we also want to point out that the maximum reward achieved in the 0 adversary is higher than any of the adversarial cases. This is the behavior that should be expected given an adversary as the cases playing against an adversary should have lower converged reward than the maximum possible reward without an adversary. \n> b. How MUCH LONGER does it take to run RAP compared with other baselines? How much more memory does it take to use n adversaries compared with a single adversary?\n\nA: Because we divide the number of samples approximately evenly between the adversaries and mini-batching is generally used in RL, the actual run-time of our training is actually equivalent to that of training a single adversary. We have added Fig. 10 and Fig. 11 to the appendix which compares the reward curves both as a function of number of iterations and on wall-clock time. From the wall-clock figure, it should be clear that there is no additional slow-down from using additional adversaries once you are already using 1 adversary. For N adversaries, the additional memory cost is the cost of storing the N-1 extra neural networks. Since RL neural network policies are small in continuous control (generally 2-3 layers of hidden size that rarely exceeds 400), this is on the order of megabytes.\n> c. Could authors compare with a naive extension of the single adversary case in which the single adversary sample n actions? Is the baseline comparable with RAP using n adversaries?\n\nA: We think this is a really good variant of the experiment as it is an alternative way to represent the mixed nash equilibrium. However, it may be difficult to get the agent to actually take advantage of the randomization. In multi-agent systems, agents have a tendency to cycle through pure strategies even if the Nash equilibrium is mixed (as established in papers like \u201cLearning with opponent-learning awareness\u201d by Foerster et al). While we have not run this particular experiment, it is suggested in our future work section.\n> d. I am confused why RAP is built upon an on-policy algorithm. A number of works using population-based methods are built upon off-policy algorithms as agents in the population can share the samples and could be beneficial. Could authors build the method upon off-policy algorithms to further improve the applicability of RAP?\n\nA: This is a good idea as we agree that sharing samples could be beneficial, although it is also possible that sharing samples could reduce the diversity of our adversary pool! We used on-policy methods because it\u2019s empirically (in our experience) more stable for multi-agent problems and we do not have to use explicit, special multi-agent methods such as MADDPG to handle non-stationarity issues. Additionally, the original Robust Adversarial Reinforcement Learning paper uses TRPO, so we chose another on-policy method for a closer comparison.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper542/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Reinforcement Learning using Adversarial Populations", "authorids": ["~Eugene_Vinitsky1", "yuqing_du@berkeley.edu", "~Kanaad_V_Parvate1", "~Kathy_Jang1", "~Pieter_Abbeel2", "~Alexandre_Bayen2"], "authors": ["Eugene Vinitsky", "Yuqing du", "Kanaad V Parvate", "Kathy Jang", "Pieter Abbeel", "Alexandre Bayen"], "keywords": ["Robust Control", "Reinforcement Learning", "Multiagent Systems"], "abstract": "Reinforcement Learning (RL) is an effective tool for controller design but can struggle with issues of robustness, failing catastrophically when the underlying system dynamics are perturbed. The Robust RL formulation tackles this by adding worst-case adversarial noise to the dynamics and constructing the noise distribution as the solution to a zero-sum minimax game. However, existing work on learning solutions to the Robust RL formulation has primarily focused on training a single RL agent against a single adversary. In this work, we demonstrate that using a single adversary does not consistently yield robustness to dynamics variations under standard parametrizations of the adversary; the resulting policy is highly exploitable by new adversaries. We propose a population-based augmentation to the Robust RL formulation in which we randomly initialize a population of adversaries and sample from the population uniformly during training. We empirically validate across a variety of benchmarks that the use of an adversarial population results in a less exploitable, more robust policy.  Finally, we demonstrate that this approach provides comparable robustness and generalization as domain randomization on these benchmarks while avoiding a ubiquitous domain randomization failure mode.", "one-sentence_summary": "We demonstrate that the standard robust RL formulation does not consistently yield robustness but that this can be alleviated using a population of adversaries.", "pdf": "/pdf/1e5388e0a8d4b52ab688222e9a6d499254c63e02.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vinitsky|robust_reinforcement_learning_using_adversarial_populations", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=r3UNEA7hm", "_bibtex": "@misc{\nvinitsky2021robust,\ntitle={Robust Reinforcement Learning using Adversarial Populations},\nauthor={Eugene Vinitsky and Yuqing du and Kanaad V Parvate and Kathy Jang and Pieter Abbeel and Alexandre Bayen},\nyear={2021},\nurl={https://openreview.net/forum?id=I6NRcao1w-X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "I6NRcao1w-X", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper542/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper542/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper542/Authors|ICLR.cc/2021/Conference/Paper542/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869849, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper542/-/Official_Comment"}}}, {"id": "13vwhyz4HtM", "original": null, "number": 2, "cdate": 1603906075329, "ddate": null, "tcdate": 1603906075329, "tmdate": 1605024664644, "tddate": null, "forum": "I6NRcao1w-X", "replyto": "I6NRcao1w-X", "invitation": "ICLR.cc/2021/Conference/Paper542/-/Official_Review", "content": {"title": "A clean formulation with some compelling results", "review": "#### Summary\n\nThe authors present a scheme that can be used to train agents to be robust against a population of adversarial policies, in which adversaries can perturb actions via an additive perturbation.  Motivated by the observation that agents trained against a single policy may overfit to that policy and hence will lack robustness to new/unseen policies, the authors seek to show that their method generalizes well to unseen policies at test time.  Their experiments consider several simulated environments, in which they show generally good performance against several baselines.\n\n#### Strengths\n\n- I find the argument that agents will overfit to a single policy convincing.  While the motivating example WRT different forces acting on an agent may not be a scenario that robustness against adaptive perturbations can handle, the general claim seems to hold water.  That is, it seems plausible that an agent might receive high reward in a zero sum min-max game subject to a single adversary simply because the adversary is not strong enough to limit the cumulative reward that agent receives.\n\n- The formulation for RAP is presented very clearly.  It is quite helpful to first consider the single minmax adversary and domain randomization formulations, both of which seemed to have played roles in the development of RAP.  Indeed, this seems a very natural way of formulating the problem.  More generally, the paper is quite well written.\n\n- The experiments, and in particular Fig 2, clearly demonstrate the utility of this approach.  One can see a notable difference when an agent is trained with respect to three adversarial policies vs. when it is only trained with a single adversarial policy.\n\n#### Weaknesses\n\n- The utility of this approach is less clear when one considers Fig. 3.  It seems that domain randomization often outperforms RAP.  In my opinion, this weakens part of the claim made by the authors.  However, it does seem true that given that DR is designed to perform well on new domains _in expectation_, it may be preferable to use RAP when one suffers worst case dynamics changes (e.g. Fig. 5).  \n\n- Further study should be done as to how many adversaries one needs to provide meaningful levels of robustness.  It seems that in different experiments, 1, 3, and 5 adversaries were considered.  How should one decided how many adversaries to use?  \n\n- I felt the paper was a bit unclear about what states are available to train adversarial and regular policies.  That is, it is unclear whether in the trajectories $\\tau_j$ and $\\tau_j^i$, the agents/adversaries observe their own actions, the actions of their counterpart (e.g. the agent observing the adversaries action), or both agent and adversary observing the perturbed action $a + \\alpha \\bar{a}$.  The latter case would lead to a lack of observability for both agent and adversary.  Perhaps the authors can clarify this in the rebuttal.\n\n- One weakness is that only one scenario (e.g. walking within these three environments) was considered.  It seems that the claims of the paper could be strengthened if more environments/tasks were considered.\n\n#### Further questions/clarifications\n\n- The reward function seems to be denoted as $\\mathcal{R}$, $R$, and $r$ in various places.\n\n- The bolding in Table 1 is a bit confusing.  I would be fairer to bold the most robust approach overall, rather than the most robust approach of the methods you propose.\n\n- In the notation of Section 4, why are the rollouts of length $M$ rather than $T$, as indicated in the formulation?\n\n#### Final Thoughts\n\nOverall, I thought this was a solid and interesting paper.  The motivation is compelling, the formulation is relatively clean, and the experiments generally back up the claims that are made by the authors.  There are a few weaknesses, as I enumerated above, but even still I think that this is a valuable contribution.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper542/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Reinforcement Learning using Adversarial Populations", "authorids": ["~Eugene_Vinitsky1", "yuqing_du@berkeley.edu", "~Kanaad_V_Parvate1", "~Kathy_Jang1", "~Pieter_Abbeel2", "~Alexandre_Bayen2"], "authors": ["Eugene Vinitsky", "Yuqing du", "Kanaad V Parvate", "Kathy Jang", "Pieter Abbeel", "Alexandre Bayen"], "keywords": ["Robust Control", "Reinforcement Learning", "Multiagent Systems"], "abstract": "Reinforcement Learning (RL) is an effective tool for controller design but can struggle with issues of robustness, failing catastrophically when the underlying system dynamics are perturbed. The Robust RL formulation tackles this by adding worst-case adversarial noise to the dynamics and constructing the noise distribution as the solution to a zero-sum minimax game. However, existing work on learning solutions to the Robust RL formulation has primarily focused on training a single RL agent against a single adversary. In this work, we demonstrate that using a single adversary does not consistently yield robustness to dynamics variations under standard parametrizations of the adversary; the resulting policy is highly exploitable by new adversaries. We propose a population-based augmentation to the Robust RL formulation in which we randomly initialize a population of adversaries and sample from the population uniformly during training. We empirically validate across a variety of benchmarks that the use of an adversarial population results in a less exploitable, more robust policy.  Finally, we demonstrate that this approach provides comparable robustness and generalization as domain randomization on these benchmarks while avoiding a ubiquitous domain randomization failure mode.", "one-sentence_summary": "We demonstrate that the standard robust RL formulation does not consistently yield robustness but that this can be alleviated using a population of adversaries.", "pdf": "/pdf/1e5388e0a8d4b52ab688222e9a6d499254c63e02.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vinitsky|robust_reinforcement_learning_using_adversarial_populations", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=r3UNEA7hm", "_bibtex": "@misc{\nvinitsky2021robust,\ntitle={Robust Reinforcement Learning using Adversarial Populations},\nauthor={Eugene Vinitsky and Yuqing du and Kanaad V Parvate and Kathy Jang and Pieter Abbeel and Alexandre Bayen},\nyear={2021},\nurl={https://openreview.net/forum?id=I6NRcao1w-X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "I6NRcao1w-X", "replyto": "I6NRcao1w-X", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper542/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538140809, "tmdate": 1606915787416, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper542/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper542/-/Official_Review"}}}, {"id": "DhQXct3DIFZ", "original": null, "number": 3, "cdate": 1604087120096, "ddate": null, "tcdate": 1604087120096, "tmdate": 1605024664580, "tddate": null, "forum": "I6NRcao1w-X", "replyto": "I6NRcao1w-X", "invitation": "ICLR.cc/2021/Conference/Paper542/-/Official_Review", "content": {"title": "The proposed algorithm is good but the motivation is unsolid and experiments are limited", "review": "This paper proposes an algorithm to improve the robustness of reinforcement learning. The algorithm , RAP, combines ideas from domain randomization and adversarial training. Specifically, during learning, it trains an ensemble of adversary to attack the learner, with the hope that the learner can be robust to various situations. The experimental results show the proposed algorithm indeed outperform the respective baselines here (single-adversary training and domain randomization) in its ability to generalize the other test domains. \n\nI think overall the proposed algorithm presents a simple and nice way to join the strengths of the adversary training and domain randomization. Indeed, we can view single-adversary training and domain randomization as special cases of RAP, which either uses a single adversary or not perform any update on the adversaries. For this, I would imagine RAP can be quite effective in practice. \n\n* Issue about motivation and explaination\n\nHowever, despite this nice design, I think the main motivation and the explanation why the proposed algorithm works are not completely correct. In introduction, the authors motivate the use of multiple adversary from that pure Nash equilibrium does not always exist in zero sum two player games. However, adversary training is not necessarily about solving the Nash equilibrium (which aims to find solutions such that minimax = maximin) but rather solving a maximin \"only\", whose solution is always well defined. \n\nThe motivating robot example is also quite misleading. In that case, the failure is due to the learner policy is not even solving maximin problem. Note in maximin, the adversary is chosen after knowing the learner's policy. This robot example is rather saying that minimax, where the learner optimizes after the adversary is chosen, is insufficient to generate robust behavior. However, this is not what adversarial training is about. \n\nI think, it's probably because of this misunderstanding, the authors motivate the issue of the single-adversary training as the agent would overfit to the single adversary. Again this is due to incorrectly interpreting maximin in (1) as minimax. \n\nNonetheless, I do believe the proposed algorithm is effective in practice, but for a different reason from what the authors explain. I think the RAP does improve upon single-adversary training. Because it uses multiple randomly initialized adversarial policies, it may have a higher chance to overcome the non-convexity issue in the min part of maxmin and  therefore has a higher chance to find the maximin solution. In other words, the failure of the single-adversary implementation is most due to optimization difficulty not that the solution concept is incorrect. And the proposed algorithm is more of an optimization heuristic to better approximate the maximin problem (which actually is quite commonly used in the optimization literature). In fact, when given the learner policy, I believe by further taking the min among the multiple adversaries here and uses that for the learner update (i.e. the leaner would not use trajectories from all but the worst one), the robustness of the algorithm might further improve. \n\n* Experiments \n\nGiven this work is lacking theoretical insights, I expect more experiments to be done to verify the proposed algorithm. The current paper only test RAP on mujoco environments. I think for understanding how well this algorithm works, the authors should test the algorithm on a more tasks with diverse characteristics (e.g. tabular, video games, tabular, traffic,) rather just the continuous robotics control domain. \n\nIn figure 2, I think a fairer comparison should let agents of both sides face the same adversary. Nonetheless, I agree that the performance plots later on are sufficient to show that RAP is better. \n\nLastly I like the in-depth discussion about the failure of domain randomization in halfcheetah, as it's also my main question when reading the previous part of the paper. However, I'm wondering if the bad performance is due to that the learner's policy is not expressive enough. I cannot find the description of the exact architecture used in the paper, but can you try training with a more expressive policy and see if domain randomization still perform worse than the direct training without the adversary?\n\n\n\n\n\n ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper542/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Reinforcement Learning using Adversarial Populations", "authorids": ["~Eugene_Vinitsky1", "yuqing_du@berkeley.edu", "~Kanaad_V_Parvate1", "~Kathy_Jang1", "~Pieter_Abbeel2", "~Alexandre_Bayen2"], "authors": ["Eugene Vinitsky", "Yuqing du", "Kanaad V Parvate", "Kathy Jang", "Pieter Abbeel", "Alexandre Bayen"], "keywords": ["Robust Control", "Reinforcement Learning", "Multiagent Systems"], "abstract": "Reinforcement Learning (RL) is an effective tool for controller design but can struggle with issues of robustness, failing catastrophically when the underlying system dynamics are perturbed. The Robust RL formulation tackles this by adding worst-case adversarial noise to the dynamics and constructing the noise distribution as the solution to a zero-sum minimax game. However, existing work on learning solutions to the Robust RL formulation has primarily focused on training a single RL agent against a single adversary. In this work, we demonstrate that using a single adversary does not consistently yield robustness to dynamics variations under standard parametrizations of the adversary; the resulting policy is highly exploitable by new adversaries. We propose a population-based augmentation to the Robust RL formulation in which we randomly initialize a population of adversaries and sample from the population uniformly during training. We empirically validate across a variety of benchmarks that the use of an adversarial population results in a less exploitable, more robust policy.  Finally, we demonstrate that this approach provides comparable robustness and generalization as domain randomization on these benchmarks while avoiding a ubiquitous domain randomization failure mode.", "one-sentence_summary": "We demonstrate that the standard robust RL formulation does not consistently yield robustness but that this can be alleviated using a population of adversaries.", "pdf": "/pdf/1e5388e0a8d4b52ab688222e9a6d499254c63e02.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vinitsky|robust_reinforcement_learning_using_adversarial_populations", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=r3UNEA7hm", "_bibtex": "@misc{\nvinitsky2021robust,\ntitle={Robust Reinforcement Learning using Adversarial Populations},\nauthor={Eugene Vinitsky and Yuqing du and Kanaad V Parvate and Kathy Jang and Pieter Abbeel and Alexandre Bayen},\nyear={2021},\nurl={https://openreview.net/forum?id=I6NRcao1w-X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "I6NRcao1w-X", "replyto": "I6NRcao1w-X", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper542/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538140809, "tmdate": 1606915787416, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper542/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper542/-/Official_Review"}}}, {"id": "OCTwp4ylrkl", "original": null, "number": 4, "cdate": 1604088765847, "ddate": null, "tcdate": 1604088765847, "tmdate": 1605024664517, "tddate": null, "forum": "I6NRcao1w-X", "replyto": "I6NRcao1w-X", "invitation": "ICLR.cc/2021/Conference/Paper542/-/Official_Review", "content": {"title": "The conceptual novelty is quite incremental but the experimental results are solid.", "review": "This paper extends the existing work on robust adversarial RL by training multiple adversarial agents from a population. Solid experimental results are presented to show that the proposed method improves the single adversary setting and domain randomization. \n\nThe experimental results in this paper seem solid to me. My biggest concern for this paper is that the conceptual novelty seems quite incremental. I mean, on the conceptual level, it is a common sense from robust control that multiple uncertainty sources can be treated together (e.g. the robust control theory handles the structured uncertainty in such a manner). One can augment all the adversary networks as a big network and then the problem formulation is the same as before. If we think each phi_i is a block in this big \"single adversary network\", then what the authors have done can be thought as doing  block coordinate descent. From this perspective, there is not too much conceptual novelty, and the main contribution of this paper is doing some more detailed study showing how to combine augmentation and adversarial RL. I am not sure whether such a contribution itself is enough for ICLR or not.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper542/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper542/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Reinforcement Learning using Adversarial Populations", "authorids": ["~Eugene_Vinitsky1", "yuqing_du@berkeley.edu", "~Kanaad_V_Parvate1", "~Kathy_Jang1", "~Pieter_Abbeel2", "~Alexandre_Bayen2"], "authors": ["Eugene Vinitsky", "Yuqing du", "Kanaad V Parvate", "Kathy Jang", "Pieter Abbeel", "Alexandre Bayen"], "keywords": ["Robust Control", "Reinforcement Learning", "Multiagent Systems"], "abstract": "Reinforcement Learning (RL) is an effective tool for controller design but can struggle with issues of robustness, failing catastrophically when the underlying system dynamics are perturbed. The Robust RL formulation tackles this by adding worst-case adversarial noise to the dynamics and constructing the noise distribution as the solution to a zero-sum minimax game. However, existing work on learning solutions to the Robust RL formulation has primarily focused on training a single RL agent against a single adversary. In this work, we demonstrate that using a single adversary does not consistently yield robustness to dynamics variations under standard parametrizations of the adversary; the resulting policy is highly exploitable by new adversaries. We propose a population-based augmentation to the Robust RL formulation in which we randomly initialize a population of adversaries and sample from the population uniformly during training. We empirically validate across a variety of benchmarks that the use of an adversarial population results in a less exploitable, more robust policy.  Finally, we demonstrate that this approach provides comparable robustness and generalization as domain randomization on these benchmarks while avoiding a ubiquitous domain randomization failure mode.", "one-sentence_summary": "We demonstrate that the standard robust RL formulation does not consistently yield robustness but that this can be alleviated using a population of adversaries.", "pdf": "/pdf/1e5388e0a8d4b52ab688222e9a6d499254c63e02.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vinitsky|robust_reinforcement_learning_using_adversarial_populations", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=r3UNEA7hm", "_bibtex": "@misc{\nvinitsky2021robust,\ntitle={Robust Reinforcement Learning using Adversarial Populations},\nauthor={Eugene Vinitsky and Yuqing du and Kanaad V Parvate and Kathy Jang and Pieter Abbeel and Alexandre Bayen},\nyear={2021},\nurl={https://openreview.net/forum?id=I6NRcao1w-X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "I6NRcao1w-X", "replyto": "I6NRcao1w-X", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper542/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538140809, "tmdate": 1606915787416, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper542/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper542/-/Official_Review"}}}], "count": 20}