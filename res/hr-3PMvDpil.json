{"notes": [{"id": "hr-3PMvDpil", "original": "UUMdqVQBfHB", "number": 426, "cdate": 1601308054774, "ddate": null, "tcdate": 1601308054774, "tmdate": 1615532417053, "tddate": null, "forum": "hr-3PMvDpil", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Efficient Certified Defenses Against Patch Attacks on Image Classifiers", "authorids": ["~Jan_Hendrik_Metzen1", "~Maksym_Yatsura1"], "authors": ["Jan Hendrik Metzen", "Maksym Yatsura"], "keywords": ["robustness", "certified defense", "adversarial patch", "aversarial examples"], "abstract": "Adversarial patches pose a realistic threat model for physical world attacks on autonomous systems via their perception component. Autonomous systems in safety-critical domains such as automated driving should thus contain a fail-safe fallback component that combines certifiable robustness against patches with efficient inference while maintaining high performance on clean inputs. We propose BagCert, a novel combination of model architecture and certification procedure that allows efficient certification. We derive a loss that enables end-to-end optimization of certified robustness against patches of different sizes and locations. On CIFAR10, BagCert certifies 10.000 examples in 43 seconds on a single GPU and obtains 86% clean and 60% certified accuracy against 5x5 patches.", "one-sentence_summary": "We propose a method for certifying robustness against adversarial patches that combines high certified accuracy with efficient inference while maintaining strong performance on clean data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "metzen|efficient_certified_defenses_against_patch_attacks_on_image_classifiers", "supplementary_material": "", "pdf": "/pdf/964f848b9674e016ee4ad4259c6491fd8c66729a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmetzen2021efficient,\ntitle={Efficient Certified Defenses Against Patch Attacks on Image Classifiers},\nauthor={Jan Hendrik Metzen and Maksym Yatsura},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=hr-3PMvDpil}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "rynAWQhtux", "original": null, "number": 1, "cdate": 1610040497131, "ddate": null, "tcdate": 1610040497131, "tmdate": 1610474103551, "tddate": null, "forum": "hr-3PMvDpil", "replyto": "hr-3PMvDpil", "invitation": "ICLR.cc/2021/Conference/Paper426/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The paper develops a novel provable defense against patch-based adversasrial attacks on image classification system, by combining a novel architecture and certification procedure. The theoretical and experimental contributions are convincing and clearly advance the state of the art in provable defenses against adversarial perturbations.\n\nThe questions raised by the reviewers were addressed convincingly by the authors during the rebuttal phase, leading to unanimous consensus amongst reviewers towards acceptance. I recommend acceptance."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Certified Defenses Against Patch Attacks on Image Classifiers", "authorids": ["~Jan_Hendrik_Metzen1", "~Maksym_Yatsura1"], "authors": ["Jan Hendrik Metzen", "Maksym Yatsura"], "keywords": ["robustness", "certified defense", "adversarial patch", "aversarial examples"], "abstract": "Adversarial patches pose a realistic threat model for physical world attacks on autonomous systems via their perception component. Autonomous systems in safety-critical domains such as automated driving should thus contain a fail-safe fallback component that combines certifiable robustness against patches with efficient inference while maintaining high performance on clean inputs. We propose BagCert, a novel combination of model architecture and certification procedure that allows efficient certification. We derive a loss that enables end-to-end optimization of certified robustness against patches of different sizes and locations. On CIFAR10, BagCert certifies 10.000 examples in 43 seconds on a single GPU and obtains 86% clean and 60% certified accuracy against 5x5 patches.", "one-sentence_summary": "We propose a method for certifying robustness against adversarial patches that combines high certified accuracy with efficient inference while maintaining strong performance on clean data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "metzen|efficient_certified_defenses_against_patch_attacks_on_image_classifiers", "supplementary_material": "", "pdf": "/pdf/964f848b9674e016ee4ad4259c6491fd8c66729a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmetzen2021efficient,\ntitle={Efficient Certified Defenses Against Patch Attacks on Image Classifiers},\nauthor={Jan Hendrik Metzen and Maksym Yatsura},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=hr-3PMvDpil}\n}"}, "tags": [], "invitation": {"reply": {"forum": "hr-3PMvDpil", "replyto": "hr-3PMvDpil", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040497118, "tmdate": 1610474103537, "id": "ICLR.cc/2021/Conference/Paper426/-/Decision"}}}, {"id": "0UiH8JN5s2k", "original": null, "number": 12, "cdate": 1606207593392, "ddate": null, "tcdate": 1606207593392, "tmdate": 1606207593392, "tddate": null, "forum": "hr-3PMvDpil", "replyto": "7teoKF66Xpz", "invitation": "ICLR.cc/2021/Conference/Paper426/-/Official_Comment", "content": {"title": "Response", "comment": "I would like to thank the authors for providing additional explanations, and appreciate the authors' hard work.  After reading the other reviews, the authors response and the updated manuscript, I'm satisfied with the revised content. "}, "signatures": ["ICLR.cc/2021/Conference/Paper426/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper426/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Certified Defenses Against Patch Attacks on Image Classifiers", "authorids": ["~Jan_Hendrik_Metzen1", "~Maksym_Yatsura1"], "authors": ["Jan Hendrik Metzen", "Maksym Yatsura"], "keywords": ["robustness", "certified defense", "adversarial patch", "aversarial examples"], "abstract": "Adversarial patches pose a realistic threat model for physical world attacks on autonomous systems via their perception component. Autonomous systems in safety-critical domains such as automated driving should thus contain a fail-safe fallback component that combines certifiable robustness against patches with efficient inference while maintaining high performance on clean inputs. We propose BagCert, a novel combination of model architecture and certification procedure that allows efficient certification. We derive a loss that enables end-to-end optimization of certified robustness against patches of different sizes and locations. On CIFAR10, BagCert certifies 10.000 examples in 43 seconds on a single GPU and obtains 86% clean and 60% certified accuracy against 5x5 patches.", "one-sentence_summary": "We propose a method for certifying robustness against adversarial patches that combines high certified accuracy with efficient inference while maintaining strong performance on clean data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "metzen|efficient_certified_defenses_against_patch_attacks_on_image_classifiers", "supplementary_material": "", "pdf": "/pdf/964f848b9674e016ee4ad4259c6491fd8c66729a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmetzen2021efficient,\ntitle={Efficient Certified Defenses Against Patch Attacks on Image Classifiers},\nauthor={Jan Hendrik Metzen and Maksym Yatsura},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=hr-3PMvDpil}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "hr-3PMvDpil", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper426/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper426/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper426/Authors|ICLR.cc/2021/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper426/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871103, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper426/-/Official_Comment"}}}, {"id": "J6C5snKgEwR", "original": null, "number": 11, "cdate": 1605597683853, "ddate": null, "tcdate": 1605597683853, "tmdate": 1605597683853, "tddate": null, "forum": "hr-3PMvDpil", "replyto": "hr-3PMvDpil", "invitation": "ICLR.cc/2021/Conference/Paper426/-/Official_Comment", "content": {"title": "Revised Version", "comment": "We have uploaded a revised version of our submission based on the comments by the reviewers. We would like to thank all reviewers again for their valuable input! Below we list the main changes compared to the initial submission:\n *  In Section A.2, we evaluate BagCert models against a practical patch attack to determine an upper bound on robustness as proposed by AnonReviewer1. The specific attack chooses the location of the patch based on certification condition 3.2 and performs a PGD attack to determine the respective patch. \n * We empirically compare and justify the choice of using a Heaviside step function in the forward pass versus using an element-wise sigmoid or channel-wise softmax, see Section A.3. We would like to thank AnonReviewer5 for this suggestion.\n * We simplify and clarify the definition of $R(l)$ as suggested by AnonReviewer1.\n * We mention that future work could investigate combining our method with the one of Chiang et al. as suggested by AnonReviewer5.\n * We state that results for baselines in Figure 2 are taken from the respective papers, clarifying a point by AnonReviewer4.\n * We state that lines in Figure 3 and Figure 4 correspond to the same model; clarifying a point made by AnonReviewer2.\n * We mention  in the outlook that applying/extending BagCert to other type of modalities or models would be a promising direction for future work, as proposed by AnonReviewer4.\n * Moreover, we mention in the outlook that covering other aggregation functions than spatial sum such as the ones proposed in prior work by Xiang et al. would be an exciting direction for future work, as discussed with AnonReviewer2.\n\nWe believe these changes improve the quality of the submission and would like to thank the reviewers again for their valuable suggestions and comments. If any points remain open, we would be grateful for discussing them further."}, "signatures": ["ICLR.cc/2021/Conference/Paper426/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper426/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Certified Defenses Against Patch Attacks on Image Classifiers", "authorids": ["~Jan_Hendrik_Metzen1", "~Maksym_Yatsura1"], "authors": ["Jan Hendrik Metzen", "Maksym Yatsura"], "keywords": ["robustness", "certified defense", "adversarial patch", "aversarial examples"], "abstract": "Adversarial patches pose a realistic threat model for physical world attacks on autonomous systems via their perception component. Autonomous systems in safety-critical domains such as automated driving should thus contain a fail-safe fallback component that combines certifiable robustness against patches with efficient inference while maintaining high performance on clean inputs. We propose BagCert, a novel combination of model architecture and certification procedure that allows efficient certification. We derive a loss that enables end-to-end optimization of certified robustness against patches of different sizes and locations. On CIFAR10, BagCert certifies 10.000 examples in 43 seconds on a single GPU and obtains 86% clean and 60% certified accuracy against 5x5 patches.", "one-sentence_summary": "We propose a method for certifying robustness against adversarial patches that combines high certified accuracy with efficient inference while maintaining strong performance on clean data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "metzen|efficient_certified_defenses_against_patch_attacks_on_image_classifiers", "supplementary_material": "", "pdf": "/pdf/964f848b9674e016ee4ad4259c6491fd8c66729a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmetzen2021efficient,\ntitle={Efficient Certified Defenses Against Patch Attacks on Image Classifiers},\nauthor={Jan Hendrik Metzen and Maksym Yatsura},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=hr-3PMvDpil}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "hr-3PMvDpil", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper426/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper426/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper426/Authors|ICLR.cc/2021/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper426/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871103, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper426/-/Official_Comment"}}}, {"id": "7teoKF66Xpz", "original": null, "number": 10, "cdate": 1605271782877, "ddate": null, "tcdate": 1605271782877, "tmdate": 1605271869030, "tddate": null, "forum": "hr-3PMvDpil", "replyto": "wCjfEX7E1Kv", "invitation": "ICLR.cc/2021/Conference/Paper426/-/Official_Comment", "content": {"title": "Addressing the concerns", "comment": "We are grateful for the helpful review and the constructive feedback. We will provide responses to the open points below:\n\nConcern 1a: \"The paper uses essentially the same certification conditions as the Derandomized smoothing except that it replaces the network architecture with BagNet.\"\nWe would like to clarify that only Condition 3.3 is equivalent to the condition in Derandomized smoothing. However, Condition 3.2 proposed by us is strictly stronger and applicable in typical cases (attacks on ConvNets with rectangular patches). Moreover, Condition 3.1 is a very general condition that can be used for settings not covered by Derandomized smoothing's condition. Moreover, our end-to-end training with a loss derived from the certification condition is an additional contribution over Derandomized smoothing.\n\nConcern 1b: \"Is it fair to compare accuracy under networks with varying numbers of parameters that differ too much (38M:11M)?\"\nWe observe that for BagNets on CIFAR10 it is beneficial in general if they are \"very wide\" (compare Appendix A), more so than for standard ResNets used in Derandomized Smoothing, which might overfit severely if they would be as wide as our BagNets. So comparing different kinds of architectures is inherently difficult, because their \"sweet spot\" is typically at different number of parameters.\nWe would like to note that on ImageNet, the best BagCert model has only 4.5M parameters, while the ResNet50 used in Derandomized smoothing has ~25M parameters. Figure 2 (right) shows that BagCert achieves superior performance on ImageNet  compared to Derandomized smoothing despite the model having ~6 times fewer parameters. Thus, competitive performance can also be obtained with smaller models in BagCert (probably larger BagNets would improve performance further on ImageNet).\n\nConcern 2: \"Could you provide the training cost compared with Derandomized smoothing?\"\nThanks for raising this point. We would like to clarify that BagCert is more efficient than Derandomized block smoothing at inference time but not necessarily at training time. The reason is that BagCert performs end-to-end training with the certification process being part of training  while Derandomized smoothing only applies certification (the smoothing) post-hoc to a pretrained network. Thus, we do not claim that our method reduces training cost. But we also do not consider training time as relevant as inference time, as long as training remains practical and can scale to large problems such as ImageNet (which BagCert does). On ImageNet, training BagCert took between 4 and 5 days (depending on the receptive field size) on 4 Tesla V100 GPUs, which we consider practical. Levine and Feizi do not report training time for Derandomized smoothing, so it is hard to compare, but we expect a similar order of magnitude.\n\nConcern 3: \"The description of R(l) in 3.1 is so unclear that it is difficult to understand the meaning of R(l).\"\nWe would add the additional sentence \"Informally, $R(l)$ is the set of all indices of the score map that can be affected by a patch attack at region $l$, that is: the set of all outputs of $f_\\theta$ whose receptive fields overlap with $l$.\". We hope this sentence would clarify the meaning of $R(l)$?\n\nConcern 4: \"Can you provide some results that uses some existing patch attack methods for evaluation?\"\nWe agree, heuristic attacks can be useful for providing a tighter upper bound on robust accuracy than clean accuracy. We are currently running an evaluation against patch attacks and will report results later.\n\nWe will shortly upload a revised version of the paper. In case our response leaves questions open, we would be grateful for discussing these further."}, "signatures": ["ICLR.cc/2021/Conference/Paper426/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper426/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Certified Defenses Against Patch Attacks on Image Classifiers", "authorids": ["~Jan_Hendrik_Metzen1", "~Maksym_Yatsura1"], "authors": ["Jan Hendrik Metzen", "Maksym Yatsura"], "keywords": ["robustness", "certified defense", "adversarial patch", "aversarial examples"], "abstract": "Adversarial patches pose a realistic threat model for physical world attacks on autonomous systems via their perception component. Autonomous systems in safety-critical domains such as automated driving should thus contain a fail-safe fallback component that combines certifiable robustness against patches with efficient inference while maintaining high performance on clean inputs. We propose BagCert, a novel combination of model architecture and certification procedure that allows efficient certification. We derive a loss that enables end-to-end optimization of certified robustness against patches of different sizes and locations. On CIFAR10, BagCert certifies 10.000 examples in 43 seconds on a single GPU and obtains 86% clean and 60% certified accuracy against 5x5 patches.", "one-sentence_summary": "We propose a method for certifying robustness against adversarial patches that combines high certified accuracy with efficient inference while maintaining strong performance on clean data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "metzen|efficient_certified_defenses_against_patch_attacks_on_image_classifiers", "supplementary_material": "", "pdf": "/pdf/964f848b9674e016ee4ad4259c6491fd8c66729a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmetzen2021efficient,\ntitle={Efficient Certified Defenses Against Patch Attacks on Image Classifiers},\nauthor={Jan Hendrik Metzen and Maksym Yatsura},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=hr-3PMvDpil}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "hr-3PMvDpil", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper426/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper426/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper426/Authors|ICLR.cc/2021/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper426/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871103, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper426/-/Official_Comment"}}}, {"id": "dBGmH7mHeVP", "original": null, "number": 8, "cdate": 1605267793284, "ddate": null, "tcdate": 1605267793284, "tmdate": 1605271807224, "tddate": null, "forum": "hr-3PMvDpil", "replyto": "Ev-AhjOKGA4", "invitation": "ICLR.cc/2021/Conference/Paper426/-/Official_Comment", "content": {"title": "Response to raised concerns and questions (Part 1/2)", "comment": "We would like to thank the reviewer for the helpful review and the provided suggestions. We will try to clarify the open points:\n\nConcern 1: \"Relation to Xiang et. al. (2020) and how fair is it to call our papers concurrent\"\nWe acknowledge that there is no consensus on what constitutes concurrent work. We started this project in March 2020 including the concept of using BagNets. The work of Xiang et al. appeared two months later (in May 2020) on a preprint server. Thus, we denote the work as concurrent, without intending to be \"disingenuous\" - after all, using BagNets (the main overlap between our work and  Xiang et al.) was something we settled on already before Xiang et al. got publicly available. We hope the reviewer agrees that this is a reasonable point of view - even in case the reviewer has a different opinion.\nIn the case the reviewers and AC disagree and consider Xiang et. al. (2020) prior work, we still believe our work has sufficient novelty:\n * We propose novel conditions for checking certifiability (Condition 3.1 and Condition 3.2) in Section 3.1\n * We propose end-to-end training with a novel margin-based loss that trains the model to be certifiable (Section 3.3)\n * Overlap with Xiang et. al. (2020)  is in the model (Section 3.2). However, even there we differ in terms of using a Heaviside step function for clipping (with a \u201dstraight-through\u201d type trick in the backward pass). Moreover,  we do not require upscaling CIFAR10 images to 192x192 as it is required in the PatchGuard, because we adapt BagNets to low resolution inputs.\n\nConcern 2: \"Applying defense to multiple, small localized patches such as in LaVAN (Karmon et al.)\"\nBagCert can also be applied to quantify robustness against multiple, small localized patches in principle. In this case, a feasible region $l$ would consist of several smaller local patches. One could define and compute $R(l)$ analogously to one connected patch. Condition 3.2 might become much more expensive computationally (because $R(l)$ would no longer be rectangular) such that one would have to resort to the weaker Condition 3.3. Moreover, we would expect that $R(l)$ would become larger for several local patches than for one connected patch of the same total size. Accordingly, we expect certified accuracy to decrease. An extreme case for this would be an L_0 threat model such as the one studied by Levine and Feizi in \"Robustness Certificates for Sparse Adversarial Attacks by Randomized Ablation\". In general, we see BagCert's main area of application on defending against adversarial patches as introduced by Brown et al., but it might be competitive in related but slightly different threat models as well.\n\nConcern 3: \"How should one tune the receptive field size to minimize the trade-off between clean/certified trade-off accuracy?\"\nThis is a good remark. Generally, both the receptive field size and train margins M affect the trade-off and larger receptive fields and smaller margins increase clean accuracy at the expense of certified accuracy. Beyond that, one would have to specify what \"minimise the trade-off\" means since the problem is inherently multi-objective. Moreover, it is admittedly difficult to predict where on the Pareto frontier a network with a specific receptive field size and train margin would end up. This could be an interesting direction for future work.\n\nConcern 4: \"Extension to other modalities? extract global (instead of local) structure within data inputs (e.g. Transformers and NLP tasks)\"\nWhile we present BagCert for the case of 2d inputs and also perform our empirical evaluation on image data, the approach could easily be extended to other domains where ConvNets are applied such as 1d time series or 3d video data. For applying it to other kind of models such as e.g. Transformers, the main challenge would be to restrict all building blocks to act locally (e.g. only allow attention to nearby indices rather than global attention). We thank the reviewer for this remark; we think it can be a good direction for future work and it nicely fits the last sentence of our paper (\"...the development of alternative choices for models with small receptive fields\"). For problems which require inherently to extract global structure (or very long-range dependencies), our approach would not be an ideal choice in our opinion."}, "signatures": ["ICLR.cc/2021/Conference/Paper426/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper426/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Certified Defenses Against Patch Attacks on Image Classifiers", "authorids": ["~Jan_Hendrik_Metzen1", "~Maksym_Yatsura1"], "authors": ["Jan Hendrik Metzen", "Maksym Yatsura"], "keywords": ["robustness", "certified defense", "adversarial patch", "aversarial examples"], "abstract": "Adversarial patches pose a realistic threat model for physical world attacks on autonomous systems via their perception component. Autonomous systems in safety-critical domains such as automated driving should thus contain a fail-safe fallback component that combines certifiable robustness against patches with efficient inference while maintaining high performance on clean inputs. We propose BagCert, a novel combination of model architecture and certification procedure that allows efficient certification. We derive a loss that enables end-to-end optimization of certified robustness against patches of different sizes and locations. On CIFAR10, BagCert certifies 10.000 examples in 43 seconds on a single GPU and obtains 86% clean and 60% certified accuracy against 5x5 patches.", "one-sentence_summary": "We propose a method for certifying robustness against adversarial patches that combines high certified accuracy with efficient inference while maintaining strong performance on clean data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "metzen|efficient_certified_defenses_against_patch_attacks_on_image_classifiers", "supplementary_material": "", "pdf": "/pdf/964f848b9674e016ee4ad4259c6491fd8c66729a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmetzen2021efficient,\ntitle={Efficient Certified Defenses Against Patch Attacks on Image Classifiers},\nauthor={Jan Hendrik Metzen and Maksym Yatsura},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=hr-3PMvDpil}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "hr-3PMvDpil", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper426/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper426/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper426/Authors|ICLR.cc/2021/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper426/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871103, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper426/-/Official_Comment"}}}, {"id": "7f1rImWOwuC", "original": null, "number": 9, "cdate": 1605267918986, "ddate": null, "tcdate": 1605267918986, "tmdate": 1605267918986, "tddate": null, "forum": "hr-3PMvDpil", "replyto": "Ev-AhjOKGA4", "invitation": "ICLR.cc/2021/Conference/Paper426/-/Official_Comment", "content": {"title": "Response to raised concerns and questions (Part 2/2)", "comment": "Concern 5: \"Are the results compared against vanilla implementations of related approaches, or against approaches with relevant hyperparameters tuned? For example, when comparing with Xiang et al. [3], did you perform a sweep over receptive field sizes that minimize the clean-robust accuracy trade-off, or a sweep over the detection threshold, T? \"\nFor the models of Zhang et al., 2020 (Clipped BagNet)  and Xiang et al., 2020 (PatchGuard, Mask-BN and Mask-DS), we use performance numbers provided by the authors of the respective approach. We assume the authors have tuned the relevant hyperparameters appropriately.\n\nConcern 6: \"Row/Column-based Derandomized Smoothing (Levine and Feizi) against non-square patches.\"\nWe generally have that column-smoothing is strong against tall-but-narrow patches and weak against short-but-wide patches. For row-smoothing, it is exactly the opposite. Since the defender cannot know the aspect ratio chosen by the attacker in advance, both strategies are strictly weaker than a block-smoothing strategy that works well for all aspect ratios (BagCert's small and square receptive field is akin to block smoothing). An interesting question for future work would be if a kind of \"ensembling\" of different types of column/row/block-smoothing could outperform pure block smoothing for all aspect ratios.\nWe would also like to note that also BagCert and other BagNet-based defenses can implement column- or row-smoothing by using mainly 1x3 or 3x1 kernels instead of 1x1 kernels. We did not explore this further but it could be an interesting direction for future work.\n\nWe will shortly upload a revised version of the paper. In case our response leaves questions open, we would be grateful for discussing these further."}, "signatures": ["ICLR.cc/2021/Conference/Paper426/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper426/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Certified Defenses Against Patch Attacks on Image Classifiers", "authorids": ["~Jan_Hendrik_Metzen1", "~Maksym_Yatsura1"], "authors": ["Jan Hendrik Metzen", "Maksym Yatsura"], "keywords": ["robustness", "certified defense", "adversarial patch", "aversarial examples"], "abstract": "Adversarial patches pose a realistic threat model for physical world attacks on autonomous systems via their perception component. Autonomous systems in safety-critical domains such as automated driving should thus contain a fail-safe fallback component that combines certifiable robustness against patches with efficient inference while maintaining high performance on clean inputs. We propose BagCert, a novel combination of model architecture and certification procedure that allows efficient certification. We derive a loss that enables end-to-end optimization of certified robustness against patches of different sizes and locations. On CIFAR10, BagCert certifies 10.000 examples in 43 seconds on a single GPU and obtains 86% clean and 60% certified accuracy against 5x5 patches.", "one-sentence_summary": "We propose a method for certifying robustness against adversarial patches that combines high certified accuracy with efficient inference while maintaining strong performance on clean data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "metzen|efficient_certified_defenses_against_patch_attacks_on_image_classifiers", "supplementary_material": "", "pdf": "/pdf/964f848b9674e016ee4ad4259c6491fd8c66729a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmetzen2021efficient,\ntitle={Efficient Certified Defenses Against Patch Attacks on Image Classifiers},\nauthor={Jan Hendrik Metzen and Maksym Yatsura},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=hr-3PMvDpil}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "hr-3PMvDpil", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper426/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper426/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper426/Authors|ICLR.cc/2021/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper426/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871103, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper426/-/Official_Comment"}}}, {"id": "RQcub27um4C", "original": null, "number": 7, "cdate": 1605265499448, "ddate": null, "tcdate": 1605265499448, "tmdate": 1605265499448, "tddate": null, "forum": "hr-3PMvDpil", "replyto": "4P_QTkuPr00", "invitation": "ICLR.cc/2021/Conference/Paper426/-/Official_Comment", "content": {"title": "Clarification on Cons and Questions Raised", "comment": "We would like to thank the reviewer for the thorough review and the constructive feedback. We will try to clarify the open points (and revise the paper accordingly):\n\nCon 1: \"Do I understand correctly that each point in figures 3 and 4 was obtained by retraining models from scratch with the chosen patch configuration?\"\nLines in Figure 3 and 4 correspond to the same model, that is: there was _no_ retraining of models for different points of the same line. In particular, models were not trained for the specific patch size or aspect ratio. This implies that the same model parameters confer certified robustness to a wide variety of patch shapes.\nThanks for raising this point; we will state this detail more clearly in the paper.\n\nQuestion 1: \"Is it possible to apply the developed certification condition to prior work? It seems that prediction by region voting is the only requirement for applicability.\"\nThe reviewer is correct, Condition 3.2 would be applicable to Derandomized Smoothing (Levine and Feizi) and we would expect similar benefits of using Condition 3.2 over Condition 3.3, that is an improvement of approx. 3 percent points of certified accuracy for Derandomized Smoothing. We see it actually as a strength of our work that individual contributions like Condition 3.2 are sufficiently general such that they are not restricted to BagCert but also applicable to other approaches.\nFor PatchGuard (Xiang et al.), Condition 3.2 and 3.3 are not directly applicable because they assume a spatial-sum aggregation, while PatchGuard applies a detect-and-mask operation in the aggregation. Condition 3.1 only requires monotonicity; the detect-and-mask step in PatchGuard makes it (to our understanding) also non-monotonic. But one could potentially modify PatchGuard to using clipping instead of masking and by this make it satisfy the assumptions of Condition 3.1. This would be interesting future work; we will add this to the outlook of the paper.\n\nQuestion 2: \"Does it make sense to consider distributions for $R^{max}(L)$ with densities shifted towards larger values?\"\nYes, it makes sense to consider different distributions for $R^{max}(L)$ if one has prior assumptions on typical patch sizes. In general, there is a trade-off between certified accuracy for small patches vs. certified accuracy for larger patches: this can be observed in Figure 3 (right) where the lines corresponding to different margins cross. So using \"distributions for $R^{max}(L)$ with densities shifted towards larger values\" will likely impair robustness against small patches (and clean accuracy). We use a uniform distribution because we want to stay agnostic with respect to the patch size (up to R) in this paper. But different choices for the distribution would be feasible, provided that they result in a closed form expression for the loss (or ones that can be efficiently approximated).\n\nWe will shortly upload a revised version of the paper. In case our response leaves questions open, we would be grateful for discussing these further."}, "signatures": ["ICLR.cc/2021/Conference/Paper426/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper426/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Certified Defenses Against Patch Attacks on Image Classifiers", "authorids": ["~Jan_Hendrik_Metzen1", "~Maksym_Yatsura1"], "authors": ["Jan Hendrik Metzen", "Maksym Yatsura"], "keywords": ["robustness", "certified defense", "adversarial patch", "aversarial examples"], "abstract": "Adversarial patches pose a realistic threat model for physical world attacks on autonomous systems via their perception component. Autonomous systems in safety-critical domains such as automated driving should thus contain a fail-safe fallback component that combines certifiable robustness against patches with efficient inference while maintaining high performance on clean inputs. We propose BagCert, a novel combination of model architecture and certification procedure that allows efficient certification. We derive a loss that enables end-to-end optimization of certified robustness against patches of different sizes and locations. On CIFAR10, BagCert certifies 10.000 examples in 43 seconds on a single GPU and obtains 86% clean and 60% certified accuracy against 5x5 patches.", "one-sentence_summary": "We propose a method for certifying robustness against adversarial patches that combines high certified accuracy with efficient inference while maintaining strong performance on clean data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "metzen|efficient_certified_defenses_against_patch_attacks_on_image_classifiers", "supplementary_material": "", "pdf": "/pdf/964f848b9674e016ee4ad4259c6491fd8c66729a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmetzen2021efficient,\ntitle={Efficient Certified Defenses Against Patch Attacks on Image Classifiers},\nauthor={Jan Hendrik Metzen and Maksym Yatsura},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=hr-3PMvDpil}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "hr-3PMvDpil", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper426/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper426/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper426/Authors|ICLR.cc/2021/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper426/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871103, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper426/-/Official_Comment"}}}, {"id": "8kcqdHhSeGN", "original": null, "number": 6, "cdate": 1605265134878, "ddate": null, "tcdate": 1605265134878, "tmdate": 1605265134878, "tddate": null, "forum": "hr-3PMvDpil", "replyto": "MwVKmHxYUdW", "invitation": "ICLR.cc/2021/Conference/Paper426/-/Official_Comment", "content": {"title": "Clarification on Heaviside Step Function and IBP", "comment": "We would like to thank the reviewer for the time devoted to reviewing the paper and the constructive feedback. We will try to clarify the open points:\n\n1) \"Why is it necessary to have the heaviside step function in the forward pass? Why wouldn't using the sigmoid function in the forward pass work?\"\nThanks for bringing up this point. Actually, we conducted additional experiments (not yet contained in the paper) with alternative choices such as softmax and sigmoid. Softmax generally performed worse than the Heaviside step function. We attribute this to softmax enforcing  $\\sum_{c} s_{i,j, c} = 1$, while the element-wise step function and sigmoid are more flexible. That seems to be helpful when dealing with small patches that can be intrinsically ambiguous.\nFor the sigmoid function, we noticed that it can work just as well as the Heaviside step function in principle. However, we also noticed that in some runs it \"stalled\" on low accuracy levels on CIFAR10. We never observed this behaviour for the step function. It might be feasible to stabilize training with the sigmoid by careful hyperparameter tuning, but we sticked to the Heaviside step function which worked robustly for us. We will update the paper with according results.\n\n2) \"All the reasoning for robustness is here done at the aggregation phase, essentially enforcing that even flipping completely a prediction for a localized region does not change the global prediction. Could this be combined with the method of Chiang et al. to show that some regions can't be changed by more than a certain amount?\"\nThanks for this suggestion. From a quick glance, IBP would allow to have tighter bounds in $s^{wc}$ than the current trivial upper bound 1 and lower bound 0. These tighter bounds would then propagate to Conditions 3.1. and 3.2 and make these stronger. This sounds like a very promising follow-up for BagCert indeed. We will add this as a potential future direction to the paper, thanks again!\n\nWe will shortly upload a revised version of the paper. In case our response leaves questions open, we would be grateful for discussing these further."}, "signatures": ["ICLR.cc/2021/Conference/Paper426/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper426/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Certified Defenses Against Patch Attacks on Image Classifiers", "authorids": ["~Jan_Hendrik_Metzen1", "~Maksym_Yatsura1"], "authors": ["Jan Hendrik Metzen", "Maksym Yatsura"], "keywords": ["robustness", "certified defense", "adversarial patch", "aversarial examples"], "abstract": "Adversarial patches pose a realistic threat model for physical world attacks on autonomous systems via their perception component. Autonomous systems in safety-critical domains such as automated driving should thus contain a fail-safe fallback component that combines certifiable robustness against patches with efficient inference while maintaining high performance on clean inputs. We propose BagCert, a novel combination of model architecture and certification procedure that allows efficient certification. We derive a loss that enables end-to-end optimization of certified robustness against patches of different sizes and locations. On CIFAR10, BagCert certifies 10.000 examples in 43 seconds on a single GPU and obtains 86% clean and 60% certified accuracy against 5x5 patches.", "one-sentence_summary": "We propose a method for certifying robustness against adversarial patches that combines high certified accuracy with efficient inference while maintaining strong performance on clean data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "metzen|efficient_certified_defenses_against_patch_attacks_on_image_classifiers", "supplementary_material": "", "pdf": "/pdf/964f848b9674e016ee4ad4259c6491fd8c66729a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmetzen2021efficient,\ntitle={Efficient Certified Defenses Against Patch Attacks on Image Classifiers},\nauthor={Jan Hendrik Metzen and Maksym Yatsura},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=hr-3PMvDpil}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "hr-3PMvDpil", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper426/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper426/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper426/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper426/Authors|ICLR.cc/2021/Conference/Paper426/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper426/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871103, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper426/-/Official_Comment"}}}, {"id": "wCjfEX7E1Kv", "original": null, "number": 1, "cdate": 1603438851255, "ddate": null, "tcdate": 1603438851255, "tmdate": 1605024691861, "tddate": null, "forum": "hr-3PMvDpil", "replyto": "hr-3PMvDpil", "invitation": "ICLR.cc/2021/Conference/Paper426/-/Official_Review", "content": {"title": "This paper presents a provable defense method called BAGCERT against patch attacks which uses an invariant of BagNet for certification. ", "review": "This paper presents a provable defense method called BAGCERT against patch attacks which uses an invariant of BagNet for certification. By using the network with small receptive fields, this paper first analyzes the worst-case classification. The basic certification process is created by using a novel aggregation function. Finally, after using the same certification conditions as the Derandomized smoothing (Levine & Feizi, 2020), the certification could be evaluated within constant time. To further reduce the impact of the adversarial patch, the proposed method uses the certification condition as the objective loss to train the network. Empirical studies show the superiority of BAGCERT over other approaches.\n\nAdvantages: \n1.The paper gives good formal descriptions and rigorous proofs. \n2.The certification section is well structured. The narrative is logically layered and well-organized. \n3. The paper gives a SOTA certified accuracy with high clean accuracy on ImageNet. \n\nConcerns: \n1. Overall, the paper uses essentially the same certification conditions as the Derandomized smoothing except that it replaces the network architecture with BagNet. In addition, the table that describes certification time shows that number of parameters of the proposed method is much larger than Derandomized smoothing. Is it fair to compare accuracy under networks with varying numbers of parameters that differ too much (38M:11M)?\n2. Could you provide the training cost compared with Derandomized smoothing? In theory, the method is faster. \n3. The description of R(l) in 3.1 is so unclear that it is difficult to understand the meaning of R(l).\n4. The supplementary material should give some experimental results of the application of the proposed method to practical examples, for example, can you provide some results that uses some existing patch attack methods for evaluation?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper426/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper426/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Certified Defenses Against Patch Attacks on Image Classifiers", "authorids": ["~Jan_Hendrik_Metzen1", "~Maksym_Yatsura1"], "authors": ["Jan Hendrik Metzen", "Maksym Yatsura"], "keywords": ["robustness", "certified defense", "adversarial patch", "aversarial examples"], "abstract": "Adversarial patches pose a realistic threat model for physical world attacks on autonomous systems via their perception component. Autonomous systems in safety-critical domains such as automated driving should thus contain a fail-safe fallback component that combines certifiable robustness against patches with efficient inference while maintaining high performance on clean inputs. We propose BagCert, a novel combination of model architecture and certification procedure that allows efficient certification. We derive a loss that enables end-to-end optimization of certified robustness against patches of different sizes and locations. On CIFAR10, BagCert certifies 10.000 examples in 43 seconds on a single GPU and obtains 86% clean and 60% certified accuracy against 5x5 patches.", "one-sentence_summary": "We propose a method for certifying robustness against adversarial patches that combines high certified accuracy with efficient inference while maintaining strong performance on clean data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "metzen|efficient_certified_defenses_against_patch_attacks_on_image_classifiers", "supplementary_material": "", "pdf": "/pdf/964f848b9674e016ee4ad4259c6491fd8c66729a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmetzen2021efficient,\ntitle={Efficient Certified Defenses Against Patch Attacks on Image Classifiers},\nauthor={Jan Hendrik Metzen and Maksym Yatsura},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=hr-3PMvDpil}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "hr-3PMvDpil", "replyto": "hr-3PMvDpil", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper426/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538143387, "tmdate": 1606915799362, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper426/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper426/-/Official_Review"}}}, {"id": "4P_QTkuPr00", "original": null, "number": 2, "cdate": 1603783250589, "ddate": null, "tcdate": 1603783250589, "tmdate": 1605024691780, "tddate": null, "forum": "hr-3PMvDpil", "replyto": "hr-3PMvDpil", "invitation": "ICLR.cc/2021/Conference/Paper426/-/Official_Review", "content": {"title": "Convincing novel training and robustness certification approach for adversarial patches in image classifiers", "review": "This paper considers a problem of the defense against adversarial patch insertion attacks for image classification. Namely, it considers rectangular adversarial patches of fixed sizes and aspect ratios inserted in arbitrary locations of input images and requires from the desired model to obtain good classification performance on both clean and corrupted versions of datasets. Moreover, it is desired for results on the corrupted data to have certified robustness (theoretically guarantied classification performance given the model and the parameters of the attack).\n\nTo deal with this problem, this work uses a combination of ideas. Following prior work, it proposes to use CNN architectures with small receptive fields that compute class predictions for every spatial region in output feature maps, and to aggregate these predictions in a voting manner to obtain final class probabilities. The authors formally address the problem of certification and derive a novel stronger certification condition (compared to prior work), which allows to guarantee better performance for the proposed models. Moreover, the authors propose a novel training objective which is based on the certification condition, which allows for an end-to-end optimization of the model directly for the certified performance, in contrast to performing post-hoc adjustments to achieve certified robustness with pretrained models. \n\nIn practice, the proposed approach achieves superior certified performance on CIFAR10 and ImageNet datasets, while maintaining high clean accuracy. Additionally the certification process is reported to be about an order of magnitude faster compared to prior work. The approach is also shown to be more robust to rectangular adversarial patches.\n\nPros:\n1) The paper is well-written and is pleasant to follow.\n2) The proposed certification condition and training objective are novel and, as far as I can tell, are technically sound.\n3) Formal analysis incorporates and properly relates the certification condition from prior work.\n4) Experimental results are convincing and additionally include efficiency comparison.\n\nCons:\n1) Might be not a con, but for me it is not clear which protocol is used for evaluations in experiments with varying patch sizes. Do I understand correctly that each point in figures 3 and 4 was obtained by retraining models from scratch with the chosen patch configuration? If that is the case, you can not imply that a single model is robust for different kinds of attacks, in that case it is the configuration of the model which is robust, but you will still need unique model parameters for any specific patch configuration.\n\nQuestion:\n1) Is it possible to apply the developed certification condition to prior work? It seems that prediction by region voting is the only requirement for applicability. If so, have you tried to use it on any prior works?\n2) When you derive the objective from the certification condition 3.3 you assume that the size of the maximum affected patch score region is uniformly distributed. As far as I understand, this makes the objective robust to variations in patch sizes and geometry. In practice though, it is harder to certify robustness for larger stretched patches (Figures 3, 4). Does it make sense to consider distributions for $R^{max}(\\mathcal{L})$ with densities shifted towards larger values?\n\nOverall, I believe this is a strong paper, containing both theoretical and practical novel contributions and I think it should be accepted.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper426/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper426/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Certified Defenses Against Patch Attacks on Image Classifiers", "authorids": ["~Jan_Hendrik_Metzen1", "~Maksym_Yatsura1"], "authors": ["Jan Hendrik Metzen", "Maksym Yatsura"], "keywords": ["robustness", "certified defense", "adversarial patch", "aversarial examples"], "abstract": "Adversarial patches pose a realistic threat model for physical world attacks on autonomous systems via their perception component. Autonomous systems in safety-critical domains such as automated driving should thus contain a fail-safe fallback component that combines certifiable robustness against patches with efficient inference while maintaining high performance on clean inputs. We propose BagCert, a novel combination of model architecture and certification procedure that allows efficient certification. We derive a loss that enables end-to-end optimization of certified robustness against patches of different sizes and locations. On CIFAR10, BagCert certifies 10.000 examples in 43 seconds on a single GPU and obtains 86% clean and 60% certified accuracy against 5x5 patches.", "one-sentence_summary": "We propose a method for certifying robustness against adversarial patches that combines high certified accuracy with efficient inference while maintaining strong performance on clean data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "metzen|efficient_certified_defenses_against_patch_attacks_on_image_classifiers", "supplementary_material": "", "pdf": "/pdf/964f848b9674e016ee4ad4259c6491fd8c66729a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmetzen2021efficient,\ntitle={Efficient Certified Defenses Against Patch Attacks on Image Classifiers},\nauthor={Jan Hendrik Metzen and Maksym Yatsura},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=hr-3PMvDpil}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "hr-3PMvDpil", "replyto": "hr-3PMvDpil", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper426/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538143387, "tmdate": 1606915799362, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper426/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper426/-/Official_Review"}}}, {"id": "MwVKmHxYUdW", "original": null, "number": 3, "cdate": 1604675556917, "ddate": null, "tcdate": 1604675556917, "tmdate": 1605024691716, "tddate": null, "forum": "hr-3PMvDpil", "replyto": "hr-3PMvDpil", "invitation": "ICLR.cc/2021/Conference/Paper426/-/Official_Review", "content": {"title": "Simple approach to verified robustness to patch attack producing good results.", "review": "Summary:\nThis paper deals with obtaining verified bounds on the accuracy of a model under attack restricted to patch modification (only a small, localized group of pixels can be modified). As opposed to previous methods (Chiang et al, 2020) which simply applied existing verification methods to the problem by enumerating possible patch locations, the proposed approach consist in modifying the structure of the network such that predictions are made densely and then aggregated. The dense prediction means that only some of the predictions can be affected by the adversarial patch, and so the certification process can use this to reason about the robustness at the aggregation level.\nFor network with small receptive fields, this will be particularly efficient (few predictions can be affected) so the authors propose to use CNN with small filter convolution in order to make the network more verifiable. Incorporating the verification procedure into the training objective is also described.\n\nThe proposed method achieves comparable or better results in terms of verified accuracy and nominal accuracy, while being significantly faster.\n\nComments:\n- Figure 1 is quite helpful for understanding the principle of the proposed architecture.\n- Why is it necessary to have the heaviside step function in the forward pass? Why wouldn't using the sigmoid function in the forward pass work? That way, the gradient used would actually corresponds to the objective.\n- All the reasoning for robustness is here done at the aggregation phase, essentially enforcing that even flipping completely a prediction for a localized region does not change the global prediction. Could this be combined with the method of Chiang et al. to show that some regions can't be changed by more than a certain amount? This might allow the certification method to deal with networks with larger receptive fields?\n\nOpinion:\nThe paper provides a simple, interesting approach, and describes it clearly. The empirical performance is also validated on both CIFAR and ImageNet.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper426/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper426/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Certified Defenses Against Patch Attacks on Image Classifiers", "authorids": ["~Jan_Hendrik_Metzen1", "~Maksym_Yatsura1"], "authors": ["Jan Hendrik Metzen", "Maksym Yatsura"], "keywords": ["robustness", "certified defense", "adversarial patch", "aversarial examples"], "abstract": "Adversarial patches pose a realistic threat model for physical world attacks on autonomous systems via their perception component. Autonomous systems in safety-critical domains such as automated driving should thus contain a fail-safe fallback component that combines certifiable robustness against patches with efficient inference while maintaining high performance on clean inputs. We propose BagCert, a novel combination of model architecture and certification procedure that allows efficient certification. We derive a loss that enables end-to-end optimization of certified robustness against patches of different sizes and locations. On CIFAR10, BagCert certifies 10.000 examples in 43 seconds on a single GPU and obtains 86% clean and 60% certified accuracy against 5x5 patches.", "one-sentence_summary": "We propose a method for certifying robustness against adversarial patches that combines high certified accuracy with efficient inference while maintaining strong performance on clean data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "metzen|efficient_certified_defenses_against_patch_attacks_on_image_classifiers", "supplementary_material": "", "pdf": "/pdf/964f848b9674e016ee4ad4259c6491fd8c66729a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmetzen2021efficient,\ntitle={Efficient Certified Defenses Against Patch Attacks on Image Classifiers},\nauthor={Jan Hendrik Metzen and Maksym Yatsura},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=hr-3PMvDpil}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "hr-3PMvDpil", "replyto": "hr-3PMvDpil", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper426/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538143387, "tmdate": 1606915799362, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper426/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper426/-/Official_Review"}}}, {"id": "Ev-AhjOKGA4", "original": null, "number": 4, "cdate": 1604928823732, "ddate": null, "tcdate": 1604928823732, "tmdate": 1605024691654, "tddate": null, "forum": "hr-3PMvDpil", "replyto": "hr-3PMvDpil", "invitation": "ICLR.cc/2021/Conference/Paper426/-/Official_Review", "content": {"title": "Promising results on certifiable robustness to visible adversarial noise. Hard to decipher the level of technical contribution.", "review": "This paper investigates provable defenses to visible adversarial perturbations. Specifically the authors concentrate on defending against adversarial patches as introduced by Brown et al. [1] and not on other formulations of visible adversarial noise such as LaVAN (Karmon et al. [2]). The provable defense builds upon work by Xiang et al. [3], who utilise robust aggregation and BagNets to show the model can always recover correct predictions on certified images against any adversarial patch. The idea behind the defense is to constrain the receptive field size in convolutional layers; given a small adversarial patch and a large receptive field, the adversarial patch will be present in most extracted features, and so is more likely to change the model\u2019s prediction. By limiting the size of the receptive field, the adversarial patch can only infect a small number of extracted features, after which the extracted features are binarized and robustly aggregated. The authors also introduce a new margin-based loss that encourages the model to be certifiable. Experiments with small adversarial patches on CIFAR-10 and ImageNet point to the efficacy of the approach. BagCert appears to outperform related certifiable approaches to patch attacks. The main boon of this approach lies in the fast certification time, since a constant number of forward-passes are required. The authors additionally have experiments showing the equivalence in robust accuracy against different shaped patches covering the same overall area size. Overall, I thought the paper was well-written, easy to follow and contains some interesting ideas. However, it is difficult to assess the level of technical contribution made here, since the core contributions in this work are also contained in Xiang et al [3].\n\nAs alluded to above, my main concern is the level of technical contribution made in this work. As far as I understand, this work is grounded in the ideas presented by Xiang et al [3] who use small receptive fields as a building block for robust classification. The main differences lie in the method of robust aggregation and that, here, the authors introduce a regularisation loss that encourages the model to be certifiable. As far as I could tell, a similar loss could have been introduced using the Xiang et al. approach and it is not specific to using the heaviside method of discretisation. It would be helpful if the authors please delineate the differences and contributions between this work and Xiang et al.? It is slightly disingenuous to say these works are concurrent contributions as stated in the contributions sections, since this work has been submitted ~4-5 months after Xiang et al. [3] was made publicly available.\n\nCould this defense be applied to attacks that contain multiple, small localised patches (c.f. Karmon et al. [2])? I think the answer is probably yes, but it would be great to see an analysis in this direction.\n\nThe condition of a small receptive field implies an inherent trade-off between clean and robust accuracy. How should one tune the receptive field size to minimise this trade-off? This defense in its current formulation seems to be quite specific to image classification. Could this be extended to other modalities of data that use architectures that often extract global (instead of local) structure within data inputs (e.g. Transformers and NLP tasks)?\n\nIn the experiments, are the results compared against vanilla implementations of related approaches, or against approaches with relevant hyperparameters tuned? For example, when comparing with Xiang et al. [3], did you perform a sweep over receptive field sizes that minimise the clean-robust accuracy trade-off, or a sweep over the detection threshold, T? \n\nAs far as I understand, in Levine et al. [4], band smoothing can certify both column and row based patches (as opposed to square patches). In Figure 4, did the authors try to compare against row-based smoothing techniques (in addition to column smoothing)? I expect that this may solve the problem of zero certified accuracy on row patches. I encourage the authors to check this as it seems the Levine et al. [4] approach is outperforming BagCert for non-square adversarial patches.\n\n\n[1] Brown, Tom B., et al. \"Adversarial patch.\" arXiv preprint arXiv:1712.09665 (2017).\n\n[2] Karmon, D., Zoran, D., and Goldberg, Y. Lavan: Localized and visible adversarial noise. arXiv preprint arXiv:1801.02608, 2018.\n\n[3] Xiang, Chong, et al. \"PatchGuard: Provable Defense against Adversarial Patches Using Masks on Small Receptive Fields.\" arXiv preprint arXiv:2005.10884 (2020).\n\n[4] Levine, Alexander, and Soheil Feizi. \"(De) Randomized Smoothing for Certifiable Defense against Patch Attacks.\" arXiv preprint arXiv:2002.10733 (2020).\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper426/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper426/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Certified Defenses Against Patch Attacks on Image Classifiers", "authorids": ["~Jan_Hendrik_Metzen1", "~Maksym_Yatsura1"], "authors": ["Jan Hendrik Metzen", "Maksym Yatsura"], "keywords": ["robustness", "certified defense", "adversarial patch", "aversarial examples"], "abstract": "Adversarial patches pose a realistic threat model for physical world attacks on autonomous systems via their perception component. Autonomous systems in safety-critical domains such as automated driving should thus contain a fail-safe fallback component that combines certifiable robustness against patches with efficient inference while maintaining high performance on clean inputs. We propose BagCert, a novel combination of model architecture and certification procedure that allows efficient certification. We derive a loss that enables end-to-end optimization of certified robustness against patches of different sizes and locations. On CIFAR10, BagCert certifies 10.000 examples in 43 seconds on a single GPU and obtains 86% clean and 60% certified accuracy against 5x5 patches.", "one-sentence_summary": "We propose a method for certifying robustness against adversarial patches that combines high certified accuracy with efficient inference while maintaining strong performance on clean data.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "metzen|efficient_certified_defenses_against_patch_attacks_on_image_classifiers", "supplementary_material": "", "pdf": "/pdf/964f848b9674e016ee4ad4259c6491fd8c66729a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmetzen2021efficient,\ntitle={Efficient Certified Defenses Against Patch Attacks on Image Classifiers},\nauthor={Jan Hendrik Metzen and Maksym Yatsura},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=hr-3PMvDpil}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "hr-3PMvDpil", "replyto": "hr-3PMvDpil", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper426/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538143387, "tmdate": 1606915799362, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper426/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper426/-/Official_Review"}}}], "count": 13}