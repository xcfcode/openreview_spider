{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487089530143, "tcdate": 1478283345718, "number": 285, "id": "BJ5UeU9xx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BJ5UeU9xx", "signatures": ["~Luisa_M_Zintgraf1"], "readers": ["everyone"], "content": {"title": "Visualizing Deep Neural Network Decisions: Prediction Difference Analysis", "abstract": "This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).", "pdf": "/pdf/9c0dd32a98e96fc0222dd2bcd99cc21e599852e9.pdf", "TL;DR": "Method for visualizing evidence for and against deep convolutional neural network classification decisions in a given input image.", "paperhash": "zintgraf|visualizing_deep_neural_network_decisions_prediction_difference_analysis", "conflicts": ["uva.nl", "vub.ac.be", "cifar.ca", "uwaterloo.ca", "ru.nl", "openai.com"], "keywords": ["Deep learning", "Applications"], "authors": ["Luisa M Zintgraf", "Taco S Cohen", "Tameem Adel", "Max Welling"], "authorids": ["lmzintgraf@gmail.com", "t.s.cohen@uva.nl", "tameem.hesham@gmail.com", "m.welling@uva.nl"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396480942, "tcdate": 1486396480942, "number": 1, "id": "BJFHhz8Ox", "invitation": "ICLR.cc/2017/conference/-/paper285/acceptance", "forum": "BJ5UeU9xx", "replyto": "BJ5UeU9xx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "Reviewers felt the paper was clearly written with necessary details given, leading to a paper that was pleasant to read. The differences with prior art raised by one of the reviewers were adequately addressed by the authors in a revision. The paper presents results on both ImageNet and medical imagery, an aspect of the paper that was appreciated by reviewers.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visualizing Deep Neural Network Decisions: Prediction Difference Analysis", "abstract": "This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).", "pdf": "/pdf/9c0dd32a98e96fc0222dd2bcd99cc21e599852e9.pdf", "TL;DR": "Method for visualizing evidence for and against deep convolutional neural network classification decisions in a given input image.", "paperhash": "zintgraf|visualizing_deep_neural_network_decisions_prediction_difference_analysis", "conflicts": ["uva.nl", "vub.ac.be", "cifar.ca", "uwaterloo.ca", "ru.nl", "openai.com"], "keywords": ["Deep learning", "Applications"], "authors": ["Luisa M Zintgraf", "Taco S Cohen", "Tameem Adel", "Max Welling"], "authorids": ["lmzintgraf@gmail.com", "t.s.cohen@uva.nl", "tameem.hesham@gmail.com", "m.welling@uva.nl"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396481436, "id": "ICLR.cc/2017/conference/-/paper285/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BJ5UeU9xx", "replyto": "BJ5UeU9xx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396481436}}}, {"tddate": null, "tmdate": 1485524523836, "tcdate": 1485524523836, "number": 7, "id": "SkE4RpOwg", "invitation": "ICLR.cc/2017/conference/-/paper285/public/comment", "forum": "BJ5UeU9xx", "replyto": "S17b8aZPe", "signatures": ["~Luisa_M_Zintgraf1"], "readers": ["everyone"], "writers": ["~Luisa_M_Zintgraf1"], "content": {"title": "Quantitative Evaluation", "comment": "Thank you again for your comment. \n\nHere are some thoughts on why we think the grey patch method seems to work better in the \u201cdish rag\u201d example:\n- Since the entire image is so uniform, we think that using a grey patch always introduces the same amount of different information; i.e, the original image + grey patch all look very similar (independent of where the grey patch is) but at the same time very different from the original image (without the grey patch). This way, the prediction difference is always very high (the classifier is drawn to prediction some other class that is closer to things with grey patches) across the whole region. Our method on the other side puts samples into the image that try to fill out the missing region. The differences are much more subtle, and the prediction difference more sensitive to the quality of the conditional sampling. This problem seems to occur, as you point out, if the content of interest has uniform texture - if there are uniform regions in the image plus some other object of interest (e.g., the parachute) we do not get this behaviour. We believe that using a better model for the conditional distribution in this case could lead to better results. We used a normal joint distribution over pixel patches, which might not be sufficient to replicate texture well enough.\n- Another thing to investigate would be to find out how the CNN usually makes a \"dish rag\" prediction (in this example, it misclassifies it and ranks \"dish rag\" as the 4th probable class); is it the texture, color, or shape? That way we could get a better sense of what the CNN is looking for, and make a better evaluation of the results.\n\nQuantitative evaluation of methods that explain classifier decisions we believe are difficult, since it is not clear what the desired output is, and thus how we should evaluate an explanation. Segmentation masks are very close to how human decision-making is done, but can be very different from how a classifier makes decisions. The main issue is that the classifier might indeed (as you pointed out) not focus on the object itself, but more on contextual information. In our experiments we observed that the method of Simonyan et al. (sensitivity maps using partial derivatives of the class score wrt the input pixels) more often tends to highlight the object itself, compared to our method. Simonyan et al. have demonstrated in their paper that their method can be used for segmentation tasks, but we do not think that our method is necessarily suited for that since it serves a different purpose. Simonyan et al. highlight for which pixels the CNN output is most sensitive to small changes in value, whereas our proposed method evaluates how much and in what way image regions influence the decision. Consequently, our method might 'split up' objects: part of a cat can look like a tabby cat, whereas a different part can look more like a siamese cat (and there are about 5 cat species in the ImageNet classes). That means we would have to use the thresholded heat maps of the absolute values, which in turn could lead to issues for other classes. Therefore, we believe that finding appropriate evaluation metrics is an interesting direction for future work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visualizing Deep Neural Network Decisions: Prediction Difference Analysis", "abstract": "This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).", "pdf": "/pdf/9c0dd32a98e96fc0222dd2bcd99cc21e599852e9.pdf", "TL;DR": "Method for visualizing evidence for and against deep convolutional neural network classification decisions in a given input image.", "paperhash": "zintgraf|visualizing_deep_neural_network_decisions_prediction_difference_analysis", "conflicts": ["uva.nl", "vub.ac.be", "cifar.ca", "uwaterloo.ca", "ru.nl", "openai.com"], "keywords": ["Deep learning", "Applications"], "authors": ["Luisa M Zintgraf", "Taco S Cohen", "Tameem Adel", "Max Welling"], "authorids": ["lmzintgraf@gmail.com", "t.s.cohen@uva.nl", "tameem.hesham@gmail.com", "m.welling@uva.nl"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287640500, "id": "ICLR.cc/2017/conference/-/paper285/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ5UeU9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper285/reviewers", "ICLR.cc/2017/conference/paper285/areachairs"], "cdate": 1485287640500}}}, {"tddate": null, "tmdate": 1485063675229, "tcdate": 1485063675229, "number": 4, "id": "S17b8aZPe", "invitation": "ICLR.cc/2017/conference/-/paper285/official/comment", "forum": "BJ5UeU9xx", "replyto": "HJlIXVZ8g", "signatures": ["ICLR.cc/2017/conference/paper285/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper285/AnonReviewer3"], "content": {"title": "Comparison with Zeiler et al. ", "comment": "Thanks for including the comparison with Zeiler et al. While for categories such as \"dam\" and \"scuba diver\" the proposed method works better, for other categories such as \"dish rag\" - the grey area based method seems to work better. It seems like if the content of interest such as sky or rag has uniform texture then the proposed method will not capture the entire area. What do the authors think about it? \n\nSecondly, it would be great if quantitative evaluation between the proposed and past approaches is made. One way would be to use the PASCAL dataset that has segmentation masks for objects. The segmentation masks obtained by thresholding the heatmaps can be compared against these segmentation masks. This comparison won't be ideal because for many objects only a subset of pixels might constitute discriminative regions and thus if the visualization technique correctly identifies these pixels it will be penalized in the intersection over union metric used for evaluation segmentation predictions. This issue can be partially mitigated by using the following metric - count of pixels within the object mask / count of pixels outside the object mask. This metric is going to penalize false positives (i.e. regions outside the object masks that are thought to be important but are actually not). Even this metric is not perfect, because the CNN might be using context for classification. However, the standard segmentation and the metric proposed above are good starting points for quantitative evaluation. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visualizing Deep Neural Network Decisions: Prediction Difference Analysis", "abstract": "This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).", "pdf": "/pdf/9c0dd32a98e96fc0222dd2bcd99cc21e599852e9.pdf", "TL;DR": "Method for visualizing evidence for and against deep convolutional neural network classification decisions in a given input image.", "paperhash": "zintgraf|visualizing_deep_neural_network_decisions_prediction_difference_analysis", "conflicts": ["uva.nl", "vub.ac.be", "cifar.ca", "uwaterloo.ca", "ru.nl", "openai.com"], "keywords": ["Deep learning", "Applications"], "authors": ["Luisa M Zintgraf", "Taco S Cohen", "Tameem Adel", "Max Welling"], "authorids": ["lmzintgraf@gmail.com", "t.s.cohen@uva.nl", "tameem.hesham@gmail.com", "m.welling@uva.nl"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287640362, "id": "ICLR.cc/2017/conference/-/paper285/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BJ5UeU9xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper285/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper285/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper285/reviewers", "ICLR.cc/2017/conference/paper285/areachairs"], "cdate": 1485287640362}}}, {"tddate": null, "tmdate": 1483977544279, "tcdate": 1483977544279, "number": 6, "id": "HJlIXVZ8g", "invitation": "ICLR.cc/2017/conference/-/paper285/public/comment", "forum": "BJ5UeU9xx", "replyto": "H12UwISrg", "signatures": ["~Luisa_M_Zintgraf1"], "readers": ["everyone"], "writers": ["~Luisa_M_Zintgraf1"], "content": {"title": "Grey areas", "comment": "Thank you very much for your review. \n\nThe method of Zeiler et al. is indeed similar to our method in that they also remove information in of the image (by occluding parts of it with a grey patch) to evaluate how important image regions are (by looking at the correct class probability as a function of the grey patch, whereas we look at the difference in output probabilities). \n\nHowever we think that when using this grey patch, there is not only information removed/occluded from the image, but also new information introduced - a grey patch looks like *something* to the classifier (e.g., the sky on a rainy day, the carrosserie of a grey car) and we therefore think that by just replacing pixels with their mean value (which is approx grey for ImageNet) could bias the results depending on what the classifier learned about which images usually contain grey areas. The output of the classifier would therefore always shift towards the classes that frequently contain a lot of grey. By instead using patches from all classes (like in the marginal distribution) and averaging the class scores, this bias can be removed (and even further by using the conditional distribution). \n\nWe ran some more experiments, and you can see the results when using a grey patch instead of conditional sampling here:\nhttps://www.dropbox.com/s/af5n8frrn0g0mh6/comparison_graySquare.png\nFor many examples the difference is not very large, but for example in the \"dam\" or \"scuba diver\" image we can see how the method assigns too much relevance to rather uniform (and uninformative) regions like the sky. Given this and our theoretical reasoning, we believe that using conditional sampling is in general more sensible. However, an interesting subject of future research is to use more sophisticated models for conditional sampling, which might also include more information from the whole image (as another reviewer suggested). We believe that unimportant regions that are easily predictable by other (neighbouring) pixels could then be downweighed even more which would lead to even more interpretable results.\n\nThe reason we used 10 samples for both marginal and conditional sampling were that on the one hand we didn't see a significant difference when using more samples, and also we wanted to have comparable computation times (conditional sampling takes just slightly longer due to the sampling procedure). Also, marginal sampling in the limit does not correspond to  just using one sample of the mean pixel value (i.e., approx grey), since the expectation cannot just be pushed into the computation of the class probabilities.\n\nWe have included the visualisations for random images (comparing to the method of Simonyan et al.) in the appendix of a revised version of the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visualizing Deep Neural Network Decisions: Prediction Difference Analysis", "abstract": "This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).", "pdf": "/pdf/9c0dd32a98e96fc0222dd2bcd99cc21e599852e9.pdf", "TL;DR": "Method for visualizing evidence for and against deep convolutional neural network classification decisions in a given input image.", "paperhash": "zintgraf|visualizing_deep_neural_network_decisions_prediction_difference_analysis", "conflicts": ["uva.nl", "vub.ac.be", "cifar.ca", "uwaterloo.ca", "ru.nl", "openai.com"], "keywords": ["Deep learning", "Applications"], "authors": ["Luisa M Zintgraf", "Taco S Cohen", "Tameem Adel", "Max Welling"], "authorids": ["lmzintgraf@gmail.com", "t.s.cohen@uva.nl", "tameem.hesham@gmail.com", "m.welling@uva.nl"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287640500, "id": "ICLR.cc/2017/conference/-/paper285/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ5UeU9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper285/reviewers", "ICLR.cc/2017/conference/paper285/areachairs"], "cdate": 1485287640500}}}, {"tddate": null, "tmdate": 1483200340061, "tcdate": 1483200340061, "number": 3, "id": "H12UwISrg", "invitation": "ICLR.cc/2017/conference/-/paper285/official/review", "forum": "BJ5UeU9xx", "replyto": "BJ5UeU9xx", "signatures": ["ICLR.cc/2017/conference/paper285/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper285/AnonReviewer3"], "content": {"title": "interesting approach to visualization", "rating": "6: Marginally above acceptance threshold", "review": "The paper presents a theoretically well motivated for visualizing what parts of the input feature map are responsible for the output decision. The key insight is that features that maximally change the output and are simultaneously more unpredictable from other features are the most important ones. Most previous work has focused on finding features that maximally change the output without accounting for their predictability from other features. Authors build upon ideas presented in the work of Robnik-\u0160ikonja & Kononenko (2008).\n\nThe results indicate that the proposed visualization mechanism based on modeling conditional distribution identifies more salient regions as compared to a mechanism based on modeling marginal distribution. I like that authors have presented visualization results for a single image across multiple networks and multiple classes. There results show that the proposed method indeed picks up on class-discriminative features. Authors have provided a link to visualizations for a random sample of images in a comment \u2013 I encourage the authors to include this in the appendix of the paper. \n\nMy one concern with the paper is \u2013 Zeiler et al., proposed a visualization method by greying small square regions in the image. This is similar to computing the visualization using the marginal distribution. Authors compute the marginal visualization using 10 samples, however in the limit of infinite samples the image region would be gray. The conditional distribution is computed using a normal distribution that provides some regularization and therefore estimating the conditional and marginal distributions using 10 samples each is not justified. \nI would like to see the comparison when grey image patches (akin to Zeiler et al.) are used for visualization against the approach based on the conditional distribution. \n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visualizing Deep Neural Network Decisions: Prediction Difference Analysis", "abstract": "This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).", "pdf": "/pdf/9c0dd32a98e96fc0222dd2bcd99cc21e599852e9.pdf", "TL;DR": "Method for visualizing evidence for and against deep convolutional neural network classification decisions in a given input image.", "paperhash": "zintgraf|visualizing_deep_neural_network_decisions_prediction_difference_analysis", "conflicts": ["uva.nl", "vub.ac.be", "cifar.ca", "uwaterloo.ca", "ru.nl", "openai.com"], "keywords": ["Deep learning", "Applications"], "authors": ["Luisa M Zintgraf", "Taco S Cohen", "Tameem Adel", "Max Welling"], "authorids": ["lmzintgraf@gmail.com", "t.s.cohen@uva.nl", "tameem.hesham@gmail.com", "m.welling@uva.nl"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483200340725, "id": "ICLR.cc/2017/conference/-/paper285/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper285/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper285/AnonReviewer1", "ICLR.cc/2017/conference/paper285/AnonReviewer2", "ICLR.cc/2017/conference/paper285/AnonReviewer3"], "reply": {"forum": "BJ5UeU9xx", "replyto": "BJ5UeU9xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper285/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper285/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483200340725}}}, {"tddate": null, "tmdate": 1482953830826, "tcdate": 1482953830826, "number": 5, "id": "SJ1uVcWre", "invitation": "ICLR.cc/2017/conference/-/paper285/public/comment", "forum": "BJ5UeU9xx", "replyto": "S1MoqFq4l", "signatures": ["~Luisa_M_Zintgraf1"], "readers": ["everyone"], "writers": ["~Luisa_M_Zintgraf1"], "content": {"title": "Thank you", "comment": "Thanks a lot for the increased rating, and valuable input for improving the paper - we have updated it accordingly."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visualizing Deep Neural Network Decisions: Prediction Difference Analysis", "abstract": "This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).", "pdf": "/pdf/9c0dd32a98e96fc0222dd2bcd99cc21e599852e9.pdf", "TL;DR": "Method for visualizing evidence for and against deep convolutional neural network classification decisions in a given input image.", "paperhash": "zintgraf|visualizing_deep_neural_network_decisions_prediction_difference_analysis", "conflicts": ["uva.nl", "vub.ac.be", "cifar.ca", "uwaterloo.ca", "ru.nl", "openai.com"], "keywords": ["Deep learning", "Applications"], "authors": ["Luisa M Zintgraf", "Taco S Cohen", "Tameem Adel", "Max Welling"], "authorids": ["lmzintgraf@gmail.com", "t.s.cohen@uva.nl", "tameem.hesham@gmail.com", "m.welling@uva.nl"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287640500, "id": "ICLR.cc/2017/conference/-/paper285/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ5UeU9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper285/reviewers", "ICLR.cc/2017/conference/paper285/areachairs"], "cdate": 1485287640500}}}, {"tddate": null, "tmdate": 1482492570095, "tcdate": 1482492570095, "number": 3, "id": "S1MoqFq4l", "invitation": "ICLR.cc/2017/conference/-/paper285/official/comment", "forum": "BJ5UeU9xx", "replyto": "H1ZmztcVg", "signatures": ["ICLR.cc/2017/conference/paper285/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper285/AnonReviewer2"], "content": {"title": "Great!", "comment": "Thankyou for giving it a try! I agree, the red color scheme is even better. The added captions additionally make the results more interpretable because now one can see what the network has been looking for. As promised, I increased the grade of my review and I look forward to trying the code myself!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visualizing Deep Neural Network Decisions: Prediction Difference Analysis", "abstract": "This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).", "pdf": "/pdf/9c0dd32a98e96fc0222dd2bcd99cc21e599852e9.pdf", "TL;DR": "Method for visualizing evidence for and against deep convolutional neural network classification decisions in a given input image.", "paperhash": "zintgraf|visualizing_deep_neural_network_decisions_prediction_difference_analysis", "conflicts": ["uva.nl", "vub.ac.be", "cifar.ca", "uwaterloo.ca", "ru.nl", "openai.com"], "keywords": ["Deep learning", "Applications"], "authors": ["Luisa M Zintgraf", "Taco S Cohen", "Tameem Adel", "Max Welling"], "authorids": ["lmzintgraf@gmail.com", "t.s.cohen@uva.nl", "tameem.hesham@gmail.com", "m.welling@uva.nl"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287640362, "id": "ICLR.cc/2017/conference/-/paper285/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BJ5UeU9xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper285/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper285/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper285/reviewers", "ICLR.cc/2017/conference/paper285/areachairs"], "cdate": 1485287640362}}}, {"tddate": null, "tmdate": 1482492232006, "tcdate": 1481909908897, "number": 2, "id": "ByacUsbVg", "invitation": "ICLR.cc/2017/conference/-/paper285/official/review", "forum": "BJ5UeU9xx", "replyto": "BJ5UeU9xx", "signatures": ["ICLR.cc/2017/conference/paper285/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper285/AnonReviewer2"], "content": {"title": "Very interesting paper", "rating": "9: Top 15% of accepted papers, strong accept", "review": "The authors propose a way to visualize which areas of an image provide mostly influence a certain DNN response mostly. They apply some very elegant and convincing improvements to the basic method by Robnik-Sikonja and Konononko from 2008 to DNNs, thus improving it's analysis and making it usable for images and DNNs.\n\nThe authors provide a very thorough analysis of their methods and show very convincing examples (which they however handpicked. It would be very nice to have maybe at least one figure showing the analysis on e.g. 24 random picks from ImageNet).\nOne thing I would like to see is how their method compares to some other methods they mention in the introduction (like gradient-based ones or deconvolution based ones). \n\nThey paper is very clearly written, all necessary details are given and the paper is very nice to read.\n\nAlltogether: The problem of understanding how DNNs function and how they draw their conclusions is discussed a lot. The author's method provides a clear contribution that can lead to further progress in this field (E.g. I like figure 8 showing how AlexNet, GoogLeNet and VGG differ in where they collect evidence from). I can think of several potential applications of the method and therefore consider it of high significance.\n\nUpdate: The authors did a great job of adopting all of my suggestions. Therefore I improve the rating from 8 to 9.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visualizing Deep Neural Network Decisions: Prediction Difference Analysis", "abstract": "This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).", "pdf": "/pdf/9c0dd32a98e96fc0222dd2bcd99cc21e599852e9.pdf", "TL;DR": "Method for visualizing evidence for and against deep convolutional neural network classification decisions in a given input image.", "paperhash": "zintgraf|visualizing_deep_neural_network_decisions_prediction_difference_analysis", "conflicts": ["uva.nl", "vub.ac.be", "cifar.ca", "uwaterloo.ca", "ru.nl", "openai.com"], "keywords": ["Deep learning", "Applications"], "authors": ["Luisa M Zintgraf", "Taco S Cohen", "Tameem Adel", "Max Welling"], "authorids": ["lmzintgraf@gmail.com", "t.s.cohen@uva.nl", "tameem.hesham@gmail.com", "m.welling@uva.nl"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483200340725, "id": "ICLR.cc/2017/conference/-/paper285/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper285/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper285/AnonReviewer1", "ICLR.cc/2017/conference/paper285/AnonReviewer2", "ICLR.cc/2017/conference/paper285/AnonReviewer3"], "reply": {"forum": "BJ5UeU9xx", "replyto": "BJ5UeU9xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper285/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper285/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483200340725}}}, {"tddate": null, "tmdate": 1482490392958, "tcdate": 1482490392958, "number": 4, "id": "H1ZmztcVg", "invitation": "ICLR.cc/2017/conference/-/paper285/public/comment", "forum": "BJ5UeU9xx", "replyto": "HJCZE8tNx", "signatures": ["~Luisa_M_Zintgraf1"], "readers": ["everyone"], "writers": ["~Luisa_M_Zintgraf1"], "content": {"title": "Colors", "comment": "Thank you for your comment.\n\nThe reason we went with white on black was that Simonyan et al. use this (https://arxiv.org/pdf/1312.6034.pdf, page 5). But we agree, for comparison to our method it's better if white parts also correspond to pixels that have no influence on the result.\n\nWe tried black on white (https://www.dropbox.com/s/6bkg27i4uz84iew/results_random_wb.png?dl=0) and red on white (https://www.dropbox.com/s/0oa2lx2d06hn5yc/results_random_rw.png?dl=0) now, and think that the latter is the most interpretable (and still fits well to the color scheme we use for our method)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visualizing Deep Neural Network Decisions: Prediction Difference Analysis", "abstract": "This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).", "pdf": "/pdf/9c0dd32a98e96fc0222dd2bcd99cc21e599852e9.pdf", "TL;DR": "Method for visualizing evidence for and against deep convolutional neural network classification decisions in a given input image.", "paperhash": "zintgraf|visualizing_deep_neural_network_decisions_prediction_difference_analysis", "conflicts": ["uva.nl", "vub.ac.be", "cifar.ca", "uwaterloo.ca", "ru.nl", "openai.com"], "keywords": ["Deep learning", "Applications"], "authors": ["Luisa M Zintgraf", "Taco S Cohen", "Tameem Adel", "Max Welling"], "authorids": ["lmzintgraf@gmail.com", "t.s.cohen@uva.nl", "tameem.hesham@gmail.com", "m.welling@uva.nl"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287640500, "id": "ICLR.cc/2017/conference/-/paper285/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ5UeU9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper285/reviewers", "ICLR.cc/2017/conference/paper285/areachairs"], "cdate": 1485287640500}}}, {"tddate": null, "tmdate": 1482413062220, "tcdate": 1482413062220, "number": 2, "id": "HJCZE8tNx", "invitation": "ICLR.cc/2017/conference/-/paper285/official/comment", "forum": "BJ5UeU9xx", "replyto": "rkmFPn8Nl", "signatures": ["ICLR.cc/2017/conference/paper285/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper285/AnonReviewer2"], "content": {"title": "Very nice plot!", "comment": "Thank you, the new plot looks very nice and helpful! I have one additional suggestion: For your own visualizations, \"white\" means \"no relevance\", while for the comparision plots from Simonyan \"black\" has the meaning of \"no relevance\". I would recomment to invert the color scale for the Simonyan visualizations to make this more comparable. Additionally I would expect the Simonyan images to be more readable with the inverted color scale because it is easier to see small structure in black on white instead of white on black.\n\nI noticed that for some images the ImageNet class was not obvious to me -- mainly because I don't know all 1000 ImageNet classes. It might be worth adding the correct (or predicted) class name e.g. in vertical to the left of each row.\n\nKudos for releasing the code!\n\nIf you give changing the Simonyan color scale a try, I will happily increase my rating."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visualizing Deep Neural Network Decisions: Prediction Difference Analysis", "abstract": "This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).", "pdf": "/pdf/9c0dd32a98e96fc0222dd2bcd99cc21e599852e9.pdf", "TL;DR": "Method for visualizing evidence for and against deep convolutional neural network classification decisions in a given input image.", "paperhash": "zintgraf|visualizing_deep_neural_network_decisions_prediction_difference_analysis", "conflicts": ["uva.nl", "vub.ac.be", "cifar.ca", "uwaterloo.ca", "ru.nl", "openai.com"], "keywords": ["Deep learning", "Applications"], "authors": ["Luisa M Zintgraf", "Taco S Cohen", "Tameem Adel", "Max Welling"], "authorids": ["lmzintgraf@gmail.com", "t.s.cohen@uva.nl", "tameem.hesham@gmail.com", "m.welling@uva.nl"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287640362, "id": "ICLR.cc/2017/conference/-/paper285/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BJ5UeU9xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper285/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper285/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper285/reviewers", "ICLR.cc/2017/conference/paper285/areachairs"], "cdate": 1485287640362}}}, {"tddate": null, "tmdate": 1482242451105, "tcdate": 1482242451105, "number": 3, "id": "SJs9Y2UEl", "invitation": "ICLR.cc/2017/conference/-/paper285/public/comment", "forum": "BJ5UeU9xx", "replyto": "ByGYq9lNx", "signatures": ["~Luisa_M_Zintgraf1"], "readers": ["everyone"], "writers": ["~Luisa_M_Zintgraf1"], "content": {"title": "Why it is so slow", "comment": "Thank you very much for your review and input. We are going to address the comments/questions below.\n\n---------------------------\n\n1. Ideally, we would condition on the full image to sample individual pixels, i.e., p(x_i | x_\\i). Since it is computationally infeasible to model such a probability distribution, we resort to an approximation and in this case decided to make the simplification of assuming translation invariance (equation 4) and conditioning only on a small neighborhood around the pixel. However, as the reviewer points out, the pixel probability could also depend on the context of the larger image (an illustrative example in \"Objects in Contexts\" is that a yellow blob can be a lemon or a tennis ball, depending on the scene in the image). We still think that given a small enough patch (k in alg. 1) that is marginalized, the pixel values (which are somewhat but not entirely coupled to their semantic meaning) are mostly dependent on their neighborhood.\n\nHaving said that, we can try to think of ways to modify (4) to get an even better approximation by using a conditional distribution that takes more information about the whole image into account. This does not necessarily have to be all the raw pixels, but could be (as pointed at by the reviewer) more semantic information, like scene labels. In fact, for the MRI scans we also performed experiments where we split up the 3D image into a 20x20x20 grid and also provide the Gaussian distribution with the index in that grid, i.e., instead of (4) we use p(x_i | x^_\\i, grid_index), since the distribution of pixel values in the special case of MRI scans does depend on spacial location as well. We found that this slightly improves the interpretability of the results.\n\nIt would be an interesting next step to analyze in more detail how a modification of (4), or a stronger probabilistic model, influences the results.\n\n---------------------------\n\n2. What we observed in our experiments is that usually the explanations for the first and second highest classes after softmax are pretty much complementary (i.e., what speaks for one class does not speak for the other and vice-versa) and that the prediction difference for the lower classes has a much lower magnitude and is also less visually expressive. As the reviewer points out, the softmax tends to enlarge differences in its inputs, so we expect that for a given pixel, the intensities of the output across classes is close to a 1-hot-vector. We think this explains why when having many classes (1000 for the ImageNet dataset) for the majority of low-scoring output classes, the visualizations are not very meaningful. Still, for the top 2-3 scoring classes the outputs of the softmax are sensitive enough to small changes in input space, and Figure 7 nicely illustrates how the classifier uses the softmax to weigh the top scoring classes against each other, and ultimately decides for one of the classes (even though they are very similar, like different dog breeds).\n\n---------------------------\n\n4. Given our default settings, for each ImageNet image, we have to roughly make 227x227x10 (around 17,000) evaluations - i.e., for each patch (~227x227 patches) take samples (10) from the multivariate Gaussian and forward pass the image through the network (it's a bit less than this, depending on the actual input size of the network and the k in algorithm 1). Analyzing one image therefore depends strongly on how fast we can sample pixel values, and even more on how fast we can evaluate the classifier.\n\nWe agree with the reviewer that for some datasets, this makes this approach less practical than other methods. E.g., the sensitivity map from Simonyan et al. (2013) requires only a single backward pass through the network and is therefore very fast compared to our method. However we also believe that there are many cases where a longer waiting time is worth having a more expressive explanation (given that we can provide signed information w.r.t. the support for/against single classes compared to the sensitivity map). For example in a medical setting we believe it would be acceptable to even wait 1-2 days for a very insightful analysis and explanation of the individual patient's data (e.g., an MRI scan). For datasets like ImageNet, other methods are generally more practical for a quick analysis (or live analysis of videos like in Yosinski's deepvis toolbox). Still, compared to the training time these DCNNs usually take we think that it might be feasible to use our method to get additional insight into how the DCNN makes decisions (e.g., let it run for a week and have around 300 images analyzed)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visualizing Deep Neural Network Decisions: Prediction Difference Analysis", "abstract": "This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).", "pdf": "/pdf/9c0dd32a98e96fc0222dd2bcd99cc21e599852e9.pdf", "TL;DR": "Method for visualizing evidence for and against deep convolutional neural network classification decisions in a given input image.", "paperhash": "zintgraf|visualizing_deep_neural_network_decisions_prediction_difference_analysis", "conflicts": ["uva.nl", "vub.ac.be", "cifar.ca", "uwaterloo.ca", "ru.nl", "openai.com"], "keywords": ["Deep learning", "Applications"], "authors": ["Luisa M Zintgraf", "Taco S Cohen", "Tameem Adel", "Max Welling"], "authorids": ["lmzintgraf@gmail.com", "t.s.cohen@uva.nl", "tameem.hesham@gmail.com", "m.welling@uva.nl"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287640500, "id": "ICLR.cc/2017/conference/-/paper285/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ5UeU9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper285/reviewers", "ICLR.cc/2017/conference/paper285/areachairs"], "cdate": 1485287640500}}}, {"tddate": null, "tmdate": 1482241914735, "tcdate": 1482241914735, "number": 2, "id": "rkmFPn8Nl", "invitation": "ICLR.cc/2017/conference/-/paper285/public/comment", "forum": "BJ5UeU9xx", "replyto": "ByacUsbVg", "signatures": ["~Luisa_M_Zintgraf1"], "readers": ["everyone"], "writers": ["~Luisa_M_Zintgraf1"], "content": {"title": "Experiments on random picks", "comment": "Thank you very much for your review. We understand that for the reader it would make a convincing point to see results on random picks, and therefore ran additional experiments on 34 randomly selected ImageNet images. We also added the results of the method from Simonyan et al. (2013) for direct comparison. Please see https://www.dropbox.com/s/0eoe1krqg4m6gv8/results_random_legend.png for the results (we will add them to the appendix of the paper in a revised version).\n\nFurther, we made our code publicly available, see https://github.com/lmzintgraf/DeepVis-PredDiff ."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visualizing Deep Neural Network Decisions: Prediction Difference Analysis", "abstract": "This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).", "pdf": "/pdf/9c0dd32a98e96fc0222dd2bcd99cc21e599852e9.pdf", "TL;DR": "Method for visualizing evidence for and against deep convolutional neural network classification decisions in a given input image.", "paperhash": "zintgraf|visualizing_deep_neural_network_decisions_prediction_difference_analysis", "conflicts": ["uva.nl", "vub.ac.be", "cifar.ca", "uwaterloo.ca", "ru.nl", "openai.com"], "keywords": ["Deep learning", "Applications"], "authors": ["Luisa M Zintgraf", "Taco S Cohen", "Tameem Adel", "Max Welling"], "authorids": ["lmzintgraf@gmail.com", "t.s.cohen@uva.nl", "tameem.hesham@gmail.com", "m.welling@uva.nl"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287640500, "id": "ICLR.cc/2017/conference/-/paper285/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ5UeU9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper285/reviewers", "ICLR.cc/2017/conference/paper285/areachairs"], "cdate": 1485287640500}}}, {"tddate": null, "tmdate": 1481841274024, "tcdate": 1481841274019, "number": 1, "id": "ByGYq9lNx", "invitation": "ICLR.cc/2017/conference/-/paper285/official/review", "forum": "BJ5UeU9xx", "replyto": "BJ5UeU9xx", "signatures": ["ICLR.cc/2017/conference/paper285/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper285/AnonReviewer1"], "content": {"title": "Why is it so slow?", "rating": "6: Marginally above acceptance threshold", "review": "The authors of this work propose an interesting approach to visualizing the predictions made by a deep neural network. The manuscript is well written is provides good insight into the problem. I also appreciate the application to medical images, as simply illustrating the point on ImageNet isn't interesting enough. I do have some questions and comments.\n1.  As the authors correctly point out in 3.1, approximating the conditional probability of a feature x_i by the marginal distribution p(x_i) is not realistic. They advocate for translation invariance, i.e. the position of the pixel in the image shouldn't affect the probability, and suggest that the pixels appearance depends on the small neighborhood around it. However, it is well known that global context makes an big impact on the semantics of pixels. In \"Objects in Contexts\", authors show that a given neighborhood of pixels can take different semantic meanings based on the global context in the image. In the context of deep neural networks, works such as \"ParseNet\" also illustrate the importance of global context on the spatial label distribution. This does not necessarily invalidate this approach, but is a significant limitation. It would be great if the authors provided a modification to (4) and empirically verified the change.\n\n2. Figure 7 shows the distribution over top 3 predictions before and after softmax. It is expected that even fairly uniform distributions will transform toward delta functions after softmax normalization. Is there an additional insight here?\n\n4. Finally, in 4.1, the authors state that it takes 30 minutes to analyze a single image with GooLeNet on a GPU? Why is this so computationally expensive? Such complexity seems to make the algorithm impractical and analyzing datasets of statistical relevance seems prohibitive. ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visualizing Deep Neural Network Decisions: Prediction Difference Analysis", "abstract": "This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).", "pdf": "/pdf/9c0dd32a98e96fc0222dd2bcd99cc21e599852e9.pdf", "TL;DR": "Method for visualizing evidence for and against deep convolutional neural network classification decisions in a given input image.", "paperhash": "zintgraf|visualizing_deep_neural_network_decisions_prediction_difference_analysis", "conflicts": ["uva.nl", "vub.ac.be", "cifar.ca", "uwaterloo.ca", "ru.nl", "openai.com"], "keywords": ["Deep learning", "Applications"], "authors": ["Luisa M Zintgraf", "Taco S Cohen", "Tameem Adel", "Max Welling"], "authorids": ["lmzintgraf@gmail.com", "t.s.cohen@uva.nl", "tameem.hesham@gmail.com", "m.welling@uva.nl"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483200340725, "id": "ICLR.cc/2017/conference/-/paper285/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper285/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper285/AnonReviewer1", "ICLR.cc/2017/conference/paper285/AnonReviewer2", "ICLR.cc/2017/conference/paper285/AnonReviewer3"], "reply": {"forum": "BJ5UeU9xx", "replyto": "BJ5UeU9xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper285/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper285/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483200340725}}}, {"tddate": null, "tmdate": 1481726258410, "tcdate": 1481726258403, "number": 1, "id": "Hk94YA0Xg", "invitation": "ICLR.cc/2017/conference/-/paper285/official/comment", "forum": "BJ5UeU9xx", "replyto": "S1qb6D6zg", "signatures": ["ICLR.cc/2017/conference/paper285/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper285/AnonReviewer2"], "content": {"title": "Suggestion", "comment": "Thank you for your answer! Your samples look very impressive. Of course one might object that this is just due to the hand picking. As a suggestion, probably you could easily prove that your method works well in general by showing a figure with something like 8x4 random stimuli from ImageNet alongside the visualization of the correct output unit (and pointing out that the stimuli are random). I think this would make a very convincing point."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visualizing Deep Neural Network Decisions: Prediction Difference Analysis", "abstract": "This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).", "pdf": "/pdf/9c0dd32a98e96fc0222dd2bcd99cc21e599852e9.pdf", "TL;DR": "Method for visualizing evidence for and against deep convolutional neural network classification decisions in a given input image.", "paperhash": "zintgraf|visualizing_deep_neural_network_decisions_prediction_difference_analysis", "conflicts": ["uva.nl", "vub.ac.be", "cifar.ca", "uwaterloo.ca", "ru.nl", "openai.com"], "keywords": ["Deep learning", "Applications"], "authors": ["Luisa M Zintgraf", "Taco S Cohen", "Tameem Adel", "Max Welling"], "authorids": ["lmzintgraf@gmail.com", "t.s.cohen@uva.nl", "tameem.hesham@gmail.com", "m.welling@uva.nl"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287640362, "id": "ICLR.cc/2017/conference/-/paper285/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BJ5UeU9xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper285/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper285/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper285/reviewers", "ICLR.cc/2017/conference/paper285/areachairs"], "cdate": 1485287640362}}}, {"tddate": null, "tmdate": 1480584449970, "tcdate": 1480584449940, "number": 1, "id": "S1qb6D6zg", "invitation": "ICLR.cc/2017/conference/-/paper285/public/comment", "forum": "BJ5UeU9xx", "replyto": "Skp-9ssGg", "signatures": ["~Luisa_M_Zintgraf1"], "readers": ["everyone"], "writers": ["~Luisa_M_Zintgraf1"], "content": {"title": "How the example images are chosen", "comment": "We chose the images from among a small set of images in order to show a range of behavior of the algorithm. The shown images are quite representative of the performance of the method in general.\n\nE.g. for Figure 3, we picked the images that are both interesting and representative; there exist examples where the difference it not as obvious, but we never observed marginal sampling to give better results than conditional sampling."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visualizing Deep Neural Network Decisions: Prediction Difference Analysis", "abstract": "This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).", "pdf": "/pdf/9c0dd32a98e96fc0222dd2bcd99cc21e599852e9.pdf", "TL;DR": "Method for visualizing evidence for and against deep convolutional neural network classification decisions in a given input image.", "paperhash": "zintgraf|visualizing_deep_neural_network_decisions_prediction_difference_analysis", "conflicts": ["uva.nl", "vub.ac.be", "cifar.ca", "uwaterloo.ca", "ru.nl", "openai.com"], "keywords": ["Deep learning", "Applications"], "authors": ["Luisa M Zintgraf", "Taco S Cohen", "Tameem Adel", "Max Welling"], "authorids": ["lmzintgraf@gmail.com", "t.s.cohen@uva.nl", "tameem.hesham@gmail.com", "m.welling@uva.nl"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287640500, "id": "ICLR.cc/2017/conference/-/paper285/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ5UeU9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper285/reviewers", "ICLR.cc/2017/conference/paper285/areachairs"], "cdate": 1485287640500}}}, {"tddate": null, "tmdate": 1480468996790, "tcdate": 1480468996786, "number": 1, "id": "Skp-9ssGg", "invitation": "ICLR.cc/2017/conference/-/paper285/pre-review/question", "forum": "BJ5UeU9xx", "replyto": "BJ5UeU9xx", "signatures": ["ICLR.cc/2017/conference/paper285/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper285/AnonReviewer2"], "content": {"title": "How are the example images choosen?", "question": "Did you use some strategy to choose the example images you are presenting (e.g. for Figure 3, you could have picked the images where the difference between marginal sampling and conditional sampling was largest), did you choose randomly or did you handpick the examples?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Visualizing Deep Neural Network Decisions: Prediction Difference Analysis", "abstract": "This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).", "pdf": "/pdf/9c0dd32a98e96fc0222dd2bcd99cc21e599852e9.pdf", "TL;DR": "Method for visualizing evidence for and against deep convolutional neural network classification decisions in a given input image.", "paperhash": "zintgraf|visualizing_deep_neural_network_decisions_prediction_difference_analysis", "conflicts": ["uva.nl", "vub.ac.be", "cifar.ca", "uwaterloo.ca", "ru.nl", "openai.com"], "keywords": ["Deep learning", "Applications"], "authors": ["Luisa M Zintgraf", "Taco S Cohen", "Tameem Adel", "Max Welling"], "authorids": ["lmzintgraf@gmail.com", "t.s.cohen@uva.nl", "tameem.hesham@gmail.com", "m.welling@uva.nl"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959361393, "id": "ICLR.cc/2017/conference/-/paper285/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper285/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper285/AnonReviewer2"], "reply": {"forum": "BJ5UeU9xx", "replyto": "BJ5UeU9xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper285/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper285/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959361393}}}], "count": 17}