{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124280792, "tcdate": 1518443546343, "number": 117, "cdate": 1518443546343, "id": "HkG7hzyvf", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "HkG7hzyvf", "signatures": ["~noe_casas1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "A differentiable BLEU loss. Analysis and first results", "abstract": "In natural language generation tasks, like neural machine translation and image captioning, there is usually a mismatch between the optimized loss and the de facto evaluation criterion, namely token-level maximum likelihood and corpus-level BLEU score. This article tries to reduce this gap by defining differentiable computations of the BLEU and GLEU scores. We test this approach on simple tasks, obtaining valuable lessons on its potential applications but also its pitfalls, mainly that these loss functions push each token in the hypothesis sequence toward the average of the tokens in the reference, resulting in a poor training signal.", "paperhash": "casas|a_differentiable_bleu_loss_analysis_and_first_results", "keywords": ["differentiable", "BLEU", "GLEU", "NMT", "seq2seq"], "_bibtex": "@misc{\n  casas2018a,\n  title={A differentiable BLEU loss. Analysis and first results},\n  author={Noe Casas and Jos\u00e9 A.R. Fonollosa and Marta R. Costa-juss\u00e0},\n  year={2018},\n  url={https://openreview.net/forum?id=HkG7hzyvf}\n}", "authorids": ["contact@noecasas.com", "jose.fonollosa@upc.edu", "marta.ruiz@upc.edu"], "authors": ["Noe Casas", "Jos\u00e9 A.R. Fonollosa", "Marta R. Costa-juss\u00e0"], "TL;DR": "Differentiable BLEU loss --> Poor results on simple tasks --> Analysis --> BLEU loss pushes weights toward average of reference tokens, hence poor training signal", "pdf": "/pdf/0ec48e6537bbe71dc444dae20085af8e32835d89.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582961938, "tcdate": 1520127500760, "number": 1, "cdate": 1520127500760, "id": "HJBMAauOf", "invitation": "ICLR.cc/2018/Workshop/-/Paper117/Official_Review", "forum": "HkG7hzyvf", "replyto": "HkG7hzyvf", "signatures": ["ICLR.cc/2018/Workshop/Paper117/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper117/AnonReviewer2"], "content": {"title": "Official Review", "rating": "6: Marginally above acceptance threshold", "review": "The paper presents a differentiable BLEU/GLEU loss, similar to Zhukov & Kretov (2017). This is an interesting research direction as it would make it easier to optimize the final automatic evaluation metrics our models are often judged by.\n\nRelated work: please also compare to earlier work on differentiable approximations to BLEU, e.g., Rosti et al. (2011), see below.\n\nSection 3.4: Please describe the leakage problem more. Why does repeating seen gold tokens improve the score? Matching counts are still clipped.\n\nIn Appendix C, you say that you train with BPTT instead of teacher forcing. Teacher forcing inputs the reference token at every time step, which allows fast training since all time steps can be computed in parallel (for non-RNN architectures). Does this mean that you do not feed back the reference when you train with BPTT? In this case, your training regime seems similar to sampling a single sequence with REINFORCE. Your method would then also not be any more efficient than REINFORCE since sampling is by far the most expensive part of REINFORCE. \n\nReferences:\n\nExpected BLEU Training for Graphs: BBN System Description for WMT11 System Combination Task. Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas, and Richard Schwartz.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A differentiable BLEU loss. Analysis and first results", "abstract": "In natural language generation tasks, like neural machine translation and image captioning, there is usually a mismatch between the optimized loss and the de facto evaluation criterion, namely token-level maximum likelihood and corpus-level BLEU score. This article tries to reduce this gap by defining differentiable computations of the BLEU and GLEU scores. We test this approach on simple tasks, obtaining valuable lessons on its potential applications but also its pitfalls, mainly that these loss functions push each token in the hypothesis sequence toward the average of the tokens in the reference, resulting in a poor training signal.", "paperhash": "casas|a_differentiable_bleu_loss_analysis_and_first_results", "keywords": ["differentiable", "BLEU", "GLEU", "NMT", "seq2seq"], "_bibtex": "@misc{\n  casas2018a,\n  title={A differentiable BLEU loss. Analysis and first results},\n  author={Noe Casas and Jos\u00e9 A.R. Fonollosa and Marta R. Costa-juss\u00e0},\n  year={2018},\n  url={https://openreview.net/forum?id=HkG7hzyvf}\n}", "authorids": ["contact@noecasas.com", "jose.fonollosa@upc.edu", "marta.ruiz@upc.edu"], "authors": ["Noe Casas", "Jos\u00e9 A.R. Fonollosa", "Marta R. Costa-juss\u00e0"], "TL;DR": "Differentiable BLEU loss --> Poor results on simple tasks --> Analysis --> BLEU loss pushes weights toward average of reference tokens, hence poor training signal", "pdf": "/pdf/0ec48e6537bbe71dc444dae20085af8e32835d89.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582961755, "id": "ICLR.cc/2018/Workshop/-/Paper117/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper117/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper117/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper117/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper117/AnonReviewer3"], "reply": {"forum": "HkG7hzyvf", "replyto": "HkG7hzyvf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper117/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper117/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582961755}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582861704, "tcdate": 1520558957222, "number": 2, "cdate": 1520558957222, "id": "rkBumPktM", "invitation": "ICLR.cc/2018/Workshop/-/Paper117/Official_Review", "forum": "HkG7hzyvf", "replyto": "HkG7hzyvf", "signatures": ["ICLR.cc/2018/Workshop/Paper117/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper117/AnonReviewer1"], "content": {"title": "Interesting analysis, methods section difficult to follow, some questions", "rating": "7: Good paper, accept", "review": "This paper describes a differentiable version of BLEU, namely one that generalizes BLEU\u2019s calculations involving 1-hot output sequences to sequences of distributions over the output vocabulary. It provides a number of experiments that lead to the conclusion that this idea does not work out-of-the-box, for reasons including incompatibility with teacher forcing, and a draw toward the mean token in the reference. This is a very dense 3-page paper that leans very heavily on its 6 pages of appendices, which explain the method and experiments in greater (but not perfect) detail.\n\nPros:\n\nInteresting error analysis providing meaningful insight - the authors have demonstrated a desire to think deeply about this problem. Probably a good fit for a workshop paper.\n\nCons:\n\nUnclear methods section.\nSome questions linger regarding connection to previous work.\nSome questions linger regarding the error analysis and resulting conclusions.\n\nClarity:\n\nI didn\u2019t like the methods section in Appendix A, which is presented almost entirely as a large bulleted list with narrative descriptions of mathematical operations. Surely this would have been easier and more precise with equations?\n\nI am also unsure about some of the terminology in the paper. When confronted with a problem resulting from feeding gold-standard tokens into a position-independent n-gram matching objective, I think the authors opt to simply drop the previous input token from the NMT decoder state update. But I didn\u2019t find their description to be particularly precise. It would be nice to specify any equation for the new network.\n\nQuality & Significance:\n\nThis paper is hampered by its proximity to a NIPS 2017 workshop paper by Zhukov and Kretov on roughly the same topic. To their credit, the authors do a good job outlining some major differences between their differentiable BLEU formulation and their competitors\u2019 in Section 4, emphasizing a modification to account for repeated n-gram in the hypothesis (I assume a connection here to BLEU\u2019s clipping?), and accounting for brevity penalty. But I was still left with questions. The 2017 paper presents itself as a lower bound on expected BLEU to enable the REINFORCE algorithm without sampling, while this paper presents itself as a differentiable BLEU that provides an alternative to the REINFORCE algorithm, with no mention of bounds or expectations. Are these differences in presentation significant? I get the impression that they are, as this paper contrasts itself again the previous effort by saying, somewhat mysteriously, \u201ctheir input is handled as a probability distribution, and their resulting score is also handled as such,\u201d - How is this paper different? Surely by generalizing BLEU to the simplex, we are effectively treating the input as a probability distribution, or am I missing something? If I am missing a difference, what are the implications of the difference? \n\nFurthermore, the 2017 paper uses Jensen\u2019s inequality to create a bound on the BLEU\u2019s product over n-grams - this paper doesn\u2019t seem to do so, instead it appears to calculate BLEU directly with the soft matches. Has something been lost here? Or gained? It feels important. The paper should at least attempt to fill in these blanks for me.\n\nOn a similar note, I find myself very curious about the token copying experiment, which works well with sequences of length 10 and vocabulary sizes of 1,000, but poorly in all other scenarios. How is this related to the observation about the draw to the average token? Why does that particular configuration bypass this destructive pull toward the average, when all the other configurations fail? It is not intuitive to me at all why small vocabulary sizes would dodge this issue. Or is the copying experiment exhibiting another issue?\n\nDo the experiments in Appendix D about the draw to the average token generalize when we consider the full BLEU score, that takes the geometric mean of 1-gram, 2-gram, 3-gram and 4-gram? While I can easily see the draw to the average in the unigram case, those same average tokens must be penalized very heavily in the bigram case, where to have a non-trivial probability of any one bigram, you would need confident scores on the simplex in adjacent positions for each of its component tokens. To support this hypothesis, if the draw to the average was that strong, you wouldn\u2019t have seen the (technically correct) copying of the gold token at time t-1, when it was available.\n\nFinally, despite citing the Concrete distribution and Gumbel-softmax in the introduction, the authors drop the input token altogether when confronted with the problem of the system mimicking the gold token provided from the previous timestep, instead of replacing the token with a distribution over tokens at the previous timestep as predicted by the current model, or with a sample from that distribution. Why is that? Is there any chance that by including the model-previous token in some form, we would see things improve?\n\nOverall grade\n\nI think this paper is a good fit for a workshop paper, mostly because it is not really an archival venue. Statements like those found in Section 5 can kill an approach for years, so I\u2019m very nervous about letting such statements through while I still have so many questions about the approach and the analysis. But a workshop can help the authors hone their arguments much more rapidly, and hopefully lead us to better answers for a future archival version.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A differentiable BLEU loss. Analysis and first results", "abstract": "In natural language generation tasks, like neural machine translation and image captioning, there is usually a mismatch between the optimized loss and the de facto evaluation criterion, namely token-level maximum likelihood and corpus-level BLEU score. This article tries to reduce this gap by defining differentiable computations of the BLEU and GLEU scores. We test this approach on simple tasks, obtaining valuable lessons on its potential applications but also its pitfalls, mainly that these loss functions push each token in the hypothesis sequence toward the average of the tokens in the reference, resulting in a poor training signal.", "paperhash": "casas|a_differentiable_bleu_loss_analysis_and_first_results", "keywords": ["differentiable", "BLEU", "GLEU", "NMT", "seq2seq"], "_bibtex": "@misc{\n  casas2018a,\n  title={A differentiable BLEU loss. Analysis and first results},\n  author={Noe Casas and Jos\u00e9 A.R. Fonollosa and Marta R. Costa-juss\u00e0},\n  year={2018},\n  url={https://openreview.net/forum?id=HkG7hzyvf}\n}", "authorids": ["contact@noecasas.com", "jose.fonollosa@upc.edu", "marta.ruiz@upc.edu"], "authors": ["Noe Casas", "Jos\u00e9 A.R. Fonollosa", "Marta R. Costa-juss\u00e0"], "TL;DR": "Differentiable BLEU loss --> Poor results on simple tasks --> Analysis --> BLEU loss pushes weights toward average of reference tokens, hence poor training signal", "pdf": "/pdf/0ec48e6537bbe71dc444dae20085af8e32835d89.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582961755, "id": "ICLR.cc/2018/Workshop/-/Paper117/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper117/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper117/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper117/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper117/AnonReviewer3"], "reply": {"forum": "HkG7hzyvf", "replyto": "HkG7hzyvf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper117/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper117/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582961755}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582852194, "tcdate": 1520573962633, "number": 3, "cdate": 1520573962633, "id": "SkmMA9kFM", "invitation": "ICLR.cc/2018/Workshop/-/Paper117/Official_Review", "forum": "HkG7hzyvf", "replyto": "HkG7hzyvf", "signatures": ["ICLR.cc/2018/Workshop/Paper117/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper117/AnonReviewer3"], "content": {"title": "Negative results on differentiable BLEU loss, but very interesting discussion.", "rating": "6: Marginally above acceptance threshold", "review": "This work proposes an differentiable BLEU loss which is expected to be used with REINFORCE for potentially better qualities measured by BLEU. Experiments are conducted on various tasks, and empirically shows that the results are mostly negative.\n\nThe proposed method look sounds, though it is hard to follow the details of the algorithm Appendix A. The negative results sound discouraging, but the discussion is interesting enough. I feel this work might spur discussion on investigating other trainable metrics for sequence problems.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A differentiable BLEU loss. Analysis and first results", "abstract": "In natural language generation tasks, like neural machine translation and image captioning, there is usually a mismatch between the optimized loss and the de facto evaluation criterion, namely token-level maximum likelihood and corpus-level BLEU score. This article tries to reduce this gap by defining differentiable computations of the BLEU and GLEU scores. We test this approach on simple tasks, obtaining valuable lessons on its potential applications but also its pitfalls, mainly that these loss functions push each token in the hypothesis sequence toward the average of the tokens in the reference, resulting in a poor training signal.", "paperhash": "casas|a_differentiable_bleu_loss_analysis_and_first_results", "keywords": ["differentiable", "BLEU", "GLEU", "NMT", "seq2seq"], "_bibtex": "@misc{\n  casas2018a,\n  title={A differentiable BLEU loss. Analysis and first results},\n  author={Noe Casas and Jos\u00e9 A.R. Fonollosa and Marta R. Costa-juss\u00e0},\n  year={2018},\n  url={https://openreview.net/forum?id=HkG7hzyvf}\n}", "authorids": ["contact@noecasas.com", "jose.fonollosa@upc.edu", "marta.ruiz@upc.edu"], "authors": ["Noe Casas", "Jos\u00e9 A.R. Fonollosa", "Marta R. Costa-juss\u00e0"], "TL;DR": "Differentiable BLEU loss --> Poor results on simple tasks --> Analysis --> BLEU loss pushes weights toward average of reference tokens, hence poor training signal", "pdf": "/pdf/0ec48e6537bbe71dc444dae20085af8e32835d89.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582961755, "id": "ICLR.cc/2018/Workshop/-/Paper117/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper117/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper117/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper117/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper117/AnonReviewer3"], "reply": {"forum": "HkG7hzyvf", "replyto": "HkG7hzyvf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper117/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper117/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582961755}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573564226, "tcdate": 1521573564226, "number": 93, "cdate": 1521573563873, "id": "SJNpRRRFf", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "HkG7hzyvf", "replyto": "HkG7hzyvf", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A differentiable BLEU loss. Analysis and first results", "abstract": "In natural language generation tasks, like neural machine translation and image captioning, there is usually a mismatch between the optimized loss and the de facto evaluation criterion, namely token-level maximum likelihood and corpus-level BLEU score. This article tries to reduce this gap by defining differentiable computations of the BLEU and GLEU scores. We test this approach on simple tasks, obtaining valuable lessons on its potential applications but also its pitfalls, mainly that these loss functions push each token in the hypothesis sequence toward the average of the tokens in the reference, resulting in a poor training signal.", "paperhash": "casas|a_differentiable_bleu_loss_analysis_and_first_results", "keywords": ["differentiable", "BLEU", "GLEU", "NMT", "seq2seq"], "_bibtex": "@misc{\n  casas2018a,\n  title={A differentiable BLEU loss. Analysis and first results},\n  author={Noe Casas and Jos\u00e9 A.R. Fonollosa and Marta R. Costa-juss\u00e0},\n  year={2018},\n  url={https://openreview.net/forum?id=HkG7hzyvf}\n}", "authorids": ["contact@noecasas.com", "jose.fonollosa@upc.edu", "marta.ruiz@upc.edu"], "authors": ["Noe Casas", "Jos\u00e9 A.R. Fonollosa", "Marta R. Costa-juss\u00e0"], "TL;DR": "Differentiable BLEU loss --> Poor results on simple tasks --> Analysis --> BLEU loss pushes weights toward average of reference tokens, hence poor training signal", "pdf": "/pdf/0ec48e6537bbe71dc444dae20085af8e32835d89.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}