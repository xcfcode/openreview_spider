{"notes": [{"id": "SkgCV205tQ", "original": "Hkebfr0qKm", "number": 1498, "cdate": 1538087989991, "ddate": null, "tcdate": 1538087989991, "tmdate": 1545355406521, "tddate": null, "forum": "SkgCV205tQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Accelerating first order optimization algorithms", "abstract": "There exist several stochastic optimization algorithms. However in most cases, it is difficult to tell for a particular problem which will be the best optimizer to choose as each of them are good. Thus, we present a simple and intuitive technique, when applied to first order optimization algorithms, is able to improve the speed of convergence and reaches a better minimum for the loss function compared to the original algorithms. The proposed solution modifies the update rule, based on the variation of the direction of the gradient during training. We conducted several tests with Adam and AMSGrad on two different  datasets. The preliminary results show that the proposed technique improves the performance of existing optimization algorithms and works well in practice.", "keywords": ["Optimization", "Optimizer", "Adam", "Gradient Descent"], "authorids": ["nyamen_tato.ange_adrienne@courrier.uqam.ca", "nkambou.roger@uqam.ca"], "authors": ["Ange tato", "Roger nkambou"], "pdf": "/pdf/916f8b0b2e74846fbd21f36c7c9e0d6ebfcf339d.pdf", "paperhash": "tato|accelerating_first_order_optimization_algorithms", "_bibtex": "@misc{\ntato2019accelerating,\ntitle={Accelerating first order optimization algorithms},\nauthor={Ange tato and Roger nkambou},\nyear={2019},\nurl={https://openreview.net/forum?id=SkgCV205tQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rJxP9921eV", "original": null, "number": 1, "cdate": 1544698510721, "ddate": null, "tcdate": 1544698510721, "tmdate": 1545354507117, "tddate": null, "forum": "SkgCV205tQ", "replyto": "SkgCV205tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1498/Meta_Review", "content": {"metareview": "Dear authors,\n\nAll reviewers commented that the paper had issues with the presentations and the results, making it unsuitable for publication to ICLR. Please address these comments should you decide to resubmit this work.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Issues with the presentation"}, "signatures": ["ICLR.cc/2019/Conference/Paper1498/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1498/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating first order optimization algorithms", "abstract": "There exist several stochastic optimization algorithms. However in most cases, it is difficult to tell for a particular problem which will be the best optimizer to choose as each of them are good. Thus, we present a simple and intuitive technique, when applied to first order optimization algorithms, is able to improve the speed of convergence and reaches a better minimum for the loss function compared to the original algorithms. The proposed solution modifies the update rule, based on the variation of the direction of the gradient during training. We conducted several tests with Adam and AMSGrad on two different  datasets. The preliminary results show that the proposed technique improves the performance of existing optimization algorithms and works well in practice.", "keywords": ["Optimization", "Optimizer", "Adam", "Gradient Descent"], "authorids": ["nyamen_tato.ange_adrienne@courrier.uqam.ca", "nkambou.roger@uqam.ca"], "authors": ["Ange tato", "Roger nkambou"], "pdf": "/pdf/916f8b0b2e74846fbd21f36c7c9e0d6ebfcf339d.pdf", "paperhash": "tato|accelerating_first_order_optimization_algorithms", "_bibtex": "@misc{\ntato2019accelerating,\ntitle={Accelerating first order optimization algorithms},\nauthor={Ange tato and Roger nkambou},\nyear={2019},\nurl={https://openreview.net/forum?id=SkgCV205tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1498/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352816204, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkgCV205tQ", "replyto": "SkgCV205tQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1498/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1498/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1498/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352816204}}}, {"id": "B1lJIzckam", "original": null, "number": 3, "cdate": 1541542471139, "ddate": null, "tcdate": 1541542471139, "tmdate": 1541542471139, "tddate": null, "forum": "SkgCV205tQ", "replyto": "SkgCV205tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1498/Official_Review", "content": {"title": "Cannot understand the paper", "review": "The paper considers a simplistic extension of first order methods typically used for neural network training. Apart from the basic idea the paper's actual algorithm is hard to read because it is full of lacking definitions. I have tried to piece together whatever I could by reading the proof. The algorithm box is very unclear. For instance the * operator is undefined. \n\nTo the best of my understanding which the paper changes the update by first checking whether the gradient has the same direction as the previous gradient if yes it uses the component wise maximum of the new gradient and the previous gradient in the update and otherwise it uses the new gradient. Now whether this if condition is checked component wise or an angle between the two vectors is completely unclear. \n\nI will really suggest the authors to at least write their algorithm with clarity. Further while stating the theorem there are undefined parameter and even the objective Regret has not been defined anywhere. Further the theorem which I could not verify due to similar unclarity shows I believe the same convergence result as AMSGrad and hence there is no theoretical advantage for the proposed algorithm. In terms of practice further I do not see a significant advantage and it could result be a step size issue . The authors do not say that they do a search over the hyper parameters. \n\nOn a philosophical level it is unclear what the motivation behind this particular change to any algorithm is. It would be good to discuss what additional advantage is added on top of acceleration. Note that the method feels very much like acceleration. \n", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1498/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating first order optimization algorithms", "abstract": "There exist several stochastic optimization algorithms. However in most cases, it is difficult to tell for a particular problem which will be the best optimizer to choose as each of them are good. Thus, we present a simple and intuitive technique, when applied to first order optimization algorithms, is able to improve the speed of convergence and reaches a better minimum for the loss function compared to the original algorithms. The proposed solution modifies the update rule, based on the variation of the direction of the gradient during training. We conducted several tests with Adam and AMSGrad on two different  datasets. The preliminary results show that the proposed technique improves the performance of existing optimization algorithms and works well in practice.", "keywords": ["Optimization", "Optimizer", "Adam", "Gradient Descent"], "authorids": ["nyamen_tato.ange_adrienne@courrier.uqam.ca", "nkambou.roger@uqam.ca"], "authors": ["Ange tato", "Roger nkambou"], "pdf": "/pdf/916f8b0b2e74846fbd21f36c7c9e0d6ebfcf339d.pdf", "paperhash": "tato|accelerating_first_order_optimization_algorithms", "_bibtex": "@misc{\ntato2019accelerating,\ntitle={Accelerating first order optimization algorithms},\nauthor={Ange tato and Roger nkambou},\nyear={2019},\nurl={https://openreview.net/forum?id=SkgCV205tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1498/Official_Review", "cdate": 1542234217002, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkgCV205tQ", "replyto": "SkgCV205tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1498/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335959296, "tmdate": 1552335959296, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1498/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Bklc29M5nX", "original": null, "number": 2, "cdate": 1541184178003, "ddate": null, "tcdate": 1541184178003, "tmdate": 1541533086149, "tddate": null, "forum": "SkgCV205tQ", "replyto": "SkgCV205tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1498/Official_Review", "content": {"title": "Paper is confusing", "review": "The paper proposes an acceleration method that slightly changes the AMSGrad algorithm when successive stochastic gradients point in different directions.  I found the paper confusing to read because the critical points of Algorithm 1 are very unclear. For instance the \\phi function defined by Reddi et al. takes as argument all the past gradients g1...gt (see paper at the bottom of page 3) but is used inside Algorithm 1 with only the current gradient --\\phi_t(g_t)-- or an enigmatic \"max\" of two vectors --\\phi_t(max(g_t,pg_t))--  I have no idea what the actual calculation is supposed to be. The proof of the theorem (equation 6 in the appendix) suggests that this is a componentwise maximum and that the other gradients are still in.  But a componentwise maximum is a surprisingly assymetric construction. What if we reparametrize by changing the sign of one particular weight?  We get a different maximum?\n\nI finally looked into the empirical evaluation. I am not sure that the purported effect cannot be ascribed to other factors such as the choice of stepsize --they do not seem to have been looking for the best stepsize for each algorithm. The MNIST experiments are performed with a bizarre variant of CNN that seems to perform substantially worse than comparable system. They show the test loss but not the test accuracy though.\n\nIn conclusion I remain confused and unconvinced.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1498/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating first order optimization algorithms", "abstract": "There exist several stochastic optimization algorithms. However in most cases, it is difficult to tell for a particular problem which will be the best optimizer to choose as each of them are good. Thus, we present a simple and intuitive technique, when applied to first order optimization algorithms, is able to improve the speed of convergence and reaches a better minimum for the loss function compared to the original algorithms. The proposed solution modifies the update rule, based on the variation of the direction of the gradient during training. We conducted several tests with Adam and AMSGrad on two different  datasets. The preliminary results show that the proposed technique improves the performance of existing optimization algorithms and works well in practice.", "keywords": ["Optimization", "Optimizer", "Adam", "Gradient Descent"], "authorids": ["nyamen_tato.ange_adrienne@courrier.uqam.ca", "nkambou.roger@uqam.ca"], "authors": ["Ange tato", "Roger nkambou"], "pdf": "/pdf/916f8b0b2e74846fbd21f36c7c9e0d6ebfcf339d.pdf", "paperhash": "tato|accelerating_first_order_optimization_algorithms", "_bibtex": "@misc{\ntato2019accelerating,\ntitle={Accelerating first order optimization algorithms},\nauthor={Ange tato and Roger nkambou},\nyear={2019},\nurl={https://openreview.net/forum?id=SkgCV205tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1498/Official_Review", "cdate": 1542234217002, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkgCV205tQ", "replyto": "SkgCV205tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1498/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335959296, "tmdate": 1552335959296, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1498/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJg32aeL2X", "original": null, "number": 1, "cdate": 1540914612475, "ddate": null, "tcdate": 1540914612475, "tmdate": 1541533085941, "tddate": null, "forum": "SkgCV205tQ", "replyto": "SkgCV205tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1498/Official_Review", "content": {"title": "Theoretical contribution is limited.", "review": "Prons: \nThis paper provides a simple and economic technique to accelerate adaptive stochastic algorithms. The idea is novel and preliminary experiments are encouraging.\n\nCons: \n1.\tThe theoretical analysis for AAMSGrad is standard and inherits from AMSGrad directly. Meanwhile, the convergence rate of AAMSGrad merely holds for strongly convex online optimization, which does not match the presented experiments. Hence, the theoretical contribution is limited. \n2.\tThe current experiments are too weak to validate the efficacy of the proposed accelerated technique. We recommend the authors to conduct more experiments on various deep neural networks. ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1498/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating first order optimization algorithms", "abstract": "There exist several stochastic optimization algorithms. However in most cases, it is difficult to tell for a particular problem which will be the best optimizer to choose as each of them are good. Thus, we present a simple and intuitive technique, when applied to first order optimization algorithms, is able to improve the speed of convergence and reaches a better minimum for the loss function compared to the original algorithms. The proposed solution modifies the update rule, based on the variation of the direction of the gradient during training. We conducted several tests with Adam and AMSGrad on two different  datasets. The preliminary results show that the proposed technique improves the performance of existing optimization algorithms and works well in practice.", "keywords": ["Optimization", "Optimizer", "Adam", "Gradient Descent"], "authorids": ["nyamen_tato.ange_adrienne@courrier.uqam.ca", "nkambou.roger@uqam.ca"], "authors": ["Ange tato", "Roger nkambou"], "pdf": "/pdf/916f8b0b2e74846fbd21f36c7c9e0d6ebfcf339d.pdf", "paperhash": "tato|accelerating_first_order_optimization_algorithms", "_bibtex": "@misc{\ntato2019accelerating,\ntitle={Accelerating first order optimization algorithms},\nauthor={Ange tato and Roger nkambou},\nyear={2019},\nurl={https://openreview.net/forum?id=SkgCV205tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1498/Official_Review", "cdate": 1542234217002, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkgCV205tQ", "replyto": "SkgCV205tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1498/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335959296, "tmdate": 1552335959296, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1498/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}