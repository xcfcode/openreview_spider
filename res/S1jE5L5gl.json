{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488753849122, "tcdate": 1478285875064, "number": 311, "id": "S1jE5L5gl", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "S1jE5L5gl", "signatures": ["~Chris_J_Maddison1"], "readers": ["everyone"], "content": {"title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables", "abstract": "The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables -- continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.\n", "pdf": "http://www.stats.ox.ac.uk/~cmaddis/pubs/concrete.pdf", "TL;DR": "Relaxed reparameterization trick for discrete stochastic units.", "paperhash": "maddison|the_concrete_distribution_a_continuous_relaxation_of_discrete_random_variables", "keywords": ["Deep learning", "Unsupervised Learning", "Structured prediction"], "conflicts": ["google.com", "stats.ox.ac.uk", "cs.toronto.edu"], "authors": ["Chris J. Maddison", "Andriy Mnih", "Yee Whye Teh"], "authorids": ["cmaddis@stats.ox.ac.uk", "amnih@google.com", "y.w.teh@stats.ox.ac.uk"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396497795, "tcdate": 1486396497795, "number": 1, "id": "BJq8hzLOl", "invitation": "ICLR.cc/2017/conference/-/paper311/acceptance", "forum": "S1jE5L5gl", "replyto": "S1jE5L5gl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This paper proposes a neat general method for relaxing models with discrete softmax choices into closely-related models with continous random variables. The method is designed to work well with the reparameterization trick used in stochastic variational inference. This work is likely to have wide impact.\n \n Related submissions at ICLR:\n \"Categorical Reparameterization with Gumbel-Softmax\" by Jang et al. contains the same core idea. \"Discrete variational autoencoders\", by Rolfe, contains an alternative relaxation for autoencoders with discrete latents, which I personally find harder to follow.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables", "abstract": "The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables -- continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.\n", "pdf": "http://www.stats.ox.ac.uk/~cmaddis/pubs/concrete.pdf", "TL;DR": "Relaxed reparameterization trick for discrete stochastic units.", "paperhash": "maddison|the_concrete_distribution_a_continuous_relaxation_of_discrete_random_variables", "keywords": ["Deep learning", "Unsupervised Learning", "Structured prediction"], "conflicts": ["google.com", "stats.ox.ac.uk", "cs.toronto.edu"], "authors": ["Chris J. Maddison", "Andriy Mnih", "Yee Whye Teh"], "authorids": ["cmaddis@stats.ox.ac.uk", "amnih@google.com", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396498341, "id": "ICLR.cc/2017/conference/-/paper311/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "S1jE5L5gl", "replyto": "S1jE5L5gl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396498341}}}, {"tddate": null, "tmdate": 1484346610353, "tcdate": 1484346610353, "number": 5, "id": "Hy9lH0ILl", "invitation": "ICLR.cc/2017/conference/-/paper311/public/comment", "forum": "S1jE5L5gl", "replyto": "S1jE5L5gl", "signatures": ["~Chris_J_Maddison1"], "readers": ["everyone"], "writers": ["~Chris_J_Maddison1"], "content": {"title": "Author response", "comment": "We thank the reviewers for their time and helpful suggestions, and we address their comments in turn.\n\nAnonReviewer1.\n\nThe temperatures used to produce the Tables in the paper are all reported in the Appendix.\n\nRegarding your question on the uniqueness of the Gumbel, we certainly could have been clearer. The statement should have been, \"In the n-ary case, given a fixed alpha, we can see that the Gumbel-Max trick holds even when we restrict it to some subset of the indices. That is, if we consider the argmax over a restricted subset, it should be distributed as a discrete random variable with the Gibbs distribution normalized over just that subset. It turns out, the Gumbel is uniquely characterized by this property (Yellott, 1977). This is important, since it gives us a guaranteed recipe for implementing a sampler for any discrete distribution as the maximization of additively perturbed logits.\" In retrospect, the original statement we made in the draft does not add much to the discussion, and we plan to remove it.\n\nAnonReviewer2.\n\nWe addressed the question of VIMCO versus Concrete when comparing linear and non-linear models in a response to a question of AnonReviewer1 below. We don\u2019t have anything to add to that answer at the moment.\n\nRegarding the question of large m. Indeed, we expect VIMCO to perform better for large values of m, but as the training time grows roughly linearly with m, using more than 25 samples is quite rare in practice.\n\nThanks for detecting all of the typos and clerical issues as well as for flagging difficult paragraphs. We will fix the typos and try to clarify the writing for the final draft.\n\nAnonReviewer3.\n\nA bias / variance analysis of this method is a great future question, though we think that a generic analysis is unlikely to be possible.  We note in passing that it isn't always the case that generic score function estimators with baselines have higher variance than their reparameterization counterparts, see (Gal, 2016). Even an analysis for a specific loss function proves challenging, since it would require deriving expectations with respect to the concrete distribution. The \"normalization\" term in the density of the concrete distribution makes computing even first order moments extremely challenging.\n\nReferences.\n\nJohn Yellott. The Relationship Between Luce's Choice Axiom, Thurstone's Theory of Comparative Judgment, and the Double Exponential Distribution. 1977.\n\nYarin Gal. Uncertainty in Deep Learning. 2016. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables", "abstract": "The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables -- continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.\n", "pdf": "http://www.stats.ox.ac.uk/~cmaddis/pubs/concrete.pdf", "TL;DR": "Relaxed reparameterization trick for discrete stochastic units.", "paperhash": "maddison|the_concrete_distribution_a_continuous_relaxation_of_discrete_random_variables", "keywords": ["Deep learning", "Unsupervised Learning", "Structured prediction"], "conflicts": ["google.com", "stats.ox.ac.uk", "cs.toronto.edu"], "authors": ["Chris J. Maddison", "Andriy Mnih", "Yee Whye Teh"], "authorids": ["cmaddis@stats.ox.ac.uk", "amnih@google.com", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287626824, "id": "ICLR.cc/2017/conference/-/paper311/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1jE5L5gl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper311/reviewers", "ICLR.cc/2017/conference/paper311/areachairs"], "cdate": 1485287626824}}}, {"tddate": null, "tmdate": 1482953329303, "tcdate": 1482953329303, "number": 3, "id": "ryKOMc-He", "invitation": "ICLR.cc/2017/conference/-/paper311/official/review", "forum": "S1jE5L5gl", "replyto": "S1jE5L5gl", "signatures": ["ICLR.cc/2017/conference/paper311/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper311/AnonReviewer2"], "content": {"title": "Nice paper, should be accepted", "rating": "7: Good paper, accept", "review": "The authors of the paper present a novel distribution for discrete variables called the \"concrete distribution\". The distribution can be seen as a continuous relaxation for a distribution over discrete random variables. The main motivation for introduction of the concrete distribution is the possibility to compute the gradient of discrete stochastic nodes in Stochastic Computational Graphs.\n\nI think the paper is well written and sound, definitely of interest for the conference program.\n\nAs to the experimental part, the authors have results which support some kind of consistent superior performance for VIMCO for linear models and for concrete relaxations for non-linear models. Any explanation for that? Is this confirmed over different models and maybe datasets?\nSimilarly, it looks like VIMCO outperforms (in Figure 4) Concrete for large m, on the test NLL. I would encourage to try with other values of m to see if this dependence on large m is confirmed or not.\n\nI believe the paper should be accepted to the conference, however please consider that I'm not an expert in this field.\n\nSome minor observations/comments/issues:\n-Section 2.1: there is a repetition \"be be\" in the first paragraph.\n-Section 2.4: I would add a reference for the \"multi-sample variational objective\"\n-Section 3.1, just before Section 3.2: \"the Gumbel is a crucial 1\". Why 1 and not \"one\"?\n-Section 3.3, last paragraph: \"Thus, in addition to relaxing the sampling pass of a SCG the log...\" I would add a comma after \"SCG\". More in general, the second part of the paragraph is very dense and not easy to \"absorb\". I don't think it's an issue with the presentation: the concepts themselves are just dense. However, maybe the authors could find a way to make the paragraph easier to assimilate for a less experienced reader.\n-Section 5.1, second paragraph: \"All our models are neural networks with layers of n-ary discrete stochastic nodes with log_2(n)-dimensional states on the corners of the hypercube {-1,1}^log_2(n). The distribution of the nodes are parametrized by n real values log alpha_k\". It is not clear to me, where does the log_2(n) come from. Similarly for the {-1,1}.\n-Section 5.2: After \"this distribution.\" and \"We will\" there is an extra space.\n-If a compare the last formula in Section 5.3 with Eq. 8, I don't see exactly why the former is a special case of the latter. Is it because q(Z^i | x) is always one?", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables", "abstract": "The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables -- continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.\n", "pdf": "http://www.stats.ox.ac.uk/~cmaddis/pubs/concrete.pdf", "TL;DR": "Relaxed reparameterization trick for discrete stochastic units.", "paperhash": "maddison|the_concrete_distribution_a_continuous_relaxation_of_discrete_random_variables", "keywords": ["Deep learning", "Unsupervised Learning", "Structured prediction"], "conflicts": ["google.com", "stats.ox.ac.uk", "cs.toronto.edu"], "authors": ["Chris J. Maddison", "Andriy Mnih", "Yee Whye Teh"], "authorids": ["cmaddis@stats.ox.ac.uk", "amnih@google.com", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482953330101, "id": "ICLR.cc/2017/conference/-/paper311/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper311/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper311/AnonReviewer3", "ICLR.cc/2017/conference/paper311/AnonReviewer1", "ICLR.cc/2017/conference/paper311/AnonReviewer2"], "reply": {"forum": "S1jE5L5gl", "replyto": "S1jE5L5gl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper311/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper311/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482953330101}}}, {"tddate": null, "tmdate": 1481892602298, "tcdate": 1481892602298, "number": 2, "id": "rJM-7wZEl", "invitation": "ICLR.cc/2017/conference/-/paper311/official/review", "forum": "S1jE5L5gl", "replyto": "S1jE5L5gl", "signatures": ["ICLR.cc/2017/conference/paper311/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper311/AnonReviewer1"], "content": {"title": "A clearly relevant paper that should be accepted ", "rating": "9: Top 15% of accepted papers, strong accept", "review": "The authors describe the concrete distribution, a continuous approximation to\ndiscrete distributions parameterized by a vector of continuous positive numbers\nproportional to the probability of each discrete result. The concrete\ndistribution is obtained by using the softmax function to approximate the\nargmax operator. The paper is clearly written, original and significant.\nThe experiments clearly illustrate the advantages of the proposed method.\n\nSome minor questions:\n\n\"for the general n-ary case the Gumbel is a crucial 1 and the Gumbel-Max trick cannot be generalized\nfor other additive noise distributions\"\n\nWhat do you mean by this? Can you be more specific?\n\nWhat is the temperature values used to obtain Table 1 and the table in Figure 4.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables", "abstract": "The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables -- continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.\n", "pdf": "http://www.stats.ox.ac.uk/~cmaddis/pubs/concrete.pdf", "TL;DR": "Relaxed reparameterization trick for discrete stochastic units.", "paperhash": "maddison|the_concrete_distribution_a_continuous_relaxation_of_discrete_random_variables", "keywords": ["Deep learning", "Unsupervised Learning", "Structured prediction"], "conflicts": ["google.com", "stats.ox.ac.uk", "cs.toronto.edu"], "authors": ["Chris J. Maddison", "Andriy Mnih", "Yee Whye Teh"], "authorids": ["cmaddis@stats.ox.ac.uk", "amnih@google.com", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482953330101, "id": "ICLR.cc/2017/conference/-/paper311/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper311/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper311/AnonReviewer3", "ICLR.cc/2017/conference/paper311/AnonReviewer1", "ICLR.cc/2017/conference/paper311/AnonReviewer2"], "reply": {"forum": "S1jE5L5gl", "replyto": "S1jE5L5gl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper311/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper311/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482953330101}}}, {"tddate": null, "tmdate": 1481729490253, "tcdate": 1481729490246, "number": 1, "id": "BycCSky4x", "invitation": "ICLR.cc/2017/conference/-/paper311/official/review", "forum": "S1jE5L5gl", "replyto": "S1jE5L5gl", "signatures": ["ICLR.cc/2017/conference/paper311/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper311/AnonReviewer3"], "content": {"title": "Very important development for implementing stochastic networks with discrete random variables", "rating": "8: Top 50% of accepted papers, clear accept", "review": "Thank you for an interesting read.\n\nI think this paper has proposed a very useful method, which significantly simplifies the implementation of gradients for discrete random variables. Using this trick quite a lot of discrete variable-based methods will be significantly easier to implement, e.g. a GAN-style generator for text (see the recent arxiv preprint arXiv:1611.04051).\n\nI've got one suggestion to make the paper even better, but maybe the authors want to leave it to future work. I think compared to lots of variance reduction techniques such as NVIL and VIMCO, this relaxation trick has smaller variance (from empirical observation of the reparameterisation trick), but in the price of introducing biases. It would be fantastic if the authors can discuss the bias-variance trade-off, either in theoretical or experimental way. My bet will be that here the variance dominates the stochastic estimation error of the gradient estimation, but it would be great if the authors can confirm this.\n\n**to area chair: concurrent paper by Jang et al. 2016**\nIt seems there's a concurrent submission by Jang et al. I havent' read that paper in detail, but maybe the conference should accept or reject both?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables", "abstract": "The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables -- continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.\n", "pdf": "http://www.stats.ox.ac.uk/~cmaddis/pubs/concrete.pdf", "TL;DR": "Relaxed reparameterization trick for discrete stochastic units.", "paperhash": "maddison|the_concrete_distribution_a_continuous_relaxation_of_discrete_random_variables", "keywords": ["Deep learning", "Unsupervised Learning", "Structured prediction"], "conflicts": ["google.com", "stats.ox.ac.uk", "cs.toronto.edu"], "authors": ["Chris J. Maddison", "Andriy Mnih", "Yee Whye Teh"], "authorids": ["cmaddis@stats.ox.ac.uk", "amnih@google.com", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482953330101, "id": "ICLR.cc/2017/conference/-/paper311/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper311/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper311/AnonReviewer3", "ICLR.cc/2017/conference/paper311/AnonReviewer1", "ICLR.cc/2017/conference/paper311/AnonReviewer2"], "reply": {"forum": "S1jE5L5gl", "replyto": "S1jE5L5gl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper311/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper311/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482953330101}}}, {"tddate": null, "tmdate": 1481296884967, "tcdate": 1481296884961, "number": 4, "id": "rk6ghr_Xe", "invitation": "ICLR.cc/2017/conference/-/paper311/public/comment", "forum": "S1jE5L5gl", "replyto": "SJlaHXRfg", "signatures": ["~Chris_J_Maddison1"], "readers": ["everyone"], "writers": ["~Chris_J_Maddison1"], "content": {"title": "Reasoning for the results of VIMCO vs. Concrete", "comment": "Thanks for your question. We haven\u2019t come up with a completely satisfying answer, but we do have a hypothesis.\n\nWe suspect that the use of gradient information by the concrete relaxation explains the difference in performance. Comparing the linear/non-linear single stochastic layer experiments (models 200H-784V and 200H~784V), we can see that changing the activity of a single latent variable has an independent linear effect on the logit parameters of the observation distribution. In the non-linear case, changing the activity of a single latent variable has a non-linear non-independent effect on the logit parameters of the observation distribution. In this case gradient information will carry richer counterfactual information. VIMCO does not exploit this gradient information, while concrete relaxations do. Concrete relaxations result in biased gradients of the discrete graph, whereas VIMCO is unbiased. We suspect that the balance between using gradients and being unbiased favours VIMCO in the linear case and concrete in the non-linear case."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables", "abstract": "The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables -- continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.\n", "pdf": "http://www.stats.ox.ac.uk/~cmaddis/pubs/concrete.pdf", "TL;DR": "Relaxed reparameterization trick for discrete stochastic units.", "paperhash": "maddison|the_concrete_distribution_a_continuous_relaxation_of_discrete_random_variables", "keywords": ["Deep learning", "Unsupervised Learning", "Structured prediction"], "conflicts": ["google.com", "stats.ox.ac.uk", "cs.toronto.edu"], "authors": ["Chris J. Maddison", "Andriy Mnih", "Yee Whye Teh"], "authorids": ["cmaddis@stats.ox.ac.uk", "amnih@google.com", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287626824, "id": "ICLR.cc/2017/conference/-/paper311/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1jE5L5gl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper311/reviewers", "ICLR.cc/2017/conference/paper311/areachairs"], "cdate": 1485287626824}}}, {"tddate": null, "tmdate": 1481296844906, "tcdate": 1481296844900, "number": 3, "id": "SkSRoSOmx", "invitation": "ICLR.cc/2017/conference/-/paper311/public/comment", "forum": "S1jE5L5gl", "replyto": "HygUM_oMl", "signatures": ["~Chris_J_Maddison1"], "readers": ["everyone"], "writers": ["~Chris_J_Maddison1"], "content": {"title": "Comparison to inverse CDF", "comment": "Thanks for your question. Our method can be applied to essentially any model with discrete latent variables, since it simply involves relaxing them into continuous variables during training and then discretizing them at test time.\n\nAn inverse CDF of a discrete distribution is a piecewise constant function, which means that using it for a reparameterization does not produce usable gradients. In the Discrete VAE paper this issue is avoided by smoothing out each discrete variable by using it to select between two continuous latent variables and working with the inverse CDF of the resulting mixture, with the discrete variable marginalized out. In that approach the final models contain mixtures of discrete and continuous latent variables, while the test time models of our approach contain only discrete latent variables."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables", "abstract": "The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables -- continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.\n", "pdf": "http://www.stats.ox.ac.uk/~cmaddis/pubs/concrete.pdf", "TL;DR": "Relaxed reparameterization trick for discrete stochastic units.", "paperhash": "maddison|the_concrete_distribution_a_continuous_relaxation_of_discrete_random_variables", "keywords": ["Deep learning", "Unsupervised Learning", "Structured prediction"], "conflicts": ["google.com", "stats.ox.ac.uk", "cs.toronto.edu"], "authors": ["Chris J. Maddison", "Andriy Mnih", "Yee Whye Teh"], "authorids": ["cmaddis@stats.ox.ac.uk", "amnih@google.com", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287626824, "id": "ICLR.cc/2017/conference/-/paper311/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1jE5L5gl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper311/reviewers", "ICLR.cc/2017/conference/paper311/areachairs"], "cdate": 1485287626824}}}, {"tddate": null, "tmdate": 1480631736049, "tcdate": 1480631736045, "number": 2, "id": "SJlaHXRfg", "invitation": "ICLR.cc/2017/conference/-/paper311/pre-review/question", "forum": "S1jE5L5gl", "replyto": "S1jE5L5gl", "signatures": ["ICLR.cc/2017/conference/paper311/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper311/AnonReviewer1"], "content": {"title": "Reasoning for the results of VIMCO vs. Concrete", "question": "Is there any explanation about why VIMCO performs better in linear models in Table 1? Similarly, why is VIMCO failing in the Table shown in Figure 4?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables", "abstract": "The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables -- continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.\n", "pdf": "http://www.stats.ox.ac.uk/~cmaddis/pubs/concrete.pdf", "TL;DR": "Relaxed reparameterization trick for discrete stochastic units.", "paperhash": "maddison|the_concrete_distribution_a_continuous_relaxation_of_discrete_random_variables", "keywords": ["Deep learning", "Unsupervised Learning", "Structured prediction"], "conflicts": ["google.com", "stats.ox.ac.uk", "cs.toronto.edu"], "authors": ["Chris J. Maddison", "Andriy Mnih", "Yee Whye Teh"], "authorids": ["cmaddis@stats.ox.ac.uk", "amnih@google.com", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959346347, "id": "ICLR.cc/2017/conference/-/paper311/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper311/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper311/AnonReviewer3", "ICLR.cc/2017/conference/paper311/AnonReviewer1"], "reply": {"forum": "S1jE5L5gl", "replyto": "S1jE5L5gl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper311/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper311/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959346347}}}, {"tddate": null, "tmdate": 1480454727792, "tcdate": 1480454727789, "number": 1, "id": "HygUM_oMl", "invitation": "ICLR.cc/2017/conference/-/paper311/pre-review/question", "forum": "S1jE5L5gl", "replyto": "S1jE5L5gl", "signatures": ["ICLR.cc/2017/conference/paper311/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper311/AnonReviewer3"], "content": {"title": "Comparison to inverse CDF", "question": "Thank you for an interesting read. \n\nHow does your method compare with the reparameterisation through inverse CDF? For example, see https://arxiv.org/abs/1609.02200"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables", "abstract": "The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables -- continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.\n", "pdf": "http://www.stats.ox.ac.uk/~cmaddis/pubs/concrete.pdf", "TL;DR": "Relaxed reparameterization trick for discrete stochastic units.", "paperhash": "maddison|the_concrete_distribution_a_continuous_relaxation_of_discrete_random_variables", "keywords": ["Deep learning", "Unsupervised Learning", "Structured prediction"], "conflicts": ["google.com", "stats.ox.ac.uk", "cs.toronto.edu"], "authors": ["Chris J. Maddison", "Andriy Mnih", "Yee Whye Teh"], "authorids": ["cmaddis@stats.ox.ac.uk", "amnih@google.com", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959346347, "id": "ICLR.cc/2017/conference/-/paper311/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper311/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper311/AnonReviewer3", "ICLR.cc/2017/conference/paper311/AnonReviewer1"], "reply": {"forum": "S1jE5L5gl", "replyto": "S1jE5L5gl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper311/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper311/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959346347}}}, {"tddate": null, "tmdate": 1478404590253, "tcdate": 1478404590246, "number": 2, "id": "B1Lg5Xngl", "invitation": "ICLR.cc/2017/conference/-/paper311/public/comment", "forum": "S1jE5L5gl", "replyto": "S1TA00ilg", "signatures": ["~Chris_J_Maddison1"], "readers": ["everyone"], "writers": ["~Chris_J_Maddison1"], "content": {"title": "straight-through", "comment": "Hi David, thanks for the comment. Can you double check the current draft, specifically the third paragraph of the Related Work section? This was updated for the ICLR submission, but our arxiv revision is outdated until Monday."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables", "abstract": "The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables -- continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.\n", "pdf": "http://www.stats.ox.ac.uk/~cmaddis/pubs/concrete.pdf", "TL;DR": "Relaxed reparameterization trick for discrete stochastic units.", "paperhash": "maddison|the_concrete_distribution_a_continuous_relaxation_of_discrete_random_variables", "keywords": ["Deep learning", "Unsupervised Learning", "Structured prediction"], "conflicts": ["google.com", "stats.ox.ac.uk", "cs.toronto.edu"], "authors": ["Chris J. Maddison", "Andriy Mnih", "Yee Whye Teh"], "authorids": ["cmaddis@stats.ox.ac.uk", "amnih@google.com", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287626824, "id": "ICLR.cc/2017/conference/-/paper311/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1jE5L5gl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper311/reviewers", "ICLR.cc/2017/conference/paper311/areachairs"], "cdate": 1485287626824}}}, {"tddate": null, "tmdate": 1478385365312, "tcdate": 1478385365306, "number": 1, "id": "S1TA00ilg", "invitation": "ICLR.cc/2017/conference/-/paper311/public/comment", "forum": "S1jE5L5gl", "replyto": "S1jE5L5gl", "signatures": ["~David_Krueger1"], "readers": ["everyone"], "writers": ["~David_Krueger1"], "content": {"title": "Straight-through estimator?", "comment": "How this relates to the straight-through estimator (see Bengio et al: https://arxiv.org/abs/1308.3432) seems worth a mention."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables", "abstract": "The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables -- continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.\n", "pdf": "http://www.stats.ox.ac.uk/~cmaddis/pubs/concrete.pdf", "TL;DR": "Relaxed reparameterization trick for discrete stochastic units.", "paperhash": "maddison|the_concrete_distribution_a_continuous_relaxation_of_discrete_random_variables", "keywords": ["Deep learning", "Unsupervised Learning", "Structured prediction"], "conflicts": ["google.com", "stats.ox.ac.uk", "cs.toronto.edu"], "authors": ["Chris J. Maddison", "Andriy Mnih", "Yee Whye Teh"], "authorids": ["cmaddis@stats.ox.ac.uk", "amnih@google.com", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287626824, "id": "ICLR.cc/2017/conference/-/paper311/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1jE5L5gl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper311/reviewers", "ICLR.cc/2017/conference/paper311/areachairs"], "cdate": 1485287626824}}}], "count": 12}