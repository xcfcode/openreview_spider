{"notes": [{"tddate": null, "tmdate": 1486579356550, "tcdate": 1486579276882, "number": 13, "id": "ryBILJKdl", "invitation": "ICLR.cc/2017/conference/-/paper229/public/comment", "forum": "BJ8fyHceg", "replyto": "S1_8KEheg", "signatures": ["~Natasha_Jaques1"], "readers": ["everyone"], "writers": ["~Natasha_Jaques1"], "content": {"title": "Further clarification of differences between dialog generation work and ours", "comment": "Just wanted to clarify that we have updated the paper to reflect the differences between this paper and our own. At first glance, this paper might appear similar to our method, because it involves synthetic reward functions designed to improve dialog generation. As you mention, the authors use a pre-trained MLE model to reward dialog turns that minimize the probability that the pre-trained model places on 8 manually chosen ``dull\" responses, such as ``I don't know\". \n\nIn contrast, our approach directly penalizes divergence from the probability distribution learned by the MLE model for every response, allowing the model to retain information about the full space of sequences originally learned from data. Further, in Li et al. (2016) only heuristic rewards are used for further training after pre-training with MLE, which can erase knowledge about the data originally encoded in the model when it is re-trained with RL. This is effectively the same as the other ML to RL papers (Ranzato et al. 2015, Bahdanau et al. 2016). In contrast, our approach does not treat the synthetic reward functions as the ultimate indicator of sequence quality, but instead allows the model to retain information learned from data, and explicitly controls the trade-off between the influence of data and synthetic reward."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tuning Recurrent Neural Networks with Reinforcement Learning", "abstract": "The approach of training sequence models using supervised learning and next-step prediction  suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. We propose a novel sequence-learning approach in which we use a pre-trained Recurrent Neural Network (RNN) to supply part of the reward value in a Reinforcement Learning (RL) model. Thus, we can refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from KL control. We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note RNN is then refined using our method and rules of music theory. We show that by combining maximum likelihood (ML) and RL in this way, we can not only produce more pleasing melodies, but  significantly reduce unwanted behaviors and failure modes of the RNN, while maintaining information learned from data.", "pdf": "/pdf/c4f118ae6bf24cd110be68a6280bac447bdfd1df.pdf", "TL;DR": "RL Tuner is a method for refining an LSTM trained on data by using RL to impose desired behaviors, while maintaining good predictive properties learned from data.", "paperhash": "jaques|tuning_recurrent_neural_networks_with_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning", "Structured prediction", "Supervised Learning", "Applications"], "conflicts": ["media.mit.edu", "google.com", "cam.ac.uk"], "authors": ["Natasha Jaques", "Shixiang Gu", "Richard E. Turner", "Douglas Eck"], "authorids": ["jaquesn@mit.edu", "sg717@cam.ac.uk", "ret26@cam.ack.uk", "deck@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674646, "id": "ICLR.cc/2017/conference/-/paper229/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ8fyHceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper229/reviewers", "ICLR.cc/2017/conference/paper229/areachairs"], "cdate": 1485287674646}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396447353, "tcdate": 1486396447353, "number": 1, "id": "ByvQhz8Oe", "invitation": "ICLR.cc/2017/conference/-/paper229/acceptance", "forum": "BJ8fyHceg", "replyto": "BJ8fyHceg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The reviewers generally liked the application; there were a number of technical points raised that leave doubt about the novelty of the approach. However, this may be an interesting avenue in the future, thus the PCs are accepting it to the workshop track.", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tuning Recurrent Neural Networks with Reinforcement Learning", "abstract": "The approach of training sequence models using supervised learning and next-step prediction  suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. We propose a novel sequence-learning approach in which we use a pre-trained Recurrent Neural Network (RNN) to supply part of the reward value in a Reinforcement Learning (RL) model. Thus, we can refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from KL control. We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note RNN is then refined using our method and rules of music theory. We show that by combining maximum likelihood (ML) and RL in this way, we can not only produce more pleasing melodies, but  significantly reduce unwanted behaviors and failure modes of the RNN, while maintaining information learned from data.", "pdf": "/pdf/c4f118ae6bf24cd110be68a6280bac447bdfd1df.pdf", "TL;DR": "RL Tuner is a method for refining an LSTM trained on data by using RL to impose desired behaviors, while maintaining good predictive properties learned from data.", "paperhash": "jaques|tuning_recurrent_neural_networks_with_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning", "Structured prediction", "Supervised Learning", "Applications"], "conflicts": ["media.mit.edu", "google.com", "cam.ac.uk"], "authors": ["Natasha Jaques", "Shixiang Gu", "Richard E. Turner", "Douglas Eck"], "authorids": ["jaquesn@mit.edu", "sg717@cam.ac.uk", "ret26@cam.ack.uk", "deck@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396447917, "id": "ICLR.cc/2017/conference/-/paper229/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BJ8fyHceg", "replyto": "BJ8fyHceg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396447917}}}, {"tddate": null, "tmdate": 1485018827354, "tcdate": 1481932273315, "number": 2, "id": "SktlRezEl", "invitation": "ICLR.cc/2017/conference/-/paper229/official/review", "forum": "BJ8fyHceg", "replyto": "BJ8fyHceg", "signatures": ["ICLR.cc/2017/conference/paper229/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper229/AnonReviewer4"], "content": {"title": "Log-likelihood models augmented by non-differentiable objectives for MIDI synthesis", "rating": "5: Marginally below acceptance threshold", "review": "The authors propose a solution for the task of synthesizing melodies. The authors claim that the \"language-model\"-type approaches with LSTMs generate melodies with certain shortcomings. They tend to lack long-range structure, to repeat notes etc. To solve this problem the authors suggest that the model could be first trained as a pure LM-style LSTM and then trained with reinforcement learning to optimize an objective which includes some non-differentiable music-theory related constraints. \n\nThe reinforcement learning methodology is appropriate but straightforward and closely resembles previous work for text modeling and dialogue generation. By itself the methodology doesn't offer a new technique. \n\nTo me, the paper's contribution then comes down to the novelty / utility / impact of the application. The authors clearly put substantial of effort into crafting the rules and user study and that is commendable. On the other hand, music itself is dealt with somewhat naively. While the user study reflects hard work, it seems premature. The semi-plausible piano melodies here are only music in the way that LSTM Shakespeare passes as poetry. So it's analogous to conducting a user study comparing LSTM Shakespeare to n-gram Shakespeare. \n\nI'd caution the author's against the uncritical motivation that a problem has previously been studied. Research contains abundant dead ends (not to say this is necessarily one) and the burden to motivate research shouldn't be forgotten. This is especially true when the application is the primary thrust of a paper.\n\nGenerally the authors should be careful about describing this model as \"composing\". By analogy to a Shakespeare-LSTM, the language model is not really composing English prose. The relationship between constructing a statistical sequence model and creating art - an activity that involves communication grounded in real-world semantics should not be overstated. \n\nI appreciate the authors' efforts to respond to some criticisms of the problem setup and encourage them to anticipate these arguments in the paper and to better motivate the work in the future. If the main contribution is the application (the methods have been used elsewhere), then the motivation is of central importance. I also appreciate their contention that the field benefits from multiple datasets and not simply relying on language modeling. Further, they are correct in asserting that MIDI can capture all the information in a score (not merely \"Gameboy music\", and that for some musics (e.g. European classical) the score is of central importance. However, the authors may overstate the role of a score in jazz music.\n\nOverall, for me, the application, while fun, doesn't add enough to the impact of the paper. And the methodology, while appropriate, doesn't stand on its own. \n\n--Update-- Thanks for your modifications and arguments. I've revised my scores to add a point. ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tuning Recurrent Neural Networks with Reinforcement Learning", "abstract": "The approach of training sequence models using supervised learning and next-step prediction  suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. We propose a novel sequence-learning approach in which we use a pre-trained Recurrent Neural Network (RNN) to supply part of the reward value in a Reinforcement Learning (RL) model. Thus, we can refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from KL control. We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note RNN is then refined using our method and rules of music theory. We show that by combining maximum likelihood (ML) and RL in this way, we can not only produce more pleasing melodies, but  significantly reduce unwanted behaviors and failure modes of the RNN, while maintaining information learned from data.", "pdf": "/pdf/c4f118ae6bf24cd110be68a6280bac447bdfd1df.pdf", "TL;DR": "RL Tuner is a method for refining an LSTM trained on data by using RL to impose desired behaviors, while maintaining good predictive properties learned from data.", "paperhash": "jaques|tuning_recurrent_neural_networks_with_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning", "Structured prediction", "Supervised Learning", "Applications"], "conflicts": ["media.mit.edu", "google.com", "cam.ac.uk"], "authors": ["Natasha Jaques", "Shixiang Gu", "Richard E. Turner", "Douglas Eck"], "authorids": ["jaquesn@mit.edu", "sg717@cam.ac.uk", "ret26@cam.ack.uk", "deck@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512655648, "id": "ICLR.cc/2017/conference/-/paper229/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper229/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper229/AnonReviewer1", "ICLR.cc/2017/conference/paper229/AnonReviewer4", "ICLR.cc/2017/conference/paper229/AnonReviewer2"], "reply": {"forum": "BJ8fyHceg", "replyto": "BJ8fyHceg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper229/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper229/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512655648}}}, {"tddate": null, "tmdate": 1484187022728, "tcdate": 1484187022728, "number": 12, "id": "B1w5HwV8x", "invitation": "ICLR.cc/2017/conference/-/paper229/public/comment", "forum": "BJ8fyHceg", "replyto": "ByY9GC-Vl", "signatures": ["~Natasha_Jaques1"], "readers": ["everyone"], "writers": ["~Natasha_Jaques1"], "content": {"title": "Response", "comment": "Thank you very much for your detailed and insightful comments.\n\nIn Section 3.1 first paragraph, we have given further explanation about the history of stochastic optimal control/KL control and discussed the references you mentioned. While inference-based techniques (Attias 2003, Toussaint et al 2006, Toussaint el al 2009, Kappen et. al. 2012) using EM and message-passing are applicable for us because we have the environment model (RNNs), we focused on building up from Rawlik et. al. (2012) because (1) we focus on extensions and comparison with Q-learning/DQNs, (2) effectively scaling inference techniques for NN models is still an active area of research, (3) model-free, off-policy KL control techniques analyzed in our paper are applicable to more general problem settings (e.g. unknown models).\n\nIn terms of the relation with DQN, the main difference of generalized Psi-learning/G-learning from DQN with log prior augmentation is simply the entropy regularization, as we show in Eq. 4-5 and Eq. 11-14. We certainly hope our work can serve as a good starting point for exploring SOC/KL control for readers familiar with DQN literatures. \n\nThe choice of E_pi \\log p(a_t|s_t) is to ensure that it corresponds to KL control; however, it is a very interesting point. Our method effectively minimizes KL(pi||p*exp(Q)) and therefore in states where prior model p is uncertain/flat, it still learns to fit distribution according to cumulative future reward, exp(Q). One way to implement your suggestion is to directly mix KL(p||pi) and entropy-regularized objective, which basically corresponds to mixing maximum likelihood learning (ML) and RL; this may be doable but may not have nice analytical interpretation. Another way may be to use a variant of reward-augmented maximum likelihood objective (Norouzi et al. 2016). This is an interesting approach for approximate KL control, because it then only requires ML training, and corresponds to minimizing KL(p*exp(Q)||pi), as opposed to KL(pi||p*exp(Q)) on our paper. However, such approach requires sampling from p*exp(Q), which is not directly possible for our application and thus requires an importance weighting approach similar to reward-weighted regression (i.e. sampling from the prior p and weigh by exp(Q)). In addition, it has standard ML vs RL training problems, e.g. ML training distribution does not correspond to test time distribution. Given such difficulties, we believe our approach is more appropriate; however, those alternatives could be interesting directions for future comparisons. \n\nThe concern about whether the RNN\u2019s policy covers the space of possible melodies is an interesting one. Considering the model was trained on over 30,000 songs (and as you point out, the action space is discrete and small), we believe that the data itself provides a reasonable basis for covering the space of human-generated melodies. However, it is a known weakness of LSTMs that when a generated sequence diverges too much from any sequence in the training data, the model has no reliable basis for continued generation. In this case, a reward-based augmentation becomes particularly important. Since our model can more thoroughly explore the space of all possible melodies (especially with the right exploration strategy), the music theory reward signal can provide guidance in data-unsupported regions of the space, and therefore make the model more robust during the generation phase than a vanilla LSTM."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tuning Recurrent Neural Networks with Reinforcement Learning", "abstract": "The approach of training sequence models using supervised learning and next-step prediction  suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. We propose a novel sequence-learning approach in which we use a pre-trained Recurrent Neural Network (RNN) to supply part of the reward value in a Reinforcement Learning (RL) model. Thus, we can refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from KL control. We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note RNN is then refined using our method and rules of music theory. We show that by combining maximum likelihood (ML) and RL in this way, we can not only produce more pleasing melodies, but  significantly reduce unwanted behaviors and failure modes of the RNN, while maintaining information learned from data.", "pdf": "/pdf/c4f118ae6bf24cd110be68a6280bac447bdfd1df.pdf", "TL;DR": "RL Tuner is a method for refining an LSTM trained on data by using RL to impose desired behaviors, while maintaining good predictive properties learned from data.", "paperhash": "jaques|tuning_recurrent_neural_networks_with_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning", "Structured prediction", "Supervised Learning", "Applications"], "conflicts": ["media.mit.edu", "google.com", "cam.ac.uk"], "authors": ["Natasha Jaques", "Shixiang Gu", "Richard E. Turner", "Douglas Eck"], "authorids": ["jaquesn@mit.edu", "sg717@cam.ac.uk", "ret26@cam.ack.uk", "deck@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674646, "id": "ICLR.cc/2017/conference/-/paper229/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ8fyHceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper229/reviewers", "ICLR.cc/2017/conference/paper229/areachairs"], "cdate": 1485287674646}}}, {"tddate": null, "tmdate": 1484186948285, "tcdate": 1484186803043, "number": 11, "id": "rJjhVwVUx", "invitation": "ICLR.cc/2017/conference/-/paper229/public/comment", "forum": "BJ8fyHceg", "replyto": "SktlRezEl", "signatures": ["~Natasha_Jaques1"], "readers": ["everyone"], "writers": ["~Natasha_Jaques1"], "content": {"title": "Response", "comment": "Thank you for your comments. However, the application is not the primary contribution of this paper. \n\nWhile we acknowledge that there has been previous work on combining maximum likelihood estimation (MLE) and RL in text generation, we are the first to design a reward function that includes the probability of each action originally learned by an RNN pre-trained on data, as well as task-related rewards. Our approach is unique not because it improves sequence generation with RL, but because it allows the model to directly preserve information about the original probability distributions learned from data, and explicitly controls the trade-off between the influence of data and heuristic rewards. This is an important novel direction of research, because in many tasks the available reward functions are not a perfect metric that alone will lead to the best task performance in the real world (e.g. BLEU score). In contrast with previous work (e.g. Ranzato et al. (2015), Bahdanau et al. (2016), Norouzi et al. (2016), Li et al. (2016)), we do not use MLE training as a way to simply bootstrap the training of an RL model (since training with RL from scratch is difficult), but rather we rely mainly on information learned from data, and use RL only as a way to refine characteristics of the output by imposing structural rules.  \n\nAt first glance, the method of Li et al. (2016) may appear similar to our method, because it involves heuristic reward functions designed to improve dialog generation, and the output of a pre-trained RNN is involved in one of the heuristic reward functions. However, it is only used to teach the model to choose dialog turns that minimize the probability that the pre-trained model places on 8 manually chosen ``dull\" responses, such as ``I don't know\"; essentially, as part of one of the heuristic rules. In contrast, our approach directly penalizes divergence from the probability distribution learned by the MLE model for every response, allowing the model to retain information about the full space of sequences originally learned from data. Further, in Li et al. (2016) only heuristic rewards are used for further training after pre-training with MLE, which alters the model to optimize only for these heuristics. This is effectively the same as the other ML to RL papers (Ranzato et al. 2015, Bahdanau et al. 2016). In contrast, our approach allows the model to retain information learned from data, while explicitly controlling the trade-off between the influence of data and heuristic reward with the c parameter. \n\nIn addition to our novel combination of ML and RL, we make the following RL contributions:\n- We show the connection between our approach and SOC/KL-control with a pre-trained RNN as a prior policy, and the explicit relationships among a generalized variant of Psi-learning, G-learning, and Q-learning with log prior augmentation. \n- We are the first to explore generalized Psi-learning and G-learning with deep neural network. Our work may serve as a reference for exploring KL-regularized RL objectives for readers familiar with DQN literatures. \n- We are the first to empirically compare generalized Psi-learning, G-learning, and Q-learning with log prior augmentation.\n\nBecause the paper involves both these contributions and a complex application, the impact of the contributions may have been diluted. We have updated the abstract, introduction, and related work sections of the paper to more clearly emphasize these contributions.\n\nThank you for pointing out that \u2018composition' is a misnomer for the melodies produced by our model. We have modified the paper to remove this term. Further, we have taken your advice to better motivate the work by adding more justification and explanation regarding the music generation problem to Section 7. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tuning Recurrent Neural Networks with Reinforcement Learning", "abstract": "The approach of training sequence models using supervised learning and next-step prediction  suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. We propose a novel sequence-learning approach in which we use a pre-trained Recurrent Neural Network (RNN) to supply part of the reward value in a Reinforcement Learning (RL) model. Thus, we can refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from KL control. We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note RNN is then refined using our method and rules of music theory. We show that by combining maximum likelihood (ML) and RL in this way, we can not only produce more pleasing melodies, but  significantly reduce unwanted behaviors and failure modes of the RNN, while maintaining information learned from data.", "pdf": "/pdf/c4f118ae6bf24cd110be68a6280bac447bdfd1df.pdf", "TL;DR": "RL Tuner is a method for refining an LSTM trained on data by using RL to impose desired behaviors, while maintaining good predictive properties learned from data.", "paperhash": "jaques|tuning_recurrent_neural_networks_with_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning", "Structured prediction", "Supervised Learning", "Applications"], "conflicts": ["media.mit.edu", "google.com", "cam.ac.uk"], "authors": ["Natasha Jaques", "Shixiang Gu", "Richard E. Turner", "Douglas Eck"], "authorids": ["jaquesn@mit.edu", "sg717@cam.ac.uk", "ret26@cam.ack.uk", "deck@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674646, "id": "ICLR.cc/2017/conference/-/paper229/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ8fyHceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper229/reviewers", "ICLR.cc/2017/conference/paper229/areachairs"], "cdate": 1485287674646}}}, {"tddate": null, "tmdate": 1484186638927, "tcdate": 1484186638927, "number": 10, "id": "BJvf4wVUg", "invitation": "ICLR.cc/2017/conference/-/paper229/public/comment", "forum": "BJ8fyHceg", "replyto": "rynOcVrEx", "signatures": ["~Natasha_Jaques1"], "readers": ["everyone"], "writers": ["~Natasha_Jaques1"], "content": {"title": "Response", "comment": "Thank you for your comments. You are right to point out that \u201ccompositions\u201d is a generous term for the monophonic melodies generated by this model. We have removed references to this term in the current version of the paper. We have also added more discussion on the musical shortcomings of the model to Section 7. \n\nYou are correct that the paper does not clarify the degree of \u201ccherry-picking\u201d of the melodies. The procedure was to generate 4-8 melodies from each model (including the Note-RNN), and choose the best four. This procedure was chosen because the Note-RNN would sometimes generate melodies that were entirely rests or contained only a single note, and this did not seem like a reasonable comparison. The quality of the RL Tuner samples was fairly consistent; we believe the study could be repeated successfully with the first samples randomly generated from each model, and would be willing to do this if it is important to your review.  \n\nWe could also include human-composed melodies of the same length and synthesized in the same way in a second iteration of the Mechanical Turk study (and therefore in Figure 3). They were not included in the initial study because we do not expect our model to be superior to a human musician at this point.\n\nDilated convolution is a promising approach for music generation. However, we think that the utility of the technique presented in this paper is not limited to music generation. Our method of trading-off information learned from data with knowledge-based constraints could be applied to a variety of applications, including generating better text, and reducing bias in models trained on data. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tuning Recurrent Neural Networks with Reinforcement Learning", "abstract": "The approach of training sequence models using supervised learning and next-step prediction  suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. We propose a novel sequence-learning approach in which we use a pre-trained Recurrent Neural Network (RNN) to supply part of the reward value in a Reinforcement Learning (RL) model. Thus, we can refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from KL control. We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note RNN is then refined using our method and rules of music theory. We show that by combining maximum likelihood (ML) and RL in this way, we can not only produce more pleasing melodies, but  significantly reduce unwanted behaviors and failure modes of the RNN, while maintaining information learned from data.", "pdf": "/pdf/c4f118ae6bf24cd110be68a6280bac447bdfd1df.pdf", "TL;DR": "RL Tuner is a method for refining an LSTM trained on data by using RL to impose desired behaviors, while maintaining good predictive properties learned from data.", "paperhash": "jaques|tuning_recurrent_neural_networks_with_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning", "Structured prediction", "Supervised Learning", "Applications"], "conflicts": ["media.mit.edu", "google.com", "cam.ac.uk"], "authors": ["Natasha Jaques", "Shixiang Gu", "Richard E. Turner", "Douglas Eck"], "authorids": ["jaquesn@mit.edu", "sg717@cam.ac.uk", "ret26@cam.ack.uk", "deck@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674646, "id": "ICLR.cc/2017/conference/-/paper229/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ8fyHceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper229/reviewers", "ICLR.cc/2017/conference/paper229/areachairs"], "cdate": 1485287674646}}}, {"tddate": null, "tmdate": 1484186560927, "tcdate": 1484186560927, "number": 9, "id": "HkFa7PEUe", "invitation": "ICLR.cc/2017/conference/-/paper229/public/comment", "forum": "BJ8fyHceg", "replyto": "BJ8fyHceg", "signatures": ["~Natasha_Jaques1"], "readers": ["everyone"], "writers": ["~Natasha_Jaques1"], "content": {"title": "Summary of changes", "comment": "The following changes to the paper were made in response to comments:\n-Updated related work with additional references\n-Updated Section 3.1 to better explain KL control, and to add more explanation on prior work, the history of optimal control, and how reward augmentation can work with SOC\n-Re-wrote the abstract and introduction to better reflect the paper\u2019s contributions\n-Removed references to composition\n-Change Psi-learning to \u2018generalized Psi learning\u2019\n-Updated Section 6 to add more discussion about the musical shortcomings of the model \n-Modified language in Section 6 to ensure the G model\u2019s different trade-off between data probability and reward was not referred to as worse\n-Updated Section 3 to give more detail on how the states and actions work with the recurrent Q-network"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tuning Recurrent Neural Networks with Reinforcement Learning", "abstract": "The approach of training sequence models using supervised learning and next-step prediction  suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. We propose a novel sequence-learning approach in which we use a pre-trained Recurrent Neural Network (RNN) to supply part of the reward value in a Reinforcement Learning (RL) model. Thus, we can refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from KL control. We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note RNN is then refined using our method and rules of music theory. We show that by combining maximum likelihood (ML) and RL in this way, we can not only produce more pleasing melodies, but  significantly reduce unwanted behaviors and failure modes of the RNN, while maintaining information learned from data.", "pdf": "/pdf/c4f118ae6bf24cd110be68a6280bac447bdfd1df.pdf", "TL;DR": "RL Tuner is a method for refining an LSTM trained on data by using RL to impose desired behaviors, while maintaining good predictive properties learned from data.", "paperhash": "jaques|tuning_recurrent_neural_networks_with_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning", "Structured prediction", "Supervised Learning", "Applications"], "conflicts": ["media.mit.edu", "google.com", "cam.ac.uk"], "authors": ["Natasha Jaques", "Shixiang Gu", "Richard E. Turner", "Douglas Eck"], "authorids": ["jaquesn@mit.edu", "sg717@cam.ac.uk", "ret26@cam.ack.uk", "deck@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674646, "id": "ICLR.cc/2017/conference/-/paper229/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ8fyHceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper229/reviewers", "ICLR.cc/2017/conference/paper229/areachairs"], "cdate": 1485287674646}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484186424517, "tcdate": 1478278926389, "number": 229, "id": "BJ8fyHceg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BJ8fyHceg", "signatures": ["~Natasha_Jaques1"], "readers": ["everyone"], "content": {"title": "Tuning Recurrent Neural Networks with Reinforcement Learning", "abstract": "The approach of training sequence models using supervised learning and next-step prediction  suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. We propose a novel sequence-learning approach in which we use a pre-trained Recurrent Neural Network (RNN) to supply part of the reward value in a Reinforcement Learning (RL) model. Thus, we can refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from KL control. We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note RNN is then refined using our method and rules of music theory. We show that by combining maximum likelihood (ML) and RL in this way, we can not only produce more pleasing melodies, but  significantly reduce unwanted behaviors and failure modes of the RNN, while maintaining information learned from data.", "pdf": "/pdf/c4f118ae6bf24cd110be68a6280bac447bdfd1df.pdf", "TL;DR": "RL Tuner is a method for refining an LSTM trained on data by using RL to impose desired behaviors, while maintaining good predictive properties learned from data.", "paperhash": "jaques|tuning_recurrent_neural_networks_with_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning", "Structured prediction", "Supervised Learning", "Applications"], "conflicts": ["media.mit.edu", "google.com", "cam.ac.uk"], "authors": ["Natasha Jaques", "Shixiang Gu", "Richard E. Turner", "Douglas Eck"], "authorids": ["jaquesn@mit.edu", "sg717@cam.ac.uk", "ret26@cam.ack.uk", "deck@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 20, "writable": false, "overwriting": ["Syyv2e-Kx"], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1482456815257, "tcdate": 1482456752392, "number": 8, "id": "BJdnAlq4e", "invitation": "ICLR.cc/2017/conference/-/paper229/public/comment", "forum": "BJ8fyHceg", "replyto": "SJ_SNRbEg", "signatures": ["~Natasha_Jaques1"], "readers": ["everyone"], "writers": ["~Natasha_Jaques1"], "content": {"title": "Sensitivity on c: results", "comment": "Thank you for your excellent question. We actually conducted additional experiments in which we varied the c parameter and observed the results. A larger c allows the models to achieve higher log p(a|s), and there are a range of c values which still produce subjectively pleasant melodies. \n\nI have uploaded an additional folder of results of the first four randomly generated samples from each model for c=10: https://goo.gl/cTZy8r. Since the value used in the paper was c=0.5, this could be considered a much more extreme trade-off in favour of log p(a|s) over music theory. While the melodies still sound somewhat pleasant, the .png files show that at least for the Q model, the distribution over the next note looks much more similar to that generated by the vanilla Note-RNN, only it is more peaky, and more concentrated on notes that are in key. \n\nWe believe the c parameter provides a valuable way to trade-off the emphasis on data vs. heuristic reward. The choice of c could be based on aesthetic preference or the needs of the application.\n\nThe additional results related to c were not included in the paper due to space constraints, but we could include them in a future version. \n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tuning Recurrent Neural Networks with Reinforcement Learning", "abstract": "The approach of training sequence models using supervised learning and next-step prediction  suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. We propose a novel sequence-learning approach in which we use a pre-trained Recurrent Neural Network (RNN) to supply part of the reward value in a Reinforcement Learning (RL) model. Thus, we can refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from KL control. We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note RNN is then refined using our method and rules of music theory. We show that by combining maximum likelihood (ML) and RL in this way, we can not only produce more pleasing melodies, but  significantly reduce unwanted behaviors and failure modes of the RNN, while maintaining information learned from data.", "pdf": "/pdf/c4f118ae6bf24cd110be68a6280bac447bdfd1df.pdf", "TL;DR": "RL Tuner is a method for refining an LSTM trained on data by using RL to impose desired behaviors, while maintaining good predictive properties learned from data.", "paperhash": "jaques|tuning_recurrent_neural_networks_with_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning", "Structured prediction", "Supervised Learning", "Applications"], "conflicts": ["media.mit.edu", "google.com", "cam.ac.uk"], "authors": ["Natasha Jaques", "Shixiang Gu", "Richard E. Turner", "Douglas Eck"], "authorids": ["jaquesn@mit.edu", "sg717@cam.ac.uk", "ret26@cam.ack.uk", "deck@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674646, "id": "ICLR.cc/2017/conference/-/paper229/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ8fyHceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper229/reviewers", "ICLR.cc/2017/conference/paper229/areachairs"], "cdate": 1485287674646}}}, {"tddate": null, "tmdate": 1482144371681, "tcdate": 1482144371681, "number": 3, "id": "rynOcVrEx", "invitation": "ICLR.cc/2017/conference/-/paper229/official/review", "forum": "BJ8fyHceg", "replyto": "BJ8fyHceg", "signatures": ["ICLR.cc/2017/conference/paper229/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper229/AnonReviewer2"], "content": {"title": "Hand-crafted musical reward for fine-tuning LSTMs", "rating": "6: Marginally above acceptance threshold", "review": "This paper suggests combining LSTMs, trained on a large midi corpus, with a handcrafted reward function that helps to fine-tune the model in a musically meaningful way. The idea to use hand-crafted rewards in such a way is great and seems promising for practical scenarios, where a musician would like to design a set of rules, rather than a set of melodies.\n\nEven though some choices made along the way seem rather ad-hoc and simplistic from a music theoretical perspective, the results sound like an improvement upon the note RNN baseline, but we also don't know how cherry picked these results are. \nI am not convinced that this approach will scale to much more complicated reward functions necessary to compose real music. Maybe LSTMs are the wrong approach altogether if they have so much trouble learning to produce pleasant melodies from such a relatively big corpus of data. Aren't there any alternative differentiable models that are more suitable? What about dilated convolution based approaches?\n\nWhat I don't like about the paper is that the short melodies are referenced as compositions while being very far from meaningful music, they are not even polyphonic after all. I think it would be great if such papers would be written with the help or feedback of people that have real musical training and are more critical towards these details. \n\nWhat I like about the paper is that the authors make an effort to understand what is going on, table 1 is interesting for instance. However, Figure 3 should have included real melody excerpts with the same sound synthesis/sample setup. Besides that, more discussion on the shortcomings of the presented method should be added.\n\nIn summary, I do like the paper and idea and I can imagine that such RL based fine-tuning approaches will indeed be useful for musicians. Even though the novelty might be limited, the paper serves as a documentation on how to achieve solid results in practice.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tuning Recurrent Neural Networks with Reinforcement Learning", "abstract": "The approach of training sequence models using supervised learning and next-step prediction  suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. We propose a novel sequence-learning approach in which we use a pre-trained Recurrent Neural Network (RNN) to supply part of the reward value in a Reinforcement Learning (RL) model. Thus, we can refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from KL control. We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note RNN is then refined using our method and rules of music theory. We show that by combining maximum likelihood (ML) and RL in this way, we can not only produce more pleasing melodies, but  significantly reduce unwanted behaviors and failure modes of the RNN, while maintaining information learned from data.", "pdf": "/pdf/c4f118ae6bf24cd110be68a6280bac447bdfd1df.pdf", "TL;DR": "RL Tuner is a method for refining an LSTM trained on data by using RL to impose desired behaviors, while maintaining good predictive properties learned from data.", "paperhash": "jaques|tuning_recurrent_neural_networks_with_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning", "Structured prediction", "Supervised Learning", "Applications"], "conflicts": ["media.mit.edu", "google.com", "cam.ac.uk"], "authors": ["Natasha Jaques", "Shixiang Gu", "Richard E. Turner", "Douglas Eck"], "authorids": ["jaquesn@mit.edu", "sg717@cam.ac.uk", "ret26@cam.ack.uk", "deck@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512655648, "id": "ICLR.cc/2017/conference/-/paper229/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper229/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper229/AnonReviewer1", "ICLR.cc/2017/conference/paper229/AnonReviewer4", "ICLR.cc/2017/conference/paper229/AnonReviewer2"], "reply": {"forum": "BJ8fyHceg", "replyto": "BJ8fyHceg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper229/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper229/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512655648}}}, {"tddate": null, "tmdate": 1481921600349, "tcdate": 1481921600349, "number": 3, "id": "SJ_SNRbEg", "invitation": "ICLR.cc/2017/conference/-/paper229/pre-review/question", "forum": "BJ8fyHceg", "replyto": "BJ8fyHceg", "signatures": ["ICLR.cc/2017/conference/paper229/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper229/AnonReviewer1"], "content": {"title": "Sensitivity on c.", "question": "It seems there may be significant sensitivity on c. Please can you comment more on this."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tuning Recurrent Neural Networks with Reinforcement Learning", "abstract": "The approach of training sequence models using supervised learning and next-step prediction  suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. We propose a novel sequence-learning approach in which we use a pre-trained Recurrent Neural Network (RNN) to supply part of the reward value in a Reinforcement Learning (RL) model. Thus, we can refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from KL control. We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note RNN is then refined using our method and rules of music theory. We show that by combining maximum likelihood (ML) and RL in this way, we can not only produce more pleasing melodies, but  significantly reduce unwanted behaviors and failure modes of the RNN, while maintaining information learned from data.", "pdf": "/pdf/c4f118ae6bf24cd110be68a6280bac447bdfd1df.pdf", "TL;DR": "RL Tuner is a method for refining an LSTM trained on data by using RL to impose desired behaviors, while maintaining good predictive properties learned from data.", "paperhash": "jaques|tuning_recurrent_neural_networks_with_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning", "Structured prediction", "Supervised Learning", "Applications"], "conflicts": ["media.mit.edu", "google.com", "cam.ac.uk"], "authors": ["Natasha Jaques", "Shixiang Gu", "Richard E. Turner", "Douglas Eck"], "authorids": ["jaquesn@mit.edu", "sg717@cam.ac.uk", "ret26@cam.ack.uk", "deck@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481921601018, "id": "ICLR.cc/2017/conference/-/paper229/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper229/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper229/AnonReviewer2", "ICLR.cc/2017/conference/paper229/AnonReviewer4", "ICLR.cc/2017/conference/paper229/AnonReviewer1"], "reply": {"forum": "BJ8fyHceg", "replyto": "BJ8fyHceg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper229/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper229/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481921601018}}}, {"tddate": null, "tmdate": 1481921169203, "tcdate": 1481921169203, "number": 1, "id": "ByY9GC-Vl", "invitation": "ICLR.cc/2017/conference/-/paper229/official/review", "forum": "BJ8fyHceg", "replyto": "BJ8fyHceg", "signatures": ["ICLR.cc/2017/conference/paper229/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper229/AnonReviewer1"], "content": {"title": "Combining data driven models and reinforcement signals", "rating": "5: Marginally below acceptance threshold", "review": "This paper uses a combination of likelihood and reward based learning to learn sequence models for music. The ability to combine likelihood and reward based learning has been long known, as a result of the unification of inference and learning first appearing in the ML literature with the EM formalism of Attias (2003) for fixed horizons, extended by Toussaint and Storkey (2006), to general horizon settings, Toussaint et al. (2011) to POMDPs and generalised further by Kappen et Al. (2012) and Rawlik et Al. (2012). These papers introduced the basic unification, and so any additional probabilistic or data driven objective can be combined with the reinforcement learning signal: it is all part of a unified reward/likelihood. Hence the optimal control target under unification is p(b=1|\\tau)E_p(A,S) \\prod_t \\pi(a_t|s_t): i.e. the probability of getting reward, and probability of the policy actions under the known data-derived distribution, thereby introducing the log p(a_t|s_t) into (9) too.\n\nThe interpretation of the secondary objective as the prior is an alternative approach under a stochastic optimal control setting, but not the most natural one given the whole principle of SOC of matching control objectives to inference objectives. The SOC off policy objective still does still contain the KL term so the approach would still differ from the approach of this paper.\n\nThough the discussion of optimal control is good, I think some further elaboration of the history and how reward augmentation can work in SOC would be valuable. This would allow SOC off-policy methods to be compared with the DQN directly, like for like.\n\nThe motivation of the objective (3) is sensible but could be made clearer via the unification argument above. Then the paper uses DCN to take a different approach from the variational SOC for achieving that objective.\n\nAnother interesting point of discussion is the choice of E_pi \\log p(a_t|s_t) \u2013 this means the policy must \u201ccover\u201d the model. But one problem in generation is that a well-trained model is often underfit, resulting in actions that, over the course of a number of iterations, move the state into data-unsupported parts of the space. As a result the model is no longer confident and quickly tends to be fairly random. This approach (as opposed to a KL(p||pi) \u2013 which is not obvious how to implement) cannot mitigate against that, without a very strong signal (to overcome the tails of a distribution). In music, with a smaller discrete alphabet, this is likely to be less of a problem than for real valued policy densities, with exponentially decaying tails. Some further discussion of what you see in light of this issue would be valuable: the use of c to balance things seems critical, and it seems clear from Figure 2 that the reward signal needed to be very high to push the log p signal into the right range.\n\nAltogether, in the music setting this paper provides a reasonable demonstration that augmentation of a sequence model with an additional reward constraint is valuable. It demonstrates that DQN is one way of learning that signal, but AFAICS it does not compare learning the same signal via other techniques. Instead for the comparator techniques it reverts to treating the p(a|s) as a \u201cprior\u201d term rather than a reward term, leaving a bit of a question as to whether DQN is particularly appropriate. \n\nAnother interesting question for the discussion is whether the music theory reward could be approximated by a differentiable model, mitigating the need for an RL approach at all.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tuning Recurrent Neural Networks with Reinforcement Learning", "abstract": "The approach of training sequence models using supervised learning and next-step prediction  suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. We propose a novel sequence-learning approach in which we use a pre-trained Recurrent Neural Network (RNN) to supply part of the reward value in a Reinforcement Learning (RL) model. Thus, we can refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from KL control. We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note RNN is then refined using our method and rules of music theory. We show that by combining maximum likelihood (ML) and RL in this way, we can not only produce more pleasing melodies, but  significantly reduce unwanted behaviors and failure modes of the RNN, while maintaining information learned from data.", "pdf": "/pdf/c4f118ae6bf24cd110be68a6280bac447bdfd1df.pdf", "TL;DR": "RL Tuner is a method for refining an LSTM trained on data by using RL to impose desired behaviors, while maintaining good predictive properties learned from data.", "paperhash": "jaques|tuning_recurrent_neural_networks_with_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning", "Structured prediction", "Supervised Learning", "Applications"], "conflicts": ["media.mit.edu", "google.com", "cam.ac.uk"], "authors": ["Natasha Jaques", "Shixiang Gu", "Richard E. Turner", "Douglas Eck"], "authorids": ["jaquesn@mit.edu", "sg717@cam.ac.uk", "ret26@cam.ack.uk", "deck@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512655648, "id": "ICLR.cc/2017/conference/-/paper229/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper229/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper229/AnonReviewer1", "ICLR.cc/2017/conference/paper229/AnonReviewer4", "ICLR.cc/2017/conference/paper229/AnonReviewer2"], "reply": {"forum": "BJ8fyHceg", "replyto": "BJ8fyHceg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper229/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper229/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512655648}}}, {"tddate": null, "tmdate": 1481914218123, "tcdate": 1481914218123, "number": 7, "id": "rkGOD3ZVl", "invitation": "ICLR.cc/2017/conference/-/paper229/public/comment", "forum": "BJ8fyHceg", "replyto": "S1EyTzeXx", "signatures": ["~Douglas_Eck1"], "readers": ["everyone"], "writers": ["~Douglas_Eck1"], "content": {"title": "Adding to Natasha's comment about the relevance of MIDI for music", "comment": "Thanks for your great posting! \n\nI\u2019d like to address the question, \u201cWhy should we care about MIDI piano melody generation? Do the authors think that this has a serious connection to music?\u201d  We believe, yes,  music composition is a relevant and difficult challenge, and that it is an important aspect of music creation.  \n\nMIDI allows us to represent two important aspects of music, (A) the music score and (B) the timing, dynamics and articulation of a music performance.  The current study is focused on \u201cA\u201d, learning to generate music scores. It is reasonable to question whether music scores are relevant. We think they are. Music scores are so ubiquitous in Western music that they can be considered, by analogy to speech and language, the \u201ctextual representation\u201d of music. There is considerable variance in music scores:  complex multi-part scores are used to represent symphonies and operas while single-line chord-annotated scores are used for jazz and rock. But in all forms, they serve as compact representations of note onset, pitch velocity and relative duration. \n\nRelevant specifically to reinforcement learning, we observe that the MIDI is good for representing the actions a musician (or robot) takes when he or she plays a music instrument such as a piano. The musician doesn\u2019t generate the audio timeseries -- that job is handled by the instrument -- but rather moves levers that launch hammers into strings. This makes MIDI particularly useful for any reinforcement learning attempts to learn to play a music instrument.\n\nDoes MIDI represent all important aspects of music? Of course not. Two aspects of music are notably absent.  First, and most obviously, MIDI does not provide information about music timbre.  Second, although MIDI does store the timing, dynamics and articulation of a music performance, it\u2019s a cumbersome format for instruments like violins capable of long, gliding phrases or guitars, with subtle articulation effects like string bending, plucking vs hammering, etc. \n\nWe believe it would be a great achievement for an ML model to write a music score with as much structure as a symphony, opera, jazz piece or pop song.  Furthermore, we claim that success in this area will likely yield a better understanding of language and, perhaps, motor control. Why? Because music is an excellent example of a human-generated sequence exhibiting complex hierarchical long-timescale structure.  Representing and generating such long-timescale structure is hard and relates to challenges in other domains such as generating a narrative that spans multiple paragraphs.  \n\nWe don\u2019t claim that music scores tell the whole story, but we hope we have convinced you that MIDI is a suitable representation for music scores, and that music scores are not limited to \u201cGameboy soundtracks.\u201d"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tuning Recurrent Neural Networks with Reinforcement Learning", "abstract": "The approach of training sequence models using supervised learning and next-step prediction  suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. We propose a novel sequence-learning approach in which we use a pre-trained Recurrent Neural Network (RNN) to supply part of the reward value in a Reinforcement Learning (RL) model. Thus, we can refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from KL control. We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note RNN is then refined using our method and rules of music theory. We show that by combining maximum likelihood (ML) and RL in this way, we can not only produce more pleasing melodies, but  significantly reduce unwanted behaviors and failure modes of the RNN, while maintaining information learned from data.", "pdf": "/pdf/c4f118ae6bf24cd110be68a6280bac447bdfd1df.pdf", "TL;DR": "RL Tuner is a method for refining an LSTM trained on data by using RL to impose desired behaviors, while maintaining good predictive properties learned from data.", "paperhash": "jaques|tuning_recurrent_neural_networks_with_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning", "Structured prediction", "Supervised Learning", "Applications"], "conflicts": ["media.mit.edu", "google.com", "cam.ac.uk"], "authors": ["Natasha Jaques", "Shixiang Gu", "Richard E. Turner", "Douglas Eck"], "authorids": ["jaquesn@mit.edu", "sg717@cam.ac.uk", "ret26@cam.ack.uk", "deck@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674646, "id": "ICLR.cc/2017/conference/-/paper229/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ8fyHceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper229/reviewers", "ICLR.cc/2017/conference/paper229/areachairs"], "cdate": 1485287674646}}}, {"tddate": null, "tmdate": 1481893531211, "tcdate": 1481893531211, "number": 6, "id": "BJmjIDW4x", "invitation": "ICLR.cc/2017/conference/-/paper229/public/comment", "forum": "BJ8fyHceg", "replyto": "S1EyTzeXx", "signatures": ["~Natasha_Jaques1"], "readers": ["everyone"], "writers": ["~Natasha_Jaques1"], "content": {"title": "Response", "comment": "Thank you for your comment - these questions could lead to an interesting discussion!\n\nFirstly, we agree that the music theory rules used for this paper are restrictive and simplistic, and could easily be improved. These rules and the user study simply serve as a proof of concept that we can improve aesthetic quality of generated samples by imposing structural rules via this framework.\n\nHowever, please note that by generating MIDI, we are not restricting ourselves to generating only gameboy music! The MIDI format can be used to encode complex music, including not only note pitch and velocity, but also more intricate sounds like vibrato and panning (https://en.wikipedia.org/wiki/MIDI). MIDI can then be used with a wide variety of synthesizers to generate music from many different types of instruments.\n\nIn terms of whether music generation is itself an interesting problem to work on, consider the fact that image generation is considered an interesting and valuable task, with a great deal of research focusing on this problem (e.g. with GANs, PixelCNN, etc.), despite the fact that the images themselves are not yet convincing enough to serve some commercial purpose and in that sense do not have \u201creal-world\u201d value. The problem of generating realistic, \u201cpleasant\u201d, and \u201cinteresting\u201d music in the absence of a better aesthetic metric than log-likelihood could be considered equally challenging. Further, the field of automatic music generation is not new, and researchers have been working on music generation with LSTMs for some time (e.g. Sturm et al., 2016, Eck & Schmidhuber, 2002, as we cite in the paper). We believe our results may offer significant aesthetic advantages over this previous work.\n\nAll of that said, we are excited about the possibility of applying this framework to a text generation problem. There are also well known structural rules about essay composition, for example, which if taught to an RNN using our framework could lead to more coherent written compositions. While this is an exciting future extension, we believe that it\u2019s important to develop deep learning algorithms for a wide variety of domains, rather than restricting attention solely to well-explored domains like images and text."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tuning Recurrent Neural Networks with Reinforcement Learning", "abstract": "The approach of training sequence models using supervised learning and next-step prediction  suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. We propose a novel sequence-learning approach in which we use a pre-trained Recurrent Neural Network (RNN) to supply part of the reward value in a Reinforcement Learning (RL) model. Thus, we can refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from KL control. We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note RNN is then refined using our method and rules of music theory. We show that by combining maximum likelihood (ML) and RL in this way, we can not only produce more pleasing melodies, but  significantly reduce unwanted behaviors and failure modes of the RNN, while maintaining information learned from data.", "pdf": "/pdf/c4f118ae6bf24cd110be68a6280bac447bdfd1df.pdf", "TL;DR": "RL Tuner is a method for refining an LSTM trained on data by using RL to impose desired behaviors, while maintaining good predictive properties learned from data.", "paperhash": "jaques|tuning_recurrent_neural_networks_with_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning", "Structured prediction", "Supervised Learning", "Applications"], "conflicts": ["media.mit.edu", "google.com", "cam.ac.uk"], "authors": ["Natasha Jaques", "Shixiang Gu", "Richard E. Turner", "Douglas Eck"], "authorids": ["jaquesn@mit.edu", "sg717@cam.ac.uk", "ret26@cam.ack.uk", "deck@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674646, "id": "ICLR.cc/2017/conference/-/paper229/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ8fyHceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper229/reviewers", "ICLR.cc/2017/conference/paper229/areachairs"], "cdate": 1485287674646}}}, {"tddate": null, "tmdate": 1481394897213, "tcdate": 1481394897206, "number": 5, "id": "S1t0qptXx", "invitation": "ICLR.cc/2017/conference/-/paper229/public/comment", "forum": "BJ8fyHceg", "replyto": "SkBEXn17e", "signatures": ["~Natasha_Jaques1"], "readers": ["everyone"], "writers": ["~Natasha_Jaques1"], "content": {"title": "Link available at:  goo.gl/XIYt9m", "comment": "Thanks for your interest! Actually, on page 8 we provide a link to the samples generated for the user study: goo.gl/XIYt9m. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tuning Recurrent Neural Networks with Reinforcement Learning", "abstract": "The approach of training sequence models using supervised learning and next-step prediction  suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. We propose a novel sequence-learning approach in which we use a pre-trained Recurrent Neural Network (RNN) to supply part of the reward value in a Reinforcement Learning (RL) model. Thus, we can refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from KL control. We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note RNN is then refined using our method and rules of music theory. We show that by combining maximum likelihood (ML) and RL in this way, we can not only produce more pleasing melodies, but  significantly reduce unwanted behaviors and failure modes of the RNN, while maintaining information learned from data.", "pdf": "/pdf/c4f118ae6bf24cd110be68a6280bac447bdfd1df.pdf", "TL;DR": "RL Tuner is a method for refining an LSTM trained on data by using RL to impose desired behaviors, while maintaining good predictive properties learned from data.", "paperhash": "jaques|tuning_recurrent_neural_networks_with_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning", "Structured prediction", "Supervised Learning", "Applications"], "conflicts": ["media.mit.edu", "google.com", "cam.ac.uk"], "authors": ["Natasha Jaques", "Shixiang Gu", "Richard E. Turner", "Douglas Eck"], "authorids": ["jaquesn@mit.edu", "sg717@cam.ac.uk", "ret26@cam.ack.uk", "deck@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674646, "id": "ICLR.cc/2017/conference/-/paper229/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ8fyHceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper229/reviewers", "ICLR.cc/2017/conference/paper229/areachairs"], "cdate": 1485287674646}}}, {"tddate": null, "tmdate": 1481385552196, "tcdate": 1481385552189, "number": 4, "id": "HkOLLsKXg", "invitation": "ICLR.cc/2017/conference/-/paper229/public/comment", "forum": "BJ8fyHceg", "replyto": "ryQ8hB0Gg", "signatures": ["~Shixiang_Gu1"], "readers": ["everyone"], "writers": ["~Shixiang_Gu1"], "content": {"title": "Clarification on psi-learing & G-learning", "comment": "Thank you very much for detailed and valuable feedback! We would incorporate necessary changes into our next revision. \n1. This is a very fair point. We named it \"Psi-learning\" because it directly follows from intermediate derivation in \"Psi-learning\" paper with simple additional temperature variable, except we are aware that \"Psi-learing\" specifically uses previous policy as prior and come up with a different value iteration method that converges to different fixed point. We tried to clarify that by footnote 2 and the last paragraph in related work, but indeed it could be confusing and not fair. We propose to call our \"Psi-learning\" as generalized psi-learning to avoid this confusion.\n2. This relates to (1) above. The point is now that \"generalized psi-learning\" and \"g-learning\" are equivalent with different parametrization, given fixed temperature. We would like to see reviews from other reviewers first to see if it's fairer to call Eq.11, 12 \"generalized psi-learning\" (given that the formula comes directly from an intermediate step in psi-learning) or \"g-learning\" with an alternative parametrization. But we certainly see your suggestion is also very reasonable.\n3. This is a good point, and we will add missing details. All Q-learning derivations are written in terms of Markovian system to simplify notations. But our DQNs are actually recurrent, similar to Note RNNs. Since extending those to RNN is simple and does not change our derivations, we have not specified details (e.g. sampling whole trajectories each time, run RNN to compute Q targets, update RNN).\n4. That is a fair point. We will update the wording in Section 6 to reflect this. We have explored various values of c, and note that G consistently achieves higher log p(a|s) values and lower adherence to the music theory metrics. We will consider adding a plot as you suggest that better describes the 2-dimensional rewards received by each algorithm for different values of c.\n5. Thank you for mentioning this. Indeed SOC is a broader field with differing work from different communities, and sometimes the terminologies seem inconsistent. SOC, KL control, linearly solvable MDP seem to share underlying problem formulations. We will include sentences about KL control."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tuning Recurrent Neural Networks with Reinforcement Learning", "abstract": "The approach of training sequence models using supervised learning and next-step prediction  suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. We propose a novel sequence-learning approach in which we use a pre-trained Recurrent Neural Network (RNN) to supply part of the reward value in a Reinforcement Learning (RL) model. Thus, we can refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from KL control. We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note RNN is then refined using our method and rules of music theory. We show that by combining maximum likelihood (ML) and RL in this way, we can not only produce more pleasing melodies, but  significantly reduce unwanted behaviors and failure modes of the RNN, while maintaining information learned from data.", "pdf": "/pdf/c4f118ae6bf24cd110be68a6280bac447bdfd1df.pdf", "TL;DR": "RL Tuner is a method for refining an LSTM trained on data by using RL to impose desired behaviors, while maintaining good predictive properties learned from data.", "paperhash": "jaques|tuning_recurrent_neural_networks_with_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning", "Structured prediction", "Supervised Learning", "Applications"], "conflicts": ["media.mit.edu", "google.com", "cam.ac.uk"], "authors": ["Natasha Jaques", "Shixiang Gu", "Richard E. Turner", "Douglas Eck"], "authorids": ["jaquesn@mit.edu", "sg717@cam.ac.uk", "ret26@cam.ack.uk", "deck@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674646, "id": "ICLR.cc/2017/conference/-/paper229/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ8fyHceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper229/reviewers", "ICLR.cc/2017/conference/paper229/areachairs"], "cdate": 1485287674646}}}, {"tddate": null, "tmdate": 1480761448855, "tcdate": 1480760539791, "number": 2, "id": "S1EyTzeXx", "invitation": "ICLR.cc/2017/conference/-/paper229/pre-review/question", "forum": "BJ8fyHceg", "replyto": "BJ8fyHceg", "signatures": ["ICLR.cc/2017/conference/paper229/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper229/AnonReviewer4"], "content": {"title": "Motivation", "question": "The paper addresses music generation from MIDI tracks. The authors suggest some technical ideas for improving upon the vanilla generation approach using a LSTM one-step prediction. \n\nI'd like to ask a question about the paper's purpose and positioning. The main motivation given in the paper's introduction is \"Music compositions adhere to relatively well-defined structural rules, making music an interesting sequence generation challenge\". So the problem would appear to be interesting primarily for the machine learning challenge it presents. \n\nThen later in the paper there's a user study and a claim to produce melodies that are both \"pleasing\" and \"interesting\" as compared to other synthetic MIDI generation methods. Presumably the reason to value a user study would be that we think there may be real-world impact and want to relate the machine learning objectives to the real world performance we really care about. \n\nSo how suited is this work to the real world? From a musical perspective, this method of generating music seems somewhat naive. And the discussion about \"pleasantness\" and \"interestingness\" seems questionable from both scientific and musical perspectives. The music theory rules addressed here are prohibitively restrictive. And the limitation to generating MIDI piano scores would seem to confine the work to Gameboy soundtracks. \n\nSo I'd ask the authors to make a clearer case for the task. Why should we care about MIDI piano melody generation? Do the authors think that this has a serious connection to music? \n\nAnd if the music itself is not the primary point of interest, then I'd be interested to know, what learning challenges does this task offer that couldn't be addressed just as easily in the text domain, one with (it seems) clearer real world utility."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tuning Recurrent Neural Networks with Reinforcement Learning", "abstract": "The approach of training sequence models using supervised learning and next-step prediction  suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. We propose a novel sequence-learning approach in which we use a pre-trained Recurrent Neural Network (RNN) to supply part of the reward value in a Reinforcement Learning (RL) model. Thus, we can refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from KL control. We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note RNN is then refined using our method and rules of music theory. We show that by combining maximum likelihood (ML) and RL in this way, we can not only produce more pleasing melodies, but  significantly reduce unwanted behaviors and failure modes of the RNN, while maintaining information learned from data.", "pdf": "/pdf/c4f118ae6bf24cd110be68a6280bac447bdfd1df.pdf", "TL;DR": "RL Tuner is a method for refining an LSTM trained on data by using RL to impose desired behaviors, while maintaining good predictive properties learned from data.", "paperhash": "jaques|tuning_recurrent_neural_networks_with_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning", "Structured prediction", "Supervised Learning", "Applications"], "conflicts": ["media.mit.edu", "google.com", "cam.ac.uk"], "authors": ["Natasha Jaques", "Shixiang Gu", "Richard E. Turner", "Douglas Eck"], "authorids": ["jaquesn@mit.edu", "sg717@cam.ac.uk", "ret26@cam.ack.uk", "deck@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481921601018, "id": "ICLR.cc/2017/conference/-/paper229/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper229/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper229/AnonReviewer2", "ICLR.cc/2017/conference/paper229/AnonReviewer4", "ICLR.cc/2017/conference/paper229/AnonReviewer1"], "reply": {"forum": "BJ8fyHceg", "replyto": "BJ8fyHceg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper229/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper229/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481921601018}}}, {"tddate": null, "tmdate": 1480733484991, "tcdate": 1480733484985, "number": 1, "id": "SkBEXn17e", "invitation": "ICLR.cc/2017/conference/-/paper229/pre-review/question", "forum": "BJ8fyHceg", "replyto": "BJ8fyHceg", "signatures": ["ICLR.cc/2017/conference/paper229/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper229/AnonReviewer2"], "content": {"title": "Link", "question": "It would be nice to add a link to some of the results, is that possible?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tuning Recurrent Neural Networks with Reinforcement Learning", "abstract": "The approach of training sequence models using supervised learning and next-step prediction  suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. We propose a novel sequence-learning approach in which we use a pre-trained Recurrent Neural Network (RNN) to supply part of the reward value in a Reinforcement Learning (RL) model. Thus, we can refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from KL control. We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note RNN is then refined using our method and rules of music theory. We show that by combining maximum likelihood (ML) and RL in this way, we can not only produce more pleasing melodies, but  significantly reduce unwanted behaviors and failure modes of the RNN, while maintaining information learned from data.", "pdf": "/pdf/c4f118ae6bf24cd110be68a6280bac447bdfd1df.pdf", "TL;DR": "RL Tuner is a method for refining an LSTM trained on data by using RL to impose desired behaviors, while maintaining good predictive properties learned from data.", "paperhash": "jaques|tuning_recurrent_neural_networks_with_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning", "Structured prediction", "Supervised Learning", "Applications"], "conflicts": ["media.mit.edu", "google.com", "cam.ac.uk"], "authors": ["Natasha Jaques", "Shixiang Gu", "Richard E. Turner", "Douglas Eck"], "authorids": ["jaquesn@mit.edu", "sg717@cam.ac.uk", "ret26@cam.ack.uk", "deck@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481921601018, "id": "ICLR.cc/2017/conference/-/paper229/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper229/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper229/AnonReviewer2", "ICLR.cc/2017/conference/paper229/AnonReviewer4", "ICLR.cc/2017/conference/paper229/AnonReviewer1"], "reply": {"forum": "BJ8fyHceg", "replyto": "BJ8fyHceg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper229/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper229/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481921601018}}}, {"tddate": null, "tmdate": 1480641610852, "tcdate": 1480641610847, "number": 3, "id": "ryQ8hB0Gg", "invitation": "ICLR.cc/2017/conference/-/paper229/public/comment", "forum": "BJ8fyHceg", "replyto": "BJ8fyHceg", "signatures": ["~Roy_Fox1"], "readers": ["everyone"], "writers": ["~Roy_Fox1"], "content": {"title": "Very nice work! A few comments:", "comment": "Very nice work!\nInteresting to see how well it works to combine a reward signal with cross-entropy from an informative prior, even without the extra entropy regularization of the KL cost.\n\nWe would like to note a few important things, though:\n\n1. Psi-learning is not the method given in (11), (12), and Appendix 8.1.1.\nUsing the current policy as the prior for each small policy update is central to Psi-learning and for the properties of the Psi function.\nThis is best seen by comparing your (23) with Rawlik et al.'s (14), where they have Psi-\\bar{Psi} substituted for log(p).\nBy having p fixed and not dependent of Psi, you are qualitatively changing the fixed point of the equation.\nFor example, their Psi function is gap increasing, in that it only recovers the value function in the optimal action, and is negative infinity for suboptimal actions \u2014 your \"Psi\" function does not have this property, and the two should not be confused.\n\n2. Psi-learning is distinct from G-learning, in that the former uses small divergences from a changing prior, while the latter uses large divergences from a fixed prior.\nBy fixing the prior p in both of your derivations, both are using G-learning with a fixed temperature, albeit with different parameterization of the function G.\nThe two functions defined in (16) and (24) differ by the constant log(p), and this is then reflected in (23) and (30).\nIntriguingly, parameterizing G+log(p) yields better results than parameterizing G.\nThis is somewhat reminiscent of advantage learning, where one parameterizes Q-V rather than Q, and certainly merits further study.\n\n3. In DQN, the Q-network is a feed-forward network that maps (s,a) inputs into real outputs.\nIf this is the architecture used here, it is unclear how it is initialized from the differently shaped Note RNN.\nOn other hand, if the Q-network here is a RNN, it is unclear what state it keeps and how it computes Q(s,a;theta).\n\n4. As Figure 2 shows, \"G-learning\" achieves a different tradeoff with worse music-theory reward but better log(p).\nIt is therefore inaccurate to characterize it in Section 6 and Table 1 as doing worse.\nFor a fair comparison, one needs to set c differently for each algorithm, so that one reward is the same, and compare the other reward.\nIt would also be useful to repeat this for various values of c, and plot the two rewards on a plane, to allow comparing the reward-achievability regions of the algorithms.\n\n5. Stochastic optimal control is a much broader field (cf. Bertsekas 1995). In Section 3.1 you are referring specifically to the field known as KL control.\n\n\nAri Pakman and Roy Fox"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tuning Recurrent Neural Networks with Reinforcement Learning", "abstract": "The approach of training sequence models using supervised learning and next-step prediction  suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. We propose a novel sequence-learning approach in which we use a pre-trained Recurrent Neural Network (RNN) to supply part of the reward value in a Reinforcement Learning (RL) model. Thus, we can refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from KL control. We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note RNN is then refined using our method and rules of music theory. We show that by combining maximum likelihood (ML) and RL in this way, we can not only produce more pleasing melodies, but  significantly reduce unwanted behaviors and failure modes of the RNN, while maintaining information learned from data.", "pdf": "/pdf/c4f118ae6bf24cd110be68a6280bac447bdfd1df.pdf", "TL;DR": "RL Tuner is a method for refining an LSTM trained on data by using RL to impose desired behaviors, while maintaining good predictive properties learned from data.", "paperhash": "jaques|tuning_recurrent_neural_networks_with_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning", "Structured prediction", "Supervised Learning", "Applications"], "conflicts": ["media.mit.edu", "google.com", "cam.ac.uk"], "authors": ["Natasha Jaques", "Shixiang Gu", "Richard E. Turner", "Douglas Eck"], "authorids": ["jaquesn@mit.edu", "sg717@cam.ac.uk", "ret26@cam.ack.uk", "deck@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674646, "id": "ICLR.cc/2017/conference/-/paper229/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ8fyHceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper229/reviewers", "ICLR.cc/2017/conference/paper229/areachairs"], "cdate": 1485287674646}}}, {"tddate": null, "tmdate": 1478803332260, "tcdate": 1478803332255, "number": 2, "id": "rkhY1HMbe", "invitation": "ICLR.cc/2017/conference/-/paper229/public/comment", "forum": "BJ8fyHceg", "replyto": "S1_8KEheg", "signatures": ["~Natasha_Jaques1"], "readers": ["everyone"], "writers": ["~Natasha_Jaques1"], "content": {"title": "Thanks, we'll add the citation!", "comment": "Glad you enjoyed the paper. We weren't aware of that work, but it sounds highly relevant. We will add the citation in the next version!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tuning Recurrent Neural Networks with Reinforcement Learning", "abstract": "The approach of training sequence models using supervised learning and next-step prediction  suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. We propose a novel sequence-learning approach in which we use a pre-trained Recurrent Neural Network (RNN) to supply part of the reward value in a Reinforcement Learning (RL) model. Thus, we can refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from KL control. We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note RNN is then refined using our method and rules of music theory. We show that by combining maximum likelihood (ML) and RL in this way, we can not only produce more pleasing melodies, but  significantly reduce unwanted behaviors and failure modes of the RNN, while maintaining information learned from data.", "pdf": "/pdf/c4f118ae6bf24cd110be68a6280bac447bdfd1df.pdf", "TL;DR": "RL Tuner is a method for refining an LSTM trained on data by using RL to impose desired behaviors, while maintaining good predictive properties learned from data.", "paperhash": "jaques|tuning_recurrent_neural_networks_with_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning", "Structured prediction", "Supervised Learning", "Applications"], "conflicts": ["media.mit.edu", "google.com", "cam.ac.uk"], "authors": ["Natasha Jaques", "Shixiang Gu", "Richard E. Turner", "Douglas Eck"], "authorids": ["jaquesn@mit.edu", "sg717@cam.ac.uk", "ret26@cam.ack.uk", "deck@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674646, "id": "ICLR.cc/2017/conference/-/paper229/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ8fyHceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper229/reviewers", "ICLR.cc/2017/conference/paper229/areachairs"], "cdate": 1485287674646}}}, {"tddate": null, "tmdate": 1478408527733, "tcdate": 1478408527727, "number": 1, "id": "S1_8KEheg", "invitation": "ICLR.cc/2017/conference/-/paper229/public/comment", "forum": "BJ8fyHceg", "replyto": "BJ8fyHceg", "signatures": ["~Aravind_Lakshminarayanan1"], "readers": ["everyone"], "writers": ["~Aravind_Lakshminarayanan1"], "content": {"title": "Nice work! Similar work exists for Dialog generation", "comment": "Nice work! There has been work on tuning RNNs that have been trained with supervised learning with additional \"engineered\" reward functions and using RL (policy gradients). Check out https://arxiv.org/pdf/1606.01541.pdf for a work on Dialog Generation where synthetic reward + policy gradients avoids regular responses like \"I don't know\". The Future Work section talks about this as something-to-be-done.. But there have been attempts to do so. \n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tuning Recurrent Neural Networks with Reinforcement Learning", "abstract": "The approach of training sequence models using supervised learning and next-step prediction  suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. We propose a novel sequence-learning approach in which we use a pre-trained Recurrent Neural Network (RNN) to supply part of the reward value in a Reinforcement Learning (RL) model. Thus, we can refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from KL control. We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note RNN is then refined using our method and rules of music theory. We show that by combining maximum likelihood (ML) and RL in this way, we can not only produce more pleasing melodies, but  significantly reduce unwanted behaviors and failure modes of the RNN, while maintaining information learned from data.", "pdf": "/pdf/c4f118ae6bf24cd110be68a6280bac447bdfd1df.pdf", "TL;DR": "RL Tuner is a method for refining an LSTM trained on data by using RL to impose desired behaviors, while maintaining good predictive properties learned from data.", "paperhash": "jaques|tuning_recurrent_neural_networks_with_reinforcement_learning", "keywords": ["Deep learning", "Reinforcement Learning", "Structured prediction", "Supervised Learning", "Applications"], "conflicts": ["media.mit.edu", "google.com", "cam.ac.uk"], "authors": ["Natasha Jaques", "Shixiang Gu", "Richard E. Turner", "Douglas Eck"], "authorids": ["jaquesn@mit.edu", "sg717@cam.ac.uk", "ret26@cam.ack.uk", "deck@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674646, "id": "ICLR.cc/2017/conference/-/paper229/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ8fyHceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper229/reviewers", "ICLR.cc/2017/conference/paper229/areachairs"], "cdate": 1485287674646}}}], "count": 21}