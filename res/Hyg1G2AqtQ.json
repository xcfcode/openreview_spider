{"notes": [{"id": "Hyg1G2AqtQ", "original": "rkl05pFGKX", "number": 1227, "cdate": 1538087942994, "ddate": null, "tcdate": 1538087942994, "tmdate": 1551245712553, "tddate": null, "forum": "Hyg1G2AqtQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Variance Reduction for Reinforcement Learning in Input-Driven Environments", "abstract": "We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with disturbances, and object tracking. Since the state dynamics and rewards depend on the input process, the state alone provides limited information for the expected future returns. Therefore, policy gradient methods with standard state-dependent baselines suffer high variance during training. We derive a bias-free, input-dependent baseline to reduce this variance, and analytically show its benefits over state-dependent baselines. We then propose a meta-learning approach to overcome the complexity of learning a baseline that depends on a long sequence of inputs. Our experimental results show that across environments from queuing systems, computer networks, and MuJoCo robotic locomotion, input-dependent baselines consistently improve training stability and result in better eventual policies.", "keywords": ["reinforcement learning", "policy gradient", "input-driven environments", "variance reduction", "baseline"], "authorids": ["hongzi@csail.mit.edu", "bjjvnkt@csail.mit.edu", "malte@csail.mit.edu", "alizadeh@csail.mit.edu"], "authors": ["Hongzi Mao", "Shaileshh Bojja Venkatakrishnan", "Malte Schwarzkopf", "Mohammad Alizadeh"], "TL;DR": "For environments dictated partially by external input processes, we derive an input-dependent baseline that provably reduces the variance for policy gradient methods and improves the policy performance in a wide range of RL tasks.", "pdf": "/pdf/5a974303c7c169a1c7f061bf56596cdf715ec0d9.pdf", "paperhash": "mao|variance_reduction_for_reinforcement_learning_in_inputdriven_environments", "_bibtex": "@inproceedings{\nmao2018variance,\ntitle={Variance Reduction for Reinforcement Learning in Input-Driven Environments},\nauthor={Hongzi Mao and Shaileshh Bojja Venkatakrishnan and Malte Schwarzkopf and Mohammad Alizadeh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyg1G2AqtQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "S1xnLMIgeV", "original": null, "number": 1, "cdate": 1544737364376, "ddate": null, "tcdate": 1544737364376, "tmdate": 1545354513982, "tddate": null, "forum": "Hyg1G2AqtQ", "replyto": "Hyg1G2AqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1227/Meta_Review", "content": {"metareview": "This paper proposes an input-dependent baseline function to reduce variance in policy gradient estimation without adding bias. The approach is novel and theoretically validated, and the experimental results are convincing. The authors addressed nearly all of the reviewer's concerns. I recommend acceptance.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "meta review"}, "signatures": ["ICLR.cc/2019/Conference/Paper1227/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1227/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Reduction for Reinforcement Learning in Input-Driven Environments", "abstract": "We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with disturbances, and object tracking. Since the state dynamics and rewards depend on the input process, the state alone provides limited information for the expected future returns. Therefore, policy gradient methods with standard state-dependent baselines suffer high variance during training. We derive a bias-free, input-dependent baseline to reduce this variance, and analytically show its benefits over state-dependent baselines. We then propose a meta-learning approach to overcome the complexity of learning a baseline that depends on a long sequence of inputs. Our experimental results show that across environments from queuing systems, computer networks, and MuJoCo robotic locomotion, input-dependent baselines consistently improve training stability and result in better eventual policies.", "keywords": ["reinforcement learning", "policy gradient", "input-driven environments", "variance reduction", "baseline"], "authorids": ["hongzi@csail.mit.edu", "bjjvnkt@csail.mit.edu", "malte@csail.mit.edu", "alizadeh@csail.mit.edu"], "authors": ["Hongzi Mao", "Shaileshh Bojja Venkatakrishnan", "Malte Schwarzkopf", "Mohammad Alizadeh"], "TL;DR": "For environments dictated partially by external input processes, we derive an input-dependent baseline that provably reduces the variance for policy gradient methods and improves the policy performance in a wide range of RL tasks.", "pdf": "/pdf/5a974303c7c169a1c7f061bf56596cdf715ec0d9.pdf", "paperhash": "mao|variance_reduction_for_reinforcement_learning_in_inputdriven_environments", "_bibtex": "@inproceedings{\nmao2018variance,\ntitle={Variance Reduction for Reinforcement Learning in Input-Driven Environments},\nauthor={Hongzi Mao and Shaileshh Bojja Venkatakrishnan and Malte Schwarzkopf and Mohammad Alizadeh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyg1G2AqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1227/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352916944, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hyg1G2AqtQ", "replyto": "Hyg1G2AqtQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1227/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1227/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1227/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352916944}}}, {"id": "Bkx4lJjByV", "original": null, "number": 13, "cdate": 1544036076001, "ddate": null, "tcdate": 1544036076001, "tmdate": 1544036076001, "tddate": null, "forum": "Hyg1G2AqtQ", "replyto": "BJehAVuXyN", "invitation": "ICLR.cc/2019/Conference/-/Paper1227/Official_Comment", "content": {"title": "Maintaining score", "comment": "The discussions have not change my opinion that this is an important and useful paper for practitioners applying RL to very noisy control settings."}, "signatures": ["ICLR.cc/2019/Conference/Paper1227/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1227/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1227/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Reduction for Reinforcement Learning in Input-Driven Environments", "abstract": "We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with disturbances, and object tracking. Since the state dynamics and rewards depend on the input process, the state alone provides limited information for the expected future returns. Therefore, policy gradient methods with standard state-dependent baselines suffer high variance during training. We derive a bias-free, input-dependent baseline to reduce this variance, and analytically show its benefits over state-dependent baselines. We then propose a meta-learning approach to overcome the complexity of learning a baseline that depends on a long sequence of inputs. Our experimental results show that across environments from queuing systems, computer networks, and MuJoCo robotic locomotion, input-dependent baselines consistently improve training stability and result in better eventual policies.", "keywords": ["reinforcement learning", "policy gradient", "input-driven environments", "variance reduction", "baseline"], "authorids": ["hongzi@csail.mit.edu", "bjjvnkt@csail.mit.edu", "malte@csail.mit.edu", "alizadeh@csail.mit.edu"], "authors": ["Hongzi Mao", "Shaileshh Bojja Venkatakrishnan", "Malte Schwarzkopf", "Mohammad Alizadeh"], "TL;DR": "For environments dictated partially by external input processes, we derive an input-dependent baseline that provably reduces the variance for policy gradient methods and improves the policy performance in a wide range of RL tasks.", "pdf": "/pdf/5a974303c7c169a1c7f061bf56596cdf715ec0d9.pdf", "paperhash": "mao|variance_reduction_for_reinforcement_learning_in_inputdriven_environments", "_bibtex": "@inproceedings{\nmao2018variance,\ntitle={Variance Reduction for Reinforcement Learning in Input-Driven Environments},\nauthor={Hongzi Mao and Shaileshh Bojja Venkatakrishnan and Malte Schwarzkopf and Mohammad Alizadeh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyg1G2AqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1227/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615728, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hyg1G2AqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1227/Authors", "ICLR.cc/2019/Conference/Paper1227/Reviewers", "ICLR.cc/2019/Conference/Paper1227/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1227/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1227/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1227/Authors|ICLR.cc/2019/Conference/Paper1227/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1227/Reviewers", "ICLR.cc/2019/Conference/Paper1227/Authors", "ICLR.cc/2019/Conference/Paper1227/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615728}}}, {"id": "BJx1GFr92X", "original": null, "number": 1, "cdate": 1541196038626, "ddate": null, "tcdate": 1541196038626, "tmdate": 1543956501466, "tddate": null, "forum": "Hyg1G2AqtQ", "replyto": "Hyg1G2AqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1227/Official_Review", "content": {"title": "Interesting research problem (input-driven MDPs), but I think they are missing the most interesting and practically relevant scenario.", "review": "\n\nSummary: This work considers the problem of learning in input-driven environments -- which are characterized by an addition stochastic variable z that can affect the dynamics of the environment and the associated reward the agent might see. The authors show how the PG theorem still applied for a input-aware critic and then they show that the best baseline one can use in conjecture with this critic is a input-dependent one. My main concerns are highlighted in points (3) and (4) in the detailed comments below. \n\nClarity: Generally it reads good, although I had to go back-and-forth between the main text and appendix several times to understand the experimental side. Even with the supplementary material, examples in Section 3 and Sections 6.2 could be improved in explanation and discussion.\n\nOriginality and Significance: Limited in this version, but could be improved significantly by something like point (3)&(4) in detailed comments. Fairly incremental extension of the PG (and TRPO) with the conditioning on the potentially (unobserved) input variables. The fact that a input-aware critic could benefit from a input-aware baseline is not that surprising. The fact that it reduces variance in the PG update is an interesting result; nevertheless I strongly feel the link or comparison needed is with the standard PG update. \n\nDisclaimer: I have not checked the proofs in the appendix.\n\nDetailed comments:\n\n1) On learning the input-dependent baselines: Generalising over context via a parametric functional approximation, like UVFAs [1] seems like a more natural first choice. Also these provide a zero-shot generalisation, bypassing the need for a burn-in period of the task. Can you comment on why something like that was not used at least as baseline?\n\n2) Motivating example. The exposition of this example lacks a bit of clarity and can use some more details as it is not a standard MDP example, so it\u2019s harder to grasp the complexity of this task or how standard methods would do on it and where would they struggle. I think it\u2019s meant to be an example of high variance but the performance in Figure 2 seems to suggest this is actually something manageable for something like A2C. It is also not clear in this example how the comparison was done. For instance, are the value functions used, input-dependent? Is the policy input aware? \n\n3) Input-driven MDP. Case 1/Case 2 : As noted by the authors, in case 1 if both s_t and z_t are observed, this somewhat uninteresting as it recovers a particular structured state variable of a normal MDP. I would argue that the more interesting case here, is where only s_t is observed and z_t is hidden, at least in acting. This might still be information available in hindsight and used in training, but won\u2019t be available \u2018online\u2019 -- similar to slack variable, or privileged information at training time.  And in this case it\u2019s not clear to me if this would still result in a variance reduction in the policy update. Case 2 has some of that flavour, but restricts z_t to an iid process. Again, I think the more interesting case is not treated or discussed at all and in my opinion, this might add the best value to this work.\n  \n4) Now, as mentioned above the interesting case, at least in my opinion, is when z is hidden. From the formulae(eq. (4),(5)), it seems to be that the policy is unaware of the input variables. Thus we are training a policy that should be able to deal with a distribution of inputs z. How does this compare with the normal PG update, that would consider a critic averaged over z-s and a z-independent baseline? Is the variance of the proposed update always smaller than that of the standard PG update when learning a policy that is unaware of z? \n\nReferences:\n[1] Schaul, T., Horgan, D., Gregor, K. and Silver, D., 2015, June. Universal value function approximators. In International Conference on Machine Learning (pp. 1312-1320).\n\n[POST-rebuttal] I've read the author's response and it clarified some of the concerns. I'm increase the score accordingly.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1227/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Variance Reduction for Reinforcement Learning in Input-Driven Environments", "abstract": "We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with disturbances, and object tracking. Since the state dynamics and rewards depend on the input process, the state alone provides limited information for the expected future returns. Therefore, policy gradient methods with standard state-dependent baselines suffer high variance during training. We derive a bias-free, input-dependent baseline to reduce this variance, and analytically show its benefits over state-dependent baselines. We then propose a meta-learning approach to overcome the complexity of learning a baseline that depends on a long sequence of inputs. Our experimental results show that across environments from queuing systems, computer networks, and MuJoCo robotic locomotion, input-dependent baselines consistently improve training stability and result in better eventual policies.", "keywords": ["reinforcement learning", "policy gradient", "input-driven environments", "variance reduction", "baseline"], "authorids": ["hongzi@csail.mit.edu", "bjjvnkt@csail.mit.edu", "malte@csail.mit.edu", "alizadeh@csail.mit.edu"], "authors": ["Hongzi Mao", "Shaileshh Bojja Venkatakrishnan", "Malte Schwarzkopf", "Mohammad Alizadeh"], "TL;DR": "For environments dictated partially by external input processes, we derive an input-dependent baseline that provably reduces the variance for policy gradient methods and improves the policy performance in a wide range of RL tasks.", "pdf": "/pdf/5a974303c7c169a1c7f061bf56596cdf715ec0d9.pdf", "paperhash": "mao|variance_reduction_for_reinforcement_learning_in_inputdriven_environments", "_bibtex": "@inproceedings{\nmao2018variance,\ntitle={Variance Reduction for Reinforcement Learning in Input-Driven Environments},\nauthor={Hongzi Mao and Shaileshh Bojja Venkatakrishnan and Malte Schwarzkopf and Mohammad Alizadeh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyg1G2AqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1227/Official_Review", "cdate": 1542234276252, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hyg1G2AqtQ", "replyto": "Hyg1G2AqtQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1227/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335900734, "tmdate": 1552335900734, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1227/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1e9N1YX6m", "original": null, "number": 3, "cdate": 1541799730042, "ddate": null, "tcdate": 1541799730042, "tmdate": 1543931460778, "tddate": null, "forum": "Hyg1G2AqtQ", "replyto": "Hyg1G2AqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1227/Official_Review", "content": {"title": "Interesting premise, needs more clarity/comparisons", "review": "\nIntroduction: \n\u201cSince the state dynamics and rewards depend on the input process\u201d -> why do the rewards depend on the input process conditioned on the state? \n\nDoes the scenario being considered basically involve any scenario with stochastic dynamics? Or is the fact that the disturbances may come from a stateful process what makes this distinct?\n\nif the input sequence following the action -> vague, would help if this would just be written a bit more clearly. \n\nIs just the baseline input dependent or does the policy need to be input dependent as well? From later reading, this point is still quite confusing. One line says \u201cAt time t, the policy only depends only on (st, zt).\u201d. Another line says that the policy is pi_theta(a|s), with no mention of z. I\u2019m pretty confused by the consistency here. This is also important in the proof of Lemma 1, because P(a|s,z) = pi_theta(a|s). Please clarify this.\n\nSection 4:\n Is the IID version of Figure 3 basically the same as stochastic dynamics? (Case 2)\n\nSection 4.1\n\u201cIn input-driven MDPs, the standard input-agnostic baseline is ineffective at reducing variance\u201d -> can you give some more intuition/proof as to why. \n\nIn Lemma 2, how come the Q function is dependent on z, but the policy is only dependent on s (not even the current and past z\u2019s). \n\nI think the proof of theorem 1 should be included in the main paper rather than unnecessary details about policy gradient. \n\nTheorem 1 and theorem 2 are really some of the most important parts of the paper, and they deserve a more thorough discussion besides the 2 lines that are in there right now. \n\n\nAlgorithm 1 -> should it be eqn 4?\n\nThe meta-algorithm provided in Section 5 is well motivated and well described. An experimental result including what happens with LSTM baselines would be very helpful. \n\nOne question is whether it is actually possible to know what the z\u2019s are at different steps? In some cases these might be latent and hard to infer?\n\nCan you compare to Clavera et al 2018? It seems like it might be a relevant comparison. \n\nThe difference between MAML and the 10 value network seems quite marginal. Can the authors discuss why this is? And when we would expect to see a bigger difference. \n\nRelated work: Another relevant piece of work\nMeta-Learning Priors for Efficient Online Bayesian Regression\n\nMajor todos:\n1. Improve clarity of what z's are observed, which are not and whether the policy is dependent on these or not. \n2. Compare with other prior work such as Clavera et al, Harrison et al. \n3. Add more naive baselines such as training an LSTM, etc. \n4. Provide more analysis of the meta-learning component, how much does it actually help.\n\nOverall impression: \u2028I think this paper covers an interesting problem, and proposes a simple, straightforward approach conditioning the baseline and the critic on the input process. What bothers me in the current version of the paper is the lack of clarity about the observability of z, where it comes from and also some lack of comparisons with other prior methods. I think these would make the paper stronger.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1227/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Variance Reduction for Reinforcement Learning in Input-Driven Environments", "abstract": "We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with disturbances, and object tracking. Since the state dynamics and rewards depend on the input process, the state alone provides limited information for the expected future returns. Therefore, policy gradient methods with standard state-dependent baselines suffer high variance during training. We derive a bias-free, input-dependent baseline to reduce this variance, and analytically show its benefits over state-dependent baselines. We then propose a meta-learning approach to overcome the complexity of learning a baseline that depends on a long sequence of inputs. Our experimental results show that across environments from queuing systems, computer networks, and MuJoCo robotic locomotion, input-dependent baselines consistently improve training stability and result in better eventual policies.", "keywords": ["reinforcement learning", "policy gradient", "input-driven environments", "variance reduction", "baseline"], "authorids": ["hongzi@csail.mit.edu", "bjjvnkt@csail.mit.edu", "malte@csail.mit.edu", "alizadeh@csail.mit.edu"], "authors": ["Hongzi Mao", "Shaileshh Bojja Venkatakrishnan", "Malte Schwarzkopf", "Mohammad Alizadeh"], "TL;DR": "For environments dictated partially by external input processes, we derive an input-dependent baseline that provably reduces the variance for policy gradient methods and improves the policy performance in a wide range of RL tasks.", "pdf": "/pdf/5a974303c7c169a1c7f061bf56596cdf715ec0d9.pdf", "paperhash": "mao|variance_reduction_for_reinforcement_learning_in_inputdriven_environments", "_bibtex": "@inproceedings{\nmao2018variance,\ntitle={Variance Reduction for Reinforcement Learning in Input-Driven Environments},\nauthor={Hongzi Mao and Shaileshh Bojja Venkatakrishnan and Malte Schwarzkopf and Mohammad Alizadeh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyg1G2AqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1227/Official_Review", "cdate": 1542234276252, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hyg1G2AqtQ", "replyto": "Hyg1G2AqtQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1227/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335900734, "tmdate": 1552335900734, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1227/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HklKcMENAm", "original": null, "number": 8, "cdate": 1542894225197, "ddate": null, "tcdate": 1542894225197, "tmdate": 1542894621282, "tddate": null, "forum": "Hyg1G2AqtQ", "replyto": "Hyg1G2AqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1227/Official_Comment", "content": {"title": "Paper update", "comment": "We again thank all reviewers for their comments, and have updated our paper accordingly.\n\nSpecifically, the major changes are:\n   - In \u00a74.1, we improved the clarity of our notations by explicitly defining the observation \\omega_t at each time t. We used \\omega_t instead o_t because the letter o is visually too similar to a. We updated our theorems and proofs using this notation.\n   - We extended the case 2 of input-driven MDP to include the POMDP case (Figure 3b), and have showed all our derivation and conclusions apply. \n   - We added a comparison to the meta-policy optimization approach (Clavera et al. 2018) in Appendix N.\n   - In addition to mentioning our findings with LSTM in \u00a75, we also added the corresponding learning curves in appendix G.\n   - We updated our motivating example (\u00a73) to give a better intuition.\n   - We shortened the policy gradients description in the introduction and background sections, and moved the proof of theorem 1 into the main text in \u00a74.1.\n   - In \u00a75, we added a discussion of when we expect the gain of MAML to further exceed that of the multi-value-network approach.\n\nPlease let us know if you have further comments. Thanks!\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1227/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1227/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1227/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Reduction for Reinforcement Learning in Input-Driven Environments", "abstract": "We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with disturbances, and object tracking. Since the state dynamics and rewards depend on the input process, the state alone provides limited information for the expected future returns. Therefore, policy gradient methods with standard state-dependent baselines suffer high variance during training. We derive a bias-free, input-dependent baseline to reduce this variance, and analytically show its benefits over state-dependent baselines. We then propose a meta-learning approach to overcome the complexity of learning a baseline that depends on a long sequence of inputs. Our experimental results show that across environments from queuing systems, computer networks, and MuJoCo robotic locomotion, input-dependent baselines consistently improve training stability and result in better eventual policies.", "keywords": ["reinforcement learning", "policy gradient", "input-driven environments", "variance reduction", "baseline"], "authorids": ["hongzi@csail.mit.edu", "bjjvnkt@csail.mit.edu", "malte@csail.mit.edu", "alizadeh@csail.mit.edu"], "authors": ["Hongzi Mao", "Shaileshh Bojja Venkatakrishnan", "Malte Schwarzkopf", "Mohammad Alizadeh"], "TL;DR": "For environments dictated partially by external input processes, we derive an input-dependent baseline that provably reduces the variance for policy gradient methods and improves the policy performance in a wide range of RL tasks.", "pdf": "/pdf/5a974303c7c169a1c7f061bf56596cdf715ec0d9.pdf", "paperhash": "mao|variance_reduction_for_reinforcement_learning_in_inputdriven_environments", "_bibtex": "@inproceedings{\nmao2018variance,\ntitle={Variance Reduction for Reinforcement Learning in Input-Driven Environments},\nauthor={Hongzi Mao and Shaileshh Bojja Venkatakrishnan and Malte Schwarzkopf and Mohammad Alizadeh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyg1G2AqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1227/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615728, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hyg1G2AqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1227/Authors", "ICLR.cc/2019/Conference/Paper1227/Reviewers", "ICLR.cc/2019/Conference/Paper1227/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1227/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1227/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1227/Authors|ICLR.cc/2019/Conference/Paper1227/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1227/Reviewers", "ICLR.cc/2019/Conference/Paper1227/Authors", "ICLR.cc/2019/Conference/Paper1227/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615728}}}, {"id": "ByxufjGtaX", "original": null, "number": 5, "cdate": 1542167311611, "ddate": null, "tcdate": 1542167311611, "tmdate": 1542221036257, "tddate": null, "forum": "Hyg1G2AqtQ", "replyto": "SylcuqGYp7", "invitation": "ICLR.cc/2019/Conference/-/Paper1227/Official_Comment", "content": {"title": "Responses to other questions", "comment": "\n-- Why do the rewards depend on the input process conditioned on the state? \n\nTo clarify, by \u201cstate dynamics and rewards depend on the input process,\u201d we mean that the input process can affect the rewards because it affects the state transitions. However, our model indeed covers the general case, in which the reward might depend on both the state and the input. For example, consider a robotics task in which the reward is the speed of the robot, the state is the current position of the robot\u2019s joints,   and the input is an external force applied to the robot at each step. The speed of the robot (reward) depends on the force (input) even with knowledge of its current position (state).   \n\n-- What makes the input process we considered distinct from any stochastic dynamics?\n\nThe main distinction here is that the input process must be \u201cexogenous,\u201d i.e. it doesn\u2019t depend on the state and actions; see the graphical models in Figure 3. This property is necessary for the input-dependent baseline to not introduce bias. \n\n-- A strong action could end up with a lower-than-average return if the input sequence following the action is unfavorable -> vague\n\nThis sentence was trying to give an intuition for why the variance in reward caused by the input process can confuse a policy gradient algorithm. We will rephrase the sentence and explain this better. We will also provide more intuition about this point in Section 3. \n\nConsider the load balancing example in Section 3. The return (total reward) for an action depends on the job arrival sequence that follows that action. For example, if the arrivals consist of a burst of large jobs, the reward (negative number of jobs in the system) will be poor, regardless of the action. We will add this intuition to Section 3. \n\n-- Is just the baseline input dependent or does the policy need to be input dependent as well? \n\nThe baseline depends on the sequence of input values z_{t:\\infty}, but the policy can only depend on the input observed at the current step t.  Note that the policy cannot depend on the future input values, since at time t, the agent has no way of knowing z_{t+1,\\infty}. \n\n-- \u201cIn input-driven MDPs, the standard input-agnostic baseline is ineffective at reducing variance\u201d -> can you give some more intuition/proof as to why.\n\nAs mentioned above, we will add more intuition for this to Section 3. \n\n-- More discussions about theorem 1 and 2.\n\nThanks for the suggestion! We will trim the discussion of policy gradient and include the proof of theorem 1.\n\n-- Algorithm 1 should use eqn 4.\n\nYes, it is more appropriate to refer to Equation 4 in Algorithm 1. We will use this.\n\n-- Is it possible to know z at each step? What if z is not observable and hard to infer\n\nIn many applications, the input process is naturally observable to the agent. For example, in most computer systems applications, the inputs to the environments (e.g., network bandwidth, workload) are measured or readily observed. However, even if the agent does not observe the input at each step, our proposed approach (multi-value-network and meta-learning) can still work as long as we can repeat the same input sequence during training. As discussed in Section 5, this can be done with a simulator (e.g., control the wind in MuJoCo simulator) or by repeating input sequences (e.g., repeat the same workload for a load balancing agent) in an actual system. For future work, we think that investigating efficient architectures for input-dependent baselines for cases where the input process cannot be controlled in training is an interesting direction.\n\n-- Meta Learning Priors for Efficient Online Bayesian Regression\n\nThank you for the suggestion. This is a relevant piece of work on applying meta learning for faster adaptation of GP regression. We will add it in the related work session.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1227/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1227/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1227/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Reduction for Reinforcement Learning in Input-Driven Environments", "abstract": "We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with disturbances, and object tracking. Since the state dynamics and rewards depend on the input process, the state alone provides limited information for the expected future returns. Therefore, policy gradient methods with standard state-dependent baselines suffer high variance during training. We derive a bias-free, input-dependent baseline to reduce this variance, and analytically show its benefits over state-dependent baselines. We then propose a meta-learning approach to overcome the complexity of learning a baseline that depends on a long sequence of inputs. Our experimental results show that across environments from queuing systems, computer networks, and MuJoCo robotic locomotion, input-dependent baselines consistently improve training stability and result in better eventual policies.", "keywords": ["reinforcement learning", "policy gradient", "input-driven environments", "variance reduction", "baseline"], "authorids": ["hongzi@csail.mit.edu", "bjjvnkt@csail.mit.edu", "malte@csail.mit.edu", "alizadeh@csail.mit.edu"], "authors": ["Hongzi Mao", "Shaileshh Bojja Venkatakrishnan", "Malte Schwarzkopf", "Mohammad Alizadeh"], "TL;DR": "For environments dictated partially by external input processes, we derive an input-dependent baseline that provably reduces the variance for policy gradient methods and improves the policy performance in a wide range of RL tasks.", "pdf": "/pdf/5a974303c7c169a1c7f061bf56596cdf715ec0d9.pdf", "paperhash": "mao|variance_reduction_for_reinforcement_learning_in_inputdriven_environments", "_bibtex": "@inproceedings{\nmao2018variance,\ntitle={Variance Reduction for Reinforcement Learning in Input-Driven Environments},\nauthor={Hongzi Mao and Shaileshh Bojja Venkatakrishnan and Malte Schwarzkopf and Mohammad Alizadeh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyg1G2AqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1227/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615728, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hyg1G2AqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1227/Authors", "ICLR.cc/2019/Conference/Paper1227/Reviewers", "ICLR.cc/2019/Conference/Paper1227/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1227/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1227/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1227/Authors|ICLR.cc/2019/Conference/Paper1227/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1227/Reviewers", "ICLR.cc/2019/Conference/Paper1227/Authors", "ICLR.cc/2019/Conference/Paper1227/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615728}}}, {"id": "SylCvSzKaX", "original": null, "number": 2, "cdate": 1542165861997, "ddate": null, "tcdate": 1542165861997, "tmdate": 1542219093190, "tddate": null, "forum": "Hyg1G2AqtQ", "replyto": "BJx1GFr92X", "invitation": "ICLR.cc/2019/Conference/-/Paper1227/Official_Comment", "content": {"title": "Author response", "comment": "Thank you for the insightful comments!\n\nRegarding these comments:\n\n1) UVFAs predict values based on specific goals. These methods require taking \u201cgoal embedding\u201d explicitly as input. In our formulation of input driven environment, however, there aren\u2019t really different goals in each task. Nonetheless, one can still use similar idea to take the exogenous sequence as an explicit input in the value function, using recurrent neural network structures such as LSTM. We actually did this and reported our findings in the paper, in the beginning of Section 5: \u201cA natural approach to train such baselines is to use models that operate on sequences (e.g., LSTMs). However, learning a sequential mapping in a high-dimensional space can be expensive. We considered an LSTM approach but ruled it out when initial experiments showed that it requires orders of magnitude more data to train than conventional baselines for our environments.\u201d We intend to add an experiment showing the learning curve with an LSTM approach to the appendix.\n\n2) The point of this example is to show that the variance from the input process can negatively affect the policy, even for an extremely simple task. In this 2-server load balance task, the agent should just learn the simple optimal policy of joining the shortest queue (visualized in Figure 2(c) left). However, the variance in the input sequence makes the PG unable to converge to the optimal. Here, we compared the vanilla A2C with the standard state-only baseline to that with the input-dependent baseline. It is clear that vanilla A2C performs suboptimally (Figure 2(b) right); and this is due to the significant difference in the PG variance in different baselines (Figure 2(b) left, notice the log scale). \n\nThe reason that vanilla A2C is ineffective in this example is that the return (total reward) for an action depends on the job arrival sequence that follows that action. For example, if the arrivals consist of a burst of large jobs, the reward (negative number of jobs in the system) will be poor, regardless of the action. We will expand the discussion in Section 3 to provide more details and intuition. \n\nAbout the input to the baseline and policy: the input-dependent baseline takes state s_t and the entire future input process z_{t:\\infty} as input; the state-only baseline only takes s_t as input; in both cases, the policy network takes s_t and z_t (only at time t) as input.\n\n3 and 4) Thank you for this interesting comment. We focused on the two cases in Figure 3 mainly because they result in fully observable MDPs, and in many applications of interest, the input is readily observable. However, the scenario in which the input z_t is not observed is indeed also interesting. This case results in a POMDP.      \n\nInput-dependent baselines reduce variance in the POMDP case as well. Our results (e.g., Theorems 1 and 2)  also apply to this setting. In fact, in the POMDP case, the input process does not even need to be Markov; it can be any general stochastic process that does not depend on the states and actions. \n\nIntuitively, the reason is that much of the variance in PG in input-driven environments is caused by the variance in the input sequence that follows an action. For example, in the windy walker environment (Figure 1c), it is the entire sequence of wind after step t that affects the total reward, not just the wind observation at time t. As a result, regardless of whether or not the input is observed at each step t, using the entire input sequence in the baseline reduces variance. \n\nInterestingly, the HalfCheetah with floating tiles environment (Figure 1d) is actually a POMDP---the agent only observes the torques of the cheetah\u2019s body but not the buoyancy of the tiles. As shown in Figure 4 (middle), our technique helped reduce variance and improve PG performance. Also, we re-ran our experiments on the Walker2d with wind environment without providing z (the wind) to the policy. The results show that our input-dependent baseline improves the policy performance similar to the case where z is observed. We will shortly add this result to the paper. \n\nIn summary, we are making the following changes to the paper. We will add a case for POMDP to Figure 3, and discuss the derivation for the POMDP (which is almost identical to the MDP case). We will also include the POMDP version of Walker2d with wind result. \n\nWe also realize that the notation was confusing. As mentioned in the 2nd paragraph of page 5, we were using s_t to denote the tuple (s_t, z_t) in the derivations. We will improve the notation by explicitly defining the observed signal, o_t, used by the policy in each case. For the MDP case, o_t = (s_t, z_t). For the POMDP case, o_t = s_t. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1227/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1227/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1227/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Reduction for Reinforcement Learning in Input-Driven Environments", "abstract": "We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with disturbances, and object tracking. Since the state dynamics and rewards depend on the input process, the state alone provides limited information for the expected future returns. Therefore, policy gradient methods with standard state-dependent baselines suffer high variance during training. We derive a bias-free, input-dependent baseline to reduce this variance, and analytically show its benefits over state-dependent baselines. We then propose a meta-learning approach to overcome the complexity of learning a baseline that depends on a long sequence of inputs. Our experimental results show that across environments from queuing systems, computer networks, and MuJoCo robotic locomotion, input-dependent baselines consistently improve training stability and result in better eventual policies.", "keywords": ["reinforcement learning", "policy gradient", "input-driven environments", "variance reduction", "baseline"], "authorids": ["hongzi@csail.mit.edu", "bjjvnkt@csail.mit.edu", "malte@csail.mit.edu", "alizadeh@csail.mit.edu"], "authors": ["Hongzi Mao", "Shaileshh Bojja Venkatakrishnan", "Malte Schwarzkopf", "Mohammad Alizadeh"], "TL;DR": "For environments dictated partially by external input processes, we derive an input-dependent baseline that provably reduces the variance for policy gradient methods and improves the policy performance in a wide range of RL tasks.", "pdf": "/pdf/5a974303c7c169a1c7f061bf56596cdf715ec0d9.pdf", "paperhash": "mao|variance_reduction_for_reinforcement_learning_in_inputdriven_environments", "_bibtex": "@inproceedings{\nmao2018variance,\ntitle={Variance Reduction for Reinforcement Learning in Input-Driven Environments},\nauthor={Hongzi Mao and Shaileshh Bojja Venkatakrishnan and Malte Schwarzkopf and Mohammad Alizadeh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyg1G2AqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1227/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615728, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hyg1G2AqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1227/Authors", "ICLR.cc/2019/Conference/Paper1227/Reviewers", "ICLR.cc/2019/Conference/Paper1227/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1227/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1227/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1227/Authors|ICLR.cc/2019/Conference/Paper1227/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1227/Reviewers", "ICLR.cc/2019/Conference/Paper1227/Authors", "ICLR.cc/2019/Conference/Paper1227/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615728}}}, {"id": "SylcuqGYp7", "original": null, "number": 3, "cdate": 1542167153572, "ddate": null, "tcdate": 1542167153572, "tmdate": 1542219006718, "tddate": null, "forum": "Hyg1G2AqtQ", "replyto": "S1e9N1YX6m", "invitation": "ICLR.cc/2019/Conference/-/Paper1227/Official_Comment", "content": {"title": "Author response", "comment": "Thank you for the constructive comments!\n\nWe first address the major comments and then respond to the detailed questions in a separated comment.\n\n1. [What is observed?] During policy inference at each MDP step t, the agent observes s_t and z_t (the current value of the input process). Therefore the policy can depend on the current observed value of the input z_t, but not on the future input sequence z_{t:\\infty} (which has not yet happened). At training time, however, the baseline computation for step t depends on the entire future sequence z_{t:\\infty}. As explained in the beginning of Section 4.1, this is possible because the entire input sequence is known at training time. \n\nWe realize that the notation was confusing. As mentioned in the 2nd paragraph of page 5, we use s_t to denote the tuple (s_t,z_t) for the derivations. We will improve the notation by explicitly defining the observed signal, o_t = (s_t, z_t), which the policy takes as input at each step t.\n\n2. [Additional comparisons to prior work] Policy adaptation approaches like Clavera et al. learn a \u201cmeta-policy\u201d that can be quickly adapted for different environments. By contrast, our goal is to learn a single policy that performs well in the presence of a stochastic input process. In other words, we are improving policy optimization itself in environments with stochastic inputs. We do not consider transfer of a policy trained for one environment to another. In terms of training a common policy, our work is more related to RARL (Pinto et al.), which we discuss and compare with in Appendix L.\n\nIt is worth noting that approaches like Clavera et. al. are well-suited to handling model discrepancy between training and testing. However, in our setting, there isn\u2019t any model discrepancy. In particular, the distribution of the input process is the same during training and testing. Nonetheless, our work shows that standard policy gradient methods have difficulty in input-driven environments, and input-dependent baselines can substantially improve performance.\n\nTherefore our work is orthogonal and complementary to policy adaptation approaches. Since some of these methods require a policy optimization step (e.g., Section 4.2 of Clavera et al. 2018), our input-dependent baseline can help these methods by reducing variance during training. Appendix L shows an example of such improvements for RARL. We will try to also add an example for a policy adaptation approach. \n             \n3. [The LSTM method for learning input-dependent baselines] LSTM suffers from unnecessarily high complexity in training. In our experiments, we considered an LSTM approach but ruled it out when initial experiments showed that it requires orders of magnitude more data to train than conventional baselines for our environments (cf. beginning of Section 5). We will add the learning curves with LSTM baseline in the appendix.\n\n4. [The meta-learning baseline] The actual performance gain for a meta-learned baseline over a multi-value-network is environment-specific. Conceptually, the multi-value-network falls short when the task requires training with a large number of input instantiations to generalize to new input instances. We have not analyzed how policy quality varies with the number of input instantiations considered during training. However, we expect that this depends on a variety of factors, such as the distribution of the input process (e.g., from a large deviations standpoint); the time horizon of the problem; the relative magnitude of the variance due to the input process compared to other sources of randomness (e.g., actions). The advantage of the meta-learning approach compared to the multi-value network approach is that we can train with an unbounded number of input instantiations. We will add this discussion to Section 5."}, "signatures": ["ICLR.cc/2019/Conference/Paper1227/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1227/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1227/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Reduction for Reinforcement Learning in Input-Driven Environments", "abstract": "We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with disturbances, and object tracking. Since the state dynamics and rewards depend on the input process, the state alone provides limited information for the expected future returns. Therefore, policy gradient methods with standard state-dependent baselines suffer high variance during training. We derive a bias-free, input-dependent baseline to reduce this variance, and analytically show its benefits over state-dependent baselines. We then propose a meta-learning approach to overcome the complexity of learning a baseline that depends on a long sequence of inputs. Our experimental results show that across environments from queuing systems, computer networks, and MuJoCo robotic locomotion, input-dependent baselines consistently improve training stability and result in better eventual policies.", "keywords": ["reinforcement learning", "policy gradient", "input-driven environments", "variance reduction", "baseline"], "authorids": ["hongzi@csail.mit.edu", "bjjvnkt@csail.mit.edu", "malte@csail.mit.edu", "alizadeh@csail.mit.edu"], "authors": ["Hongzi Mao", "Shaileshh Bojja Venkatakrishnan", "Malte Schwarzkopf", "Mohammad Alizadeh"], "TL;DR": "For environments dictated partially by external input processes, we derive an input-dependent baseline that provably reduces the variance for policy gradient methods and improves the policy performance in a wide range of RL tasks.", "pdf": "/pdf/5a974303c7c169a1c7f061bf56596cdf715ec0d9.pdf", "paperhash": "mao|variance_reduction_for_reinforcement_learning_in_inputdriven_environments", "_bibtex": "@inproceedings{\nmao2018variance,\ntitle={Variance Reduction for Reinforcement Learning in Input-Driven Environments},\nauthor={Hongzi Mao and Shaileshh Bojja Venkatakrishnan and Malte Schwarzkopf and Mohammad Alizadeh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyg1G2AqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1227/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615728, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hyg1G2AqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1227/Authors", "ICLR.cc/2019/Conference/Paper1227/Reviewers", "ICLR.cc/2019/Conference/Paper1227/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1227/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1227/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1227/Authors|ICLR.cc/2019/Conference/Paper1227/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1227/Reviewers", "ICLR.cc/2019/Conference/Paper1227/Authors", "ICLR.cc/2019/Conference/Paper1227/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615728}}}, {"id": "H1xPbNztp7", "original": null, "number": 1, "cdate": 1542165503047, "ddate": null, "tcdate": 1542165503047, "tmdate": 1542165503047, "tddate": null, "forum": "Hyg1G2AqtQ", "replyto": "ryxDVNFc2X", "invitation": "ICLR.cc/2019/Conference/-/Paper1227/Official_Comment", "content": {"title": "Author response", "comment": "We appreciate your encouraging comments!\n\nWe agree that the traffic control environment is a perfect fit for the techniques we proposed. Thanks for the suggestions and the pointers to the existing simulators---we will mention these potential applications in the introduction/conclusions. \n\nIn our submission, we moved the proofs to appendix due to space constraints. We will trim down the text of the facts about PG methods to clear up rooms for the proof of Theorem 1.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1227/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1227/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1227/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Reduction for Reinforcement Learning in Input-Driven Environments", "abstract": "We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with disturbances, and object tracking. Since the state dynamics and rewards depend on the input process, the state alone provides limited information for the expected future returns. Therefore, policy gradient methods with standard state-dependent baselines suffer high variance during training. We derive a bias-free, input-dependent baseline to reduce this variance, and analytically show its benefits over state-dependent baselines. We then propose a meta-learning approach to overcome the complexity of learning a baseline that depends on a long sequence of inputs. Our experimental results show that across environments from queuing systems, computer networks, and MuJoCo robotic locomotion, input-dependent baselines consistently improve training stability and result in better eventual policies.", "keywords": ["reinforcement learning", "policy gradient", "input-driven environments", "variance reduction", "baseline"], "authorids": ["hongzi@csail.mit.edu", "bjjvnkt@csail.mit.edu", "malte@csail.mit.edu", "alizadeh@csail.mit.edu"], "authors": ["Hongzi Mao", "Shaileshh Bojja Venkatakrishnan", "Malte Schwarzkopf", "Mohammad Alizadeh"], "TL;DR": "For environments dictated partially by external input processes, we derive an input-dependent baseline that provably reduces the variance for policy gradient methods and improves the policy performance in a wide range of RL tasks.", "pdf": "/pdf/5a974303c7c169a1c7f061bf56596cdf715ec0d9.pdf", "paperhash": "mao|variance_reduction_for_reinforcement_learning_in_inputdriven_environments", "_bibtex": "@inproceedings{\nmao2018variance,\ntitle={Variance Reduction for Reinforcement Learning in Input-Driven Environments},\nauthor={Hongzi Mao and Shaileshh Bojja Venkatakrishnan and Malte Schwarzkopf and Mohammad Alizadeh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyg1G2AqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1227/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615728, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hyg1G2AqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1227/Authors", "ICLR.cc/2019/Conference/Paper1227/Reviewers", "ICLR.cc/2019/Conference/Paper1227/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1227/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1227/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1227/Authors|ICLR.cc/2019/Conference/Paper1227/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1227/Reviewers", "ICLR.cc/2019/Conference/Paper1227/Authors", "ICLR.cc/2019/Conference/Paper1227/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615728}}}, {"id": "ryxDVNFc2X", "original": null, "number": 2, "cdate": 1541211183213, "ddate": null, "tcdate": 1541211183213, "tmdate": 1541533314114, "tddate": null, "forum": "Hyg1G2AqtQ", "replyto": "Hyg1G2AqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1227/Official_Review", "content": {"title": "Strong paper for environment in which outcomes are strongly influenced by exogenous factors", "review": "The paper introduces and develops the notion of input-dependent baselines for Policy Gradient Methods in RL.\n\nThe insight developed in the paper is clear: in environments such as data centers or outside settings external factors (traffic load or wind) constitute high magnitude perturbations that ultimately strongly change rewards.\nLearning an input-dependent baseline function helps clear out the variance created by such perturbations in a way that does not bias the policy gradient estimate (the authors provide a theoretical proof of that fact).\n\nThe authors propose different methods to train the input dependent baseline function:\n   o) a multi-value network based approach\n   o) a meta-learning approach\nThe performance of these two methods is compared on simulated robotic locomotion tasks as well as a load balancing and video bitrate adaptation task.\nThe input dependent baseline strongly outperforms the state dependent baseline in both cases.\n\nStrengths:\n   o) The paper is well written\n   o) The method is novel and simple while strongly reducing variance in Monte Carlo policy gradient estimates without inducing bias.\n   o) The experiment evidence is strong.\n\nWeaknesses:\n   o) Vehicular traffic has been the subject of recent development through deep reinforcement learning (e.g. https://arxiv.org/pdf/1701.08832.pdf and https://arxiv.org/pdf/1710.05465.pdf). In this particular setting exogenous noise (demand for throughput and accidents) could strongly benefit from input dependent baselines. I believe the authors should mention such potential applications of the method which may have major societal impact.\n   o) There is a lot of space dedicated to well know facts about policy gradient methods. I believe it could be more impactful to put the proof of Theorem 1 in the main body of the paper as it is clearly a key theoretical property.", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1227/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Reduction for Reinforcement Learning in Input-Driven Environments", "abstract": "We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with disturbances, and object tracking. Since the state dynamics and rewards depend on the input process, the state alone provides limited information for the expected future returns. Therefore, policy gradient methods with standard state-dependent baselines suffer high variance during training. We derive a bias-free, input-dependent baseline to reduce this variance, and analytically show its benefits over state-dependent baselines. We then propose a meta-learning approach to overcome the complexity of learning a baseline that depends on a long sequence of inputs. Our experimental results show that across environments from queuing systems, computer networks, and MuJoCo robotic locomotion, input-dependent baselines consistently improve training stability and result in better eventual policies.", "keywords": ["reinforcement learning", "policy gradient", "input-driven environments", "variance reduction", "baseline"], "authorids": ["hongzi@csail.mit.edu", "bjjvnkt@csail.mit.edu", "malte@csail.mit.edu", "alizadeh@csail.mit.edu"], "authors": ["Hongzi Mao", "Shaileshh Bojja Venkatakrishnan", "Malte Schwarzkopf", "Mohammad Alizadeh"], "TL;DR": "For environments dictated partially by external input processes, we derive an input-dependent baseline that provably reduces the variance for policy gradient methods and improves the policy performance in a wide range of RL tasks.", "pdf": "/pdf/5a974303c7c169a1c7f061bf56596cdf715ec0d9.pdf", "paperhash": "mao|variance_reduction_for_reinforcement_learning_in_inputdriven_environments", "_bibtex": "@inproceedings{\nmao2018variance,\ntitle={Variance Reduction for Reinforcement Learning in Input-Driven Environments},\nauthor={Hongzi Mao and Shaileshh Bojja Venkatakrishnan and Malte Schwarzkopf and Mohammad Alizadeh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyg1G2AqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1227/Official_Review", "cdate": 1542234276252, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hyg1G2AqtQ", "replyto": "Hyg1G2AqtQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1227/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335900734, "tmdate": 1552335900734, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1227/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 11}