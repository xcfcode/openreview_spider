{"notes": [{"id": "HkxAS6VFDB", "original": "B1xK9jPDPr", "number": 542, "cdate": 1569439045823, "ddate": null, "tcdate": 1569439045823, "tmdate": 1577168212314, "tddate": null, "forum": "HkxAS6VFDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Prune or quantize? Strategy for Pareto-optimally low-cost and accurate CNN", "authors": ["Kengo Nakata", "Daisuke Miyashita", "Asuka Maki", "Fumihiko Tachibana", "Shinichi Sasaki", "Jun Deguchi"], "authorids": ["kengo1.nakata@toshiba.co.jp", "daisuke1.miyashita@toshiba.co.jp", "asuka.maki@toshiba.co.jp", "fumihiko.tachibana@toshiba.co.jp", "shinichi8.sasaki@toshiba.co.jp", "jun.deguchi@toshiba.co.jp"], "keywords": ["CNN", "Quantization", "Pruning", "Accelerator", "Computational cost"], "TL;DR": "This paper reveals that \"prune-then-quantize method\" is the best strategy to achieve Pareto-optimal performance by using a proposed hardware-agnostic metric to measure computational cost.", "abstract": "Pruning and quantization are typical approaches to reduce the computational cost of CNN inference. Although the idea to combine them together seems natural, it is being unexpectedly difficult to figure out the resultant effect of the combination unless measuring the performance on a certain hardware which a user is going to use. This is because the benefits of pruning and quantization strongly depend on the hardware architecture where the model is executed. For example, a CPU-like architecture without any parallelization may fully exploit the reduction of computations by unstructured pruning for speeding up, but a GPU-like massive parallel architecture would not. Besides, there have been emerging proposals of novel hardware architectures such as one supporting variable bit precision quantization. From an engineering viewpoint, optimization for each hardware architecture is useful and important in practice, but this is quite a brute-force approach. Therefore, in this paper, we first propose hardware-agnostic metric to measure the computational cost. And using the metric, we demonstrate that Pareto-optimal performance, where the best accuracy is obtained at a given computational cost, is achieved when a slim model with smaller number of parameters is quantized moderately rather than a fat model with huge number of parameters is quantized to extremely low bit precision such as binary or ternary. Furthermore, we empirically found the possible quantitative relation between the proposed metric and the signal to noise ratio during SGD training, by which the information obtained during SGD training provides the optimal policy of quantization and pruning. We show the Pareto frontier is improved by 4 times in post-training quantization scenario based on these findings. These findings are available not only to improve the Pareto frontier for accuracy vs. computational cost, but also give us some new insights on deep neural network.", "pdf": "/pdf/739e3925990713d557e2dbf1739383585bdf0c7c.pdf", "paperhash": "nakata|prune_or_quantize_strategy_for_paretooptimally_lowcost_and_accurate_cnn", "original_pdf": "/attachment/276428aecdbce6ccfd6e99352520aaa47b98b95a.pdf", "_bibtex": "@misc{\nnakata2020prune,\ntitle={Prune or quantize? Strategy for Pareto-optimally low-cost and accurate {\\{}CNN{\\}}},\nauthor={Kengo Nakata and Daisuke Miyashita and Asuka Maki and Fumihiko Tachibana and Shinichi Sasaki and Jun Deguchi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxAS6VFDB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "LNcAEsznct", "original": null, "number": 1, "cdate": 1576798699286, "ddate": null, "tcdate": 1576798699286, "tmdate": 1576800936561, "tddate": null, "forum": "HkxAS6VFDB", "replyto": "HkxAS6VFDB", "invitation": "ICLR.cc/2020/Conference/Paper542/-/Decision", "content": {"decision": "Reject", "comment": "The authors propose a hardware-agnostic metric called effective signal norm (ESN) to measure the computational cost of convolutional neural networks. They then demonstrate that models with fewer parameters achieve far better accuracy after quantization. The main novelty is on the metric ESN. However, ESN is based on ideal hardware, and thus not suitable for existing hardware. Assumptions made in the paper are hard to be proved. Experimental results are not convincing, and related pruning methods are not compared. Finally, the paper is not written clearly, and the structure and some arguments are confusing.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prune or quantize? Strategy for Pareto-optimally low-cost and accurate CNN", "authors": ["Kengo Nakata", "Daisuke Miyashita", "Asuka Maki", "Fumihiko Tachibana", "Shinichi Sasaki", "Jun Deguchi"], "authorids": ["kengo1.nakata@toshiba.co.jp", "daisuke1.miyashita@toshiba.co.jp", "asuka.maki@toshiba.co.jp", "fumihiko.tachibana@toshiba.co.jp", "shinichi8.sasaki@toshiba.co.jp", "jun.deguchi@toshiba.co.jp"], "keywords": ["CNN", "Quantization", "Pruning", "Accelerator", "Computational cost"], "TL;DR": "This paper reveals that \"prune-then-quantize method\" is the best strategy to achieve Pareto-optimal performance by using a proposed hardware-agnostic metric to measure computational cost.", "abstract": "Pruning and quantization are typical approaches to reduce the computational cost of CNN inference. Although the idea to combine them together seems natural, it is being unexpectedly difficult to figure out the resultant effect of the combination unless measuring the performance on a certain hardware which a user is going to use. This is because the benefits of pruning and quantization strongly depend on the hardware architecture where the model is executed. For example, a CPU-like architecture without any parallelization may fully exploit the reduction of computations by unstructured pruning for speeding up, but a GPU-like massive parallel architecture would not. Besides, there have been emerging proposals of novel hardware architectures such as one supporting variable bit precision quantization. From an engineering viewpoint, optimization for each hardware architecture is useful and important in practice, but this is quite a brute-force approach. Therefore, in this paper, we first propose hardware-agnostic metric to measure the computational cost. And using the metric, we demonstrate that Pareto-optimal performance, where the best accuracy is obtained at a given computational cost, is achieved when a slim model with smaller number of parameters is quantized moderately rather than a fat model with huge number of parameters is quantized to extremely low bit precision such as binary or ternary. Furthermore, we empirically found the possible quantitative relation between the proposed metric and the signal to noise ratio during SGD training, by which the information obtained during SGD training provides the optimal policy of quantization and pruning. We show the Pareto frontier is improved by 4 times in post-training quantization scenario based on these findings. These findings are available not only to improve the Pareto frontier for accuracy vs. computational cost, but also give us some new insights on deep neural network.", "pdf": "/pdf/739e3925990713d557e2dbf1739383585bdf0c7c.pdf", "paperhash": "nakata|prune_or_quantize_strategy_for_paretooptimally_lowcost_and_accurate_cnn", "original_pdf": "/attachment/276428aecdbce6ccfd6e99352520aaa47b98b95a.pdf", "_bibtex": "@misc{\nnakata2020prune,\ntitle={Prune or quantize? Strategy for Pareto-optimally low-cost and accurate {\\{}CNN{\\}}},\nauthor={Kengo Nakata and Daisuke Miyashita and Asuka Maki and Fumihiko Tachibana and Shinichi Sasaki and Jun Deguchi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxAS6VFDB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HkxAS6VFDB", "replyto": "HkxAS6VFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795709230, "tmdate": 1576800257916, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper542/-/Decision"}}}, {"id": "rylzNrlaYB", "original": null, "number": 1, "cdate": 1571779881864, "ddate": null, "tcdate": 1571779881864, "tmdate": 1574425506847, "tddate": null, "forum": "HkxAS6VFDB", "replyto": "HkxAS6VFDB", "invitation": "ICLR.cc/2020/Conference/Paper542/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #2", "review": "The paper proposes a new metric to evaluate both the amount of pruning and quantization. This metric is agnostic to the hardware architecture and is simply obtained by computing the Frobenius norm of some point-wise transformation of the quantized weights. They first show empirically that this Evaluation metric is correlated with the validation accuracy. Then use this metric to provide some general rules for pruning/quantizing to preserve the highest validation accuracy. Finally, they derive a strategy to perform pruning by monitory the signal to noise ratio during training and show experimentally that such method performs better than competing ones.\n\nPros: - Extensive experiments were performed to test the methods, and the results seem promising.\n\nCons: - The paper is not very clear, and the structure is somehow confusing.\n- It is not easy at first to understand the experimental setup and requires to make a lot of guesses in my opinion. \n- The paper didn't motivate properly the use of a hardware-agnostic metric in the context of the quantization and pruning. Isn't the ultimate goal of pruning/quantization is to optimize the run time/energy consumption of the specific device with the least compromise on the accuracy?\n\nI feel that the paper currently jumps between very different ideas: \n\t- Evolution of the proposed metric during training: 2.3 and 2.5. While the 2.3, the take-home message is relatively clear:  the ESN is correlated with the validation accuracy, I don't fully get the point of section 2.5: It suggests that the optimizer does some sort of pruning just by choosing a higher learning rate.   \n\t- Finding an optimal strategy for pruning/quantizing a network: 2.4 and 2.6. Those two sections are relatively clear, although I have some questions about the experiments.\n\t- Developing a new strategy based on the proposed metric to quantize and prune a network in a Pareto optimal sense: This is briefly and not very well explained in section 2.7, which sends back to 2.3, but it is hard to understand how it is exactly done. It seems that section 3 provides some empirical evidence supporting this strategy, but the description of the method is hidden in the experimental details.\n\n\nSome questions: \n- In figure 3, the blue dots represent validation vs ESN at each training iteration? What about the red plot, is it obtained by quantization of the parameters at different stages of training, or is it using the final parameters? Which equation was used to compute the red curve (2) or (4)?  How much quantization was performed? If the quantization was chosen to match the level of noise then it seems natural to expect such behavior in figure 3.  \n\n- In figure 4, how much pruning was performed for each network and was it the same quantization? In other words how each point in the plot was obtained?  The authors come to the conclusion that one should 'prune until the limit of the desired accuracy and then quantize', but it is hard for me to reach the same conclusions as I don't see the separate effect of pruning and quantization in those figures. Or maybe pruning is implicitly done by choosing a small network? In this case, it makes more sense, but still, some clarifications are needed.\n\n- Which equation for the ESN was used to produce figure 5? Equation (2) or (4)? \n\n- What is the Pareto frontier? I think it is worth first introducing this concept and describing more precisely how those curves are obtained. For someone who is not very familiar with these concepts, which is my case, it makes the reading very hard. \n\n- How was the number of pruned filters computed in figure 5 (right)? I don't expect the solutions to be sparse during training, especially that no sparsity constraint was imposed, or was it?\n\t\n\n\n-------------------------------------------------------------------------------\nRevision:\n\nThank you for all the clarifications and the effort to make provide a clearer version of the paper. \n\nRegarding section 2.7: ESNa FOR QUANTIZATION: Would it make sense to include the paragraph 2.7 at the end of 2.3, since it related to it and doesn't seem to require any of the intermediate subsections.\n\nresponse to Comment 3: Unfortunately, I'm not convinced by the explanation about the effect of the lr on sparsity. The decay coefficient controls the saparsity indeed, but not the lr. That is because unlike the lr, the decay coefficient defines the cost functions to be optimized:  L+dc ||W||^2, while the lr corresponds simply to the discretization of some gradient flow.   For instance, in a deterministic and convex setting, the solution that is obtained would be the same, regardless of the chosen lr  ( provided the lr is smal enough so that the algorithm converges) see for instance [1].  In a non-convex and stochastic setting do the authors have a particular reference in mind? I'm not aware of such behavior. I would expect a similar sparsity if dc is kept fixed and only lr changed. Is it likely that with smaller lr, the algorithm just didn't have time to converge? This would explain why the obtained solutions were less sparse.\nresponse to answer 5:  it is indeed well known that L1 norm induces sparsity, however l2 doesn't, it just encourages the weights to be smaller. In the optimization litterature sparsity of x% means x% of the parameters  are exactly 0. This is achieved with l1 norm, however l2 norm would only enforce that the coefficients are small but not necessarily 0 (see [1])\n[1]:  ROBERT TIBSHIRANI, Regression Shrinkage and Selection via the Lasso.\n\nAlthough the paper improved in terms of clarifications and experimental details, I still think it will benefit from additional work on careful interpretation of the results.\n\n\n\n\n\n\n\n\n\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper542/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper542/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prune or quantize? Strategy for Pareto-optimally low-cost and accurate CNN", "authors": ["Kengo Nakata", "Daisuke Miyashita", "Asuka Maki", "Fumihiko Tachibana", "Shinichi Sasaki", "Jun Deguchi"], "authorids": ["kengo1.nakata@toshiba.co.jp", "daisuke1.miyashita@toshiba.co.jp", "asuka.maki@toshiba.co.jp", "fumihiko.tachibana@toshiba.co.jp", "shinichi8.sasaki@toshiba.co.jp", "jun.deguchi@toshiba.co.jp"], "keywords": ["CNN", "Quantization", "Pruning", "Accelerator", "Computational cost"], "TL;DR": "This paper reveals that \"prune-then-quantize method\" is the best strategy to achieve Pareto-optimal performance by using a proposed hardware-agnostic metric to measure computational cost.", "abstract": "Pruning and quantization are typical approaches to reduce the computational cost of CNN inference. Although the idea to combine them together seems natural, it is being unexpectedly difficult to figure out the resultant effect of the combination unless measuring the performance on a certain hardware which a user is going to use. This is because the benefits of pruning and quantization strongly depend on the hardware architecture where the model is executed. For example, a CPU-like architecture without any parallelization may fully exploit the reduction of computations by unstructured pruning for speeding up, but a GPU-like massive parallel architecture would not. Besides, there have been emerging proposals of novel hardware architectures such as one supporting variable bit precision quantization. From an engineering viewpoint, optimization for each hardware architecture is useful and important in practice, but this is quite a brute-force approach. Therefore, in this paper, we first propose hardware-agnostic metric to measure the computational cost. And using the metric, we demonstrate that Pareto-optimal performance, where the best accuracy is obtained at a given computational cost, is achieved when a slim model with smaller number of parameters is quantized moderately rather than a fat model with huge number of parameters is quantized to extremely low bit precision such as binary or ternary. Furthermore, we empirically found the possible quantitative relation between the proposed metric and the signal to noise ratio during SGD training, by which the information obtained during SGD training provides the optimal policy of quantization and pruning. We show the Pareto frontier is improved by 4 times in post-training quantization scenario based on these findings. These findings are available not only to improve the Pareto frontier for accuracy vs. computational cost, but also give us some new insights on deep neural network.", "pdf": "/pdf/739e3925990713d557e2dbf1739383585bdf0c7c.pdf", "paperhash": "nakata|prune_or_quantize_strategy_for_paretooptimally_lowcost_and_accurate_cnn", "original_pdf": "/attachment/276428aecdbce6ccfd6e99352520aaa47b98b95a.pdf", "_bibtex": "@misc{\nnakata2020prune,\ntitle={Prune or quantize? Strategy for Pareto-optimally low-cost and accurate {\\{}CNN{\\}}},\nauthor={Kengo Nakata and Daisuke Miyashita and Asuka Maki and Fumihiko Tachibana and Shinichi Sasaki and Jun Deguchi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxAS6VFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxAS6VFDB", "replyto": "HkxAS6VFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper542/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper542/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575117497632, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper542/Reviewers"], "noninvitees": [], "tcdate": 1570237750629, "tmdate": 1575117497649, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper542/-/Official_Review"}}}, {"id": "SJx81WQnor", "original": null, "number": 11, "cdate": 1573822686468, "ddate": null, "tcdate": 1573822686468, "tmdate": 1573822686468, "tddate": null, "forum": "HkxAS6VFDB", "replyto": "SJgEtPI9jS", "invitation": "ICLR.cc/2020/Conference/Paper542/-/Official_Comment", "content": {"title": "Upload the revised manuscript 3", "comment": "We had the manuscript proofread by native speakers and modified spelling and grammar mistakes. We uploaded the revised manuscript."}, "signatures": ["ICLR.cc/2020/Conference/Paper542/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prune or quantize? Strategy for Pareto-optimally low-cost and accurate CNN", "authors": ["Kengo Nakata", "Daisuke Miyashita", "Asuka Maki", "Fumihiko Tachibana", "Shinichi Sasaki", "Jun Deguchi"], "authorids": ["kengo1.nakata@toshiba.co.jp", "daisuke1.miyashita@toshiba.co.jp", "asuka.maki@toshiba.co.jp", "fumihiko.tachibana@toshiba.co.jp", "shinichi8.sasaki@toshiba.co.jp", "jun.deguchi@toshiba.co.jp"], "keywords": ["CNN", "Quantization", "Pruning", "Accelerator", "Computational cost"], "TL;DR": "This paper reveals that \"prune-then-quantize method\" is the best strategy to achieve Pareto-optimal performance by using a proposed hardware-agnostic metric to measure computational cost.", "abstract": "Pruning and quantization are typical approaches to reduce the computational cost of CNN inference. Although the idea to combine them together seems natural, it is being unexpectedly difficult to figure out the resultant effect of the combination unless measuring the performance on a certain hardware which a user is going to use. This is because the benefits of pruning and quantization strongly depend on the hardware architecture where the model is executed. For example, a CPU-like architecture without any parallelization may fully exploit the reduction of computations by unstructured pruning for speeding up, but a GPU-like massive parallel architecture would not. Besides, there have been emerging proposals of novel hardware architectures such as one supporting variable bit precision quantization. From an engineering viewpoint, optimization for each hardware architecture is useful and important in practice, but this is quite a brute-force approach. Therefore, in this paper, we first propose hardware-agnostic metric to measure the computational cost. And using the metric, we demonstrate that Pareto-optimal performance, where the best accuracy is obtained at a given computational cost, is achieved when a slim model with smaller number of parameters is quantized moderately rather than a fat model with huge number of parameters is quantized to extremely low bit precision such as binary or ternary. Furthermore, we empirically found the possible quantitative relation between the proposed metric and the signal to noise ratio during SGD training, by which the information obtained during SGD training provides the optimal policy of quantization and pruning. We show the Pareto frontier is improved by 4 times in post-training quantization scenario based on these findings. These findings are available not only to improve the Pareto frontier for accuracy vs. computational cost, but also give us some new insights on deep neural network.", "pdf": "/pdf/739e3925990713d557e2dbf1739383585bdf0c7c.pdf", "paperhash": "nakata|prune_or_quantize_strategy_for_paretooptimally_lowcost_and_accurate_cnn", "original_pdf": "/attachment/276428aecdbce6ccfd6e99352520aaa47b98b95a.pdf", "_bibtex": "@misc{\nnakata2020prune,\ntitle={Prune or quantize? Strategy for Pareto-optimally low-cost and accurate {\\{}CNN{\\}}},\nauthor={Kengo Nakata and Daisuke Miyashita and Asuka Maki and Fumihiko Tachibana and Shinichi Sasaki and Jun Deguchi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxAS6VFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxAS6VFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference/Paper542/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper542/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper542/Reviewers", "ICLR.cc/2020/Conference/Paper542/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper542/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper542/Authors|ICLR.cc/2020/Conference/Paper542/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169895, "tmdate": 1576860562158, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference/Paper542/Reviewers", "ICLR.cc/2020/Conference/Paper542/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper542/-/Official_Comment"}}}, {"id": "rkl1rymnjB", "original": null, "number": 10, "cdate": 1573822262837, "ddate": null, "tcdate": 1573822262837, "tmdate": 1573822262837, "tddate": null, "forum": "HkxAS6VFDB", "replyto": "H1ePR0f3iS", "invitation": "ICLR.cc/2020/Conference/Paper542/-/Official_Comment", "content": {"title": "Response to Reviewer #1 (2/2)", "comment": "# # # continued from previous comment (1/2) # # # \n\nComment 3\n> Finally given that there are already quite a few methods that prunes model based on real hardware evaluations, it would be great to compare to these methods as well.\n\nAnswer 3\nCertainly, there are a lot of conventional methods that prune model based on evaluations with existing hardware. However, our goal is to properly evaluate models where both pruning and quantization are applied, and as we described above, the existing hardware cannot reflect the effect of quantization in addition to that of pruning. Therefore, we think we should evaluate and compare methods for reducing computational costs of CNN including our proposal and also conventional pruning and/or quantization schemes using hardware that supports both pruning and quantization. We believe such hardware will appear in the near future. Although we used weight-regularization based pruning method in this study, any pruning method can be employed to produce a pruned slim model. We expect that such hardware can fully exploit the benefit of quantization, taking advantage of the best pruning method.\n\n\nReferences\n[1] A. Maki et al., \u201cFPGA-based CNN processor with filter-wise-optimized bit precision\u201d, In 2018 IEEE Asian Solid-State Circuits Conference (A-SSCC), pp. 47\u201350, Nov 2018. doi: 10.1109/ASSCC.2018.8579342.\n(We have referred this paper as (Maki et al., 2018) in the original manuscript.)\n\n[2] J. Lee et al., \u201cUNPU: A 50.6TOPS/W Unified Deep Neural Network Accelerator with 1b-to-16b Fully-Variable Weight Bit-Precision\u201d, In 2018 IEEE International Solid State Circuits Conference (ISSCC), pp. 218-220, Feb 2018. doi: 10.1109/ISSCC.2018.8310262.\n\n[3] Z. Yuan et al., \u201cSTICKER: A 0.41-62.1 TOPS/W 8bit Neural Network Processor with Multi-Sparsity Compatible Convolution Arrays and Online Tuning Acceleration for Fully Connected Layers\u201d, In 2018 IEEE Symposium on VLSI Circuits, pp. 33-34, Jun 2018. doi: 10.1109/VLSIC.2018.8502404.\n\n[4] J. Song et al., \u201cAn 11.5TOPS/W 1024-MAC Butterfly Structure Dual-Core Sparsity-Aware Neural Processing Unit in 8nm Flagship Mobile SoC\u201d, In 2019 IEEE International Solid State Circuits Conference (ISSCC), pp. 130-132, Feb 2019. doi: 10.1109/ISSCC.2019.8662476. \n\n[5] S. Sasaki, et al., \u201cPost training weight compression with distribution-based filter-wise quantization step\u201d, In 2019 IEEE Symposium in Low-Power and High-Speed Chips (COOLCHIPS), pp. 1\u20133, April 2019. doi: 10.1109/CoolChips.2019.8721356.\n(We have referred this paper as \u201cSasaki et al., 2019\u201d in the original manuscript)\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper542/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prune or quantize? Strategy for Pareto-optimally low-cost and accurate CNN", "authors": ["Kengo Nakata", "Daisuke Miyashita", "Asuka Maki", "Fumihiko Tachibana", "Shinichi Sasaki", "Jun Deguchi"], "authorids": ["kengo1.nakata@toshiba.co.jp", "daisuke1.miyashita@toshiba.co.jp", "asuka.maki@toshiba.co.jp", "fumihiko.tachibana@toshiba.co.jp", "shinichi8.sasaki@toshiba.co.jp", "jun.deguchi@toshiba.co.jp"], "keywords": ["CNN", "Quantization", "Pruning", "Accelerator", "Computational cost"], "TL;DR": "This paper reveals that \"prune-then-quantize method\" is the best strategy to achieve Pareto-optimal performance by using a proposed hardware-agnostic metric to measure computational cost.", "abstract": "Pruning and quantization are typical approaches to reduce the computational cost of CNN inference. Although the idea to combine them together seems natural, it is being unexpectedly difficult to figure out the resultant effect of the combination unless measuring the performance on a certain hardware which a user is going to use. This is because the benefits of pruning and quantization strongly depend on the hardware architecture where the model is executed. For example, a CPU-like architecture without any parallelization may fully exploit the reduction of computations by unstructured pruning for speeding up, but a GPU-like massive parallel architecture would not. Besides, there have been emerging proposals of novel hardware architectures such as one supporting variable bit precision quantization. From an engineering viewpoint, optimization for each hardware architecture is useful and important in practice, but this is quite a brute-force approach. Therefore, in this paper, we first propose hardware-agnostic metric to measure the computational cost. And using the metric, we demonstrate that Pareto-optimal performance, where the best accuracy is obtained at a given computational cost, is achieved when a slim model with smaller number of parameters is quantized moderately rather than a fat model with huge number of parameters is quantized to extremely low bit precision such as binary or ternary. Furthermore, we empirically found the possible quantitative relation between the proposed metric and the signal to noise ratio during SGD training, by which the information obtained during SGD training provides the optimal policy of quantization and pruning. We show the Pareto frontier is improved by 4 times in post-training quantization scenario based on these findings. These findings are available not only to improve the Pareto frontier for accuracy vs. computational cost, but also give us some new insights on deep neural network.", "pdf": "/pdf/739e3925990713d557e2dbf1739383585bdf0c7c.pdf", "paperhash": "nakata|prune_or_quantize_strategy_for_paretooptimally_lowcost_and_accurate_cnn", "original_pdf": "/attachment/276428aecdbce6ccfd6e99352520aaa47b98b95a.pdf", "_bibtex": "@misc{\nnakata2020prune,\ntitle={Prune or quantize? Strategy for Pareto-optimally low-cost and accurate {\\{}CNN{\\}}},\nauthor={Kengo Nakata and Daisuke Miyashita and Asuka Maki and Fumihiko Tachibana and Shinichi Sasaki and Jun Deguchi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxAS6VFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxAS6VFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference/Paper542/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper542/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper542/Reviewers", "ICLR.cc/2020/Conference/Paper542/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper542/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper542/Authors|ICLR.cc/2020/Conference/Paper542/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169895, "tmdate": 1576860562158, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference/Paper542/Reviewers", "ICLR.cc/2020/Conference/Paper542/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper542/-/Official_Comment"}}}, {"id": "H1ePR0f3iS", "original": null, "number": 9, "cdate": 1573822158853, "ddate": null, "tcdate": 1573822158853, "tmdate": 1573822158853, "tddate": null, "forum": "HkxAS6VFDB", "replyto": "rylETCDaFS", "invitation": "ICLR.cc/2020/Conference/Paper542/-/Official_Comment", "content": {"title": "Response to Reviewer #1 (1/2)", "comment": "We appreciate your detailed comments and insightful feedback on our paper. We reply to your comments as follows.\n\nComment 1\n>The main drawback of the paper is the lack of novelty in proposed method. The metric itself appeared to be the main contribution, but as the author said, the metric was based on an ideal hardware setup that ignores the memory hierarchy and data transfer cost, which could be the bottleneck in reality.\n\nAnswer 1\nWe mainly focus on CNN, which has large computational intensity and therefore is less prone to be memory bottleneck. Although pruning and quantization are typical approach to reduce the computational cost of CNN, existing hardware such as CPU and GPU cannot exploit them, and e.g. a metric of speed measured using CPU cannot evaluate the pruned and quantized models properly.\n\nOur motivation is to provide a metric which can evaluate them.\n(We would like you to see the response to Comment 2 in the discussion with Reviewer #2 as well.)\nSo the proposed metric of ESN has such novelty that is able to evaluate pruned and quantized models.\n\nAlthough the proposed metric assumes ideal hardware, we believe that our assumption is reasonable.\nFor example, there have been a lot of proposals that handle either quantized or pruned CNN models efficiently [1-4], and the fact that they demonstrated the energy efficiency was improved by applying quantization or pruning supports our assumption.\n(We would like you to see the response to Negative Points 1 & 2 in the discussion in Reviewer #4.)\n\nWe agree that even in CNN or a situation with high computational intensity, the energy consumption for memory access is still significant. Pruning and quantization, which reduces the amount of data transferred between memory and processor, contributes to reducing memory accesses. The tendency to reduce memory accesses by pruning and quantization is similar to that to reduce computational costs. Hence, ESN allows us to estimate the tendency to reduce memory accesses as well as computational costs by pruning and quantization.\n\nWe agree that it is important to fine-tune models to optimize the computational cost considering memory hierarchy and data transfer cost, and we should do that even after novel devices that support both quantization and pruning are developed. We believe, however, we also need such metric as our proposal for making sure that the both algorithmic and hardware architecture researches are on the same page for future emerging devices.\n\n\nComment 2\n> More importantly, it is hard to use the empirical evaluation to justify the conclusion \u201cfatter models are worse\u201d. As we know that extremely low bit quantized models are very hard to train, and it is not surprising -- by setting the number of bits to extremely low bits, the models cannot recover in a few rounds of re-training.\nThis being said, I cannot think of a better alternative. While I do not think the experimental results offers new insights (other than it is hard to re-train lower bits models). How to quickly train low bits models remains an open question.\n\nAnswer 2\nWe are sorry for making you misunderstanding. The conclusion of our paper is not \u201cfatter models are worse\u201d but \u201cthe accuracy of fat models quantized to extremely low bit is worse than that of quantized slim models in the region of low computational cost.\u201d\nWe assume a use-case scenario where a user (e.g. at edge side) quantizes pretrained models in order to obtain models which can maintain original accuracy with a low computational cost. In that case, it is not necessary to train low-bit models through backpropagation. Therefore, the situation that the user trains extremely low-bit-quantized models, which is very hard as you pointed out, does not occur. The user only fine-tunes models quantized to low bits for tuning statistic parameters in BatchNorm layers. Then, the user can tune the parameters quickly without backpropagation or any labeled data [5].\nFor the above reason, to re-train low bit models and to train quickly low-bit models are outside the scope of this study.\n\n# # # continued to next comment # # #"}, "signatures": ["ICLR.cc/2020/Conference/Paper542/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prune or quantize? Strategy for Pareto-optimally low-cost and accurate CNN", "authors": ["Kengo Nakata", "Daisuke Miyashita", "Asuka Maki", "Fumihiko Tachibana", "Shinichi Sasaki", "Jun Deguchi"], "authorids": ["kengo1.nakata@toshiba.co.jp", "daisuke1.miyashita@toshiba.co.jp", "asuka.maki@toshiba.co.jp", "fumihiko.tachibana@toshiba.co.jp", "shinichi8.sasaki@toshiba.co.jp", "jun.deguchi@toshiba.co.jp"], "keywords": ["CNN", "Quantization", "Pruning", "Accelerator", "Computational cost"], "TL;DR": "This paper reveals that \"prune-then-quantize method\" is the best strategy to achieve Pareto-optimal performance by using a proposed hardware-agnostic metric to measure computational cost.", "abstract": "Pruning and quantization are typical approaches to reduce the computational cost of CNN inference. Although the idea to combine them together seems natural, it is being unexpectedly difficult to figure out the resultant effect of the combination unless measuring the performance on a certain hardware which a user is going to use. This is because the benefits of pruning and quantization strongly depend on the hardware architecture where the model is executed. For example, a CPU-like architecture without any parallelization may fully exploit the reduction of computations by unstructured pruning for speeding up, but a GPU-like massive parallel architecture would not. Besides, there have been emerging proposals of novel hardware architectures such as one supporting variable bit precision quantization. From an engineering viewpoint, optimization for each hardware architecture is useful and important in practice, but this is quite a brute-force approach. Therefore, in this paper, we first propose hardware-agnostic metric to measure the computational cost. And using the metric, we demonstrate that Pareto-optimal performance, where the best accuracy is obtained at a given computational cost, is achieved when a slim model with smaller number of parameters is quantized moderately rather than a fat model with huge number of parameters is quantized to extremely low bit precision such as binary or ternary. Furthermore, we empirically found the possible quantitative relation between the proposed metric and the signal to noise ratio during SGD training, by which the information obtained during SGD training provides the optimal policy of quantization and pruning. We show the Pareto frontier is improved by 4 times in post-training quantization scenario based on these findings. These findings are available not only to improve the Pareto frontier for accuracy vs. computational cost, but also give us some new insights on deep neural network.", "pdf": "/pdf/739e3925990713d557e2dbf1739383585bdf0c7c.pdf", "paperhash": "nakata|prune_or_quantize_strategy_for_paretooptimally_lowcost_and_accurate_cnn", "original_pdf": "/attachment/276428aecdbce6ccfd6e99352520aaa47b98b95a.pdf", "_bibtex": "@misc{\nnakata2020prune,\ntitle={Prune or quantize? Strategy for Pareto-optimally low-cost and accurate {\\{}CNN{\\}}},\nauthor={Kengo Nakata and Daisuke Miyashita and Asuka Maki and Fumihiko Tachibana and Shinichi Sasaki and Jun Deguchi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxAS6VFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxAS6VFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference/Paper542/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper542/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper542/Reviewers", "ICLR.cc/2020/Conference/Paper542/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper542/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper542/Authors|ICLR.cc/2020/Conference/Paper542/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169895, "tmdate": 1576860562158, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference/Paper542/Reviewers", "ICLR.cc/2020/Conference/Paper542/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper542/-/Official_Comment"}}}, {"id": "SJgEtPI9jS", "original": null, "number": 8, "cdate": 1573705595657, "ddate": null, "tcdate": 1573705595657, "tmdate": 1573705595657, "tddate": null, "forum": "HkxAS6VFDB", "replyto": "rylP-pcvor", "invitation": "ICLR.cc/2020/Conference/Paper542/-/Official_Comment", "content": {"title": "Upload the revised manuscript 2", "comment": "We uploaded the revised manuscript. We revised the manuscript as follows.\n\n# We added the following sentence about experiments to the first paragraph in Section 2.4, taking the feedbacks from reviewer #2 as reference.\n\n\"In this experiment, we employ networks with originally small number of parameters as slim models instead of pruning fat models.\""}, "signatures": ["ICLR.cc/2020/Conference/Paper542/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prune or quantize? Strategy for Pareto-optimally low-cost and accurate CNN", "authors": ["Kengo Nakata", "Daisuke Miyashita", "Asuka Maki", "Fumihiko Tachibana", "Shinichi Sasaki", "Jun Deguchi"], "authorids": ["kengo1.nakata@toshiba.co.jp", "daisuke1.miyashita@toshiba.co.jp", "asuka.maki@toshiba.co.jp", "fumihiko.tachibana@toshiba.co.jp", "shinichi8.sasaki@toshiba.co.jp", "jun.deguchi@toshiba.co.jp"], "keywords": ["CNN", "Quantization", "Pruning", "Accelerator", "Computational cost"], "TL;DR": "This paper reveals that \"prune-then-quantize method\" is the best strategy to achieve Pareto-optimal performance by using a proposed hardware-agnostic metric to measure computational cost.", "abstract": "Pruning and quantization are typical approaches to reduce the computational cost of CNN inference. Although the idea to combine them together seems natural, it is being unexpectedly difficult to figure out the resultant effect of the combination unless measuring the performance on a certain hardware which a user is going to use. This is because the benefits of pruning and quantization strongly depend on the hardware architecture where the model is executed. For example, a CPU-like architecture without any parallelization may fully exploit the reduction of computations by unstructured pruning for speeding up, but a GPU-like massive parallel architecture would not. Besides, there have been emerging proposals of novel hardware architectures such as one supporting variable bit precision quantization. From an engineering viewpoint, optimization for each hardware architecture is useful and important in practice, but this is quite a brute-force approach. Therefore, in this paper, we first propose hardware-agnostic metric to measure the computational cost. And using the metric, we demonstrate that Pareto-optimal performance, where the best accuracy is obtained at a given computational cost, is achieved when a slim model with smaller number of parameters is quantized moderately rather than a fat model with huge number of parameters is quantized to extremely low bit precision such as binary or ternary. Furthermore, we empirically found the possible quantitative relation between the proposed metric and the signal to noise ratio during SGD training, by which the information obtained during SGD training provides the optimal policy of quantization and pruning. We show the Pareto frontier is improved by 4 times in post-training quantization scenario based on these findings. These findings are available not only to improve the Pareto frontier for accuracy vs. computational cost, but also give us some new insights on deep neural network.", "pdf": "/pdf/739e3925990713d557e2dbf1739383585bdf0c7c.pdf", "paperhash": "nakata|prune_or_quantize_strategy_for_paretooptimally_lowcost_and_accurate_cnn", "original_pdf": "/attachment/276428aecdbce6ccfd6e99352520aaa47b98b95a.pdf", "_bibtex": "@misc{\nnakata2020prune,\ntitle={Prune or quantize? Strategy for Pareto-optimally low-cost and accurate {\\{}CNN{\\}}},\nauthor={Kengo Nakata and Daisuke Miyashita and Asuka Maki and Fumihiko Tachibana and Shinichi Sasaki and Jun Deguchi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxAS6VFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxAS6VFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference/Paper542/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper542/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper542/Reviewers", "ICLR.cc/2020/Conference/Paper542/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper542/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper542/Authors|ICLR.cc/2020/Conference/Paper542/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169895, "tmdate": 1576860562158, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference/Paper542/Reviewers", "ICLR.cc/2020/Conference/Paper542/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper542/-/Official_Comment"}}}, {"id": "H1eH_IIqiB", "original": null, "number": 7, "cdate": 1573705325359, "ddate": null, "tcdate": 1573705325359, "tmdate": 1573705325359, "tddate": null, "forum": "HkxAS6VFDB", "replyto": "ryl--I85sB", "invitation": "ICLR.cc/2020/Conference/Paper542/-/Official_Comment", "content": {"title": "Response to Reviewer 2 (4)", "comment": "# # # continued from previous comment (3/3) # # # \n\nQuestion 4\n> - What is the Pareto frontier? I think it is worth first introducing this concept and describing more precisely how those curves are obtained. For someone who is not very familiar with these concepts, which is my case, it makes the reading very hard.\n\nAnswer 4\nWe appreciate your valuable advice regarding \u201cPareto frontier\u201d. We give a supplementary explanation about that. In order to explain \u201cPareto frontier\u201d, we describe multi-objective optimization problem. We assume the situation that we have two or more objective functions, and that we optimize the objective functions based on some criteria. Pareto optimality is a state that there is no alternative state that would make some objective functions better off without making any other objective functions worse off. Then, Pareto frontier is the set of Pareto optimal states for various conditions.\n\nFor example, in the experiment of section 2.6, objective functions are ESN_a and validation accuracy. In the experiment, we evaluated only pruned, only quantized, and prune-then-quantize mixed models. At a certain value of ESN_a, the highest accuracy of them is picked up as the Pareto optimal point. In figure 6, the Pareto frontier is the plot of prune-then-quantize mixed model (red curve).\n\n\nQuestion 5\n>- How was the number of pruned filters computed in figure 5 (right)? I don't expect the solutions to be sparse during training, especially that no sparsity constraint was imposed, or was it?\n\nAnswer 5\nIn the experiment of section 2.5, we impose L2 norm regularization (so-called weight decay) on weights parameters during training. According to a previous work [5], the constraint such as L1 or L2 regularization has an effect of sparsity constraint on the weights. Moreover, those regularization methods can be applied for pruning method, and we used this technique in section 2.6.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper542/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prune or quantize? Strategy for Pareto-optimally low-cost and accurate CNN", "authors": ["Kengo Nakata", "Daisuke Miyashita", "Asuka Maki", "Fumihiko Tachibana", "Shinichi Sasaki", "Jun Deguchi"], "authorids": ["kengo1.nakata@toshiba.co.jp", "daisuke1.miyashita@toshiba.co.jp", "asuka.maki@toshiba.co.jp", "fumihiko.tachibana@toshiba.co.jp", "shinichi8.sasaki@toshiba.co.jp", "jun.deguchi@toshiba.co.jp"], "keywords": ["CNN", "Quantization", "Pruning", "Accelerator", "Computational cost"], "TL;DR": "This paper reveals that \"prune-then-quantize method\" is the best strategy to achieve Pareto-optimal performance by using a proposed hardware-agnostic metric to measure computational cost.", "abstract": "Pruning and quantization are typical approaches to reduce the computational cost of CNN inference. Although the idea to combine them together seems natural, it is being unexpectedly difficult to figure out the resultant effect of the combination unless measuring the performance on a certain hardware which a user is going to use. This is because the benefits of pruning and quantization strongly depend on the hardware architecture where the model is executed. For example, a CPU-like architecture without any parallelization may fully exploit the reduction of computations by unstructured pruning for speeding up, but a GPU-like massive parallel architecture would not. Besides, there have been emerging proposals of novel hardware architectures such as one supporting variable bit precision quantization. From an engineering viewpoint, optimization for each hardware architecture is useful and important in practice, but this is quite a brute-force approach. Therefore, in this paper, we first propose hardware-agnostic metric to measure the computational cost. And using the metric, we demonstrate that Pareto-optimal performance, where the best accuracy is obtained at a given computational cost, is achieved when a slim model with smaller number of parameters is quantized moderately rather than a fat model with huge number of parameters is quantized to extremely low bit precision such as binary or ternary. Furthermore, we empirically found the possible quantitative relation between the proposed metric and the signal to noise ratio during SGD training, by which the information obtained during SGD training provides the optimal policy of quantization and pruning. We show the Pareto frontier is improved by 4 times in post-training quantization scenario based on these findings. These findings are available not only to improve the Pareto frontier for accuracy vs. computational cost, but also give us some new insights on deep neural network.", "pdf": "/pdf/739e3925990713d557e2dbf1739383585bdf0c7c.pdf", "paperhash": "nakata|prune_or_quantize_strategy_for_paretooptimally_lowcost_and_accurate_cnn", "original_pdf": "/attachment/276428aecdbce6ccfd6e99352520aaa47b98b95a.pdf", "_bibtex": "@misc{\nnakata2020prune,\ntitle={Prune or quantize? Strategy for Pareto-optimally low-cost and accurate {\\{}CNN{\\}}},\nauthor={Kengo Nakata and Daisuke Miyashita and Asuka Maki and Fumihiko Tachibana and Shinichi Sasaki and Jun Deguchi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxAS6VFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxAS6VFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference/Paper542/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper542/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper542/Reviewers", "ICLR.cc/2020/Conference/Paper542/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper542/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper542/Authors|ICLR.cc/2020/Conference/Paper542/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169895, "tmdate": 1576860562158, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference/Paper542/Reviewers", "ICLR.cc/2020/Conference/Paper542/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper542/-/Official_Comment"}}}, {"id": "ryl--I85sB", "original": null, "number": 6, "cdate": 1573705209010, "ddate": null, "tcdate": 1573705209010, "tmdate": 1573705209010, "tddate": null, "forum": "HkxAS6VFDB", "replyto": "rygKkH8qir", "invitation": "ICLR.cc/2020/Conference/Paper542/-/Official_Comment", "content": {"title": "Response to Reviewer 2 (3/3)", "comment": "# # # continued from previous comment (2/3) # # #\n\nComment 5\n>- Developing a new strategy based on the proposed metric to quantize and prune a network in a Pareto optimal sense: This is briefly and not very well explained in section 2.7, which sends back to 2.3, but it is hard to understand how it is exactly done. It seems that section 3 provides some empirical evidence supporting this strategy, but the description of the method is hidden in the experimental details.\n\nThank you for this comment. We agree that it should be explained in detail with more experimental results. In appendix B, we have shown a preliminary experimental result. We would appreciate it if you refer to that. Besides, we apologize for making you misunderstanding. The quantization policy mentioned in section 2.7 is not used for the experiment in section 3. Although section 2.7 is not the main body of our paper, we would like to share the interesting result showing one of the ESN-based potential applications. If you don\u2019t think that section 2.7 may fit in the main topic of our paper, we consider moving the contents of section 2.7 to appendix. We would appreciate it if you could provide your comments or suggestions as one of our reviewers.\n\n\nSome questions:\nQuestion 1-(1)\n> In figure 3, the blue dots represent validation vs ESN at each training iteration? What about the red plot, is it obtained by quantization of the parameters at different stages of training, or is it using the final parameters? Which equation was used to compute the red curve (2) or (4)?\n\nAnswer 1-(1)\nWe are sorry for the lack of explanations. In figure 3, the blue dots represent validation vs ESN at each training iteration. The red plot is obtained by quantization of the final parameters. We used equation (2) to compute the red curves as we mentioned in the caption of figure 3. We expect the similar plot can be obtained by using equation (4) if quantization noise is approximated by uniform distribution (please refer to appendix E for details).\n\n\nQuestion 1-(2)\n> How much quantization was performed? If the quantization was chosen to match the level of noise then it seems natural to expect such behavior in figure 3.\n\nAnswer 1-(2)\nThe procedure to have figure 3 is as follows. We sweep the quantization step (\\varDelta) as we define in the first paragraph of 4th page. When we set the quantization step to a value and quantize weight parameters, we can obtain the value of ESN_a by using equation (2). Then, we evaluate the validation accuracy of quantized model. We sweep the quantization step and plot the values of ESN_a and validation accuracy on the graph as red curves in figure 3. Interestingly, models during training and after quantization achieve similar validation accuracy at the same value of ESN_a, though the noise of each ESN_a is computed based on different concepts such as weight perturbation during SGD training and quantization noise.\n\n\nQuestion 2\n>- In figure 4, how much pruning was performed for each network and was it the same quantization? In other words how each point in the plot was obtained? The authors come to the conclusion that one should 'prune until the limit of the desired accuracy and then quantize', but it is hard for me to reach the same conclusions as I don't see the separate effect of pruning and quantization in those figures. Or maybe pruning is implicitly done by choosing a small network? In this case, it makes more sense, but still, some clarifications are needed.\n\nAnswer 2 \nWe apologize for the lack of clarifications in this part. In section 2.4, as you guessed, we employ an originally small network as a slim model instead of pruning a fat model. We added the sentence to the updated manuscript in the first paragraph of section 2.4, as follows.\n\n\u201cIn this experiment, we employ networks with originally small number of parameters as slim models instead of pruning fat models.\u201d\n\n\nQuestion 3\n>- Which equation for the ESN was used to produce figure 5? Equation (2) or (4)?\n\nAnswer 3\nWe are sorry for inadequate explanations. We used equation (4) and (5) to calculate ESN_a in figure 5. We added the sentence to the caption of figure 5 as follows, and uploaded the modified manuscript.\n\n\u201cESN_a is calculated by equation (4) and (5).\u201d\n\n# # # continued to next # # #"}, "signatures": ["ICLR.cc/2020/Conference/Paper542/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prune or quantize? Strategy for Pareto-optimally low-cost and accurate CNN", "authors": ["Kengo Nakata", "Daisuke Miyashita", "Asuka Maki", "Fumihiko Tachibana", "Shinichi Sasaki", "Jun Deguchi"], "authorids": ["kengo1.nakata@toshiba.co.jp", "daisuke1.miyashita@toshiba.co.jp", "asuka.maki@toshiba.co.jp", "fumihiko.tachibana@toshiba.co.jp", "shinichi8.sasaki@toshiba.co.jp", "jun.deguchi@toshiba.co.jp"], "keywords": ["CNN", "Quantization", "Pruning", "Accelerator", "Computational cost"], "TL;DR": "This paper reveals that \"prune-then-quantize method\" is the best strategy to achieve Pareto-optimal performance by using a proposed hardware-agnostic metric to measure computational cost.", "abstract": "Pruning and quantization are typical approaches to reduce the computational cost of CNN inference. Although the idea to combine them together seems natural, it is being unexpectedly difficult to figure out the resultant effect of the combination unless measuring the performance on a certain hardware which a user is going to use. This is because the benefits of pruning and quantization strongly depend on the hardware architecture where the model is executed. For example, a CPU-like architecture without any parallelization may fully exploit the reduction of computations by unstructured pruning for speeding up, but a GPU-like massive parallel architecture would not. Besides, there have been emerging proposals of novel hardware architectures such as one supporting variable bit precision quantization. From an engineering viewpoint, optimization for each hardware architecture is useful and important in practice, but this is quite a brute-force approach. Therefore, in this paper, we first propose hardware-agnostic metric to measure the computational cost. And using the metric, we demonstrate that Pareto-optimal performance, where the best accuracy is obtained at a given computational cost, is achieved when a slim model with smaller number of parameters is quantized moderately rather than a fat model with huge number of parameters is quantized to extremely low bit precision such as binary or ternary. Furthermore, we empirically found the possible quantitative relation between the proposed metric and the signal to noise ratio during SGD training, by which the information obtained during SGD training provides the optimal policy of quantization and pruning. We show the Pareto frontier is improved by 4 times in post-training quantization scenario based on these findings. These findings are available not only to improve the Pareto frontier for accuracy vs. computational cost, but also give us some new insights on deep neural network.", "pdf": "/pdf/739e3925990713d557e2dbf1739383585bdf0c7c.pdf", "paperhash": "nakata|prune_or_quantize_strategy_for_paretooptimally_lowcost_and_accurate_cnn", "original_pdf": "/attachment/276428aecdbce6ccfd6e99352520aaa47b98b95a.pdf", "_bibtex": "@misc{\nnakata2020prune,\ntitle={Prune or quantize? Strategy for Pareto-optimally low-cost and accurate {\\{}CNN{\\}}},\nauthor={Kengo Nakata and Daisuke Miyashita and Asuka Maki and Fumihiko Tachibana and Shinichi Sasaki and Jun Deguchi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxAS6VFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxAS6VFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference/Paper542/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper542/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper542/Reviewers", "ICLR.cc/2020/Conference/Paper542/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper542/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper542/Authors|ICLR.cc/2020/Conference/Paper542/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169895, "tmdate": 1576860562158, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference/Paper542/Reviewers", "ICLR.cc/2020/Conference/Paper542/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper542/-/Official_Comment"}}}, {"id": "rygKkH8qir", "original": null, "number": 5, "cdate": 1573704929061, "ddate": null, "tcdate": 1573704929061, "tmdate": 1573704929061, "tddate": null, "forum": "HkxAS6VFDB", "replyto": "H1lMKQL5oB", "invitation": "ICLR.cc/2020/Conference/Paper542/-/Official_Comment", "content": {"title": "Response to Reviewer 2 (2/3)", "comment": "# # # continued from previous comment (1/3) # # #\n\n Comment 3\n> - Evolution of the proposed metric during training: 2.3 and 2.5. While the 2.3, the take-home message is relatively clear: the ESN is correlated with the validation accuracy, I don't fully get the point of section 2.5: It suggests that the optimizer does some sort of pruning just by choosing a higher learning rate.\n\nFirstly, we explain the message of section 2.5. We apologize that the message of section 2.5 is unclear. In the experiment of section 2.5, we observed how weight parameters were optimized by SGD under low ESN condition. Based on the equation (4), following two conditions are possible under low ESN condition. \n\n(i)\tA lot of small S/Ns are accumulated.      (e.g. S/N_i=0.1, i=1,2,,\u2026,10)\n(ii)\tA few large S/Ns are accumulated. \t     (e.g. S/N_1=1, S/N_i=0, i=2,,\u2026,10)\n\nHere, S/N represents signal-to-noise ratio. The condition of (i) corresponds to a fat model with a huge number of parameters quantized to extremely low bit width, and the condition of (ii) corresponds to a slim model with a small number of parameters quantized to moderately low bit width. Interestingly, when we see the right graph in figure 5 showing solutions found by the optimizer, the optimizer seems to give priority to reducing the number of parameters by pruning filters and increasing the amplitude of remaining parameters, which corresponds to the condition of (ii). This finding gave a hint for us to conclude that models with fewer parameters achieve far better accuracy in the region of low computational cost after quantization. We understand that to unveil the reason of this phenomenon is also necessary as future work.\n\nSecondly, we explain pruning filters by the optimizer, as a supplement.\nIn the experiment of section 2.5, we impose weight decay regularization on weights parameters during training. According to a previous work [5], the constraint such as L1 or L2 norm regularization has an effect of sparsity constraint on the weights. The effect of weight decay is determined by learning rate as well as decay coefficient, following the equation(*) of weight update.\n W^(t+1) = W^(t) \u2013 lr * {dL/dW^(t) + dc * W^(t)}\t(*)\nHere, lr represents learning rate, dc represents decay coefficient of weight decay, and L represents loss function. If the learning rate is higher, the decay effect will be larger and the effect of sparsity constraint will be also larger. As the result, in the right graph of figure 5, the higher the learning rate is, the larger the number of pruned filters is. \n\nReference\n[5] Y. Wang et al, \u201cNonstructured DNN weight pruning considered harmful\u201d, CoRR, abs/1907.02124, 2019b. URL: http://arxiv.org/abs/1907.02124.\n(We have referred this paper as \u201cWang et al., 2019b\u201d in the first paragraph in section 2.6.)\n\n\nComment 4\n> - Finding an optimal strategy for pruning/quantizing a network: 2.4 and 2.6. Those two sections are relatively clear, although I have some questions about the experiments.\n\nWe apologize for the lack of experimental setup such as network architecture in section 2.4 and 2.6. We added the information as follows and updated the manuscript.\n\nSection 2.4\nWe added the information about network architectures of VGG7 in Appendix F, and modified the second to fifth sentences in the first paragraph as follows. \n(Before): We evaluate 6 network architectures with different width and depth on CIFAR-10 dataset.\n(After): We evaluate 6 network architectures with different depth and width: ResNet-8, 14, 20 and VGG7 with different channel width (1x, 0.5x, 0.25x) on CIFAR-10 dataset.\nIn this experiment, we employ networks with originally small number of parameters as slim models instead of pruning fat models.\nThe network architecture of ResNet is based on the original one without bottleneck architecture (He et al., 2016). The detailed network architecture of VGG7 is shown in Appendix F. \n\nSection 2.6\nWe modified the first sentence in the second paragraph as follows.\n(Before): In this experiment, we take pretrained weight parameters and update them through SGD algorithm with ADMM regularization (Wang et al., 2019b) for a few additional epochs.\n(After): In this experiment, we evaluate ResNet-20 on CIFAR-10 dataset. We take pretrained weight parameters and update them through SGD algorithm with ADMM regularization (Wang et al., 2019b) for a few additional epochs.\n\n# # # continued to next comment # # # "}, "signatures": ["ICLR.cc/2020/Conference/Paper542/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prune or quantize? Strategy for Pareto-optimally low-cost and accurate CNN", "authors": ["Kengo Nakata", "Daisuke Miyashita", "Asuka Maki", "Fumihiko Tachibana", "Shinichi Sasaki", "Jun Deguchi"], "authorids": ["kengo1.nakata@toshiba.co.jp", "daisuke1.miyashita@toshiba.co.jp", "asuka.maki@toshiba.co.jp", "fumihiko.tachibana@toshiba.co.jp", "shinichi8.sasaki@toshiba.co.jp", "jun.deguchi@toshiba.co.jp"], "keywords": ["CNN", "Quantization", "Pruning", "Accelerator", "Computational cost"], "TL;DR": "This paper reveals that \"prune-then-quantize method\" is the best strategy to achieve Pareto-optimal performance by using a proposed hardware-agnostic metric to measure computational cost.", "abstract": "Pruning and quantization are typical approaches to reduce the computational cost of CNN inference. Although the idea to combine them together seems natural, it is being unexpectedly difficult to figure out the resultant effect of the combination unless measuring the performance on a certain hardware which a user is going to use. This is because the benefits of pruning and quantization strongly depend on the hardware architecture where the model is executed. For example, a CPU-like architecture without any parallelization may fully exploit the reduction of computations by unstructured pruning for speeding up, but a GPU-like massive parallel architecture would not. Besides, there have been emerging proposals of novel hardware architectures such as one supporting variable bit precision quantization. From an engineering viewpoint, optimization for each hardware architecture is useful and important in practice, but this is quite a brute-force approach. Therefore, in this paper, we first propose hardware-agnostic metric to measure the computational cost. And using the metric, we demonstrate that Pareto-optimal performance, where the best accuracy is obtained at a given computational cost, is achieved when a slim model with smaller number of parameters is quantized moderately rather than a fat model with huge number of parameters is quantized to extremely low bit precision such as binary or ternary. Furthermore, we empirically found the possible quantitative relation between the proposed metric and the signal to noise ratio during SGD training, by which the information obtained during SGD training provides the optimal policy of quantization and pruning. We show the Pareto frontier is improved by 4 times in post-training quantization scenario based on these findings. These findings are available not only to improve the Pareto frontier for accuracy vs. computational cost, but also give us some new insights on deep neural network.", "pdf": "/pdf/739e3925990713d557e2dbf1739383585bdf0c7c.pdf", "paperhash": "nakata|prune_or_quantize_strategy_for_paretooptimally_lowcost_and_accurate_cnn", "original_pdf": "/attachment/276428aecdbce6ccfd6e99352520aaa47b98b95a.pdf", "_bibtex": "@misc{\nnakata2020prune,\ntitle={Prune or quantize? Strategy for Pareto-optimally low-cost and accurate {\\{}CNN{\\}}},\nauthor={Kengo Nakata and Daisuke Miyashita and Asuka Maki and Fumihiko Tachibana and Shinichi Sasaki and Jun Deguchi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxAS6VFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxAS6VFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference/Paper542/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper542/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper542/Reviewers", "ICLR.cc/2020/Conference/Paper542/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper542/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper542/Authors|ICLR.cc/2020/Conference/Paper542/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169895, "tmdate": 1576860562158, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference/Paper542/Reviewers", "ICLR.cc/2020/Conference/Paper542/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper542/-/Official_Comment"}}}, {"id": "H1lMKQL5oB", "original": null, "number": 4, "cdate": 1573704570209, "ddate": null, "tcdate": 1573704570209, "tmdate": 1573704570209, "tddate": null, "forum": "HkxAS6VFDB", "replyto": "rylzNrlaYB", "invitation": "ICLR.cc/2020/Conference/Paper542/-/Official_Comment", "content": {"title": "Response to Reviewer 2 (1/3)", "comment": "Thank you for detailed comments and questions. We reply to your comments and questions as follows.\n\nComment 1\n>Cons: - The paper is not very clear, and the structure is somehow confusing.\n>- It is not easy at first to understand the experimental setup and requires to make a lot of guesses in my opinion.\n\nThank you for kind feedbacks on our paper. We would add the information about experimental setup.\nWe assume the main reasons why the structure is confusing are, as you pointed out,\n1.\tThe message of section 2.5 is not clear.\n2.\tThe discussions in section 2.7 (and section 2.3) interrupt the flow of the body contents of this paper.\nLet us explain some improvement plans below.\n\n\nComment 2\u3000\n> - The paper didn't motivate properly the use of a hardware-agnostic metric in the context of the quantization and pruning. Isn't the ultimate goal of pruning/quantization is to optimize the run time/energy consumption of the specific device with the least compromise on the accuracy?\n\nPerformances measured using existing hardware such as CPUs and GPUs cannot evaluate some cutting edge ideas proposed by researchers in algorithmic side, e.g. 1 or 2 bits quantization. For example, as discussed in section 2, the energy consumption of CPUs and GPUs does not decrease even when the bit width is reduced to 1 or 2 bits by quantization or when the number of non-zero weight parameters is reduced by pruning.\n\nOur motivation is to evaluate such ideas properly.\n\nMany researches of new hardware architecture handling the both quantization and pruning are ongoing [1], [2], [3], [4]. For instance, previous works [1], [2] proposed accelerators which support variable weight bit precision from 1 to 16 bit and achieve higher energy efficiency with lower bit precision by quantization. \nIn addition, previous works [3], [4] proposed sparsity-aware accelerators which support sparse convolution and achieve higher energy efficiency with higher rate of zero-weights by pruning. \n\nFollowing the appearance of such dedicated accelerators, we expect that accelerators dedicated to both quantization and pruning will appear (or rather should be developed) in the near future.\n\nOur proposed metric allows us to estimate the performance when a quantized and pruned model runs on such devices that support both quantization and pruning.\n\nWe agree that it is important to fine-tune models to optimize the computational cost of existing specific devices, and we should do that even after novel devices that support both quantization and pruning are developed.\n\nWe believe, however, we also need such metric as our proposal for making sure that the both algorithmic and hardware architecture researches are on the same page for future emerging devices.\n\nReferences\n[1] A. Maki et al., \u201cFPGA-based CNN processor with filter-wise-optimized bit precision\u201d, In 2018 IEEE Asian Solid-State Circuits Conference (A-SSCC), pp. 47\u201350, Nov 2018. doi: 10.1109/ASSCC.2018.8579342.\n(We have referred this paper as \u201cMaki et al., 2018\u201d in the original manuscript.)\n\n[2] J. Lee et al., \u201cUNPU: A 50.6TOPS/W Unified Deep Neural Network Accelerator with 1b-to-16b Fully-Variable Weight Bit-Precision\u201d, In 2018 IEEE International Solid State Circuits Conference (ISSCC), pp. 218-220, Feb 2018. doi: 10.1109/ISSCC.2018.8310262.\n\n[3] Z. Yuan et al., \u201cSTICKER: A 0.41-62.1 TOPS/W 8bit Neural Network Processor with Multi-Sparsity Compatible Convolution Arrays and Online Tuning Acceleration for Fully Connected Layers\u201d, In 2018 IEEE Symposium on VLSI Circuits, pp. 33-34, Jun 2018. doi: 10.1109/VLSIC.2018.8502404.\n\n[4] J. Song et al., \u201cAn 11.5TOPS/W 1024-MAC Butterfly Structure Dual-Core Sparsity-Aware Neural Processing Unit in 8nm Flagship Mobile SoC\u201d, In 2019 IEEE International Solid State Circuits Conference (ISSCC), pp. 130-132, Feb 2019. doi: 10.1109/ISSCC.2019.8662476.\n\n# # # continued to next comment # # # "}, "signatures": ["ICLR.cc/2020/Conference/Paper542/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prune or quantize? Strategy for Pareto-optimally low-cost and accurate CNN", "authors": ["Kengo Nakata", "Daisuke Miyashita", "Asuka Maki", "Fumihiko Tachibana", "Shinichi Sasaki", "Jun Deguchi"], "authorids": ["kengo1.nakata@toshiba.co.jp", "daisuke1.miyashita@toshiba.co.jp", "asuka.maki@toshiba.co.jp", "fumihiko.tachibana@toshiba.co.jp", "shinichi8.sasaki@toshiba.co.jp", "jun.deguchi@toshiba.co.jp"], "keywords": ["CNN", "Quantization", "Pruning", "Accelerator", "Computational cost"], "TL;DR": "This paper reveals that \"prune-then-quantize method\" is the best strategy to achieve Pareto-optimal performance by using a proposed hardware-agnostic metric to measure computational cost.", "abstract": "Pruning and quantization are typical approaches to reduce the computational cost of CNN inference. Although the idea to combine them together seems natural, it is being unexpectedly difficult to figure out the resultant effect of the combination unless measuring the performance on a certain hardware which a user is going to use. This is because the benefits of pruning and quantization strongly depend on the hardware architecture where the model is executed. For example, a CPU-like architecture without any parallelization may fully exploit the reduction of computations by unstructured pruning for speeding up, but a GPU-like massive parallel architecture would not. Besides, there have been emerging proposals of novel hardware architectures such as one supporting variable bit precision quantization. From an engineering viewpoint, optimization for each hardware architecture is useful and important in practice, but this is quite a brute-force approach. Therefore, in this paper, we first propose hardware-agnostic metric to measure the computational cost. And using the metric, we demonstrate that Pareto-optimal performance, where the best accuracy is obtained at a given computational cost, is achieved when a slim model with smaller number of parameters is quantized moderately rather than a fat model with huge number of parameters is quantized to extremely low bit precision such as binary or ternary. Furthermore, we empirically found the possible quantitative relation between the proposed metric and the signal to noise ratio during SGD training, by which the information obtained during SGD training provides the optimal policy of quantization and pruning. We show the Pareto frontier is improved by 4 times in post-training quantization scenario based on these findings. These findings are available not only to improve the Pareto frontier for accuracy vs. computational cost, but also give us some new insights on deep neural network.", "pdf": "/pdf/739e3925990713d557e2dbf1739383585bdf0c7c.pdf", "paperhash": "nakata|prune_or_quantize_strategy_for_paretooptimally_lowcost_and_accurate_cnn", "original_pdf": "/attachment/276428aecdbce6ccfd6e99352520aaa47b98b95a.pdf", "_bibtex": "@misc{\nnakata2020prune,\ntitle={Prune or quantize? Strategy for Pareto-optimally low-cost and accurate {\\{}CNN{\\}}},\nauthor={Kengo Nakata and Daisuke Miyashita and Asuka Maki and Fumihiko Tachibana and Shinichi Sasaki and Jun Deguchi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxAS6VFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxAS6VFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference/Paper542/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper542/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper542/Reviewers", "ICLR.cc/2020/Conference/Paper542/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper542/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper542/Authors|ICLR.cc/2020/Conference/Paper542/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169895, "tmdate": 1576860562158, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference/Paper542/Reviewers", "ICLR.cc/2020/Conference/Paper542/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper542/-/Official_Comment"}}}, {"id": "rylP-pcvor", "original": null, "number": 3, "cdate": 1573526783204, "ddate": null, "tcdate": 1573526783204, "tmdate": 1573526783204, "tddate": null, "forum": "HkxAS6VFDB", "replyto": "HkxAS6VFDB", "invitation": "ICLR.cc/2020/Conference/Paper542/-/Official_Comment", "content": {"title": "Upload the revised manuscript", "comment": "Thank all reviewers for insightful comments and feedback on our paper.\nWe uploaded the revised manuscript.\nWe revised the manuscript as follows.\n\n# We modified right graph in figure 2 and the caption as follows.\n\u30fb The range of vertical axis is changed to be narrow.\n\u30fb Moving average curves between 3-epochs are added over the original plots.\n* The data of original plots is \u201cnot changed\u201d.\n\u30fb We added the sentence: In the right graph, moving average curve between 3-epochs is overlapped on each plot, to the caption.\n\n# We added the information about network architectures in section 2.4 and 2.6.\n- the forth sentence in the first paragraph of Section 2.4\n- the first sentence in the second paragraph of Section 2.6\n# In the caption of figure 5, we added the explanation about calculation of ESN_a.\n# We added Appendix F for the more detailed information about VGG7.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper542/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prune or quantize? Strategy for Pareto-optimally low-cost and accurate CNN", "authors": ["Kengo Nakata", "Daisuke Miyashita", "Asuka Maki", "Fumihiko Tachibana", "Shinichi Sasaki", "Jun Deguchi"], "authorids": ["kengo1.nakata@toshiba.co.jp", "daisuke1.miyashita@toshiba.co.jp", "asuka.maki@toshiba.co.jp", "fumihiko.tachibana@toshiba.co.jp", "shinichi8.sasaki@toshiba.co.jp", "jun.deguchi@toshiba.co.jp"], "keywords": ["CNN", "Quantization", "Pruning", "Accelerator", "Computational cost"], "TL;DR": "This paper reveals that \"prune-then-quantize method\" is the best strategy to achieve Pareto-optimal performance by using a proposed hardware-agnostic metric to measure computational cost.", "abstract": "Pruning and quantization are typical approaches to reduce the computational cost of CNN inference. Although the idea to combine them together seems natural, it is being unexpectedly difficult to figure out the resultant effect of the combination unless measuring the performance on a certain hardware which a user is going to use. This is because the benefits of pruning and quantization strongly depend on the hardware architecture where the model is executed. For example, a CPU-like architecture without any parallelization may fully exploit the reduction of computations by unstructured pruning for speeding up, but a GPU-like massive parallel architecture would not. Besides, there have been emerging proposals of novel hardware architectures such as one supporting variable bit precision quantization. From an engineering viewpoint, optimization for each hardware architecture is useful and important in practice, but this is quite a brute-force approach. Therefore, in this paper, we first propose hardware-agnostic metric to measure the computational cost. And using the metric, we demonstrate that Pareto-optimal performance, where the best accuracy is obtained at a given computational cost, is achieved when a slim model with smaller number of parameters is quantized moderately rather than a fat model with huge number of parameters is quantized to extremely low bit precision such as binary or ternary. Furthermore, we empirically found the possible quantitative relation between the proposed metric and the signal to noise ratio during SGD training, by which the information obtained during SGD training provides the optimal policy of quantization and pruning. We show the Pareto frontier is improved by 4 times in post-training quantization scenario based on these findings. These findings are available not only to improve the Pareto frontier for accuracy vs. computational cost, but also give us some new insights on deep neural network.", "pdf": "/pdf/739e3925990713d557e2dbf1739383585bdf0c7c.pdf", "paperhash": "nakata|prune_or_quantize_strategy_for_paretooptimally_lowcost_and_accurate_cnn", "original_pdf": "/attachment/276428aecdbce6ccfd6e99352520aaa47b98b95a.pdf", "_bibtex": "@misc{\nnakata2020prune,\ntitle={Prune or quantize? Strategy for Pareto-optimally low-cost and accurate {\\{}CNN{\\}}},\nauthor={Kengo Nakata and Daisuke Miyashita and Asuka Maki and Fumihiko Tachibana and Shinichi Sasaki and Jun Deguchi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxAS6VFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxAS6VFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference/Paper542/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper542/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper542/Reviewers", "ICLR.cc/2020/Conference/Paper542/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper542/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper542/Authors|ICLR.cc/2020/Conference/Paper542/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169895, "tmdate": 1576860562158, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference/Paper542/Reviewers", "ICLR.cc/2020/Conference/Paper542/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper542/-/Official_Comment"}}}, {"id": "Hyge3qqDsr", "original": null, "number": 2, "cdate": 1573526183991, "ddate": null, "tcdate": 1573526183991, "tmdate": 1573526266597, "tddate": null, "forum": "HkxAS6VFDB", "replyto": "Syx2cK9wiS", "invitation": "ICLR.cc/2020/Conference/Paper542/-/Official_Comment", "content": {"title": "Response to Reviewer 4 (2/2)", "comment": "# # continued from previous comment (1/2) # # \n\nAs for the trade-off in 6th comment, we think ESN can make the trade-off between model size, accuracy and execution time or speed in model compression (detailed explanations to the reason is as follows[*]). On the other hand, the metric evaluated with existing hardware (e.g. execution time on CPU) cannot make such trade-off, because existing hardware cannot fully exploit the benefits of quantization and pruning.\n\n[*] Detailed explanations to the trade-off, mentioned in 6th comment\nIn our paper, the assumption for ESN definition is related to energy consumption that is consumed when the model is executed on the ideal hardware. Then, if ESN is reduced by pruning or quantization, the model size and energy consumption will decrease, and the accuracy will deteriorate (as you can see the tendency in figure 2, 3, and 4), and vice versa. Therefore, ESN can make the trade-off between model size, energy consumption and accuracy. \nHere, the assumption for ESN definition can be also related to execution time that the ideal hardware takes when the model is executed on the hardware. Then, ESN can make the trade-off between model size, accuracy and execution time (or speed), as is the case with the definition based on energy consumption.\n\nAs for clear applications of ENS in 6th comment, we applied ESN to evaluate models where both quantization and pruning were applied, in section 3. In addition, we expect ESN can be also applied to evaluate computational cost of accelerators dedicated to both quantization and pruning, which we believe will appear in the near future. Besides, we think evaluations by using ESN can be applied to determining quantization policy (as we mentioned in section 2.7 and appendix B). Detailed experiments and considerations are left for future work. \n\nReferences\n[1] A. Maki et al., \u201cFPGA-based CNN processor with filter-wise-optimized bit precision\u201d, In 2018 IEEE Asian Solid-State Circuits Conference (A-SSCC), pp. 47\u201350, Nov 2018. doi: 10.1109/ASSCC.2018.8579342.\n(We have referred this paper as (Maki et al., 2018) in the original manuscript.)\n\n[2] J. Lee et al., \u201cUNPU: A 50.6TOPS/W Unified Deep Neural Network Accelerator with 1b-to-16b Fully-Variable Weight Bit-Precision\u201d, In 2018 IEEE International Solid State Circuits Conference (ISSCC), pp. 218-220, Feb 2018. doi: 10.1109/ISSCC.2018.8310262.\n\n[3] Z. Yuan et al., \u201cSTICKER: A 0.41-62.1 TOPS/W 8bit Neural Network Processor with Multi-Sparsity Compatible Convolution Arrays and Online Tuning Acceleration for Fully Connected Layers\u201d, In 2018 IEEE Symposium on VLSI Circuits, pp. 33-34, Jun 2018. doi: 10.1109/VLSIC.2018.8502404.\n\n[4] J. Song et al., \u201cAn 11.5TOPS/W 1024-MAC Butterfly Structure Dual-Core Sparsity-Aware Neural Processing Unit in 8nm Flagship Mobile SoC\u201d, In 2019 IEEE International Solid State Circuits Conference (ISSCC), pp. 130-132, Feb 2019. doi: 10.1109/ISSCC.2019.8662476.\n\n\nHere, we reply to 4th, 5th comments and minor issues.\n\n> 4. Please give more details about the quantitative relation between the quantization noise and the perturbation of weight parameters during SGD training.\n\nWe are sorry that this part was inadequate in the original manuscript. We agree that we should show more detailed experimental results about the \u201cquantitative\u201d relation between the quantization noise and the perturbation of weight parameters during SGD training. If necessary, we would modify the word of \u201cquantitative\u201d to more appropriate description, or delete it.\n\n> 5. In terms of Figure 2, the author's description is not objective. The similarity between the ESN_a curves and the validation accuracy curves is not very high. Besides, the authors mention that \u201cafter steep rise at 80 epoch, both ESN_a and validation accuracy decrease gradually\u201d. However, the performance degradation in Figure 2 is not obvious. Therefore, the conclusions drew by the observation are also unreliable.\n\nWe apologize for unclear graphs in figure 2 about validation accuracy and ESN_a. We revised the graphs as follows, and updated the manuscript.\n\u30fb The range of vertical axis is changed to be narrow.\n\u30fb Moving average curves between 3-epochs are added over the original plots.\n* The data of original plots is not changed.\n\n> Minor issues:\n> 1. There are lots of spelling and grammar mistakes in the paper, such as \u201can ideal hardware \u201d, \u201ca extremely quantized fat model\u201d and so on.\n\nAccording to minor issues such as spelling and grammar mistakes in our paper, we will have it proofread by native speakers and modify them before uploading again the revised manuscript by the end of discussion term (15th Nov.)."}, "signatures": ["ICLR.cc/2020/Conference/Paper542/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prune or quantize? Strategy for Pareto-optimally low-cost and accurate CNN", "authors": ["Kengo Nakata", "Daisuke Miyashita", "Asuka Maki", "Fumihiko Tachibana", "Shinichi Sasaki", "Jun Deguchi"], "authorids": ["kengo1.nakata@toshiba.co.jp", "daisuke1.miyashita@toshiba.co.jp", "asuka.maki@toshiba.co.jp", "fumihiko.tachibana@toshiba.co.jp", "shinichi8.sasaki@toshiba.co.jp", "jun.deguchi@toshiba.co.jp"], "keywords": ["CNN", "Quantization", "Pruning", "Accelerator", "Computational cost"], "TL;DR": "This paper reveals that \"prune-then-quantize method\" is the best strategy to achieve Pareto-optimal performance by using a proposed hardware-agnostic metric to measure computational cost.", "abstract": "Pruning and quantization are typical approaches to reduce the computational cost of CNN inference. Although the idea to combine them together seems natural, it is being unexpectedly difficult to figure out the resultant effect of the combination unless measuring the performance on a certain hardware which a user is going to use. This is because the benefits of pruning and quantization strongly depend on the hardware architecture where the model is executed. For example, a CPU-like architecture without any parallelization may fully exploit the reduction of computations by unstructured pruning for speeding up, but a GPU-like massive parallel architecture would not. Besides, there have been emerging proposals of novel hardware architectures such as one supporting variable bit precision quantization. From an engineering viewpoint, optimization for each hardware architecture is useful and important in practice, but this is quite a brute-force approach. Therefore, in this paper, we first propose hardware-agnostic metric to measure the computational cost. And using the metric, we demonstrate that Pareto-optimal performance, where the best accuracy is obtained at a given computational cost, is achieved when a slim model with smaller number of parameters is quantized moderately rather than a fat model with huge number of parameters is quantized to extremely low bit precision such as binary or ternary. Furthermore, we empirically found the possible quantitative relation between the proposed metric and the signal to noise ratio during SGD training, by which the information obtained during SGD training provides the optimal policy of quantization and pruning. We show the Pareto frontier is improved by 4 times in post-training quantization scenario based on these findings. These findings are available not only to improve the Pareto frontier for accuracy vs. computational cost, but also give us some new insights on deep neural network.", "pdf": "/pdf/739e3925990713d557e2dbf1739383585bdf0c7c.pdf", "paperhash": "nakata|prune_or_quantize_strategy_for_paretooptimally_lowcost_and_accurate_cnn", "original_pdf": "/attachment/276428aecdbce6ccfd6e99352520aaa47b98b95a.pdf", "_bibtex": "@misc{\nnakata2020prune,\ntitle={Prune or quantize? Strategy for Pareto-optimally low-cost and accurate {\\{}CNN{\\}}},\nauthor={Kengo Nakata and Daisuke Miyashita and Asuka Maki and Fumihiko Tachibana and Shinichi Sasaki and Jun Deguchi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxAS6VFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxAS6VFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference/Paper542/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper542/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper542/Reviewers", "ICLR.cc/2020/Conference/Paper542/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper542/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper542/Authors|ICLR.cc/2020/Conference/Paper542/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169895, "tmdate": 1576860562158, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference/Paper542/Reviewers", "ICLR.cc/2020/Conference/Paper542/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper542/-/Official_Comment"}}}, {"id": "Syx2cK9wiS", "original": null, "number": 1, "cdate": 1573525907872, "ddate": null, "tcdate": 1573525907872, "tmdate": 1573525907872, "tddate": null, "forum": "HkxAS6VFDB", "replyto": "S1lnx65qqS", "invitation": "ICLR.cc/2020/Conference/Paper542/-/Official_Comment", "content": {"title": "Response to Reviewer 4 (1/2)", "comment": "Thank you for detailed comments and insightful feedback on our paper.\nIn the following, we reply to your comments, especially relating to negative points.\n\n> Negative Points:\n> 1. The authors argue that ESN is a hardware-agnostic metric. However, ESN is based on ideal hardware, where the energy consumption is linearly proportional to the number of non-zero weight parameters and\u3000monotonically depends on the bit-width of weight parameters. Therefore, ESN is not suitable for existing hardware.\n> 2. Both of ESN_a and ESN_d are based on the assumptions instead of the fact, which is not convincing. What\u2019s more, the assumptions are hard to be proved.\n> 3. The experiments are not strong enough to support the author's conclusions. For example, the authors argue that \u201cmodels with fewer parameters achieve far better accuracy in low computational cost region after quantization\u201d. However, this conclusion is only based on the ESN metric. Since ESN is based on some assumptions, this conclusion is not convincing\n> 6. The ESN cannot make an accurate evaluation of a model. The trade-off between accuracy, speed, model size is often required in model compression. However, the ESN cannot make the trade-off between them. \n\nIn the beginning, we reply to 1st, 2nd, 3rd and 6th negative comments together.\n\nAlthough 1st to 3rd negative comments are based on a skeptical view about adopting a \"virtual\" metric of ESN to evaluate models instead of performances such as speed measured using existing hardware, 6th negative comment is exactly relevant to the motivation for adopting the \u201cvirtual\u201d metric of ESN.\n\nFor example, as discussed in section 2, the energy consumption of CPUs or GPUs, which are the most widely utilized existing hardware for general purposes, does not decrease even when the bit width is reduced to 1 or 2 bits by quantization or when the number of non-zero weight parameters is reduced by pruning. In other words, they cannot fully exploit the benefits of quantization and pruning.\n\nIt is clear that we should not use performances such as energy consumption and speed for evaluating models where quantization or pruning is applied. One reason is that such metric evaluated with existing hardware cannot make the trade-off between accuracy, speed, model size (this is the challenge as you mentioned in the 6th comment). \n\nActually, a lot of hardware architecture dedicated to either quantization or pruning have been proposed recently. For example, previous works [1], [2] proposed accelerators which support variable weight bit precision from 1 to 16 bit and achieve higher energy efficiency with lower bit precision by quantization. The previous work [2] shows an experimental result that the energy efficiency is improved from 3.08 to 11.6 [TOPS/W] when the bit width is reduced from 16 to 4 bit by quantization.\nIn addition, previous works [3], [4] proposed sparsity-aware accelerators which support sparse convolution and achieve higher energy efficiency with higher rate of zero-weights by pruning. The previous work [4] shows the energy efficiency is improved from 0.41 to 62.1 [TOPS/W] when the sparse rate of weights and activations is changed from 5 to 95 %.\n\nThe assumption for ESN is derived from these facts. Also, following the appearance of such dedicated accelerators, we expect that accelerators dedicated to both quantization and pruning will appear (or rather should be developed) in the near future, and that those accelerators can satisfy our assumption.\n\nAs for 1st comment, it is true that \"ESN is not suitable for existing hardware\", because our target is not existing hardware but more properly designed hardware that should appear in the future.\n\nAs for 2nd comment, we believe our assumption that the energy consumption is linearly proportional to the number of non-zero weight parameters and monotonically depends on the bit-width of weight parameter is reasonable. There are many supporting works such as [1], [2], [3] and [4], which have shown experimental results that the energy consumption is reduced and energy efficiency is improved if the data size for model parameters is reduced by quantization or pruning. \n\nAs for 3rd comment, since the assumption is based on those facts, the conclusion based on the evaluation using ESN should be convincing.\n\n# # continued to next comment # # "}, "signatures": ["ICLR.cc/2020/Conference/Paper542/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prune or quantize? Strategy for Pareto-optimally low-cost and accurate CNN", "authors": ["Kengo Nakata", "Daisuke Miyashita", "Asuka Maki", "Fumihiko Tachibana", "Shinichi Sasaki", "Jun Deguchi"], "authorids": ["kengo1.nakata@toshiba.co.jp", "daisuke1.miyashita@toshiba.co.jp", "asuka.maki@toshiba.co.jp", "fumihiko.tachibana@toshiba.co.jp", "shinichi8.sasaki@toshiba.co.jp", "jun.deguchi@toshiba.co.jp"], "keywords": ["CNN", "Quantization", "Pruning", "Accelerator", "Computational cost"], "TL;DR": "This paper reveals that \"prune-then-quantize method\" is the best strategy to achieve Pareto-optimal performance by using a proposed hardware-agnostic metric to measure computational cost.", "abstract": "Pruning and quantization are typical approaches to reduce the computational cost of CNN inference. Although the idea to combine them together seems natural, it is being unexpectedly difficult to figure out the resultant effect of the combination unless measuring the performance on a certain hardware which a user is going to use. This is because the benefits of pruning and quantization strongly depend on the hardware architecture where the model is executed. For example, a CPU-like architecture without any parallelization may fully exploit the reduction of computations by unstructured pruning for speeding up, but a GPU-like massive parallel architecture would not. Besides, there have been emerging proposals of novel hardware architectures such as one supporting variable bit precision quantization. From an engineering viewpoint, optimization for each hardware architecture is useful and important in practice, but this is quite a brute-force approach. Therefore, in this paper, we first propose hardware-agnostic metric to measure the computational cost. And using the metric, we demonstrate that Pareto-optimal performance, where the best accuracy is obtained at a given computational cost, is achieved when a slim model with smaller number of parameters is quantized moderately rather than a fat model with huge number of parameters is quantized to extremely low bit precision such as binary or ternary. Furthermore, we empirically found the possible quantitative relation between the proposed metric and the signal to noise ratio during SGD training, by which the information obtained during SGD training provides the optimal policy of quantization and pruning. We show the Pareto frontier is improved by 4 times in post-training quantization scenario based on these findings. These findings are available not only to improve the Pareto frontier for accuracy vs. computational cost, but also give us some new insights on deep neural network.", "pdf": "/pdf/739e3925990713d557e2dbf1739383585bdf0c7c.pdf", "paperhash": "nakata|prune_or_quantize_strategy_for_paretooptimally_lowcost_and_accurate_cnn", "original_pdf": "/attachment/276428aecdbce6ccfd6e99352520aaa47b98b95a.pdf", "_bibtex": "@misc{\nnakata2020prune,\ntitle={Prune or quantize? Strategy for Pareto-optimally low-cost and accurate {\\{}CNN{\\}}},\nauthor={Kengo Nakata and Daisuke Miyashita and Asuka Maki and Fumihiko Tachibana and Shinichi Sasaki and Jun Deguchi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxAS6VFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxAS6VFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference/Paper542/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper542/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper542/Reviewers", "ICLR.cc/2020/Conference/Paper542/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper542/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper542/Authors|ICLR.cc/2020/Conference/Paper542/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169895, "tmdate": 1576860562158, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper542/Authors", "ICLR.cc/2020/Conference/Paper542/Reviewers", "ICLR.cc/2020/Conference/Paper542/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper542/-/Official_Comment"}}}, {"id": "rylETCDaFS", "original": null, "number": 2, "cdate": 1571811003674, "ddate": null, "tcdate": 1571811003674, "tmdate": 1572972582316, "tddate": null, "forum": "HkxAS6VFDB", "replyto": "HkxAS6VFDB", "invitation": "ICLR.cc/2020/Conference/Paper542/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an effective signal norm metric that measures the cost of the neural networks under both compute(ESN_a) or memory(ESN_d) in an ideal hardware setting. The authors then show that the slimmer models with fewer parameters are better than fatter models.\n\nStrength\n\nFirst of all, I like the discussion about different hardware(CPU) setups and corresponding cost. I also like the usage of pareto frontier to compare experimental methods. The proposed metric was simple but the author justifies it with respect to possible hardware setups in an ideal setting.\n\nWeakness\n\nThe main drawback of the paper is the lack of novelty in proposed method. The metric itself appeared to be the main contribution, but as the author said, the metric was based on an ideal hardware setup that ignores the memory hierarchy and data transfer cost, which could be the bottleneck in reality.\n\nMore importantly, it is hard to use the empirical evaluation to justify the conclusion \u201cfatter models are worse\u201d. As we know that extremely low bit quantized models are very hard to train, and it is not surprising -- by setting the number of bits to extremely low bits, the models cannot recover in a few rounds of re-training. \n\nThis being said, I cannot think of a better alternative. While I do not think the experimental results offers new insights(other than it is hard to re-train lower bits models). How to quickly train low bits models remains an open question.\n\nFinally given that there are already quite a few methods that prunes model based on real hardware evaluations, it would be great to compare to these methods as well.\n\nBecause the above weakness of the paper, I think this paper should not be accepted to the program.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper542/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper542/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prune or quantize? Strategy for Pareto-optimally low-cost and accurate CNN", "authors": ["Kengo Nakata", "Daisuke Miyashita", "Asuka Maki", "Fumihiko Tachibana", "Shinichi Sasaki", "Jun Deguchi"], "authorids": ["kengo1.nakata@toshiba.co.jp", "daisuke1.miyashita@toshiba.co.jp", "asuka.maki@toshiba.co.jp", "fumihiko.tachibana@toshiba.co.jp", "shinichi8.sasaki@toshiba.co.jp", "jun.deguchi@toshiba.co.jp"], "keywords": ["CNN", "Quantization", "Pruning", "Accelerator", "Computational cost"], "TL;DR": "This paper reveals that \"prune-then-quantize method\" is the best strategy to achieve Pareto-optimal performance by using a proposed hardware-agnostic metric to measure computational cost.", "abstract": "Pruning and quantization are typical approaches to reduce the computational cost of CNN inference. Although the idea to combine them together seems natural, it is being unexpectedly difficult to figure out the resultant effect of the combination unless measuring the performance on a certain hardware which a user is going to use. This is because the benefits of pruning and quantization strongly depend on the hardware architecture where the model is executed. For example, a CPU-like architecture without any parallelization may fully exploit the reduction of computations by unstructured pruning for speeding up, but a GPU-like massive parallel architecture would not. Besides, there have been emerging proposals of novel hardware architectures such as one supporting variable bit precision quantization. From an engineering viewpoint, optimization for each hardware architecture is useful and important in practice, but this is quite a brute-force approach. Therefore, in this paper, we first propose hardware-agnostic metric to measure the computational cost. And using the metric, we demonstrate that Pareto-optimal performance, where the best accuracy is obtained at a given computational cost, is achieved when a slim model with smaller number of parameters is quantized moderately rather than a fat model with huge number of parameters is quantized to extremely low bit precision such as binary or ternary. Furthermore, we empirically found the possible quantitative relation between the proposed metric and the signal to noise ratio during SGD training, by which the information obtained during SGD training provides the optimal policy of quantization and pruning. We show the Pareto frontier is improved by 4 times in post-training quantization scenario based on these findings. These findings are available not only to improve the Pareto frontier for accuracy vs. computational cost, but also give us some new insights on deep neural network.", "pdf": "/pdf/739e3925990713d557e2dbf1739383585bdf0c7c.pdf", "paperhash": "nakata|prune_or_quantize_strategy_for_paretooptimally_lowcost_and_accurate_cnn", "original_pdf": "/attachment/276428aecdbce6ccfd6e99352520aaa47b98b95a.pdf", "_bibtex": "@misc{\nnakata2020prune,\ntitle={Prune or quantize? Strategy for Pareto-optimally low-cost and accurate {\\{}CNN{\\}}},\nauthor={Kengo Nakata and Daisuke Miyashita and Asuka Maki and Fumihiko Tachibana and Shinichi Sasaki and Jun Deguchi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxAS6VFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxAS6VFDB", "replyto": "HkxAS6VFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper542/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper542/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575117497632, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper542/Reviewers"], "noninvitees": [], "tcdate": 1570237750629, "tmdate": 1575117497649, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper542/-/Official_Review"}}}, {"id": "S1lnx65qqS", "original": null, "number": 3, "cdate": 1572674804341, "ddate": null, "tcdate": 1572674804341, "tmdate": 1572972582268, "tddate": null, "forum": "HkxAS6VFDB", "replyto": "HkxAS6VFDB", "invitation": "ICLR.cc/2020/Conference/Paper542/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors propose a hardware-agnostic metric called effective signal norm (ESN) to measure the computational cost of convolutional neural networks. This metric aims to fairly measure the effects of pruning and quantization. What\u2019s more, based on the metric, the authors demonstrate that models with fewer parameters achieve far better accuracy after quantization. A large number of experiments are carried out to prove the effects of the metric and related conclusions, however, several experiments and arguments are confusing. \n\nPlease see my detailed comments below.\n\nPositive Points:\n1. The authors propose a hardware-agnostic metric named effective signal norm (ESN), aiming to measure the computational cost of the pruned and quantized models.\n\n2. Based on the proposed metric, the authors conclude that a moderately quantized slim model with fewer weight parameters achieves better performance rather than an extremely quantized fat model with huge number of parameters.\n\n3. This paper presents many interesting assumptions and possibilities, which can be further researched and explored in the future.\n\n\nNegative Points:\n1. The authors argue that ESN is a hardware-agnostic metric. However, ESN is based on ideal hardware, where the energy consumption is linearly proportional to the number of non-zero weight parameters and monotonically depends on the bit-width of weight parameters. Therefore, ESN is not suitable for existing hardware.\n\n2. Both of ESN_a and ESN_d are based on the assumptions instead of the fact, which is not convincing. What\u2019s more, the assumptions are hard to be proved.\n\n3. The experiments are not strong enough to support the author's conclusions. For example, the authors argue that \u201cmodels with fewer parameters achieve far better accuracy in low computational cost region after quantization\u201d. However, this conclusion is only based on the ESN metric. Since ESN is based on some assumptions, this conclusion is not convincing.\n\n4. Please give more details about the quantitative relation between the quantization noise and the perturbation of weight parameters during SGD training.\n\n5. In terms of Figure 2, the author's description is not objective. The similarity between the ESN_a curves and the validation accuracy curves is not very high. Besides, the authors mention that \u201cafter steep rise at 80 epoch, both ESN_a and validation accuracy decrease gradually\u201d. However, the performance degradation in Figure 2 is not obvious. Therefore, the conclusions drew by the observation are also unreliable.\n\n6. The ESN can not make an accurate evaluation of a model. The trade-off between accuracy, speed, model size is often required in model compression. However, the ESN can not make the trade-off between them. Moreover, the ESN does not have a clear application scenario.\n\nMinor issues:\n1.\tThere are lots of spelling and grammar mistakes in the paper, such as \u201can ideal hardware \u201d, \u201ca extremely quantized fat model\u201d and so on.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper542/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper542/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prune or quantize? Strategy for Pareto-optimally low-cost and accurate CNN", "authors": ["Kengo Nakata", "Daisuke Miyashita", "Asuka Maki", "Fumihiko Tachibana", "Shinichi Sasaki", "Jun Deguchi"], "authorids": ["kengo1.nakata@toshiba.co.jp", "daisuke1.miyashita@toshiba.co.jp", "asuka.maki@toshiba.co.jp", "fumihiko.tachibana@toshiba.co.jp", "shinichi8.sasaki@toshiba.co.jp", "jun.deguchi@toshiba.co.jp"], "keywords": ["CNN", "Quantization", "Pruning", "Accelerator", "Computational cost"], "TL;DR": "This paper reveals that \"prune-then-quantize method\" is the best strategy to achieve Pareto-optimal performance by using a proposed hardware-agnostic metric to measure computational cost.", "abstract": "Pruning and quantization are typical approaches to reduce the computational cost of CNN inference. Although the idea to combine them together seems natural, it is being unexpectedly difficult to figure out the resultant effect of the combination unless measuring the performance on a certain hardware which a user is going to use. This is because the benefits of pruning and quantization strongly depend on the hardware architecture where the model is executed. For example, a CPU-like architecture without any parallelization may fully exploit the reduction of computations by unstructured pruning for speeding up, but a GPU-like massive parallel architecture would not. Besides, there have been emerging proposals of novel hardware architectures such as one supporting variable bit precision quantization. From an engineering viewpoint, optimization for each hardware architecture is useful and important in practice, but this is quite a brute-force approach. Therefore, in this paper, we first propose hardware-agnostic metric to measure the computational cost. And using the metric, we demonstrate that Pareto-optimal performance, where the best accuracy is obtained at a given computational cost, is achieved when a slim model with smaller number of parameters is quantized moderately rather than a fat model with huge number of parameters is quantized to extremely low bit precision such as binary or ternary. Furthermore, we empirically found the possible quantitative relation between the proposed metric and the signal to noise ratio during SGD training, by which the information obtained during SGD training provides the optimal policy of quantization and pruning. We show the Pareto frontier is improved by 4 times in post-training quantization scenario based on these findings. These findings are available not only to improve the Pareto frontier for accuracy vs. computational cost, but also give us some new insights on deep neural network.", "pdf": "/pdf/739e3925990713d557e2dbf1739383585bdf0c7c.pdf", "paperhash": "nakata|prune_or_quantize_strategy_for_paretooptimally_lowcost_and_accurate_cnn", "original_pdf": "/attachment/276428aecdbce6ccfd6e99352520aaa47b98b95a.pdf", "_bibtex": "@misc{\nnakata2020prune,\ntitle={Prune or quantize? Strategy for Pareto-optimally low-cost and accurate {\\{}CNN{\\}}},\nauthor={Kengo Nakata and Daisuke Miyashita and Asuka Maki and Fumihiko Tachibana and Shinichi Sasaki and Jun Deguchi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxAS6VFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxAS6VFDB", "replyto": "HkxAS6VFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper542/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper542/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575117497632, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper542/Reviewers"], "noninvitees": [], "tcdate": 1570237750629, "tmdate": 1575117497649, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper542/-/Official_Review"}}}], "count": 16}