{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396553236, "tcdate": 1486396553236, "number": 1, "id": "Skb9hGI_e", "invitation": "ICLR.cc/2017/conference/-/paper381/acceptance", "forum": "H1_QSDqxl", "replyto": "H1_QSDqxl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The reviewers concur that the paper is well written and the topic is interesting, but that the authors have not put sufficient effort into motivating their approach and evaluating it. The baselines seem too simple, the evaluation is incomplete. It is furthermore disappointing that the authors not only did not respond to the reviews, but did not respond to the pre-review questions. There is little in this review process that would support the paper being accepted, and therefore I concur with the reviewers' opinion and support rejection."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rule Mining in Feature Space", "abstract": "Relational embeddings have emerged as an excellent tool for inferring novel facts\nfrom partially observed knowledge bases. Recently, it was shown that some\nclasses of embeddings can also be exploited to perform a simplified form of rule\nmining. By interpreting logical conjunction as a form of composition between re-\nlation embeddings, simplified logical theories can be mined directly in the space\nof latent representations. In this paper, we present a method to mine full-fledged\nlogical theories, which are significantly more expressive, by casting the semantics\nof the logical operators to the space of the embeddings. In order to extract relevant\nrules in the space of relation compositions we borrow sparse reconstruction pro-\ncedures from the field of compressed sensing. Our empirical analysis showcases\nthe advantages of our approach.", "pdf": "/pdf/24986b2763968aab2e7a6923afd8326304365dba.pdf", "TL;DR": "We propose an algorithm to discover logical theories from relational embeddings of knowledge bases.", "paperhash": "teso|rule_mining_in_feature_space", "keywords": ["Unsupervised Learning"], "conflicts": ["unitn.it"], "authors": ["Stefano Teso", "Andrea Passerini"], "authorids": ["teso@disi.unitn.it", "passerini@disi.unitn.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396553695, "id": "ICLR.cc/2017/conference/-/paper381/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "H1_QSDqxl", "replyto": "H1_QSDqxl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396553695}}}, {"tddate": null, "tmdate": 1482314310815, "tcdate": 1482314266113, "number": 3, "id": "BkzXf0D4x", "invitation": "ICLR.cc/2017/conference/-/paper381/official/review", "forum": "H1_QSDqxl", "replyto": "H1_QSDqxl", "signatures": ["ICLR.cc/2017/conference/paper381/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper381/AnonReviewer2"], "content": {"title": "review", "rating": "4: Ok but not good enough - rejection", "review": "This paper aims to mine explicit rules from KB embedding space, and casts it into a sparse reconstruction problem. Experiments demonstrate its ability of extracting reasonable rules on a few link prediction datasets.\n\nThe solution part sounds plausible. However, it confuses me that why we need to mine rules from learned KB embeddings. \n\n- It is still unclear what information these KB embeddings encode and it looks strange that we aim to learn rules including negation / disjunction from them.\n\n- If the goal is to extract useful rules (for other applications), it is necessary to compare it to \u201cgraph random walk\u201d (http://rtw.ml.cmu.edu/papers/lao-emnlp11.pdf) which could learn rules from KB graph directly.\n\n- As there is only one KNN baseline, the experimental results seem pretty weak. At the least, it is necessary to show the original precision / recall of RESCAL, together with the proposed rule mining approach (with different max length), so we know how much the current information the current rule miner could recover.\n\nIn addition, the four datasets are all very small. Would it be able to scale it to WordNet or Freebase?\n\n[Minor comments]\n\n\u201cRelational embedding\u201d and \u201crelation embedding\u201d are used mixedly throughout the paper. I am not sure if they are well-defined terms (it is better to cite relevant paper).\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rule Mining in Feature Space", "abstract": "Relational embeddings have emerged as an excellent tool for inferring novel facts\nfrom partially observed knowledge bases. Recently, it was shown that some\nclasses of embeddings can also be exploited to perform a simplified form of rule\nmining. By interpreting logical conjunction as a form of composition between re-\nlation embeddings, simplified logical theories can be mined directly in the space\nof latent representations. In this paper, we present a method to mine full-fledged\nlogical theories, which are significantly more expressive, by casting the semantics\nof the logical operators to the space of the embeddings. In order to extract relevant\nrules in the space of relation compositions we borrow sparse reconstruction pro-\ncedures from the field of compressed sensing. Our empirical analysis showcases\nthe advantages of our approach.", "pdf": "/pdf/24986b2763968aab2e7a6923afd8326304365dba.pdf", "TL;DR": "We propose an algorithm to discover logical theories from relational embeddings of knowledge bases.", "paperhash": "teso|rule_mining_in_feature_space", "keywords": ["Unsupervised Learning"], "conflicts": ["unitn.it"], "authors": ["Stefano Teso", "Andrea Passerini"], "authorids": ["teso@disi.unitn.it", "passerini@disi.unitn.it"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512603678, "id": "ICLR.cc/2017/conference/-/paper381/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper381/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper381/AnonReviewer3", "ICLR.cc/2017/conference/paper381/AnonReviewer1", "ICLR.cc/2017/conference/paper381/AnonReviewer2"], "reply": {"forum": "H1_QSDqxl", "replyto": "H1_QSDqxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper381/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper381/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512603678}}}, {"tddate": null, "tmdate": 1482180248670, "tcdate": 1482180248670, "number": 2, "id": "S1Wo8pSEx", "invitation": "ICLR.cc/2017/conference/-/paper381/official/review", "forum": "H1_QSDqxl", "replyto": "H1_QSDqxl", "signatures": ["ICLR.cc/2017/conference/paper381/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper381/AnonReviewer1"], "content": {"title": "Review", "rating": "3: Clear rejection", "review": "This paper proposes a process to mine rules from vector space representations learned from KBs (using nonnegative RESCAL).\n\nThe paper is nicely written. \nBut its motivations are unclear: what is the underlying motivation to mine rules from embedding spaces?\n- If it is for better performance on link prediction then the paper does not show this. The experiments do not compare FRM against the performance of the original vector space model.\n- If it is for a better interpretability and debugging of the representations learned by vector space models, then there should have more elements on this in the paper.\n\nOther remarks:\n- The fact that the performance of the methods in Figure 1 and 2 are not compared to any baseline is problematic.\n- The scalability of the rule miner is a big drawback that should be addressed.\n- Figure 3 does not do a good job at convincing that rule based systems should be used for prediction or interpretation. The learned rules are bad for both cases.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rule Mining in Feature Space", "abstract": "Relational embeddings have emerged as an excellent tool for inferring novel facts\nfrom partially observed knowledge bases. Recently, it was shown that some\nclasses of embeddings can also be exploited to perform a simplified form of rule\nmining. By interpreting logical conjunction as a form of composition between re-\nlation embeddings, simplified logical theories can be mined directly in the space\nof latent representations. In this paper, we present a method to mine full-fledged\nlogical theories, which are significantly more expressive, by casting the semantics\nof the logical operators to the space of the embeddings. In order to extract relevant\nrules in the space of relation compositions we borrow sparse reconstruction pro-\ncedures from the field of compressed sensing. Our empirical analysis showcases\nthe advantages of our approach.", "pdf": "/pdf/24986b2763968aab2e7a6923afd8326304365dba.pdf", "TL;DR": "We propose an algorithm to discover logical theories from relational embeddings of knowledge bases.", "paperhash": "teso|rule_mining_in_feature_space", "keywords": ["Unsupervised Learning"], "conflicts": ["unitn.it"], "authors": ["Stefano Teso", "Andrea Passerini"], "authorids": ["teso@disi.unitn.it", "passerini@disi.unitn.it"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512603678, "id": "ICLR.cc/2017/conference/-/paper381/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper381/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper381/AnonReviewer3", "ICLR.cc/2017/conference/paper381/AnonReviewer1", "ICLR.cc/2017/conference/paper381/AnonReviewer2"], "reply": {"forum": "H1_QSDqxl", "replyto": "H1_QSDqxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper381/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper381/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512603678}}}, {"tddate": null, "tmdate": 1482138158254, "tcdate": 1482138158254, "number": 1, "id": "B1I4zQrEl", "invitation": "ICLR.cc/2017/conference/-/paper381/official/review", "forum": "H1_QSDqxl", "replyto": "H1_QSDqxl", "signatures": ["ICLR.cc/2017/conference/paper381/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper381/AnonReviewer3"], "content": {"title": "unconclusive experiments and missing theoretical motivation", "rating": "4: Ok but not good enough - rejection", "review": "The paper presents a nice idea of directly finding rules such as brother(father) => uncle in knowledge bases, by directly searching in embedding space. The idea is to interpret the successive application of relationships as the multiplication of the relation-dependent matrices in non-negative RESCAL. \n\nThe experimental section provides an evaluation of the rules that are found by the algorithm. Nonetheless, the work seems only at its first stages for now, and many questions are left open:\n\n1) while the approach to find rules seems very general, the reason why it should work is unclear. What properties of the embedding space or of the initial algorithm are required for this approach to find meaningful rules? Can we apply the same principles to other algorithms than non-negative RESCAL?\n\n2) there is no real evaluation in terms of link prediction. How can we use these rules in conjunction with the original algorithm to improve link prediction? What performance gains can be expected? Can these rules find links that would not be found be the original algorithm in the first place?\n\n3) scaling: for now the number of parameters of the rule miner is (#relationships)^(max. path length + 1). How does this method scale on standard benchmarks such as FB15k where there is more than a 1000 of relationships?\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rule Mining in Feature Space", "abstract": "Relational embeddings have emerged as an excellent tool for inferring novel facts\nfrom partially observed knowledge bases. Recently, it was shown that some\nclasses of embeddings can also be exploited to perform a simplified form of rule\nmining. By interpreting logical conjunction as a form of composition between re-\nlation embeddings, simplified logical theories can be mined directly in the space\nof latent representations. In this paper, we present a method to mine full-fledged\nlogical theories, which are significantly more expressive, by casting the semantics\nof the logical operators to the space of the embeddings. In order to extract relevant\nrules in the space of relation compositions we borrow sparse reconstruction pro-\ncedures from the field of compressed sensing. Our empirical analysis showcases\nthe advantages of our approach.", "pdf": "/pdf/24986b2763968aab2e7a6923afd8326304365dba.pdf", "TL;DR": "We propose an algorithm to discover logical theories from relational embeddings of knowledge bases.", "paperhash": "teso|rule_mining_in_feature_space", "keywords": ["Unsupervised Learning"], "conflicts": ["unitn.it"], "authors": ["Stefano Teso", "Andrea Passerini"], "authorids": ["teso@disi.unitn.it", "passerini@disi.unitn.it"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512603678, "id": "ICLR.cc/2017/conference/-/paper381/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper381/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper381/AnonReviewer3", "ICLR.cc/2017/conference/paper381/AnonReviewer1", "ICLR.cc/2017/conference/paper381/AnonReviewer2"], "reply": {"forum": "H1_QSDqxl", "replyto": "H1_QSDqxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper381/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper381/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512603678}}}, {"tddate": null, "tmdate": 1480806684436, "tcdate": 1480806684430, "number": 2, "id": "ByEXWCl7x", "invitation": "ICLR.cc/2017/conference/-/paper381/pre-review/question", "forum": "H1_QSDqxl", "replyto": "H1_QSDqxl", "signatures": ["ICLR.cc/2017/conference/paper381/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper381/AnonReviewer3"], "content": {"title": "questions", "question": "Thank you for this interesting paper. The weighting of different paths for link prediction was also present in Lao et al. Random Walk Inference and Learning in A Large Scale Knowledge Base (EMNLP 11); the current paper proposes to learn from paths directly in embedding space, which may be much faster. I still have a few questions:\n\n1) The experimental section shows the relative performance of different rule miners, but there does not seem to be any reference with respect to the original algorithm (here, nonnegative RESCAL). How can we use these rules together with the original algorithm to perform link prediction? What performance gains can be expected? \n\n2) While the method to learn the paths seems applicable to many embedding-based algorithms for relational learning, I wonder what properties of the underlying algorithm are necessary for this principle to effectively work well. For instance, the authors use nonnegative RESCAL; to what extent is the non-negativity a critical feature for the approach to work?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rule Mining in Feature Space", "abstract": "Relational embeddings have emerged as an excellent tool for inferring novel facts\nfrom partially observed knowledge bases. Recently, it was shown that some\nclasses of embeddings can also be exploited to perform a simplified form of rule\nmining. By interpreting logical conjunction as a form of composition between re-\nlation embeddings, simplified logical theories can be mined directly in the space\nof latent representations. In this paper, we present a method to mine full-fledged\nlogical theories, which are significantly more expressive, by casting the semantics\nof the logical operators to the space of the embeddings. In order to extract relevant\nrules in the space of relation compositions we borrow sparse reconstruction pro-\ncedures from the field of compressed sensing. Our empirical analysis showcases\nthe advantages of our approach.", "pdf": "/pdf/24986b2763968aab2e7a6923afd8326304365dba.pdf", "TL;DR": "We propose an algorithm to discover logical theories from relational embeddings of knowledge bases.", "paperhash": "teso|rule_mining_in_feature_space", "keywords": ["Unsupervised Learning"], "conflicts": ["unitn.it"], "authors": ["Stefano Teso", "Andrea Passerini"], "authorids": ["teso@disi.unitn.it", "passerini@disi.unitn.it"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959310896, "id": "ICLR.cc/2017/conference/-/paper381/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper381/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper381/AnonReviewer1", "ICLR.cc/2017/conference/paper381/AnonReviewer3"], "reply": {"forum": "H1_QSDqxl", "replyto": "H1_QSDqxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper381/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper381/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959310896}}}, {"tddate": null, "tmdate": 1480741349746, "tcdate": 1480741349741, "number": 1, "id": "HkCkG0yme", "invitation": "ICLR.cc/2017/conference/-/paper381/pre-review/question", "forum": "H1_QSDqxl", "replyto": "H1_QSDqxl", "signatures": ["ICLR.cc/2017/conference/paper381/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper381/AnonReviewer1"], "content": {"title": "Questions", "question": "- Nations, Kinship and UMLS are standard relational learning benchmarks. But most previous works use area under the precision recall curve as metric. How does the proposed approach compare to previous work with this metric?\n- All datasets used in the experiments are rather small. How would FRM scale with larger datasets (high number of entities and/or of relation types and/or of triples)?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rule Mining in Feature Space", "abstract": "Relational embeddings have emerged as an excellent tool for inferring novel facts\nfrom partially observed knowledge bases. Recently, it was shown that some\nclasses of embeddings can also be exploited to perform a simplified form of rule\nmining. By interpreting logical conjunction as a form of composition between re-\nlation embeddings, simplified logical theories can be mined directly in the space\nof latent representations. In this paper, we present a method to mine full-fledged\nlogical theories, which are significantly more expressive, by casting the semantics\nof the logical operators to the space of the embeddings. In order to extract relevant\nrules in the space of relation compositions we borrow sparse reconstruction pro-\ncedures from the field of compressed sensing. Our empirical analysis showcases\nthe advantages of our approach.", "pdf": "/pdf/24986b2763968aab2e7a6923afd8326304365dba.pdf", "TL;DR": "We propose an algorithm to discover logical theories from relational embeddings of knowledge bases.", "paperhash": "teso|rule_mining_in_feature_space", "keywords": ["Unsupervised Learning"], "conflicts": ["unitn.it"], "authors": ["Stefano Teso", "Andrea Passerini"], "authorids": ["teso@disi.unitn.it", "passerini@disi.unitn.it"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959310896, "id": "ICLR.cc/2017/conference/-/paper381/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper381/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper381/AnonReviewer1", "ICLR.cc/2017/conference/paper381/AnonReviewer3"], "reply": {"forum": "H1_QSDqxl", "replyto": "H1_QSDqxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper381/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper381/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959310896}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478289254001, "tcdate": 1478288672156, "number": 381, "id": "H1_QSDqxl", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "H1_QSDqxl", "signatures": ["~Stefano_Teso1"], "readers": ["everyone"], "content": {"title": "Rule Mining in Feature Space", "abstract": "Relational embeddings have emerged as an excellent tool for inferring novel facts\nfrom partially observed knowledge bases. Recently, it was shown that some\nclasses of embeddings can also be exploited to perform a simplified form of rule\nmining. By interpreting logical conjunction as a form of composition between re-\nlation embeddings, simplified logical theories can be mined directly in the space\nof latent representations. In this paper, we present a method to mine full-fledged\nlogical theories, which are significantly more expressive, by casting the semantics\nof the logical operators to the space of the embeddings. In order to extract relevant\nrules in the space of relation compositions we borrow sparse reconstruction pro-\ncedures from the field of compressed sensing. Our empirical analysis showcases\nthe advantages of our approach.", "pdf": "/pdf/24986b2763968aab2e7a6923afd8326304365dba.pdf", "TL;DR": "We propose an algorithm to discover logical theories from relational embeddings of knowledge bases.", "paperhash": "teso|rule_mining_in_feature_space", "keywords": ["Unsupervised Learning"], "conflicts": ["unitn.it"], "authors": ["Stefano Teso", "Andrea Passerini"], "authorids": ["teso@disi.unitn.it", "passerini@disi.unitn.it"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 7}