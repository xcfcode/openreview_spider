{"notes": [{"id": "Bkxdqj0cFQ", "original": "S1xG1J2cKm", "number": 546, "cdate": 1538087823756, "ddate": null, "tcdate": 1538087823756, "tmdate": 1545355412125, "tddate": null, "forum": "Bkxdqj0cFQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Calibration of neural network logit vectors to combat adversarial attacks", "abstract": "Adversarial examples remain an issue for contemporary neural networks. This paper draws on Background Check (Perello-Nieto et al., 2016), a technique in model calibration, to assist two-class neural networks in detecting adversarial examples, using the one dimensional difference between logit values as the underlying measure. This method interestingly tends to achieve the highest average recall on image sets that are generated with large perturbation vectors, which is unlike the existing literature on adversarial attacks (Cubuk et al., 2017). The proposed method does not need knowledge of the attack parameters or methods at training time, unlike a great deal of the literature that uses deep learning based methods to detect adversarial examples, such as Metzen et al. (2017), imbuing the proposed method with additional flexibility.", "keywords": ["Adversarial attacks", "calibration", "probability", "adversarial defence"], "authorids": ["og14775@my.bristol.ac.uk"], "authors": ["Oliver Goldstein"], "TL;DR": "This paper uses principles from the field of calibration in machine learning on the logits of a neural network to defend against adversarial attacks", "pdf": "/pdf/9076625201c7930f54c5f98d51c5d00273de4763.pdf", "paperhash": "goldstein|calibration_of_neural_network_logit_vectors_to_combat_adversarial_attacks", "_bibtex": "@misc{\ngoldstein2019calibration,\ntitle={Calibration of neural network logit vectors to combat adversarial attacks},\nauthor={Oliver Goldstein},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkxdqj0cFQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Sklfa33Ak4", "original": null, "number": 1, "cdate": 1544633530473, "ddate": null, "tcdate": 1544633530473, "tmdate": 1545354502329, "tddate": null, "forum": "Bkxdqj0cFQ", "replyto": "Bkxdqj0cFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper546/Meta_Review", "content": {"metareview": "The reviewers and AC note the potential weaknesses of the paper in various aspects, and decided that the authors need more works to publish. ", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Limited contribution and no effort for rebuttal"}, "signatures": ["ICLR.cc/2019/Conference/Paper546/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper546/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Calibration of neural network logit vectors to combat adversarial attacks", "abstract": "Adversarial examples remain an issue for contemporary neural networks. This paper draws on Background Check (Perello-Nieto et al., 2016), a technique in model calibration, to assist two-class neural networks in detecting adversarial examples, using the one dimensional difference between logit values as the underlying measure. This method interestingly tends to achieve the highest average recall on image sets that are generated with large perturbation vectors, which is unlike the existing literature on adversarial attacks (Cubuk et al., 2017). The proposed method does not need knowledge of the attack parameters or methods at training time, unlike a great deal of the literature that uses deep learning based methods to detect adversarial examples, such as Metzen et al. (2017), imbuing the proposed method with additional flexibility.", "keywords": ["Adversarial attacks", "calibration", "probability", "adversarial defence"], "authorids": ["og14775@my.bristol.ac.uk"], "authors": ["Oliver Goldstein"], "TL;DR": "This paper uses principles from the field of calibration in machine learning on the logits of a neural network to defend against adversarial attacks", "pdf": "/pdf/9076625201c7930f54c5f98d51c5d00273de4763.pdf", "paperhash": "goldstein|calibration_of_neural_network_logit_vectors_to_combat_adversarial_attacks", "_bibtex": "@misc{\ngoldstein2019calibration,\ntitle={Calibration of neural network logit vectors to combat adversarial attacks},\nauthor={Oliver Goldstein},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkxdqj0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper546/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353177130, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bkxdqj0cFQ", "replyto": "Bkxdqj0cFQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper546/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper546/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper546/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353177130}}}, {"id": "ByxKo_ry07", "original": null, "number": 4, "cdate": 1542572193468, "ddate": null, "tcdate": 1542572193468, "tmdate": 1542572193468, "tddate": null, "forum": "Bkxdqj0cFQ", "replyto": "Hkgp682Ihm", "invitation": "ICLR.cc/2019/Conference/-/Paper546/Official_Comment", "content": {"title": "Response", "comment": "Thanks you for your comments. It is a simple technique and I appreciate the highlighted shortcomings mentioned. I agree with all of the points mentioned."}, "signatures": ["ICLR.cc/2019/Conference/Paper546/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper546/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper546/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Calibration of neural network logit vectors to combat adversarial attacks", "abstract": "Adversarial examples remain an issue for contemporary neural networks. This paper draws on Background Check (Perello-Nieto et al., 2016), a technique in model calibration, to assist two-class neural networks in detecting adversarial examples, using the one dimensional difference between logit values as the underlying measure. This method interestingly tends to achieve the highest average recall on image sets that are generated with large perturbation vectors, which is unlike the existing literature on adversarial attacks (Cubuk et al., 2017). The proposed method does not need knowledge of the attack parameters or methods at training time, unlike a great deal of the literature that uses deep learning based methods to detect adversarial examples, such as Metzen et al. (2017), imbuing the proposed method with additional flexibility.", "keywords": ["Adversarial attacks", "calibration", "probability", "adversarial defence"], "authorids": ["og14775@my.bristol.ac.uk"], "authors": ["Oliver Goldstein"], "TL;DR": "This paper uses principles from the field of calibration in machine learning on the logits of a neural network to defend against adversarial attacks", "pdf": "/pdf/9076625201c7930f54c5f98d51c5d00273de4763.pdf", "paperhash": "goldstein|calibration_of_neural_network_logit_vectors_to_combat_adversarial_attacks", "_bibtex": "@misc{\ngoldstein2019calibration,\ntitle={Calibration of neural network logit vectors to combat adversarial attacks},\nauthor={Oliver Goldstein},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkxdqj0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper546/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623861, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bkxdqj0cFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper546/Authors", "ICLR.cc/2019/Conference/Paper546/Reviewers", "ICLR.cc/2019/Conference/Paper546/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper546/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper546/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper546/Authors|ICLR.cc/2019/Conference/Paper546/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper546/Reviewers", "ICLR.cc/2019/Conference/Paper546/Authors", "ICLR.cc/2019/Conference/Paper546/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623861}}}, {"id": "r1l1jdryAX", "original": null, "number": 3, "cdate": 1542572183198, "ddate": null, "tcdate": 1542572183198, "tmdate": 1542572183198, "tddate": null, "forum": "Bkxdqj0cFQ", "replyto": "Syx3aoWDnm", "invitation": "ICLR.cc/2019/Conference/-/Paper546/Official_Comment", "content": {"title": "Response", "comment": "Thanks you for your comments. It is a simple technique and I appreciate the highlighted shortcomings mentioned. I agree with all of the points mentioned."}, "signatures": ["ICLR.cc/2019/Conference/Paper546/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper546/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper546/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Calibration of neural network logit vectors to combat adversarial attacks", "abstract": "Adversarial examples remain an issue for contemporary neural networks. This paper draws on Background Check (Perello-Nieto et al., 2016), a technique in model calibration, to assist two-class neural networks in detecting adversarial examples, using the one dimensional difference between logit values as the underlying measure. This method interestingly tends to achieve the highest average recall on image sets that are generated with large perturbation vectors, which is unlike the existing literature on adversarial attacks (Cubuk et al., 2017). The proposed method does not need knowledge of the attack parameters or methods at training time, unlike a great deal of the literature that uses deep learning based methods to detect adversarial examples, such as Metzen et al. (2017), imbuing the proposed method with additional flexibility.", "keywords": ["Adversarial attacks", "calibration", "probability", "adversarial defence"], "authorids": ["og14775@my.bristol.ac.uk"], "authors": ["Oliver Goldstein"], "TL;DR": "This paper uses principles from the field of calibration in machine learning on the logits of a neural network to defend against adversarial attacks", "pdf": "/pdf/9076625201c7930f54c5f98d51c5d00273de4763.pdf", "paperhash": "goldstein|calibration_of_neural_network_logit_vectors_to_combat_adversarial_attacks", "_bibtex": "@misc{\ngoldstein2019calibration,\ntitle={Calibration of neural network logit vectors to combat adversarial attacks},\nauthor={Oliver Goldstein},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkxdqj0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper546/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623861, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bkxdqj0cFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper546/Authors", "ICLR.cc/2019/Conference/Paper546/Reviewers", "ICLR.cc/2019/Conference/Paper546/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper546/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper546/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper546/Authors|ICLR.cc/2019/Conference/Paper546/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper546/Reviewers", "ICLR.cc/2019/Conference/Paper546/Authors", "ICLR.cc/2019/Conference/Paper546/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623861}}}, {"id": "r1lfYOBJRX", "original": null, "number": 2, "cdate": 1542572154036, "ddate": null, "tcdate": 1542572154036, "tmdate": 1542572154036, "tddate": null, "forum": "Bkxdqj0cFQ", "replyto": "r1xbeoD627", "invitation": "ICLR.cc/2019/Conference/-/Paper546/Official_Comment", "content": {"title": "Response", "comment": "Thanks you for your comments. It is a simple technique and I appreciate the highlighted shortcomings mentioned. I agree with all of the points mentioned."}, "signatures": ["ICLR.cc/2019/Conference/Paper546/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper546/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper546/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Calibration of neural network logit vectors to combat adversarial attacks", "abstract": "Adversarial examples remain an issue for contemporary neural networks. This paper draws on Background Check (Perello-Nieto et al., 2016), a technique in model calibration, to assist two-class neural networks in detecting adversarial examples, using the one dimensional difference between logit values as the underlying measure. This method interestingly tends to achieve the highest average recall on image sets that are generated with large perturbation vectors, which is unlike the existing literature on adversarial attacks (Cubuk et al., 2017). The proposed method does not need knowledge of the attack parameters or methods at training time, unlike a great deal of the literature that uses deep learning based methods to detect adversarial examples, such as Metzen et al. (2017), imbuing the proposed method with additional flexibility.", "keywords": ["Adversarial attacks", "calibration", "probability", "adversarial defence"], "authorids": ["og14775@my.bristol.ac.uk"], "authors": ["Oliver Goldstein"], "TL;DR": "This paper uses principles from the field of calibration in machine learning on the logits of a neural network to defend against adversarial attacks", "pdf": "/pdf/9076625201c7930f54c5f98d51c5d00273de4763.pdf", "paperhash": "goldstein|calibration_of_neural_network_logit_vectors_to_combat_adversarial_attacks", "_bibtex": "@misc{\ngoldstein2019calibration,\ntitle={Calibration of neural network logit vectors to combat adversarial attacks},\nauthor={Oliver Goldstein},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkxdqj0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper546/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623861, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bkxdqj0cFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper546/Authors", "ICLR.cc/2019/Conference/Paper546/Reviewers", "ICLR.cc/2019/Conference/Paper546/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper546/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper546/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper546/Authors|ICLR.cc/2019/Conference/Paper546/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper546/Reviewers", "ICLR.cc/2019/Conference/Paper546/Authors", "ICLR.cc/2019/Conference/Paper546/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623861}}}, {"id": "r1xbeoD627", "original": null, "number": 3, "cdate": 1541401320866, "ddate": null, "tcdate": 1541401320866, "tmdate": 1541533902039, "tddate": null, "forum": "Bkxdqj0cFQ", "replyto": "Bkxdqj0cFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper546/Official_Review", "content": {"title": "Density estimation for adversarial regions seems beneficial, but a stronger justification is needed", "review": "This paper addresses adversarial detection through the absolute-value difference between the two logit vector values of a DNN binary classifier, with one class associated with normal data and the other with adversarial data. Assignment of examples to an \"adversarial\" class is problematic in that adversarial examples are typically generated in regions for which training data is very sparse. To cope with this, the authors propose use of the Background Check calibration techniques recently proposed by Perello-Nieto et al. (ICDM 2016).  \n\nHere, BC is used to estimate probabilities in a sparse \"background\" class (here, the adversarial class)  through a form of interpolation based on foreground and background densities. The underlying distributional assumption used for estimating foreground densities was that of a gamma function.  Rather than using BC's affine bias for estimating background density from the foreground density, the authors adapt it by raise the weighting for the \"adversarial\" decision to the fifth power. Unfortunately, no justification for this choice is given, other than to say that this was done with \"domain knowledge informing the use of a power value\". \n\nIn their experimentation, the authors generate from CIFAR-10 data four kinds of adversarial attacks: noise alone, images with moderate noise, clear images with noticeable noise, and clear images with imperceptible noise. For a variety of attacks, they showed (in Table 2) differences between the average recall for normal examples vs the average recall for normal plus adversarial images. However, without knowing the proportion of adversarial examples used in testing, the significance of the reported differences cannot be judged. They also list the true positive rates of adversarial examples, which showed much variation from experiment to experiment (trending to rather poor performance for attacks with imperceptible noise). Again, the significance of the results cannot be judged without knowing the false negative rate, true negative rate, etc. Moreover, the results are reported without clearly identifying two of the attacks used (\"Mom.\" is presumably Dong et al.'s attack using momentum in gradient descent, and Miyato et al.'s \"VAT\" is not properly introduced in the related work). Crucially, no evaluation of their method is made with respect to other adversarial detection strategies.\n\nPros:\n* Overall, the calibration approach is well motivated, and likely to be of some benefit.\n* The paper is generally readable and understandable. The issues behind calibration and the use of BC are well explained.\n\nCons:\n* The result is a simple and straightforward application of an existing technique - not greatly original.\n* Many design choices in the model (particularly the raising of one of the weights to a seemingly-arbitrary power) are mysterious. No indication is given as to other alternatives or how they might perform.\n* The experimental results are inadequate to judge the impact of the proposed calibration approach.\n* There is no comparison against other detection methods.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper546/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Calibration of neural network logit vectors to combat adversarial attacks", "abstract": "Adversarial examples remain an issue for contemporary neural networks. This paper draws on Background Check (Perello-Nieto et al., 2016), a technique in model calibration, to assist two-class neural networks in detecting adversarial examples, using the one dimensional difference between logit values as the underlying measure. This method interestingly tends to achieve the highest average recall on image sets that are generated with large perturbation vectors, which is unlike the existing literature on adversarial attacks (Cubuk et al., 2017). The proposed method does not need knowledge of the attack parameters or methods at training time, unlike a great deal of the literature that uses deep learning based methods to detect adversarial examples, such as Metzen et al. (2017), imbuing the proposed method with additional flexibility.", "keywords": ["Adversarial attacks", "calibration", "probability", "adversarial defence"], "authorids": ["og14775@my.bristol.ac.uk"], "authors": ["Oliver Goldstein"], "TL;DR": "This paper uses principles from the field of calibration in machine learning on the logits of a neural network to defend against adversarial attacks", "pdf": "/pdf/9076625201c7930f54c5f98d51c5d00273de4763.pdf", "paperhash": "goldstein|calibration_of_neural_network_logit_vectors_to_combat_adversarial_attacks", "_bibtex": "@misc{\ngoldstein2019calibration,\ntitle={Calibration of neural network logit vectors to combat adversarial attacks},\nauthor={Oliver Goldstein},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkxdqj0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper546/Official_Review", "cdate": 1542234436831, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Bkxdqj0cFQ", "replyto": "Bkxdqj0cFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper546/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335748685, "tmdate": 1552335748685, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper546/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Syx3aoWDnm", "original": null, "number": 2, "cdate": 1540983748252, "ddate": null, "tcdate": 1540983748252, "tmdate": 1541533901792, "tddate": null, "forum": "Bkxdqj0cFQ", "replyto": "Bkxdqj0cFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper546/Official_Review", "content": {"title": "Lack of clarity and weak experimental evaluation", "review": "On the positive side, I think it's a good idea to experiment with various approaches to defend DNNs against adversarial attacks, like the Background Check approach considered in this manuscript (which hasn't gotten a lot of traction in the Machine Learning community so far).\n\nHowever, the manuscript has a number of shortcomings which in my opinion makes it a strong rejection. My main concern is about the experimental evaluation: \n- The authors should test their approach on Carlini & Wagner's attack which allows for explicit control of logit differences and thus could entirely defeat the Background Check.\n- Moreover, any paper on this topic should evaluate defenses in a complete white-box setting, i.e. the adversary is aware of the detection method and actively tries to bypass it. \n- A comparison with other detection methods from the literature is missing, too, and the two-class classifier setting is very limited.\n\nBesides that, I find there is a general lack of clarity:\n- It really becomes clear only towards the end of the paper what the Background Check is applied for, namely, the detection of adversarial samples. This should be clearly articulated from the beginning.\n- Notation isn't always properly introduced (e.g. in the formula for 3-class average recall on page 6), and the same goes for \nsome acronyms (e.g. what is TPR?).\n- Where does Table 2 show a \"mean reduction in average recall of 11.6\", and what does that mean exactly?", "rating": "2: Strong rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper546/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Calibration of neural network logit vectors to combat adversarial attacks", "abstract": "Adversarial examples remain an issue for contemporary neural networks. This paper draws on Background Check (Perello-Nieto et al., 2016), a technique in model calibration, to assist two-class neural networks in detecting adversarial examples, using the one dimensional difference between logit values as the underlying measure. This method interestingly tends to achieve the highest average recall on image sets that are generated with large perturbation vectors, which is unlike the existing literature on adversarial attacks (Cubuk et al., 2017). The proposed method does not need knowledge of the attack parameters or methods at training time, unlike a great deal of the literature that uses deep learning based methods to detect adversarial examples, such as Metzen et al. (2017), imbuing the proposed method with additional flexibility.", "keywords": ["Adversarial attacks", "calibration", "probability", "adversarial defence"], "authorids": ["og14775@my.bristol.ac.uk"], "authors": ["Oliver Goldstein"], "TL;DR": "This paper uses principles from the field of calibration in machine learning on the logits of a neural network to defend against adversarial attacks", "pdf": "/pdf/9076625201c7930f54c5f98d51c5d00273de4763.pdf", "paperhash": "goldstein|calibration_of_neural_network_logit_vectors_to_combat_adversarial_attacks", "_bibtex": "@misc{\ngoldstein2019calibration,\ntitle={Calibration of neural network logit vectors to combat adversarial attacks},\nauthor={Oliver Goldstein},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkxdqj0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper546/Official_Review", "cdate": 1542234436831, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Bkxdqj0cFQ", "replyto": "Bkxdqj0cFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper546/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335748685, "tmdate": 1552335748685, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper546/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hkgp682Ihm", "original": null, "number": 1, "cdate": 1540961989270, "ddate": null, "tcdate": 1540961989270, "tmdate": 1541533901589, "tddate": null, "forum": "Bkxdqj0cFQ", "replyto": "Bkxdqj0cFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper546/Official_Review", "content": {"title": "Review", "review": "Strength: \n\nIntuition that multiple sources of uncertainty are relevant to adversarial examples \n\nWeaknesses:\n\nThreat model is unclear\nNo adaptive adversaries are considered\nAttack parameters could be better justified\n\nThe intuition presented at the beginning of Section 4 is interesting. There are indeed multiple sources of uncertainty in machine learning, and the softmax probability only captures confidence partially. In particular, estimating the support of training data for a particular prediction and the density of that support is conceptually relevant to understanding and mitigating outlier test points like adversarial examples. \n\nGiven that the approach is motivated as a defense (see Section 7 for instance), it needs to be evaluated in a realistic adversarial setting. In particular, it would greatly strengthen the paper if a clear threat model was specified. In your rebuttal, would you be able to formulate clearly what adversarial capabilities and goals were assumed when designing this defense? \n\nAll experiments are performed on a binary variant of CIFAR-10. In addition, all pairs chosen for the experiments are well-separated: dogs are semantically further apart from airplanes than they are from horses. Would you be able to clarify in your rebuttal how the approach would generalize to multi-class classification? \n\nPerhaps the strongest limitation of the evaluation is that it does not consider adaptive adversaries. This goes back to the threat model point raised previously. Adaptive strategies will be put forward by adversaries aware of the defense being deployed (security should not be obtained through obscurity). For instance, the adversary could modify their attack to have it minimize the difference between logits on the training and adversarial data. This would help evading detection by the proposed scheme. However, results from Section 6 are shown for attacks that do not attempt to reduce the L1 difference between adversarial and training data. \n\nSome attack parameters could also be better justified. The naming convention for the perturbation sizes reads a bit imprecise and is perhaps more confusing than it is informative. Furthermore, could you explain in your rebuttal why epsilon is larger than 1.0 for the FGSM---when the inputs where normalized between 0 and 1?\n\nDetails: \n\nPage 1: Typo in \u201cdefence\u201d\nPage 2: Notation s_i is overloaded multiple times making it difficult to parse expressions\nPage 2: Citation to Kull et al. should use \\citep after \u201cBeta calibration\u201d\nPage 3: Citation to Rozsa et al. should use \\citep after \u201cPASS score\u201d\nPage 5: Generally-speaking, it\u2019s best to compute attacks at the logit layer rather than the probabilities to avoid numerical instabilities, which can then lead to gradient masking. However, the following sentence suggests the opposite: \u201cThe attacks were all white-box attacks and performed on the network which included a final softmax layer in its structure.\u201d\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper546/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Calibration of neural network logit vectors to combat adversarial attacks", "abstract": "Adversarial examples remain an issue for contemporary neural networks. This paper draws on Background Check (Perello-Nieto et al., 2016), a technique in model calibration, to assist two-class neural networks in detecting adversarial examples, using the one dimensional difference between logit values as the underlying measure. This method interestingly tends to achieve the highest average recall on image sets that are generated with large perturbation vectors, which is unlike the existing literature on adversarial attacks (Cubuk et al., 2017). The proposed method does not need knowledge of the attack parameters or methods at training time, unlike a great deal of the literature that uses deep learning based methods to detect adversarial examples, such as Metzen et al. (2017), imbuing the proposed method with additional flexibility.", "keywords": ["Adversarial attacks", "calibration", "probability", "adversarial defence"], "authorids": ["og14775@my.bristol.ac.uk"], "authors": ["Oliver Goldstein"], "TL;DR": "This paper uses principles from the field of calibration in machine learning on the logits of a neural network to defend against adversarial attacks", "pdf": "/pdf/9076625201c7930f54c5f98d51c5d00273de4763.pdf", "paperhash": "goldstein|calibration_of_neural_network_logit_vectors_to_combat_adversarial_attacks", "_bibtex": "@misc{\ngoldstein2019calibration,\ntitle={Calibration of neural network logit vectors to combat adversarial attacks},\nauthor={Oliver Goldstein},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkxdqj0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper546/Official_Review", "cdate": 1542234436831, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Bkxdqj0cFQ", "replyto": "Bkxdqj0cFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper546/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335748685, "tmdate": 1552335748685, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper546/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HyltlbNMhX", "original": null, "number": 1, "cdate": 1540665585131, "ddate": null, "tcdate": 1540665585131, "tmdate": 1540665585131, "tddate": null, "forum": "Bkxdqj0cFQ", "replyto": "ryluik7Cim", "invitation": "ICLR.cc/2019/Conference/-/Paper546/Official_Comment", "content": {"title": "Confirmation that no adaptive attack was performed against the defense.", "comment": "This threat model was not considered in the context of this defense. An attack that optimised for this defense, would attempt to find examples with similar logit differences to the test and training data. This could very well happen. This paper demonstrates and then exploits the fact that the logit differences for adversarial examples tend to be far larger or far smaller than the training and test data. Your criticism of the nature of the empirical result in this paper is valid. Future work would definitely explore such a threat model."}, "signatures": ["ICLR.cc/2019/Conference/Paper546/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper546/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper546/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Calibration of neural network logit vectors to combat adversarial attacks", "abstract": "Adversarial examples remain an issue for contemporary neural networks. This paper draws on Background Check (Perello-Nieto et al., 2016), a technique in model calibration, to assist two-class neural networks in detecting adversarial examples, using the one dimensional difference between logit values as the underlying measure. This method interestingly tends to achieve the highest average recall on image sets that are generated with large perturbation vectors, which is unlike the existing literature on adversarial attacks (Cubuk et al., 2017). The proposed method does not need knowledge of the attack parameters or methods at training time, unlike a great deal of the literature that uses deep learning based methods to detect adversarial examples, such as Metzen et al. (2017), imbuing the proposed method with additional flexibility.", "keywords": ["Adversarial attacks", "calibration", "probability", "adversarial defence"], "authorids": ["og14775@my.bristol.ac.uk"], "authors": ["Oliver Goldstein"], "TL;DR": "This paper uses principles from the field of calibration in machine learning on the logits of a neural network to defend against adversarial attacks", "pdf": "/pdf/9076625201c7930f54c5f98d51c5d00273de4763.pdf", "paperhash": "goldstein|calibration_of_neural_network_logit_vectors_to_combat_adversarial_attacks", "_bibtex": "@misc{\ngoldstein2019calibration,\ntitle={Calibration of neural network logit vectors to combat adversarial attacks},\nauthor={Oliver Goldstein},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkxdqj0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper546/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623861, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bkxdqj0cFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper546/Authors", "ICLR.cc/2019/Conference/Paper546/Reviewers", "ICLR.cc/2019/Conference/Paper546/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper546/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper546/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper546/Authors|ICLR.cc/2019/Conference/Paper546/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper546/Reviewers", "ICLR.cc/2019/Conference/Paper546/Authors", "ICLR.cc/2019/Conference/Paper546/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623861}}}, {"id": "ryluik7Cim", "original": null, "number": 1, "cdate": 1540399007995, "ddate": null, "tcdate": 1540399007995, "tmdate": 1540399007995, "tddate": null, "forum": "Bkxdqj0cFQ", "replyto": "Bkxdqj0cFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper546/Public_Comment", "content": {"comment": "Much prior work has already shown FGSM/PGD/JSMA can be detected when the adversary is not attempting to evade the defense (Metzen et al. 2017, Grosse et al. 2017). However, Carlini & Wagner 2017 showed that almost all of these detection schemes could be easily bypassed if the attacker actively attempts to evade the defense.\n\nHave you tried such an attack?\n\nNicholas Carlini and David A. Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods. AISec 2017.", "title": "No adaptive attack?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper546/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Calibration of neural network logit vectors to combat adversarial attacks", "abstract": "Adversarial examples remain an issue for contemporary neural networks. This paper draws on Background Check (Perello-Nieto et al., 2016), a technique in model calibration, to assist two-class neural networks in detecting adversarial examples, using the one dimensional difference between logit values as the underlying measure. This method interestingly tends to achieve the highest average recall on image sets that are generated with large perturbation vectors, which is unlike the existing literature on adversarial attacks (Cubuk et al., 2017). The proposed method does not need knowledge of the attack parameters or methods at training time, unlike a great deal of the literature that uses deep learning based methods to detect adversarial examples, such as Metzen et al. (2017), imbuing the proposed method with additional flexibility.", "keywords": ["Adversarial attacks", "calibration", "probability", "adversarial defence"], "authorids": ["og14775@my.bristol.ac.uk"], "authors": ["Oliver Goldstein"], "TL;DR": "This paper uses principles from the field of calibration in machine learning on the logits of a neural network to defend against adversarial attacks", "pdf": "/pdf/9076625201c7930f54c5f98d51c5d00273de4763.pdf", "paperhash": "goldstein|calibration_of_neural_network_logit_vectors_to_combat_adversarial_attacks", "_bibtex": "@misc{\ngoldstein2019calibration,\ntitle={Calibration of neural network logit vectors to combat adversarial attacks},\nauthor={Oliver Goldstein},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkxdqj0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper546/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311814960, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "Bkxdqj0cFQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper546/Authors", "ICLR.cc/2019/Conference/Paper546/Reviewers", "ICLR.cc/2019/Conference/Paper546/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper546/Authors", "ICLR.cc/2019/Conference/Paper546/Reviewers", "ICLR.cc/2019/Conference/Paper546/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311814960}}}], "count": 10}