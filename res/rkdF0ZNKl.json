{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028575959, "tcdate": 1490028575959, "number": 1, "id": "rkO7_Ypjx", "invitation": "ICLR.cc/2017/workshop/-/paper62/acceptance", "forum": "rkdF0ZNKl", "replyto": "rkdF0ZNKl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Generation for Convolutional Autoregressive Models", "abstract": "Convolutional autoregressive models have recently demonstrated state-of-the-art performance on a number of generation tasks. While fast, parallel training methods have been crucial for their success, generation is typically implemented in a naive fashion where redundant computations are unnecessarily repeated. This results in slow generation, making such models infeasible for production environments. In this work, we describe a method to speed up generation in convolutional autoregressive models. The key idea is to cache hidden states to avoid redundant computation. We apply our fast generation method to the Wavenet and PixelCNN++ models and achieve up to 21x and 183x speedups respectively. ", "pdf": "/pdf/507e60a68f04aa025122162daa36917022510661.pdf", "TL;DR": "We significantly speedup the generation in autoregressive models like Wavenet and PixelCNN up to 183 times.", "paperhash": "ramachandran|fast_generation_for_convolutional_autoregressive_models", "keywords": ["Deep learning", "Applications"], "conflicts": ["illinois.edu", "ibm.com"], "authors": ["Prajit Ramachandran", "Tom Le Paine", "Pooya Khorrami", "Mohammad Babaeizadeh", "Shiyu Chang", "Yang Zhang", "Mark A. Hasegawa-Johnson", "Roy H. Campbell", "Thomas S. Huang"], "authorids": ["prmchnd2@illinois.edu", "paine1@illinois.edu", "pkhorra2@illinois.edu", "mb2@illinois.edu", "chang87@illinois.edu", "yzhan143@illinois.edu", "jhasegaw@illinois.edu", "rhc@illinois.edu", "t-huang1@illinois.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028576550, "id": "ICLR.cc/2017/workshop/-/paper62/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rkdF0ZNKl", "replyto": "rkdF0ZNKl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028576550}}}, {"tddate": null, "tmdate": 1489188757381, "tcdate": 1489188757381, "number": 2, "id": "rkpqv3xoe", "invitation": "ICLR.cc/2017/workshop/-/paper62/official/review", "forum": "rkdF0ZNKl", "replyto": "rkdF0ZNKl", "signatures": ["ICLR.cc/2017/workshop/paper62/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper62/AnonReviewer1"], "content": {"title": "simple and good", "rating": "7: Good paper, accept", "review": "This is a nice workshop paper.  its a simple idea but people will be interested in it.  If nothing else, the released code is valuable, and having the poster to advertise it is a good use of workshop poster space.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Generation for Convolutional Autoregressive Models", "abstract": "Convolutional autoregressive models have recently demonstrated state-of-the-art performance on a number of generation tasks. While fast, parallel training methods have been crucial for their success, generation is typically implemented in a naive fashion where redundant computations are unnecessarily repeated. This results in slow generation, making such models infeasible for production environments. In this work, we describe a method to speed up generation in convolutional autoregressive models. The key idea is to cache hidden states to avoid redundant computation. We apply our fast generation method to the Wavenet and PixelCNN++ models and achieve up to 21x and 183x speedups respectively. ", "pdf": "/pdf/507e60a68f04aa025122162daa36917022510661.pdf", "TL;DR": "We significantly speedup the generation in autoregressive models like Wavenet and PixelCNN up to 183 times.", "paperhash": "ramachandran|fast_generation_for_convolutional_autoregressive_models", "keywords": ["Deep learning", "Applications"], "conflicts": ["illinois.edu", "ibm.com"], "authors": ["Prajit Ramachandran", "Tom Le Paine", "Pooya Khorrami", "Mohammad Babaeizadeh", "Shiyu Chang", "Yang Zhang", "Mark A. Hasegawa-Johnson", "Roy H. Campbell", "Thomas S. Huang"], "authorids": ["prmchnd2@illinois.edu", "paine1@illinois.edu", "pkhorra2@illinois.edu", "mb2@illinois.edu", "chang87@illinois.edu", "yzhan143@illinois.edu", "jhasegaw@illinois.edu", "rhc@illinois.edu", "t-huang1@illinois.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489188758116, "id": "ICLR.cc/2017/workshop/-/paper62/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper62/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper62/AnonReviewer2", "ICLR.cc/2017/workshop/paper62/AnonReviewer1"], "reply": {"forum": "rkdF0ZNKl", "replyto": "rkdF0ZNKl", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper62/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper62/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489188758116}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489168792643, "tcdate": 1489168726329, "number": 1, "id": "B1C8tvgil", "invitation": "ICLR.cc/2017/workshop/-/paper62/official/review", "forum": "rkdF0ZNKl", "replyto": "rkdF0ZNKl", "signatures": ["ICLR.cc/2017/workshop/paper62/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper62/AnonReviewer2"], "content": {"title": "Very simple idea, but likely to be used", "rating": "7: Good paper, accept", "review": "This paper proposes a simple technique for speeding up generation from Convolutional Autoregressive Models (e.g., WaveNet and PixelCNN). The key observation is that if one naively generates each output from scratch without re-using any computation, then it is wasteful. The paper instead proposes to cache hidden state values across the generation of all the outputs that share the intermediate results. Experimentally the paper shows large speedups over the naive approach when the depth of a WaveNet is increased to 13+ layers and PixelCNN++ when the batch size is large.\n\nOverall the paper is clear, and the approach is a clear improvement over the naive version. One question I have, though, is if it wouldn't be simpler to just build a TensorFlow model that generates an entire output at once. That is, instead of building a TensorFlow model that generates the next pixel and then calling this model repeatedly, would it be possible to define a TensorFlow model that outputs a full image?  (To deal with having to sample output values, the Gumbel-max trick could be used with all of the randomness needed supplied as an input). Then presumably the TensorFlow execution model would take care of all the necessary caching.\n\nA second question is about the relevance of the technique in the WaveNet experiments. The headline improvement of 21x doesn't happen until there are 15 layers in the WaveNet. Is this a useful parameter regime for the model?\n\nPros:\n- This is a clearly better method than the naive approach, and the naive approach does appear to have been used before\n- The idea is simple and clearly explained\n- The authors are open-sourcing their implementation, which will likely be used by a number of people in the ICLR audience\n\nCons:\n- It's not obvious to me that this is the simplest way to implement the idea\n- The idea is very simple, effectively being \"cache in the obvious way\"\n\nOverall I'd lean towards accepting, but I wouldn't fight strongly for it.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Generation for Convolutional Autoregressive Models", "abstract": "Convolutional autoregressive models have recently demonstrated state-of-the-art performance on a number of generation tasks. While fast, parallel training methods have been crucial for their success, generation is typically implemented in a naive fashion where redundant computations are unnecessarily repeated. This results in slow generation, making such models infeasible for production environments. In this work, we describe a method to speed up generation in convolutional autoregressive models. The key idea is to cache hidden states to avoid redundant computation. We apply our fast generation method to the Wavenet and PixelCNN++ models and achieve up to 21x and 183x speedups respectively. ", "pdf": "/pdf/507e60a68f04aa025122162daa36917022510661.pdf", "TL;DR": "We significantly speedup the generation in autoregressive models like Wavenet and PixelCNN up to 183 times.", "paperhash": "ramachandran|fast_generation_for_convolutional_autoregressive_models", "keywords": ["Deep learning", "Applications"], "conflicts": ["illinois.edu", "ibm.com"], "authors": ["Prajit Ramachandran", "Tom Le Paine", "Pooya Khorrami", "Mohammad Babaeizadeh", "Shiyu Chang", "Yang Zhang", "Mark A. Hasegawa-Johnson", "Roy H. Campbell", "Thomas S. Huang"], "authorids": ["prmchnd2@illinois.edu", "paine1@illinois.edu", "pkhorra2@illinois.edu", "mb2@illinois.edu", "chang87@illinois.edu", "yzhan143@illinois.edu", "jhasegaw@illinois.edu", "rhc@illinois.edu", "t-huang1@illinois.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489188758116, "id": "ICLR.cc/2017/workshop/-/paper62/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper62/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper62/AnonReviewer2", "ICLR.cc/2017/workshop/paper62/AnonReviewer1"], "reply": {"forum": "rkdF0ZNKl", "replyto": "rkdF0ZNKl", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper62/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper62/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489188758116}}}, {"tddate": null, "replyto": null, "nonreaders": null, "ddate": null, "tmdate": 1487698100780, "tcdate": 1487310464037, "number": 62, "id": "rkdF0ZNKl", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "rkdF0ZNKl", "signatures": ["~Mohammad_Babaeizadeh1"], "readers": ["everyone"], "content": {"title": "Fast Generation for Convolutional Autoregressive Models", "abstract": "Convolutional autoregressive models have recently demonstrated state-of-the-art performance on a number of generation tasks. While fast, parallel training methods have been crucial for their success, generation is typically implemented in a naive fashion where redundant computations are unnecessarily repeated. This results in slow generation, making such models infeasible for production environments. In this work, we describe a method to speed up generation in convolutional autoregressive models. The key idea is to cache hidden states to avoid redundant computation. We apply our fast generation method to the Wavenet and PixelCNN++ models and achieve up to 21x and 183x speedups respectively. ", "pdf": "/pdf/507e60a68f04aa025122162daa36917022510661.pdf", "TL;DR": "We significantly speedup the generation in autoregressive models like Wavenet and PixelCNN up to 183 times.", "paperhash": "ramachandran|fast_generation_for_convolutional_autoregressive_models", "keywords": ["Deep learning", "Applications"], "conflicts": ["illinois.edu", "ibm.com"], "authors": ["Prajit Ramachandran", "Tom Le Paine", "Pooya Khorrami", "Mohammad Babaeizadeh", "Shiyu Chang", "Yang Zhang", "Mark A. Hasegawa-Johnson", "Roy H. Campbell", "Thomas S. Huang"], "authorids": ["prmchnd2@illinois.edu", "paine1@illinois.edu", "pkhorra2@illinois.edu", "mb2@illinois.edu", "chang87@illinois.edu", "yzhan143@illinois.edu", "jhasegaw@illinois.edu", "rhc@illinois.edu", "t-huang1@illinois.edu"]}, "writers": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 4}