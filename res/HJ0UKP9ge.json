{"notes": [{"tddate": null, "nonreaders": null, "tmdate": 1495085491854, "tcdate": 1495084902685, "number": 19, "id": "S1Ju1h5lZ", "invitation": "ICLR.cc/2017/conference/-/paper411/public/comment", "forum": "HJ0UKP9ge", "replyto": "HJ0UKP9ge", "signatures": ["~Rajarshee_Mitra1"], "readers": ["everyone"], "writers": ["~Rajarshee_Mitra1"], "content": {"title": "Can't understand few intuitions behind some operations", "comment": "The authors are doing some operations but I cannot understand the intuitions (or precisely causes) behind them. Are those experiments-backed  or have solid theoretical reasoning ?\n\nTo begin with, I think it could have been made more clear behind the choice of similarity functions used - alpha and beta (using paper notations), mostly the choice of operands inside concatenations. For eg. while computing alpha, what might be the need of concatenating h and u when hadamard product of h and u alone might do the job. Moreover, while computing G, what is the reason behind using contextual embedding H and why U~ and H~ being attended embedding are not enough ?\n\nAlso, in the output layer, to predict the start index the probability distributions are computed using the concatenation of G and M. But why M is passed through a bi-LSTM to get M^2 and then used G and M^2 to predict the end index. I mean what is the intuition behind the bi-LSTM here. \n\nHowever, I found the Modeling Layer quite intuitive -- capturing interaction between context words which are already query-aware from the previous Attention Flow Layer!\n\n"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287589403, "id": "ICLR.cc/2017/conference/-/paper411/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJ0UKP9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper411/reviewers", "ICLR.cc/2017/conference/paper411/areachairs"], "cdate": 1485287589403}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488588071818, "tcdate": 1478289750329, "number": 411, "id": "HJ0UKP9ge", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HJ0UKP9ge", "signatures": ["~Minjoon_Seo1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 27, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1487965941516, "tcdate": 1487965941516, "number": 18, "id": "rkpxyGAKl", "invitation": "ICLR.cc/2017/conference/-/paper411/public/comment", "forum": "HJ0UKP9ge", "replyto": "HJ0UKP9ge", "signatures": ["~Minjoon_Seo1"], "readers": ["everyone"], "writers": ["~Minjoon_Seo1"], "content": {"title": "Revised on 24 Feb 2017", "comment": "Final version uploaded:\n- fixed typos\n- more spacious formatting\n- added acknowledgments"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287589403, "id": "ICLR.cc/2017/conference/-/paper411/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJ0UKP9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper411/reviewers", "ICLR.cc/2017/conference/paper411/areachairs"], "cdate": 1485287589403}}}, {"tddate": null, "ddate": null, "writable": true, "revisions": false, "tmdate": 1486400507679, "tcdate": 1486075663525, "number": 17, "replyCount": 0, "id": "rJvGDEZ_x", "invitation": "ICLR.cc/2017/conference/-/paper411/public/comment", "forum": "HJ0UKP9ge", "replyto": "HJ0UKP9ge", "signatures": ["~Minjoon_Seo1"], "readers": ["everyone"], "writers": ["~Minjoon_Seo1"], "content": {"title": "Revised on 2 Feb 2017; Voided on 6 Feb 2017", "comment": "- Added WikiQA Experiment (Appendix C for now), giving <2% higher performance than the previous SOTA without seeing any example on WikiQA, i.e. zero-shot scenario.\n- Voided the above revision, because zero-shot is a widely different experiment from the other QA experiments."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287589403, "id": "ICLR.cc/2017/conference/-/paper411/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJ0UKP9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper411/reviewers", "ICLR.cc/2017/conference/paper411/areachairs"], "cdate": 1485287589403}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396566518, "tcdate": 1486396566518, "number": 1, "id": "HkR52zUul", "invitation": "ICLR.cc/2017/conference/-/paper411/acceptance", "forum": "HJ0UKP9ge", "replyto": "HJ0UKP9ge", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The program committee appreciates the authors' response to concerns raised in the reviews. All reviewers agree that this is a good piece of work that should be accepted to ICLR. Authors are encouraged to incorporate reviewer feedback to further strengthen the paper.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396567053, "id": "ICLR.cc/2017/conference/-/paper411/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HJ0UKP9ge", "replyto": "HJ0UKP9ge", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396567053}}}, {"tddate": null, "tmdate": 1484957226001, "tcdate": 1484957226001, "number": 2, "id": "rJMNLmeDg", "invitation": "ICLR.cc/2017/conference/-/paper411/official/comment", "forum": "HJ0UKP9ge", "replyto": "HkpQsp0Sl", "signatures": ["ICLR.cc/2017/conference/paper411/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper411/AnonReviewer2"], "content": {"title": "Accept!", "comment": "After reading the the comments of other reviewers and the answers of the authors I stay with my recommendation to accept the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287589265, "id": "ICLR.cc/2017/conference/-/paper411/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "HJ0UKP9ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper411/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper411/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper411/reviewers", "ICLR.cc/2017/conference/paper411/areachairs"], "cdate": 1485287589265}}}, {"tddate": null, "tmdate": 1483819851003, "tcdate": 1483819572403, "number": 13, "id": "r1hNca0rg", "invitation": "ICLR.cc/2017/conference/-/paper411/public/comment", "forum": "HJ0UKP9ge", "replyto": "ry5GeJBBx", "signatures": ["~Minjoon_Seo1"], "readers": ["everyone"], "writers": ["~Minjoon_Seo1"], "content": {"title": "Fixed", "comment": "Thank you for spotting this! Now it is fixed :)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287589403, "id": "ICLR.cc/2017/conference/-/paper411/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJ0UKP9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper411/reviewers", "ICLR.cc/2017/conference/paper411/areachairs"], "cdate": 1485287589403}}}, {"tddate": null, "tmdate": 1483819812860, "tcdate": 1483819812860, "number": 16, "id": "HkpQsp0Sl", "invitation": "ICLR.cc/2017/conference/-/paper411/public/comment", "forum": "HJ0UKP9ge", "replyto": "H1xR7sfNe", "signatures": ["~Minjoon_Seo1"], "readers": ["everyone"], "writers": ["~Minjoon_Seo1"], "content": {"title": "Our response to R2's review", "comment": "We thank reviewer R2 for insightful comments and suggestions.\n\nRegarding qualitative results, please note that the paper already includes qualitative results for correct (Figure 3) and incorrect (Appendix A) examples. We will add more analysis on success cases in the appendix of a future revision. \n\nRegarding missing predicted answer in Figure 3, we used a red font to denote the the answer words, but perhaps this is not clear in grayscale prints. We have now made this more clear with underlines in the current revision.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287589403, "id": "ICLR.cc/2017/conference/-/paper411/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJ0UKP9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper411/reviewers", "ICLR.cc/2017/conference/paper411/areachairs"], "cdate": 1485287589403}}}, {"tddate": null, "tmdate": 1483819781870, "tcdate": 1483819781870, "number": 15, "id": "S1CWj6CHl", "invitation": "ICLR.cc/2017/conference/-/paper411/public/comment", "forum": "HJ0UKP9ge", "replyto": "HJNbgWFEg", "signatures": ["~Minjoon_Seo1"], "readers": ["everyone"], "writers": ["~Minjoon_Seo1"], "content": {"title": "Our response to R3's review", "comment": "We thank reviewer R3 for insightful comments and suggestions.\n\nRegarding qualitative examples of success and failures, please note that we have already listed some success cases in Figure 3 and failures cases in Appendix A. We will add a further analysis on success cases in the appendix of a future revision. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287589403, "id": "ICLR.cc/2017/conference/-/paper411/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJ0UKP9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper411/reviewers", "ICLR.cc/2017/conference/paper411/areachairs"], "cdate": 1485287589403}}}, {"tddate": null, "tmdate": 1483819637963, "tcdate": 1483819637963, "number": 14, "id": "B1R_5pArl", "invitation": "ICLR.cc/2017/conference/-/paper411/public/comment", "forum": "HJ0UKP9ge", "replyto": "HJ5Bubo4l", "signatures": ["~Minjoon_Seo1"], "readers": ["everyone"], "writers": ["~Minjoon_Seo1"], "content": {"title": "Our response to R1's review", "comment": "We thank reviewer R1 for insightful comments, suggestions as well as pointing out some spelling and grammar issues. We have incorporated this feedback in our latest revision and we discuss these changes below.\n\nNovelty of BiDAF: \nMost modern neural network models consist of standard elements, and how they are composed is often a major contribution. As pointed out by reviewers R2 and R3, the major novelties of our model are: the memory-less attention flow mechanism and the bi-directionality of the attention, as well as how they can be combined with other standard elements (Char-CNN, LSTM, etc.) to perform well in machine comprehension tasks.\n\nSecond paragraph of the introduction is unclear: \nWe have reworded the first and second paragraphs to make them more clear to readers. \n\n\u201cMost previous work summarizes each modality into a single vector\u201d is incorrect: \nWe meant that in most previous MC and QA works, the computed attention weights are often used to summarize the context (paragraph or image) into a single fixed-size vector, thus extracting the most relevant information from the context for answering the question. We realize that our previous wording could have been a bit confusing, and we have updated the paper accordingly.\n\n\u201cModeling layer\u201d: \nWe named it \u201cmodeling layer\u201d to merely indicate its functionality. This is similar to the nomenclature used for others layers in the network (similar to R1\u2019s suggestion on using the name \u201ccontextual embedding layer\u201d). We use RNNs in several different layers in the system, but they play different roles in different layers, and the names assigned to the different layers in the system indicate these roles.\n\n\u201cPhrase embedding layer\u201d: \nFollowing the suggestion, we have changed this to \u201ccontextual embedding layer\u201d.\n\nSection 2 point 4 needs citation for \u201cpopular\u201d: \nPlease note that Section 3 contains a detailed list of related work. In the current revision, we have now added citations to Memory Networks (Weston et al., 2015) and three recent papers in machine comprehension tasks.\n\nOutput layer has changed to adapt to CNN/DailyMail: \nIn the current revision, we now mention in the introduction that we changed the output module for the CNN/DailyMail dataset.\n\nTypos and grammatical issues:\nWe have fixed several typos as well as grammatical issues throughout the paper. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287589403, "id": "ICLR.cc/2017/conference/-/paper411/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJ0UKP9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper411/reviewers", "ICLR.cc/2017/conference/paper411/areachairs"], "cdate": 1485287589403}}}, {"tddate": null, "tmdate": 1483819531725, "tcdate": 1483819531725, "number": 12, "id": "S1VG9pRHe", "invitation": "ICLR.cc/2017/conference/-/paper411/public/comment", "forum": "HJ0UKP9ge", "replyto": "HJ0UKP9ge", "signatures": ["~Minjoon_Seo1"], "readers": ["everyone"], "writers": ["~Minjoon_Seo1"], "content": {"title": "Revised on 7 Jan 2017", "comment": "Incorporated reviewers' and Kelly Zhang's suggestions:\n- Fixed typos / grammatical errors\n- \"phase embedding layer\" -> \"contextual embedding layer\"\n- Added more citation to Section 2\n- Reworded abstract and first two paragraphs of intro\n- Fixed Stanford AR reader's numbers"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287589403, "id": "ICLR.cc/2017/conference/-/paper411/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJ0UKP9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper411/reviewers", "ICLR.cc/2017/conference/paper411/areachairs"], "cdate": 1485287589403}}}, {"tddate": null, "tmdate": 1483169810459, "tcdate": 1483169810459, "number": 11, "id": "ry5GeJBBx", "invitation": "ICLR.cc/2017/conference/-/paper411/public/comment", "forum": "HJ0UKP9ge", "replyto": "HJ0UKP9ge", "signatures": ["~Kelly_Zhang1"], "readers": ["everyone"], "writers": ["~Kelly_Zhang1"], "content": {"title": "Stanford AR Performance", "comment": "In Table 3 (Results), you cite Chen et al.'s Stanford AR reader performance on CNN and DailyMail, but I think the numbers are incorrect. You can check for yourself here: https://arxiv.org/abs/1606.02858\n\nIn fact in your table, the performance numbers for the Stanford AR reader are same as those for the AS Reader by Kadlec et al. in the previous line. I think this is a typo.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287589403, "id": "ICLR.cc/2017/conference/-/paper411/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJ0UKP9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper411/reviewers", "ICLR.cc/2017/conference/paper411/areachairs"], "cdate": 1485287589403}}}, {"tddate": null, "tmdate": 1482524874987, "tcdate": 1482524738369, "number": 3, "id": "HJ5Bubo4l", "invitation": "ICLR.cc/2017/conference/-/paper411/official/review", "forum": "HJ0UKP9ge", "replyto": "HJ0UKP9ge", "signatures": ["ICLR.cc/2017/conference/paper411/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper411/AnonReviewer1"], "content": {"title": "Conditional accept", "rating": "7: Good paper, accept", "review": "This is a solid paper with good results. However, there aren't many very interesting takeaways (most of the architecture seems a concatenation of standard elements to do well in the leaderboard) and some issues in the writing.\n\nThe second paragraph of the introduction is very confusing. It's clear the authors got really deep into their world and made no attempts to actually clearly explain their model, not even to an expert in the field, let a lone somebody who isn't familiar with similar approaches.\nThe authors keep referring to \"previously popular attention paradigms\" without any citation and then, I believe, incorrectly describe whatever those are supposed to be by writing that these unknown but popular approaches \"summarize each modality into a single vector.\" That's one of the most incorrect descriptions I've yet seen for attention mechanisms. First, I don't know what model works over several modalities in a single attention pass. Maybe the authors don't know what a \"modality\" is? More importantly, the whole point of most attention mechanisms is that one does not simply summarize the whole input but instead can access all elements of it. So, this paper's supposedly new way of using attention is pretty much exactly the standard way.\nBoth modeling and modelling spellings are in the text.\nI understand the need to sometimes invent new terminology to describe a model but in this paragraph, the authors 3 times talk about a \"modeling layer (RNN)\"... It's just an RNN, you don't need to give an RNN another name, especially one that's as nondescript as \"modeling layer\" all layers are part of a model? \nTypo: \"let's the modeling (RNN) layer to learn\"\nThis paragraph is supposed to give an overview of the model but just confuses readers.\nI would delete it.\n\n\"Phrase embedding layer\" -- terrible word choice as you are not embedding phrases here. It's a standard bidirectional LSTM over words, not phrases. In all subsequent parts of the paper you just give examples of words embedded in context. No phrases. Please change this to \"contextual word embedding layer\" or something less incorrect.\nYour phrase layer embeddings only show single words, as expected in Table 2.\nSection 2: point 4. Second sentence needs citations for \"popular\" \nTypo: \"from both *of* the context and query word\"\nTypo: \"aveaged\"\n\nIt seems like your output layer changes quite substantially so your claim in the abstract/intro of using the same model isn't quite accurate. I'd say you're changing one module or part of your model.\nSection 4: attention isn't countable (no \"a\" in front of \"huge attention\"). Also, academic writing usually doesn't include such adjectives in the first place. ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482524739041, "id": "ICLR.cc/2017/conference/-/paper411/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper411/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper411/AnonReviewer2", "ICLR.cc/2017/conference/paper411/AnonReviewer3", "ICLR.cc/2017/conference/paper411/AnonReviewer1"], "reply": {"forum": "HJ0UKP9ge", "replyto": "HJ0UKP9ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper411/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper411/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482524739041}}}, {"tddate": null, "tmdate": 1482391548219, "tcdate": 1482391548219, "number": 2, "id": "HJNbgWFEg", "invitation": "ICLR.cc/2017/conference/-/paper411/official/review", "forum": "HJ0UKP9ge", "replyto": "HJ0UKP9ge", "signatures": ["ICLR.cc/2017/conference/paper411/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper411/AnonReviewer3"], "content": {"title": "Review", "rating": "8: Top 50% of accepted papers, clear accept", "review": "\nPaper Summary: \nThe paper presents a novel approach for machine comprehension. The proposed model, Bi-Directional Attention Flow (BIDAF) network is a multi-stage hierarchical approach that represents the context and query at different levels of granularity and uses bi-directional attention mechanisms to predict an answer. The proposed approach achieves state-of-the-art results on SQuAD dataset and CNN/DailyMail cloze test. \n\nPaper Strengths: \n-- The proposed model uses attention in both directions instead of uni-directional attention mechanisms used in previous approaches, prevents early summarization by passing all information to RNNs and uses memory-less attention mechanism, which is a novel combination for machine comprehension task.\n-- The proposed model is modular, and can be easily changed for other related tasks, as shown through two types of experiments in the paper.\n-- The paper presents thorough ablation study of the proposed model, clearly presenting the importance of all major components in the model. The authors also added further detailed studies as requested by reviewer. \n-- Relation of their model to Visual Question Answering approaches, and computing features from context and query at different levels of granularity to multiple layers in Convolutional Neural Networks, are interesting and help in better understanding of the model. It would be very interesting to see how this approach would work for the task of Visual Question Answering.\n-- Further visualizations and error analysis can help in identifying failure modes of the model and help in designing next generation of models.\n-- The proposed model achieves state-of-the-art result on SQuAD dataset, and CNN/DailyMail cloze test.\n-- The paper is well written and the architecture is described in sufficient detail.\n\nPaper Weaknesses / Future Thoughts: \n-- As with many deep learning approaches, the overall architecture seems quite complex, and the design choices seem to be driven by performance numbers. As future work, authors might try to analyze qualitative advantages of different modules in the proposed model. What type of questions are correctly answered after adding Context-to-Query attention? What about Query-to-Context attention? \n-- Some qualitative examples of success and failure cases should be added to the paper.\n\nPreliminary Evaluation: \nNovel, state-of-the-art, and well-studied machine comprehension approach. Paper is well written. In my thoughts, a clear accept.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482524739041, "id": "ICLR.cc/2017/conference/-/paper411/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper411/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper411/AnonReviewer2", "ICLR.cc/2017/conference/paper411/AnonReviewer3", "ICLR.cc/2017/conference/paper411/AnonReviewer1"], "reply": {"forum": "HJ0UKP9ge", "replyto": "HJ0UKP9ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper411/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper411/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482524739041}}}, {"tddate": null, "tmdate": 1481974728321, "tcdate": 1481974728321, "number": 1, "id": "H1xR7sfNe", "invitation": "ICLR.cc/2017/conference/-/paper411/official/review", "forum": "HJ0UKP9ge", "replyto": "HJ0UKP9ge", "signatures": ["ICLR.cc/2017/conference/paper411/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper411/AnonReviewer2"], "content": {"title": "Review: New deep end-to-end attention architecture for machine comprehension with novel aspects and convincing results", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The paper presents an architecture for answering questions about text. The paper proposes a novel architecture which jointly attends over the context and the query.\n\n1.\tThe paper is clearly written and illustrated.\n\n2.\tThe architecture is new and incorporates novel and interesting aspects:\n2.1.\tThe attention is not summarized immediately but the features are only weighted with the attention to not loose information.\n2.2.\tThe approach estimates two directions of attention, by maximizing in two directions of the similarity matrix S \u2013 towards the context and towards the query.\n\n3.\tThe paper extensively evaluates the approach on three datasets SQuAD, CNN and Daily Mail. In all cases showing state-of-the-art performance. It is worth noting that the SQuAD and the CNN/Daily Mail are slightly different tasks and it is positive that the model works well in both scenarios. \n3.1.\tIt is worth noting that the paper even compares mainly favorably to concurrent work (including other ICLR 2017 submissions), recently published/listed on the evaluation server for SQuAD\n\n4.\tThe paper also includes an ablation study and qualitative results.\n\n5.\tI think the paper provides a good discussion of related work and I like that it points out the relations to Visual question answering (VQA). It would be interesting to see how the architecture can be adapted and works on the VQA task. \n\n6.\tThe authors revised the paper based on the comments from reviewers and others.\n\n7.\tIt would be interesting to see more qualitative results, e.g. in an appendix. \n7.1.\tFig. 3 seems to miss the predicted answer. \n7.2.\tIt would also be interesting to compare the results of different approaches, maybe in a more compact format.\n\nGiven the new architecture with novel aspects and the strong experimental evaluation I recommend to accept the paper.\n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482524739041, "id": "ICLR.cc/2017/conference/-/paper411/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper411/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper411/AnonReviewer2", "ICLR.cc/2017/conference/paper411/AnonReviewer3", "ICLR.cc/2017/conference/paper411/AnonReviewer1"], "reply": {"forum": "HJ0UKP9ge", "replyto": "HJ0UKP9ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper411/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper411/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482524739041}}}, {"tddate": null, "tmdate": 1481100339705, "tcdate": 1481100339702, "number": 10, "id": "Hy2E2SHmg", "invitation": "ICLR.cc/2017/conference/-/paper411/public/comment", "forum": "HJ0UKP9ge", "replyto": "S1h5XlgQl", "signatures": ["~Minjoon_Seo1"], "readers": ["everyone"], "writers": ["~Minjoon_Seo1"], "content": {"title": "our response to R2 questions", "comment": "Thank you for your question!\n\nThe Similarity matrix S is computed once, and is used to compute both directions of attention. Note that S is not the attention itself, because it is an unnormalized 2D similarity measure between the context and the query words. However, by pooling and normalizing the matrix on one dimension or the other, we obtain the attention weights in either direction. We made our description of attention more explicit in the newest (current) revision."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287589403, "id": "ICLR.cc/2017/conference/-/paper411/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJ0UKP9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper411/reviewers", "ICLR.cc/2017/conference/paper411/areachairs"], "cdate": 1485287589403}}}, {"tddate": null, "tmdate": 1481100297711, "tcdate": 1481100297706, "number": 9, "id": "rkGz2HBQx", "invitation": "ICLR.cc/2017/conference/-/paper411/public/comment", "forum": "HJ0UKP9ge", "replyto": "HyOV5iyml", "signatures": ["~Minjoon_Seo1"], "readers": ["everyone"], "writers": ["~Minjoon_Seo1"], "content": {"title": "our response to R3 questions", "comment": "Thank you for your questions!\n\nWe are aware of a few different approaches for encoding similarity between vectors, such as dot product (Hill et al., 2016) and bilinear term (Chen et al., 2016). During our experiments, we found that concatenating elementwise multiplications (e.g. h.u) before vanilla linear transformation gives better performance than the previous approaches.\n\nAs you suggested, we included the comparisons between different definitions of Equations 1 and 2 in the newest (current) revision. Please see Table 5 in Appendix B; in short, our choice of similarity and vector functions performs similarly or better than other choices.Thank you for the suggestion!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287589403, "id": "ICLR.cc/2017/conference/-/paper411/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJ0UKP9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper411/reviewers", "ICLR.cc/2017/conference/paper411/areachairs"], "cdate": 1485287589403}}}, {"tddate": null, "tmdate": 1481100238322, "tcdate": 1481100238318, "number": 8, "id": "Hk8CjBHme", "invitation": "ICLR.cc/2017/conference/-/paper411/public/comment", "forum": "HJ0UKP9ge", "replyto": "ryReQt3Ml", "signatures": ["~Minjoon_Seo1"], "readers": ["everyone"], "writers": ["~Minjoon_Seo1"], "content": {"title": "Thanks!", "comment": "Thanks, I hope they are useful!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287589403, "id": "ICLR.cc/2017/conference/-/paper411/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJ0UKP9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper411/reviewers", "ICLR.cc/2017/conference/paper411/areachairs"], "cdate": 1485287589403}}}, {"tddate": null, "tmdate": 1481100181706, "tcdate": 1481100181699, "number": 7, "id": "H109iSrXx", "invitation": "ICLR.cc/2017/conference/-/paper411/public/comment", "forum": "HJ0UKP9ge", "replyto": "BJYIzSRzl", "signatures": ["~Minjoon_Seo1"], "readers": ["everyone"], "writers": ["~Minjoon_Seo1"], "content": {"title": "our response to R1 questions", "comment": "Thank you for your questions!\n\nWe used the term \u201cmachine comprehension\u201d to mean \u201canswering query given context in natural language\u201d, referring to both SQuAD and CNN/DailyMail cloze test. Please note that the terms \u201cmachine comprehension\u201d or \u201creading comprehension\u201d were also used in these dataset papers as well as in their follow-ups. Since QA is a generic term and does not necessarily require \u201ccontext in natural language\u201d (e.g. QA on databases such as Freebase), we thought MC is a better term for the two tasks.\n\nWe just verified that the most recent version of the paper and the leaderboard both report EM=73.3, F1=81.1. Please note that there is usually a delay of 2-3 days between obtaining the test result (by email) and the result being updated on the leaderboard. The difference between the previous model and the current model is highlighted in our Nov 28 revision comment below. In short, we fixed few bugs and loaded moving average instead of raw weights."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287589403, "id": "ICLR.cc/2017/conference/-/paper411/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJ0UKP9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper411/reviewers", "ICLR.cc/2017/conference/paper411/areachairs"], "cdate": 1485287589403}}}, {"tddate": null, "tmdate": 1481100001403, "tcdate": 1481100001397, "number": 6, "id": "HJtJsSB7g", "invitation": "ICLR.cc/2017/conference/-/paper411/public/comment", "forum": "HJ0UKP9ge", "replyto": "HJ0UKP9ge", "signatures": ["~Minjoon_Seo1"], "readers": ["everyone"], "writers": ["~Minjoon_Seo1"], "content": {"title": "Revised on 7 Dec 2016", "comment": "In response to the questions of the reviewers: \n1. Added Appendix B for comparing performance with different choices of similarity and fusion functions.\n2. Clarified the description of the similarity matrix and how the attention is obtained from the similarity matrix."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287589403, "id": "ICLR.cc/2017/conference/-/paper411/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJ0UKP9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper411/reviewers", "ICLR.cc/2017/conference/paper411/areachairs"], "cdate": 1485287589403}}}, {"tddate": null, "tmdate": 1480749972232, "tcdate": 1480749972227, "number": 3, "id": "S1h5XlgQl", "invitation": "ICLR.cc/2017/conference/-/paper411/pre-review/question", "forum": "HJ0UKP9ge", "replyto": "HJ0UKP9ge", "signatures": ["ICLR.cc/2017/conference/paper411/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper411/AnonReviewer2"], "content": {"title": "Clarification", "question": "You write \"These attentions are derived from the similarity matrix, S\".\n\nso the attentions are equal to S, or what is the attention exactly equal to.\n\nThanks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959298119, "id": "ICLR.cc/2017/conference/-/paper411/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper411/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper411/AnonReviewer1", "ICLR.cc/2017/conference/paper411/AnonReviewer3", "ICLR.cc/2017/conference/paper411/AnonReviewer2"], "reply": {"forum": "HJ0UKP9ge", "replyto": "HJ0UKP9ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper411/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper411/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959298119}}}, {"tddate": null, "tmdate": 1480731270530, "tcdate": 1480731183798, "number": 2, "id": "HyOV5iyml", "invitation": "ICLR.cc/2017/conference/-/paper411/pre-review/question", "forum": "HJ0UKP9ge", "replyto": "HJ0UKP9ge", "signatures": ["ICLR.cc/2017/conference/paper411/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper411/AnonReviewer3"], "content": {"title": "Modelling choices", "question": "I am interested in knowing how the authors decided the scalar function (equation 1) that encodes the similarity between the phrase-level embeddings of the context (H) and the query (U). In the paper, they mention that it is a linear function of [h; u; h.u]. Did the authors try any other function e.g., a linear function of [h.u]?\n\nSimilarly, how was the vector function (equation 2) determined? What happens if this function is an MLP?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959298119, "id": "ICLR.cc/2017/conference/-/paper411/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper411/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper411/AnonReviewer1", "ICLR.cc/2017/conference/paper411/AnonReviewer3", "ICLR.cc/2017/conference/paper411/AnonReviewer2"], "reply": {"forum": "HJ0UKP9ge", "replyto": "HJ0UKP9ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper411/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper411/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959298119}}}, {"tddate": null, "tmdate": 1480639057288, "tcdate": 1480639057284, "number": 1, "id": "BJYIzSRzl", "invitation": "ICLR.cc/2017/conference/-/paper411/pre-review/question", "forum": "HJ0UKP9ge", "replyto": "HJ0UKP9ge", "signatures": ["ICLR.cc/2017/conference/paper411/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper411/AnonReviewer1"], "content": {"title": "questions", "question": "\"machine comprehension\" is both an overhyped and underspecified title.\nwhy not just use existing terms like question answering?\n\nThe final ensemble numers in the paper and the SQUAD leaderboard are different. What makes the difference?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959298119, "id": "ICLR.cc/2017/conference/-/paper411/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper411/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper411/AnonReviewer1", "ICLR.cc/2017/conference/paper411/AnonReviewer3", "ICLR.cc/2017/conference/paper411/AnonReviewer2"], "reply": {"forum": "HJ0UKP9ge", "replyto": "HJ0UKP9ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper411/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper411/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959298119}}}, {"tddate": null, "tmdate": 1480524533740, "tcdate": 1480524533736, "number": 5, "id": "ryReQt3Ml", "invitation": "ICLR.cc/2017/conference/-/paper411/public/comment", "forum": "HJ0UKP9ge", "replyto": "HJ0UKP9ge", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "It is interesting to see how training and validation acc. change through time.", "comment": "Although nobody did that, it is interesting to see the training curve and validation err. curve so that we could have better understanding of the model."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287589403, "id": "ICLR.cc/2017/conference/-/paper411/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJ0UKP9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper411/reviewers", "ICLR.cc/2017/conference/paper411/areachairs"], "cdate": 1485287589403}}}, {"tddate": null, "tmdate": 1480383794717, "tcdate": 1480383746504, "number": 4, "id": "rJ5ZpUqGg", "invitation": "ICLR.cc/2017/conference/-/paper411/public/comment", "forum": "HJ0UKP9ge", "replyto": "HJ0UKP9ge", "signatures": ["~Minjoon_Seo1"], "readers": ["everyone"], "writers": ["~Minjoon_Seo1"], "content": {"title": "Revised on 28 Nov 2016", "comment": "Change #1: New results (state of the art) on SQuAD, after fixing some bugs in the preprocessing step of the data and the postprocessing step of the answers, and using moving averages instead of raw weights during test. Also included all recent results in the SQuAD leaderboard as of 12pm 28 Nov 2016.\nChange #2: New results on CNN/DailyMail datasets, after using moving averages instead of raw weights during test during test."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287589403, "id": "ICLR.cc/2017/conference/-/paper411/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJ0UKP9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper411/reviewers", "ICLR.cc/2017/conference/paper411/areachairs"], "cdate": 1485287589403}}}, {"tddate": null, "tmdate": 1479763744076, "tcdate": 1479763744070, "number": 3, "id": "ByOQvkWfx", "invitation": "ICLR.cc/2017/conference/-/paper411/public/comment", "forum": "HJ0UKP9ge", "replyto": "HJ0UKP9ge", "signatures": ["~Minjoon_Seo1"], "readers": ["everyone"], "writers": ["~Minjoon_Seo1"], "content": {"title": "Revised on 21 Nov 2016", "comment": "Change #1. A non-affiliated researcher pointed out that, after carefully examining our publicly available code, entities in CNN/DailyMail datasets should have been shuffled every time an example (article-query pair) is loaded. We re-ran the experiment with the correction (with some changes in hyperparameters), and we now report the corrected single-run results. We will report our ensemble method results as soon as they are available, which take several days/weeks to train.\n\nChange #2. We retouched and clarified the wording of the paper, mainly Sections 1 and 3. \n\n "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287589403, "id": "ICLR.cc/2017/conference/-/paper411/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJ0UKP9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper411/reviewers", "ICLR.cc/2017/conference/paper411/areachairs"], "cdate": 1485287589403}}}, {"tddate": null, "tmdate": 1478734260471, "tcdate": 1478734260463, "number": 2, "id": "B12hZNWZl", "invitation": "ICLR.cc/2017/conference/-/paper411/public/comment", "forum": "HJ0UKP9ge", "replyto": "ByglaX-Wl", "signatures": ["~Minjoon_Seo1"], "readers": ["everyone"], "writers": ["~Minjoon_Seo1"], "content": {"title": "citation fixed", "comment": "Thanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287589403, "id": "ICLR.cc/2017/conference/-/paper411/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJ0UKP9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper411/reviewers", "ICLR.cc/2017/conference/paper411/areachairs"], "cdate": 1485287589403}}}, {"tddate": null, "tmdate": 1478733032436, "tcdate": 1478733032430, "number": 1, "id": "ByglaX-Wl", "invitation": "ICLR.cc/2017/conference/-/paper411/public/comment", "forum": "HJ0UKP9ge", "replyto": "HJ0UKP9ge", "signatures": ["~Bhuwan_Dhingra1"], "readers": ["everyone"], "writers": ["~Bhuwan_Dhingra1"], "content": {"title": "Incorrect citation", "comment": "Hi, the citation to the GA Reader ensemble in Table 3 is incorrect. Best,"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "pdf": "https://arxiv.org/pdf/1611.01603.pdf", "paperhash": "seo|bidirectional_attention_flow_for_machine_comprehension", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["washington.edu", "allenai.org"], "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "authorids": ["minjoon@cs.washington.edu", "anik@allenai.org", "alif@allenai.org", "hannaneh@cs.washington.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287589403, "id": "ICLR.cc/2017/conference/-/paper411/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJ0UKP9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper411/reviewers", "ICLR.cc/2017/conference/paper411/areachairs"], "cdate": 1485287589403}}}], "count": 28}