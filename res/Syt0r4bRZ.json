{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1518730155803, "tcdate": 1509144016851, "number": 1093, "cdate": 1518730155780, "id": "Syt0r4bRZ", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "Syt0r4bRZ", "original": "H1f81QZRW", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Tree2Tree Learning with Memory Unit", "abstract": "Traditional recurrent neural network (RNN) or convolutional neural net- work (CNN) based sequence-to-sequence model can not handle tree structural data well. To alleviate this problem, in this paper, we propose a tree-to-tree model with specially designed encoder unit and decoder unit, which recursively encodes tree inputs into highly folded tree embeddings and decodes the embeddings into tree outputs. Our model could represent the complex information of a tree while also restore a tree from embeddings.\nWe evaluate our model in random tree recovery task and neural machine translation task. Experiments show that our model outperforms the baseline model.", "pdf": "/pdf/9558215bb47a09abcef80ac65b52474a09da0be1.pdf", "paperhash": "miao|tree2tree_learning_with_memory_unit", "_bibtex": "@misc{\nmiao2018treetree,\ntitle={Tree2Tree Learning with Memory Unit},\nauthor={Ning Miao and Hengliang Wang and Ran Le and Chongyang Tao and Mingyue Shang and Rui Yan and Dongyan Zhao},\nyear={2018},\nurl={https://openreview.net/forum?id=Syt0r4bRZ},\n}", "keywords": [], "authors": ["Ning Miao", "Hengliang Wang", "Ran Le", "Chongyang Tao", "Mingyue Shang", "Rui Yan", "Dongyan Zhao"], "authorids": ["miaoning@pku.edu.cn", "wanghl@pku.edu.cn", "leran@buaa.edu.cn", "chongyangtao@pku.edu.cn", "shangmy@pku.edu.cn", "ruiyan@pku.edu.cn", "zhaody@pku.edu.cn"]}, "nonreaders": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "OpenReview.net"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260084335, "tcdate": 1517249913941, "number": 608, "cdate": 1517249913926, "id": "BkfYr1arM", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "Syt0r4bRZ", "replyto": "Syt0r4bRZ", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "the problem is interesting, and the reviewers acknowledge it's worth an effort to tackle. unfortunately all the reviewers found the work to be too preliminary without a convincing evidence supporting the proposed approach against other alternatives (or on its own.)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tree2Tree Learning with Memory Unit", "abstract": "Traditional recurrent neural network (RNN) or convolutional neural net- work (CNN) based sequence-to-sequence model can not handle tree structural data well. To alleviate this problem, in this paper, we propose a tree-to-tree model with specially designed encoder unit and decoder unit, which recursively encodes tree inputs into highly folded tree embeddings and decodes the embeddings into tree outputs. Our model could represent the complex information of a tree while also restore a tree from embeddings.\nWe evaluate our model in random tree recovery task and neural machine translation task. Experiments show that our model outperforms the baseline model.", "pdf": "/pdf/9558215bb47a09abcef80ac65b52474a09da0be1.pdf", "paperhash": "miao|tree2tree_learning_with_memory_unit", "_bibtex": "@misc{\nmiao2018treetree,\ntitle={Tree2Tree Learning with Memory Unit},\nauthor={Ning Miao and Hengliang Wang and Ran Le and Chongyang Tao and Mingyue Shang and Rui Yan and Dongyan Zhao},\nyear={2018},\nurl={https://openreview.net/forum?id=Syt0r4bRZ},\n}", "keywords": [], "authors": ["Ning Miao", "Hengliang Wang", "Ran Le", "Chongyang Tao", "Mingyue Shang", "Rui Yan", "Dongyan Zhao"], "authorids": ["miaoning@pku.edu.cn", "wanghl@pku.edu.cn", "leran@buaa.edu.cn", "chongyangtao@pku.edu.cn", "shangmy@pku.edu.cn", "ruiyan@pku.edu.cn", "zhaody@pku.edu.cn"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642382832, "tcdate": 1511783941900, "number": 1, "cdate": 1511783941900, "id": "HJA-R_Fxf", "invitation": "ICLR.cc/2018/Conference/-/Paper1093/Official_Review", "forum": "Syt0r4bRZ", "replyto": "Syt0r4bRZ", "signatures": ["ICLR.cc/2018/Conference/Paper1093/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "review ", "rating": "2: Strong rejection", "review": "Summary: the paper proposes a tree2tree architecture for NLP tasks. Both the encoder and decoder of this architecture make use of memory cells: the encoder looks like a tree-lstm to encode a tree bottom-up, the decoder generates a tree top-down by predicting the number of children first. The objective function is a linear mixture of the cost of generating the tree structure and the target sentence. The proposed architecture outperforms recursive autoencoder on a self-to-self predicting trees, and outperforms an lstm seq2seq on En-Cn translation.\n\nComment:\n\n- The idea of tree2tree has been around recently but it is difficult to make it work. I thus appreciate the authors\u2019 effort. However, I wish the authors would have done it more properly.\n- The computation of the encoder and decoder is not novel. I was wondering how the encoder differs from tree-lstm. The decoder predicts the number of children first, but the authors don\u2019t explain why they do that, nor compare this to existing tree generators. \n- I don\u2019t understand the objective function (eq 4 and 5). Both Ls are not cross-entropy because label and childnum are not probabilities. I also don\u2019t see why using Adam is more convenient than using SGD.\n- I think eq 9 is incorrect, because the decoder is not Markovian. To see this we can look at recurrent neural networks for language modeling: generating the current word is conditioning on the whole history (not only the previous word).\n- I expect the authors would explain more about how difficult the tasks are (eg. some statistics about the datasets), how to choose values for lambda, what the contribution of the new objective is.\n\nAbout writing:\n- the paper has so many problems with wording, e.g. articles, plurality.\n- many terms are incorrect, e.g. \u201cdependent parsing tree\u201d (should be \u201cdependency tree\u201d), \u201cconsistency parsing\u201d (should be \u201cconstituency parsing\u201d)\n- In 3.1, Socher et al. do not use lstm\n- I suggest the authors to do some more literature review on tree generation\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tree2Tree Learning with Memory Unit", "abstract": "Traditional recurrent neural network (RNN) or convolutional neural net- work (CNN) based sequence-to-sequence model can not handle tree structural data well. To alleviate this problem, in this paper, we propose a tree-to-tree model with specially designed encoder unit and decoder unit, which recursively encodes tree inputs into highly folded tree embeddings and decodes the embeddings into tree outputs. Our model could represent the complex information of a tree while also restore a tree from embeddings.\nWe evaluate our model in random tree recovery task and neural machine translation task. Experiments show that our model outperforms the baseline model.", "pdf": "/pdf/9558215bb47a09abcef80ac65b52474a09da0be1.pdf", "paperhash": "miao|tree2tree_learning_with_memory_unit", "_bibtex": "@misc{\nmiao2018treetree,\ntitle={Tree2Tree Learning with Memory Unit},\nauthor={Ning Miao and Hengliang Wang and Ran Le and Chongyang Tao and Mingyue Shang and Rui Yan and Dongyan Zhao},\nyear={2018},\nurl={https://openreview.net/forum?id=Syt0r4bRZ},\n}", "keywords": [], "authors": ["Ning Miao", "Hengliang Wang", "Ran Le", "Chongyang Tao", "Mingyue Shang", "Rui Yan", "Dongyan Zhao"], "authorids": ["miaoning@pku.edu.cn", "wanghl@pku.edu.cn", "leran@buaa.edu.cn", "chongyangtao@pku.edu.cn", "shangmy@pku.edu.cn", "ruiyan@pku.edu.cn", "zhaody@pku.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642382739, "id": "ICLR.cc/2018/Conference/-/Paper1093/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1093/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1093/AnonReviewer2", "ICLR.cc/2018/Conference/Paper1093/AnonReviewer3", "ICLR.cc/2018/Conference/Paper1093/AnonReviewer1"], "reply": {"forum": "Syt0r4bRZ", "replyto": "Syt0r4bRZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1093/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642382739}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642382792, "tcdate": 1511806686837, "number": 2, "cdate": 1511806686837, "id": "rkP1vRFxz", "invitation": "ICLR.cc/2018/Conference/-/Paper1093/Official_Review", "forum": "Syt0r4bRZ", "replyto": "Syt0r4bRZ", "signatures": ["ICLR.cc/2018/Conference/Paper1093/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "The paper is not ready for publication yet. It has very limited contributions and evaluation is preliminary.", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposes a tree-to-tree model aiming to encode an input tree into embedding and then decode that back to a tree. The contributions of the work are very limited.  Basic attention models, which have been shown to help model structures, are not included (or compared). Method-wise, the encoder is not novel and decoder is rather straightforward. The contributions of the work are in general very limited. Moreover, this manuscript contains many grammatical errors.  In general, it is not ready for publication. \n\nPros:\n- Investigating the ability of distributed representation in encoding input structured is in general interesting. Although there have been much previous work, this paper is along this line.\n\nCons:\n- The contributions of the work are very limited. For example, attention, which have been widely used and been shown to help capture structures in many tasks, are not included and compared in this paper.\n- Evaluation is not very convincing. The baseline performance in MT is too low. It is unclear if the proposed model is still helpful when other components are considered (e.g., attention). \n- For the objective function defined in the paper, it may be hard to balance the \"structure loss\" and \"content loss\" in different problems, and moreover, the loss function may not be even useful in real tasks (e.g, in MT), which often have their own objectives (as discussed in this paper). Earlier work on tree kernels (in terms of defining tree distances) may be related to this work. \n- The manuscript is full of grammatical errors, and the following are some of them:\n\"encoder only only need to\"\n\"For for tree reconstruction task\"\n\"The Socher et al. (2011b) propose a basic form\"\n\"experiments and theroy analysis are done\"\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tree2Tree Learning with Memory Unit", "abstract": "Traditional recurrent neural network (RNN) or convolutional neural net- work (CNN) based sequence-to-sequence model can not handle tree structural data well. To alleviate this problem, in this paper, we propose a tree-to-tree model with specially designed encoder unit and decoder unit, which recursively encodes tree inputs into highly folded tree embeddings and decodes the embeddings into tree outputs. Our model could represent the complex information of a tree while also restore a tree from embeddings.\nWe evaluate our model in random tree recovery task and neural machine translation task. Experiments show that our model outperforms the baseline model.", "pdf": "/pdf/9558215bb47a09abcef80ac65b52474a09da0be1.pdf", "paperhash": "miao|tree2tree_learning_with_memory_unit", "_bibtex": "@misc{\nmiao2018treetree,\ntitle={Tree2Tree Learning with Memory Unit},\nauthor={Ning Miao and Hengliang Wang and Ran Le and Chongyang Tao and Mingyue Shang and Rui Yan and Dongyan Zhao},\nyear={2018},\nurl={https://openreview.net/forum?id=Syt0r4bRZ},\n}", "keywords": [], "authors": ["Ning Miao", "Hengliang Wang", "Ran Le", "Chongyang Tao", "Mingyue Shang", "Rui Yan", "Dongyan Zhao"], "authorids": ["miaoning@pku.edu.cn", "wanghl@pku.edu.cn", "leran@buaa.edu.cn", "chongyangtao@pku.edu.cn", "shangmy@pku.edu.cn", "ruiyan@pku.edu.cn", "zhaody@pku.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642382739, "id": "ICLR.cc/2018/Conference/-/Paper1093/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1093/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1093/AnonReviewer2", "ICLR.cc/2018/Conference/Paper1093/AnonReviewer3", "ICLR.cc/2018/Conference/Paper1093/AnonReviewer1"], "reply": {"forum": "Syt0r4bRZ", "replyto": "Syt0r4bRZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1093/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642382739}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642382756, "tcdate": 1511816214153, "number": 3, "cdate": 1511816214153, "id": "rkAf3gcgM", "invitation": "ICLR.cc/2018/Conference/-/Paper1093/Official_Review", "forum": "Syt0r4bRZ", "replyto": "Syt0r4bRZ", "signatures": ["ICLR.cc/2018/Conference/Paper1093/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Please compare with other methods", "rating": "4: Ok but not good enough - rejection", "review": "This paper presents a model to encode and decode trees in distributed representations. \nThis is not the first attempt of doing these encoders and decoders. However, there is not a comparative evalution with these methods.\nIn fact, it has been demonstrated that it is possible to encode and decode trees in distributed structures without learning parameters, see \"Decoding Distributed Tree Structures\" and \"Distributed tree kernels\".\nThe paper should present a comparison with such kinds of models.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tree2Tree Learning with Memory Unit", "abstract": "Traditional recurrent neural network (RNN) or convolutional neural net- work (CNN) based sequence-to-sequence model can not handle tree structural data well. To alleviate this problem, in this paper, we propose a tree-to-tree model with specially designed encoder unit and decoder unit, which recursively encodes tree inputs into highly folded tree embeddings and decodes the embeddings into tree outputs. Our model could represent the complex information of a tree while also restore a tree from embeddings.\nWe evaluate our model in random tree recovery task and neural machine translation task. Experiments show that our model outperforms the baseline model.", "pdf": "/pdf/9558215bb47a09abcef80ac65b52474a09da0be1.pdf", "paperhash": "miao|tree2tree_learning_with_memory_unit", "_bibtex": "@misc{\nmiao2018treetree,\ntitle={Tree2Tree Learning with Memory Unit},\nauthor={Ning Miao and Hengliang Wang and Ran Le and Chongyang Tao and Mingyue Shang and Rui Yan and Dongyan Zhao},\nyear={2018},\nurl={https://openreview.net/forum?id=Syt0r4bRZ},\n}", "keywords": [], "authors": ["Ning Miao", "Hengliang Wang", "Ran Le", "Chongyang Tao", "Mingyue Shang", "Rui Yan", "Dongyan Zhao"], "authorids": ["miaoning@pku.edu.cn", "wanghl@pku.edu.cn", "leran@buaa.edu.cn", "chongyangtao@pku.edu.cn", "shangmy@pku.edu.cn", "ruiyan@pku.edu.cn", "zhaody@pku.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642382739, "id": "ICLR.cc/2018/Conference/-/Paper1093/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1093/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1093/AnonReviewer2", "ICLR.cc/2018/Conference/Paper1093/AnonReviewer3", "ICLR.cc/2018/Conference/Paper1093/AnonReviewer1"], "reply": {"forum": "Syt0r4bRZ", "replyto": "Syt0r4bRZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1093/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642382739}}}, {"tddate": null, "ddate": null, "tmdate": 1511926564867, "tcdate": 1511926564867, "number": 1, "cdate": 1511926564867, "id": "Hy6misoxf", "invitation": "ICLR.cc/2018/Conference/-/Paper1093/Public_Comment", "forum": "Syt0r4bRZ", "replyto": "Syt0r4bRZ", "signatures": ["~bruce_matthew_kuzak1"], "readers": ["everyone"], "writers": ["~bruce_matthew_kuzak1"], "content": {"title": "Class Project", "comment": "We are tasked to evaluate a research paper as a class project and we need to evaluate your results and their plausibility. Could we have access to your source code for training the Neural Nets and the training data to analyze the results.\n\nIt would be of great help and we could forward a lot of positive feedback hopefully."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tree2Tree Learning with Memory Unit", "abstract": "Traditional recurrent neural network (RNN) or convolutional neural net- work (CNN) based sequence-to-sequence model can not handle tree structural data well. To alleviate this problem, in this paper, we propose a tree-to-tree model with specially designed encoder unit and decoder unit, which recursively encodes tree inputs into highly folded tree embeddings and decodes the embeddings into tree outputs. Our model could represent the complex information of a tree while also restore a tree from embeddings.\nWe evaluate our model in random tree recovery task and neural machine translation task. Experiments show that our model outperforms the baseline model.", "pdf": "/pdf/9558215bb47a09abcef80ac65b52474a09da0be1.pdf", "paperhash": "miao|tree2tree_learning_with_memory_unit", "_bibtex": "@misc{\nmiao2018treetree,\ntitle={Tree2Tree Learning with Memory Unit},\nauthor={Ning Miao and Hengliang Wang and Ran Le and Chongyang Tao and Mingyue Shang and Rui Yan and Dongyan Zhao},\nyear={2018},\nurl={https://openreview.net/forum?id=Syt0r4bRZ},\n}", "keywords": [], "authors": ["Ning Miao", "Hengliang Wang", "Ran Le", "Chongyang Tao", "Mingyue Shang", "Rui Yan", "Dongyan Zhao"], "authorids": ["miaoning@pku.edu.cn", "wanghl@pku.edu.cn", "leran@buaa.edu.cn", "chongyangtao@pku.edu.cn", "shangmy@pku.edu.cn", "ruiyan@pku.edu.cn", "zhaody@pku.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791673095, "id": "ICLR.cc/2018/Conference/-/Paper1093/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "Syt0r4bRZ", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper1093/Authors", "ICLR.cc/2018/Conference/Paper1093/Reviewers", "ICLR.cc/2018/Conference/Paper1093/Area_Chair"], "cdate": 1512791673095}}}], "count": 6}