{"notes": [{"id": "BJevJCVYvB", "original": "HyxG5FG_wr", "number": 896, "cdate": 1569439199004, "ddate": null, "tcdate": 1569439199004, "tmdate": 1577168230797, "tddate": null, "forum": "BJevJCVYvB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Training Neural Networks for and by Interpolation", "authors": ["Leonard Berrada", "Andrew Zisserman", "Pawan M. Kumar"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"], "keywords": ["optimization", "adaptive learning-rate", "Polyak step-size", "Newton-Raphson"], "TL;DR": "An adaptive learning-rate with a single hyper-parameter for neural networks that can interpolate the data", "abstract": "In modern supervised learning, many deep neural networks are able to interpolate the data: the empirical loss can be driven to near zero on all samples simultaneously. In this work, we explicitly exploit this interpolation property for the design of a new optimization algorithm for deep learning. Specifically, we use it to compute an adaptive learning-rate in closed form at each iteration. This results in the Adaptive Learning-rates for Interpolation with Gradients (ALI-G) algorithm. ALI-G retains the main advantage of SGD which is a low computational cost per iteration. But unlike SGD, the learning-rate of ALI-G uses a single constant hyper-parameter and does not require a decay schedule, which makes it considerably easier to tune. We provide convergence guarantees of ALI-G in the stochastic convex setting. Notably, all our convergence results tackle the realistic case where the interpolation property is satisfied up to some tolerance. We provide experiments on a variety of architectures and tasks: (i) learning a differentiable neural computer; (ii) training a wide residual network on the SVHN data set; (iii) training a Bi-LSTM on the SNLI data set; and (iv) training wide residual networks and densely connected networks on the CIFAR data sets. ALI-G produces state-of-the-art results among adaptive methods, and even yields comparable performance with SGD, which requires manually tuned learning-rate schedules. Furthermore, ALI-G is simple to implement in any standard deep learning framework and can be used as a drop-in replacement in existing code.", "pdf": "/pdf/f134558a175667f1bfb61244a540c8bfe386d168.pdf", "code": "https://anonymous.4open.science/repository/14f2b37d-2bef-4b3f-b47c-dd257ce75543", "paperhash": "berrada|training_neural_networks_for_and_by_interpolation", "original_pdf": "/attachment/00858758a646a139bf647f1c91c057e9df9deb07.pdf", "_bibtex": "@misc{\nberrada2020training,\ntitle={Training Neural Networks for and by Interpolation},\nauthor={Leonard Berrada and Andrew Zisserman and Pawan M. Kumar},\nyear={2020},\nurl={https://openreview.net/forum?id=BJevJCVYvB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "kXBkmdmI66", "original": null, "number": 1, "cdate": 1576798709034, "ddate": null, "tcdate": 1576798709034, "tmdate": 1576800927330, "tddate": null, "forum": "BJevJCVYvB", "replyto": "BJevJCVYvB", "invitation": "ICLR.cc/2020/Conference/Paper896/-/Decision", "content": {"decision": "Reject", "comment": "This paper uses the interpolation property to design a new optimization algorithm for deep learning, which computes an adaptive learning-rate in closed form at each iteration. The authors also analyzed the convergence rate of the proposed algorithm in the stochastic convex optimization setting. Experiments on several benchmark neural networks and datasets verify the effectiveness of the proposed algorithm. This is a borderline paper and has been carefully discussed. The main objection of the reviewers include: (1) The interplay between regularization and the interpolation property is not clear; and (2) the proposed algorithm  is no better than SGD in any of the benchmarks except one, where SGD's learning rate is set to be a constant. After the author response, this paper still does not gather sufficient support. So I encourage the authors to improve this paper and resubmit it to future conference.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Neural Networks for and by Interpolation", "authors": ["Leonard Berrada", "Andrew Zisserman", "Pawan M. Kumar"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"], "keywords": ["optimization", "adaptive learning-rate", "Polyak step-size", "Newton-Raphson"], "TL;DR": "An adaptive learning-rate with a single hyper-parameter for neural networks that can interpolate the data", "abstract": "In modern supervised learning, many deep neural networks are able to interpolate the data: the empirical loss can be driven to near zero on all samples simultaneously. In this work, we explicitly exploit this interpolation property for the design of a new optimization algorithm for deep learning. Specifically, we use it to compute an adaptive learning-rate in closed form at each iteration. This results in the Adaptive Learning-rates for Interpolation with Gradients (ALI-G) algorithm. ALI-G retains the main advantage of SGD which is a low computational cost per iteration. But unlike SGD, the learning-rate of ALI-G uses a single constant hyper-parameter and does not require a decay schedule, which makes it considerably easier to tune. We provide convergence guarantees of ALI-G in the stochastic convex setting. Notably, all our convergence results tackle the realistic case where the interpolation property is satisfied up to some tolerance. We provide experiments on a variety of architectures and tasks: (i) learning a differentiable neural computer; (ii) training a wide residual network on the SVHN data set; (iii) training a Bi-LSTM on the SNLI data set; and (iv) training wide residual networks and densely connected networks on the CIFAR data sets. ALI-G produces state-of-the-art results among adaptive methods, and even yields comparable performance with SGD, which requires manually tuned learning-rate schedules. Furthermore, ALI-G is simple to implement in any standard deep learning framework and can be used as a drop-in replacement in existing code.", "pdf": "/pdf/f134558a175667f1bfb61244a540c8bfe386d168.pdf", "code": "https://anonymous.4open.science/repository/14f2b37d-2bef-4b3f-b47c-dd257ce75543", "paperhash": "berrada|training_neural_networks_for_and_by_interpolation", "original_pdf": "/attachment/00858758a646a139bf647f1c91c057e9df9deb07.pdf", "_bibtex": "@misc{\nberrada2020training,\ntitle={Training Neural Networks for and by Interpolation},\nauthor={Leonard Berrada and Andrew Zisserman and Pawan M. Kumar},\nyear={2020},\nurl={https://openreview.net/forum?id=BJevJCVYvB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJevJCVYvB", "replyto": "BJevJCVYvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795715715, "tmdate": 1576800265690, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper896/-/Decision"}}}, {"id": "B1xbXQwnKB", "original": null, "number": 1, "cdate": 1571742489345, "ddate": null, "tcdate": 1571742489345, "tmdate": 1575744777409, "tddate": null, "forum": "BJevJCVYvB", "replyto": "BJevJCVYvB", "invitation": "ICLR.cc/2020/Conference/Paper896/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "This paper proposes a new gradient descent methods for training deep neural network which can take the adaptive step size with only one hyper-parameter to tune -- the maximum learning rate -- and achieve comparable results to stochastic gradient descent (SGD) on various tasks and models. In order to achieve that, they develop a stochastic extension of the Polyak step-size for the non-convex setting, namely the adaptive learning-rates for interpolation with gradients (ALI-G), in which the minimal value of the objective loss is set to 0 due to interpolation in neural networks and the learning rates are clipped by a chosen maximal value. The problem is formulated clearly, and the review on the Polyak step-size and related works are well done. Another main contribution of the paper is to provide the convergence guarantees for ALI-G in the convex setting where the objective loss is Lipschitz-continuous (Theorem 1 in the paper). Their theorem also takes into account the error in the estimate of the minimal value of the objective loss. In addition, they derive the connections between  ALI-G and SGD and show that compared to SGD, ALI-G take into consideration that the objective loss is non-negative and set the loss to 0 when it is negative. They perform empirical study to compare their algorithm with other methods including Adagrad, Adam, DFW, L4Adam and SGD on learning a differentiable neural computer, object recognition, and a natural language processing task. Their experimental results show that ALI-G performance is comparable with that of SGD with schedule learning rate.\n\nOverall, this paper could be an interesting contribution. However, some points in the theory and experiments need to be verified. I weakly reject this paper, but given these clarifications in an author response, I would be willing to increase the score. \n\nFor the algorithm and theory, there are some points that need to be verified and further clarification on novelty:\n\n1. When there are regularization such as the weight decay regularization, the minimal objective loss will not be 0. In such cases, Theorem 1 in the paper only guarantees that ALI-G reaches an objective loss less than or equal to a multiple of the estimate error \\epsilon of the true minimal objective loss. It cannot guarantee that the objective loss reached by ALI-G can converge to the true minimall objective loss. Furthermore, when training neural networks with, for example, the weight decay regularization, often times the value of the regularization loss, i.e.  the estimate error \\epsilon, is not small. Therefore, the upper bound given by Theorem 1 is rather loose. \n\n2.  The paper mentions that when no regularization is used, ALI-G and Deep Frank-Wolfe (DFW) are identical algorithms. The difference between the two algorithms are when regularization is used. However, given my concern for Theorem 1 above, the convergence of ALI-G and advantage of ALI-G over DFW in this setting is questionable, and the claim that \u201cALI-G can handle arbitrary (lower-bounded) loss functions\u201d also needs to be verified. \n\nFor the experiments, the following should be addressed:\n1. In the experiment with the differentiable neural computers, even though ALI-G obtains better performance for a large range of \\eta, its best objective loss is still worse than RMSProp, L4Adam, and L4Mom.\n\n2. Given the merit of Theorem 1 is that the convergence guarantee takes into account the estimate error of the minimal objective loss, an ablation study that compares ALI-G with other methods in the same setting with and without regularization are needed. For example, it would be more convincing if similar results to those in Table 2 or 3 but without regularization are provided and discussed.\n\n3. In Section 5.5, given that ALI-G and DFW are related, why is there no result for DFW in Figure \n4.?\n\n4.  As the paper mentions, AProx algorithm and ALI-G are related, why is there no comparison with AProx in the experiments?\nThings to improve the paper that did not impact the score:\n1. In all experiments, the performance differences between ALI-G and competitive methods are small. Thus, error bars are needed for these results.\n\n2. In Table 3, the gap between SGD and ALI-G can be significantly different on different architectures. For example, in CIFAR-100 experiments, while ALI-G achieves the same result as SGD with the DenseNet, ALI-G\u2019s performance on Wide ResNet is much worse than SGD. Do you have any explanation for this?\n\n3. How is ALI-G compared with the methods proposed in the paper \u201cStochastic Gradient Descent with Polyak's Learning Rate\u201d (https://arxiv.org/abs/1903.08688)\n\n\nThe following paper also proved the convergence of adaptive gradient methods for nonconvex optimization:\nDongruo Zhou*, Yiqi Tang*, Ziyan Yang*, Yuan Cao, Quanquan Gu. On the Convergence of Adaptive Gradient Methods for Nonconvex Optimization.\n\n-------------------------------------------------------------\nAfter rebuttal:\nI noticed that the authors:\n\n1. did not compare with SGD or Adam with good hyperparameters (https://arxiv.org/abs/1907.08610)\n\n2. did not test on the large scale datasets, e.g., imagenet\n\n3. the proposed algorithm is not as good as SGD on most numerical experiments.\n\nFor the above reasons, I think this paper should be rejected.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper896/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper896/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Neural Networks for and by Interpolation", "authors": ["Leonard Berrada", "Andrew Zisserman", "Pawan M. Kumar"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"], "keywords": ["optimization", "adaptive learning-rate", "Polyak step-size", "Newton-Raphson"], "TL;DR": "An adaptive learning-rate with a single hyper-parameter for neural networks that can interpolate the data", "abstract": "In modern supervised learning, many deep neural networks are able to interpolate the data: the empirical loss can be driven to near zero on all samples simultaneously. In this work, we explicitly exploit this interpolation property for the design of a new optimization algorithm for deep learning. Specifically, we use it to compute an adaptive learning-rate in closed form at each iteration. This results in the Adaptive Learning-rates for Interpolation with Gradients (ALI-G) algorithm. ALI-G retains the main advantage of SGD which is a low computational cost per iteration. But unlike SGD, the learning-rate of ALI-G uses a single constant hyper-parameter and does not require a decay schedule, which makes it considerably easier to tune. We provide convergence guarantees of ALI-G in the stochastic convex setting. Notably, all our convergence results tackle the realistic case where the interpolation property is satisfied up to some tolerance. We provide experiments on a variety of architectures and tasks: (i) learning a differentiable neural computer; (ii) training a wide residual network on the SVHN data set; (iii) training a Bi-LSTM on the SNLI data set; and (iv) training wide residual networks and densely connected networks on the CIFAR data sets. ALI-G produces state-of-the-art results among adaptive methods, and even yields comparable performance with SGD, which requires manually tuned learning-rate schedules. Furthermore, ALI-G is simple to implement in any standard deep learning framework and can be used as a drop-in replacement in existing code.", "pdf": "/pdf/f134558a175667f1bfb61244a540c8bfe386d168.pdf", "code": "https://anonymous.4open.science/repository/14f2b37d-2bef-4b3f-b47c-dd257ce75543", "paperhash": "berrada|training_neural_networks_for_and_by_interpolation", "original_pdf": "/attachment/00858758a646a139bf647f1c91c057e9df9deb07.pdf", "_bibtex": "@misc{\nberrada2020training,\ntitle={Training Neural Networks for and by Interpolation},\nauthor={Leonard Berrada and Andrew Zisserman and Pawan M. Kumar},\nyear={2020},\nurl={https://openreview.net/forum?id=BJevJCVYvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJevJCVYvB", "replyto": "BJevJCVYvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper896/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper896/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575767975855, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper896/Reviewers"], "noninvitees": [], "tcdate": 1570237745407, "tmdate": 1575767975868, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper896/-/Official_Review"}}}, {"id": "rJg9gcITcB", "original": null, "number": 4, "cdate": 1572854258264, "ddate": null, "tcdate": 1572854258264, "tmdate": 1574654952800, "tddate": null, "forum": "BJevJCVYvB", "replyto": "BJevJCVYvB", "invitation": "ICLR.cc/2020/Conference/Paper896/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #4", "review": "Thanks for the responses and my concerns seem to be addressed. But since I do not know much about this area, I would like to stick to my initial rating 6.\n==================================================================================================\nThis work designs a new optimization SGD algorithm named ALI-G for deep neural network with interpolation property. \nThis algorithm only has a single hyper-parameter and doesn\u2019t have a decay schedule. The authors provide the convergence guarantees of ALI-G in the stochastic convex setting as well as the experiment results on four tasks.This paper shows state-of-the-art results but I still have have two concerns.\n\nMy main concern is that the performances of other SGD algorithms may be potentially better than the results showed in section 5 because it is not easy to tune the parameter. It would be better if the authors can tune the hyper-parameter more carefully. Take figure 3 as an example. The settings where step size bigger than 1e+1 can hardly shows something, because the step-size is too big for SGD to converge. The settings where step size smaller 1e-2 can also hardly shows something, because the step-size is too small and the experiments only runs 10k steps. It would be better if authors can do more experiments in the settings where step size is from 1e-3 to 1e+0. Moreover, the optimal step size of different optimization algorithms may differ a lot. It would be much more fair if the authors can compare the best performance of different algorithms.\n\nAnother concern is that the authors only give the convergence rate of ALI-G in section 3 but haven\u2019t make any comparisons. For example, it would be better if the authors can show that ALI-G has better convergence result than vanilla SGD without decay schedule.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper896/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper896/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Neural Networks for and by Interpolation", "authors": ["Leonard Berrada", "Andrew Zisserman", "Pawan M. Kumar"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"], "keywords": ["optimization", "adaptive learning-rate", "Polyak step-size", "Newton-Raphson"], "TL;DR": "An adaptive learning-rate with a single hyper-parameter for neural networks that can interpolate the data", "abstract": "In modern supervised learning, many deep neural networks are able to interpolate the data: the empirical loss can be driven to near zero on all samples simultaneously. In this work, we explicitly exploit this interpolation property for the design of a new optimization algorithm for deep learning. Specifically, we use it to compute an adaptive learning-rate in closed form at each iteration. This results in the Adaptive Learning-rates for Interpolation with Gradients (ALI-G) algorithm. ALI-G retains the main advantage of SGD which is a low computational cost per iteration. But unlike SGD, the learning-rate of ALI-G uses a single constant hyper-parameter and does not require a decay schedule, which makes it considerably easier to tune. We provide convergence guarantees of ALI-G in the stochastic convex setting. Notably, all our convergence results tackle the realistic case where the interpolation property is satisfied up to some tolerance. We provide experiments on a variety of architectures and tasks: (i) learning a differentiable neural computer; (ii) training a wide residual network on the SVHN data set; (iii) training a Bi-LSTM on the SNLI data set; and (iv) training wide residual networks and densely connected networks on the CIFAR data sets. ALI-G produces state-of-the-art results among adaptive methods, and even yields comparable performance with SGD, which requires manually tuned learning-rate schedules. Furthermore, ALI-G is simple to implement in any standard deep learning framework and can be used as a drop-in replacement in existing code.", "pdf": "/pdf/f134558a175667f1bfb61244a540c8bfe386d168.pdf", "code": "https://anonymous.4open.science/repository/14f2b37d-2bef-4b3f-b47c-dd257ce75543", "paperhash": "berrada|training_neural_networks_for_and_by_interpolation", "original_pdf": "/attachment/00858758a646a139bf647f1c91c057e9df9deb07.pdf", "_bibtex": "@misc{\nberrada2020training,\ntitle={Training Neural Networks for and by Interpolation},\nauthor={Leonard Berrada and Andrew Zisserman and Pawan M. Kumar},\nyear={2020},\nurl={https://openreview.net/forum?id=BJevJCVYvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJevJCVYvB", "replyto": "BJevJCVYvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper896/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper896/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575767975855, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper896/Reviewers"], "noninvitees": [], "tcdate": 1570237745407, "tmdate": 1575767975868, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper896/-/Official_Review"}}}, {"id": "BJgm9PbTqH", "original": null, "number": 3, "cdate": 1572833162689, "ddate": null, "tcdate": 1572833162689, "tmdate": 1574648097207, "tddate": null, "forum": "BJevJCVYvB", "replyto": "BJevJCVYvB", "invitation": "ICLR.cc/2020/Conference/Paper896/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "This paper addresses designing and analyzing an optimization algorithm. Like SGD, it maintains a low computational cost per optimization iteration, but unlike SGD, it does not require manually tuning a decay schedule. This work uses the interpolation property (that the empirical loss can be driven to near zero on all samples simultaneously in a neural network) to compute an adaptive learning rate in closed form at each optimization iteration, and results show that this method produces state-of-the-art results among adaptive methods. I can say the paper was very well written and easy to follow along/understand. Prior work seems comprehensive, and the intuitive comparisons to the prior methods were also useful for the reader. \n\nMy current decision is a weak accept, for a well-written paper, thorough results including meaningful baselines and numerous hyperparameter searches, and a seemingly high-impact tool. Some concerns are listed as follows:\n1-\tConvergence was only discussed in the stochastic convex setting, which seems limiting because we rarely deal with convex problems in problems requiring neural networks.\n2-\tRegularization of the weights during the optimization is dealt with by projecting onto the feasible set of weights, but it seems like there are other types of losses that don\u2019t necessarily to go 0. For example, terms in the objective such as entropy seem worrisome.\n3-\tOne detail that I did not fully follow along with is section 3.1. How does Theorem 1 (Regarding convexity) related to the \u201ceach training sample can use its own learning rate without harming progress on the other ones\u201d and/or \u201callow the updates to rely on the stochastic estimate rather than the exact\u201d? \n4-\tUnfortunately, I am not an expert in this particular area, so I\u2019m not confident about the novelty. For example, the difference between L4 and this is stated to be the utilization of the interpolation policy (which just sets f*=0) and the maximal learning rate, and the stated benefit of convergence guarantees in stochastic convex settings seems poor since most problems will not be convex anyway. More generally, it seems like all details of the algorithm came from elsewhere, although the presented synthesis of ideas does have clear benefits.\n\nAfter reading the author response: \n-I'm fine with points 4 and 3. \n-My feelings about 1 are still the same.\n-My comment on 2 wasn't about cross-entopy loss, but rather other types of objectives that people are often interested in optimizing (such as max-ent RL, where we aim for maximizing rewards as well as maximizing entropy of the policy), in which case, it's not clear to me how we could apply this optimizer. \n-My decision stays as a weak accept", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper896/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper896/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Neural Networks for and by Interpolation", "authors": ["Leonard Berrada", "Andrew Zisserman", "Pawan M. Kumar"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"], "keywords": ["optimization", "adaptive learning-rate", "Polyak step-size", "Newton-Raphson"], "TL;DR": "An adaptive learning-rate with a single hyper-parameter for neural networks that can interpolate the data", "abstract": "In modern supervised learning, many deep neural networks are able to interpolate the data: the empirical loss can be driven to near zero on all samples simultaneously. In this work, we explicitly exploit this interpolation property for the design of a new optimization algorithm for deep learning. Specifically, we use it to compute an adaptive learning-rate in closed form at each iteration. This results in the Adaptive Learning-rates for Interpolation with Gradients (ALI-G) algorithm. ALI-G retains the main advantage of SGD which is a low computational cost per iteration. But unlike SGD, the learning-rate of ALI-G uses a single constant hyper-parameter and does not require a decay schedule, which makes it considerably easier to tune. We provide convergence guarantees of ALI-G in the stochastic convex setting. Notably, all our convergence results tackle the realistic case where the interpolation property is satisfied up to some tolerance. We provide experiments on a variety of architectures and tasks: (i) learning a differentiable neural computer; (ii) training a wide residual network on the SVHN data set; (iii) training a Bi-LSTM on the SNLI data set; and (iv) training wide residual networks and densely connected networks on the CIFAR data sets. ALI-G produces state-of-the-art results among adaptive methods, and even yields comparable performance with SGD, which requires manually tuned learning-rate schedules. Furthermore, ALI-G is simple to implement in any standard deep learning framework and can be used as a drop-in replacement in existing code.", "pdf": "/pdf/f134558a175667f1bfb61244a540c8bfe386d168.pdf", "code": "https://anonymous.4open.science/repository/14f2b37d-2bef-4b3f-b47c-dd257ce75543", "paperhash": "berrada|training_neural_networks_for_and_by_interpolation", "original_pdf": "/attachment/00858758a646a139bf647f1c91c057e9df9deb07.pdf", "_bibtex": "@misc{\nberrada2020training,\ntitle={Training Neural Networks for and by Interpolation},\nauthor={Leonard Berrada and Andrew Zisserman and Pawan M. Kumar},\nyear={2020},\nurl={https://openreview.net/forum?id=BJevJCVYvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJevJCVYvB", "replyto": "BJevJCVYvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper896/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper896/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575767975855, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper896/Reviewers"], "noninvitees": [], "tcdate": 1570237745407, "tmdate": 1575767975868, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper896/-/Official_Review"}}}, {"id": "SJeJz1scjr", "original": null, "number": 1, "cdate": 1573723911212, "ddate": null, "tcdate": 1573723911212, "tmdate": 1573724874335, "tddate": null, "forum": "BJevJCVYvB", "replyto": "rJg9gcITcB", "invitation": "ICLR.cc/2020/Conference/Paper896/-/Official_Comment", "content": {"title": "Response to Review #4", "comment": "We thank the reviewer for their comments, which we answer below:\n* \u201cMy main concern is that the performances of other SGD algorithms may be potentially better than the results showed in section 5 because it is not easy to tune the parameter.\u201d\n\nWe respectfully disagree with the statement that SGD is not a strong baseline for our experimental setting. For each of the SVHN, SNLI and CIFAR tasks, SGD uses a schedule of the learning-rate that has been tuned by an experienced practitioner on the particular combination of dataset and architecture. More specifically, these schedules come from (Zagoruyko & Komodakis, 2016), (Conneau et al., 2017), and (Huang et al., 2017), each of which tuned the learning-rate schedule to obtain then state-of-the-art results with their model. To the best of our knowledge, this constitutes the strongest baseline available in the literature.\n\nWe do agree that in general SGD is difficult to tune, and this is a major motivation for this work. We hope to show that ALI-G alleviates this empirical difficulty in the interpolation setting by providing good performance while requiring little tuning.\n\n* \u201cThe settings where step size bigger than 1e+1 can hardly shows something, because the step-size is too big for SGD to converge. The settings where step size smaller 1e-2 can also hardly shows something, because the step-size is too small and the experiments only runs 10k steps. It would be better if authors can do more experiments in the settings where step size is from 1e-3 to 1e+0\u201d.\n\nWe have run the experiment with a finer grid-search and restricted the range of learning-rates in the plot, please see Figure 3 in the updated paper.\n\n* \u201c It would be much more fair if the authors can compare the best performance of different algorithms.\u201d\n\nWhile the main point of Figure 3 is to evaluate robustness to hyper-parameters, the best performance can still be compared by comparing the best value per row. We hope that the figure in the revised paper makes this more visible by having fewer cells per row.\n\n* \u201cit would be better if the authors can show that ALI-G has better convergence result than vanilla SGD without decay schedule.\u201d\n\nWe are not aware of convergence guarantees for vanilla stochastic gradient descent with a constant learning-rate. To the best of our knowledge, the closest result to it in the interpolation setting is SGD with line-search (Vaswani et al., 2019b), which comes at the cost of (i) additional forward passes and (ii) up to four hyper-parameters instead of one for ALI-G. \n\nIn more detail, SGD with line-search provides the following convergence results:\nSmooth setting: O(1/T) in Theorem 1 of (Vaswani et al., 2019b), to be compared to O(1/T) in Theorem 4 of this work.\nSmooth and strongly-convex setting: O(exp(-K T)) in Theorem 2 of (Vaswani et al., 2019b), to be compared to O(exp(-K T / 8)) in Theorem 8 of this work.\n\nIt is also worth noting that, without assuming smoothness or strong convexity, the rate of convergence of O(1/sqrt(T)) in Theorem 1 is optimal among first-order methods in Nemirovski / Nesterov sense (Theorem 3.13 in Bubeck, 2015).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper896/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper896/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Neural Networks for and by Interpolation", "authors": ["Leonard Berrada", "Andrew Zisserman", "Pawan M. Kumar"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"], "keywords": ["optimization", "adaptive learning-rate", "Polyak step-size", "Newton-Raphson"], "TL;DR": "An adaptive learning-rate with a single hyper-parameter for neural networks that can interpolate the data", "abstract": "In modern supervised learning, many deep neural networks are able to interpolate the data: the empirical loss can be driven to near zero on all samples simultaneously. In this work, we explicitly exploit this interpolation property for the design of a new optimization algorithm for deep learning. Specifically, we use it to compute an adaptive learning-rate in closed form at each iteration. This results in the Adaptive Learning-rates for Interpolation with Gradients (ALI-G) algorithm. ALI-G retains the main advantage of SGD which is a low computational cost per iteration. But unlike SGD, the learning-rate of ALI-G uses a single constant hyper-parameter and does not require a decay schedule, which makes it considerably easier to tune. We provide convergence guarantees of ALI-G in the stochastic convex setting. Notably, all our convergence results tackle the realistic case where the interpolation property is satisfied up to some tolerance. We provide experiments on a variety of architectures and tasks: (i) learning a differentiable neural computer; (ii) training a wide residual network on the SVHN data set; (iii) training a Bi-LSTM on the SNLI data set; and (iv) training wide residual networks and densely connected networks on the CIFAR data sets. ALI-G produces state-of-the-art results among adaptive methods, and even yields comparable performance with SGD, which requires manually tuned learning-rate schedules. Furthermore, ALI-G is simple to implement in any standard deep learning framework and can be used as a drop-in replacement in existing code.", "pdf": "/pdf/f134558a175667f1bfb61244a540c8bfe386d168.pdf", "code": "https://anonymous.4open.science/repository/14f2b37d-2bef-4b3f-b47c-dd257ce75543", "paperhash": "berrada|training_neural_networks_for_and_by_interpolation", "original_pdf": "/attachment/00858758a646a139bf647f1c91c057e9df9deb07.pdf", "_bibtex": "@misc{\nberrada2020training,\ntitle={Training Neural Networks for and by Interpolation},\nauthor={Leonard Berrada and Andrew Zisserman and Pawan M. Kumar},\nyear={2020},\nurl={https://openreview.net/forum?id=BJevJCVYvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJevJCVYvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper896/Authors", "ICLR.cc/2020/Conference/Paper896/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper896/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper896/Reviewers", "ICLR.cc/2020/Conference/Paper896/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper896/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper896/Authors|ICLR.cc/2020/Conference/Paper896/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164537, "tmdate": 1576860554890, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper896/Authors", "ICLR.cc/2020/Conference/Paper896/Reviewers", "ICLR.cc/2020/Conference/Paper896/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper896/-/Official_Comment"}}}, {"id": "SJgXsZicsS", "original": null, "number": 5, "cdate": 1573724571385, "ddate": null, "tcdate": 1573724571385, "tmdate": 1573724846318, "tddate": null, "forum": "BJevJCVYvB", "replyto": "ryeGtWj5sH", "invitation": "ICLR.cc/2020/Conference/Paper896/-/Official_Comment", "content": {"title": "Response to Review #1 (2/2)", "comment": "* \u201cIn all experiments, the performance differences between ALI-G and competitive methods are small. Thus, error bars are needed for these results.\u201d\n\nAll results of Table 3 (which seem to have the largest variance among our set of experiments) are averaged over three independent runs. The standard deviation of these runs are reported in Appendix E.1 of the revised paper. One can check that the difference of performance between ALI-G and other adaptive methods (in particular AMSGrad and L4 methods) is statistically significant.\n\n* \u201cIn Table 3, the gap between SGD and ALI-G can be significantly different on different architectures. For example, in CIFAR-100 experiments, while ALI-G achieves the same result as SGD with the DenseNet, ALI-G\u2019s performance on Wide ResNet is much worse than SGD. Do you have any explanation for this?\u201d\n\nThe WRN architecture has a significantly larger number of parameters than the DN architecture (8M vs 2M). As a consequence, optimizers tend to overfit more easily on WRN, and SGD benefits more from the implicit regularization of its manually tuned schedule with many iterations at a large learning-rate. In contrast, ALI-G tends to converge fast on this problem which limits its implicit regularization. This could probably be mitigated by using a finer grain tuning of the l2 regularization, or \u201clocking\u201d the learning-rate of ALI-G to its maximal value for a large number of steps before letting it decay.\n\n* \u201c How is ALI-G compared with the methods proposed in the paper \u201cStochastic Gradient Descent with Polyak's Learning Rate\u201d (https://arxiv.org/abs/1903.08688)\u201d.\n\nIn contrast to ALI-G which uses only $\\ell_{z_t}(w_t)$ and $\\nabla \\ell_{z_t}(w_t)$ (loss function and its gradient for a sample / mini-batch) to compute its learning-rate, the aforementioned work requires $f(w_t)$ (objective function over entire dataset) to compute its learning-rate. In addition, since they do not do exploit the interpolation setting (and the fact that regularization can be expressed as a constraint), they also require the optimal objective function $f*$ in advance. We have added this comparison to the revised version of the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper896/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper896/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Neural Networks for and by Interpolation", "authors": ["Leonard Berrada", "Andrew Zisserman", "Pawan M. Kumar"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"], "keywords": ["optimization", "adaptive learning-rate", "Polyak step-size", "Newton-Raphson"], "TL;DR": "An adaptive learning-rate with a single hyper-parameter for neural networks that can interpolate the data", "abstract": "In modern supervised learning, many deep neural networks are able to interpolate the data: the empirical loss can be driven to near zero on all samples simultaneously. In this work, we explicitly exploit this interpolation property for the design of a new optimization algorithm for deep learning. Specifically, we use it to compute an adaptive learning-rate in closed form at each iteration. This results in the Adaptive Learning-rates for Interpolation with Gradients (ALI-G) algorithm. ALI-G retains the main advantage of SGD which is a low computational cost per iteration. But unlike SGD, the learning-rate of ALI-G uses a single constant hyper-parameter and does not require a decay schedule, which makes it considerably easier to tune. We provide convergence guarantees of ALI-G in the stochastic convex setting. Notably, all our convergence results tackle the realistic case where the interpolation property is satisfied up to some tolerance. We provide experiments on a variety of architectures and tasks: (i) learning a differentiable neural computer; (ii) training a wide residual network on the SVHN data set; (iii) training a Bi-LSTM on the SNLI data set; and (iv) training wide residual networks and densely connected networks on the CIFAR data sets. ALI-G produces state-of-the-art results among adaptive methods, and even yields comparable performance with SGD, which requires manually tuned learning-rate schedules. Furthermore, ALI-G is simple to implement in any standard deep learning framework and can be used as a drop-in replacement in existing code.", "pdf": "/pdf/f134558a175667f1bfb61244a540c8bfe386d168.pdf", "code": "https://anonymous.4open.science/repository/14f2b37d-2bef-4b3f-b47c-dd257ce75543", "paperhash": "berrada|training_neural_networks_for_and_by_interpolation", "original_pdf": "/attachment/00858758a646a139bf647f1c91c057e9df9deb07.pdf", "_bibtex": "@misc{\nberrada2020training,\ntitle={Training Neural Networks for and by Interpolation},\nauthor={Leonard Berrada and Andrew Zisserman and Pawan M. Kumar},\nyear={2020},\nurl={https://openreview.net/forum?id=BJevJCVYvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJevJCVYvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper896/Authors", "ICLR.cc/2020/Conference/Paper896/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper896/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper896/Reviewers", "ICLR.cc/2020/Conference/Paper896/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper896/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper896/Authors|ICLR.cc/2020/Conference/Paper896/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164537, "tmdate": 1576860554890, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper896/Authors", "ICLR.cc/2020/Conference/Paper896/Reviewers", "ICLR.cc/2020/Conference/Paper896/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper896/-/Official_Comment"}}}, {"id": "ryeGtWj5sH", "original": null, "number": 4, "cdate": 1573724537821, "ddate": null, "tcdate": 1573724537821, "tmdate": 1573724537821, "tddate": null, "forum": "BJevJCVYvB", "replyto": "B1xbXQwnKB", "invitation": "ICLR.cc/2020/Conference/Paper896/-/Official_Comment", "content": {"title": "Response to Review #1 (1/2)", "comment": "We thank the reviewer for their comments, which we answer below:\n\n* \u201cWhen there are regularization such as the weight decay regularization, the minimal objective loss will not be 0.\u201d; \u201cTherefore, the upper bound given by Theorem 1 is rather loose. \u201c; \u201cgiven my concern for Theorem 1 above, the convergence of ALI-G and advantage of ALI-G over DFW in this setting is questionable [...].\u201d\n\nWe believe that this concern stems from a misunderstanding of our method. As described in the paragraph \u201cRegularization\u201d in section 2.1, we express the regularization as a constraint on the feasible set rather than a soft penalty in the objective function. This allows us to realistically assume that even for regularized models, the minimal objective value is close to zero. This is verified in practice in our experiments, where regularized models achieve near zero objective value while remaining within the feasible set. Furthermore, please note that our theoretical results have also been proven for the constrained setting, i.e., no assumption is made regarding the feasible region other than the standard ones (convexity, availability of efficient projection algorithms).\n\n* \u201cthe claim that \u201cALI-G can handle arbitrary (lower-bounded) loss functions\u201d also needs to be verified.\u201d\n\nThe above answer shows that we can use arbitrary loss functions that have an infimum of 0.  In the more general case, if the loss function $\\ell$ is just lower-bounded, we can recover the previous case of an infimum of 0 by using the function shifted by a constant $\\ell - \\inf \\ell$, which does not change the solution of the problem (using $inf$ is well-defined because the function is real-valued and lower-bounded).\n\n* \u201cGiven the merit of Theorem 1 is that the convergence guarantee takes into account the estimate error of the minimal objective loss, an ablation study that compares ALI-G with other methods in the same setting with and without regularization are needed. For example, it would be more convincing if similar results to those in Table 2 or 3 but without regularization are provided and discussed.\u201d\n\nIn Tables 2 and 3, we focus on generalization performance, which is why we keep the regularization activated for a fair comparison between all methods. In contrast, sections 5.1 and 5.5 do not use l2 regularization because they focus on training performance, and section 5.3 does not either because the corresponding model from (Conneau et al., 2017) did not originally use any regularization. Thus our set of experiments arguably cover varied settings in terms of regularization.\n\n* \u201c In the experiment with the differentiable neural computers, even though ALI-G obtains better performance for a large range of \\eta, its best objective loss is still worse than RMSProp, L4Adam, and L4Mom.\u201d\n\nBy being more careful about numerical precision (a bottleneck for very accurate optimization on this task), we have been able to improve the performance of ALI-G. ALI-G, the L4 methods and RMSProp obtain approximately the same performance -- the difference is insignificant due to the limit of numerical precision on single-float numbers. We have updated the results and added a comment about numerical precision.\n\n* \u201cIn Section 5.5, given that ALI-G and DFW are related, why is there no result for DFW in Figure 4.?\u201d\n\nThis is because DFW does not support the use of the cross-entropy loss, and therefore its performance cannot be fairly compared on a cross-entropy objective function.\n\n* \u201cAs the paper mentions, AProx algorithm and ALI-G are related, why is there no comparison with AProx in the experiments?\u201d\n\nThe aProx algorithm coincides with the ALI-G algorithm when the exponential decay rate of aProx is set to zero. In the general case, this decay rate is an additional hyper-parameter that has to be chosen by the user (which adds to the tuning burden of tuning hyper-parameters). This work shows that this hyper-parameter can be avoided since ALI-G provides good performance in practice without needing to tune it \u2014 this is also visible in our convergence results that do not require it either. In other words, ALI-G shows that a simplified version of aProx with one fewer hyper-parameter provides good performance in practice. Finally, we point out that given the extent of our experiments and our available hardware, tuning a method with two hyper-parameters may require multiple weeks of computation if at all feasible."}, "signatures": ["ICLR.cc/2020/Conference/Paper896/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper896/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Neural Networks for and by Interpolation", "authors": ["Leonard Berrada", "Andrew Zisserman", "Pawan M. Kumar"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"], "keywords": ["optimization", "adaptive learning-rate", "Polyak step-size", "Newton-Raphson"], "TL;DR": "An adaptive learning-rate with a single hyper-parameter for neural networks that can interpolate the data", "abstract": "In modern supervised learning, many deep neural networks are able to interpolate the data: the empirical loss can be driven to near zero on all samples simultaneously. In this work, we explicitly exploit this interpolation property for the design of a new optimization algorithm for deep learning. Specifically, we use it to compute an adaptive learning-rate in closed form at each iteration. This results in the Adaptive Learning-rates for Interpolation with Gradients (ALI-G) algorithm. ALI-G retains the main advantage of SGD which is a low computational cost per iteration. But unlike SGD, the learning-rate of ALI-G uses a single constant hyper-parameter and does not require a decay schedule, which makes it considerably easier to tune. We provide convergence guarantees of ALI-G in the stochastic convex setting. Notably, all our convergence results tackle the realistic case where the interpolation property is satisfied up to some tolerance. We provide experiments on a variety of architectures and tasks: (i) learning a differentiable neural computer; (ii) training a wide residual network on the SVHN data set; (iii) training a Bi-LSTM on the SNLI data set; and (iv) training wide residual networks and densely connected networks on the CIFAR data sets. ALI-G produces state-of-the-art results among adaptive methods, and even yields comparable performance with SGD, which requires manually tuned learning-rate schedules. Furthermore, ALI-G is simple to implement in any standard deep learning framework and can be used as a drop-in replacement in existing code.", "pdf": "/pdf/f134558a175667f1bfb61244a540c8bfe386d168.pdf", "code": "https://anonymous.4open.science/repository/14f2b37d-2bef-4b3f-b47c-dd257ce75543", "paperhash": "berrada|training_neural_networks_for_and_by_interpolation", "original_pdf": "/attachment/00858758a646a139bf647f1c91c057e9df9deb07.pdf", "_bibtex": "@misc{\nberrada2020training,\ntitle={Training Neural Networks for and by Interpolation},\nauthor={Leonard Berrada and Andrew Zisserman and Pawan M. Kumar},\nyear={2020},\nurl={https://openreview.net/forum?id=BJevJCVYvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJevJCVYvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper896/Authors", "ICLR.cc/2020/Conference/Paper896/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper896/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper896/Reviewers", "ICLR.cc/2020/Conference/Paper896/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper896/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper896/Authors|ICLR.cc/2020/Conference/Paper896/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164537, "tmdate": 1576860554890, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper896/Authors", "ICLR.cc/2020/Conference/Paper896/Reviewers", "ICLR.cc/2020/Conference/Paper896/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper896/-/Official_Comment"}}}, {"id": "ryecmgiqiB", "original": null, "number": 3, "cdate": 1573724193823, "ddate": null, "tcdate": 1573724193823, "tmdate": 1573724193823, "tddate": null, "forum": "BJevJCVYvB", "replyto": "Syxp4JxAKH", "invitation": "ICLR.cc/2020/Conference/Paper896/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "We thank the reviewer for their comments, which we answer below:\n\n* \u201cdid you train a single or multiple models for each result that is reported?\u201d\n\nFor all datasets except CIFAR, we train a single model. On the CIFAR tasks (the ones with the largest possible variance), we now provide the standard deviation over three independent runs in the appendix \u2014 these remain lower than 0.3 for ALI-G and SGD. We have added these experimental details in the revised submission.\n\n* \u201cDo different runs start from the exact same weight initialization?\u201d\n\nFor all experiments, the model starts with a random initialization that may be different: the random seed is itself randomly generated for each experiment. We believe that such tasks tend to be relatively robust to using different random seeds (see answer above).\n\n* \u201cWhat condition was used to stop the training?\u201d, \u201cThe graphs in Figure 4 do look like the models did not converge yet\u201d\n\nIn each experiment, the training loop terminates after a fixed number of iterations/ epochs, set as follows: \n10k steps for DNC (following (Graves et al., 2016); \n160 epochs for WRN on SVHN (following (Zagoruyko & Komodakis, 2016)); \n20 epochs for SNLI (following Conneau et al., 2017); \n200 epochs for WRN on CIFAR (following ( Zagoruyko & Komodakis, 2016)); \n300 epochs for DN on CIFAR (following (Huang et al., 2017)). \n We have added these experimental details in the revised submission.\n\n* \u201cDoes the method lead to divergence in this case, or is it subpar to other methods?\u201d\n\nWhen the interpolation assumption is not satisfied, the learning-rate of ALI-G falls back to its maximal value, and then ALI-G behaves exactly as SGD. Indeed, rephrasing Proposition 1, ALI-G can be seen as an extension of SGD which exploits the fact that the loss is non-negative to adapt its learning-rate. When the interpolation property is satisfied, this knowledge is useful for convergence of the algorithm, because the lower bound of zero corresponds to the minimum value that can be attained. When the interpolation property is not satisfied, the lower bound is too loose to be useful and therefore ALI-G behaves exactly like non-adaptive SGD.\n\n* \u201cI think section 2.2. could benefit from a short motivational introduction\u201d\n\nWe thank the reviewer for the useful suggestion, we have updated the paper accordingly."}, "signatures": ["ICLR.cc/2020/Conference/Paper896/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper896/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Neural Networks for and by Interpolation", "authors": ["Leonard Berrada", "Andrew Zisserman", "Pawan M. Kumar"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"], "keywords": ["optimization", "adaptive learning-rate", "Polyak step-size", "Newton-Raphson"], "TL;DR": "An adaptive learning-rate with a single hyper-parameter for neural networks that can interpolate the data", "abstract": "In modern supervised learning, many deep neural networks are able to interpolate the data: the empirical loss can be driven to near zero on all samples simultaneously. In this work, we explicitly exploit this interpolation property for the design of a new optimization algorithm for deep learning. Specifically, we use it to compute an adaptive learning-rate in closed form at each iteration. This results in the Adaptive Learning-rates for Interpolation with Gradients (ALI-G) algorithm. ALI-G retains the main advantage of SGD which is a low computational cost per iteration. But unlike SGD, the learning-rate of ALI-G uses a single constant hyper-parameter and does not require a decay schedule, which makes it considerably easier to tune. We provide convergence guarantees of ALI-G in the stochastic convex setting. Notably, all our convergence results tackle the realistic case where the interpolation property is satisfied up to some tolerance. We provide experiments on a variety of architectures and tasks: (i) learning a differentiable neural computer; (ii) training a wide residual network on the SVHN data set; (iii) training a Bi-LSTM on the SNLI data set; and (iv) training wide residual networks and densely connected networks on the CIFAR data sets. ALI-G produces state-of-the-art results among adaptive methods, and even yields comparable performance with SGD, which requires manually tuned learning-rate schedules. Furthermore, ALI-G is simple to implement in any standard deep learning framework and can be used as a drop-in replacement in existing code.", "pdf": "/pdf/f134558a175667f1bfb61244a540c8bfe386d168.pdf", "code": "https://anonymous.4open.science/repository/14f2b37d-2bef-4b3f-b47c-dd257ce75543", "paperhash": "berrada|training_neural_networks_for_and_by_interpolation", "original_pdf": "/attachment/00858758a646a139bf647f1c91c057e9df9deb07.pdf", "_bibtex": "@misc{\nberrada2020training,\ntitle={Training Neural Networks for and by Interpolation},\nauthor={Leonard Berrada and Andrew Zisserman and Pawan M. Kumar},\nyear={2020},\nurl={https://openreview.net/forum?id=BJevJCVYvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJevJCVYvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper896/Authors", "ICLR.cc/2020/Conference/Paper896/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper896/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper896/Reviewers", "ICLR.cc/2020/Conference/Paper896/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper896/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper896/Authors|ICLR.cc/2020/Conference/Paper896/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164537, "tmdate": 1576860554890, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper896/Authors", "ICLR.cc/2020/Conference/Paper896/Reviewers", "ICLR.cc/2020/Conference/Paper896/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper896/-/Official_Comment"}}}, {"id": "B1lwP1oqiH", "original": null, "number": 2, "cdate": 1573723998681, "ddate": null, "tcdate": 1573723998681, "tmdate": 1573723998681, "tddate": null, "forum": "BJevJCVYvB", "replyto": "BJgm9PbTqH", "invitation": "ICLR.cc/2020/Conference/Paper896/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "We thank the reviewer for their detailed comments, which we answer below:\n\n* \u201c[...] but it seems like there are other types of losses that don\u2019t necessarily to go 0. For example, terms in the objective such as entropy seem worrisome.\u201d\n\nWe reassert that the cross-entropy loss is well-handled by our framework: it is lower bounded by zero, and when the model can \u201cseparate\u201d the data with an arbitrary margin in logits space, its value can get arbitrarily close to zero. In addition, the proximity between the actual value and the ideal value of zero is captured by the interpolation tolerance $\\varepsilon$. For a practical example of the cross-entropy loss going near zero, we refer the reviewer to Figure 4, which plots the evolution of the cross-entropy loss during training: final values obtained by ALI-G are indeed very close to zero.\n\n* \u201cHow does Theorem 1 (Regarding convexity) related to the \u2018each training sample can use its own learning rate without harming progress on the other ones\u2019 and/or \u2018allow the updates to rely on the stochastic estimate rather than the exact\u2019.\u201d\n\nThese statements refer to the fact that, as shown in Theorem 1, by assuming interpolation ALI-G provably converges while using only $\\ell_{z_t}(w_t)$ and $\\nabla \\ell_{z_t}(w_t)$ (stochastic estimation per sample) to compute its learning-rate. In contrast, the Polyak step-size, which does not exploit interpolation, would use $f(w_t)$ and $\\nabla f(w_t)$ to compute the learning-rate (exact / non-stochastic computation over all training samples). This constitutes a major computational advantage of ALI-G over the usual Polyak step-size. We have added the above clarification in the new version of the paper.\n\n* On novelty:\n\nBeyond our theoretical analysis \u2014 which required a significant amount of work by itself, it is important to note that ALI-G requires only one hyper-parameter instead of four for L4. Since hyper-parameter sweeps are often the computational bottleneck for applied machine learning (the cost of extensive tuning grows exponentially with the number of hyper-parameters), this constitutes a considerable advantage for practical applications.\n\nIn addition, L4 is sensitive to noise,which makes it very difficult to tune: we observed in practice that L4 sometimes fails to learn anything or even diverges in some cases \u2014 similar issues have been reported in (Vaswani et al., 2019b). In contrast, ALI-G consistently provides reliable optimization in the interpolation setting at considerably less tuning cost. "}, "signatures": ["ICLR.cc/2020/Conference/Paper896/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper896/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Neural Networks for and by Interpolation", "authors": ["Leonard Berrada", "Andrew Zisserman", "Pawan M. Kumar"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"], "keywords": ["optimization", "adaptive learning-rate", "Polyak step-size", "Newton-Raphson"], "TL;DR": "An adaptive learning-rate with a single hyper-parameter for neural networks that can interpolate the data", "abstract": "In modern supervised learning, many deep neural networks are able to interpolate the data: the empirical loss can be driven to near zero on all samples simultaneously. In this work, we explicitly exploit this interpolation property for the design of a new optimization algorithm for deep learning. Specifically, we use it to compute an adaptive learning-rate in closed form at each iteration. This results in the Adaptive Learning-rates for Interpolation with Gradients (ALI-G) algorithm. ALI-G retains the main advantage of SGD which is a low computational cost per iteration. But unlike SGD, the learning-rate of ALI-G uses a single constant hyper-parameter and does not require a decay schedule, which makes it considerably easier to tune. We provide convergence guarantees of ALI-G in the stochastic convex setting. Notably, all our convergence results tackle the realistic case where the interpolation property is satisfied up to some tolerance. We provide experiments on a variety of architectures and tasks: (i) learning a differentiable neural computer; (ii) training a wide residual network on the SVHN data set; (iii) training a Bi-LSTM on the SNLI data set; and (iv) training wide residual networks and densely connected networks on the CIFAR data sets. ALI-G produces state-of-the-art results among adaptive methods, and even yields comparable performance with SGD, which requires manually tuned learning-rate schedules. Furthermore, ALI-G is simple to implement in any standard deep learning framework and can be used as a drop-in replacement in existing code.", "pdf": "/pdf/f134558a175667f1bfb61244a540c8bfe386d168.pdf", "code": "https://anonymous.4open.science/repository/14f2b37d-2bef-4b3f-b47c-dd257ce75543", "paperhash": "berrada|training_neural_networks_for_and_by_interpolation", "original_pdf": "/attachment/00858758a646a139bf647f1c91c057e9df9deb07.pdf", "_bibtex": "@misc{\nberrada2020training,\ntitle={Training Neural Networks for and by Interpolation},\nauthor={Leonard Berrada and Andrew Zisserman and Pawan M. Kumar},\nyear={2020},\nurl={https://openreview.net/forum?id=BJevJCVYvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJevJCVYvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper896/Authors", "ICLR.cc/2020/Conference/Paper896/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper896/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper896/Reviewers", "ICLR.cc/2020/Conference/Paper896/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper896/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper896/Authors|ICLR.cc/2020/Conference/Paper896/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164537, "tmdate": 1576860554890, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper896/Authors", "ICLR.cc/2020/Conference/Paper896/Reviewers", "ICLR.cc/2020/Conference/Paper896/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper896/-/Official_Comment"}}}, {"id": "Syxp4JxAKH", "original": null, "number": 2, "cdate": 1571843893319, "ddate": null, "tcdate": 1571843893319, "tmdate": 1572972538764, "tddate": null, "forum": "BJevJCVYvB", "replyto": "BJevJCVYvB", "invitation": "ICLR.cc/2020/Conference/Paper896/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes a new adaptive learning rate method which is tailored to the optimization of deep neural networks. The motivating observation is that over-parameterized DNNs are able to interpolate the training data (i.e. they are able to reach near-zero training error). This enables application of the Polyak update rule to stochastic updates and a simplification by assuming a zero minimal training loss. A number of proofs for convergence in various convex settings are provided, and empirical evaluation on several benchmarks demonstrates (a) ability to optimize complex architectures, (b) performance improvements over, and (c) performance close to manually tuned SGD learning rates.\n\nI vote for accepting this paper. The approach is well-motivated, the method is described clearly and detail, and the experiments support the paper's claims well. What I would still like to see are a few additional details regarding the experimental protocol. In particular, did you train a single or multiple models for each result that is reported? Do different runs start from the exact same weight initialization? What condition was used to stop the training? The results in section 5.2. are all very close to each other, and it would be helpful to have a sense of the variability of the different methods. The graphs in Figure 4 do look like the models did not converge yet.\n\nGenerally, it would be nice to examine the behavior of the method in cases where the neural network is underparameterized or is otherwise unable to effectively interpolate the training data. Does the method lead to divergence in this case, or is it subpar to other methods? I think section 2.2. could benefit from a short motivational introduction; on the first read, I was not clear about the purpose of introducing the Polyak step size as it is not mentioned explicitly in the text leading to it."}, "signatures": ["ICLR.cc/2020/Conference/Paper896/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper896/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Neural Networks for and by Interpolation", "authors": ["Leonard Berrada", "Andrew Zisserman", "Pawan M. Kumar"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"], "keywords": ["optimization", "adaptive learning-rate", "Polyak step-size", "Newton-Raphson"], "TL;DR": "An adaptive learning-rate with a single hyper-parameter for neural networks that can interpolate the data", "abstract": "In modern supervised learning, many deep neural networks are able to interpolate the data: the empirical loss can be driven to near zero on all samples simultaneously. In this work, we explicitly exploit this interpolation property for the design of a new optimization algorithm for deep learning. Specifically, we use it to compute an adaptive learning-rate in closed form at each iteration. This results in the Adaptive Learning-rates for Interpolation with Gradients (ALI-G) algorithm. ALI-G retains the main advantage of SGD which is a low computational cost per iteration. But unlike SGD, the learning-rate of ALI-G uses a single constant hyper-parameter and does not require a decay schedule, which makes it considerably easier to tune. We provide convergence guarantees of ALI-G in the stochastic convex setting. Notably, all our convergence results tackle the realistic case where the interpolation property is satisfied up to some tolerance. We provide experiments on a variety of architectures and tasks: (i) learning a differentiable neural computer; (ii) training a wide residual network on the SVHN data set; (iii) training a Bi-LSTM on the SNLI data set; and (iv) training wide residual networks and densely connected networks on the CIFAR data sets. ALI-G produces state-of-the-art results among adaptive methods, and even yields comparable performance with SGD, which requires manually tuned learning-rate schedules. Furthermore, ALI-G is simple to implement in any standard deep learning framework and can be used as a drop-in replacement in existing code.", "pdf": "/pdf/f134558a175667f1bfb61244a540c8bfe386d168.pdf", "code": "https://anonymous.4open.science/repository/14f2b37d-2bef-4b3f-b47c-dd257ce75543", "paperhash": "berrada|training_neural_networks_for_and_by_interpolation", "original_pdf": "/attachment/00858758a646a139bf647f1c91c057e9df9deb07.pdf", "_bibtex": "@misc{\nberrada2020training,\ntitle={Training Neural Networks for and by Interpolation},\nauthor={Leonard Berrada and Andrew Zisserman and Pawan M. Kumar},\nyear={2020},\nurl={https://openreview.net/forum?id=BJevJCVYvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJevJCVYvB", "replyto": "BJevJCVYvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper896/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper896/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575767975855, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper896/Reviewers"], "noninvitees": [], "tcdate": 1570237745407, "tmdate": 1575767975868, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper896/-/Official_Review"}}}], "count": 11}