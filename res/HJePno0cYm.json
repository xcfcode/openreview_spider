{"notes": [{"id": "SJgCkbBfXV", "original": null, "number": 13, "cdate": 1548009701538, "ddate": null, "tcdate": 1548009701538, "tmdate": 1548009701538, "tddate": null, "forum": "HJePno0cYm", "replyto": "Skx4CMJfQN", "invitation": "ICLR.cc/2019/Conference/-/Paper717/Official_Comment", "content": {"title": "response", "comment": "Hi Rajarshi, thanks a lot for your comments. For the $m$, we define it in the paragraph right before section 3.3. It refers to the \"memory\", which can contain $h_{\\tau - 1}^{n-1}$ or additionally more faraway segments like $h_{\\tau - 2}^{n-1}$. "}, "signatures": ["ICLR.cc/2019/Conference/Paper717/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper717/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer-XL: Language Modeling with Longer-Term Dependency", "abstract": "We propose a novel neural architecture, Transformer-XL, for modeling longer-term dependency. To address the limitation of fixed-length contexts, we introduce a notion of recurrence by reusing the representations from the history. Empirically, we show state-of-the-art (SoTA) results on both word-level and character-level language modeling datasets, including WikiText-103, One Billion Word, Penn Treebank, and enwiki8. Notably, we improve the SoTA results from 1.06 to 0.99 in bpc on enwiki8, from 33.0 to 18.9 in perplexity on WikiText-103, and from 28.0 to 23.5 in perplexity on One Billion Word. Performance improves when the attention length increases during evaluation, and our best model attends to up to 1,600 words and 3,800 characters. To quantify the effective length of dependency, we devise a new metric and show that on WikiText-103 Transformer-XL manages to model dependency that is about 80% longer than recurrent networks and 450% longer than Transformer. Moreover, Transformer-XL is up to 1,800+ times faster than vanilla Transformer during evaluation.", "keywords": ["Language Modeling", "Self-Attention"], "authorids": ["zander.dai@gmail.com", "zhiliny@cs.cmu.edu", "yiming@cs.cmu.edu", "wcohen@google.com", "jgc@cs.cmu.edu", "qvl@google.com", "rsalakhu@cs.cmu.edu"], "authors": ["Zihang Dai*", "Zhilin Yang*", "Yiming Yang", "William W. Cohen", "Jaime Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov"], "pdf": "/pdf/4ccd6a96dea7974a19a037cd6374beb1b5fbfa2d.pdf", "paperhash": "dai|transformerxl_language_modeling_with_longerterm_dependency", "_bibtex": "@misc{\ndai*2019transformerxl,\ntitle={Transformer-{XL}: Language Modeling with Longer-Term Dependency},\nauthor={Zihang Dai* and Zhilin Yang* and Yiming Yang and William W. Cohen and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},\nyear={2019},\nurl={https://openreview.net/forum?id=HJePno0cYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper717/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606557, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJePno0cYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper717/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper717/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper717/Authors|ICLR.cc/2019/Conference/Paper717/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606557}}}, {"id": "Skx4CMJfQN", "original": null, "number": 3, "cdate": 1547985611813, "ddate": null, "tcdate": 1547985611813, "tmdate": 1547992605950, "tddate": null, "forum": "HJePno0cYm", "replyto": "HJePno0cYm", "invitation": "ICLR.cc/2019/Conference/-/Paper717/Public_Comment", "content": {"comment": "This paper makes great contributions and like many, I was sad to see it get rejected. \n\nI was looking at the equations closely and the final equations describing the model has a variable \"m\" which hasn't been defined before. Specifically, I am referring to the \"m\" within the stop-gradient operator in the equation below.\n\n$\\tilde{h}_{\\tau}^{n - 1} = [SG(m_{\\tau}^{n-1}) \\cdot h_{tau}^{n-1}]$\n\nThis set of equations does not have a direct dependence on $h_{\\tau - 1}^{n-1}$ (the previous segment), so I am guessing \"m\" is capturing it somehow and it is not very clear presently.\n\nThank you in advance for the clarification.", "title": "Variable \"m\" in equation (end of page 5)"}, "signatures": ["~Rajarshi_Das1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Rajarshi_Das1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer-XL: Language Modeling with Longer-Term Dependency", "abstract": "We propose a novel neural architecture, Transformer-XL, for modeling longer-term dependency. To address the limitation of fixed-length contexts, we introduce a notion of recurrence by reusing the representations from the history. Empirically, we show state-of-the-art (SoTA) results on both word-level and character-level language modeling datasets, including WikiText-103, One Billion Word, Penn Treebank, and enwiki8. Notably, we improve the SoTA results from 1.06 to 0.99 in bpc on enwiki8, from 33.0 to 18.9 in perplexity on WikiText-103, and from 28.0 to 23.5 in perplexity on One Billion Word. Performance improves when the attention length increases during evaluation, and our best model attends to up to 1,600 words and 3,800 characters. To quantify the effective length of dependency, we devise a new metric and show that on WikiText-103 Transformer-XL manages to model dependency that is about 80% longer than recurrent networks and 450% longer than Transformer. Moreover, Transformer-XL is up to 1,800+ times faster than vanilla Transformer during evaluation.", "keywords": ["Language Modeling", "Self-Attention"], "authorids": ["zander.dai@gmail.com", "zhiliny@cs.cmu.edu", "yiming@cs.cmu.edu", "wcohen@google.com", "jgc@cs.cmu.edu", "qvl@google.com", "rsalakhu@cs.cmu.edu"], "authors": ["Zihang Dai*", "Zhilin Yang*", "Yiming Yang", "William W. Cohen", "Jaime Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov"], "pdf": "/pdf/4ccd6a96dea7974a19a037cd6374beb1b5fbfa2d.pdf", "paperhash": "dai|transformerxl_language_modeling_with_longerterm_dependency", "_bibtex": "@misc{\ndai*2019transformerxl,\ntitle={Transformer-{XL}: Language Modeling with Longer-Term Dependency},\nauthor={Zihang Dai* and Zhilin Yang* and Yiming Yang and William W. Cohen and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},\nyear={2019},\nurl={https://openreview.net/forum?id=HJePno0cYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper717/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311768784, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HJePno0cYm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311768784}}}, {"id": "BJxw7iX4fN", "original": null, "number": 12, "cdate": 1547086623371, "ddate": null, "tcdate": 1547086623371, "tmdate": 1547086623371, "tddate": null, "forum": "HJePno0cYm", "replyto": "HJePno0cYm", "invitation": "ICLR.cc/2019/Conference/-/Paper717/Official_Comment", "content": {"title": "[DEPRECATED] This version is outdated", "comment": "We have released a new version of this paper on arxiv https://arxiv.org/abs/1901.02860\nalong with code, pretrained models, hyperparameters, as well as new (even better) results.\n\nPlease refer to our arxiv version in your future work."}, "signatures": ["ICLR.cc/2019/Conference/Paper717/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper717/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer-XL: Language Modeling with Longer-Term Dependency", "abstract": "We propose a novel neural architecture, Transformer-XL, for modeling longer-term dependency. To address the limitation of fixed-length contexts, we introduce a notion of recurrence by reusing the representations from the history. Empirically, we show state-of-the-art (SoTA) results on both word-level and character-level language modeling datasets, including WikiText-103, One Billion Word, Penn Treebank, and enwiki8. Notably, we improve the SoTA results from 1.06 to 0.99 in bpc on enwiki8, from 33.0 to 18.9 in perplexity on WikiText-103, and from 28.0 to 23.5 in perplexity on One Billion Word. Performance improves when the attention length increases during evaluation, and our best model attends to up to 1,600 words and 3,800 characters. To quantify the effective length of dependency, we devise a new metric and show that on WikiText-103 Transformer-XL manages to model dependency that is about 80% longer than recurrent networks and 450% longer than Transformer. Moreover, Transformer-XL is up to 1,800+ times faster than vanilla Transformer during evaluation.", "keywords": ["Language Modeling", "Self-Attention"], "authorids": ["zander.dai@gmail.com", "zhiliny@cs.cmu.edu", "yiming@cs.cmu.edu", "wcohen@google.com", "jgc@cs.cmu.edu", "qvl@google.com", "rsalakhu@cs.cmu.edu"], "authors": ["Zihang Dai*", "Zhilin Yang*", "Yiming Yang", "William W. Cohen", "Jaime Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov"], "pdf": "/pdf/4ccd6a96dea7974a19a037cd6374beb1b5fbfa2d.pdf", "paperhash": "dai|transformerxl_language_modeling_with_longerterm_dependency", "_bibtex": "@misc{\ndai*2019transformerxl,\ntitle={Transformer-{XL}: Language Modeling with Longer-Term Dependency},\nauthor={Zihang Dai* and Zhilin Yang* and Yiming Yang and William W. Cohen and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},\nyear={2019},\nurl={https://openreview.net/forum?id=HJePno0cYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper717/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606557, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJePno0cYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper717/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper717/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper717/Authors|ICLR.cc/2019/Conference/Paper717/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606557}}}, {"id": "Syx0H-YlbN", "original": null, "number": 11, "cdate": 1545797957609, "ddate": null, "tcdate": 1545797957609, "tmdate": 1545803345341, "tddate": null, "forum": "HJePno0cYm", "replyto": "rkg1fe3xlE", "invitation": "ICLR.cc/2019/Conference/-/Paper717/Official_Comment", "content": {"title": "response", "comment": "\n=== About the technical contribution ===\n\nFirstly, as we have explained in the rebuttal and paper, trivially applying truncated BPTT to Transformer will NOT work due to the temporal confusion caused by using the same absolute positional encoding on two consecutive segments. In this work, we identify that it is the temporal confusion problem which prevents the reuse of historical hidden states. More importantly, we figure out the confusion could be resolved by only injecting relative positional information. This process of identifying, analyzing and solving the problem is a non-trivial scientific process, as no other previous or contemporary work targeting at using self-attention for language modeling has provided such a solution despite the fact that everyone working in LM is familiar with truncated BPTT.  \n\nAdditionally, to facilitate the learning of the recurrence mechanism, we also propose a more generalizable relative positional encoding and establish its non-trivial performance advantage in ablation.\n\nHence, we respectfully disagree with the argument that the proposed approach is \u201ca rather trivial application of earlier approaches such as truncated backprop.\u201d\n\n=== About the value of enabling recurrence for self-attention in the context of LM ===\n\nWe think the question can be broken into three sub-questions with different levels:\n(1) Is a better language model by itself important or not?\n(2) What is the application value of a better self-attention LM that can utilize recurrence?\n(3) Is it useful to create recurrence in self-attention in general, e.g., beyond the text domain?\n\nThe question (1) essentially asks whether we need a better density estimator for text. The answer to this question can be rather subjective and differ from person to person. That said, as one of the most fundamental statistical questions, density estimation should have its scientific values.\n\nFor question (2), it is not difficult to come up with a list of potential applications. Firstly, many document-level problems could benefit from the proposed model, such as the document-level summarization, translation, seqeuntial labeling, and reading comprehension. Note that these tasks don\u2019t have to be restricted to text generation. Secondly, besides serving as an architecture for downstream tasks, language models can also be used to perform \u201cunsupervised feature learning\u201d as demonstrated by recent advancement in NLP [1,2]. Hence, given a language model that can better capture the contextual information, it is very likely the hidden representations within the language model are also superior.\n\nFinally, question (3) is concerned with whether the techniques proposed in this work can be applied to other domains other than language. On this matter, we believe there exists a common desire of capturing longer-term dependency in sequence modeling. For example, in the speech domain, the raw data often has a sample rate of 16K Hz, which means that each second of speech data is a sequence of 16K steps. Similarly, in the domain of time seriers analysis (e.g. sensor data), the sequence length can also be very long.\n\nIn summary, we believe language modeling is a reasonable testbed of model and algorithm development for NLP and more broadly sequence modeling.\n\n--------------------------------\n[1] Peters, M., et. al. (2017) Deep contextualized word representations\n[2] Devlin, J., et. al. (2018) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, "signatures": ["ICLR.cc/2019/Conference/Paper717/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper717/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer-XL: Language Modeling with Longer-Term Dependency", "abstract": "We propose a novel neural architecture, Transformer-XL, for modeling longer-term dependency. To address the limitation of fixed-length contexts, we introduce a notion of recurrence by reusing the representations from the history. Empirically, we show state-of-the-art (SoTA) results on both word-level and character-level language modeling datasets, including WikiText-103, One Billion Word, Penn Treebank, and enwiki8. Notably, we improve the SoTA results from 1.06 to 0.99 in bpc on enwiki8, from 33.0 to 18.9 in perplexity on WikiText-103, and from 28.0 to 23.5 in perplexity on One Billion Word. Performance improves when the attention length increases during evaluation, and our best model attends to up to 1,600 words and 3,800 characters. To quantify the effective length of dependency, we devise a new metric and show that on WikiText-103 Transformer-XL manages to model dependency that is about 80% longer than recurrent networks and 450% longer than Transformer. Moreover, Transformer-XL is up to 1,800+ times faster than vanilla Transformer during evaluation.", "keywords": ["Language Modeling", "Self-Attention"], "authorids": ["zander.dai@gmail.com", "zhiliny@cs.cmu.edu", "yiming@cs.cmu.edu", "wcohen@google.com", "jgc@cs.cmu.edu", "qvl@google.com", "rsalakhu@cs.cmu.edu"], "authors": ["Zihang Dai*", "Zhilin Yang*", "Yiming Yang", "William W. Cohen", "Jaime Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov"], "pdf": "/pdf/4ccd6a96dea7974a19a037cd6374beb1b5fbfa2d.pdf", "paperhash": "dai|transformerxl_language_modeling_with_longerterm_dependency", "_bibtex": "@misc{\ndai*2019transformerxl,\ntitle={Transformer-{XL}: Language Modeling with Longer-Term Dependency},\nauthor={Zihang Dai* and Zhilin Yang* and Yiming Yang and William W. Cohen and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},\nyear={2019},\nurl={https://openreview.net/forum?id=HJePno0cYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper717/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606557, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJePno0cYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper717/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper717/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper717/Authors|ICLR.cc/2019/Conference/Paper717/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606557}}}, {"id": "HJePno0cYm", "original": "H1ll8Y2qKm", "number": 717, "cdate": 1538087854695, "ddate": null, "tcdate": 1538087854695, "tmdate": 1545355440132, "tddate": null, "forum": "HJePno0cYm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Transformer-XL: Language Modeling with Longer-Term Dependency", "abstract": "We propose a novel neural architecture, Transformer-XL, for modeling longer-term dependency. To address the limitation of fixed-length contexts, we introduce a notion of recurrence by reusing the representations from the history. Empirically, we show state-of-the-art (SoTA) results on both word-level and character-level language modeling datasets, including WikiText-103, One Billion Word, Penn Treebank, and enwiki8. Notably, we improve the SoTA results from 1.06 to 0.99 in bpc on enwiki8, from 33.0 to 18.9 in perplexity on WikiText-103, and from 28.0 to 23.5 in perplexity on One Billion Word. Performance improves when the attention length increases during evaluation, and our best model attends to up to 1,600 words and 3,800 characters. To quantify the effective length of dependency, we devise a new metric and show that on WikiText-103 Transformer-XL manages to model dependency that is about 80% longer than recurrent networks and 450% longer than Transformer. Moreover, Transformer-XL is up to 1,800+ times faster than vanilla Transformer during evaluation.", "keywords": ["Language Modeling", "Self-Attention"], "authorids": ["zander.dai@gmail.com", "zhiliny@cs.cmu.edu", "yiming@cs.cmu.edu", "wcohen@google.com", "jgc@cs.cmu.edu", "qvl@google.com", "rsalakhu@cs.cmu.edu"], "authors": ["Zihang Dai*", "Zhilin Yang*", "Yiming Yang", "William W. Cohen", "Jaime Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov"], "pdf": "/pdf/4ccd6a96dea7974a19a037cd6374beb1b5fbfa2d.pdf", "paperhash": "dai|transformerxl_language_modeling_with_longerterm_dependency", "_bibtex": "@misc{\ndai*2019transformerxl,\ntitle={Transformer-{XL}: Language Modeling with Longer-Term Dependency},\nauthor={Zihang Dai* and Zhilin Yang* and Yiming Yang and William W. Cohen and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},\nyear={2019},\nurl={https://openreview.net/forum?id=HJePno0cYm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 19, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rkg1fe3xlE", "original": null, "number": 1, "cdate": 1544761350973, "ddate": null, "tcdate": 1544761350973, "tmdate": 1545354477249, "tddate": null, "forum": "HJePno0cYm", "replyto": "HJePno0cYm", "invitation": "ICLR.cc/2019/Conference/-/Paper717/Meta_Review", "content": {"metareview": "despite the (significant) improvement in language modelling, it has always been a thorny issue whether better language models (at this level) lead to better performance in the downstream task or whether such a technique could be used to build a better conditional language model which often focuses on the aspect of generation. in this context, the reviewers found it difficult to see the merit of the proposed approach, as the technique itself may be considered a rather trivial application of earlier approaches such as truncated backprop. it would be good to apply this technique to e.g. document-level generation and see if the proposed approach can strike an amazing balance between computational efficiency and generation performance.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "reject"}, "signatures": ["ICLR.cc/2019/Conference/Paper717/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper717/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer-XL: Language Modeling with Longer-Term Dependency", "abstract": "We propose a novel neural architecture, Transformer-XL, for modeling longer-term dependency. To address the limitation of fixed-length contexts, we introduce a notion of recurrence by reusing the representations from the history. Empirically, we show state-of-the-art (SoTA) results on both word-level and character-level language modeling datasets, including WikiText-103, One Billion Word, Penn Treebank, and enwiki8. Notably, we improve the SoTA results from 1.06 to 0.99 in bpc on enwiki8, from 33.0 to 18.9 in perplexity on WikiText-103, and from 28.0 to 23.5 in perplexity on One Billion Word. Performance improves when the attention length increases during evaluation, and our best model attends to up to 1,600 words and 3,800 characters. To quantify the effective length of dependency, we devise a new metric and show that on WikiText-103 Transformer-XL manages to model dependency that is about 80% longer than recurrent networks and 450% longer than Transformer. Moreover, Transformer-XL is up to 1,800+ times faster than vanilla Transformer during evaluation.", "keywords": ["Language Modeling", "Self-Attention"], "authorids": ["zander.dai@gmail.com", "zhiliny@cs.cmu.edu", "yiming@cs.cmu.edu", "wcohen@google.com", "jgc@cs.cmu.edu", "qvl@google.com", "rsalakhu@cs.cmu.edu"], "authors": ["Zihang Dai*", "Zhilin Yang*", "Yiming Yang", "William W. Cohen", "Jaime Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov"], "pdf": "/pdf/4ccd6a96dea7974a19a037cd6374beb1b5fbfa2d.pdf", "paperhash": "dai|transformerxl_language_modeling_with_longerterm_dependency", "_bibtex": "@misc{\ndai*2019transformerxl,\ntitle={Transformer-{XL}: Language Modeling with Longer-Term Dependency},\nauthor={Zihang Dai* and Zhilin Yang* and Yiming Yang and William W. Cohen and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},\nyear={2019},\nurl={https://openreview.net/forum?id=HJePno0cYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper717/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353111707, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJePno0cYm", "replyto": "HJePno0cYm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper717/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper717/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper717/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353111707}}}, {"id": "HJe5zCaEeV", "original": null, "number": 10, "cdate": 1545031185604, "ddate": null, "tcdate": 1545031185604, "tmdate": 1545031185604, "tddate": null, "forum": "HJePno0cYm", "replyto": "rJeP_0nEx4", "invitation": "ICLR.cc/2019/Conference/-/Paper717/Official_Comment", "content": {"title": "response", "comment": "Thank you for your questions. We will publish our code along with our hyper-parameters on all the datasets very soon!"}, "signatures": ["ICLR.cc/2019/Conference/Paper717/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper717/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer-XL: Language Modeling with Longer-Term Dependency", "abstract": "We propose a novel neural architecture, Transformer-XL, for modeling longer-term dependency. To address the limitation of fixed-length contexts, we introduce a notion of recurrence by reusing the representations from the history. Empirically, we show state-of-the-art (SoTA) results on both word-level and character-level language modeling datasets, including WikiText-103, One Billion Word, Penn Treebank, and enwiki8. Notably, we improve the SoTA results from 1.06 to 0.99 in bpc on enwiki8, from 33.0 to 18.9 in perplexity on WikiText-103, and from 28.0 to 23.5 in perplexity on One Billion Word. Performance improves when the attention length increases during evaluation, and our best model attends to up to 1,600 words and 3,800 characters. To quantify the effective length of dependency, we devise a new metric and show that on WikiText-103 Transformer-XL manages to model dependency that is about 80% longer than recurrent networks and 450% longer than Transformer. Moreover, Transformer-XL is up to 1,800+ times faster than vanilla Transformer during evaluation.", "keywords": ["Language Modeling", "Self-Attention"], "authorids": ["zander.dai@gmail.com", "zhiliny@cs.cmu.edu", "yiming@cs.cmu.edu", "wcohen@google.com", "jgc@cs.cmu.edu", "qvl@google.com", "rsalakhu@cs.cmu.edu"], "authors": ["Zihang Dai*", "Zhilin Yang*", "Yiming Yang", "William W. Cohen", "Jaime Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov"], "pdf": "/pdf/4ccd6a96dea7974a19a037cd6374beb1b5fbfa2d.pdf", "paperhash": "dai|transformerxl_language_modeling_with_longerterm_dependency", "_bibtex": "@misc{\ndai*2019transformerxl,\ntitle={Transformer-{XL}: Language Modeling with Longer-Term Dependency},\nauthor={Zihang Dai* and Zhilin Yang* and Yiming Yang and William W. Cohen and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},\nyear={2019},\nurl={https://openreview.net/forum?id=HJePno0cYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper717/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606557, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJePno0cYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper717/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper717/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper717/Authors|ICLR.cc/2019/Conference/Paper717/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606557}}}, {"id": "rJeP_0nEx4", "original": null, "number": 2, "cdate": 1545027183151, "ddate": null, "tcdate": 1545027183151, "tmdate": 1545027183151, "tddate": null, "forum": "HJePno0cYm", "replyto": "HJePno0cYm", "invitation": "ICLR.cc/2019/Conference/-/Paper717/Public_Comment", "content": {"comment": "Very impressive results!  For the billion-word benchmark, you are getting better perplexity numbers (23.5) than we have for models of comparable size (see https://arxiv.org/pdf/1811.02084.pdf).  Since, as you mention, context length is not an issue for this dataset, I would like to know what you are doing better so that we can improve our own results.  In particular, what are your settings for the following for the PPL=23.5 model:\n\nHyperparameters as defined in https://arxiv.org/pdf/1706.03762.pdf:\n  1. Number of layers (n)\n  2. Dimensionality of embedding matrices, layer inputs/outputs (d_model)\n  3. Feed-forward hidden size (d_ff)\n  4. Number of attention heads (h)\n  5. key/value dimensionality (d_k), (d_v)\n  7. Dropout rate\nIn addition:\n  6. Did you use the original ~800K-word vocabulary or a character-level or word-piece-level encoding scheme\n  7. Was your setup based on the tensor2tensor library or other open-source implementation?\n  8. Dropout rates\n  9. Number of training epochs\n\nThank you in advance for the clarification.", "title": "Requesting details for billion-word-lm model hyperparameters"}, "signatures": ["~Noam_Shazeer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Noam_Shazeer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer-XL: Language Modeling with Longer-Term Dependency", "abstract": "We propose a novel neural architecture, Transformer-XL, for modeling longer-term dependency. To address the limitation of fixed-length contexts, we introduce a notion of recurrence by reusing the representations from the history. Empirically, we show state-of-the-art (SoTA) results on both word-level and character-level language modeling datasets, including WikiText-103, One Billion Word, Penn Treebank, and enwiki8. Notably, we improve the SoTA results from 1.06 to 0.99 in bpc on enwiki8, from 33.0 to 18.9 in perplexity on WikiText-103, and from 28.0 to 23.5 in perplexity on One Billion Word. Performance improves when the attention length increases during evaluation, and our best model attends to up to 1,600 words and 3,800 characters. To quantify the effective length of dependency, we devise a new metric and show that on WikiText-103 Transformer-XL manages to model dependency that is about 80% longer than recurrent networks and 450% longer than Transformer. Moreover, Transformer-XL is up to 1,800+ times faster than vanilla Transformer during evaluation.", "keywords": ["Language Modeling", "Self-Attention"], "authorids": ["zander.dai@gmail.com", "zhiliny@cs.cmu.edu", "yiming@cs.cmu.edu", "wcohen@google.com", "jgc@cs.cmu.edu", "qvl@google.com", "rsalakhu@cs.cmu.edu"], "authors": ["Zihang Dai*", "Zhilin Yang*", "Yiming Yang", "William W. Cohen", "Jaime Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov"], "pdf": "/pdf/4ccd6a96dea7974a19a037cd6374beb1b5fbfa2d.pdf", "paperhash": "dai|transformerxl_language_modeling_with_longerterm_dependency", "_bibtex": "@misc{\ndai*2019transformerxl,\ntitle={Transformer-{XL}: Language Modeling with Longer-Term Dependency},\nauthor={Zihang Dai* and Zhilin Yang* and Yiming Yang and William W. Cohen and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},\nyear={2019},\nurl={https://openreview.net/forum?id=HJePno0cYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper717/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311768784, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HJePno0cYm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311768784}}}, {"id": "S1xnzg5cyN", "original": null, "number": 9, "cdate": 1544359955563, "ddate": null, "tcdate": 1544359955563, "tmdate": 1544359955563, "tddate": null, "forum": "HJePno0cYm", "replyto": "Hkla0-dp27", "invitation": "ICLR.cc/2019/Conference/-/Paper717/Official_Comment", "content": {"title": "update", "comment": "Dear reviewer, we believe we have addressed your concerns in the rebuttal (see the General Response above and the comments below). Especially, we have further improved over state-of-the-art results ever since. Do you have an updated assessment or other concerns of our paper? Thank you!"}, "signatures": ["ICLR.cc/2019/Conference/Paper717/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper717/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer-XL: Language Modeling with Longer-Term Dependency", "abstract": "We propose a novel neural architecture, Transformer-XL, for modeling longer-term dependency. To address the limitation of fixed-length contexts, we introduce a notion of recurrence by reusing the representations from the history. Empirically, we show state-of-the-art (SoTA) results on both word-level and character-level language modeling datasets, including WikiText-103, One Billion Word, Penn Treebank, and enwiki8. Notably, we improve the SoTA results from 1.06 to 0.99 in bpc on enwiki8, from 33.0 to 18.9 in perplexity on WikiText-103, and from 28.0 to 23.5 in perplexity on One Billion Word. Performance improves when the attention length increases during evaluation, and our best model attends to up to 1,600 words and 3,800 characters. To quantify the effective length of dependency, we devise a new metric and show that on WikiText-103 Transformer-XL manages to model dependency that is about 80% longer than recurrent networks and 450% longer than Transformer. Moreover, Transformer-XL is up to 1,800+ times faster than vanilla Transformer during evaluation.", "keywords": ["Language Modeling", "Self-Attention"], "authorids": ["zander.dai@gmail.com", "zhiliny@cs.cmu.edu", "yiming@cs.cmu.edu", "wcohen@google.com", "jgc@cs.cmu.edu", "qvl@google.com", "rsalakhu@cs.cmu.edu"], "authors": ["Zihang Dai*", "Zhilin Yang*", "Yiming Yang", "William W. Cohen", "Jaime Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov"], "pdf": "/pdf/4ccd6a96dea7974a19a037cd6374beb1b5fbfa2d.pdf", "paperhash": "dai|transformerxl_language_modeling_with_longerterm_dependency", "_bibtex": "@misc{\ndai*2019transformerxl,\ntitle={Transformer-{XL}: Language Modeling with Longer-Term Dependency},\nauthor={Zihang Dai* and Zhilin Yang* and Yiming Yang and William W. Cohen and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},\nyear={2019},\nurl={https://openreview.net/forum?id=HJePno0cYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper717/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606557, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJePno0cYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper717/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper717/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper717/Authors|ICLR.cc/2019/Conference/Paper717/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606557}}}, {"id": "rJx2-lcqk4", "original": null, "number": 8, "cdate": 1544359939710, "ddate": null, "tcdate": 1544359939710, "tmdate": 1544359939710, "tddate": null, "forum": "HJePno0cYm", "replyto": "rkgHT-tq3m", "invitation": "ICLR.cc/2019/Conference/-/Paper717/Official_Comment", "content": {"title": "update", "comment": "Dear reviewer, we believe we have addressed your concerns in the rebuttal (see the General Response above and the comments below). Especially, we have further improved over state-of-the-art results ever since. Do you have an updated assessment or other concerns of our paper? Thank you!"}, "signatures": ["ICLR.cc/2019/Conference/Paper717/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper717/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer-XL: Language Modeling with Longer-Term Dependency", "abstract": "We propose a novel neural architecture, Transformer-XL, for modeling longer-term dependency. To address the limitation of fixed-length contexts, we introduce a notion of recurrence by reusing the representations from the history. Empirically, we show state-of-the-art (SoTA) results on both word-level and character-level language modeling datasets, including WikiText-103, One Billion Word, Penn Treebank, and enwiki8. Notably, we improve the SoTA results from 1.06 to 0.99 in bpc on enwiki8, from 33.0 to 18.9 in perplexity on WikiText-103, and from 28.0 to 23.5 in perplexity on One Billion Word. Performance improves when the attention length increases during evaluation, and our best model attends to up to 1,600 words and 3,800 characters. To quantify the effective length of dependency, we devise a new metric and show that on WikiText-103 Transformer-XL manages to model dependency that is about 80% longer than recurrent networks and 450% longer than Transformer. Moreover, Transformer-XL is up to 1,800+ times faster than vanilla Transformer during evaluation.", "keywords": ["Language Modeling", "Self-Attention"], "authorids": ["zander.dai@gmail.com", "zhiliny@cs.cmu.edu", "yiming@cs.cmu.edu", "wcohen@google.com", "jgc@cs.cmu.edu", "qvl@google.com", "rsalakhu@cs.cmu.edu"], "authors": ["Zihang Dai*", "Zhilin Yang*", "Yiming Yang", "William W. Cohen", "Jaime Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov"], "pdf": "/pdf/4ccd6a96dea7974a19a037cd6374beb1b5fbfa2d.pdf", "paperhash": "dai|transformerxl_language_modeling_with_longerterm_dependency", "_bibtex": "@misc{\ndai*2019transformerxl,\ntitle={Transformer-{XL}: Language Modeling with Longer-Term Dependency},\nauthor={Zihang Dai* and Zhilin Yang* and Yiming Yang and William W. Cohen and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},\nyear={2019},\nurl={https://openreview.net/forum?id=HJePno0cYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper717/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606557, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJePno0cYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper717/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper717/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper717/Authors|ICLR.cc/2019/Conference/Paper717/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606557}}}, {"id": "Syxqex55JN", "original": null, "number": 7, "cdate": 1544359921950, "ddate": null, "tcdate": 1544359921950, "tmdate": 1544359921950, "tddate": null, "forum": "HJePno0cYm", "replyto": "SJgjpKAko7", "invitation": "ICLR.cc/2019/Conference/-/Paper717/Official_Comment", "content": {"title": "update", "comment": "Dear reviewer, we believe we have addressed your concerns in the rebuttal (see the General Response above and the comments below). Especially, we have further improved over state-of-the-art results ever since. Do you have an updated assessment or other concerns of our paper? Thank you!"}, "signatures": ["ICLR.cc/2019/Conference/Paper717/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper717/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer-XL: Language Modeling with Longer-Term Dependency", "abstract": "We propose a novel neural architecture, Transformer-XL, for modeling longer-term dependency. To address the limitation of fixed-length contexts, we introduce a notion of recurrence by reusing the representations from the history. Empirically, we show state-of-the-art (SoTA) results on both word-level and character-level language modeling datasets, including WikiText-103, One Billion Word, Penn Treebank, and enwiki8. Notably, we improve the SoTA results from 1.06 to 0.99 in bpc on enwiki8, from 33.0 to 18.9 in perplexity on WikiText-103, and from 28.0 to 23.5 in perplexity on One Billion Word. Performance improves when the attention length increases during evaluation, and our best model attends to up to 1,600 words and 3,800 characters. To quantify the effective length of dependency, we devise a new metric and show that on WikiText-103 Transformer-XL manages to model dependency that is about 80% longer than recurrent networks and 450% longer than Transformer. Moreover, Transformer-XL is up to 1,800+ times faster than vanilla Transformer during evaluation.", "keywords": ["Language Modeling", "Self-Attention"], "authorids": ["zander.dai@gmail.com", "zhiliny@cs.cmu.edu", "yiming@cs.cmu.edu", "wcohen@google.com", "jgc@cs.cmu.edu", "qvl@google.com", "rsalakhu@cs.cmu.edu"], "authors": ["Zihang Dai*", "Zhilin Yang*", "Yiming Yang", "William W. Cohen", "Jaime Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov"], "pdf": "/pdf/4ccd6a96dea7974a19a037cd6374beb1b5fbfa2d.pdf", "paperhash": "dai|transformerxl_language_modeling_with_longerterm_dependency", "_bibtex": "@misc{\ndai*2019transformerxl,\ntitle={Transformer-{XL}: Language Modeling with Longer-Term Dependency},\nauthor={Zihang Dai* and Zhilin Yang* and Yiming Yang and William W. Cohen and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},\nyear={2019},\nurl={https://openreview.net/forum?id=HJePno0cYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper717/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606557, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJePno0cYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper717/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper717/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper717/Authors|ICLR.cc/2019/Conference/Paper717/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606557}}}, {"id": "Hkla0-dp27", "original": null, "number": 3, "cdate": 1541403092693, "ddate": null, "tcdate": 1541403092693, "tmdate": 1543810135992, "tddate": null, "forum": "HJePno0cYm", "replyto": "HJePno0cYm", "invitation": "ICLR.cc/2019/Conference/-/Paper717/Official_Review", "content": {"title": "This paper proposes a variant of transformer to train language model", "review": "This paper proposes a variant of transformer to train language model, it uses two modifications, one is the segment level recurrence with state reuse, the other is relative positional encoding, which significantly enhances the power to model long range dependency. Extensive experiments in terms of perplexity results are reported, specially on WikiText-103 corpus, significant perplexity reduction has been achieved.\n\nPerplexity is not a gold standard for language model, the authors are encouraged to report experimental results on real world applications such as word rate reduction ASR on BLEU score improvement machine translation.  \n\nCiprian Chelba and Frederick Jelinek, Structured language modeling. Computer Speech and Language (2000) 14, 283\u2013332. \n\nPeng Xu, Frederick Jelinek: Random forests and the data sparseness problem in language modeling. Computer Speech & Language 21(1): 105-152 (2007).", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper717/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Transformer-XL: Language Modeling with Longer-Term Dependency", "abstract": "We propose a novel neural architecture, Transformer-XL, for modeling longer-term dependency. To address the limitation of fixed-length contexts, we introduce a notion of recurrence by reusing the representations from the history. Empirically, we show state-of-the-art (SoTA) results on both word-level and character-level language modeling datasets, including WikiText-103, One Billion Word, Penn Treebank, and enwiki8. Notably, we improve the SoTA results from 1.06 to 0.99 in bpc on enwiki8, from 33.0 to 18.9 in perplexity on WikiText-103, and from 28.0 to 23.5 in perplexity on One Billion Word. Performance improves when the attention length increases during evaluation, and our best model attends to up to 1,600 words and 3,800 characters. To quantify the effective length of dependency, we devise a new metric and show that on WikiText-103 Transformer-XL manages to model dependency that is about 80% longer than recurrent networks and 450% longer than Transformer. Moreover, Transformer-XL is up to 1,800+ times faster than vanilla Transformer during evaluation.", "keywords": ["Language Modeling", "Self-Attention"], "authorids": ["zander.dai@gmail.com", "zhiliny@cs.cmu.edu", "yiming@cs.cmu.edu", "wcohen@google.com", "jgc@cs.cmu.edu", "qvl@google.com", "rsalakhu@cs.cmu.edu"], "authors": ["Zihang Dai*", "Zhilin Yang*", "Yiming Yang", "William W. Cohen", "Jaime Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov"], "pdf": "/pdf/4ccd6a96dea7974a19a037cd6374beb1b5fbfa2d.pdf", "paperhash": "dai|transformerxl_language_modeling_with_longerterm_dependency", "_bibtex": "@misc{\ndai*2019transformerxl,\ntitle={Transformer-{XL}: Language Modeling with Longer-Term Dependency},\nauthor={Zihang Dai* and Zhilin Yang* and Yiming Yang and William W. Cohen and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},\nyear={2019},\nurl={https://openreview.net/forum?id=HJePno0cYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper717/Official_Review", "cdate": 1542234395798, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJePno0cYm", "replyto": "HJePno0cYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper717/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335787499, "tmdate": 1552335787499, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper717/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SyeITxE1JN", "original": null, "number": 6, "cdate": 1543614653730, "ddate": null, "tcdate": 1543614653730, "tmdate": 1543614653730, "tddate": null, "forum": "HJePno0cYm", "replyto": "S1g-IItiRQ", "invitation": "ICLR.cc/2019/Conference/-/Paper717/Official_Comment", "content": {"title": "Response", "comment": "Thanks for the comment. \n\n1. We don\u2019t fully understand the suggestion. One interpretation is to evaluate the RNN and the proposed model on longer sequences than that used in training. In this case, since truncated BPTT is used in the training, one can always pass the last-step hidden state from the previous segment to the next segment as the initial state of the RNN. Hence, the RNN is actually evaluated on the entire text sequence.\n\n2. Thanks for pointing out this related work. We will check it and address the relationship properly in a later version. "}, "signatures": ["ICLR.cc/2019/Conference/Paper717/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper717/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer-XL: Language Modeling with Longer-Term Dependency", "abstract": "We propose a novel neural architecture, Transformer-XL, for modeling longer-term dependency. To address the limitation of fixed-length contexts, we introduce a notion of recurrence by reusing the representations from the history. Empirically, we show state-of-the-art (SoTA) results on both word-level and character-level language modeling datasets, including WikiText-103, One Billion Word, Penn Treebank, and enwiki8. Notably, we improve the SoTA results from 1.06 to 0.99 in bpc on enwiki8, from 33.0 to 18.9 in perplexity on WikiText-103, and from 28.0 to 23.5 in perplexity on One Billion Word. Performance improves when the attention length increases during evaluation, and our best model attends to up to 1,600 words and 3,800 characters. To quantify the effective length of dependency, we devise a new metric and show that on WikiText-103 Transformer-XL manages to model dependency that is about 80% longer than recurrent networks and 450% longer than Transformer. Moreover, Transformer-XL is up to 1,800+ times faster than vanilla Transformer during evaluation.", "keywords": ["Language Modeling", "Self-Attention"], "authorids": ["zander.dai@gmail.com", "zhiliny@cs.cmu.edu", "yiming@cs.cmu.edu", "wcohen@google.com", "jgc@cs.cmu.edu", "qvl@google.com", "rsalakhu@cs.cmu.edu"], "authors": ["Zihang Dai*", "Zhilin Yang*", "Yiming Yang", "William W. Cohen", "Jaime Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov"], "pdf": "/pdf/4ccd6a96dea7974a19a037cd6374beb1b5fbfa2d.pdf", "paperhash": "dai|transformerxl_language_modeling_with_longerterm_dependency", "_bibtex": "@misc{\ndai*2019transformerxl,\ntitle={Transformer-{XL}: Language Modeling with Longer-Term Dependency},\nauthor={Zihang Dai* and Zhilin Yang* and Yiming Yang and William W. Cohen and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},\nyear={2019},\nurl={https://openreview.net/forum?id=HJePno0cYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper717/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606557, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJePno0cYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper717/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper717/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper717/Authors|ICLR.cc/2019/Conference/Paper717/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606557}}}, {"id": "S1g-IItiRQ", "original": null, "number": 1, "cdate": 1543374408694, "ddate": null, "tcdate": 1543374408694, "tmdate": 1543469175459, "tddate": null, "forum": "HJePno0cYm", "replyto": "HJePno0cYm", "invitation": "ICLR.cc/2019/Conference/-/Paper717/Public_Comment", "content": {"comment": "Hello, \n\nI believe this paper is addressing an interesting problem i.e enabling self attention to scale. And I enjoyed reading this paper! And results of this paper are pretty interesting too! :-)\n\nFew points:\n\n1. Generally while evaluating long term dependencies, I'm a bit skeptical about evaluating \"bpc\" or ppl. What has worked well for me in the past is to evaluate on longer sequences then it was trained for. As RNN's are generally trained using one step ahead prediction, so evaluating for longer sequences generally pose a more difficult problem. I personally always use this metric and only mostly use bpc for the sake of submitting papers. So, if authors have pre-trained model, I actually encourage them to use them and report  (same) metric on longer sequences, as compared to the respective baselines. I also think, this might make the paper stronger, and also, it may increase the chances of paper getting accepted. :-) Again, good work.\n\n\n2. I would also like to point, that we had a paper in which we propose to enable self attention to scale, \"Sparse Attentive Backtracking\" (NIPS'18/NeurIPS'18), our motivation was  very different. https://arxiv.org/abs/1809.03702. It would be interesting if the authors can reference/cite this.", "title": "Interesting paper"}, "signatures": ["~Anirudh_Goyal1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Anirudh_Goyal1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer-XL: Language Modeling with Longer-Term Dependency", "abstract": "We propose a novel neural architecture, Transformer-XL, for modeling longer-term dependency. To address the limitation of fixed-length contexts, we introduce a notion of recurrence by reusing the representations from the history. Empirically, we show state-of-the-art (SoTA) results on both word-level and character-level language modeling datasets, including WikiText-103, One Billion Word, Penn Treebank, and enwiki8. Notably, we improve the SoTA results from 1.06 to 0.99 in bpc on enwiki8, from 33.0 to 18.9 in perplexity on WikiText-103, and from 28.0 to 23.5 in perplexity on One Billion Word. Performance improves when the attention length increases during evaluation, and our best model attends to up to 1,600 words and 3,800 characters. To quantify the effective length of dependency, we devise a new metric and show that on WikiText-103 Transformer-XL manages to model dependency that is about 80% longer than recurrent networks and 450% longer than Transformer. Moreover, Transformer-XL is up to 1,800+ times faster than vanilla Transformer during evaluation.", "keywords": ["Language Modeling", "Self-Attention"], "authorids": ["zander.dai@gmail.com", "zhiliny@cs.cmu.edu", "yiming@cs.cmu.edu", "wcohen@google.com", "jgc@cs.cmu.edu", "qvl@google.com", "rsalakhu@cs.cmu.edu"], "authors": ["Zihang Dai*", "Zhilin Yang*", "Yiming Yang", "William W. Cohen", "Jaime Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov"], "pdf": "/pdf/4ccd6a96dea7974a19a037cd6374beb1b5fbfa2d.pdf", "paperhash": "dai|transformerxl_language_modeling_with_longerterm_dependency", "_bibtex": "@misc{\ndai*2019transformerxl,\ntitle={Transformer-{XL}: Language Modeling with Longer-Term Dependency},\nauthor={Zihang Dai* and Zhilin Yang* and Yiming Yang and William W. Cohen and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},\nyear={2019},\nurl={https://openreview.net/forum?id=HJePno0cYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper717/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311768784, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HJePno0cYm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311768784}}}, {"id": "Hylxe2VcA7", "original": null, "number": 5, "cdate": 1543289831662, "ddate": null, "tcdate": 1543289831662, "tmdate": 1543367166175, "tddate": null, "forum": "HJePno0cYm", "replyto": "SJgjpKAko7", "invitation": "ICLR.cc/2019/Conference/-/Paper717/Official_Comment", "content": {"title": "response", "comment": "Thanks for your valuable comments!\n\n[Speed Comparison]\nAs shown in our paper, Transformer is the state-of-the-art model on language modeling, and Al Rfou et al was the previous SoTA of Transformer language models. The main argument of our results on computational time is that Transformer-XL substantially improves the speed while getting even better results. It is less interesting to obtain speedup over a poorly performing model. On the other hand, as our speedup techniques specifically target Transformers, we believe Al Rfou et al is the most appropriate baseline to test the effects of our proposed methods.\n\nPlease see our comments above regarding the significance and novelty of our contributions.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper717/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper717/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer-XL: Language Modeling with Longer-Term Dependency", "abstract": "We propose a novel neural architecture, Transformer-XL, for modeling longer-term dependency. To address the limitation of fixed-length contexts, we introduce a notion of recurrence by reusing the representations from the history. Empirically, we show state-of-the-art (SoTA) results on both word-level and character-level language modeling datasets, including WikiText-103, One Billion Word, Penn Treebank, and enwiki8. Notably, we improve the SoTA results from 1.06 to 0.99 in bpc on enwiki8, from 33.0 to 18.9 in perplexity on WikiText-103, and from 28.0 to 23.5 in perplexity on One Billion Word. Performance improves when the attention length increases during evaluation, and our best model attends to up to 1,600 words and 3,800 characters. To quantify the effective length of dependency, we devise a new metric and show that on WikiText-103 Transformer-XL manages to model dependency that is about 80% longer than recurrent networks and 450% longer than Transformer. Moreover, Transformer-XL is up to 1,800+ times faster than vanilla Transformer during evaluation.", "keywords": ["Language Modeling", "Self-Attention"], "authorids": ["zander.dai@gmail.com", "zhiliny@cs.cmu.edu", "yiming@cs.cmu.edu", "wcohen@google.com", "jgc@cs.cmu.edu", "qvl@google.com", "rsalakhu@cs.cmu.edu"], "authors": ["Zihang Dai*", "Zhilin Yang*", "Yiming Yang", "William W. Cohen", "Jaime Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov"], "pdf": "/pdf/4ccd6a96dea7974a19a037cd6374beb1b5fbfa2d.pdf", "paperhash": "dai|transformerxl_language_modeling_with_longerterm_dependency", "_bibtex": "@misc{\ndai*2019transformerxl,\ntitle={Transformer-{XL}: Language Modeling with Longer-Term Dependency},\nauthor={Zihang Dai* and Zhilin Yang* and Yiming Yang and William W. Cohen and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},\nyear={2019},\nurl={https://openreview.net/forum?id=HJePno0cYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper717/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606557, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJePno0cYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper717/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper717/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper717/Authors|ICLR.cc/2019/Conference/Paper717/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606557}}}, {"id": "HkxXKjV5RX", "original": null, "number": 2, "cdate": 1543289723019, "ddate": null, "tcdate": 1543289723019, "tmdate": 1543290035725, "tddate": null, "forum": "HJePno0cYm", "replyto": "HJePno0cYm", "invitation": "ICLR.cc/2019/Conference/-/Paper717/Official_Comment", "content": {"title": "General response to the reviewers and AC", "comment": "\n[Latest results and significance]\nWe have experimented with increasing the model sizes so as to match the previous work for fair comparison. As a result, we have advanced the SoTA performance from 1.06 to 0.99 in bpc on enwiki8, from 33.0 to 18.9 in perplexity on WT103, and from 28.0 to 23.5 in perplexity on One Billion Word. Note that our result on enwiki8 is the first result below 1.0 bpc on widely-studied char-level LM benchmarks. We believe the improvement is significant compared to any previous results and we also believe this substantiates the significance of the proposed methods. Changes have been made accordingly in the paper.\n\n\n[Why language modeling]\nAlthough we believe our technique will be useful where long-term dependency is involved, with applications like paragraph-level machine translation, summarization, multi-paragraph question answering, text generation, etc, we would also like to emphasize that language modeling itself is important.\n--- Firstly, language modeling has been an independent research direction in natural language processing (NLP) for decades [1-5]. Even when we restrict our attention to neural language models in the last two years, there has been a significant amount of work focused solely on this topic [6-18] in venues like ICLR, ICML, and NeurIPS.\n--- Secondly, language modeling is an important unsupervised pretraining objective. The biggest advance in NLP recently originated from training large-scale language models for unsupervised feature learning [19,20].\n\n[1] Chen, S. F., & Goodman, J. (1996). An empirical study of smoothing techniques for language modeling\n[2] Manning, C. D., & Sch\u00fctze, H. (1999). Foundations of statistical natural language processing\n[3] Bengio, Y., et. al. (2003). A neural probabilistic language model\n[4] Mikolov, T. et al.  (2010) Recurrent neural network based language model\n[5] Zaremba, W., et. al. (2014). Recurrent neural network regularization\n[6] Jozefowicz, R., et. al. (2016). Exploring the limits of language modeling\n[7] Grave, E., et. al. (2016). Improving neural language models with a continuous cache\n[8] Press, O., & Wolf, L. (2016). Using the output embedding to improve language models\n[9] Krause, B., et. al. (2016). Multiplicative LSTM for sequence modeling\n[10] Merity, S., et. al. (2016). Pointer sentinel mixture models\n[11] Dauphin, Y. N., et. al. (2017). Language modeling with gated convolutional networks\n[12] Merity, S., et. al. (2017). Regularizing and optimizing LSTM language models\n[13] Melis, G., et. al. (2017). On the state of the art of evaluation in neural language models.\n[14] Yang, Z., et. al. (2017). Breaking the softmax bottleneck\n[15] Merity, S., et. al. (2018). An Analysis of Neural Language Modeling at Multiple Scales\n[16] Rae, J. W., et. al. (2018). Fast Parametric Learning with Activation Memorization\n[17] Kanai, S., et. al. (2018). Sigsoftmax: Reanalysis of the Softmax Bottleneck\n[18] Al-Rfou, R., et. al. (2018). Character-level language modeling with deeper self-attention.\n[19] Peters, M., et. al. (2017) Deep contextualized word representations\n[20] Devlin, J., et. al. (2018) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n\n\n[Our contributions and novelty]\nWe believe Transformer-XL addresses an important problem. The key question we answer in this work is how to enable self-attention, an architecture which has a potential optimization advantage in learning long-term dependency, to really capture a longer context beyond a fixed length.\n\nOur main contribution is to propose a complete set of techniques that jointly enable recurrency in self-attention, rather than a set of unrelated, individual techniques.\n--- As described in Section 3 and shown in Table 5, state reuse is not even possible without relative positional encodings, because the absolute positions in the current segment are not the same as in the next segment.\n---Similarly, relative positional encodings alone do not improve the ability to model long-term dependency.\n\nAlthough our positional encodings share somewhat similar formulation to previous work such as Shaw et al, the motivation is completely different, and it is non-trivial to figure out such a combination of techniques for modeling long-term dependency with the self-attention architecture.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper717/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper717/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer-XL: Language Modeling with Longer-Term Dependency", "abstract": "We propose a novel neural architecture, Transformer-XL, for modeling longer-term dependency. To address the limitation of fixed-length contexts, we introduce a notion of recurrence by reusing the representations from the history. Empirically, we show state-of-the-art (SoTA) results on both word-level and character-level language modeling datasets, including WikiText-103, One Billion Word, Penn Treebank, and enwiki8. Notably, we improve the SoTA results from 1.06 to 0.99 in bpc on enwiki8, from 33.0 to 18.9 in perplexity on WikiText-103, and from 28.0 to 23.5 in perplexity on One Billion Word. Performance improves when the attention length increases during evaluation, and our best model attends to up to 1,600 words and 3,800 characters. To quantify the effective length of dependency, we devise a new metric and show that on WikiText-103 Transformer-XL manages to model dependency that is about 80% longer than recurrent networks and 450% longer than Transformer. Moreover, Transformer-XL is up to 1,800+ times faster than vanilla Transformer during evaluation.", "keywords": ["Language Modeling", "Self-Attention"], "authorids": ["zander.dai@gmail.com", "zhiliny@cs.cmu.edu", "yiming@cs.cmu.edu", "wcohen@google.com", "jgc@cs.cmu.edu", "qvl@google.com", "rsalakhu@cs.cmu.edu"], "authors": ["Zihang Dai*", "Zhilin Yang*", "Yiming Yang", "William W. Cohen", "Jaime Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov"], "pdf": "/pdf/4ccd6a96dea7974a19a037cd6374beb1b5fbfa2d.pdf", "paperhash": "dai|transformerxl_language_modeling_with_longerterm_dependency", "_bibtex": "@misc{\ndai*2019transformerxl,\ntitle={Transformer-{XL}: Language Modeling with Longer-Term Dependency},\nauthor={Zihang Dai* and Zhilin Yang* and Yiming Yang and William W. Cohen and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},\nyear={2019},\nurl={https://openreview.net/forum?id=HJePno0cYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper717/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606557, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJePno0cYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper717/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper717/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper717/Authors|ICLR.cc/2019/Conference/Paper717/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606557}}}, {"id": "SyeGCoV5RX", "original": null, "number": 4, "cdate": 1543289802091, "ddate": null, "tcdate": 1543289802091, "tmdate": 1543289802091, "tddate": null, "forum": "HJePno0cYm", "replyto": "rkgHT-tq3m", "invitation": "ICLR.cc/2019/Conference/-/Paper717/Official_Comment", "content": {"title": "response", "comment": "Thanks for your valuable comments!\n\n[WT2]\nWT2 shares the same test set as WT103, and the only difference is that WT103 has more training data. Since language modeling has almost unlimited training data in nature, we believe it brings less benefit to compare models on more small-scale datasets as we already have results on Penn Treebank which is also a small dataset.\n\n[Test-time evaluation techniques]\nIn Table 1, we show that our method without any test-time evaluation techniques is still 21+ points better than Grave et al which employs test-time continuous cache on WT103. On enwiki8, mLSTM + dynamic eval [1] achieves a BPC of 1.08, which is still 0.09 worse than Transformer-XL without dynamic evaluation. On One Billion Word, the best previous result did not use test-time evaluation techniques. The only exception is Penn Treebank, where we exclude results with test-time techniques to focus on comparing different architectures. This is fair comparison because all considered models do not use test-time techniques. Moreover, according to previous results, test-time evaluation techniques bring consistent improvement to different architectures (Yang et al 2017, Merity et al 2017).\n\nPlease see our comments above regarding the importance of language modeling on its own.\n\n[1] Ben Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals. Dynamic evaluation of\nneural sequence models.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper717/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper717/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer-XL: Language Modeling with Longer-Term Dependency", "abstract": "We propose a novel neural architecture, Transformer-XL, for modeling longer-term dependency. To address the limitation of fixed-length contexts, we introduce a notion of recurrence by reusing the representations from the history. Empirically, we show state-of-the-art (SoTA) results on both word-level and character-level language modeling datasets, including WikiText-103, One Billion Word, Penn Treebank, and enwiki8. Notably, we improve the SoTA results from 1.06 to 0.99 in bpc on enwiki8, from 33.0 to 18.9 in perplexity on WikiText-103, and from 28.0 to 23.5 in perplexity on One Billion Word. Performance improves when the attention length increases during evaluation, and our best model attends to up to 1,600 words and 3,800 characters. To quantify the effective length of dependency, we devise a new metric and show that on WikiText-103 Transformer-XL manages to model dependency that is about 80% longer than recurrent networks and 450% longer than Transformer. Moreover, Transformer-XL is up to 1,800+ times faster than vanilla Transformer during evaluation.", "keywords": ["Language Modeling", "Self-Attention"], "authorids": ["zander.dai@gmail.com", "zhiliny@cs.cmu.edu", "yiming@cs.cmu.edu", "wcohen@google.com", "jgc@cs.cmu.edu", "qvl@google.com", "rsalakhu@cs.cmu.edu"], "authors": ["Zihang Dai*", "Zhilin Yang*", "Yiming Yang", "William W. Cohen", "Jaime Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov"], "pdf": "/pdf/4ccd6a96dea7974a19a037cd6374beb1b5fbfa2d.pdf", "paperhash": "dai|transformerxl_language_modeling_with_longerterm_dependency", "_bibtex": "@misc{\ndai*2019transformerxl,\ntitle={Transformer-{XL}: Language Modeling with Longer-Term Dependency},\nauthor={Zihang Dai* and Zhilin Yang* and Yiming Yang and William W. Cohen and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},\nyear={2019},\nurl={https://openreview.net/forum?id=HJePno0cYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper717/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606557, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJePno0cYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper717/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper717/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper717/Authors|ICLR.cc/2019/Conference/Paper717/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606557}}}, {"id": "rJeQ2jV50X", "original": null, "number": 3, "cdate": 1543289771051, "ddate": null, "tcdate": 1543289771051, "tmdate": 1543289771051, "tddate": null, "forum": "HJePno0cYm", "replyto": "Hkla0-dp27", "invitation": "ICLR.cc/2019/Conference/-/Paper717/Official_Comment", "content": {"title": "response", "comment": "Thanks for your valuable comments!\n\nAs far as we know, almost all language models were evaluated by perplexity in previous work.\n\nPlease see our comments above regarding the importance of language modeling on its own.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper717/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper717/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer-XL: Language Modeling with Longer-Term Dependency", "abstract": "We propose a novel neural architecture, Transformer-XL, for modeling longer-term dependency. To address the limitation of fixed-length contexts, we introduce a notion of recurrence by reusing the representations from the history. Empirically, we show state-of-the-art (SoTA) results on both word-level and character-level language modeling datasets, including WikiText-103, One Billion Word, Penn Treebank, and enwiki8. Notably, we improve the SoTA results from 1.06 to 0.99 in bpc on enwiki8, from 33.0 to 18.9 in perplexity on WikiText-103, and from 28.0 to 23.5 in perplexity on One Billion Word. Performance improves when the attention length increases during evaluation, and our best model attends to up to 1,600 words and 3,800 characters. To quantify the effective length of dependency, we devise a new metric and show that on WikiText-103 Transformer-XL manages to model dependency that is about 80% longer than recurrent networks and 450% longer than Transformer. Moreover, Transformer-XL is up to 1,800+ times faster than vanilla Transformer during evaluation.", "keywords": ["Language Modeling", "Self-Attention"], "authorids": ["zander.dai@gmail.com", "zhiliny@cs.cmu.edu", "yiming@cs.cmu.edu", "wcohen@google.com", "jgc@cs.cmu.edu", "qvl@google.com", "rsalakhu@cs.cmu.edu"], "authors": ["Zihang Dai*", "Zhilin Yang*", "Yiming Yang", "William W. Cohen", "Jaime Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov"], "pdf": "/pdf/4ccd6a96dea7974a19a037cd6374beb1b5fbfa2d.pdf", "paperhash": "dai|transformerxl_language_modeling_with_longerterm_dependency", "_bibtex": "@misc{\ndai*2019transformerxl,\ntitle={Transformer-{XL}: Language Modeling with Longer-Term Dependency},\nauthor={Zihang Dai* and Zhilin Yang* and Yiming Yang and William W. Cohen and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},\nyear={2019},\nurl={https://openreview.net/forum?id=HJePno0cYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper717/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606557, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJePno0cYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper717/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper717/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper717/Authors|ICLR.cc/2019/Conference/Paper717/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper717/Reviewers", "ICLR.cc/2019/Conference/Paper717/Authors", "ICLR.cc/2019/Conference/Paper717/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606557}}}, {"id": "rkgHT-tq3m", "original": null, "number": 2, "cdate": 1541210556809, "ddate": null, "tcdate": 1541210556809, "tmdate": 1541533746520, "tddate": null, "forum": "HJePno0cYm", "replyto": "HJePno0cYm", "invitation": "ICLR.cc/2019/Conference/-/Paper717/Official_Review", "content": {"title": "Using Transformer as a RNN cell applied to equal-length segments, good experimental results, but need to cover standard benchmarks and use SOTA decoding techniques for comparison.", "review": "This paper proposes a Transformer based RNN structure \"Transformer-XL\" to capture long-range contextual relations and targets on language model task. The idea is straightforward: it splits the input sequence into equal and fixed length segments, and recurrently apply the Transformer over the sequence of segments, in which the hidden states for the previous segment are treated as a memory to attend for the next segment. \n\nThis paper is well-organized and well-written, and easy to follow. The empirical results also demonstrate the proposed model can achieve SoTA performance on several word- and character-based language model benchmarks. \n\n\nPros:\n\n1. The model is designed based on a careful engineering: 1) taking into account the history hidden states for long-term dependency modeling and 2) alignment scores calculated from multiple perspectives for relative position modeling and global significance capturing. In addition, in contrast to the previous Transformer-based language model, benefiting from the recurrent architecture, both training and decoding can be accelerated.\n2. The experimental results show that the proposed Transformer-XL can surpass the baseline model and achieve new state-of-the-art perplexity or bpc on word- or char-based language model task. And, based on the proposed new metric, RECL, the analysis for context length modeling verifies the proposed model can make the best of long-range dependencies.\n\n\nCons:\n\n1. The proposed model is ad-hoc and is only compatible with language model task. Is it possible to extend the proposed model to more general and practical tasks (e.g., seq2seq tasks)?\n2. The absence of a popular language model benchmark, WikiText-2, which has been evaluated in most previous papers.\n3. It is notable that there are no ubiquitous decoding techniques for the language used in both the proposed model and baselines, such as dynamical evaluation and continuous cache pointer. However, these techniques are essential for the RNN-LM baselines to achieve state-of-the-art performance, and has been standardly used in most previous works. Therefore, the comparison seems unfair. \n\nMinor comments: In Figure 1 and 2, it is better to include a legend explaining the meaning of different colors for different nodes.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper717/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer-XL: Language Modeling with Longer-Term Dependency", "abstract": "We propose a novel neural architecture, Transformer-XL, for modeling longer-term dependency. To address the limitation of fixed-length contexts, we introduce a notion of recurrence by reusing the representations from the history. Empirically, we show state-of-the-art (SoTA) results on both word-level and character-level language modeling datasets, including WikiText-103, One Billion Word, Penn Treebank, and enwiki8. Notably, we improve the SoTA results from 1.06 to 0.99 in bpc on enwiki8, from 33.0 to 18.9 in perplexity on WikiText-103, and from 28.0 to 23.5 in perplexity on One Billion Word. Performance improves when the attention length increases during evaluation, and our best model attends to up to 1,600 words and 3,800 characters. To quantify the effective length of dependency, we devise a new metric and show that on WikiText-103 Transformer-XL manages to model dependency that is about 80% longer than recurrent networks and 450% longer than Transformer. Moreover, Transformer-XL is up to 1,800+ times faster than vanilla Transformer during evaluation.", "keywords": ["Language Modeling", "Self-Attention"], "authorids": ["zander.dai@gmail.com", "zhiliny@cs.cmu.edu", "yiming@cs.cmu.edu", "wcohen@google.com", "jgc@cs.cmu.edu", "qvl@google.com", "rsalakhu@cs.cmu.edu"], "authors": ["Zihang Dai*", "Zhilin Yang*", "Yiming Yang", "William W. Cohen", "Jaime Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov"], "pdf": "/pdf/4ccd6a96dea7974a19a037cd6374beb1b5fbfa2d.pdf", "paperhash": "dai|transformerxl_language_modeling_with_longerterm_dependency", "_bibtex": "@misc{\ndai*2019transformerxl,\ntitle={Transformer-{XL}: Language Modeling with Longer-Term Dependency},\nauthor={Zihang Dai* and Zhilin Yang* and Yiming Yang and William W. Cohen and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},\nyear={2019},\nurl={https://openreview.net/forum?id=HJePno0cYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper717/Official_Review", "cdate": 1542234395798, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJePno0cYm", "replyto": "HJePno0cYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper717/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335787499, "tmdate": 1552335787499, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper717/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJgjpKAko7", "original": null, "number": 1, "cdate": 1539463618595, "ddate": null, "tcdate": 1539463618595, "tmdate": 1541533746305, "tddate": null, "forum": "HJePno0cYm", "replyto": "HJePno0cYm", "invitation": "ICLR.cc/2019/Conference/-/Paper717/Official_Review", "content": {"title": "Marginal innovation", "review": "This paper puts forward a new schema for language modeling, especially for relationship between two parts far apart.\n\nThe experimental results on WikiText-103 are good, improving the STOA PPL by 9.0. On the other three datasets, however, there's little or no gain. The speed comparison should be carried out over more LM models, as Al-Rfou is not the fastest.\n\nThe writing is not very clear, especially around equations.\n\nOverall the contribution of this paper is marginally incremental:\n1. The major proposed idea is just to add one no-grad previous segment into the prediction for next segment. This is similar to Residual network idea but more simplified.\n2. Using relative positional encoding is not a new idea, e.g. https://arxiv.org/pdf/1803.02155.pdf.\n3. Reusing previous level/segment computation with gradient fixed is also not a big innovation.\n\ntypo:\n1. end of page 3, and \"W.\" denotes\".\n2. The speed experiment should be put in the main text.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper717/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer-XL: Language Modeling with Longer-Term Dependency", "abstract": "We propose a novel neural architecture, Transformer-XL, for modeling longer-term dependency. To address the limitation of fixed-length contexts, we introduce a notion of recurrence by reusing the representations from the history. Empirically, we show state-of-the-art (SoTA) results on both word-level and character-level language modeling datasets, including WikiText-103, One Billion Word, Penn Treebank, and enwiki8. Notably, we improve the SoTA results from 1.06 to 0.99 in bpc on enwiki8, from 33.0 to 18.9 in perplexity on WikiText-103, and from 28.0 to 23.5 in perplexity on One Billion Word. Performance improves when the attention length increases during evaluation, and our best model attends to up to 1,600 words and 3,800 characters. To quantify the effective length of dependency, we devise a new metric and show that on WikiText-103 Transformer-XL manages to model dependency that is about 80% longer than recurrent networks and 450% longer than Transformer. Moreover, Transformer-XL is up to 1,800+ times faster than vanilla Transformer during evaluation.", "keywords": ["Language Modeling", "Self-Attention"], "authorids": ["zander.dai@gmail.com", "zhiliny@cs.cmu.edu", "yiming@cs.cmu.edu", "wcohen@google.com", "jgc@cs.cmu.edu", "qvl@google.com", "rsalakhu@cs.cmu.edu"], "authors": ["Zihang Dai*", "Zhilin Yang*", "Yiming Yang", "William W. Cohen", "Jaime Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov"], "pdf": "/pdf/4ccd6a96dea7974a19a037cd6374beb1b5fbfa2d.pdf", "paperhash": "dai|transformerxl_language_modeling_with_longerterm_dependency", "_bibtex": "@misc{\ndai*2019transformerxl,\ntitle={Transformer-{XL}: Language Modeling with Longer-Term Dependency},\nauthor={Zihang Dai* and Zhilin Yang* and Yiming Yang and William W. Cohen and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},\nyear={2019},\nurl={https://openreview.net/forum?id=HJePno0cYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper717/Official_Review", "cdate": 1542234395798, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJePno0cYm", "replyto": "HJePno0cYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper717/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335787499, "tmdate": 1552335787499, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper717/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 20}