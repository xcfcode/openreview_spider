{"notes": [{"id": "HJeYalBKvr", "original": "HJgNIZbYPH", "number": 2581, "cdate": 1569439936895, "ddate": null, "tcdate": 1569439936895, "tmdate": 1577168212547, "tddate": null, "forum": "HJeYalBKvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["cui.wanyun@sufe.edu.cn"], "title": "Attention over Phrases", "authors": ["Wanyun Cui"], "pdf": "/pdf/e17d21e6827ee934813c066a345f6f2f059ae0ea.pdf", "TL;DR": "We study the problem of representing phrases as atoms in attention.", "abstract": "How to represent the sentence ``That's the last straw for her''? The answer of the self-attention is a weighted sum of each individual words, i.e. $$semantics=\\alpha_1Emb(\\text{That})+\\alpha_2Emb(\\text{'s})+\\cdots+\\alpha_nEmb(\\text{her})$$. But the weighted sum of ``That's'', ``the'', ``last'', ``straw'' can hardly represent the semantics of the phrase. We argue that the phrases play an important role in attention.\nIf we combine some words into phrases, a more reasonable representation with compositions is \n$$semantics=\\alpha_1Emb(\\text{That's})+Emb_2(\\text{the last straw})+\\alpha_3Emb(\\text{for})+\\alpha_4Emb(\\text{her})$$.\nWhile recent studies prefer to use the attention mechanism to represent the natural language, few noticed the word compositions. In this paper, we study the problem of representing such compositional attentions in phrases. In this paper, we proposed a new attention architecture called HyperTransformer. Besides representing the words of the sentence, we introduce hypernodes to represent the candidate phrases in attention. \nHyperTransformer has two phases. The first phase is used to attend over all word/phrase pairs, which is similar to the standard Transformer. The second phase is used to represent the inductive bias within each phrase. Specially, we incorporate the non-linear attention in the second phase. The non-linearity represents the the semantic mutations in phrases. The experimental performance has been greatly improved. In WMT16 English-German translation task, the BLEU increases from 20.90 (by Transformer) to 34.61 (by HyperTransformer).", "keywords": ["representation learning", "natural language processing", "attention"], "paperhash": "cui|attention_over_phrases", "original_pdf": "/attachment/e17d21e6827ee934813c066a345f6f2f059ae0ea.pdf", "_bibtex": "@misc{\ncui2020attention,\ntitle={Attention over Phrases},\nauthor={Wanyun Cui},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeYalBKvr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "aR5hnzfg8o", "original": null, "number": 1, "cdate": 1576798752618, "ddate": null, "tcdate": 1576798752618, "tmdate": 1576800882964, "tddate": null, "forum": "HJeYalBKvr", "replyto": "HJeYalBKvr", "invitation": "ICLR.cc/2020/Conference/Paper2581/-/Decision", "content": {"decision": "Reject", "comment": "This paper incorporates phrases within the transformer architecture.\n\nThe underlying idea is interesting, but the reviewers have raised serious concerns with both clarity and the trustworthiness of the experimental evaluation, and thus I cannot recommend acceptance at this time.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["cui.wanyun@sufe.edu.cn"], "title": "Attention over Phrases", "authors": ["Wanyun Cui"], "pdf": "/pdf/e17d21e6827ee934813c066a345f6f2f059ae0ea.pdf", "TL;DR": "We study the problem of representing phrases as atoms in attention.", "abstract": "How to represent the sentence ``That's the last straw for her''? The answer of the self-attention is a weighted sum of each individual words, i.e. $$semantics=\\alpha_1Emb(\\text{That})+\\alpha_2Emb(\\text{'s})+\\cdots+\\alpha_nEmb(\\text{her})$$. But the weighted sum of ``That's'', ``the'', ``last'', ``straw'' can hardly represent the semantics of the phrase. We argue that the phrases play an important role in attention.\nIf we combine some words into phrases, a more reasonable representation with compositions is \n$$semantics=\\alpha_1Emb(\\text{That's})+Emb_2(\\text{the last straw})+\\alpha_3Emb(\\text{for})+\\alpha_4Emb(\\text{her})$$.\nWhile recent studies prefer to use the attention mechanism to represent the natural language, few noticed the word compositions. In this paper, we study the problem of representing such compositional attentions in phrases. In this paper, we proposed a new attention architecture called HyperTransformer. Besides representing the words of the sentence, we introduce hypernodes to represent the candidate phrases in attention. \nHyperTransformer has two phases. The first phase is used to attend over all word/phrase pairs, which is similar to the standard Transformer. The second phase is used to represent the inductive bias within each phrase. Specially, we incorporate the non-linear attention in the second phase. The non-linearity represents the the semantic mutations in phrases. The experimental performance has been greatly improved. In WMT16 English-German translation task, the BLEU increases from 20.90 (by Transformer) to 34.61 (by HyperTransformer).", "keywords": ["representation learning", "natural language processing", "attention"], "paperhash": "cui|attention_over_phrases", "original_pdf": "/attachment/e17d21e6827ee934813c066a345f6f2f059ae0ea.pdf", "_bibtex": "@misc{\ncui2020attention,\ntitle={Attention over Phrases},\nauthor={Wanyun Cui},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeYalBKvr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJeYalBKvr", "replyto": "HJeYalBKvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795724462, "tmdate": 1576800276115, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2581/-/Decision"}}}, {"id": "rJxkd5v0Fr", "original": null, "number": 1, "cdate": 1571875431000, "ddate": null, "tcdate": 1571875431000, "tmdate": 1572972319552, "tddate": null, "forum": "HJeYalBKvr", "replyto": "HJeYalBKvr", "invitation": "ICLR.cc/2020/Conference/Paper2581/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper addresses an issue of compositionality in self-attention models such Transformer. A simple idea of composing multiple words into a phrase as a hypernode and representing it using a non-linear function to capture the semantic mutation is proposed. In the machine translation and PoS tagging tasks, the proposed PhraseTransformer achieves impressive gain, especially +13.7 BLEU score compared to the Transformer.\n\nThe motivation of the paper is very clear, and I love this kind of paper; with a simple idea, making a huge impact on the field. I appreciate the real example to compare how word-level self-attention is different from the phrase-level self-attention in Abstract and Figure 1. The problem itself; tackling the semantic compositionality of self-attention, is a very important problem, and I like the part that authors described it as an inductive bias as a model perspective. \n\nHowever, this work seems to be problematic in terms of presentation, clarity, and meaningful comparisons. Please see my detailed comments below.\n\nFirst, what exactly is \u201csemantic mutation\u201d? The term has been used here and there to describe the inductive bias in semantic compositionality and to show how the nonlinearity can effectively capture it. But, I couldn\u2019t find any definition from the paper, couldn\u2019t find any formal definition from any ACL papers, and couldn\u2019t guess myself based on the context. I am guessing it is probably some sort of combination of meaning in phrase-level words. If so, more importantly, how does the simple non-linear function (i.e., sigmoid) can capture such semantic combinations of words? How could it make such a huge gain (PhraseTransformer vs Linear PhraseTransformer) in Table 1 on MT task? This seems to be the most important contribution of this paper, of which I don\u2019t understand yet. \n\n\nWhat is the \u201cconcept of hypernodes in hypergraph\u201d? I think it is not a common word that I can understand it clearly without any references or any background. It would be better to add some references for the concept. Again, I am guessing it is a sort of graph theory that decomposes a large node into small pieces but keeps their connectivity. But, then how is that exactly linked to phrases of words? If you make phrases only on consecutive words, it is basically just chunking. I don\u2019t find any relevance of phrases in a sentence with the (hyper)graph something. \n\nIn Figure 2, how is the bidirectional path made between the word representations and phrase representations? In my understanding of your algorithm based on Equation 2-6, I only see the attention of phrases is computed by word attention, but not the other way.  Please clarify this. If so, how does the gradient back-propagate to each other?\n\nThe biggest concern of this work is the scores reported in Table 1. I have checked the recent papers which used the Mutli30K(de-en) and other results from WMT[16-18] reports, but the BLEU score reported in Table 1 (20.90) seems way lower than the scores reported by any systems trained by either non-Transformer or Transformer systems. For that reason, it would be fair to include some results from the state-of-the-art systems on the same dataset. \n\nA minor point but in the complexity analysis, your m is basically n because you take consecutive words from n length of sentence. You better distinguish which variables are dependent on each other first. \n\n\nThere are MANY typos, missing captions, grammatical errors in the paper. Here are only some of them:\nThere is no caption for Figure 1. \nWhen you cite a reference with its model name, you better make the citation under parenthesis. \n\u201cequation equation 5\u201d -? \u201cequation 5\u201d\n\u201cWee\u201d -> \u201cWe\u201d\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2581/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2581/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["cui.wanyun@sufe.edu.cn"], "title": "Attention over Phrases", "authors": ["Wanyun Cui"], "pdf": "/pdf/e17d21e6827ee934813c066a345f6f2f059ae0ea.pdf", "TL;DR": "We study the problem of representing phrases as atoms in attention.", "abstract": "How to represent the sentence ``That's the last straw for her''? The answer of the self-attention is a weighted sum of each individual words, i.e. $$semantics=\\alpha_1Emb(\\text{That})+\\alpha_2Emb(\\text{'s})+\\cdots+\\alpha_nEmb(\\text{her})$$. But the weighted sum of ``That's'', ``the'', ``last'', ``straw'' can hardly represent the semantics of the phrase. We argue that the phrases play an important role in attention.\nIf we combine some words into phrases, a more reasonable representation with compositions is \n$$semantics=\\alpha_1Emb(\\text{That's})+Emb_2(\\text{the last straw})+\\alpha_3Emb(\\text{for})+\\alpha_4Emb(\\text{her})$$.\nWhile recent studies prefer to use the attention mechanism to represent the natural language, few noticed the word compositions. In this paper, we study the problem of representing such compositional attentions in phrases. In this paper, we proposed a new attention architecture called HyperTransformer. Besides representing the words of the sentence, we introduce hypernodes to represent the candidate phrases in attention. \nHyperTransformer has two phases. The first phase is used to attend over all word/phrase pairs, which is similar to the standard Transformer. The second phase is used to represent the inductive bias within each phrase. Specially, we incorporate the non-linear attention in the second phase. The non-linearity represents the the semantic mutations in phrases. The experimental performance has been greatly improved. In WMT16 English-German translation task, the BLEU increases from 20.90 (by Transformer) to 34.61 (by HyperTransformer).", "keywords": ["representation learning", "natural language processing", "attention"], "paperhash": "cui|attention_over_phrases", "original_pdf": "/attachment/e17d21e6827ee934813c066a345f6f2f059ae0ea.pdf", "_bibtex": "@misc{\ncui2020attention,\ntitle={Attention over Phrases},\nauthor={Wanyun Cui},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeYalBKvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJeYalBKvr", "replyto": "HJeYalBKvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2581/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2581/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2581/Reviewers"], "noninvitees": [], "tcdate": 1570237720797, "tmdate": 1574723093423, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2581/-/Official_Review"}}}, {"id": "BkxidjcRtB", "original": null, "number": 2, "cdate": 1571887987303, "ddate": null, "tcdate": 1571887987303, "tmdate": 1572972319509, "tddate": null, "forum": "HJeYalBKvr", "replyto": "HJeYalBKvr", "invitation": "ICLR.cc/2020/Conference/Paper2581/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This submission proposes to consider to put attention on \"phrases\" in NLP. The phrases are generated by taking consecutive words in sentences. Each phrase is treated as a \"node\" in the same way as words. Then representations of phrases are learned in the network. The algorithm is applied to two applications, translation and pos tagging. The proposed method achieved better performance than transformer. \n\nCritics: \n\n1. In the abstract and the introduction, the submission argues that usefulness of phrases, which are sementic units represented by word groups. However, in the model development, \"phrases\" are really bigrams and trigrams. I don't know how much the previous argument is still valid. Particularly, there are so many bigrams and trigrams. The effect from these word combinations should have strong effect on the model, but the effect may not be explained as the argument. \n\n2. I think transformer can somewhat capture word combinations in bigrams and trigrams. In higher layers, transformer actually combine words in representations. What is the advantage of the proposed method over the type of combination done in transformer? \n\n3. The experiment only compares to transformer in the translation task. It only compares to transformer and semantic phrase transformer. Other SOTA methods (e.g. different versions of transformers) should be compared. \n\n4. The comparison is not really fair. In each \"layer\" of the proposed phrase transformer, it has actually two self-attention layers, but the baseline has only one self-attention layer. \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2581/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2581/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["cui.wanyun@sufe.edu.cn"], "title": "Attention over Phrases", "authors": ["Wanyun Cui"], "pdf": "/pdf/e17d21e6827ee934813c066a345f6f2f059ae0ea.pdf", "TL;DR": "We study the problem of representing phrases as atoms in attention.", "abstract": "How to represent the sentence ``That's the last straw for her''? The answer of the self-attention is a weighted sum of each individual words, i.e. $$semantics=\\alpha_1Emb(\\text{That})+\\alpha_2Emb(\\text{'s})+\\cdots+\\alpha_nEmb(\\text{her})$$. But the weighted sum of ``That's'', ``the'', ``last'', ``straw'' can hardly represent the semantics of the phrase. We argue that the phrases play an important role in attention.\nIf we combine some words into phrases, a more reasonable representation with compositions is \n$$semantics=\\alpha_1Emb(\\text{That's})+Emb_2(\\text{the last straw})+\\alpha_3Emb(\\text{for})+\\alpha_4Emb(\\text{her})$$.\nWhile recent studies prefer to use the attention mechanism to represent the natural language, few noticed the word compositions. In this paper, we study the problem of representing such compositional attentions in phrases. In this paper, we proposed a new attention architecture called HyperTransformer. Besides representing the words of the sentence, we introduce hypernodes to represent the candidate phrases in attention. \nHyperTransformer has two phases. The first phase is used to attend over all word/phrase pairs, which is similar to the standard Transformer. The second phase is used to represent the inductive bias within each phrase. Specially, we incorporate the non-linear attention in the second phase. The non-linearity represents the the semantic mutations in phrases. The experimental performance has been greatly improved. In WMT16 English-German translation task, the BLEU increases from 20.90 (by Transformer) to 34.61 (by HyperTransformer).", "keywords": ["representation learning", "natural language processing", "attention"], "paperhash": "cui|attention_over_phrases", "original_pdf": "/attachment/e17d21e6827ee934813c066a345f6f2f059ae0ea.pdf", "_bibtex": "@misc{\ncui2020attention,\ntitle={Attention over Phrases},\nauthor={Wanyun Cui},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeYalBKvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJeYalBKvr", "replyto": "HJeYalBKvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2581/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2581/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2581/Reviewers"], "noninvitees": [], "tcdate": 1570237720797, "tmdate": 1574723093423, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2581/-/Official_Review"}}}, {"id": "ryg1sde8qB", "original": null, "number": 3, "cdate": 1572370582738, "ddate": null, "tcdate": 1572370582738, "tmdate": 1572972319461, "tddate": null, "forum": "HJeYalBKvr", "replyto": "HJeYalBKvr", "invitation": "ICLR.cc/2020/Conference/Paper2581/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "I think the paper needs a deep review in the English part. For example, in the abstract, they repeat \"In this paper\" a couple of time and it is complicated to understand the introduction and methodology. Also, I think the paper needs a better structure. The related work should be first in order to understand the relevance of this paper. \n\nFrom the experiment point of view, it is necessary a better explanation about the hyperparameters or the experiments which were carried out. In addition, the single database was used to evaluate the technology which is not enough to show the big different respect to the transformed paper. Also, the comparison is not really fair. In each \"layer\" of the proposed phrase transformer, it has actually two self-attention layers, but the baseline has only one self-attention layer. In addition more methodologies should be necessary to compare the results of the experiment. \n\nThe architecture part is complicated to follow and I don't understand the big contribution of this paper. \n\nFor that reason, I recommend a reject the paper and work more for the final version "}, "signatures": ["ICLR.cc/2020/Conference/Paper2581/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2581/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["cui.wanyun@sufe.edu.cn"], "title": "Attention over Phrases", "authors": ["Wanyun Cui"], "pdf": "/pdf/e17d21e6827ee934813c066a345f6f2f059ae0ea.pdf", "TL;DR": "We study the problem of representing phrases as atoms in attention.", "abstract": "How to represent the sentence ``That's the last straw for her''? The answer of the self-attention is a weighted sum of each individual words, i.e. $$semantics=\\alpha_1Emb(\\text{That})+\\alpha_2Emb(\\text{'s})+\\cdots+\\alpha_nEmb(\\text{her})$$. But the weighted sum of ``That's'', ``the'', ``last'', ``straw'' can hardly represent the semantics of the phrase. We argue that the phrases play an important role in attention.\nIf we combine some words into phrases, a more reasonable representation with compositions is \n$$semantics=\\alpha_1Emb(\\text{That's})+Emb_2(\\text{the last straw})+\\alpha_3Emb(\\text{for})+\\alpha_4Emb(\\text{her})$$.\nWhile recent studies prefer to use the attention mechanism to represent the natural language, few noticed the word compositions. In this paper, we study the problem of representing such compositional attentions in phrases. In this paper, we proposed a new attention architecture called HyperTransformer. Besides representing the words of the sentence, we introduce hypernodes to represent the candidate phrases in attention. \nHyperTransformer has two phases. The first phase is used to attend over all word/phrase pairs, which is similar to the standard Transformer. The second phase is used to represent the inductive bias within each phrase. Specially, we incorporate the non-linear attention in the second phase. The non-linearity represents the the semantic mutations in phrases. The experimental performance has been greatly improved. In WMT16 English-German translation task, the BLEU increases from 20.90 (by Transformer) to 34.61 (by HyperTransformer).", "keywords": ["representation learning", "natural language processing", "attention"], "paperhash": "cui|attention_over_phrases", "original_pdf": "/attachment/e17d21e6827ee934813c066a345f6f2f059ae0ea.pdf", "_bibtex": "@misc{\ncui2020attention,\ntitle={Attention over Phrases},\nauthor={Wanyun Cui},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeYalBKvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJeYalBKvr", "replyto": "HJeYalBKvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2581/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2581/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2581/Reviewers"], "noninvitees": [], "tcdate": 1570237720797, "tmdate": 1574723093423, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2581/-/Official_Review"}}}, {"id": "SyxgHKR_qS", "original": null, "number": 5, "cdate": 1572559159608, "ddate": null, "tcdate": 1572559159608, "tmdate": 1572559159608, "tddate": null, "forum": "HJeYalBKvr", "replyto": "rylp9-vO9S", "invitation": "ICLR.cc/2020/Conference/Paper2581/-/Public_Comment", "content": {"title": "Thank you for your response ", "comment": "Your answer makes perfect sense as the non-linearity is proposed by your method and does not exist in normal attention.  Thanks again for the clarifications. "}, "signatures": ["~Hamed_GHAZAVI_KHORASGANI1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Hamed_GHAZAVI_KHORASGANI1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["cui.wanyun@sufe.edu.cn"], "title": "Attention over Phrases", "authors": ["Wanyun Cui"], "pdf": "/pdf/e17d21e6827ee934813c066a345f6f2f059ae0ea.pdf", "TL;DR": "We study the problem of representing phrases as atoms in attention.", "abstract": "How to represent the sentence ``That's the last straw for her''? The answer of the self-attention is a weighted sum of each individual words, i.e. $$semantics=\\alpha_1Emb(\\text{That})+\\alpha_2Emb(\\text{'s})+\\cdots+\\alpha_nEmb(\\text{her})$$. But the weighted sum of ``That's'', ``the'', ``last'', ``straw'' can hardly represent the semantics of the phrase. We argue that the phrases play an important role in attention.\nIf we combine some words into phrases, a more reasonable representation with compositions is \n$$semantics=\\alpha_1Emb(\\text{That's})+Emb_2(\\text{the last straw})+\\alpha_3Emb(\\text{for})+\\alpha_4Emb(\\text{her})$$.\nWhile recent studies prefer to use the attention mechanism to represent the natural language, few noticed the word compositions. In this paper, we study the problem of representing such compositional attentions in phrases. In this paper, we proposed a new attention architecture called HyperTransformer. Besides representing the words of the sentence, we introduce hypernodes to represent the candidate phrases in attention. \nHyperTransformer has two phases. The first phase is used to attend over all word/phrase pairs, which is similar to the standard Transformer. The second phase is used to represent the inductive bias within each phrase. Specially, we incorporate the non-linear attention in the second phase. The non-linearity represents the the semantic mutations in phrases. The experimental performance has been greatly improved. In WMT16 English-German translation task, the BLEU increases from 20.90 (by Transformer) to 34.61 (by HyperTransformer).", "keywords": ["representation learning", "natural language processing", "attention"], "paperhash": "cui|attention_over_phrases", "original_pdf": "/attachment/e17d21e6827ee934813c066a345f6f2f059ae0ea.pdf", "_bibtex": "@misc{\ncui2020attention,\ntitle={Attention over Phrases},\nauthor={Wanyun Cui},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeYalBKvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeYalBKvr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504178219, "tmdate": 1576860595165, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2581/Authors", "ICLR.cc/2020/Conference/Paper2581/Reviewers", "ICLR.cc/2020/Conference/Paper2581/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2581/-/Public_Comment"}}}, {"id": "rylp9-vO9S", "original": null, "number": 3, "cdate": 1572528533102, "ddate": null, "tcdate": 1572528533102, "tmdate": 1572528533102, "tddate": null, "forum": "HJeYalBKvr", "replyto": "SJxYE6JI9H", "invitation": "ICLR.cc/2020/Conference/Paper2581/-/Official_Comment", "content": {"title": "Response", "comment": "I think they are different. The information related to phrases in our approach cannot be captured in higher layers in normal attention over words. For k=2, our approach represents the inductive bias and non-linearity of phrases with length 2. For \"deep\" network structure, such inductive bias and non-linearity of 2-word phrases propagate to more complicated phrases. But in normal attention over words, neither the inductive bias nor the non-linearity are captured. So we cannot expect the similar performance of the normal attention over words if we have more data or deeper structures."}, "signatures": ["ICLR.cc/2020/Conference/Paper2581/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2581/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["cui.wanyun@sufe.edu.cn"], "title": "Attention over Phrases", "authors": ["Wanyun Cui"], "pdf": "/pdf/e17d21e6827ee934813c066a345f6f2f059ae0ea.pdf", "TL;DR": "We study the problem of representing phrases as atoms in attention.", "abstract": "How to represent the sentence ``That's the last straw for her''? The answer of the self-attention is a weighted sum of each individual words, i.e. $$semantics=\\alpha_1Emb(\\text{That})+\\alpha_2Emb(\\text{'s})+\\cdots+\\alpha_nEmb(\\text{her})$$. But the weighted sum of ``That's'', ``the'', ``last'', ``straw'' can hardly represent the semantics of the phrase. We argue that the phrases play an important role in attention.\nIf we combine some words into phrases, a more reasonable representation with compositions is \n$$semantics=\\alpha_1Emb(\\text{That's})+Emb_2(\\text{the last straw})+\\alpha_3Emb(\\text{for})+\\alpha_4Emb(\\text{her})$$.\nWhile recent studies prefer to use the attention mechanism to represent the natural language, few noticed the word compositions. In this paper, we study the problem of representing such compositional attentions in phrases. In this paper, we proposed a new attention architecture called HyperTransformer. Besides representing the words of the sentence, we introduce hypernodes to represent the candidate phrases in attention. \nHyperTransformer has two phases. The first phase is used to attend over all word/phrase pairs, which is similar to the standard Transformer. The second phase is used to represent the inductive bias within each phrase. Specially, we incorporate the non-linear attention in the second phase. The non-linearity represents the the semantic mutations in phrases. The experimental performance has been greatly improved. In WMT16 English-German translation task, the BLEU increases from 20.90 (by Transformer) to 34.61 (by HyperTransformer).", "keywords": ["representation learning", "natural language processing", "attention"], "paperhash": "cui|attention_over_phrases", "original_pdf": "/attachment/e17d21e6827ee934813c066a345f6f2f059ae0ea.pdf", "_bibtex": "@misc{\ncui2020attention,\ntitle={Attention over Phrases},\nauthor={Wanyun Cui},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeYalBKvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeYalBKvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2581/Authors", "ICLR.cc/2020/Conference/Paper2581/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2581/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2581/Reviewers", "ICLR.cc/2020/Conference/Paper2581/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2581/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2581/Authors|ICLR.cc/2020/Conference/Paper2581/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504139164, "tmdate": 1576860562084, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2581/Authors", "ICLR.cc/2020/Conference/Paper2581/Reviewers", "ICLR.cc/2020/Conference/Paper2581/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2581/-/Official_Comment"}}}, {"id": "SJxYE6JI9H", "original": null, "number": 4, "cdate": 1572367664549, "ddate": null, "tcdate": 1572367664549, "tmdate": 1572367664549, "tddate": null, "forum": "HJeYalBKvr", "replyto": "HJg-aRtxqr", "invitation": "ICLR.cc/2020/Conference/Paper2581/-/Public_Comment", "content": {"title": "How about attention over words, does that propagate to more complicated compositions in higher layers as well?", "comment": "Thank you so much for your comprehensive response. If we accept the second explanation (\"the information of the 2 word compositions in lower layers will propagate to more complicated compositions in higher layers\"), can we also say the information related to phrases (whether 2, 3 or more) would be implicitly captured in higher layers when we use normal attention over words. If this is the case, attention over phrases is only necessary when we don't have enough data or computational resources for extensive training.   "}, "signatures": ["~Hamed_GHAZAVI_KHORASGANI1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Hamed_GHAZAVI_KHORASGANI1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["cui.wanyun@sufe.edu.cn"], "title": "Attention over Phrases", "authors": ["Wanyun Cui"], "pdf": "/pdf/e17d21e6827ee934813c066a345f6f2f059ae0ea.pdf", "TL;DR": "We study the problem of representing phrases as atoms in attention.", "abstract": "How to represent the sentence ``That's the last straw for her''? The answer of the self-attention is a weighted sum of each individual words, i.e. $$semantics=\\alpha_1Emb(\\text{That})+\\alpha_2Emb(\\text{'s})+\\cdots+\\alpha_nEmb(\\text{her})$$. But the weighted sum of ``That's'', ``the'', ``last'', ``straw'' can hardly represent the semantics of the phrase. We argue that the phrases play an important role in attention.\nIf we combine some words into phrases, a more reasonable representation with compositions is \n$$semantics=\\alpha_1Emb(\\text{That's})+Emb_2(\\text{the last straw})+\\alpha_3Emb(\\text{for})+\\alpha_4Emb(\\text{her})$$.\nWhile recent studies prefer to use the attention mechanism to represent the natural language, few noticed the word compositions. In this paper, we study the problem of representing such compositional attentions in phrases. In this paper, we proposed a new attention architecture called HyperTransformer. Besides representing the words of the sentence, we introduce hypernodes to represent the candidate phrases in attention. \nHyperTransformer has two phases. The first phase is used to attend over all word/phrase pairs, which is similar to the standard Transformer. The second phase is used to represent the inductive bias within each phrase. Specially, we incorporate the non-linear attention in the second phase. The non-linearity represents the the semantic mutations in phrases. The experimental performance has been greatly improved. In WMT16 English-German translation task, the BLEU increases from 20.90 (by Transformer) to 34.61 (by HyperTransformer).", "keywords": ["representation learning", "natural language processing", "attention"], "paperhash": "cui|attention_over_phrases", "original_pdf": "/attachment/e17d21e6827ee934813c066a345f6f2f059ae0ea.pdf", "_bibtex": "@misc{\ncui2020attention,\ntitle={Attention over Phrases},\nauthor={Wanyun Cui},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeYalBKvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeYalBKvr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504178219, "tmdate": 1576860595165, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2581/Authors", "ICLR.cc/2020/Conference/Paper2581/Reviewers", "ICLR.cc/2020/Conference/Paper2581/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2581/-/Public_Comment"}}}, {"id": "HJg-aRtxqr", "original": null, "number": 2, "cdate": 1572015800734, "ddate": null, "tcdate": 1572015800734, "tmdate": 1572015800734, "tddate": null, "forum": "HJeYalBKvr", "replyto": "H1lsJqssFB", "invitation": "ICLR.cc/2020/Conference/Paper2581/-/Official_Comment", "content": {"title": "Response", "comment": "Thanks for your comments. Very interesting question.\n\nOne possible answer is, higher k means more parameters, which leads to overfitting. \n\nWe further investigate such phenomenon in PhraseTransformer. Another possible explanation is, as we are using a #deep# neural network, the information of the 2 word composisions in lower layers will propagate to more complicated compositions in higher layers. So we do not need to explicitly use higher k to represent longer phrases. To verify this, we investigate the effectiveness of our proposed method for n_layer=1. In this case, there is no such information propagation. We obtained the results below. In this setting, PhraseTransformer(k=3) performs slightly better than PhraseTransformer(k=2). This is consistent with our above explanation.\n\n model                                                             BLEU  \n \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 \n PhraseTransformer(k=2,n_layer=1)            29.94 \n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \n PhraseTransformer(k=3,n_layer=1)            30.14 "}, "signatures": ["ICLR.cc/2020/Conference/Paper2581/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2581/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["cui.wanyun@sufe.edu.cn"], "title": "Attention over Phrases", "authors": ["Wanyun Cui"], "pdf": "/pdf/e17d21e6827ee934813c066a345f6f2f059ae0ea.pdf", "TL;DR": "We study the problem of representing phrases as atoms in attention.", "abstract": "How to represent the sentence ``That's the last straw for her''? The answer of the self-attention is a weighted sum of each individual words, i.e. $$semantics=\\alpha_1Emb(\\text{That})+\\alpha_2Emb(\\text{'s})+\\cdots+\\alpha_nEmb(\\text{her})$$. But the weighted sum of ``That's'', ``the'', ``last'', ``straw'' can hardly represent the semantics of the phrase. We argue that the phrases play an important role in attention.\nIf we combine some words into phrases, a more reasonable representation with compositions is \n$$semantics=\\alpha_1Emb(\\text{That's})+Emb_2(\\text{the last straw})+\\alpha_3Emb(\\text{for})+\\alpha_4Emb(\\text{her})$$.\nWhile recent studies prefer to use the attention mechanism to represent the natural language, few noticed the word compositions. In this paper, we study the problem of representing such compositional attentions in phrases. In this paper, we proposed a new attention architecture called HyperTransformer. Besides representing the words of the sentence, we introduce hypernodes to represent the candidate phrases in attention. \nHyperTransformer has two phases. The first phase is used to attend over all word/phrase pairs, which is similar to the standard Transformer. The second phase is used to represent the inductive bias within each phrase. Specially, we incorporate the non-linear attention in the second phase. The non-linearity represents the the semantic mutations in phrases. The experimental performance has been greatly improved. In WMT16 English-German translation task, the BLEU increases from 20.90 (by Transformer) to 34.61 (by HyperTransformer).", "keywords": ["representation learning", "natural language processing", "attention"], "paperhash": "cui|attention_over_phrases", "original_pdf": "/attachment/e17d21e6827ee934813c066a345f6f2f059ae0ea.pdf", "_bibtex": "@misc{\ncui2020attention,\ntitle={Attention over Phrases},\nauthor={Wanyun Cui},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeYalBKvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeYalBKvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2581/Authors", "ICLR.cc/2020/Conference/Paper2581/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2581/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2581/Reviewers", "ICLR.cc/2020/Conference/Paper2581/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2581/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2581/Authors|ICLR.cc/2020/Conference/Paper2581/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504139164, "tmdate": 1576860562084, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2581/Authors", "ICLR.cc/2020/Conference/Paper2581/Reviewers", "ICLR.cc/2020/Conference/Paper2581/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2581/-/Official_Comment"}}}, {"id": "H1lsJqssFB", "original": null, "number": 3, "cdate": 1571695074649, "ddate": null, "tcdate": 1571695074649, "tmdate": 1571695177002, "tddate": null, "forum": "HJeYalBKvr", "replyto": "HJeYalBKvr", "invitation": "ICLR.cc/2020/Conference/Paper2581/-/Public_Comment", "content": {"title": "k-2 vs k-3 Perfromance", "comment": "Very interesting work! I think the authors should explain why in all the experiments (Table 1 and Table 2) PhraseTransformer (k=2) outperforms PhraseTransformer (k=3)? If attention over phrases is the answer, shouldn't we get better results with higher k? "}, "signatures": ["~Hamed_GHAZAVI_KHORASGANI1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Hamed_GHAZAVI_KHORASGANI1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["cui.wanyun@sufe.edu.cn"], "title": "Attention over Phrases", "authors": ["Wanyun Cui"], "pdf": "/pdf/e17d21e6827ee934813c066a345f6f2f059ae0ea.pdf", "TL;DR": "We study the problem of representing phrases as atoms in attention.", "abstract": "How to represent the sentence ``That's the last straw for her''? The answer of the self-attention is a weighted sum of each individual words, i.e. $$semantics=\\alpha_1Emb(\\text{That})+\\alpha_2Emb(\\text{'s})+\\cdots+\\alpha_nEmb(\\text{her})$$. But the weighted sum of ``That's'', ``the'', ``last'', ``straw'' can hardly represent the semantics of the phrase. We argue that the phrases play an important role in attention.\nIf we combine some words into phrases, a more reasonable representation with compositions is \n$$semantics=\\alpha_1Emb(\\text{That's})+Emb_2(\\text{the last straw})+\\alpha_3Emb(\\text{for})+\\alpha_4Emb(\\text{her})$$.\nWhile recent studies prefer to use the attention mechanism to represent the natural language, few noticed the word compositions. In this paper, we study the problem of representing such compositional attentions in phrases. In this paper, we proposed a new attention architecture called HyperTransformer. Besides representing the words of the sentence, we introduce hypernodes to represent the candidate phrases in attention. \nHyperTransformer has two phases. The first phase is used to attend over all word/phrase pairs, which is similar to the standard Transformer. The second phase is used to represent the inductive bias within each phrase. Specially, we incorporate the non-linear attention in the second phase. The non-linearity represents the the semantic mutations in phrases. The experimental performance has been greatly improved. In WMT16 English-German translation task, the BLEU increases from 20.90 (by Transformer) to 34.61 (by HyperTransformer).", "keywords": ["representation learning", "natural language processing", "attention"], "paperhash": "cui|attention_over_phrases", "original_pdf": "/attachment/e17d21e6827ee934813c066a345f6f2f059ae0ea.pdf", "_bibtex": "@misc{\ncui2020attention,\ntitle={Attention over Phrases},\nauthor={Wanyun Cui},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeYalBKvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeYalBKvr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504178219, "tmdate": 1576860595165, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2581/Authors", "ICLR.cc/2020/Conference/Paper2581/Reviewers", "ICLR.cc/2020/Conference/Paper2581/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2581/-/Public_Comment"}}}, {"id": "BylzE-XI_r", "original": null, "number": 1, "cdate": 1570283818172, "ddate": null, "tcdate": 1570283818172, "tmdate": 1570283818172, "tddate": null, "forum": "HJeYalBKvr", "replyto": "B1ec1wCzOS", "invitation": "ICLR.cc/2020/Conference/Paper2581/-/Official_Comment", "content": {"comment": "Thanks for your comments.\n\nAccording to your first comment, we tried to run the Transformer using OpenNMT over the Multi30k dataset, and got BLEU score of 23.47. Could you specific the detailed hyper-parameters you use in OpenNMT? We are glad to see a better baseline model that you implemented, which may further verify the effectiveness of our method if we apply our optimizations over such a baseline.\n\nBy following the parameters in <Attention Is All You Need>, currently we use the command below:\nonmt_train -encoder_type transformer -decoder_type transformer -gpu_ranks 0 -enc_layers 6 -dec_layers 6 -heads 8 -rnn_size 512 -src_word_vec_size 512 -tgt_word_vec_size 512 -learning_rate 0.001 -optim adam -adam_beta2 0.98\n\nAs to the second comment, we are trying to add more experimental analysis. It may take several days.", "title": "Detailed results by OpenNMT"}, "signatures": ["ICLR.cc/2020/Conference/Paper2581/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2581/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["cui.wanyun@sufe.edu.cn"], "title": "Attention over Phrases", "authors": ["Wanyun Cui"], "pdf": "/pdf/e17d21e6827ee934813c066a345f6f2f059ae0ea.pdf", "TL;DR": "We study the problem of representing phrases as atoms in attention.", "abstract": "How to represent the sentence ``That's the last straw for her''? The answer of the self-attention is a weighted sum of each individual words, i.e. $$semantics=\\alpha_1Emb(\\text{That})+\\alpha_2Emb(\\text{'s})+\\cdots+\\alpha_nEmb(\\text{her})$$. But the weighted sum of ``That's'', ``the'', ``last'', ``straw'' can hardly represent the semantics of the phrase. We argue that the phrases play an important role in attention.\nIf we combine some words into phrases, a more reasonable representation with compositions is \n$$semantics=\\alpha_1Emb(\\text{That's})+Emb_2(\\text{the last straw})+\\alpha_3Emb(\\text{for})+\\alpha_4Emb(\\text{her})$$.\nWhile recent studies prefer to use the attention mechanism to represent the natural language, few noticed the word compositions. In this paper, we study the problem of representing such compositional attentions in phrases. In this paper, we proposed a new attention architecture called HyperTransformer. Besides representing the words of the sentence, we introduce hypernodes to represent the candidate phrases in attention. \nHyperTransformer has two phases. The first phase is used to attend over all word/phrase pairs, which is similar to the standard Transformer. The second phase is used to represent the inductive bias within each phrase. Specially, we incorporate the non-linear attention in the second phase. The non-linearity represents the the semantic mutations in phrases. The experimental performance has been greatly improved. In WMT16 English-German translation task, the BLEU increases from 20.90 (by Transformer) to 34.61 (by HyperTransformer).", "keywords": ["representation learning", "natural language processing", "attention"], "paperhash": "cui|attention_over_phrases", "original_pdf": "/attachment/e17d21e6827ee934813c066a345f6f2f059ae0ea.pdf", "_bibtex": "@misc{\ncui2020attention,\ntitle={Attention over Phrases},\nauthor={Wanyun Cui},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeYalBKvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeYalBKvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2581/Authors", "ICLR.cc/2020/Conference/Paper2581/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2581/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2581/Reviewers", "ICLR.cc/2020/Conference/Paper2581/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2581/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2581/Authors|ICLR.cc/2020/Conference/Paper2581/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504139164, "tmdate": 1576860562084, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2581/Authors", "ICLR.cc/2020/Conference/Paper2581/Reviewers", "ICLR.cc/2020/Conference/Paper2581/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2581/-/Official_Comment"}}}, {"id": "B1ec1wCzOS", "original": null, "number": 2, "cdate": 1570068194153, "ddate": null, "tcdate": 1570068194153, "tmdate": 1570068194153, "tddate": null, "forum": "HJeYalBKvr", "replyto": "HJeYalBKvr", "invitation": "ICLR.cc/2020/Conference/Paper2581/-/Public_Comment", "content": {"comment": "A good idea to add phrase information in the transformer.\nHowever, I have questions about your experiments.\n\nI think your results on table 1 for transformer is not good. I run the transformer using OpenNMT code on Multi30K dataset, achieving the Blue value: 34.2, without adjust hyper parameters seriously. So I want to know why you report 20? \n\nThe second question is that why do not perform your experiments on standard dataset such as wmt14' or more languages as done on \"attention is all your need\"?\n\nThanks.", "title": "Results on your table 1"}, "signatures": ["~Hao_Zhang1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Hao_Zhang1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["cui.wanyun@sufe.edu.cn"], "title": "Attention over Phrases", "authors": ["Wanyun Cui"], "pdf": "/pdf/e17d21e6827ee934813c066a345f6f2f059ae0ea.pdf", "TL;DR": "We study the problem of representing phrases as atoms in attention.", "abstract": "How to represent the sentence ``That's the last straw for her''? The answer of the self-attention is a weighted sum of each individual words, i.e. $$semantics=\\alpha_1Emb(\\text{That})+\\alpha_2Emb(\\text{'s})+\\cdots+\\alpha_nEmb(\\text{her})$$. But the weighted sum of ``That's'', ``the'', ``last'', ``straw'' can hardly represent the semantics of the phrase. We argue that the phrases play an important role in attention.\nIf we combine some words into phrases, a more reasonable representation with compositions is \n$$semantics=\\alpha_1Emb(\\text{That's})+Emb_2(\\text{the last straw})+\\alpha_3Emb(\\text{for})+\\alpha_4Emb(\\text{her})$$.\nWhile recent studies prefer to use the attention mechanism to represent the natural language, few noticed the word compositions. In this paper, we study the problem of representing such compositional attentions in phrases. In this paper, we proposed a new attention architecture called HyperTransformer. Besides representing the words of the sentence, we introduce hypernodes to represent the candidate phrases in attention. \nHyperTransformer has two phases. The first phase is used to attend over all word/phrase pairs, which is similar to the standard Transformer. The second phase is used to represent the inductive bias within each phrase. Specially, we incorporate the non-linear attention in the second phase. The non-linearity represents the the semantic mutations in phrases. The experimental performance has been greatly improved. In WMT16 English-German translation task, the BLEU increases from 20.90 (by Transformer) to 34.61 (by HyperTransformer).", "keywords": ["representation learning", "natural language processing", "attention"], "paperhash": "cui|attention_over_phrases", "original_pdf": "/attachment/e17d21e6827ee934813c066a345f6f2f059ae0ea.pdf", "_bibtex": "@misc{\ncui2020attention,\ntitle={Attention over Phrases},\nauthor={Wanyun Cui},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeYalBKvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeYalBKvr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504178219, "tmdate": 1576860595165, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2581/Authors", "ICLR.cc/2020/Conference/Paper2581/Reviewers", "ICLR.cc/2020/Conference/Paper2581/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2581/-/Public_Comment"}}}, {"id": "B1lGqREgur", "original": null, "number": 1, "cdate": 1569898121864, "ddate": null, "tcdate": 1569898121864, "tmdate": 1569898426116, "tddate": null, "forum": "HJeYalBKvr", "replyto": "HJeYalBKvr", "invitation": "ICLR.cc/2020/Conference/Paper2581/-/Public_Comment", "content": {"comment": "I liked the overall idea and implementation.\n\nThere are couple of typing mistakes in the paper:\n1. In the overall complexity formula on page 4, I think '=' should be replaced with '+'\n2. In section 2.3 first paragraph, H \\n R^{(n+m)xd} instead of H \\n R^{(n+m)}\n3. wee -> we\n\nIt would be awesome if you can also show how the hypernode attention helps via visualizations or reduction in errors dealing with phrases.\nCan you comment on the training time changes?", "title": "Interesting approach"}, "signatures": ["~Sai_Prabhakar_Pandi_Selvaraj1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Sai_Prabhakar_Pandi_Selvaraj1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["cui.wanyun@sufe.edu.cn"], "title": "Attention over Phrases", "authors": ["Wanyun Cui"], "pdf": "/pdf/e17d21e6827ee934813c066a345f6f2f059ae0ea.pdf", "TL;DR": "We study the problem of representing phrases as atoms in attention.", "abstract": "How to represent the sentence ``That's the last straw for her''? The answer of the self-attention is a weighted sum of each individual words, i.e. $$semantics=\\alpha_1Emb(\\text{That})+\\alpha_2Emb(\\text{'s})+\\cdots+\\alpha_nEmb(\\text{her})$$. But the weighted sum of ``That's'', ``the'', ``last'', ``straw'' can hardly represent the semantics of the phrase. We argue that the phrases play an important role in attention.\nIf we combine some words into phrases, a more reasonable representation with compositions is \n$$semantics=\\alpha_1Emb(\\text{That's})+Emb_2(\\text{the last straw})+\\alpha_3Emb(\\text{for})+\\alpha_4Emb(\\text{her})$$.\nWhile recent studies prefer to use the attention mechanism to represent the natural language, few noticed the word compositions. In this paper, we study the problem of representing such compositional attentions in phrases. In this paper, we proposed a new attention architecture called HyperTransformer. Besides representing the words of the sentence, we introduce hypernodes to represent the candidate phrases in attention. \nHyperTransformer has two phases. The first phase is used to attend over all word/phrase pairs, which is similar to the standard Transformer. The second phase is used to represent the inductive bias within each phrase. Specially, we incorporate the non-linear attention in the second phase. The non-linearity represents the the semantic mutations in phrases. The experimental performance has been greatly improved. In WMT16 English-German translation task, the BLEU increases from 20.90 (by Transformer) to 34.61 (by HyperTransformer).", "keywords": ["representation learning", "natural language processing", "attention"], "paperhash": "cui|attention_over_phrases", "original_pdf": "/attachment/e17d21e6827ee934813c066a345f6f2f059ae0ea.pdf", "_bibtex": "@misc{\ncui2020attention,\ntitle={Attention over Phrases},\nauthor={Wanyun Cui},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeYalBKvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeYalBKvr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504178219, "tmdate": 1576860595165, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2581/Authors", "ICLR.cc/2020/Conference/Paper2581/Reviewers", "ICLR.cc/2020/Conference/Paper2581/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2581/-/Public_Comment"}}}], "count": 13}