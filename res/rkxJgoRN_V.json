{"notes": [{"id": "rkxJgoRN_V", "original": "Bke0doGVOV", "number": 46, "cdate": 1553423078701, "ddate": null, "tcdate": 1553423078701, "tmdate": 1562082115532, "tddate": null, "forum": "rkxJgoRN_V", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "content": {"title": "Automatic Labeling of Data for Transfer Learning", "authors": ["Parijat Dube", "Bishwaranjan Bhattacharjee", "Siyu Huo", "Patrick Watson", "John Kender", "Brian Belgodere"], "authorids": ["pdube@us.ibm.com", "bhatta@us.ibm.com", "siyu.huo@us.ibm.com", "pwatson@us.ibm.com", "jrk@cs.columbia.edu", "bmbelgod@us.ibm.com"], "keywords": ["transfer learning", "fine-tuning", "divergence", "pseudo labeling", "automated labeling", "experiments"], "TL;DR": "A technique for automatically labeling large unlabeled datasets so that they can train source models for transfer learning and its experimental evaluation. ", "abstract": "Transfer learning uses trained weights from a source model as the initial weightsfor the training of a target dataset.  A well chosen source with a large numberof labeled data leads to significant improvement in accuracy.  We demonstrate atechnique that automatically labels large unlabeled datasets so that they can trainsource models for transfer learning. We experimentally evaluate this method, usinga baseline dataset of human-annotated ImageNet1K labels, against five variationsof this technique.  We show that the performance of these automatically trainedmodels come within 17% of baseline on average.", "pdf": "/pdf/690eef7631dd3d690867765c99a4905f551b1f1b.pdf", "paperhash": "dube|automatic_labeling_of_data_for_transfer_learning"}, "signatures": ["ICLR.cc/2019/Workshop/LLD"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "cdate": 1548689671889, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "content": {"authors": {"values-regex": ".*"}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1548689671889, "tmdate": 1557933709646, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/LLD"], "details": {"writable": true}}}, "tauthor": "OpenReview.net"}, {"id": "rJlzGd5TtE", "original": null, "number": 1, "cdate": 1555044362295, "ddate": null, "tcdate": 1555044362295, "tmdate": 1555511876476, "tddate": null, "forum": "rkxJgoRN_V", "replyto": "rkxJgoRN_V", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper46/Official_Review", "content": {"title": "Variations on pseudo-label generation", "review": "This paper describes 5 methods for generating pseudo-labels for images using a pre-trained VGG net. They involve concatenating N-nearest labels, clustering, and a geometric method.\n\n- The first three methods might also have been classified as three variants of N nearest neighbors with N=1,2,3\n\n- The geometric method creating a maximum surface triangle doesn't appear to have any motivation\n\n- An important contribution in this domain, Hsu et al, 2018, https://arxiv.org/abs/1810.02334, which performs clustering on neural network features, bearing a fair amount of similarity to at least method 4, is not mentioned in this paper\n\n- The specific notion of \"KL divergence\" is not explained or stated as a formula, making the exact probability distributions over which it is computed impossible to know\n\n- Minor: \"figure 2\" should be a table (but appears to be a cropped screenshot from a spreadsheet)\n\nWhile it is good that different types of pseudolabeling are explored and evaluated, this paper is lacking on many fronts.", "rating": "2: Marginally below acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper46/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper46/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatic Labeling of Data for Transfer Learning", "authors": ["Parijat Dube", "Bishwaranjan Bhattacharjee", "Siyu Huo", "Patrick Watson", "John Kender", "Brian Belgodere"], "authorids": ["pdube@us.ibm.com", "bhatta@us.ibm.com", "siyu.huo@us.ibm.com", "pwatson@us.ibm.com", "jrk@cs.columbia.edu", "bmbelgod@us.ibm.com"], "keywords": ["transfer learning", "fine-tuning", "divergence", "pseudo labeling", "automated labeling", "experiments"], "TL;DR": "A technique for automatically labeling large unlabeled datasets so that they can train source models for transfer learning and its experimental evaluation. ", "abstract": "Transfer learning uses trained weights from a source model as the initial weightsfor the training of a target dataset.  A well chosen source with a large numberof labeled data leads to significant improvement in accuracy.  We demonstrate atechnique that automatically labels large unlabeled datasets so that they can trainsource models for transfer learning. We experimentally evaluate this method, usinga baseline dataset of human-annotated ImageNet1K labels, against five variationsof this technique.  We show that the performance of these automatically trainedmodels come within 17% of baseline on average.", "pdf": "/pdf/690eef7631dd3d690867765c99a4905f551b1f1b.pdf", "paperhash": "dube|automatic_labeling_of_data_for_transfer_learning"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper46/Official_Review", "cdate": 1553713414087, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "rkxJgoRN_V", "replyto": "rkxJgoRN_V", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper46/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper46/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713414087, "tmdate": 1555511824251, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper46/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "r1ea_iJZ9V", "original": null, "number": 2, "cdate": 1555262325501, "ddate": null, "tcdate": 1555262325501, "tmdate": 1555511873649, "tddate": null, "forum": "rkxJgoRN_V", "replyto": "rkxJgoRN_V", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper46/Official_Review", "content": {"title": "Review", "review": "The main idea of the paper is to algorithmically label (or pseudolabel) a large amount of image data so that the model pre-trained on these data can better transfer to some target task. The idea is to use some specialized (also pre-trained) models that can identify certain classes as soft labeling functions (i.e., at pre-training time, use the distance to the average output activations as the pre-training learning signal).\n\nThe idea is intuitive and seems working, but the downside of the approach is that it requires these gold-standard specialized models used for source labeling (or known labeled datasets used to construct such models). Of course, this is just a pre-training method, and the goal is to transfer the model to a target domain that has some (limited) gold-standard annotations. However, the approach seems quite heuristic, it's unclear whether such pre-training introduces any biases (during pre-training), and generally a more thorough analysis of how the quality of the known labeled data affects model pretraining is necessary.", "rating": "2: Marginally below acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper46/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper46/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatic Labeling of Data for Transfer Learning", "authors": ["Parijat Dube", "Bishwaranjan Bhattacharjee", "Siyu Huo", "Patrick Watson", "John Kender", "Brian Belgodere"], "authorids": ["pdube@us.ibm.com", "bhatta@us.ibm.com", "siyu.huo@us.ibm.com", "pwatson@us.ibm.com", "jrk@cs.columbia.edu", "bmbelgod@us.ibm.com"], "keywords": ["transfer learning", "fine-tuning", "divergence", "pseudo labeling", "automated labeling", "experiments"], "TL;DR": "A technique for automatically labeling large unlabeled datasets so that they can train source models for transfer learning and its experimental evaluation. ", "abstract": "Transfer learning uses trained weights from a source model as the initial weightsfor the training of a target dataset.  A well chosen source with a large numberof labeled data leads to significant improvement in accuracy.  We demonstrate atechnique that automatically labels large unlabeled datasets so that they can trainsource models for transfer learning. We experimentally evaluate this method, usinga baseline dataset of human-annotated ImageNet1K labels, against five variationsof this technique.  We show that the performance of these automatically trainedmodels come within 17% of baseline on average.", "pdf": "/pdf/690eef7631dd3d690867765c99a4905f551b1f1b.pdf", "paperhash": "dube|automatic_labeling_of_data_for_transfer_learning"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper46/Official_Review", "cdate": 1553713414087, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "rkxJgoRN_V", "replyto": "rkxJgoRN_V", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper46/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper46/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713414087, "tmdate": 1555511824251, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper46/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "HylRWyMfcV", "original": null, "number": 1, "cdate": 1555336966303, "ddate": null, "tcdate": 1555336966303, "tmdate": 1555510982106, "tddate": null, "forum": "rkxJgoRN_V", "replyto": "rkxJgoRN_V", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper46/Decision", "content": {"title": "Acceptance Decision", "decision": "Reject"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatic Labeling of Data for Transfer Learning", "authors": ["Parijat Dube", "Bishwaranjan Bhattacharjee", "Siyu Huo", "Patrick Watson", "John Kender", "Brian Belgodere"], "authorids": ["pdube@us.ibm.com", "bhatta@us.ibm.com", "siyu.huo@us.ibm.com", "pwatson@us.ibm.com", "jrk@cs.columbia.edu", "bmbelgod@us.ibm.com"], "keywords": ["transfer learning", "fine-tuning", "divergence", "pseudo labeling", "automated labeling", "experiments"], "TL;DR": "A technique for automatically labeling large unlabeled datasets so that they can train source models for transfer learning and its experimental evaluation. ", "abstract": "Transfer learning uses trained weights from a source model as the initial weightsfor the training of a target dataset.  A well chosen source with a large numberof labeled data leads to significant improvement in accuracy.  We demonstrate atechnique that automatically labels large unlabeled datasets so that they can trainsource models for transfer learning. We experimentally evaluate this method, usinga baseline dataset of human-annotated ImageNet1K labels, against five variationsof this technique.  We show that the performance of these automatically trainedmodels come within 17% of baseline on average.", "pdf": "/pdf/690eef7631dd3d690867765c99a4905f551b1f1b.pdf", "paperhash": "dube|automatic_labeling_of_data_for_transfer_learning"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper46/Decision", "cdate": 1554736075544, "reply": {"forum": "rkxJgoRN_V", "replyto": "rkxJgoRN_V", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554736075544, "tmdate": 1555510963305, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}], "count": 4}