{"notes": [{"id": "defQ1AG6IWn", "original": "bqp6R4974KK", "number": 482, "cdate": 1601308060968, "ddate": null, "tcdate": 1601308060968, "tmdate": 1614985766724, "tddate": null, "forum": "defQ1AG6IWn", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Neighbor Class Consistency on Unsupervised Domain Adaptation", "authorids": ["liu.chang6@northeastern.edu", "~Kai_Li3", "~Yun_Fu1"], "authors": ["Chang Liu", "Kai Li", "Yun Fu"], "keywords": ["Unsupervised domain adaptation", "Consistency regularization", "Neighbor"], "abstract": "Unsupervised domain adaptation (UDA) is to make predictions for unlabeled data in a target domain with labeled data from source domain available. Recent advances exploit entropy minimization and self-training to align the feature of two domains. However, as decision boundary is largely biased towards source data, class-wise pseudo labels generated by target predictions are usually very noisy, and trusting those noisy supervisions might potentially deteriorate the intrinsic target discriminative feature. Motivated by agglomerative clustering which assumes that features in the near neighborhood should be clustered together, we observe that target features from source pre-trained model are highly intrinsic discriminative and have a high probability of sharing the same label with their neighbors. Based on those observations, we propose a simple but effective method to impose Neighbor Class Consistency on target features to preserve and further strengthen the intrinsic discriminative nature of target data while regularizing the unified classifier less biased towards source data. We also introduce an entropy-based weighting scheme to help our framework more robust to the potential noisy neighbor supervision. We conduct ablation studies and extensive experiments on three UDA image classification benchmarks. Our method outperforms all existing UDA state-of-the-art. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|neighbor_class_consistency_on_unsupervised_domain_adaptation", "pdf": "/pdf/9f7b790dc1004f0c42d1c2507e9f564f7ebaa6f3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=crc6I6R8c", "_bibtex": "@misc{\nliu2021neighbor,\ntitle={Neighbor Class Consistency on Unsupervised Domain Adaptation},\nauthor={Chang Liu and Kai Li and Yun Fu},\nyear={2021},\nurl={https://openreview.net/forum?id=defQ1AG6IWn}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "egPak1cZj2g", "original": null, "number": 1, "cdate": 1610040366554, "ddate": null, "tcdate": 1610040366554, "tmdate": 1610473957305, "tddate": null, "forum": "defQ1AG6IWn", "replyto": "defQ1AG6IWn", "invitation": "ICLR.cc/2021/Conference/Paper482/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper proposes a method (via a novel objective) for feature alignment between source and target tasks in an unsupervised domain adaptation scenario.\n\nPro\n- the proposed approach is sensible in many realistic scenarios of distribution shift\n- the submission provides an extensive empirical evaluation establishing state of the art results on several benchmark tasks\n\nCon\n- there is no thorough discussion of the the underlying assumptions (when should we expect them to hold? for shich types of tasks? which types of shifts? can they generally be reliably tested? from which type of data? unlabeled?)\n- one reviewer raised concerns over novelty, which should be more clearly addressed before publication\n- two reviewers raised concerns over use of target data for hyper-parameter selection, which seem valid; these should be fixed or clearly explained (and implications of this be discussed) in subsequent versions of this work\n\nI agree with concerns of the reviewers (the last two points), and would therefore not recommend this work for publication in the current state."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neighbor Class Consistency on Unsupervised Domain Adaptation", "authorids": ["liu.chang6@northeastern.edu", "~Kai_Li3", "~Yun_Fu1"], "authors": ["Chang Liu", "Kai Li", "Yun Fu"], "keywords": ["Unsupervised domain adaptation", "Consistency regularization", "Neighbor"], "abstract": "Unsupervised domain adaptation (UDA) is to make predictions for unlabeled data in a target domain with labeled data from source domain available. Recent advances exploit entropy minimization and self-training to align the feature of two domains. However, as decision boundary is largely biased towards source data, class-wise pseudo labels generated by target predictions are usually very noisy, and trusting those noisy supervisions might potentially deteriorate the intrinsic target discriminative feature. Motivated by agglomerative clustering which assumes that features in the near neighborhood should be clustered together, we observe that target features from source pre-trained model are highly intrinsic discriminative and have a high probability of sharing the same label with their neighbors. Based on those observations, we propose a simple but effective method to impose Neighbor Class Consistency on target features to preserve and further strengthen the intrinsic discriminative nature of target data while regularizing the unified classifier less biased towards source data. We also introduce an entropy-based weighting scheme to help our framework more robust to the potential noisy neighbor supervision. We conduct ablation studies and extensive experiments on three UDA image classification benchmarks. Our method outperforms all existing UDA state-of-the-art. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|neighbor_class_consistency_on_unsupervised_domain_adaptation", "pdf": "/pdf/9f7b790dc1004f0c42d1c2507e9f564f7ebaa6f3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=crc6I6R8c", "_bibtex": "@misc{\nliu2021neighbor,\ntitle={Neighbor Class Consistency on Unsupervised Domain Adaptation},\nauthor={Chang Liu and Kai Li and Yun Fu},\nyear={2021},\nurl={https://openreview.net/forum?id=defQ1AG6IWn}\n}"}, "tags": [], "invitation": {"reply": {"forum": "defQ1AG6IWn", "replyto": "defQ1AG6IWn", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040366541, "tmdate": 1610473957285, "id": "ICLR.cc/2021/Conference/Paper482/-/Decision"}}}, {"id": "WvVLaG4FUIK", "original": null, "number": 3, "cdate": 1603810722396, "ddate": null, "tcdate": 1603810722396, "tmdate": 1606667862262, "tddate": null, "forum": "defQ1AG6IWn", "replyto": "defQ1AG6IWn", "invitation": "ICLR.cc/2021/Conference/Paper482/-/Official_Review", "content": {"title": "A consistent and effective approach making use of neighbor samples", "review": "This paper tackles Unsupervised Domain Adaptation. The authors focus on the intrinsic discriminative feature for target samples. The proposed method, Neighborhood Class Consistency among target samples and augmented ones, is proposed as a set of multiple losses to calculate the consistency from several aspects. The experimental results show that the proposed method achieves state-of-the-art performance using the same backbone network.\n\n**Pros**\n- The consistent approach making use of neighborhood structure in the target domain is well motivated and easy to understand.\n- The proposed method achieves SoTA performance on several benchmark datasets.\n\n**Cons**\n- There is a related paper by Gu et al. (2020), although it is not referred to in this paper. Gu et al. also focus on a pseudo labeling approach and discriminative distributions in the target domain. Additionally, the experimental results show that the performance is very close to that in this paper.  Although the detailed method is different, the authors should have compared them qualitatively and quantitatively.\n\nGu et al., Spherical Space Domain Adaptation with Robust Pseudo-label Loss. In CVPR, 2020.\n\n- Section 4.2 says that the authors use different values for the hyperparameter, such as $\\lambda$, for each dataset. How do they tune them? Since the method is for UDA, there cannot be a validation set in the target domain. Therefore, different values of $\\lambda$ seem unnatural. Alternatively, the authors should report the performance with different hyperparameter values not only for the number of neighbor samples in Fig. 3 (b) but also for $\\lambda$ and triplet margin $m$.\u3000\n\n**Minor comments**\n- In the main text, there are some points the authors should insert a white space before a bracket and a citation.\n- In the last paragraph of Section 1, an abbreviation \"NC\" is used without an explanation. What does it stand for?\n- How often do we need to update the memory bank $\\mathbb{V}_t$ while a model is trained? Frequent updates of the memory would be time-consuming.\n-The format of References is messy. Some papers, such as CyCADA, should not be arXiv preprints but published ones. Some proceeding names are capitalized while the others are not. Some conference names are abbreviated while the others are not.\n-The blank Appendix section should be deleted.\n\n**Overall rating**\n\nAlthough there should be more descriptions about the existing method and experimental results, the reviewer is leaning toward acceptance. The rating can be upgraded if the authors could solve the cons above.\n\n**Additional comment after rebuttal**\n\nUnfortunately, the reviewer would like to downgrade my first score. The remaining concerns are about R2-A1 and R2-A2.\n\nIn R2-A1, the authors add some discussions about experimental results and explanations about Gu et al. (2020). The second discussion about generating pseudo labels would be the most considerable theoretical difference between the proposed method and Gu et al. (2020). The authors state that the proposed method addresses the problem by incorrect pseudo labeling of Gu et al. (2020); however, they fail to show quantitative nor qualitative discussions that support the statement. The third discussion seems just showing the proposed method achieves similar performance to that of Gu et al. (2020). As discussed in the fourth comment, the authors could add another result showing the proposed method is complementary to Gu et al. (2020).\n\nIn R2-A2, the authors honestly state that the hyperparameters are tuned according to the test data evaluation. However, it should be avoided to evaluate unsupervised domain adaptation methods since there are no labeled data in the target domain in a real setting. Moreover, Figure 4 (a) shows that the proposed method is sensitive to lambda on Office31 dataset. When lambda=0.5 on all dataset, the proposed method achieves SOTA on VisDA as shown in Table 5, but seems to fail on Office31.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper482/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper482/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neighbor Class Consistency on Unsupervised Domain Adaptation", "authorids": ["liu.chang6@northeastern.edu", "~Kai_Li3", "~Yun_Fu1"], "authors": ["Chang Liu", "Kai Li", "Yun Fu"], "keywords": ["Unsupervised domain adaptation", "Consistency regularization", "Neighbor"], "abstract": "Unsupervised domain adaptation (UDA) is to make predictions for unlabeled data in a target domain with labeled data from source domain available. Recent advances exploit entropy minimization and self-training to align the feature of two domains. However, as decision boundary is largely biased towards source data, class-wise pseudo labels generated by target predictions are usually very noisy, and trusting those noisy supervisions might potentially deteriorate the intrinsic target discriminative feature. Motivated by agglomerative clustering which assumes that features in the near neighborhood should be clustered together, we observe that target features from source pre-trained model are highly intrinsic discriminative and have a high probability of sharing the same label with their neighbors. Based on those observations, we propose a simple but effective method to impose Neighbor Class Consistency on target features to preserve and further strengthen the intrinsic discriminative nature of target data while regularizing the unified classifier less biased towards source data. We also introduce an entropy-based weighting scheme to help our framework more robust to the potential noisy neighbor supervision. We conduct ablation studies and extensive experiments on three UDA image classification benchmarks. Our method outperforms all existing UDA state-of-the-art. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|neighbor_class_consistency_on_unsupervised_domain_adaptation", "pdf": "/pdf/9f7b790dc1004f0c42d1c2507e9f564f7ebaa6f3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=crc6I6R8c", "_bibtex": "@misc{\nliu2021neighbor,\ntitle={Neighbor Class Consistency on Unsupervised Domain Adaptation},\nauthor={Chang Liu and Kai Li and Yun Fu},\nyear={2021},\nurl={https://openreview.net/forum?id=defQ1AG6IWn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "defQ1AG6IWn", "replyto": "defQ1AG6IWn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper482/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538142179, "tmdate": 1606915763072, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper482/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper482/-/Official_Review"}}}, {"id": "_-oX3u5U5b", "original": null, "number": 1, "cdate": 1603687625993, "ddate": null, "tcdate": 1603687625993, "tmdate": 1606544508289, "tddate": null, "forum": "defQ1AG6IWn", "replyto": "defQ1AG6IWn", "invitation": "ICLR.cc/2021/Conference/Paper482/-/Official_Review", "content": {"title": "Official Blind Review #1", "review": "This paper proposed neighbor class consistency regularization together with an entropy-based weighting factor to tackle the problem of unsupervised domain adaptation. Another self class consistency regularization was further introduced to help training. The difference between \"neighbor class\" and \"self class\" is the positive pair selection, where \"neighbor class\" uses k-nearest neighbors as positive pairs, and \"self class\" uses an augmented version of the anchor itself.\n\nThe paper is generally well-written and easy to follow. Sufficient experiments are conducted. However, I think the quality of the paper is not publishable now due to the following weaknesses.\n\nMajor weaknesses:\n\n1. The core contribution of this paper is the class consistency loss (Eq. (5)&(7)) between the anchor and its neighbors, however, similar ideas have been investigated in existing works, e.g. [A, B]. But unfortunately, this paper even did not include them in the related works.\n\n\n\n2. I think the work lacks novelty. The memory bank in Sec 3.3 (1) is similar to [C, D] and the data augmentation rules in Sec 3.4 are similar to [E]. The consistency loss is popular in knowledge distillation tasks. In my point of view, the paper heavily borrowed techniques from existing works.  The author should at least acknowledge the relations between the paper and these works.\n\n\n\n3. It's somewhat counter-intuitive that the paper required the similarity between the anchor and its augmented version to be closer than the similarity between the anchor and its nearest neighbor (Eq. (9)). A situation may exist: the augmentation is very heavy while the nearest neighbor shows very similar visual features with the anchor.\nThe author has shown the effectiveness of Eq. (9) in Table 2, namely \"FR\". However, it's not convincing enough, I recommend to evaluate \"ENC + SC + SP (NC-SP)\", i.e. removing \"FR\" from the whole model.\n\n\nMinor weaknesses:\n\n4. The paper claimed its state-of-the-art performances on various benchmarks. But it missed some previous methods, e.g. [F]. The paper achieved even inferior performance than [F] on the VisDA17 benchmark, i.e. 86.2 in this paper v.s. 87.2 in [F].\n\n5. As I mentioned in weakness-3, it's better to use \"minus\" rather than \"plus\" in ablation studies.\n\n6. More hyper-parameter analysis would help improve paper quality. For example, as described in Sec 4.2, the author adopted different $\\lambda$ values for different datasets. It's better to show how much does the value of $\\lambda$ impact the final performance.\n\n\n\n[A] Regularizing Class-wise Predictions via Self-knowledge Distillation. CVPR 2020.\n\n[B] Invariance Matters: Exemplar Memory for Domain Adaptive Person Re-identification. CVPR 2019.\n\n[C] Unsupervised Feature Learning via Non-Parametric Instance Discrimination. CVPR 2018.\n\n[D] Momentum Contrast for Unsupervised Visual Representation Learning. CVPR 2020.\n\n[E] A Simple Framework for Contrastive Learning of Visual Representations. ICML 2020.\n\n[F] Contrastive Adaptation Network for Unsupervised Domain Adaptation. CVPR 2019.\n\n\n=========================================================================================================\n\nUpdate:\n\nThanks for the authors' response. But unfortunately, the most significant concern has not been addressed well, which is regarding of the paper's novelty. I thought the contributions of this work are incremental, and the authors' rebuttal did not convince me well. Btw, the authors claimed that [A] used KL-divergence while their work used mutual information maximization, however, minimizing KL-divergence is actually one kind of mutual information maximization. What's more, the authors even did not clearly point out which kind of mutual information maximization they used in either the manuscript or the rebuttal. Thanks again for the authors' efforts, but I choose to maintain my original score.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper482/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper482/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neighbor Class Consistency on Unsupervised Domain Adaptation", "authorids": ["liu.chang6@northeastern.edu", "~Kai_Li3", "~Yun_Fu1"], "authors": ["Chang Liu", "Kai Li", "Yun Fu"], "keywords": ["Unsupervised domain adaptation", "Consistency regularization", "Neighbor"], "abstract": "Unsupervised domain adaptation (UDA) is to make predictions for unlabeled data in a target domain with labeled data from source domain available. Recent advances exploit entropy minimization and self-training to align the feature of two domains. However, as decision boundary is largely biased towards source data, class-wise pseudo labels generated by target predictions are usually very noisy, and trusting those noisy supervisions might potentially deteriorate the intrinsic target discriminative feature. Motivated by agglomerative clustering which assumes that features in the near neighborhood should be clustered together, we observe that target features from source pre-trained model are highly intrinsic discriminative and have a high probability of sharing the same label with their neighbors. Based on those observations, we propose a simple but effective method to impose Neighbor Class Consistency on target features to preserve and further strengthen the intrinsic discriminative nature of target data while regularizing the unified classifier less biased towards source data. We also introduce an entropy-based weighting scheme to help our framework more robust to the potential noisy neighbor supervision. We conduct ablation studies and extensive experiments on three UDA image classification benchmarks. Our method outperforms all existing UDA state-of-the-art. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|neighbor_class_consistency_on_unsupervised_domain_adaptation", "pdf": "/pdf/9f7b790dc1004f0c42d1c2507e9f564f7ebaa6f3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=crc6I6R8c", "_bibtex": "@misc{\nliu2021neighbor,\ntitle={Neighbor Class Consistency on Unsupervised Domain Adaptation},\nauthor={Chang Liu and Kai Li and Yun Fu},\nyear={2021},\nurl={https://openreview.net/forum?id=defQ1AG6IWn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "defQ1AG6IWn", "replyto": "defQ1AG6IWn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper482/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538142179, "tmdate": 1606915763072, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper482/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper482/-/Official_Review"}}}, {"id": "dIVUNMUFYtq", "original": null, "number": 8, "cdate": 1606285488496, "ddate": null, "tcdate": 1606285488496, "tmdate": 1606293968792, "tddate": null, "forum": "defQ1AG6IWn", "replyto": "_-oX3u5U5b", "invitation": "ICLR.cc/2021/Conference/Paper482/-/Official_Comment", "content": {"title": "Response to Q1 and Q2.", "comment": "`R1-Q1`:   ...similar ideas have been investigated in existing works, e.g. [A, B].\n\n`R1-A1`:  **Compared to [A]**,  even though we both explore the class-wise prediction consistency, we want to kindly specify the two major differences as follows:\n\n1) **Problem setting and the choice of positive samples are different**: [A] choose the two different samples with the same label and enforce the class-wise prediction consistency between these two samples. So, [A] is essentially supervised learning problem. In contrast, we do not know the label for target data in the UDA setting. Instead, we choose a target sample with its self-augmentation and neighbors to be class consistent which is novel in the UDA domain. \n\n2) **Objective function is different while ours is a better objective to enforce class consistency**:  Additionally, [A] utilizes KL divergence as an objective function for class-wise consistency while we use mutual information maximization as objectives. To demonstrate the effectiveness of our loss function with mutual information (MI), we replace the MI loss term with the KL divergence in our ENC experiment for comparison and term it as CS-KD. As table 6 from the appendix shows, using MI as an objective function achieves much better performance than KL divergence by  9.6%.\n\n| Method | A->W |\n|-|-|\n| CS-KD [A] | 84.5 | \n| ENC (ours) | 94.1 | \n\n`R1-Q2`: I think the work lacks novelty. The memory bank in Sec 3.3 (1) is similar to [C, D] and the data augmentation rules in Sec 3.4 are similar to [E]. The paper heavily borrowed techniques from existing works. The author should at least acknowledge the relations between the paper and these works.\n\n`R1-A2`: Thanks for your valuable comments. We would like to kindly argue the difference to [C, D] in terms of the memory bank and data augmentation. Further, we want to emphasize our novelty is non-trivial in the UDA domain. \n1) **Functionality of memory bank is different**:As we reply in A1, [B,C,D] all utilize memory bank as an instance-level classifier which consists of the features of unlabeled samples. In contrast, we are solving a close-set classification problem in UDA where the source classifier is applicable. Therefore, we just utilize the memory bank for computational saving purposes. Specifically, we use it as a lookup table to retrieve the neighbor features given an anchor, feed them into the classifier, and computing the consistency loss. \n\n2) **The objective function of utilizing data augmentation is different**: We utilize Mutual information as an objective function to enforce class prediction consistency between anchors and self-augmentation while [E] utilizes the cross-entropy as an objective to enforce feature representation between self-augmentations being invariant. Further, we emphasize that our major contribution is on neighbor class consistency where self-augmentation is a special case. To show the performance self class consistency only, we add more ablation studies in table 2 for SC only. We can see that SC alone can achieve a decent performance compared to source only models but is inferior to Vanilla VNC by 0.6%. Therefore, our major components for boosting the performance are still neighbor class consistency. \n\n3) **Summary of our novelty**:  Motivated by the self-training methods which generally suffer from the class-wise noisy pseudo labels, we conduct the first attempt to leverage the more clean pairwise pseudo label in the closed-set UDA problem.  To differentiate from the related work where the classifier is missing, our major contribution is on providing a regularization on the shared classifier to make it less biased toward source data by introducing neighbor class consistency via Mutual information maximization. We demonstrate the neighbor class consistency is a more important factor than neighbor feature invariance in Table 6. We also introduce several important modules such as ENC to alleviate wrong supervision, FR to enforce the feature ranking relations, SP to initialize a good feature representation. \nAs you notice that consistency loss is popular and important in both self-distillation, semi/unsupervised learning problems, the majority of previous works [A, C,D,E,G,H] are either utilizing different forms of self-supervision (data augmentation) in semi/unsupervised setting or utilizing label to explore positive samples given anchors in self-distillation (supervised) setting. In contrast, our method is novel in terms of leveraging neighbor pairwise supervision in an unsupervised setting. \n\n\n[G] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii.  Virtual adversarial training: a regularization method for supervised and semi-supervised learning. TPAMI2018\n\n[H] Seungmin  Lee,  Dongwan  Kim,  Namil  Kim,  and  Seong-Gyun  Jeong.   Drop  to  adapt:  Learning discriminative features for unsupervised domain adaptation, ICCV2019\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper482/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper482/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neighbor Class Consistency on Unsupervised Domain Adaptation", "authorids": ["liu.chang6@northeastern.edu", "~Kai_Li3", "~Yun_Fu1"], "authors": ["Chang Liu", "Kai Li", "Yun Fu"], "keywords": ["Unsupervised domain adaptation", "Consistency regularization", "Neighbor"], "abstract": "Unsupervised domain adaptation (UDA) is to make predictions for unlabeled data in a target domain with labeled data from source domain available. Recent advances exploit entropy minimization and self-training to align the feature of two domains. However, as decision boundary is largely biased towards source data, class-wise pseudo labels generated by target predictions are usually very noisy, and trusting those noisy supervisions might potentially deteriorate the intrinsic target discriminative feature. Motivated by agglomerative clustering which assumes that features in the near neighborhood should be clustered together, we observe that target features from source pre-trained model are highly intrinsic discriminative and have a high probability of sharing the same label with their neighbors. Based on those observations, we propose a simple but effective method to impose Neighbor Class Consistency on target features to preserve and further strengthen the intrinsic discriminative nature of target data while regularizing the unified classifier less biased towards source data. We also introduce an entropy-based weighting scheme to help our framework more robust to the potential noisy neighbor supervision. We conduct ablation studies and extensive experiments on three UDA image classification benchmarks. Our method outperforms all existing UDA state-of-the-art. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|neighbor_class_consistency_on_unsupervised_domain_adaptation", "pdf": "/pdf/9f7b790dc1004f0c42d1c2507e9f564f7ebaa6f3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=crc6I6R8c", "_bibtex": "@misc{\nliu2021neighbor,\ntitle={Neighbor Class Consistency on Unsupervised Domain Adaptation},\nauthor={Chang Liu and Kai Li and Yun Fu},\nyear={2021},\nurl={https://openreview.net/forum?id=defQ1AG6IWn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "defQ1AG6IWn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper482/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper482/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper482/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper482/Authors|ICLR.cc/2021/Conference/Paper482/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper482/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870500, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper482/-/Official_Comment"}}}, {"id": "RrEian4gIyE", "original": null, "number": 7, "cdate": 1606284872980, "ddate": null, "tcdate": 1606284872980, "tmdate": 1606293865349, "tddate": null, "forum": "defQ1AG6IWn", "replyto": "_-oX3u5U5b", "invitation": "ICLR.cc/2021/Conference/Paper482/-/Official_Comment", "content": {"title": "Response to Q1", "comment": "`R1-Q1`:  The core contribution of this paper is the class consistency loss (Eq. (5)&(7)) between the anchor and its neighbors, however, similar ideas have been investigated in existing works, e.g. [A, B]. But unfortunately, this paper even did not include them in the related works.\n\n`R1-A1`: Thanks for pointing out those insightful papers. We will definitely cite them into our final version and explain the relations and differences between them.\n\n**Compared to [B]**:,  we agree that we share high-level similarity with [B] in terms of exploring neighbors information. But we would like to kindly argue the essential difference that  **they focus on enforcing the neighbor features invariance while we focus on the neighbor class prediction consistency**.  We also introduce a new entropy-based weighting scheme and Feature ranking constraint where [B] hasn't explored.  We will explain those core difference in detail and demonstrate that those difference will result in a large distinction in terms of performance as follows:\n\n1) **The problem setting, and methodology focus are different**:\n(a) **Unsupervised cross-domain person Re-ID**: The key difference between [B] and our method lies in the different settings between unsupervised domain adaptation (UDA) and Unsupervised cross-domain person RID (UDA RID). In UDA RID, the source and target data do not share any class label and the number of classes for target domain is unknown. As the source classifier cannot be applied to target data, [B] follows [C] to utilize an instance-level classifier for target data. Specifically, they store each target feature into the memory bank and use the memory bank as the instance-level classifier to make the target neighbors be close in the feature space in a multi-label learning manner. (Please refer to the Github repo of B for more details: https://github.com/zhunzhong07/ECN/blob/master/reid/loss/invariance.py). So, [B] essentially enforces the neighbor features to be close. \n(b) **Unsupervised domain adaptation**:In comparison, we can use the classifier trained on source to make class predictions for target data as the source and target data share the label space in UDA setting. We think the features of target neighbors are naturally close to each other in feature space but the source biased classifier fails to provide consistent class predictions between them. Therefore, in our point of view,  providing regularization on the classifier via our neighbor class consistency loss is more important than enforcing neighbor feature invariance from [B] in UDA problem. Further, we claim our main contribution is on providing a new regularization method on the shared classifier to make it less biased toward source data. Even though we share conceptual similarity with [B] by using neighbor information, our method is essentially different from [B] in terms of the functionality of methods.  \n\n\n2) **Our method significantly outperforms [B] in UDA setting**: To quantitatively demonstrate the difference between neighbor features invariance and neighbor class prediction consistency, we follow the official github repo of [B] at https://github.com/zhunzhong07/ECN/blob/master/reid/loss/invariance.py to implement the neighbor features invariance idea in UDA setting on Office31 dataset A->W task.  From the table 6 in appendix, we can see that our method outperforms them (ECN) by a significant margin (6.2%). It demonstrates that providing regularization on the source biased classifier via neighbor class consistency is more effective than enforcing neighbor features invariance which it is already naturally preserved to some degree. \n| Method | A->W |\n|-|-|\n| ECN [B]| 87.9| \n| ENC (ours) | 94.1 | \n\n3)**The loss design and functionality of the memory bank are different**: From the loss function perspective, our method is also different from [B]. We enforce the neighbor class consistency via maximizing the mutual information between class predictions of anchor and neighbor. In comparison, [B] utilizes the standard cross-entropy with multi-label ground truth. \nAlso, though we both utilize the memory bank, the functionality of the memory bank is different too. [B] utilize memory bank as instance-level classifier while we just use it as a lookup table to retrieve the target neighbor features for computing loss and save the computational time for the inference of those neighbor samples.\n\n(To be continued on the next page.)\n\n[A] Regularizing Class-wise Predictions via Self-knowledge Distillation. CVPR 2020.\n\n[B] Invariance Matters: Exemplar Memory for Domain Adaptive Person Re-identification. CVPR 2019.\n\n[C] Unsupervised Feature Learning via Non-Parametric Instance Discrimination. CVPR 2018.\n\n[D] Momentum Contrast for Unsupervised Visual Representation Learning. CVPR 2020.\n\n[E] A Simple Framework for Contrastive Learning of Visual Representations. ICML 2020.\n\n[F] Contrastive Adaptation Network for Unsupervised Domain Adaptation. CVPR 2019."}, "signatures": ["ICLR.cc/2021/Conference/Paper482/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper482/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neighbor Class Consistency on Unsupervised Domain Adaptation", "authorids": ["liu.chang6@northeastern.edu", "~Kai_Li3", "~Yun_Fu1"], "authors": ["Chang Liu", "Kai Li", "Yun Fu"], "keywords": ["Unsupervised domain adaptation", "Consistency regularization", "Neighbor"], "abstract": "Unsupervised domain adaptation (UDA) is to make predictions for unlabeled data in a target domain with labeled data from source domain available. Recent advances exploit entropy minimization and self-training to align the feature of two domains. However, as decision boundary is largely biased towards source data, class-wise pseudo labels generated by target predictions are usually very noisy, and trusting those noisy supervisions might potentially deteriorate the intrinsic target discriminative feature. Motivated by agglomerative clustering which assumes that features in the near neighborhood should be clustered together, we observe that target features from source pre-trained model are highly intrinsic discriminative and have a high probability of sharing the same label with their neighbors. Based on those observations, we propose a simple but effective method to impose Neighbor Class Consistency on target features to preserve and further strengthen the intrinsic discriminative nature of target data while regularizing the unified classifier less biased towards source data. We also introduce an entropy-based weighting scheme to help our framework more robust to the potential noisy neighbor supervision. We conduct ablation studies and extensive experiments on three UDA image classification benchmarks. Our method outperforms all existing UDA state-of-the-art. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|neighbor_class_consistency_on_unsupervised_domain_adaptation", "pdf": "/pdf/9f7b790dc1004f0c42d1c2507e9f564f7ebaa6f3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=crc6I6R8c", "_bibtex": "@misc{\nliu2021neighbor,\ntitle={Neighbor Class Consistency on Unsupervised Domain Adaptation},\nauthor={Chang Liu and Kai Li and Yun Fu},\nyear={2021},\nurl={https://openreview.net/forum?id=defQ1AG6IWn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "defQ1AG6IWn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper482/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper482/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper482/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper482/Authors|ICLR.cc/2021/Conference/Paper482/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper482/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870500, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper482/-/Official_Comment"}}}, {"id": "Ix2xI2nJd_l", "original": null, "number": 9, "cdate": 1606285824310, "ddate": null, "tcdate": 1606285824310, "tmdate": 1606291043939, "tddate": null, "forum": "defQ1AG6IWn", "replyto": "_-oX3u5U5b", "invitation": "ICLR.cc/2021/Conference/Paper482/-/Official_Comment", "content": {"title": "Response to Q3, Q4 and Q5", "comment": "`R1-Q3`:  It's somewhat counter-intuitive that the paper required the similarity between the anchor and its augmented version to be closer than the similarity between the anchor and its nearest neighbor (Eq. (9)). A situation may exist: the augmentation is very heavy while the nearest neighbor shows very similar visual features with the anchor. The author has shown the effectiveness of Eq. (9) in Table 2, namely \"FR\". However, it's not convincing enough, I recommend to evaluate \"ENC + SC + SP (NC-SP)\", i.e. removing \"FR\" from the whole model.\n\n`R1-A3`:   Thanks for your constructive comments. We add \"ENC-SC- SP\" removing \u201cFR\u201d in ablation table 2. We can see that \u201cENC-SC-SP\u201d achieves comparable performance to \u201cENC-SC-FR\u201d on office-31 but with minor performance differences for each task. Compared to our ENC-SC-FR-SP (NC-SP), we could see that FR indeed plays an important role in boosting performance. We will elaborate on the possible reason as follows:\nTo summarize, there are two situations for the 1st neighbor sample:\n1) When the first neighbor samples are negative but visually similar to the anchors, introducing this feature ranking regularization benefits the feature representation learning by ensuring the positive samples (self-augmentation) rank higher than the negative samples (the first neighbors) in feature space. This generally makes sense. \n2) When the first neighbor samples are positive and visually similar to the anchors as you say, it can also enforce an inductive bias on model training to rank the positive samples with pre-defined variations (such as cropping, grayscale, and color distortion) higher than the positive samples with unknown variations (the first neighbors). To make sure that the self-augmentation is not too far from the first neighbors in this case, we should avoid a large triplet margin in Eqn. 9. Note that we add hyperparameter analysis in section 4.5 and Figure 4(b) for the triplet margin. \n\n`R1-Q4`: The paper claimed its state-of-the-art performances on various benchmarks. But it missed some previous methods, e.g. [F]. The paper achieved even inferior performance than [F] on the VisDA17 benchmark, i.e. 86.2 in this paper v.s. 87.2 in [F].\n\n`R1-A4`:  Thanks for pointing out this insightful paper. We agree that our method is 1% behind [F] on VisDA17 benchmark. We also add more comparisons to [F] on office31 and ImageCLEF-DA in table 4 and table 5. We can find that our method is 0.4% better on Office31 and 1.9% better on ImageCLEF-DA than [F].  In general, our method is much simpler than [F] in terms of optimization where they perform alternative optimization between updating the target label hypothesis through k-mean clustering and adapting feature representations through back-propagation. \n\nMore importantly, we acknowledge that their inter-class domain discrepancy is orthogonal to our method. We believe that combining with [F] could potentially push our performance to a higher number. \n\n`R1-Q5`:  As I mentioned in weakness-3, it's better to use \"minus\" rather than \"plus\" in ablation studies\n\n`R1-A5`:  Thank you for pointing this out. We will correct this in the final version. \n\n`R1-Q6`:  More hyper-parameter analysis would help improve paper quality...\n\n`R1-A6`:  Thanks for your constructive suggestions.  We add the hyper-parameter sensitivity analysis on lambda and triplet margin in section 4.5. Please refer to the text and figure 4 for more details. Overall, our method is robust to the local changes of lambda, and setting the triplet margin too large will harm the performance. \n\n[F] Contrastive Adaptation Network for Unsupervised Domain Adaptation. CVPR 2019.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper482/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper482/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neighbor Class Consistency on Unsupervised Domain Adaptation", "authorids": ["liu.chang6@northeastern.edu", "~Kai_Li3", "~Yun_Fu1"], "authors": ["Chang Liu", "Kai Li", "Yun Fu"], "keywords": ["Unsupervised domain adaptation", "Consistency regularization", "Neighbor"], "abstract": "Unsupervised domain adaptation (UDA) is to make predictions for unlabeled data in a target domain with labeled data from source domain available. Recent advances exploit entropy minimization and self-training to align the feature of two domains. However, as decision boundary is largely biased towards source data, class-wise pseudo labels generated by target predictions are usually very noisy, and trusting those noisy supervisions might potentially deteriorate the intrinsic target discriminative feature. Motivated by agglomerative clustering which assumes that features in the near neighborhood should be clustered together, we observe that target features from source pre-trained model are highly intrinsic discriminative and have a high probability of sharing the same label with their neighbors. Based on those observations, we propose a simple but effective method to impose Neighbor Class Consistency on target features to preserve and further strengthen the intrinsic discriminative nature of target data while regularizing the unified classifier less biased towards source data. We also introduce an entropy-based weighting scheme to help our framework more robust to the potential noisy neighbor supervision. We conduct ablation studies and extensive experiments on three UDA image classification benchmarks. Our method outperforms all existing UDA state-of-the-art. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|neighbor_class_consistency_on_unsupervised_domain_adaptation", "pdf": "/pdf/9f7b790dc1004f0c42d1c2507e9f564f7ebaa6f3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=crc6I6R8c", "_bibtex": "@misc{\nliu2021neighbor,\ntitle={Neighbor Class Consistency on Unsupervised Domain Adaptation},\nauthor={Chang Liu and Kai Li and Yun Fu},\nyear={2021},\nurl={https://openreview.net/forum?id=defQ1AG6IWn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "defQ1AG6IWn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper482/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper482/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper482/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper482/Authors|ICLR.cc/2021/Conference/Paper482/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper482/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870500, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper482/-/Official_Comment"}}}, {"id": "Es6lwQoMeSl", "original": null, "number": 6, "cdate": 1606284152068, "ddate": null, "tcdate": 1606284152068, "tmdate": 1606289433387, "tddate": null, "forum": "defQ1AG6IWn", "replyto": "Yf3eF5g9Aas", "invitation": "ICLR.cc/2021/Conference/Paper482/-/Official_Comment", "content": {"title": "Response to Q1, Q2 and Q3", "comment": "`R4-Q1`:  Compared with VNC, the improvement of ENC is very limited. Whether the authors have repeated the experiments several times. It is better to show the variance of the results in Table 2, which may be more suitable to demonstrate the effectiveness of ENC.\n\n`R4-A1`: Thanks for your suggestion. We include the variance of the results in Table 2 in the final version. As you can see from table 2,  ENC consistently outperforms VNC on all 6 tasks with very small variance. It is also worth mentioning that ENC is more robust to neighborhood size K than VNC from figure 3(b), and it can alleviate the negative effect from the false neighbor supervision by assigning less loss weight to them.\n\n`R4-Q2`:  In Eq.(9), it forces that the anchor samples should be closed to the neighbor samples by comparing with the distance between the anchor sample and the augmented sample. What is the advantage of such a strategy compared with triplet loss? It is also better to show the comparison in the experiments.\n\n`R4-A2`: Sorry for causing the confusion. In Eq. 9, we enforce the anchor samples to be closer to their augmented samples than their first neighbors in features space. Specifically, we use triplet loss to introduce this feature ranking constraint between anchors, augmented samples, and anchors\u2019 first neighbors. \n\nFor the advantage of such a strategy, \n1)  When the first neighbors given target anchors are negative  (do not share the same label with anchors),  introducing this feature ranking regularization benefits the feature representation learning by ensuring the positive samples (self augmentation) rank higher than the negative samples in feature space.\n\n2)  When the first neighbors given target anchors are positive,  it can also enforce a inductive bias on model training to rank the positive samples (self augmentation) with pre-defined variations (such as cropping, grayscale, and color distortion) higher than the positive samples with unknown variations (the first neighbors). \n\nFrom table 2, we could observe that adding FR improves performance by ~0.5 % on office31. We also add hyper-parameter sensitivity analysis on triplet Marin in fig 4 and section 4.5. \n\n\n`R4-Q3`:  For the neighbors, when k is equal to 1, the proposed method achieved the best performance. Hence, the neighbors with the minimum distance could not guarantee that they share the same labels. How does the proposed method estimate confidence?\n\n`R4-A3`: Thanks for your question. As you notice, the neighbors with the minimum distance in feature space could not guarantee that they share the same labels in the source pre-trained model. The main reason is that the original classifier is largely biased towards the source data.  Based on this motivation, our main contribution is to regularize the classifier less biased by making the class predictions of target neighbors being consistent. \n\nFor confidence estimation, our ENC (Eq 6,7) introduces an entropy-based weighting scheme (EW) to assign different loss weight on different neighbor pairs given an anchor. This loss weight can be considered as a measure of confidence estimation. After applying our EW, the negative neighbor pairs will be assigned less loss weight because they have low confidence while positive neighbor pairs will be assigned larger loss weight because they have high confidence. Please also refer to the fig 3(c) which visualizes the loss weight for positive and negative pairs. \n\nFrom fig 3(b), our method with the entropy-based weighting is robust to the size of k. But for simplicity, we set k equals to 1 for all experiments from table 3-5. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper482/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper482/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neighbor Class Consistency on Unsupervised Domain Adaptation", "authorids": ["liu.chang6@northeastern.edu", "~Kai_Li3", "~Yun_Fu1"], "authors": ["Chang Liu", "Kai Li", "Yun Fu"], "keywords": ["Unsupervised domain adaptation", "Consistency regularization", "Neighbor"], "abstract": "Unsupervised domain adaptation (UDA) is to make predictions for unlabeled data in a target domain with labeled data from source domain available. Recent advances exploit entropy minimization and self-training to align the feature of two domains. However, as decision boundary is largely biased towards source data, class-wise pseudo labels generated by target predictions are usually very noisy, and trusting those noisy supervisions might potentially deteriorate the intrinsic target discriminative feature. Motivated by agglomerative clustering which assumes that features in the near neighborhood should be clustered together, we observe that target features from source pre-trained model are highly intrinsic discriminative and have a high probability of sharing the same label with their neighbors. Based on those observations, we propose a simple but effective method to impose Neighbor Class Consistency on target features to preserve and further strengthen the intrinsic discriminative nature of target data while regularizing the unified classifier less biased towards source data. We also introduce an entropy-based weighting scheme to help our framework more robust to the potential noisy neighbor supervision. We conduct ablation studies and extensive experiments on three UDA image classification benchmarks. Our method outperforms all existing UDA state-of-the-art. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|neighbor_class_consistency_on_unsupervised_domain_adaptation", "pdf": "/pdf/9f7b790dc1004f0c42d1c2507e9f564f7ebaa6f3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=crc6I6R8c", "_bibtex": "@misc{\nliu2021neighbor,\ntitle={Neighbor Class Consistency on Unsupervised Domain Adaptation},\nauthor={Chang Liu and Kai Li and Yun Fu},\nyear={2021},\nurl={https://openreview.net/forum?id=defQ1AG6IWn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "defQ1AG6IWn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper482/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper482/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper482/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper482/Authors|ICLR.cc/2021/Conference/Paper482/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper482/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870500, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper482/-/Official_Comment"}}}, {"id": "TAwTlqvsJ7a", "original": null, "number": 3, "cdate": 1606282766868, "ddate": null, "tcdate": 1606282766868, "tmdate": 1606289325405, "tddate": null, "forum": "defQ1AG6IWn", "replyto": "dqzzzWSh6Nc", "invitation": "ICLR.cc/2021/Conference/Paper482/-/Official_Comment", "content": {"title": "Response to Question 2 , 3 and 4", "comment": "`R3-Q2`: NC-SP outperforms state-of-the-art methods with only a small margin. It is suggested to add some comparisons to Table 3-5 that only the proposed VNC or ENC is used without other tricks.\n\n`R3-A2`: It is also worth mentioning that we think Self class consistency (SC), Feature ranking between self-augmentation and the first neighbor (FR), SP (self-consistency pre-training) are all important components of our method. Specifically, SC is the special case of Neighbor class consistency where the nearest neighbor given an anchor is itself. FR provides a regularization constraint in feature space to ensure the intrinsic discriminativeness.  Similarly, SP could also help result in a better feature representation initialization for more precise neighborhood discovery. \n\n`R3-Q3`: It is suggested to evaluate the influence of hyper-parameter \\lambda in Eq. (10) and (11).\n\n`R3-A3`:  Thanks a lot for your constructive suggestion. We add the hyper-parameter sensitivity analysis on lambda and triplet margin in section 4.5. Please refer to the final submission and figure 4 for more details. Overall, our method is robust to the local changes of lambda, and setting the triplet margin too large will harm the performance. \n\n`R3-Q4`: It is suggested to evaluate neighbor size K on more datasets, in order to validate the association of K with the target dataset and number of classes.\n\n`R3-A4`: Thanks for pointing it out. We add an additional figure 5 in appendix section A1 for analyzing neighbor size K on all tasks in Office31 (3 datasets and 6 tasks). We can conclude that our method is very robust to the size of K where k=1 and k=5 have similar performance. \n\n\n\n[1] Ge, Yixiao, Dapeng Chen, and Hongsheng Li. \"Mutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification.\" ICLR (2020).\n\n[2] K. G. Dizaji, A. Herandi, C. Deng, W. Cai, and H. Huang. Deep clustering via joint convolutional autoencoder embedding and relative entropy minimization. ICCV 2017\n\n[3] G. Kang, L. Jiang, Y. Yang, and A. G. Hauptmann. Contrastive adaptation network for unsupervised domain adaptation, CVPR2019"}, "signatures": ["ICLR.cc/2021/Conference/Paper482/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper482/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neighbor Class Consistency on Unsupervised Domain Adaptation", "authorids": ["liu.chang6@northeastern.edu", "~Kai_Li3", "~Yun_Fu1"], "authors": ["Chang Liu", "Kai Li", "Yun Fu"], "keywords": ["Unsupervised domain adaptation", "Consistency regularization", "Neighbor"], "abstract": "Unsupervised domain adaptation (UDA) is to make predictions for unlabeled data in a target domain with labeled data from source domain available. Recent advances exploit entropy minimization and self-training to align the feature of two domains. However, as decision boundary is largely biased towards source data, class-wise pseudo labels generated by target predictions are usually very noisy, and trusting those noisy supervisions might potentially deteriorate the intrinsic target discriminative feature. Motivated by agglomerative clustering which assumes that features in the near neighborhood should be clustered together, we observe that target features from source pre-trained model are highly intrinsic discriminative and have a high probability of sharing the same label with their neighbors. Based on those observations, we propose a simple but effective method to impose Neighbor Class Consistency on target features to preserve and further strengthen the intrinsic discriminative nature of target data while regularizing the unified classifier less biased towards source data. We also introduce an entropy-based weighting scheme to help our framework more robust to the potential noisy neighbor supervision. We conduct ablation studies and extensive experiments on three UDA image classification benchmarks. Our method outperforms all existing UDA state-of-the-art. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|neighbor_class_consistency_on_unsupervised_domain_adaptation", "pdf": "/pdf/9f7b790dc1004f0c42d1c2507e9f564f7ebaa6f3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=crc6I6R8c", "_bibtex": "@misc{\nliu2021neighbor,\ntitle={Neighbor Class Consistency on Unsupervised Domain Adaptation},\nauthor={Chang Liu and Kai Li and Yun Fu},\nyear={2021},\nurl={https://openreview.net/forum?id=defQ1AG6IWn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "defQ1AG6IWn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper482/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper482/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper482/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper482/Authors|ICLR.cc/2021/Conference/Paper482/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper482/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870500, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper482/-/Official_Comment"}}}, {"id": "nfNJozyll_2", "original": null, "number": 2, "cdate": 1606282536996, "ddate": null, "tcdate": 1606282536996, "tmdate": 1606289268352, "tddate": null, "forum": "defQ1AG6IWn", "replyto": "dqzzzWSh6Nc", "invitation": "ICLR.cc/2021/Conference/Paper482/-/Official_Comment", "content": {"title": "Response to Question 1 and 2", "comment": "`R3-Q1`: The idea of neighborhood consistency in UDA is not very significant. It lies on the assumption that target features from source pre-trained models are highly intrinsic discriminative...\n\n`R3-A1`: We sincerely appreciate your insightful comments.  We agree that the domain discrepancy has a large impact on the performance of our method as you can see that the performance on VisDA17 dataset (in Table 5), where the domain discrepancy is very large,  is much lower than the Office31 dataset  (in Table 3). But we want to kindly argue from the following three points.\n1) **Our method is more robust to domain discrepancy w.r.t performance**:The existing methods such as adversarial-based domain adaptation methods or self-training based methods also suffer from performance drop when domain discrepancy is large. e.g in VisDa17. Compared to those baselines in the VisDa dataset, our method still outperforms them by a significant margin. It demonstrates that our method is robust and can achieve consistent better performance in datasets with different levels of domain discrepancy than the baselines. \n2) **Existing Self-training methods and ours are based on the similar discriminative assumption of source model while ours relies on a looser assumption which is easier to achieve**: Specifically, our method shares similar motivation with Self-training methods in terms of leveraging the Pseudo label for unlabeled target data. In comparison, we utilize the pairwise pseudo label while they use class-wise pseudo labels. As source and target data share the label space in unsupervised domain adaptation, both two threads of methods follow the similar assumption that the feature representation has explored some degree of discriminativeness with the help of source data pre-training. But compared to the class-wise pseudo label which relies on a hard assumption that the target features are globally discriminative across all categories, our pairwise pseudo label relies on the looser assumption that the target features are locally intrinsic discriminative (target features share the same label with their k nearest neighbors). Therefore, we claim that our method is more robust to domain discrepancy than self-training methods with the looser assumption especially when k is small.  It is also worth noting that the self-training method [1] based on the source pre-trained model also achieves state-of-the-art performance in cross-domain person RE-ID tasks where the discrepancy is large. We think both self-training and our methods are effective in UDA and UDA person re-ID with further pseudo label refinement. \n3) **We proposed specific components to improve the discriminativeness of initial feature space**:To ensure the intrinsic discriminativeness of feature space, we propose the self-consistency loss pre-training(SP) and feature ranking between self-augmentation and the first neighbor. From Table 2, we can see that our method improves by a large margin with those important components.  \n\n`R3-Q2`: NC-SP outperforms state-of-the-art methods with only a small margin. It is suggested to add some comparisons to Table 3-5 that only the proposed VNC or ENC is used without other tricks.\n\n`R3-A2`: Thanks for your constructive advice. We would like to kindly argue that the improvement of our method on Table 3-5 is not trivial in UDA task. Also, it is worth noting that we introduce a simple and new way of utilizing the pseudo label while outperforming the state-of-the-art methods which are generally more complex and usually combine several techniques together. For example, in table 3,  SRDC is based on a clustering method [2] with source sample selections. In table 4, ALP claims that it is orthogonal to existing methods and built on top of another paper \u201cCAN\u201d [3].\n\nAdditionally, we would like to add ENC as a comparison in three datasets for your reference in appendix Table 7 and Table 8. Thanks for pointing it out. Note that the ablation table 2 and table 3 both refer to the Office-31 dataset and thus you can compare the ENC result in table 2 directly with other state-of-the-art methods in table 3. We could observe that our method with ENC only achieves the third best performance (exclude NC-SP) in Office-31, top 1 performance in ImageCLEF-DA and the second best performance in VisDA17.  Compared to the self-training based methods (ENT, MRENT, SAFN+Ent), our ENC outperforms them by a large margin without bells and whistles. \n\n(To be continued on the next page.)\n\n[1] Ge, Yixiao, Dapeng Chen, and Hongsheng Li. \"Mutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification.\" ICLR (2020).\n\n[2] K. G. Dizaji, A. Herandi, C. Deng, W. Cai, and H. Huang. Deep clustering via joint convolutional autoencoder embedding and relative entropy minimization. ICCV 2017\n\n[3] G. Kang, L. Jiang, Y. Yang, and A. G. Hauptmann. Contrastive adaptation network for unsupervised domain adaptation, CVPR2019\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper482/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper482/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neighbor Class Consistency on Unsupervised Domain Adaptation", "authorids": ["liu.chang6@northeastern.edu", "~Kai_Li3", "~Yun_Fu1"], "authors": ["Chang Liu", "Kai Li", "Yun Fu"], "keywords": ["Unsupervised domain adaptation", "Consistency regularization", "Neighbor"], "abstract": "Unsupervised domain adaptation (UDA) is to make predictions for unlabeled data in a target domain with labeled data from source domain available. Recent advances exploit entropy minimization and self-training to align the feature of two domains. However, as decision boundary is largely biased towards source data, class-wise pseudo labels generated by target predictions are usually very noisy, and trusting those noisy supervisions might potentially deteriorate the intrinsic target discriminative feature. Motivated by agglomerative clustering which assumes that features in the near neighborhood should be clustered together, we observe that target features from source pre-trained model are highly intrinsic discriminative and have a high probability of sharing the same label with their neighbors. Based on those observations, we propose a simple but effective method to impose Neighbor Class Consistency on target features to preserve and further strengthen the intrinsic discriminative nature of target data while regularizing the unified classifier less biased towards source data. We also introduce an entropy-based weighting scheme to help our framework more robust to the potential noisy neighbor supervision. We conduct ablation studies and extensive experiments on three UDA image classification benchmarks. Our method outperforms all existing UDA state-of-the-art. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|neighbor_class_consistency_on_unsupervised_domain_adaptation", "pdf": "/pdf/9f7b790dc1004f0c42d1c2507e9f564f7ebaa6f3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=crc6I6R8c", "_bibtex": "@misc{\nliu2021neighbor,\ntitle={Neighbor Class Consistency on Unsupervised Domain Adaptation},\nauthor={Chang Liu and Kai Li and Yun Fu},\nyear={2021},\nurl={https://openreview.net/forum?id=defQ1AG6IWn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "defQ1AG6IWn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper482/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper482/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper482/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper482/Authors|ICLR.cc/2021/Conference/Paper482/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper482/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870500, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper482/-/Official_Comment"}}}, {"id": "tMoPBbduAzD", "original": null, "number": 5, "cdate": 1606283609625, "ddate": null, "tcdate": 1606283609625, "tmdate": 1606288670567, "tddate": null, "forum": "defQ1AG6IWn", "replyto": "WvVLaG4FUIK", "invitation": "ICLR.cc/2021/Conference/Paper482/-/Official_Comment", "content": {"title": "Response to Question 2, 3 and 4", "comment": "`R2-Q2`:  Section 4.2 says that the authors use different values for the hyperparameter, such as \u03bb, for each dataset. How do they tune them? Since the method is for UDA, there cannot be a validation set in the target domain. Therefore, different values of \u03bb seem unnatural. Alternatively, the authors should report the performance with different hyperparameter values not only for the number of neighbor samples in Fig. 3 (b) but also for  \u03bb and triplet margin m\n\n`R2-A2`:  Thanks for your constructive suggestions.  We add the hyper-parameter sensitivity analysis on lambda and triplet margin in section 4.5. Please refer to the text and figure 4 for more details. Overall, our method is robust to the local changes of lambda around 0.1, and setting the triplet margin too large such as 1 will harm the performance. \n\nAs you notice, we choose different \u03bb for office31 and VisDA17. For tuning those \u03bb, we are referring to the performance of the test dataset. Similarly to figure 4, we tuned the \u03bb from  { 0.1, 0.25, 0.5, 0.75} on VisDA17 and found that \u03bb=0.5 achieved the best performance with 86.2 %. Note that the performance of \u03bb=0.1 is 85.7 %. As the performance does not change a lot for those two choices of \u03bb,  we are safe to say that our method is still robust to the regional changes of hyper-parameter \u03bb in VisDA17.\n\n`R2-Q3`: How often do we need to update the memory bank Vt while a model is trained? Frequent updates of the memory would be time-consuming.\n\n`R2-A3`:Thanks for pointing this out. We update the memory bank at every epoch. But we would like to kindly argue that it will not introduce extra computational cost but more time-efficient. \n\n1) Before the training starts, we feed forward the entire dataset to the model, get the target features for each target id and store them in the memory bank Vt. Then we apply kNN on the memory bank Vt for neighborhood discovery in Section 3.3.1. Thus, we can associate each target id with its k nearest neighbors. \n\n2) When the training starts, we optimize our model in a batch-wise manner. Given the current batch, we extract the target features and retrieve their kNN features from the memory bank for computing loss. Note that we retrieve the kNN features directly from the memory bank instead of extracting them again from raw images by enlarging the batch size. Thus, using a memory bank will be k times faster than without it. After finishing the current batch update, we do not discard the target features of the current batch but use it to update (technically replace) the target features of the same id inside the memory bank. This process will not introduce any computational burden. \n\nThanks again for pointing this out. We will include this detail in the final submission. \n\n`R2-Q4`:Authors should insert a white space before a bracket and a citation; The format of References is messy; an abbreviation \"NC\" is used without an explanation; The blank Appendix section should be deleted.\n\n`R2-A4`:Thank you for pointing this out. We will correct this and make the references correct and consistent in the final version. \n\n[1]Ruijia Xu, Guanbin Li, Jihan Yang, and Liang Lin. Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation. In ICCV, 2019.\n\n[2] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc\u00b8ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. JMLR, 17\n\n[3]  Shaoan Xie, Zibin Zheng, Liang Chen, and Chuan Chen. Learning semantic representations for unsupervised domain adaptation. In ICML, 2018\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper482/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper482/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neighbor Class Consistency on Unsupervised Domain Adaptation", "authorids": ["liu.chang6@northeastern.edu", "~Kai_Li3", "~Yun_Fu1"], "authors": ["Chang Liu", "Kai Li", "Yun Fu"], "keywords": ["Unsupervised domain adaptation", "Consistency regularization", "Neighbor"], "abstract": "Unsupervised domain adaptation (UDA) is to make predictions for unlabeled data in a target domain with labeled data from source domain available. Recent advances exploit entropy minimization and self-training to align the feature of two domains. However, as decision boundary is largely biased towards source data, class-wise pseudo labels generated by target predictions are usually very noisy, and trusting those noisy supervisions might potentially deteriorate the intrinsic target discriminative feature. Motivated by agglomerative clustering which assumes that features in the near neighborhood should be clustered together, we observe that target features from source pre-trained model are highly intrinsic discriminative and have a high probability of sharing the same label with their neighbors. Based on those observations, we propose a simple but effective method to impose Neighbor Class Consistency on target features to preserve and further strengthen the intrinsic discriminative nature of target data while regularizing the unified classifier less biased towards source data. We also introduce an entropy-based weighting scheme to help our framework more robust to the potential noisy neighbor supervision. We conduct ablation studies and extensive experiments on three UDA image classification benchmarks. Our method outperforms all existing UDA state-of-the-art. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|neighbor_class_consistency_on_unsupervised_domain_adaptation", "pdf": "/pdf/9f7b790dc1004f0c42d1c2507e9f564f7ebaa6f3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=crc6I6R8c", "_bibtex": "@misc{\nliu2021neighbor,\ntitle={Neighbor Class Consistency on Unsupervised Domain Adaptation},\nauthor={Chang Liu and Kai Li and Yun Fu},\nyear={2021},\nurl={https://openreview.net/forum?id=defQ1AG6IWn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "defQ1AG6IWn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper482/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper482/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper482/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper482/Authors|ICLR.cc/2021/Conference/Paper482/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper482/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870500, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper482/-/Official_Comment"}}}, {"id": "8snB21kXWX", "original": null, "number": 4, "cdate": 1606283193304, "ddate": null, "tcdate": 1606283193304, "tmdate": 1606288574324, "tddate": null, "forum": "defQ1AG6IWn", "replyto": "WvVLaG4FUIK", "invitation": "ICLR.cc/2021/Conference/Paper482/-/Official_Comment", "content": {"title": "Response to Question 1", "comment": "`R2-Q1`:There is a related paper by Gu et al. (2020) [4] also focus on a pseudo labeling approach and discriminative distributions in the target domain. Additionally, the experimental results show that the performance is very close to that in this paper. Although the detailed method is different, the authors should have compared them qualitatively and quantitatively.\n\n`R2-A1`:Thanks for pointing out this insightful paper and we will definitely cite it in the revised version. We would like to kindly elaborate the distinct differences between Gu et al. (2020) and our NC-SP as follows.  \n\n1) Gu et al. (2020) projects images to a spherical feature space where domain alignment techniques are performed, while we focus on enforcing consistency constraints in a standard feature space. \n\n2) One of the domain alignment techniques in Gu et al. (2020) is based on pseudo labelling in the spherical space, which bears some similarity with NC-SP. But Gu et al. (2020) generates hard pseudo labels (one-hot vectors obtained by the argmax operator), where the incorrect pseudo ones may bring negative impact for the performance. This is exactly the problem which NC-SP aims to address. NC-SP utilizes  high-quality pairwise neighbor supervision. No hard pseudo labels are generated in the process. \n\n3) It is worth noting that Gu et al. (2020) combine several loss terms from prior literature such as conditional entropy minimization[1],  adversarial domain classification term [2], Semantic matching loss[3]. We list the comparison of Gu et al. (2020) and NC-SP as belows. We can see that NC-SP is close to Gu et al. (2020) in office-31 while better in ImageCLEF-DA. It is also worth mentioning that our NC-SP can achieve much better performance than \u201cGu et al. (2020) without using other external loss terms such as [1][3]\u201d. \n\n|  Method              |  Office-31  | ImageCLEF-DA | \n|-|-|-|\n| Gu et al. (2020)                  |91.1            |90.5|\n| Gu et al. (2020) wo [3]     | 90.2            | 90.1 | \n| Gu et al. (2020) wo [1][3]| 89.2            | 89.4 | \n| NC-SP                 | 91.0            |90.9|\n\n\n4) More importantly, our proposed method is orthogonal with the spherical alignment techniques proposed in Gu et al. (2020). We can apply the proposed consistent constraints in the spherical space to further mitigate the domain shifts, along with other techniques proposed in Gu et al. (2020) such as [1][3], which very likely contributes to even better performance.  \n\n[1]Ruijia Xu, Guanbin Li, Jihan Yang, and Liang Lin. Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation. In ICCV, 2019.\n\n[2] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc\u00b8ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. JMLR, 17\n\n[3]  Shaoan Xie, Zibin Zheng, Liang Chen, and Chuan Chen. Learning semantic representations for unsupervised domain adaptation. In ICML, 2018\n\n[4] Gu et al., Spherical Space Domain Adaptation with Robust Pseudo-label Loss. In CVPR, 2020.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper482/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper482/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neighbor Class Consistency on Unsupervised Domain Adaptation", "authorids": ["liu.chang6@northeastern.edu", "~Kai_Li3", "~Yun_Fu1"], "authors": ["Chang Liu", "Kai Li", "Yun Fu"], "keywords": ["Unsupervised domain adaptation", "Consistency regularization", "Neighbor"], "abstract": "Unsupervised domain adaptation (UDA) is to make predictions for unlabeled data in a target domain with labeled data from source domain available. Recent advances exploit entropy minimization and self-training to align the feature of two domains. However, as decision boundary is largely biased towards source data, class-wise pseudo labels generated by target predictions are usually very noisy, and trusting those noisy supervisions might potentially deteriorate the intrinsic target discriminative feature. Motivated by agglomerative clustering which assumes that features in the near neighborhood should be clustered together, we observe that target features from source pre-trained model are highly intrinsic discriminative and have a high probability of sharing the same label with their neighbors. Based on those observations, we propose a simple but effective method to impose Neighbor Class Consistency on target features to preserve and further strengthen the intrinsic discriminative nature of target data while regularizing the unified classifier less biased towards source data. We also introduce an entropy-based weighting scheme to help our framework more robust to the potential noisy neighbor supervision. We conduct ablation studies and extensive experiments on three UDA image classification benchmarks. Our method outperforms all existing UDA state-of-the-art. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|neighbor_class_consistency_on_unsupervised_domain_adaptation", "pdf": "/pdf/9f7b790dc1004f0c42d1c2507e9f564f7ebaa6f3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=crc6I6R8c", "_bibtex": "@misc{\nliu2021neighbor,\ntitle={Neighbor Class Consistency on Unsupervised Domain Adaptation},\nauthor={Chang Liu and Kai Li and Yun Fu},\nyear={2021},\nurl={https://openreview.net/forum?id=defQ1AG6IWn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "defQ1AG6IWn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper482/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper482/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper482/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper482/Authors|ICLR.cc/2021/Conference/Paper482/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper482/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870500, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper482/-/Official_Comment"}}}, {"id": "kEES7kVhVV2", "original": null, "number": 10, "cdate": 1606285991136, "ddate": null, "tcdate": 1606285991136, "tmdate": 1606285991136, "tddate": null, "forum": "defQ1AG6IWn", "replyto": "defQ1AG6IWn", "invitation": "ICLR.cc/2021/Conference/Paper482/-/Official_Comment", "content": {"title": "Appreciate your valuable comments and time", "comment": "We sincerely appreciate your valuable comments that help us improve the paper. "}, "signatures": ["ICLR.cc/2021/Conference/Paper482/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper482/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neighbor Class Consistency on Unsupervised Domain Adaptation", "authorids": ["liu.chang6@northeastern.edu", "~Kai_Li3", "~Yun_Fu1"], "authors": ["Chang Liu", "Kai Li", "Yun Fu"], "keywords": ["Unsupervised domain adaptation", "Consistency regularization", "Neighbor"], "abstract": "Unsupervised domain adaptation (UDA) is to make predictions for unlabeled data in a target domain with labeled data from source domain available. Recent advances exploit entropy minimization and self-training to align the feature of two domains. However, as decision boundary is largely biased towards source data, class-wise pseudo labels generated by target predictions are usually very noisy, and trusting those noisy supervisions might potentially deteriorate the intrinsic target discriminative feature. Motivated by agglomerative clustering which assumes that features in the near neighborhood should be clustered together, we observe that target features from source pre-trained model are highly intrinsic discriminative and have a high probability of sharing the same label with their neighbors. Based on those observations, we propose a simple but effective method to impose Neighbor Class Consistency on target features to preserve and further strengthen the intrinsic discriminative nature of target data while regularizing the unified classifier less biased towards source data. We also introduce an entropy-based weighting scheme to help our framework more robust to the potential noisy neighbor supervision. We conduct ablation studies and extensive experiments on three UDA image classification benchmarks. Our method outperforms all existing UDA state-of-the-art. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|neighbor_class_consistency_on_unsupervised_domain_adaptation", "pdf": "/pdf/9f7b790dc1004f0c42d1c2507e9f564f7ebaa6f3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=crc6I6R8c", "_bibtex": "@misc{\nliu2021neighbor,\ntitle={Neighbor Class Consistency on Unsupervised Domain Adaptation},\nauthor={Chang Liu and Kai Li and Yun Fu},\nyear={2021},\nurl={https://openreview.net/forum?id=defQ1AG6IWn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "defQ1AG6IWn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper482/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper482/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper482/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper482/Authors|ICLR.cc/2021/Conference/Paper482/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper482/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870500, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper482/-/Official_Comment"}}}, {"id": "Yf3eF5g9Aas", "original": null, "number": 2, "cdate": 1603775953362, "ddate": null, "tcdate": 1603775953362, "tmdate": 1605024679259, "tddate": null, "forum": "defQ1AG6IWn", "replyto": "defQ1AG6IWn", "invitation": "ICLR.cc/2021/Conference/Paper482/-/Official_Review", "content": {"title": "In this paper, a simple but effective method to impose Neighbor Class Consistency on target features and to learn the discriminative features is proposed. ", "review": "Based on the observation that the target features from source pre-trained model are highly intrinsic discriminative and have a high probability of sharing the same label with their neighbors, a simple but effective method to impose Neighbor Class Consistency on target features is proposed. The experimental results are promising.\n[1] Compared with VNC, the improvement of ENC is very limited. Whether the authors have repeated the experiments in several times. It is better to show the variance of the results in Table 2, which may be more suitable to demonstrate the effectiveness of ENC.\n[2] In Eq.(9), it forces that the anchor samples should be closed to the neighbor samples by comparing with the distance between the anchor sample and the augmented sample. What is the advantage of such a strategy compared with triplet loss. It is also better to show the comparison in the experiments.\n[3] For the neighbors, when k is equal to 1, the proposed method achieved the best performance. Hence, the neighbors with the minimum distance could not guarantee that they share the same labels. How does the proposed method estimate the confidence.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper482/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper482/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neighbor Class Consistency on Unsupervised Domain Adaptation", "authorids": ["liu.chang6@northeastern.edu", "~Kai_Li3", "~Yun_Fu1"], "authors": ["Chang Liu", "Kai Li", "Yun Fu"], "keywords": ["Unsupervised domain adaptation", "Consistency regularization", "Neighbor"], "abstract": "Unsupervised domain adaptation (UDA) is to make predictions for unlabeled data in a target domain with labeled data from source domain available. Recent advances exploit entropy minimization and self-training to align the feature of two domains. However, as decision boundary is largely biased towards source data, class-wise pseudo labels generated by target predictions are usually very noisy, and trusting those noisy supervisions might potentially deteriorate the intrinsic target discriminative feature. Motivated by agglomerative clustering which assumes that features in the near neighborhood should be clustered together, we observe that target features from source pre-trained model are highly intrinsic discriminative and have a high probability of sharing the same label with their neighbors. Based on those observations, we propose a simple but effective method to impose Neighbor Class Consistency on target features to preserve and further strengthen the intrinsic discriminative nature of target data while regularizing the unified classifier less biased towards source data. We also introduce an entropy-based weighting scheme to help our framework more robust to the potential noisy neighbor supervision. We conduct ablation studies and extensive experiments on three UDA image classification benchmarks. Our method outperforms all existing UDA state-of-the-art. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|neighbor_class_consistency_on_unsupervised_domain_adaptation", "pdf": "/pdf/9f7b790dc1004f0c42d1c2507e9f564f7ebaa6f3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=crc6I6R8c", "_bibtex": "@misc{\nliu2021neighbor,\ntitle={Neighbor Class Consistency on Unsupervised Domain Adaptation},\nauthor={Chang Liu and Kai Li and Yun Fu},\nyear={2021},\nurl={https://openreview.net/forum?id=defQ1AG6IWn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "defQ1AG6IWn", "replyto": "defQ1AG6IWn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper482/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538142179, "tmdate": 1606915763072, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper482/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper482/-/Official_Review"}}}, {"id": "dqzzzWSh6Nc", "original": null, "number": 4, "cdate": 1603884801656, "ddate": null, "tcdate": 1603884801656, "tmdate": 1605024679136, "tddate": null, "forum": "defQ1AG6IWn", "replyto": "defQ1AG6IWn", "invitation": "ICLR.cc/2021/Conference/Paper482/-/Official_Review", "content": {"title": "The idea of neighbor consistency heavily lies on the assumption that target features from source pre-trained model are highly intrinsic discriminative. It is a controversial assumption. ", "review": "This paper addresses the unsupervised domain adaption (UDA) problem. Particularly, the paper proposes to impose neighbor class consistency on target features to preserve intrinsic discriminative nature of target data and presents an entropy-based weighting scheme to improve robustness against the potential noisy neighbor supervision. The motivation of the paper is clear and the method is well presented. Extensive experiments show the effectiveness of the proposed method. However, the paper suffers some problems, such as\n1. The idea of neighborhood consistency in UDA is not very significant. It lies on the assumption that target features from source pre-trained model are highly intrinsic discriminative. However, the distribution discrepancy between source domain and target domain may be very large, such as unsupervised cross-dataset person re-identification. It is not sure whether such assumption/observation is still satisfied.\n2. From Table 3-5, NC-SP outperforms the state-of-the-art methods with only a small margin. From Table 3, SC, FR , SP make a great contribution to the performance. It is suggested to add some comparisons to Table 3-5 that only the proposed VNC or ENC is used without other tricks.\n3. It is suggested to evaluate the influence of hyper-parameter \\lambda in Eq. (10) and (11)\n4.  It is suggested to evaluate neighbor size K on more datasets, in order validate the association of K with target dataset and number of classes.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper482/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper482/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neighbor Class Consistency on Unsupervised Domain Adaptation", "authorids": ["liu.chang6@northeastern.edu", "~Kai_Li3", "~Yun_Fu1"], "authors": ["Chang Liu", "Kai Li", "Yun Fu"], "keywords": ["Unsupervised domain adaptation", "Consistency regularization", "Neighbor"], "abstract": "Unsupervised domain adaptation (UDA) is to make predictions for unlabeled data in a target domain with labeled data from source domain available. Recent advances exploit entropy minimization and self-training to align the feature of two domains. However, as decision boundary is largely biased towards source data, class-wise pseudo labels generated by target predictions are usually very noisy, and trusting those noisy supervisions might potentially deteriorate the intrinsic target discriminative feature. Motivated by agglomerative clustering which assumes that features in the near neighborhood should be clustered together, we observe that target features from source pre-trained model are highly intrinsic discriminative and have a high probability of sharing the same label with their neighbors. Based on those observations, we propose a simple but effective method to impose Neighbor Class Consistency on target features to preserve and further strengthen the intrinsic discriminative nature of target data while regularizing the unified classifier less biased towards source data. We also introduce an entropy-based weighting scheme to help our framework more robust to the potential noisy neighbor supervision. We conduct ablation studies and extensive experiments on three UDA image classification benchmarks. Our method outperforms all existing UDA state-of-the-art. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|neighbor_class_consistency_on_unsupervised_domain_adaptation", "pdf": "/pdf/9f7b790dc1004f0c42d1c2507e9f564f7ebaa6f3.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=crc6I6R8c", "_bibtex": "@misc{\nliu2021neighbor,\ntitle={Neighbor Class Consistency on Unsupervised Domain Adaptation},\nauthor={Chang Liu and Kai Li and Yun Fu},\nyear={2021},\nurl={https://openreview.net/forum?id=defQ1AG6IWn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "defQ1AG6IWn", "replyto": "defQ1AG6IWn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper482/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538142179, "tmdate": 1606915763072, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper482/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper482/-/Official_Review"}}}], "count": 15}