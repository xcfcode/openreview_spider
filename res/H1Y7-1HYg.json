{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028619505, "tcdate": 1490028619505, "number": 1, "id": "H178dtpjx", "invitation": "ICLR.cc/2017/workshop/-/paper132/acceptance", "forum": "H1Y7-1HYg", "replyto": "H1Y7-1HYg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unseen Style Transfer Based on a Conditional Fast Style Transfer Network", "abstract": "In this paper, we propose a feed-forward neural style transfer network which can\ntransfer unseen arbitrary styles. To do that, first, we extend the fast neural style\ntransfer network proposed by Johnson et al. (2016) so that the network can learn\nmultiple styles at the same time by adding a conditional input. We call this as \u201ca\nconditional style transfer network\u201d. Next, we add a style condition network which\ngenerates a conditional signal from a style image directly, and train \u201ca conditional\nstyle transfer network with a style condition network\u201d in an end-to-end manner.\nThe proposed network can generate a stylized image from a content image and a\nstyle image in one-time feed-forward computation instantly.", "pdf": "/pdf/8ab8b9827b12824da39072aeb40957d906f47b25.pdf", "TL;DR": "A feed-forward neural style transfer network which can transfer unseen arbitrary styles", "paperhash": "yanai|unseen_style_transfer_based_on_a_conditional_fast_style_transfer_network", "conflicts": ["uec.ac.jp"], "authors": ["Keiji Yanai"], "authorids": ["yanai@inf.uec.ac.jp"], "keywords": []}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028620018, "id": "ICLR.cc/2017/workshop/-/paper132/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "H1Y7-1HYg", "replyto": "H1Y7-1HYg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028620018}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489447183157, "tcdate": 1489447033208, "number": 2, "id": "ByZKuiVje", "invitation": "ICLR.cc/2017/workshop/-/paper132/official/review", "forum": "H1Y7-1HYg", "replyto": "H1Y7-1HYg", "signatures": ["ICLR.cc/2017/workshop/paper132/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper132/AnonReviewer2"], "content": {"title": "Interesting ideas for doing fast style transfer using unseen styles", "rating": "7: Good paper, accept", "review": "The paper proposes two primary novel ideas of doing fast style transfer even on styles which are as yet unseen by the model. First, the authors extend the model of Johnson et al by making it take a conditional vector which encodes the style to be transferred. Second, they propose another neural network architecture which generates such a conditional style vector. This allows the authors directly plug the conditional vector generator (which they call Style Condition Network) into the Conditional Fast Style Transfer Network, allowing them to train the entire architecture end-to-end. In addition the architecture is flexible enough that one can represent the styles as a continuous vector and hence can potentially represent a mixture of different styles. The experimental results show that the model has been successfully able to transfer the styles. \n\nWhile the approach proposed seems to be novel, I thought the experimental section is a bit weak. The styles on which the model has been trained on seems to have transferred well. However those on which the model is not trained, the transfer seems to be a bit weak. Furthermore, since all the results are qualitative, it is somewhat hard to get a sense how good or bad the proposed approach is, with regards to the unseen styles. \n\nIn any case I think paper has enough interesting ideas that it justifies being spoken about at a workshop. \n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unseen Style Transfer Based on a Conditional Fast Style Transfer Network", "abstract": "In this paper, we propose a feed-forward neural style transfer network which can\ntransfer unseen arbitrary styles. To do that, first, we extend the fast neural style\ntransfer network proposed by Johnson et al. (2016) so that the network can learn\nmultiple styles at the same time by adding a conditional input. We call this as \u201ca\nconditional style transfer network\u201d. Next, we add a style condition network which\ngenerates a conditional signal from a style image directly, and train \u201ca conditional\nstyle transfer network with a style condition network\u201d in an end-to-end manner.\nThe proposed network can generate a stylized image from a content image and a\nstyle image in one-time feed-forward computation instantly.", "pdf": "/pdf/8ab8b9827b12824da39072aeb40957d906f47b25.pdf", "TL;DR": "A feed-forward neural style transfer network which can transfer unseen arbitrary styles", "paperhash": "yanai|unseen_style_transfer_based_on_a_conditional_fast_style_transfer_network", "conflicts": ["uec.ac.jp"], "authors": ["Keiji Yanai"], "authorids": ["yanai@inf.uec.ac.jp"], "keywords": []}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489447034057, "id": "ICLR.cc/2017/workshop/-/paper132/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper132/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper132/AnonReviewer1", "ICLR.cc/2017/workshop/paper132/AnonReviewer2"], "reply": {"forum": "H1Y7-1HYg", "replyto": "H1Y7-1HYg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper132/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper132/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489447034057}}}, {"tddate": null, "tmdate": 1489258806598, "tcdate": 1489258806598, "number": 2, "id": "BkyHFTWie", "invitation": "ICLR.cc/2017/workshop/-/paper132/public/comment", "forum": "H1Y7-1HYg", "replyto": "HJ-hZsgix", "signatures": ["~Keiji_Yanai1"], "readers": ["everyone"], "writers": ["~Keiji_Yanai1"], "content": {"title": "We added qualitative comparison with the results by Johnson's fast single style transfer network in the Appendix.  ", "comment": "Thank you for the comments and recognizing the novelty of the proposed approach.\n\nWe added qualitative comparison with the results by Johnson's fast single style transfer network in the Appendix, and uploaded the revised manuscript.  We trained four independent models for Johnson's network to generate results for four styles, since it can learn a single style.\n\nThe quality of Johnson\u2019s model and Conditional Style Transfer (trained 14 styles at once) are almost the same, while the results by Unseen Transfer is slightly different from them. However, we think Unseen Style Transfer is a good approximation of Johnson\u2019s and Conditional Style Transfer."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unseen Style Transfer Based on a Conditional Fast Style Transfer Network", "abstract": "In this paper, we propose a feed-forward neural style transfer network which can\ntransfer unseen arbitrary styles. To do that, first, we extend the fast neural style\ntransfer network proposed by Johnson et al. (2016) so that the network can learn\nmultiple styles at the same time by adding a conditional input. We call this as \u201ca\nconditional style transfer network\u201d. Next, we add a style condition network which\ngenerates a conditional signal from a style image directly, and train \u201ca conditional\nstyle transfer network with a style condition network\u201d in an end-to-end manner.\nThe proposed network can generate a stylized image from a content image and a\nstyle image in one-time feed-forward computation instantly.", "pdf": "/pdf/8ab8b9827b12824da39072aeb40957d906f47b25.pdf", "TL;DR": "A feed-forward neural style transfer network which can transfer unseen arbitrary styles", "paperhash": "yanai|unseen_style_transfer_based_on_a_conditional_fast_style_transfer_network", "conflicts": ["uec.ac.jp"], "authors": ["Keiji Yanai"], "authorids": ["yanai@inf.uec.ac.jp"], "keywords": []}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487364385404, "tcdate": 1487364385404, "id": "ICLR.cc/2017/workshop/-/paper132/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper132/reviewers"], "reply": {"forum": "H1Y7-1HYg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487364385404}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1489257888401, "tcdate": 1487364384790, "number": 132, "id": "H1Y7-1HYg", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "H1Y7-1HYg", "signatures": ["~Keiji_Yanai1"], "readers": ["everyone"], "content": {"title": "Unseen Style Transfer Based on a Conditional Fast Style Transfer Network", "abstract": "In this paper, we propose a feed-forward neural style transfer network which can\ntransfer unseen arbitrary styles. To do that, first, we extend the fast neural style\ntransfer network proposed by Johnson et al. (2016) so that the network can learn\nmultiple styles at the same time by adding a conditional input. We call this as \u201ca\nconditional style transfer network\u201d. Next, we add a style condition network which\ngenerates a conditional signal from a style image directly, and train \u201ca conditional\nstyle transfer network with a style condition network\u201d in an end-to-end manner.\nThe proposed network can generate a stylized image from a content image and a\nstyle image in one-time feed-forward computation instantly.", "pdf": "/pdf/8ab8b9827b12824da39072aeb40957d906f47b25.pdf", "TL;DR": "A feed-forward neural style transfer network which can transfer unseen arbitrary styles", "paperhash": "yanai|unseen_style_transfer_based_on_a_conditional_fast_style_transfer_network", "conflicts": ["uec.ac.jp"], "authors": ["Keiji Yanai"], "authorids": ["yanai@inf.uec.ac.jp"], "keywords": []}, "writers": [], "nonreaders": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "tmdate": 1489183144969, "tcdate": 1489183144969, "number": 1, "id": "HJ-hZsgix", "invitation": "ICLR.cc/2017/workshop/-/paper132/official/review", "forum": "H1Y7-1HYg", "replyto": "H1Y7-1HYg", "signatures": ["ICLR.cc/2017/workshop/paper132/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper132/AnonReviewer1"], "content": {"title": "feedforward network for arbitrary styles", "rating": "6: Marginally above acceptance threshold", "review": "- A brief summary of the paper's contributions, in the context of prior work.\n\nThis paper addresses the problem of fast style transfer for unseen style images.  The paper builds on the fast conv-deconv network of Johnson et al.  The key technical insight is to have a \u201cconditional vector\u201d input which encodes different styles.  The vector is concatenated to the convolutional responses of the content image.  This representation is then deconvolved to the final style-transferred image output.  The network can also be extended to regress to the conditional vector given a style image.\n\n- An assessment of novelty, clarity, significance, and quality.\n\nThe approach is novel, as far as I\u2019m aware, but I\u2019m not an expert in this topic.  The paper is clear enough and cites relevant work that I\u2019m familiar with in this space.  As I\u2019m not an expert in this area, I\u2019m happy to support a champion. \n\n- A list of pros and cons (reasons to accept/reject).\n\nPros: The approach appears to be novel. \n\nCons: The paper shows qualitative results only and doesn\u2019t compare against any baselines.  However, given the limited number of pages for the paper submission, this is probably ok.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unseen Style Transfer Based on a Conditional Fast Style Transfer Network", "abstract": "In this paper, we propose a feed-forward neural style transfer network which can\ntransfer unseen arbitrary styles. To do that, first, we extend the fast neural style\ntransfer network proposed by Johnson et al. (2016) so that the network can learn\nmultiple styles at the same time by adding a conditional input. We call this as \u201ca\nconditional style transfer network\u201d. Next, we add a style condition network which\ngenerates a conditional signal from a style image directly, and train \u201ca conditional\nstyle transfer network with a style condition network\u201d in an end-to-end manner.\nThe proposed network can generate a stylized image from a content image and a\nstyle image in one-time feed-forward computation instantly.", "pdf": "/pdf/8ab8b9827b12824da39072aeb40957d906f47b25.pdf", "TL;DR": "A feed-forward neural style transfer network which can transfer unseen arbitrary styles", "paperhash": "yanai|unseen_style_transfer_based_on_a_conditional_fast_style_transfer_network", "conflicts": ["uec.ac.jp"], "authors": ["Keiji Yanai"], "authorids": ["yanai@inf.uec.ac.jp"], "keywords": []}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489447034057, "id": "ICLR.cc/2017/workshop/-/paper132/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper132/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper132/AnonReviewer1", "ICLR.cc/2017/workshop/paper132/AnonReviewer2"], "reply": {"forum": "H1Y7-1HYg", "replyto": "H1Y7-1HYg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper132/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper132/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489447034057}}}, {"tddate": null, "nonreaders": null, "tmdate": 1488515555914, "tcdate": 1488515208925, "number": 1, "id": "SyZqx_U9x", "invitation": "ICLR.cc/2017/workshop/-/paper132/public/comment", "forum": "H1Y7-1HYg", "replyto": "H1Y7-1HYg", "signatures": ["~Keiji_Yanai1"], "readers": ["everyone"], "writers": ["~Keiji_Yanai1"], "content": {"title": "We have prepared an interactive demo page.", "comment": "We have prepared an interactive demo page where an image matrix of multiple-styles-by-multiple-contents for any given style and content images is shown instantly.  Please enjoy it !\nhttp://foodcam.mobi/yanai/fast_style/unseen2.cgi\n\nWe also have a demo page for the conditional style transfer. I recommend to try \"random-weight style mixing\" by pushing the \"random\" button. It is fun !\nhttp://foodcam.mobi/yanai/fast_style/\n"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unseen Style Transfer Based on a Conditional Fast Style Transfer Network", "abstract": "In this paper, we propose a feed-forward neural style transfer network which can\ntransfer unseen arbitrary styles. To do that, first, we extend the fast neural style\ntransfer network proposed by Johnson et al. (2016) so that the network can learn\nmultiple styles at the same time by adding a conditional input. We call this as \u201ca\nconditional style transfer network\u201d. Next, we add a style condition network which\ngenerates a conditional signal from a style image directly, and train \u201ca conditional\nstyle transfer network with a style condition network\u201d in an end-to-end manner.\nThe proposed network can generate a stylized image from a content image and a\nstyle image in one-time feed-forward computation instantly.", "pdf": "/pdf/8ab8b9827b12824da39072aeb40957d906f47b25.pdf", "TL;DR": "A feed-forward neural style transfer network which can transfer unseen arbitrary styles", "paperhash": "yanai|unseen_style_transfer_based_on_a_conditional_fast_style_transfer_network", "conflicts": ["uec.ac.jp"], "authors": ["Keiji Yanai"], "authorids": ["yanai@inf.uec.ac.jp"], "keywords": []}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487364385404, "tcdate": 1487364385404, "id": "ICLR.cc/2017/workshop/-/paper132/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper132/reviewers"], "reply": {"forum": "H1Y7-1HYg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487364385404}}}], "count": 6}