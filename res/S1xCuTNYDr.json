{"notes": [{"id": "S1xCuTNYDr", "original": "r1xM0FhPPH", "number": 651, "cdate": 1569439093666, "ddate": null, "tcdate": 1569439093666, "tmdate": 1577168233197, "tddate": null, "forum": "S1xCuTNYDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Regularizing Black-box Models for Improved Interpretability", "authors": ["Gregory Plumb", "Maruan Al-Shedivat", "Eric Xing", "Ameet Talwalkar"], "authorids": ["gdplumb@andrew.cmu.edu", "alshedivat@cs.cmu.edu", "epxing@cs.cmu.edu", "talwalkar@cmu.edu"], "keywords": ["Interpretable Machine Learning", "Local Explanations", "Regularization"], "TL;DR": "If you train your model with our regularizers, black-box explanations systems will work better on the resulting model.  Further, its likely that the resulting model will be more accurate as well.  ", "abstract": "Most of the work on interpretable machine learning has focused on designingeither inherently interpretable models, which typically trade-off accuracyfor interpretability, or post-hoc explanation systems, which lack guarantees about their explanation quality.  We explore an alternative to theseapproaches by directly regularizing a black-box model for interpretabilityat training time.  Our approach explicitly connects three key aspects ofinterpretable machine learning:  (i) the model\u2019s internal interpretability, (ii)the explanation system used at test time, and (iii) the metrics that measureexplanation quality.  Our regularization results in substantial improvementin terms of the explanation fidelity and stability metrics across a range ofdatasets and black-box explanation systems while slightly improving accuracy.  Finally, we justify theoretically that the benefits of our regularizationgeneralize to unseen points.", "pdf": "/pdf/548bba94a10c97d5c137c61261285f78b353a820.pdf", "code": "https://github.com/ForReview11235/CodeForICLR2020", "paperhash": "plumb|regularizing_blackbox_models_for_improved_interpretability", "original_pdf": "/attachment/548bba94a10c97d5c137c61261285f78b353a820.pdf", "_bibtex": "@misc{\nplumb2020regularizing,\ntitle={Regularizing Black-box Models for Improved Interpretability},\nauthor={Gregory Plumb and Maruan Al-Shedivat and Eric Xing and Ameet Talwalkar},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xCuTNYDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "vYHO8j_5Bv", "original": null, "number": 1, "cdate": 1576798702350, "ddate": null, "tcdate": 1576798702350, "tmdate": 1576800933664, "tddate": null, "forum": "S1xCuTNYDr", "replyto": "S1xCuTNYDr", "invitation": "ICLR.cc/2020/Conference/Paper651/-/Decision", "content": {"decision": "Reject", "comment": "This paper investigates a promising direction on the important topic of interpretability; the reviewers find a variety of issues with the work, and I urge the authors to refine and extend their investigations.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Black-box Models for Improved Interpretability", "authors": ["Gregory Plumb", "Maruan Al-Shedivat", "Eric Xing", "Ameet Talwalkar"], "authorids": ["gdplumb@andrew.cmu.edu", "alshedivat@cs.cmu.edu", "epxing@cs.cmu.edu", "talwalkar@cmu.edu"], "keywords": ["Interpretable Machine Learning", "Local Explanations", "Regularization"], "TL;DR": "If you train your model with our regularizers, black-box explanations systems will work better on the resulting model.  Further, its likely that the resulting model will be more accurate as well.  ", "abstract": "Most of the work on interpretable machine learning has focused on designingeither inherently interpretable models, which typically trade-off accuracyfor interpretability, or post-hoc explanation systems, which lack guarantees about their explanation quality.  We explore an alternative to theseapproaches by directly regularizing a black-box model for interpretabilityat training time.  Our approach explicitly connects three key aspects ofinterpretable machine learning:  (i) the model\u2019s internal interpretability, (ii)the explanation system used at test time, and (iii) the metrics that measureexplanation quality.  Our regularization results in substantial improvementin terms of the explanation fidelity and stability metrics across a range ofdatasets and black-box explanation systems while slightly improving accuracy.  Finally, we justify theoretically that the benefits of our regularizationgeneralize to unseen points.", "pdf": "/pdf/548bba94a10c97d5c137c61261285f78b353a820.pdf", "code": "https://github.com/ForReview11235/CodeForICLR2020", "paperhash": "plumb|regularizing_blackbox_models_for_improved_interpretability", "original_pdf": "/attachment/548bba94a10c97d5c137c61261285f78b353a820.pdf", "_bibtex": "@misc{\nplumb2020regularizing,\ntitle={Regularizing Black-box Models for Improved Interpretability},\nauthor={Gregory Plumb and Maruan Al-Shedivat and Eric Xing and Ameet Talwalkar},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xCuTNYDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1xCuTNYDr", "replyto": "S1xCuTNYDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795728964, "tmdate": 1576800281477, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper651/-/Decision"}}}, {"id": "BylipNhFoH", "original": null, "number": 5, "cdate": 1573663939202, "ddate": null, "tcdate": 1573663939202, "tmdate": 1573663939202, "tddate": null, "forum": "S1xCuTNYDr", "replyto": "rJlTVc1PsS", "invitation": "ICLR.cc/2020/Conference/Paper651/-/Official_Comment", "content": {"title": "Additional comments from reviewer#2", "comment": "-- \u201cThe distinction\u2026agnostic. Further, we believe \u2026total variation.\u201d \n\nIn the present manuscript the authors claim:  \"Moreover, recent approaches that claim to overcome this apparent trade-off between prediction accuracy and explanation quality are in fact by-design proposals that impose certain constraints on the underlying model families they consider\"\n\nThe reviewer maintains that this is an overstatement which should be nuanced. Indeed by regularizing, one enforces a constraint, which implicitly imposes some form of restriction on the models being considered. The reviewer did not claim that the proposed regularizers are *equivalent* with LC and TV. As stated by the authors, the proposed regularizers will encourage models that can be represented locally as the sum of a linear explanation plus a term whose LC/TV is controlled. This reflects the general idea that the regularizers constrain the types of models being considered. To reiterate, in the case of local linear explanations, the proposed objective states that the estimator should minimize a predictive loss *subject* to the fact that it should be accurately modeled locally by linear functions in neighborhoods of the training points. Again, this will implicitly constrain the models being produced. The issue is similar for the stability metric.  More generally, there is implicit control, b/c the proposed regularizers introduce coupling between the function being estimated and its explanations, and this coupling ends up impacting the \u2018form\u2019 of the estimator itself.\n\n--\u201cThe fidelity and stability metrics have been used to measure interpretability\u2026Moreover, we disagree with the sentiment that research needs to have a \u201csurprise factor\u201d in order to be impactful.\u201d\n\nSurprise factor in-and-of-itself is not the issue. What is an issue is that since regularizing via fidelity or stability will obviously lead to models with better fidelity/stability, it will be crucial to compare against alternative approaches to demonstrate research impact. As stated in a public comment, it should be possible to compare with the work of Lee. Even though Wang and Rudin use a global standpoint, it would be important to benchmark the methods, perhaps even assessing them on both global and local interpretability. Similarly with other related work.\n\nAnother issue is that the fidelity and stability metrics are proxies; using them in a post-hoc manner to evaluate models might be useful, but it is a more questionable \u2018leap\u2019 to *regularize/constrain* the model itself itself using these proxies.\n\n--\"Computational Complexity is an issue\"  The reviewer maintains that this is an issue. ExpO-1D-fidelity is performing much worse than its full-fledged counterpart, and the baseline models considered are insufficient to assess whether ExpO-1D would be valuable at all. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper651/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper651/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Black-box Models for Improved Interpretability", "authors": ["Gregory Plumb", "Maruan Al-Shedivat", "Eric Xing", "Ameet Talwalkar"], "authorids": ["gdplumb@andrew.cmu.edu", "alshedivat@cs.cmu.edu", "epxing@cs.cmu.edu", "talwalkar@cmu.edu"], "keywords": ["Interpretable Machine Learning", "Local Explanations", "Regularization"], "TL;DR": "If you train your model with our regularizers, black-box explanations systems will work better on the resulting model.  Further, its likely that the resulting model will be more accurate as well.  ", "abstract": "Most of the work on interpretable machine learning has focused on designingeither inherently interpretable models, which typically trade-off accuracyfor interpretability, or post-hoc explanation systems, which lack guarantees about their explanation quality.  We explore an alternative to theseapproaches by directly regularizing a black-box model for interpretabilityat training time.  Our approach explicitly connects three key aspects ofinterpretable machine learning:  (i) the model\u2019s internal interpretability, (ii)the explanation system used at test time, and (iii) the metrics that measureexplanation quality.  Our regularization results in substantial improvementin terms of the explanation fidelity and stability metrics across a range ofdatasets and black-box explanation systems while slightly improving accuracy.  Finally, we justify theoretically that the benefits of our regularizationgeneralize to unseen points.", "pdf": "/pdf/548bba94a10c97d5c137c61261285f78b353a820.pdf", "code": "https://github.com/ForReview11235/CodeForICLR2020", "paperhash": "plumb|regularizing_blackbox_models_for_improved_interpretability", "original_pdf": "/attachment/548bba94a10c97d5c137c61261285f78b353a820.pdf", "_bibtex": "@misc{\nplumb2020regularizing,\ntitle={Regularizing Black-box Models for Improved Interpretability},\nauthor={Gregory Plumb and Maruan Al-Shedivat and Eric Xing and Ameet Talwalkar},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xCuTNYDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1xCuTNYDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper651/Authors", "ICLR.cc/2020/Conference/Paper651/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper651/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper651/Reviewers", "ICLR.cc/2020/Conference/Paper651/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper651/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper651/Authors|ICLR.cc/2020/Conference/Paper651/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168289, "tmdate": 1576860553955, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper651/Authors", "ICLR.cc/2020/Conference/Paper651/Reviewers", "ICLR.cc/2020/Conference/Paper651/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper651/-/Official_Comment"}}}, {"id": "rJlTVc1PsS", "original": null, "number": 1, "cdate": 1573481013269, "ddate": null, "tcdate": 1573481013269, "tmdate": 1573481467052, "tddate": null, "forum": "S1xCuTNYDr", "replyto": "H1lHgrse9S", "invitation": "ICLR.cc/2020/Conference/Paper651/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "This is another by-design approach: \"- The paper states ... total variation across neighborhoods.\"\n\nThe distinction being made here is that the other approaches enforce structural restrictions on the model architecture while our regularization is model agnostic.  \n\nFurther, we believe that you may have misunderstood the connection between our regularization and the Lipschitz constant or the total variation.  The three are only equivalent after the part of the function that can be explained by a local linear explanation has been subtracted; this is discussed when we discuss Figure 2.  As a result, our regularization does not necessarily encourage models with a lower Lipschitz constant or a smaller total variation. \n\n\nResults are Unsurprising: \"- I also find ... to evaluate interpretability.\"\n\nThe fidelity and stability metrics have been used to measure interpretability, for local explanations, in previous work.  Although they are proxies for the qualitative concept of \u201cinterpretability\u201d in the general sense, they are useful; because a local explanation answers the question \u201cWhat could I have done differently to get a different outcome?\u201d, a higher fidelity explanation will yield more accurate advice and a more stable explanation will be more trustworthy.  A major contribution of our work is showing that these metrics can in fact be improved using a simple and general method.  Moreover, we disagree with the sentiment that research needs to have a \u201csurprise factor\u201d in order to be impactful.\n\n\nNeed to explore different neighborhoods: \"- It would be important ... help with generalization. \"\n\nWe agree that exploring this further is an interesting direction for future work and we said as much.  However, we do not think it is necessary for the current results to be meaningful.  \n\n\nWhat is the effect of ExpO on local variance?:  \"- Proposition 1 supports ... for Algorithm 1.\"\n\nFirst, we would like to point out that if a regularized model and an ExpO regularized model have the same local variances across neighborhoods, then the fidelity-metric would generalize equally well.  However, the results in the Github repo show that ExpO also reduces this variance.  We excluded this result from the main paper because this variance is neither a measure of accuracy nor interpretability.  \n\nSecond, the purpose of the generalization bound is to demonstrate theoretically that regularizing for fidelity on the training points should improve fidelity on new points.  The results we provide are on the testing data and consequently are empirical estimates of the true explanation fidelity.  As a result, their importance does not depend on the fact that ExpO also reduced this variance.  \n\nComputational Complexity is an issue:  \"- Computational complexity ... performing much worse.\"\n\nWe disagree with the characterization that \u201cExpO-1D-fidelity is performing much worse\u201d.  It still represents a significant improvement over the baseline models.  \n\nComparisons against baselines are missing: \"- Comparison against alternative ... Wang and Rudin,2015 etc).\"\n\nLee et al 2019 was a concurrent work with ours and has no publicly available source code.  As discussed in the related work, it differs significantly from our method.  \n\nWang and Rudin, 2015 is a global interpretability method which is fundamentally different from our approach which is related to local interpretability.  These two areas use different metrics and making a direct comparison would be difficult.  \n\n\n\" Overall I feel that more work is needed to convincingly demonstrate the importante of the proposed approach.\"\n\nThe work demonstrates a simple approach for making post-hoc explanations systems work significantly better.  To the best of our knowledge, there is no other model-agnostic regularization scheme that gives a practitioner the kind of control over local interpretability that we show in Figure 1 Left.  "}, "signatures": ["ICLR.cc/2020/Conference/Paper651/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper651/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Black-box Models for Improved Interpretability", "authors": ["Gregory Plumb", "Maruan Al-Shedivat", "Eric Xing", "Ameet Talwalkar"], "authorids": ["gdplumb@andrew.cmu.edu", "alshedivat@cs.cmu.edu", "epxing@cs.cmu.edu", "talwalkar@cmu.edu"], "keywords": ["Interpretable Machine Learning", "Local Explanations", "Regularization"], "TL;DR": "If you train your model with our regularizers, black-box explanations systems will work better on the resulting model.  Further, its likely that the resulting model will be more accurate as well.  ", "abstract": "Most of the work on interpretable machine learning has focused on designingeither inherently interpretable models, which typically trade-off accuracyfor interpretability, or post-hoc explanation systems, which lack guarantees about their explanation quality.  We explore an alternative to theseapproaches by directly regularizing a black-box model for interpretabilityat training time.  Our approach explicitly connects three key aspects ofinterpretable machine learning:  (i) the model\u2019s internal interpretability, (ii)the explanation system used at test time, and (iii) the metrics that measureexplanation quality.  Our regularization results in substantial improvementin terms of the explanation fidelity and stability metrics across a range ofdatasets and black-box explanation systems while slightly improving accuracy.  Finally, we justify theoretically that the benefits of our regularizationgeneralize to unseen points.", "pdf": "/pdf/548bba94a10c97d5c137c61261285f78b353a820.pdf", "code": "https://github.com/ForReview11235/CodeForICLR2020", "paperhash": "plumb|regularizing_blackbox_models_for_improved_interpretability", "original_pdf": "/attachment/548bba94a10c97d5c137c61261285f78b353a820.pdf", "_bibtex": "@misc{\nplumb2020regularizing,\ntitle={Regularizing Black-box Models for Improved Interpretability},\nauthor={Gregory Plumb and Maruan Al-Shedivat and Eric Xing and Ameet Talwalkar},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xCuTNYDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1xCuTNYDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper651/Authors", "ICLR.cc/2020/Conference/Paper651/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper651/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper651/Reviewers", "ICLR.cc/2020/Conference/Paper651/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper651/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper651/Authors|ICLR.cc/2020/Conference/Paper651/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168289, "tmdate": 1576860553955, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper651/Authors", "ICLR.cc/2020/Conference/Paper651/Reviewers", "ICLR.cc/2020/Conference/Paper651/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper651/-/Official_Comment"}}}, {"id": "BJxxPi1woH", "original": null, "number": 4, "cdate": 1573481303666, "ddate": null, "tcdate": 1573481303666, "tmdate": 1573481303666, "tddate": null, "forum": "S1xCuTNYDr", "replyto": "HkgJFF9GoS", "invitation": "ICLR.cc/2020/Conference/Paper651/-/Official_Comment", "content": {"title": "RE:  Some comments", "comment": "In general, we view [1] as addressing a different, but related, problem than our work. The regularization in [1] makes it so that it is easier to explain what the model will predict once it sees the next point in the time series.  This definitely makes the model more interpretable in the general sense (ie, why it\u2019s related).  However, it is conceptually quite different from a local explanation (the focus of ExpO), which explains how the models prediction would change if the time series itself was changed (ie, why it\u2019s different).    \n\nWe agree that the framework in [2] is defined broadly enough that it includes ExpO as a special case as you describe.  Further, we agree that the applications of [2] to graphs are conceptually similar to local interpretability (unlike the time series experiments in [1]). \n\nHowever, the optimization in both [1,2] is performed by alternating between updating the witness function and updating the model. This is significantly different from ExpO\u2019s regularizers which are differentiable and require no special optimization strategy.\n\nWe claimed to be concurrent with [2] and we noted that [1] preceded our work.  We believe that the claim of concurrency is fair since our work was preprinted before the publication of [2].   We will update the draft to clarify:\n-  That [1] is solving a different, but related, problem and will give more credit to it in the introduction when discussing the idea of regularizing a model to be more interpretable by some metric.\n-  That [2] is solving a similar problem, but with a very different optimization strategy. \n\nSince [2] was concurrent with ExpO and [1] isn\u2019t solving the same problem, we do not think it is necessary to describe ExpO as a special case of either [1,2].\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper651/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper651/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Black-box Models for Improved Interpretability", "authors": ["Gregory Plumb", "Maruan Al-Shedivat", "Eric Xing", "Ameet Talwalkar"], "authorids": ["gdplumb@andrew.cmu.edu", "alshedivat@cs.cmu.edu", "epxing@cs.cmu.edu", "talwalkar@cmu.edu"], "keywords": ["Interpretable Machine Learning", "Local Explanations", "Regularization"], "TL;DR": "If you train your model with our regularizers, black-box explanations systems will work better on the resulting model.  Further, its likely that the resulting model will be more accurate as well.  ", "abstract": "Most of the work on interpretable machine learning has focused on designingeither inherently interpretable models, which typically trade-off accuracyfor interpretability, or post-hoc explanation systems, which lack guarantees about their explanation quality.  We explore an alternative to theseapproaches by directly regularizing a black-box model for interpretabilityat training time.  Our approach explicitly connects three key aspects ofinterpretable machine learning:  (i) the model\u2019s internal interpretability, (ii)the explanation system used at test time, and (iii) the metrics that measureexplanation quality.  Our regularization results in substantial improvementin terms of the explanation fidelity and stability metrics across a range ofdatasets and black-box explanation systems while slightly improving accuracy.  Finally, we justify theoretically that the benefits of our regularizationgeneralize to unseen points.", "pdf": "/pdf/548bba94a10c97d5c137c61261285f78b353a820.pdf", "code": "https://github.com/ForReview11235/CodeForICLR2020", "paperhash": "plumb|regularizing_blackbox_models_for_improved_interpretability", "original_pdf": "/attachment/548bba94a10c97d5c137c61261285f78b353a820.pdf", "_bibtex": "@misc{\nplumb2020regularizing,\ntitle={Regularizing Black-box Models for Improved Interpretability},\nauthor={Gregory Plumb and Maruan Al-Shedivat and Eric Xing and Ameet Talwalkar},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xCuTNYDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1xCuTNYDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper651/Authors", "ICLR.cc/2020/Conference/Paper651/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper651/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper651/Reviewers", "ICLR.cc/2020/Conference/Paper651/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper651/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper651/Authors|ICLR.cc/2020/Conference/Paper651/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168289, "tmdate": 1576860553955, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper651/Authors", "ICLR.cc/2020/Conference/Paper651/Reviewers", "ICLR.cc/2020/Conference/Paper651/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper651/-/Official_Comment"}}}, {"id": "H1xNq5JvjH", "original": null, "number": 3, "cdate": 1573481100451, "ddate": null, "tcdate": 1573481100451, "tmdate": 1573481100451, "tddate": null, "forum": "S1xCuTNYDr", "replyto": "SkgZLZH2Kr", "invitation": "ICLR.cc/2020/Conference/Paper651/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "The fidelity and stability metrics have been used to measure interpretability, for local explanations, in previous work.  Although they are proxies for the qualitative concept of \u201cinterpretability\u201d in the general sense, they are useful; because a local explanation answers the question \u201cWhat could I have done differently to get a different outcome?\u201d, a higher fidelity explanation will yield more accurate advice and a more stable explanation will be more trustworthy.  A major contribution of our work is showing that these metrics can in fact be improved using a simple and general method.  Moreover, we disagree with the sentiment that research needs to have a \u201csurprise factor\u201d in order to be impactful.\n\nThe purpose of the generalization bound is to demonstrate theoretically that regularizing for fidelity on the training points should improve fidelity on new points.  While not a main contribution of the work, and even though the result itself is not technically difficult to derive, we do think that it is a positive addition to the work. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper651/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper651/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Black-box Models for Improved Interpretability", "authors": ["Gregory Plumb", "Maruan Al-Shedivat", "Eric Xing", "Ameet Talwalkar"], "authorids": ["gdplumb@andrew.cmu.edu", "alshedivat@cs.cmu.edu", "epxing@cs.cmu.edu", "talwalkar@cmu.edu"], "keywords": ["Interpretable Machine Learning", "Local Explanations", "Regularization"], "TL;DR": "If you train your model with our regularizers, black-box explanations systems will work better on the resulting model.  Further, its likely that the resulting model will be more accurate as well.  ", "abstract": "Most of the work on interpretable machine learning has focused on designingeither inherently interpretable models, which typically trade-off accuracyfor interpretability, or post-hoc explanation systems, which lack guarantees about their explanation quality.  We explore an alternative to theseapproaches by directly regularizing a black-box model for interpretabilityat training time.  Our approach explicitly connects three key aspects ofinterpretable machine learning:  (i) the model\u2019s internal interpretability, (ii)the explanation system used at test time, and (iii) the metrics that measureexplanation quality.  Our regularization results in substantial improvementin terms of the explanation fidelity and stability metrics across a range ofdatasets and black-box explanation systems while slightly improving accuracy.  Finally, we justify theoretically that the benefits of our regularizationgeneralize to unseen points.", "pdf": "/pdf/548bba94a10c97d5c137c61261285f78b353a820.pdf", "code": "https://github.com/ForReview11235/CodeForICLR2020", "paperhash": "plumb|regularizing_blackbox_models_for_improved_interpretability", "original_pdf": "/attachment/548bba94a10c97d5c137c61261285f78b353a820.pdf", "_bibtex": "@misc{\nplumb2020regularizing,\ntitle={Regularizing Black-box Models for Improved Interpretability},\nauthor={Gregory Plumb and Maruan Al-Shedivat and Eric Xing and Ameet Talwalkar},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xCuTNYDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1xCuTNYDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper651/Authors", "ICLR.cc/2020/Conference/Paper651/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper651/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper651/Reviewers", "ICLR.cc/2020/Conference/Paper651/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper651/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper651/Authors|ICLR.cc/2020/Conference/Paper651/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168289, "tmdate": 1576860553955, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper651/Authors", "ICLR.cc/2020/Conference/Paper651/Reviewers", "ICLR.cc/2020/Conference/Paper651/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper651/-/Official_Comment"}}}, {"id": "SJlIdq1woB", "original": null, "number": 2, "cdate": 1573481069667, "ddate": null, "tcdate": 1573481069667, "tmdate": 1573481069667, "tddate": null, "forum": "S1xCuTNYDr", "replyto": "B1eqkesCFH", "invitation": "ICLR.cc/2020/Conference/Paper651/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "The fidelity and stability metrics have been used to measure interpretability, for local explanations, in previous work.  They are proxies for the qualitative concept of \u201cinterpretability\u201d in the general sense, and we would argue are quite useful; because a local explanation answers the question \u201cWhat could I have done differently to get a different outcome?\u201d, a higher fidelity explanation will yield more accurate advice and a more stable explanation will be more trustworthy.  A major contribution of our work is showing that these metrics can in fact be improved using a simple and general method.  Moreover, we disagree with the sentiment that research needs to have a \u201csurprise factor\u201d in order to be impactful.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper651/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper651/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Black-box Models for Improved Interpretability", "authors": ["Gregory Plumb", "Maruan Al-Shedivat", "Eric Xing", "Ameet Talwalkar"], "authorids": ["gdplumb@andrew.cmu.edu", "alshedivat@cs.cmu.edu", "epxing@cs.cmu.edu", "talwalkar@cmu.edu"], "keywords": ["Interpretable Machine Learning", "Local Explanations", "Regularization"], "TL;DR": "If you train your model with our regularizers, black-box explanations systems will work better on the resulting model.  Further, its likely that the resulting model will be more accurate as well.  ", "abstract": "Most of the work on interpretable machine learning has focused on designingeither inherently interpretable models, which typically trade-off accuracyfor interpretability, or post-hoc explanation systems, which lack guarantees about their explanation quality.  We explore an alternative to theseapproaches by directly regularizing a black-box model for interpretabilityat training time.  Our approach explicitly connects three key aspects ofinterpretable machine learning:  (i) the model\u2019s internal interpretability, (ii)the explanation system used at test time, and (iii) the metrics that measureexplanation quality.  Our regularization results in substantial improvementin terms of the explanation fidelity and stability metrics across a range ofdatasets and black-box explanation systems while slightly improving accuracy.  Finally, we justify theoretically that the benefits of our regularizationgeneralize to unseen points.", "pdf": "/pdf/548bba94a10c97d5c137c61261285f78b353a820.pdf", "code": "https://github.com/ForReview11235/CodeForICLR2020", "paperhash": "plumb|regularizing_blackbox_models_for_improved_interpretability", "original_pdf": "/attachment/548bba94a10c97d5c137c61261285f78b353a820.pdf", "_bibtex": "@misc{\nplumb2020regularizing,\ntitle={Regularizing Black-box Models for Improved Interpretability},\nauthor={Gregory Plumb and Maruan Al-Shedivat and Eric Xing and Ameet Talwalkar},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xCuTNYDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1xCuTNYDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper651/Authors", "ICLR.cc/2020/Conference/Paper651/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper651/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper651/Reviewers", "ICLR.cc/2020/Conference/Paper651/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper651/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper651/Authors|ICLR.cc/2020/Conference/Paper651/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168289, "tmdate": 1576860553955, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper651/Authors", "ICLR.cc/2020/Conference/Paper651/Reviewers", "ICLR.cc/2020/Conference/Paper651/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper651/-/Official_Comment"}}}, {"id": "HkgJFF9GoS", "original": null, "number": 2, "cdate": 1573198198613, "ddate": null, "tcdate": 1573198198613, "tmdate": 1573198198613, "tddate": null, "forum": "S1xCuTNYDr", "replyto": "S1xCuTNYDr", "invitation": "ICLR.cc/2020/Conference/Paper651/-/Public_Comment", "content": {"title": "Some comments", "comment": "Dear authors,\n\nThank you for this work. We would like to clarify a few things:\n\n1. in Section 2.3 the authors describe their work as \"concurrent with\" [1,2]. This is a bit strange wording since [1] has been available for more than a year.\n\n2. the authors state that their approach \u201cdiffers substantially\u201d from [1,2]. The difference seems primarily to be whether the neighborhood is defined as a subset of data space or as a distribution. A subset can be of course defined as samples. So wouldn't the symmetric game formulation in [1, 2] yield the ExpO-Fidelity regularizer if we defined the neighborhood as samples from a distribution together with squared error as deviation and a linear witness class? ExpO-1D-Fidelity also looks like a special case if we restrict the coordinate usage of the linear witness (explainer). Of course restrictions are sometimes valuable but it would be helpful if the authors could clarify these further.\n\n\n[1] Guang-He Lee, David Alvarez-Melis, and Tommi S. Jaakkola. \"Game-Theoretic Interpretability for Temporal Modeling.\" FAT/ML (ICML workshop), 2018\n[2] Guang-He Lee, Wengong Jin, David Alvarez-Melis, and Tommi S. Jaakkola. \"Functional Transparency for Structured Data: a Game-Theoretic Approach.\" ICML 2019."}, "signatures": ["~Guang-He_Lee1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Guang-He_Lee1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Black-box Models for Improved Interpretability", "authors": ["Gregory Plumb", "Maruan Al-Shedivat", "Eric Xing", "Ameet Talwalkar"], "authorids": ["gdplumb@andrew.cmu.edu", "alshedivat@cs.cmu.edu", "epxing@cs.cmu.edu", "talwalkar@cmu.edu"], "keywords": ["Interpretable Machine Learning", "Local Explanations", "Regularization"], "TL;DR": "If you train your model with our regularizers, black-box explanations systems will work better on the resulting model.  Further, its likely that the resulting model will be more accurate as well.  ", "abstract": "Most of the work on interpretable machine learning has focused on designingeither inherently interpretable models, which typically trade-off accuracyfor interpretability, or post-hoc explanation systems, which lack guarantees about their explanation quality.  We explore an alternative to theseapproaches by directly regularizing a black-box model for interpretabilityat training time.  Our approach explicitly connects three key aspects ofinterpretable machine learning:  (i) the model\u2019s internal interpretability, (ii)the explanation system used at test time, and (iii) the metrics that measureexplanation quality.  Our regularization results in substantial improvementin terms of the explanation fidelity and stability metrics across a range ofdatasets and black-box explanation systems while slightly improving accuracy.  Finally, we justify theoretically that the benefits of our regularizationgeneralize to unseen points.", "pdf": "/pdf/548bba94a10c97d5c137c61261285f78b353a820.pdf", "code": "https://github.com/ForReview11235/CodeForICLR2020", "paperhash": "plumb|regularizing_blackbox_models_for_improved_interpretability", "original_pdf": "/attachment/548bba94a10c97d5c137c61261285f78b353a820.pdf", "_bibtex": "@misc{\nplumb2020regularizing,\ntitle={Regularizing Black-box Models for Improved Interpretability},\nauthor={Gregory Plumb and Maruan Al-Shedivat and Eric Xing and Ameet Talwalkar},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xCuTNYDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1xCuTNYDr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504206099, "tmdate": 1576860587119, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper651/Authors", "ICLR.cc/2020/Conference/Paper651/Reviewers", "ICLR.cc/2020/Conference/Paper651/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper651/-/Public_Comment"}}}, {"id": "SkgZLZH2Kr", "original": null, "number": 1, "cdate": 1571733832860, "ddate": null, "tcdate": 1571733832860, "tmdate": 1572972569102, "tddate": null, "forum": "S1xCuTNYDr", "replyto": "S1xCuTNYDr", "invitation": "ICLR.cc/2020/Conference/Paper651/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThe paper proposes a new type of regularizer to improve explainability in neural networks. The proposed regularizer is largely based on two metrics, namely fidelity and stability. It optimizes for fidelity and stability as a regularization objective in a differentiable manner.\n\nI would recommend for accept, as the paper shows in its experiments that the explainability of neural networks can be improved with the two proposed regularizer, which outperforms simple baselines of l1 and l2 regularizers. The paper's method is generic, and can be applied to almost all machine learning models with gradient-based optimization, making it helpful to building explainable machine learning systems.\n\nHowever, I would also like to note that the results in this paper are somewhat unsurprising. The ExpO-Fidelity and ExpO-Stability regularizers can be seen as (almost) directly optimizing the fidelity and stability metric for explainability, so one would naturally expect that models trained with these regularizers will do better on the two metrics above. In addition, I do not see much value in the derivations of Section 3.2. The conclusion that \"explainable models with smaller local variances ... are likely to have explanations of higher fidelity\" is unsurprising and almost a straightforward claim."}, "signatures": ["ICLR.cc/2020/Conference/Paper651/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper651/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Black-box Models for Improved Interpretability", "authors": ["Gregory Plumb", "Maruan Al-Shedivat", "Eric Xing", "Ameet Talwalkar"], "authorids": ["gdplumb@andrew.cmu.edu", "alshedivat@cs.cmu.edu", "epxing@cs.cmu.edu", "talwalkar@cmu.edu"], "keywords": ["Interpretable Machine Learning", "Local Explanations", "Regularization"], "TL;DR": "If you train your model with our regularizers, black-box explanations systems will work better on the resulting model.  Further, its likely that the resulting model will be more accurate as well.  ", "abstract": "Most of the work on interpretable machine learning has focused on designingeither inherently interpretable models, which typically trade-off accuracyfor interpretability, or post-hoc explanation systems, which lack guarantees about their explanation quality.  We explore an alternative to theseapproaches by directly regularizing a black-box model for interpretabilityat training time.  Our approach explicitly connects three key aspects ofinterpretable machine learning:  (i) the model\u2019s internal interpretability, (ii)the explanation system used at test time, and (iii) the metrics that measureexplanation quality.  Our regularization results in substantial improvementin terms of the explanation fidelity and stability metrics across a range ofdatasets and black-box explanation systems while slightly improving accuracy.  Finally, we justify theoretically that the benefits of our regularizationgeneralize to unseen points.", "pdf": "/pdf/548bba94a10c97d5c137c61261285f78b353a820.pdf", "code": "https://github.com/ForReview11235/CodeForICLR2020", "paperhash": "plumb|regularizing_blackbox_models_for_improved_interpretability", "original_pdf": "/attachment/548bba94a10c97d5c137c61261285f78b353a820.pdf", "_bibtex": "@misc{\nplumb2020regularizing,\ntitle={Regularizing Black-box Models for Improved Interpretability},\nauthor={Gregory Plumb and Maruan Al-Shedivat and Eric Xing and Ameet Talwalkar},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xCuTNYDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1xCuTNYDr", "replyto": "S1xCuTNYDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper651/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper651/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574783295585, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper651/Reviewers"], "noninvitees": [], "tcdate": 1570237749058, "tmdate": 1574783295598, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper651/-/Official_Review"}}}, {"id": "B1eqkesCFH", "original": null, "number": 2, "cdate": 1571889122229, "ddate": null, "tcdate": 1571889122229, "tmdate": 1572972569059, "tddate": null, "forum": "S1xCuTNYDr", "replyto": "S1xCuTNYDr", "invitation": "ICLR.cc/2020/Conference/Paper651/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an approach for local post-hoc explanation with introduction of a new regularization that helps regulate the \"fidelity\" and \"stability\" of the output explanation (in the style of LIME).  The fidelity regularizer is essentially the squared error of the explainer as compared to the given model in the local neighborhood, whereas the stability regularizer measures the total squared differences between the explanation outcomes between the sample in question and other samples in the local neighborhood. \nIn the experimental evaluation section, the authors evaluates the performance fo the proposed regularizers, used as part of both LIME and MAPLE, against three interpretability metrics: point fidelity, neighborhood fidelity and stability. The results verify that indeed the use of the regularizers improve the performance of both LIME and MAPLE over the unregularized versions, with respect to the corresponding metrics. This is in a way \"expected\", since the regularizer used in the method and that in the metric are closely related, and is an unsatisfying aspect of the work. \nUsing image data, they also demonstrate that qualitatively the use of stability regularizer seems to significantly improve the saliency of the output explanation. \nThe paper is well written, the proposal is reasonable, but the contribution is modest and experimental evaluation is not entirely convincing. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper651/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper651/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Black-box Models for Improved Interpretability", "authors": ["Gregory Plumb", "Maruan Al-Shedivat", "Eric Xing", "Ameet Talwalkar"], "authorids": ["gdplumb@andrew.cmu.edu", "alshedivat@cs.cmu.edu", "epxing@cs.cmu.edu", "talwalkar@cmu.edu"], "keywords": ["Interpretable Machine Learning", "Local Explanations", "Regularization"], "TL;DR": "If you train your model with our regularizers, black-box explanations systems will work better on the resulting model.  Further, its likely that the resulting model will be more accurate as well.  ", "abstract": "Most of the work on interpretable machine learning has focused on designingeither inherently interpretable models, which typically trade-off accuracyfor interpretability, or post-hoc explanation systems, which lack guarantees about their explanation quality.  We explore an alternative to theseapproaches by directly regularizing a black-box model for interpretabilityat training time.  Our approach explicitly connects three key aspects ofinterpretable machine learning:  (i) the model\u2019s internal interpretability, (ii)the explanation system used at test time, and (iii) the metrics that measureexplanation quality.  Our regularization results in substantial improvementin terms of the explanation fidelity and stability metrics across a range ofdatasets and black-box explanation systems while slightly improving accuracy.  Finally, we justify theoretically that the benefits of our regularizationgeneralize to unseen points.", "pdf": "/pdf/548bba94a10c97d5c137c61261285f78b353a820.pdf", "code": "https://github.com/ForReview11235/CodeForICLR2020", "paperhash": "plumb|regularizing_blackbox_models_for_improved_interpretability", "original_pdf": "/attachment/548bba94a10c97d5c137c61261285f78b353a820.pdf", "_bibtex": "@misc{\nplumb2020regularizing,\ntitle={Regularizing Black-box Models for Improved Interpretability},\nauthor={Gregory Plumb and Maruan Al-Shedivat and Eric Xing and Ameet Talwalkar},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xCuTNYDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1xCuTNYDr", "replyto": "S1xCuTNYDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper651/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper651/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574783295585, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper651/Reviewers"], "noninvitees": [], "tcdate": 1570237749058, "tmdate": 1574783295598, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper651/-/Official_Review"}}}, {"id": "H1lHgrse9S", "original": null, "number": 3, "cdate": 1572021485140, "ddate": null, "tcdate": 1572021485140, "tmdate": 1572972569015, "tddate": null, "forum": "S1xCuTNYDr", "replyto": "S1xCuTNYDr", "invitation": "ICLR.cc/2020/Conference/Paper651/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper considers the problem of training black box models for improved interpretability, and  proposes to penalize black-box models at training time using two regularizers that correspond to fidelity and stability explanation metrics. As computing the regularizers exactly is computationally intensive they propose two approximating algorithm. In addition, as the one for fidelity is still prohibitive, a randomized variant is proposed. Connections are established between the regularizers and the model's Lipschitz constant or total variation. A generalization bound is presented for local linear explanations. The proposed approach is evaluated on a variety of datasets. \n\nThe paper deals with an important problem and the exposition is clear.  While regularizing deep learning models is a pertinent  direction, I feel the paper makes a couple of overstatements, and overall I am not fully convinced by the approach and empirical evaluation, as outlined below.\n\n- The paper states \"recent approaches that claim to overcome this apparent trade-off between prediction accuracy and explanation quality are in fact by-design proposals that impose certain constants on the underlying model families they consider\" and that they are addressing this shortcoming. But in fact, the proposed regularizations do exactly the same: they impose certain constraints. Indeed the regularizers encourages models with lower Lipschitz constant or with small total variation across neighborhoods.\n\n- I also find that it is unsurprising that regularizing via fidelity or stability will lead to models with better fidelity/stability so it's an artificial way to yield \"improved interpretability\" and this is more of an issue because fidelity and stability are kind of proxy metrics to evaluate interpretability.\n\n- It would be important to investigate further the difference between regularization and explanation neighborhoods. This might not be a bad thing which in fact help with generalization. \n\n- Proposition 1 supports algorithm 2, but it is not a given at all that Algorithm 1 will have smaller local variances across neightborhoods and hence might generalize well.  It would be important to proceed with an empirical study of the local variance across neighborhoods for Algorithm 1.\n \n- Computational complexity remains an issue as ExpO-1D-fidelity is performing much worse.\n\n- Comparison against alternative approaches beyond SENN are lacking (e.g. Lee et al 2019, Wang and Rudin,2015 etc).\n \n Overall I feel that more work is needed to convincingly demonstrate the importante of the proposed approach.\n "}, "signatures": ["ICLR.cc/2020/Conference/Paper651/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper651/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Black-box Models for Improved Interpretability", "authors": ["Gregory Plumb", "Maruan Al-Shedivat", "Eric Xing", "Ameet Talwalkar"], "authorids": ["gdplumb@andrew.cmu.edu", "alshedivat@cs.cmu.edu", "epxing@cs.cmu.edu", "talwalkar@cmu.edu"], "keywords": ["Interpretable Machine Learning", "Local Explanations", "Regularization"], "TL;DR": "If you train your model with our regularizers, black-box explanations systems will work better on the resulting model.  Further, its likely that the resulting model will be more accurate as well.  ", "abstract": "Most of the work on interpretable machine learning has focused on designingeither inherently interpretable models, which typically trade-off accuracyfor interpretability, or post-hoc explanation systems, which lack guarantees about their explanation quality.  We explore an alternative to theseapproaches by directly regularizing a black-box model for interpretabilityat training time.  Our approach explicitly connects three key aspects ofinterpretable machine learning:  (i) the model\u2019s internal interpretability, (ii)the explanation system used at test time, and (iii) the metrics that measureexplanation quality.  Our regularization results in substantial improvementin terms of the explanation fidelity and stability metrics across a range ofdatasets and black-box explanation systems while slightly improving accuracy.  Finally, we justify theoretically that the benefits of our regularizationgeneralize to unseen points.", "pdf": "/pdf/548bba94a10c97d5c137c61261285f78b353a820.pdf", "code": "https://github.com/ForReview11235/CodeForICLR2020", "paperhash": "plumb|regularizing_blackbox_models_for_improved_interpretability", "original_pdf": "/attachment/548bba94a10c97d5c137c61261285f78b353a820.pdf", "_bibtex": "@misc{\nplumb2020regularizing,\ntitle={Regularizing Black-box Models for Improved Interpretability},\nauthor={Gregory Plumb and Maruan Al-Shedivat and Eric Xing and Ameet Talwalkar},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xCuTNYDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1xCuTNYDr", "replyto": "S1xCuTNYDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper651/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper651/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574783295585, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper651/Reviewers"], "noninvitees": [], "tcdate": 1570237749058, "tmdate": 1574783295598, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper651/-/Official_Review"}}}], "count": 11}