{"notes": [{"tddate": null, "ddate": null, "tmdate": 1501490309192, "tcdate": 1501490309192, "number": 22, "cdate": 1501490309192, "id": "SkTK2vn8W", "invitation": "ICLR.cc/2017/conference/-/paper170/public/comment", "forum": "SJ3rcZcxl", "replyto": "HkoyInUUW", "signatures": ["~Lei_Tai1"], "readers": ["everyone"], "writers": ["~Lei_Tai1"], "content": {"title": "Thanks!"}, "nonreaders": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287701679, "id": "ICLR.cc/2017/conference/-/paper170/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJ3rcZcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper170/reviewers", "ICLR.cc/2017/conference/paper170/areachairs"], "cdate": 1485287701679}}}, {"tddate": null, "ddate": null, "tmdate": 1501115875006, "tcdate": 1501115875006, "number": 21, "cdate": 1501115875006, "id": "HkoyInUUW", "invitation": "ICLR.cc/2017/conference/-/paper170/public/comment", "forum": "SJ3rcZcxl", "replyto": "Sk63xNS8Z", "signatures": ["~Shixiang_Gu1"], "readers": ["everyone"], "writers": ["~Shixiang_Gu1"], "content": {"title": "Reply", "comment": "1. Because Q = A - V, where V only depends on the state. It's the standard baseline trick in policy gradient, applied to both Qhat and Qbar. \n2. The comparison is difficult, since the original ACER code is not open sourced, and it includes a variety of additional design choices. We did some follow-up analysis on this paper: https://arxiv.org/abs/1706.00387, under simplified conditions. Overall, how to integrate critic-based policy gradient is different, and we could also apply Q-Prop on top of ACER algorithm.  "}, "nonreaders": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287701679, "id": "ICLR.cc/2017/conference/-/paper170/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJ3rcZcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper170/reviewers", "ICLR.cc/2017/conference/paper170/areachairs"], "cdate": 1485287701679}}}, {"tddate": null, "ddate": null, "tmdate": 1501016244566, "tcdate": 1501016244566, "number": 20, "cdate": 1501016244566, "id": "Sk63xNS8Z", "invitation": "ICLR.cc/2017/conference/-/paper170/public/comment", "forum": "SJ3rcZcxl", "replyto": "SJ3rcZcxl", "signatures": ["~Lei_Tai1"], "readers": ["everyone"], "writers": ["~Lei_Tai1"], "content": {"title": "From equation 7 to equation 8.", "comment": "1. Why the Q can be directly replaced with A from equation 7 to equation 8?\n\n2. And what do you think the pros and cons of Q-prop compared with ACER(https://arxiv.org/abs/1611.01224)?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287701679, "id": "ICLR.cc/2017/conference/-/paper170/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJ3rcZcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper170/reviewers", "ICLR.cc/2017/conference/paper170/areachairs"], "cdate": 1485287701679}}}, {"tddate": null, "nonreaders": null, "ddate": null, "writable": true, "revisions": false, "tmdate": 1491316198258, "tcdate": 1488365928107, "number": 16, "replyCount": 0, "id": "rkluF7Ncl", "invitation": "ICLR.cc/2017/conference/-/paper170/public/comment", "forum": "SJ3rcZcxl", "replyto": "HJT0sL79g", "signatures": ["~Kashif_Rasul1"], "readers": ["everyone"], "writers": ["~Kashif_Rasul1"], "content": {"title": "another one", "comment": "no problem!\n\nyou have \" in the equal for\" which should be \"in the equation for\"."}, "tags": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287701679, "id": "ICLR.cc/2017/conference/-/paper170/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJ3rcZcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper170/reviewers", "ICLR.cc/2017/conference/paper170/areachairs"], "cdate": 1485287701679}}}, {"tddate": null, "tmdate": 1489547349859, "tcdate": 1489547349859, "number": 19, "id": "SkA8eN8jx", "invitation": "ICLR.cc/2017/conference/-/paper170/public/comment", "forum": "SJ3rcZcxl", "replyto": "SyrILFSig", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "you are right", "comment": "I see, my bad. Thank you"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287701679, "id": "ICLR.cc/2017/conference/-/paper170/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJ3rcZcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper170/reviewers", "ICLR.cc/2017/conference/paper170/areachairs"], "cdate": 1485287701679}}}, {"tddate": null, "tmdate": 1489503820724, "tcdate": 1489503820724, "number": 18, "id": "SyrILFSig", "invitation": "ICLR.cc/2017/conference/-/paper170/public/comment", "forum": "SJ3rcZcxl", "replyto": "S1_tJyY9g", "signatures": ["~Shixiang_Gu1"], "readers": ["everyone"], "writers": ["~Shixiang_Gu1"], "content": {"title": "derivation clarification", "comment": "We used the property described in the first equation of Appendix A, since the expectation of score function is wrt a_t, all terms not involving a_t have 0 expectation and can be thought of as a part of state-dependent baseline. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287701679, "id": "ICLR.cc/2017/conference/-/paper170/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJ3rcZcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper170/reviewers", "ICLR.cc/2017/conference/paper170/areachairs"], "cdate": 1485287701679}}}, {"tddate": null, "tmdate": 1488674687577, "tcdate": 1488674687577, "number": 17, "id": "S1_tJyY9g", "invitation": "ICLR.cc/2017/conference/-/paper170/public/comment", "forum": "SJ3rcZcxl", "replyto": "SJ3rcZcxl", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "paper equations", "comment": "One thing confuse me is that in the appendix equation (14), when author derive the 1st order Taylor expansion, from line 2 to line 3 in g(theta). The author derive that the term \"f(s_t,bar(a_t))+grad(f(s_t,bar(a_t)))*(a_t-bar(a_t))\" is equal to \"grad(f(s_t,bar(a_t)))*a_t\". This is problematic not only for general f(.), but also when replace it with Q function."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287701679, "id": "ICLR.cc/2017/conference/-/paper170/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJ3rcZcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper170/reviewers", "ICLR.cc/2017/conference/paper170/areachairs"], "cdate": 1485287701679}}}, {"tddate": null, "nonreaders": null, "tmdate": 1488372438529, "tcdate": 1488299992418, "number": 14, "id": "B1gk_7X5x", "invitation": "ICLR.cc/2017/conference/-/paper170/public/comment", "forum": "SJ3rcZcxl", "replyto": "SJ3rcZcxl", "signatures": ["~Kashif_Rasul1"], "readers": ["everyone"], "writers": ["~Kashif_Rasul1"], "content": {"title": "typo in appendix", "comment": "the equation under (14) has a typo: missing left bracket ')' after \\hat{f}"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287701679, "id": "ICLR.cc/2017/conference/-/paper170/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJ3rcZcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper170/reviewers", "ICLR.cc/2017/conference/paper170/areachairs"], "cdate": 1485287701679}}}, {"tddate": null, "tmdate": 1488313301033, "tcdate": 1488313301033, "number": 15, "id": "HJT0sL79g", "invitation": "ICLR.cc/2017/conference/-/paper170/public/comment", "forum": "SJ3rcZcxl", "replyto": "B1gk_7X5x", "signatures": ["~Shixiang_Gu1"], "readers": ["everyone"], "writers": ["~Shixiang_Gu1"], "content": {"title": "thank you!", "comment": "thanks for pointing out!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287701679, "id": "ICLR.cc/2017/conference/-/paper170/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJ3rcZcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper170/reviewers", "ICLR.cc/2017/conference/paper170/areachairs"], "cdate": 1485287701679}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488313261554, "tcdate": 1478265412222, "number": 170, "id": "SJ3rcZcxl", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SJ3rcZcxl", "signatures": ["~Shixiang_Gu1"], "readers": ["everyone"], "content": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 29, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396410513, "tcdate": 1486396410513, "number": 1, "id": "rJzWhGLOe", "invitation": "ICLR.cc/2017/conference/-/paper170/acceptance", "forum": "SJ3rcZcxl", "replyto": "SJ3rcZcxl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This paper presents a nice contribution to the RL literature, finding an intermediate point between the high-variance (but unbiased) gradient estimates from policy optimization methods, and low(er)-variance (but biased) gradient estimates from off-policy actor-critic methods like DDPG. The basic idea (as I'm interpreting it, similar to one of the reviewers below), that that we can use an action-dependent baseline, based upon off-policy learning, to lower the variance of the gradient, assuming we also correct for it in the gradient computation. The experiments clearly show the benefit of the proposed approach. The work is a nice combination/unification of two prominent trends in RL (with the overarching goal of reducing sample complexity, which is of course crucial here), and I believe is absolutely worth accepting. The authors also did an excellent job responding to reviewer concerns and adjusting the manuscript to address any issues raised.\n \n Pros:\n + Nice contribution combining off-policy and on-policy methods, with a novel and compelling algorithm\n + Good evaluation on a wide variety of control tasks\n \n Cons:\n - Somewhat difficult to understand (the interpretation I give above is not quite how the paper is presented, though I believe they are equivalent), and the given presentation is somewhat dense at time", "decision": "Accept (Oral)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396411062, "id": "ICLR.cc/2017/conference/-/paper170/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SJ3rcZcxl", "replyto": "SJ3rcZcxl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396411062}}}, {"tddate": null, "tmdate": 1484987924739, "tcdate": 1482437203642, "number": 1, "id": "S1hUM3FVx", "invitation": "ICLR.cc/2017/conference/-/paper170/official/review", "forum": "SJ3rcZcxl", "replyto": "SJ3rcZcxl", "signatures": ["ICLR.cc/2017/conference/paper170/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper170/AnonReviewer1"], "content": {"title": "Off-policy TD learning of the critic", "rating": "7: Good paper, accept", "review": "This paper proposed a new policy gradient method that uses the Taylor expansion of a critic as the control variate to reduce the variance in gradient estimation. The key idea is that the critic can be learned in an off-policy manner so that it is more sample efficient. Although the algorithm structure is similar to actor-critic, the critic information is \u201ctruncated\u201d in a proper manner to reduce the variance in policy gradient. The proposed methods are evaluated on OpenAI Gym\u2019s MuJoCo domains. Q-Prop is shown to produce more stable performance compared to DDPG and has higher sample efficiency than TRPO. \n\nThe stability of off-policy TD learning for the critic is not guaranteed. Do the authors observe instability of it in the experiment? As the authors stated in the paper, the critic does not need to approximate the actual value function very well as long as it is correlated with \\hat{A}. In the two adaptive Q-Prop schemes, the authors apply some tricks (conservative and aggressive adaptation) to control the possible unreliable estimate of the critic. This could be another evidence that the off-policy critic is not reliable. The authors may need to comment more on this point. Especially, it will be useful if the authors could show/justify that by such a design Q-Prop is robust against unreliable critic estimate. \n\nThe authors seem to indicate that the advantage of Q-Prop over DDPG is in its insensitivity to hyperparameters. In Figure 3(a), the authors show that DDPG is sensitive to hyperparameters. However, the sensitivity of Q-Prop to the same hyperparameter is not shown. Experiments in the paper show that Q-Prop has advantage over TRPO in sample complexity. However, not much experiments are shown to justify the advantage of Q-Prop over DDPG. This is important because Table 1 shows that TR-c-Q-Prop needs significantly more samples than DDPG on Hopper, HalfCheetah and Swimmer. Any comment on that?\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483071415909, "id": "ICLR.cc/2017/conference/-/paper170/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper170/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper170/AnonReviewer1", "ICLR.cc/2017/conference/paper170/AnonReviewer3", "ICLR.cc/2017/conference/paper170/AnonReviewer2", "ICLR.cc/2017/conference/paper170/AnonReviewer4"], "reply": {"forum": "SJ3rcZcxl", "replyto": "SJ3rcZcxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper170/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper170/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483071415909}}}, {"tddate": null, "tmdate": 1484587237353, "tcdate": 1484587237353, "number": 3, "id": "SJTyWt9Ul", "invitation": "ICLR.cc/2017/conference/-/paper170/official/comment", "forum": "SJ3rcZcxl", "replyto": "rydEliF8e", "signatures": ["ICLR.cc/2017/conference/paper170/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper170/AnonReviewer4"], "content": {"title": "Re-evaluation", "comment": "I have edited the original review to note that I now suggest the paper for acceptance."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287701554, "id": "ICLR.cc/2017/conference/-/paper170/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "SJ3rcZcxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper170/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper170/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper170/reviewers", "ICLR.cc/2017/conference/paper170/areachairs"], "cdate": 1485287701554}}}, {"tddate": null, "tmdate": 1484587165927, "tcdate": 1483071415131, "number": 4, "id": "SyJakD7Bl", "invitation": "ICLR.cc/2017/conference/-/paper170/official/review", "forum": "SJ3rcZcxl", "replyto": "SJ3rcZcxl", "signatures": ["ICLR.cc/2017/conference/paper170/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper170/AnonReviewer4"], "content": {"title": "A good idea, but not a research paper", "rating": "7: Good paper, accept", "review": "**Edit: Based on the discussion below, my main problem (#2) was not correct. I have changed my overall rating from a 3 to a 7**\n\nThis paper makes a fascinating observation: one can introduce an action-dependent baseline (control variate) into REINFORCE, which introduces bias, and then include a correction term to remove the bias. The variance of the correction term is low relative to the REINFORCE update and the action-dependent baseline, and so this results in benefits.\n\nHowever, the paper is poorly executed. Below I list my concerns.\n\n1. The paper tries to distinguish between \"policy gradient\" methods and \"actor critic\" methods by defining them in a non-standard way. Specifically, when this paper says \"policy gradient\" it means REINFORCE. Historically, the two have meant different things: some policy gradient algorithms are actor-critics (e.g., Degris et al's INAC algorithm) while others are not (e.g. REINFORCE).\n\n2. The proposed Q-Prop algorithm includes many interesting design choices that make in unclear what the real source of improved performance is. Is the improved performance due to the use of the action-dependent control variate? Would the same setup but using a state-value baseline still perform just as well? Are the performance benefits due to the use of an off-policy advantage estimation algorithm, GAE(lambda)? Or, would performance have been similar with an on-policy advantage estimation algorithm? What about if a different off-policy advantage estimation algorithm was used, like Retrace(lambda), GTD2, ETD, or WIS-LSTD? Or, is the improved performance due to the use of a replay buffer?\n\nComparisons are not performed between variants of Q-Prop that show the importances of these different components. Rather the authors opt to show better performance on a benchmark task. I find this to be non-scientific, and more of a paper showing a feat of engineering (by combining many different ideas) than it is a research paper that studies the details of which parts of Q-Prop make it work well. For example, after reading this paper, it is not clear whether having the action-dependent baseline (or using the first order Taylor approximation for the baseline) is beneficial or not - it could be that the strong performance comes from GAE(lambda) or the use of a replay buffer. At the very least I would have expected comparisons to Q-Prop using a state-value baseline (which would then be a variant of REINFORCE using off-policy data and a replay buffer, and which would show whether the action-dependent baseline is important).\n\n3. There is a fair amount of discussion about unbiased policy gradient algorithms, which is not accurate. Most policy gradient algorithms are biased, and making them unbiased tends to hurt performance. This is discussed in the paper \"Bias in Natural Actor-Critic Algorithms\", which applies to non-natural algorithms as well. Also, I suspect that the use of GAE(lambda) results in the exact sort of bias discussed in that paper, even when lambda=1. As a result, Q-Prop may act more like an average reward method than expected. This should be discussed.\n\n4. The proposed algorithm can be applied to deep architectures, just as most linear-time policy gradient algorithms can. However, it does not have to be applied to deep architectures. The emphasis on \"deep\" therefore seems to detract from the core ideas of the paper.\n\n5. The paper repeatedly says that importance sampling based methods result in high variance. This ignores weighted importance sampling methods that have very low variance. A good example of this is Mahmood et al's WIS-LSTD algorithm. WIS-LSTD has high computational complexity, so it would only be compared to on non-deep RL problems, of which there are plenty. Alternatively, algorithms like Retrace(lambda) have quite low variance since the likelihood ratios are never bigger than one. Others might argue that ETD algorithms are currently the most effective. The simple dismissal of these algorithms because the original importance sampling estimator proposed in 2000 has high variance is not sufficient.\n\n6. The paper does not compare to natural actor-critic algorithms. Once the weights, w, have been computed, REINFORCE uses samples of states from the normalized discounted state distribution and samples of the corresponding returns to estimate the policy gradient. One of the main reasons Q-Prop should work better than REINFORCE is that it includes a control variate that reduces the variance of the policy gradient update after w has been computed. Now, compare this to natural policy gradient algorithms. Once the weights, w, have been computed (admittedly, using compatible features for the advantage estimation but any features for the state-value estimation) the resulting update is = w. That is, is has zero variance and does not require additional sampling. It is as though a perfect control variate was used. Furthermore, natural gradient algorithms can be applied to deep architectures. Degris et al's INAC algorithm is linear time. Desjardin et al's \"natural neural networks\" paper also discusses efficient implementations of natural gradients for neural networks. Dabney's Natural Temporal Difference algorithms have linear time variants that fit this paper's description of actor-critic algorithms.\n\nTo summarize, given the weights w, REINFORCE has high variance, and Q-Prop claims to reduce the variance of REINFORCE. However, natural policy gradient methods have zero variance given the weights w. So, what is the benefit of Q-Prop over natural gradient algorithms using off-policy value function estimation methods to estimate Q (or A)? That is, why should we expect Q-Prop to perform better than NAC-LSTD using GAE(lambda) with experience replay in place of LSTD?\n\n7. Equation (2) is false. The right side is proportional to the left side, not equal to it. There is a (1-gamma) term missing. There are also other typos throughout (e.g., Q and A sometimes are missing their action arguments).\n\n\nAlthough I have listed my concerns, I would like to re-iterate that I do find the idea of an action-dependent baseline fascinating. My problem with this paper is with its execution, not with the novelty, impact, or quality of the core idea.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483071415909, "id": "ICLR.cc/2017/conference/-/paper170/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper170/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper170/AnonReviewer1", "ICLR.cc/2017/conference/paper170/AnonReviewer3", "ICLR.cc/2017/conference/paper170/AnonReviewer2", "ICLR.cc/2017/conference/paper170/AnonReviewer4"], "reply": {"forum": "SJ3rcZcxl", "replyto": "SJ3rcZcxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper170/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper170/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483071415909}}}, {"tddate": null, "tmdate": 1484529712356, "tcdate": 1484529712356, "number": 13, "id": "rydEliF8e", "invitation": "ICLR.cc/2017/conference/-/paper170/public/comment", "forum": "SJ3rcZcxl", "replyto": "r1MiVT18g", "signatures": ["~Shixiang_Gu1"], "readers": ["everyone"], "writers": ["~Shixiang_Gu1"], "content": {"title": "Re-evaluation", "comment": "Thank you for your feedback, and we're glad to hear that our response addressed your concerns. Since your answer indicates that you've reevaluated the paper based on our responses and additional experiments, we would appreciate a revision to the original score as well, since your current score is substantially lower than that of the other reviewers. And of course we would appreciate any other feedback if, e.g., we have not addressed all of your concerns in enough detail."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287701679, "id": "ICLR.cc/2017/conference/-/paper170/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJ3rcZcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper170/reviewers", "ICLR.cc/2017/conference/paper170/areachairs"], "cdate": 1485287701679}}}, {"tddate": null, "tmdate": 1484318449851, "tcdate": 1484318449851, "number": 12, "id": "rJ5evvULl", "invitation": "ICLR.cc/2017/conference/-/paper170/public/comment", "forum": "SJ3rcZcxl", "replyto": "SklQBhLNe", "signatures": ["~Shixiang_Gu1"], "readers": ["everyone"], "writers": ["~Shixiang_Gu1"], "content": {"title": "Q-Prop vs GProp", "comment": "Thank you for mentioning this interesting work. GProp proposes a TD-based approach for learning value gradient directly and analyzes its stability property from deterministic policy gradient theorem, and appears an interesting extension/related work to DPG (Silver et. al., 2014), DDPG (Lillicrap et. al., 2016), and Dueling network (Wang et. al., 2016). \n\nGProp, however, is orthogonal to Q-Prop. In fact, Q-Prop can be used with GProp easily, by substituting \\nabla_a Q_w(s,a)|_{a=\\mu(s)} in Eq. 8 in the Q-Prop paper with G_w(s,\\mu(s)) in the GProp paper. Besides the analysis provided from the DPG theorem perspective, predicting the value gradient directly may provide better gradient estimate than differentiating a NN-fitted action-value function and improve current Q-Prop implementation with standard TD-based Q-fitting.\n\nWhile GProp analyzes compatibility conditions in detail, it makes approximation when deriving a TD-based algorithm (see Section 3.4 L_BGE Equation in GProp) and its policy gradient is biased. By contrast, Q-Prop is guaranteed to be as unbiased as standard Monte Carlo policy gradient methods such as REINFORCE (see Section 2.1, 3, 3.1 in Q-Prop). It is interesting to therefore evaluate stability across DDPG, GProp, Q-Prop with DDPG-style critic learning, Q-Prop with GProp.   \n\nSimilarity in namings is by coincidence. We took inspiration from MuProp (Gu et. al., 2016). "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287701679, "id": "ICLR.cc/2017/conference/-/paper170/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJ3rcZcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper170/reviewers", "ICLR.cc/2017/conference/paper170/areachairs"], "cdate": 1485287701679}}}, {"tddate": null, "tmdate": 1484313476898, "tcdate": 1484313476898, "number": 11, "id": "HJ6FQLI8g", "invitation": "ICLR.cc/2017/conference/-/paper170/public/comment", "forum": "SJ3rcZcxl", "replyto": "S1hUM3FVx", "signatures": ["~Shixiang_Gu1"], "readers": ["everyone"], "writers": ["~Shixiang_Gu1"], "content": {"title": "Response to AnonReviewer1", "comment": "Thank you for your review and constructive feedback! \n\nFigure 2.a shows that on HalfCheetah-v1 the conservative variant of Q-Prop (TR-c-Q-Prop) learns more stably than the standard (TR-Q-Prop) and aggressive (TR-a-Q-Prop) variants, and empirically appears more robust toward unreliable critic estimate. This is expected from the derivation, since the conservative variant effectively turns off Q-Prop and back-off to TRPO for states that are likely to have bad critic estimates. We also updated Section 3.4 second paragraph to include discussions on the robustness to bad critics.\n\nWe performed hyperparameter search on both Q-Prop and DDPG as detailed in Appendix D and showing that the best attainable performance is more stable for Q-Prop than DDPG as shown in Table 1. Our claim is that Q-Prop is more robust to bad critics and less sensitive to hyperparameters than DDPG. In Table 1, it is important to weigh significances of task results based on task difficulties. While DDPG could solve tasks like HalfCheetah and Swimmer (which have 17 and 8 state dimensions and 6 and 2 action dimensions respectively), Figure 3.b and Table 1 show that Q-Prop achieves significantly better performance within the same number of episodes on the much harder Humanoid task (376 state dimensions and 17 action dimension). Note that Q-Prop was able to solve all tasks as well as TRPO, though sometimes slightly slower than DDPG, while DDPG was unable to achieve good performance on Humanoid. Empirically, we observe that the harder the tasks become (e.g. 2D HalfCheetah -> 3D Humanoid, see curves comparing TR-c-Q-Prop and DDPG in Figures 3.a, 3.b and Table 1), the benefit of Q-prop over purely off-policy method like DDPG becomes more obvious. A crucial point about DDPG from our experiment is that given a reasonable hyperparameter search done in Appendix D for DDPG, DDPG couldn\u2019t find good solutions for some of the harder problems. This practically makes DDPG difficult to be applied for solving hard problems, even though it sometimes can solve simpler tasks better than TRPO and Q-Prop. \n\nWe also ran additional experiments to validate the hyperparameter sensitivity further, as available in https://docs.google.com/document/d/1ow_BIrKYt11r4BXMbM7w9qBxbedhedKrBrvMhnXm7nI/edit?usp=sharing. It builds on the discussion on Figure 3.a, further demonstrating that Q-Prop is less sensitive to reward scales than DDPG."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287701679, "id": "ICLR.cc/2017/conference/-/paper170/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJ3rcZcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper170/reviewers", "ICLR.cc/2017/conference/paper170/areachairs"], "cdate": 1485287701679}}}, {"tddate": null, "tmdate": 1484313359413, "tcdate": 1484313359413, "number": 10, "id": "ByDfXUI8g", "invitation": "ICLR.cc/2017/conference/-/paper170/public/comment", "forum": "SJ3rcZcxl", "replyto": "r11-EhWBx", "signatures": ["~Shixiang_Gu1"], "readers": ["everyone"], "writers": ["~Shixiang_Gu1"], "content": {"title": "Response to AnonReviewer3", "comment": "Thank you for your review and positive feedback!\n1) compute time: We expanded current discussion on computation time in Section 3.4. The running time of Q-Prop is effectively time(TRPO) + time(DDPG)/2, if we use the same parameter update per step of experience as DDPG (i.e. one update per one step). If the simulator is very fast, time(DDPG)/2 >> time(TRPO), so effectively the run-time per episode of Q-Prop is about half of DDPG and slower than TRPO. However, for applications where we care about sample-efficiency, the experience collection is often the bottleneck, and in such cases, the off-policy critic learning and the on-policy data collection and policy update can be parallelized, and will run at about the same speed as both TRPO and DDPG. \n2) limitations of this technique: We added Section 3.4 to discuss the main limitations. \n\nWe also clarified the confusion in Eq (4). Q without subscript represents the target network."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287701679, "id": "ICLR.cc/2017/conference/-/paper170/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJ3rcZcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper170/reviewers", "ICLR.cc/2017/conference/paper170/areachairs"], "cdate": 1485287701679}}}, {"tddate": null, "tmdate": 1484313290332, "tcdate": 1484313290332, "number": 9, "id": "r1M0fULUl", "invitation": "ICLR.cc/2017/conference/-/paper170/public/comment", "forum": "SJ3rcZcxl", "replyto": "rJVKwJMSe", "signatures": ["~Shixiang_Gu1"], "readers": ["everyone"], "writers": ["~Shixiang_Gu1"], "content": {"title": "Response", "comment": "Thank you for your detailed reviews and pointing to aspects of the paper that can be improved!\n- compute time: We expanded current discussion on computation time in Section 3.4. The running time of Q-Prop is effectively time(TRPO) + time(DDPG)/2, if we use the same parameter update per step of experience as DDPG (i.e. one update per one step). If the simulator is very fast, time(DDPG)/2 >> time(TRPO), so effectively the run-time per episode of Q-Prop is about half of DDPG and slower than TRPO. However, for applications where we care about sample-efficiency, the experience collection is the bottleneck, and in such cases, the off-policy critic learning and the on-policy data collection and policy update can be parallelized, and will run at about the same speed as both TRPO and DDPG. \n- sensitivity experiment: We ran additional experiments to validate the hyperparameter sensitivity further, as available in https://docs.google.com/document/d/1ow_BIrKYt11r4BXMbM7w9qBxbedhedKrBrvMhnXm7nI/edit?usp=sharing. It builds on the discussion on Figure 3.a, further demonstrating that Q-Prop is less sensitive to reward scales than DDPG."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287701679, "id": "ICLR.cc/2017/conference/-/paper170/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJ3rcZcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper170/reviewers", "ICLR.cc/2017/conference/paper170/areachairs"], "cdate": 1485287701679}}}, {"tddate": null, "tmdate": 1484144379361, "tcdate": 1484144379361, "number": 8, "id": "HJXW1TXUe", "invitation": "ICLR.cc/2017/conference/-/paper170/public/comment", "forum": "SJ3rcZcxl", "replyto": "rkXaJfGIl", "signatures": ["~Shixiang_Gu1"], "readers": ["everyone"], "writers": ["~Shixiang_Gu1"], "content": {"title": "Response", "comment": "Thank you for your recommendation. The run-time per episode of Q-Prop is time(TRPO) + time(DDPG)/2. If simulation is very fast, then it\u2019s roughly time(DDPG)/2. As you mentioned, it takes a long time to run until 100k episodes just like DDPG. This is a limitation of Q-Prop which we discussed in Section 3.3 last paragraph, but it should not be a concern if collecting samples is the bottleneck, which is the case in most real-world applications (e.g. physical robots). We are in the process of running those experiments for longer episodes and hope to post available videos after.\n\nDuan et. al. 2016 is the main benchmark paper we aimed to compare with, and seems to have different reward scales/results from the Gym website. We primarily followed their example to compare numbers across many domains, rather than specifics in videos which are difficult to quantify.  "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287701679, "id": "ICLR.cc/2017/conference/-/paper170/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJ3rcZcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper170/reviewers", "ICLR.cc/2017/conference/paper170/areachairs"], "cdate": 1485287701679}}}, {"tddate": null, "tmdate": 1484034420620, "tcdate": 1484033978800, "number": 7, "id": "rkXaJfGIl", "invitation": "ICLR.cc/2017/conference/-/paper170/public/comment", "forum": "SJ3rcZcxl", "replyto": "ryhaysWIx", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Some recommendations", "comment": "Thanks for the response. In light of what you said, I have the following recommendations:\n\n1. Make public a video of all your policies (and policies that you have with TRPO-GAE and DDPG). As mentioned before, numbers aren't very helpful. A visual comparison between the different policies will be much more revealing. \n\n2. Include wall clock times in Table 1. The selling point seems time, but it need not necessarily be correlated with sample complexity. For instance, some algorithms run on a batch setting (update once per say 10k time-steps) while others run on some approximate online setting (update every 100 or so steps). This is why DDPG is slower (in wall-clock sense) than TRPO, even though the former could be more sample efficient. Where does Q-prop stand in this regard?\n\n3. Why not run for more episodes (say 100k) and then compare performances? This will likely give a better idea about asymptotic performance. The results reported in the gym website (e.g. https://gym.openai.com/evaluations/eval_W27eCzLQBy60FciaSGSJw) take only 5-20 hours, which isn't that bad. At least, run Q-prop alone for 100k episodes and compare to assymptotic performance reported on the gym website.\n\nPlease note that this isn't a comment against the ideas presented in the paper. I also appreciate the efforts of author to compare against other algorithms. However, I think the above suggestions are easy enough to implement and would be much more revealing of the strengths and weaknesses of Q-prop, that I highly recommend them."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287701679, "id": "ICLR.cc/2017/conference/-/paper170/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJ3rcZcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper170/reviewers", "ICLR.cc/2017/conference/paper170/areachairs"], "cdate": 1485287701679}}}, {"tddate": null, "tmdate": 1484005316439, "tcdate": 1484005316439, "number": 6, "id": "ryhaysWIx", "invitation": "ICLR.cc/2017/conference/-/paper170/public/comment", "forum": "SJ3rcZcxl", "replyto": "HJD3gzgIx", "signatures": ["~Shixiang_Gu1"], "readers": ["everyone"], "writers": ["~Shixiang_Gu1"], "content": {"title": "Reward functions are the same as the tasks in OpenAI gym suite", "comment": "Thank you for your suggestions. We are using the same reward definitions and default settings as given in OpenAI Gym. In addition, our TRPO-GAE and DDPG benchmarks are run with TensorFlow implementations in https://github.com/openai/rllab. Since the authors of the code moved from Theano to TensorFlow, they have not fully benchmarked the codes, and small bugs were fixed on my end as well. We used LinearFeatureBaseline for all Monte Carlo policy gradient algorithms (including Q-Prop), primarily because the NN Baseline was not available in their open-source TF implementation. We believe our results are sufficiently close; however, some discrepancies may be due to Linear baseline vs NN baseline. \n\nIn Table 2, all algorithms were run for 30k episodes. Note that the OpenAI gym website reports reward up to e.g. 100k episodes, which we were unable to do for all methods due to computational constraints. However, since the point of the evaluation is to evaluate speed of learning, we believe it is reasonable to choose a fixed episode horizon for all methods. Our results are comparable with the gym website results until 30k episodes, e.g. better than gym on Humanoid-v1, Ant-v1 and slightly worse but comparable on HalfCheetah-v1. \n\nWhile we tried our best to benchmark our results to be similar as Duan et. al. 2016 paper, their paper appears to use different MuJoCo domain settings from the OpenAI Gym ones, e.g. the reward scale for HalfCheetah-v1 is quite off, and they do not have videos available. \n\nFor HalfCheetah-v1, a setting of DDPG (see Figure 3.a \u201cDDPG-r0.1\u201d curve) appears to outperform all the results in OpenAI Gym website in terms of both sample efficiency and final performance. For Humanoid-v1, Figure 3.b shows that TRPO performs about as well for the 30k episodes as the gym website (https://gym.openai.com/evaluations/eval_i3x1JpReRukTZrznxypCw, where performance increases only much later), so the results are in fact similar. We confirm that our Humanoid-v1 policy with reward~3500 at 30k episodes does walk but a bit less elegantly than the gym policy with reward~6000 at 80-90k episodes. We only ran the methods up to 30k episodes, so the final performances are lower than the best reported after 80k-90k episodes. However, within the first 30k episodes ours is better than their result, and given that both our Q-Prop and TRPO curves learn monotonically, we feel the current setting was sufficient to demonstrate the relative performance with and without the Q-Prop control variate. \n\nWe appreciate you bring up important challenges in RL: how to have fair benchmarks. We have tried our best in the paper, using the open-source domains/codes without any modification. We also have open-sourced our codes here: https://github.com/shaneshixiang/rllabplusplus. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287701679, "id": "ICLR.cc/2017/conference/-/paper170/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJ3rcZcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper170/reviewers", "ICLR.cc/2017/conference/paper170/areachairs"], "cdate": 1485287701679}}}, {"tddate": null, "tmdate": 1483923218260, "tcdate": 1483903151542, "number": 5, "id": "HJD3gzgIx", "invitation": "ICLR.cc/2017/conference/-/paper170/public/comment", "forum": "SJ3rcZcxl", "replyto": "SJ3rcZcxl", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Are the reward functions different from the tasks in OpenAI gym suite?", "comment": "The results reported seem very different from what is available on the gym website. Are you using different reward definitions, and if so, you should make this very transparent. \n\nFor instance, look at Table~1.\nFor hopper, TRPO can attain a reward close to 3600, much higher than what is reported for TRPO/Q-Prop: https://gym.openai.com/evaluations/eval_W27eCzLQBy60FciaSGSJw\nSimilarly, for the humanoid-v1, TRPO-GAE/DDPG can obtain orders of magnitude more return than results reported in this paper: https://gym.openai.com/evaluations/eval_i3x1JpReRukTZrznxypCw\n\nIf your environment definitions are different, you should re-run your experiments on the definitions packaged with the gym suite. At the very least, you should take your policies and test them on the reward functions with the gym tasks. As is, it's very hard to interpret your numbers and make any conclusions, since we don't know your reward function. Do you have videos for the different policies? What does a reward of 3500 mean? Is the system moving, or just standing still, or just just applying less control effort to get a higher score?\n\nThe idea seems promising, but I appeal to the authors to do the above \"sanity check\" experiments, without which the paper would be incomplete. Also, for the TRPO experiments, which value function baseline did you use (linear, NN -- if so, how many layers etc)? You just have a pointer to Duan's paper, but it would be good to be explicit, since they have multiple options available in their code base."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287701679, "id": "ICLR.cc/2017/conference/-/paper170/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJ3rcZcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper170/reviewers", "ICLR.cc/2017/conference/paper170/areachairs"], "cdate": 1485287701679}}}, {"tddate": null, "tmdate": 1483883674019, "tcdate": 1483883674019, "number": 4, "id": "r1MiVT18g", "invitation": "ICLR.cc/2017/conference/-/paper170/public/comment", "forum": "SJ3rcZcxl", "replyto": "H1WxcN6Hx", "signatures": ["~Shixiang_Gu1"], "readers": ["everyone"], "writers": ["~Shixiang_Gu1"], "content": {"title": "Q-Prop implementation", "comment": "Thank you for your quick response. Yes, the implementation of TRPO-GAE is exactly the same as Q-Prop if \\eta_{t,e}=0. We will include the sentence about equivalence in the paper. \n\nThe codes are available: https://github.com/shaneshixiang/rllabplusplus. Q-Prop codes are directly written on top of the open-sourced TRPO-GAE TensorFlow implementation (e.g. sandbox/rocky/tf/algos/batch_opt.py). "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287701679, "id": "ICLR.cc/2017/conference/-/paper170/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJ3rcZcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper170/reviewers", "ICLR.cc/2017/conference/paper170/areachairs"], "cdate": 1485287701679}}}, {"tddate": null, "tmdate": 1483717096567, "tcdate": 1483717096567, "number": 2, "id": "H1WxcN6Hx", "invitation": "ICLR.cc/2017/conference/-/paper170/official/comment", "forum": "SJ3rcZcxl", "replyto": "B1NxP19re", "signatures": ["ICLR.cc/2017/conference/paper170/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper170/AnonReviewer4"], "content": {"title": "Response", "comment": "Thanks for the response.\n\n2. This was my main concern, and perhaps I was wrong. What I missed was that your implementation of TRPO is *exactly* the same as Q-Prop if you use \\eta_{t,e}=0. Can you confirm that this is correct? If so, then my primary concern about this paper was wrong. This implementation equivalence may be worth stating in the paper.\n\nThe other points are all relatively minor. Also, is the source code for the experiments available somewhere?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287701554, "id": "ICLR.cc/2017/conference/-/paper170/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "SJ3rcZcxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper170/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper170/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper170/reviewers", "ICLR.cc/2017/conference/paper170/areachairs"], "cdate": 1485287701554}}}, {"tddate": null, "tmdate": 1483499243815, "tcdate": 1483499243815, "number": 3, "id": "B1NxP19re", "invitation": "ICLR.cc/2017/conference/-/paper170/public/comment", "forum": "SJ3rcZcxl", "replyto": "SyJakD7Bl", "signatures": ["~Shixiang_Gu1"], "readers": ["everyone"], "writers": ["~Shixiang_Gu1"], "content": {"title": "Rebuttal", "comment": "Thank you for valuable feedback. We hope we could engage in constructive discussions to fully clarify and address your concerns and questions. We have incorporated some of your suggestions into the paper, available on OpenReview. Below are our initial responses to your seven comments. \n\n1. Given your feedback, we renamed Section 2.1 as \u201cMonte Carlo policy gradient methods\u201d and Section 2.2 as \u201cpolicy gradient methods with function approximation\u201d, added a discussion in Section 2, and clarified some references to \u201cpolicy gradient\u201d through the paper. We decided to use \u201cMonte Carlo policy gradient\u201d over \u201cREINFORCE\u201d because we would like it to cover a more general class of algorithms including REINFORCE/vanilla policy gradient, TRPO, etc. Please let us know if you have other suggestions.\n\n2. We agree that a detailed evaluation of the components of the algorithm is important, and we endeavored to provide these in Figure 2.a (see curves comparing variants of Q-Prop) and Figure 3.a (see Q-Prop variants with and without trust-region, i.e. TR-c-Q-Prop and v-c-Q-Prop). Note, however, that Q-Prop is a general control variates approach that can be combined with a number of prior techniques, including TRPO-GAE and DDPG,  as well as natural gradient, standard REINFORCE, and standard (on-policy) actor-critic algorithms. We've clarified this in the second-to-last paragraph in Section 3.1. While the particular implementation used in our experiments has a number of design choices, these are inherited from the prior methods. We believe it is therefore outside the scope of this paper (and outside of the conference paper length limits) to evaluate each of these decisions, but further discussion of these points can be found in the corresponding prior work: DQN (Mnih et. al. 2015), DDPG (Lillicrap et. al. 2016), TRPO-GAE (Schulman et. al., 2016). Below we respond to specific lines in this comment.\n\n\u201cIs the improved performance due to the use of the action-dependent control variate? Would the same setup but using a state-value baseline still perform just as well? Are the performance benefits due to the use of an off-policy advantage estimation algorithm, GAE(lambda)\u201d\nAll our policy gradient/REINFORCE-based benchmark methods defined in section 5 first paragraph are implemented with GAE(lambda=0.97), which itself includes a state-value baseline, to give a fair comparison. Q-prop is implemented on top of these, so all improvements seen come from the use of our action-dependent baseline (see Figures 2.b, 3.a, 3.b and Table 1, e.g. compare Figure 2.b. \"TRPO-5000\" vs \"TR-c-Q-Prop-5000\", which indicates the benefit of Q-Prop over TRPO using the same exact estimator, with the only difference being the action-dependent baseline). In Figure 3.a, we also compared REINFORCE with and without Q-Prop (see \u201cVPG\u201d and \u201cv-c-Q-Prop\u201d curves). Note that REINFORCE here also uses the GAE state-dependent baseline, making this a reasonable REINFORCE comparison of action dependent vs state dependent baselines. Lastly to clarify a detail, the original GAE paper uses on-policy advantage estimation and does not use replay buffer. This was in fact an important motivation for us to explore Q-Prop with a critic learned off-policy. We are in the process of running these experiments, but preliminary results suggest that an off-policy action-independent baseline (GAE with off-policy value function) performs worse than both an off-policy action-dependent baseline (Q-Prop) and an on-policy action-independent baseline (GAE).\n\n\u201cOr, would performance have been similar with an on-policy advantage estimation algorithm? What about if a different off-policy advantage estimation algorithm was used, like Retrace(lambda), GTD2, ETD, or WIS-LSTD? Or, is the improved performance due to the use of a replay buffer?\u201d \nWe do not think Q-Prop with on-policy advantage estimation will provide much benefit over GAE because GAE already performs on-policy advantage estimation via an on-policy state-value function estimator. Using Q-Prop with an on-policy action-value function estimator is a possible alternative to investigate, which we are currently working to add to the experiments. That said, we specifically focused on a simple off-policy action-value function fitting approach in this paper because we were aiming to improve sample-efficiency and wanted a simple demonstration to show using off-policy data in on-policy REINFORCE-style algorithms helps. We believe that good alternative off-policy advantage estimation algorithms are likely to do equally well or better, and we\u2019ve added a discussion in the last paragraph of Section 4 on this. Comparing with all prior off-policy estimation approaches like Retrace(lambda), GTD2, ETD, or WIS-LSTD is interesting empirical work for the future; however, we can only present a limited number of experimental results in a conference format, so we instead chose to implement Q-Prop on top of the current state-of-the-art deep RL policy gradient and actor-critic methods (TRPO and DDPG-style Q-function estimation), so as to cleanly illustrate the improvement provided by an action-dependent control variate obtained from off-policy data.\n\n\u201cComparisons are not performed between variants of Q-Prop that show the importances of these different components.\u201d\nFigure 2.a is dedicated to comparing various variants of Q-Prop: no-Q-Prop (\u201cTRPO\u201d) vs. standard Q-Prop (\u201cTR-Q-Prop\u201d) vs conservative (\u201cTR-c-Q-Prop\u201d) vs aggressive (\u201cTR-a-Q-Prop\u201d). We felt this is more important than comparing the off-policy learning component with all previous variants in the limited space available in a conference paper. The results in Figure 2.a that TRPO-GAE+conservative Q-Prop performed most reliably and is indeed what we expected.\n\n\u201cFor example, after reading this paper, it is not clear whether having the action-dependent baseline (or using the first order Taylor approximation for the baseline) is beneficial or not - it could be that the strong performance comes from GAE(lambda) or the use of a replay buffer.\u201d\nAgain, Q-Prop is implemented on top of GAE, and comparing TR-c-Q-Prop curves with TRPO curves in Figures 2.b, 3.a, 3.b and Table 1 clearly show that Q-Prop further improves upon GAE.\n\n\nTo summarize, the Q-Prop method itself is very simple, and most implementation designs are directly inherited from prior state-of-the-art method papers (TRPO-GAE and DDPG). Q-Prop offers considerable flexibility on what on-policy and off-policy advantage learning techniques, and we have added discussion in the second-to-last paragraph in Section 3.1. Given that the current off-policy advantage learning is based on prior techniques (e.g. DDPG), more sophisticated methods (e.g. Retrace(lambda)) will likely lead to further improvements, and we\u2019ve added a discussion of this to Section 4 last paragraph.\n\n3. Thank you for pointing this out. It can be made unbiased either if gamma = 1 or if gamma^t multiplies the gradient at each time step, which is generally undesirable. This is general for most policy gradient/REINFORCE methods. We have included the discussion on this in Section 2.1 below Eq. 2, and appropriately qualified all claims of unbiasedness in the paper.\n\n4. Thank you for pointing this out. Indeed, the paper can be written differently to emphasize the point that this is not specific to deep neural networks. However, our immediate goal in this paper is to make deep RL work better, and the design decisions in the algorithm are tailored to specifically addressed the difficulties of training deep architectures. Our aim in focusing on deep RL is to properly scope out the contribution of the paper. We are NOT claiming to propose a better RL algorithm for everything, because we don't have evidence for this, so while it's true that the contribution is not deep network specific, the evaluation is, and the focus of the paper reflects this limitation of the evaluation (due to the natural limitations of a fixed-size conference paper). We believe this scope is reasonable for ICLR, as evidenced by a number of other deep RL submissions that also propose improvements not specific to deep networks. It would be an interesting direction for future (e.g. an extended journal version) to evaluate the method in other, non-deep RL settings.\n\n5. Thank you. We will update the discussion about importance sampling to include those references. If you would like to suggest a comparison to an importance sampling method that is suitable for large continuous state and action spaces, we would be happy to compare to it. \n\n6. We already compare to a natural gradient method, namely TRPO, which is a variant of natural gradient with a specific step size rule (see Section 7 in Schulman et. al. 2015). On the tasks in our experiments, TRPO was shown to perform as well or better than standard natural policy gradient methods (see Figure 4 in Schulman et. al., 2015, and see Table 1 and Section 6 \u201cTNPG and TRPO\u201d in Duan et. al., 2016), and we therefore picked TRPO as the baseline in our experiments. In our paper, we show that adding Q-Prop further improves TRPO (see TR-c-Q-Prop curves with TRPO curves in Figures 2.b, 3.a, 3.b and Table 1). Thus, we have compared our method with a natural gradient algorithm and shown improvement.\n\n7. We fixed the problem by modifying rho_pi definition in Eq 2 and fixed the Q, A typos in Section 2.1 and \\hat{Q} typo in Section 3.1. Thank you!\n\nWe appreciate that you find the idea of Q-Prop interesting. We made the decision to prioritize comparisons with the state-of-the-art deep RL methods because our priority in this paper is to improve upon deep RL methods; however, we look forward to making more concrete analytical and empirical comparison with a broader class of RL techniques in future work, which we agree will be another valuable research contribution. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287701679, "id": "ICLR.cc/2017/conference/-/paper170/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJ3rcZcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper170/reviewers", "ICLR.cc/2017/conference/paper170/areachairs"], "cdate": 1485287701679}}}, {"tddate": null, "tmdate": 1482975099947, "tcdate": 1482975099947, "number": 3, "id": "rJVKwJMSe", "invitation": "ICLR.cc/2017/conference/-/paper170/official/review", "forum": "SJ3rcZcxl", "replyto": "SJ3rcZcxl", "signatures": ["ICLR.cc/2017/conference/paper170/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper170/AnonReviewer2"], "content": {"title": "An interesting approach for using control variables to improve stability of deep RL control", "rating": "7: Good paper, accept", "review": "The paper proposes using a first-order Taylor expansion as a control variate in policy gradient-style methods. Empirical results in dynamical control tasks suggest that this algorithm reduces the sample complexity, while the theoretical results presented suggest the algorithm is unbiased but of lower variance.\nThe use of control variates is very important and the present paper is an interesting approach in this direction. I am not fully convinced of the approach, because it is one of many possible, and the theoretical analysis relies on an approximation of the variance rather than exact calculations, which makes it less compelling. However, this paper is a step in the right direction so it is worth accepting. In the experiments, a few things need to be discussed further:\n- What is the running time of the proposed approach? The computation of the extra terms required looks like it could be expensive. running time comparison in addition to sample comparison should be included\n- The sensitivity to parameter settings of the proposed algorithm needs to be illustrated in separate graphs, since this is one of the main claims in the paper\n- It would be nice to have a toy example included in which one can actually compute exact values and plot learning curves to compare more directly bias and variance. It would especially be nice to do this with a task that includes rare states, which is the case in which variance of other methods (eg importance sampling) really becomes significant.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483071415909, "id": "ICLR.cc/2017/conference/-/paper170/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper170/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper170/AnonReviewer1", "ICLR.cc/2017/conference/paper170/AnonReviewer3", "ICLR.cc/2017/conference/paper170/AnonReviewer2", "ICLR.cc/2017/conference/paper170/AnonReviewer4"], "reply": {"forum": "SJ3rcZcxl", "replyto": "SJ3rcZcxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper170/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper170/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483071415909}}}, {"tddate": null, "tmdate": 1482961911313, "tcdate": 1482961911313, "number": 2, "id": "r11-EhWBx", "invitation": "ICLR.cc/2017/conference/-/paper170/official/review", "forum": "SJ3rcZcxl", "replyto": "SJ3rcZcxl", "signatures": ["ICLR.cc/2017/conference/paper170/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper170/AnonReviewer3"], "content": {"title": "Efficient Policy Gradient using a Critic ", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper presents  a model-free policy gradient approach for reinforcement learning that combines on-policy updates with an off-policy critic. The hope is to learn continuous control in a sample-efficient fashion. The approach is validated on a number of low-dimensional continuous control tasks in a simulated environment.\n\nThe paper is very well written, easy to follow, and provides an adequate context with which to appreciate the contributions it brings. Although this reviewer is not an expert in this literature, the proposed approach appears novel. The Q-Prop estimator appears to be a general and useful method for policy learning, and the experimental validations provide adequate support for the claims of improved sample efficiency. The detailed derivations given in the Supplementary Materials are very useful.\n\nI like the paper and I don\u2019t have much to comment on. Perhaps a discussion of the following aspects would add to the depth:\n1) comparison of the methods at a given computational cost, instead of by the number of episodes seen.\n2) discussion of the limitations of the technique: are there situations where convergence is difficult\n\nPossible typo: in equation (4), should we read $\u2026 + \\gamma Q_w( \u2026$ instead of $\u2026 + \\gamma Q( \u2026$ ?\nIf not, then what is Q() without subscript w?\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483071415909, "id": "ICLR.cc/2017/conference/-/paper170/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper170/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper170/AnonReviewer1", "ICLR.cc/2017/conference/paper170/AnonReviewer3", "ICLR.cc/2017/conference/paper170/AnonReviewer2", "ICLR.cc/2017/conference/paper170/AnonReviewer4"], "reply": {"forum": "SJ3rcZcxl", "replyto": "SJ3rcZcxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper170/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper170/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483071415909}}}, {"tddate": null, "tmdate": 1482864782587, "tcdate": 1482864782587, "number": 1, "id": "B1v5dVxBx", "invitation": "ICLR.cc/2017/conference/-/paper170/pre-review/question", "forum": "SJ3rcZcxl", "replyto": "SJ3rcZcxl", "signatures": ["ICLR.cc/2017/conference/paper170/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper170/AnonReviewer2"], "content": {"title": "No questions", "question": "No questions"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1482864783183, "id": "ICLR.cc/2017/conference/-/paper170/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper170/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper170/AnonReviewer2"], "reply": {"forum": "SJ3rcZcxl", "replyto": "SJ3rcZcxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper170/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper170/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1482864783183}}}, {"tddate": null, "tmdate": 1482241304446, "tcdate": 1482241304446, "number": 2, "id": "SklQBhLNe", "invitation": "ICLR.cc/2017/conference/-/paper170/public/comment", "forum": "SJ3rcZcxl", "replyto": "SJ3rcZcxl", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Related work", "comment": "Could you explain how Q-Prop is related to GProp from Compatible Value Gradients for Reinforcement Learning of Continuous Deep Policies by Balduzzi & Ghifary (http://arxiv.org/abs/1509.03005)?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "pdf": "/pdf/c210ff1a4868a532ec87ee0da3c6e4254ee567fb.pdf", "TL;DR": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "paperhash": "gu|qprop_sampleefficient_policy_gradient_with_an_offpolicy_critic", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["cam.ac.uk", "google.com", "eecs.berkeley.edu", "tuebingen.mpg.de"], "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287701679, "id": "ICLR.cc/2017/conference/-/paper170/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJ3rcZcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper170/reviewers", "ICLR.cc/2017/conference/paper170/areachairs"], "cdate": 1485287701679}}}], "count": 30}