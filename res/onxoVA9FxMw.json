{"notes": [{"id": "onxoVA9FxMw", "original": "NefcpmGjWg", "number": 557, "cdate": 1601308068347, "ddate": null, "tcdate": 1601308068347, "tmdate": 1614649657889, "tddate": null, "forum": "onxoVA9FxMw", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "On Position Embeddings in BERT", "authorids": ["~Benyou_Wang2", "~Lifeng_Shang1", "~Christina_Lioma1", "~Xin_Jiang1", "~Hao_Yang7", "~Qun_Liu1", "~Jakob_Grue_Simonsen1"], "authors": ["Benyou Wang", "Lifeng Shang", "Christina Lioma", "Xin Jiang", "Hao Yang", "Qun Liu", "Jakob Grue Simonsen"], "keywords": ["Position Embedding", "BERT", "pretrained language model."], "abstract": "Various Position Embeddings (PEs) have been proposed in Transformer based architectures~(e.g. BERT) to model word order. These are empirically-driven and perform well, but no formal framework exists to systematically study them. To address this, we present three properties of PEs that capture word distance in vector space:  translation invariance, monotonicity, and  symmetry. These properties formally capture the behaviour of PEs and allow us to reinterpret sinusoidal PEs in a principled way.\nMoreover, we propose a new probing test (called `identical word probing') and  mathematical  indicators to quantitatively detect the general  attention patterns with respect to the above properties. An empirical evaluation of seven PEs (and their combinations) for classification (GLUE) and span prediction (SQuAD) shows that: (1) both  classification and span prediction benefit from  translation invariance and local monotonicity, while symmetry slightly decreases performance;\n(2) The fully-learnable absolute PE performs better in classification, while relative PEs perform better in span prediction.  We contribute the first formal and quantitative analysis of desiderata for PEs, and  a principled discussion about their correlation to the performance of typical downstream tasks.", "one-sentence_summary": "This paper amis to understand and evaluate position embeddings, especially in pretrain language models", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|on_position_embeddings_in_bert", "pdf": "/pdf/be0283e323f1b118c975dbc46f7f75c59b467fe0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021on,\ntitle={On Position Embeddings in {\\{}BERT{\\}}},\nauthor={Benyou Wang and Lifeng Shang and Christina Lioma and Xin Jiang and Hao Yang and Qun Liu and Jakob Grue Simonsen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=onxoVA9FxMw}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "dlmpwJYA6n6", "original": null, "number": 1, "cdate": 1610040383172, "ddate": null, "tcdate": 1610040383172, "tmdate": 1610473976385, "tddate": null, "forum": "onxoVA9FxMw", "replyto": "onxoVA9FxMw", "invitation": "ICLR.cc/2021/Conference/Paper557/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "With the advent of non-recurrent sequence-processing models, it has become costumary to augment input tokens with positional embeddings providing implicit positional information. Despite their potentially crucial role in modern architectures, such positional embeddings are rarely addressed in analytical studies. The current paper provides a systematic investigation of positional embeddings, characterized in terms of properties such as monotonicity, translation invariance, and symmetry. These properties are studies for different positional embeddings using language models fine-tuned on two separated benchmarks, with an emphasis on visual analysis.\n\nThe authors provided an impressive rebuttal, adding many of the experiments required by the reviewers. The latter are still somewhat split about the paper. I lean towards the positive side. I find that some of the criticism, while valid, is not really granting a rejection, especially after the authors' clarifications. In particular, one reviewer assumed that the authors claim that symmetry should be a property of an ideal positional embedding, whereas the authors are rather studying whether it is an important property of them, in light of the previous literature. Some claims about the results being \"interesting\" or \"surprising\" enough might depend on what the reader is looking for in the paper. I think that many readers in the \"black box NLP\" community will find the methods and analyses presented in this paper interesting and useful.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Position Embeddings in BERT", "authorids": ["~Benyou_Wang2", "~Lifeng_Shang1", "~Christina_Lioma1", "~Xin_Jiang1", "~Hao_Yang7", "~Qun_Liu1", "~Jakob_Grue_Simonsen1"], "authors": ["Benyou Wang", "Lifeng Shang", "Christina Lioma", "Xin Jiang", "Hao Yang", "Qun Liu", "Jakob Grue Simonsen"], "keywords": ["Position Embedding", "BERT", "pretrained language model."], "abstract": "Various Position Embeddings (PEs) have been proposed in Transformer based architectures~(e.g. BERT) to model word order. These are empirically-driven and perform well, but no formal framework exists to systematically study them. To address this, we present three properties of PEs that capture word distance in vector space:  translation invariance, monotonicity, and  symmetry. These properties formally capture the behaviour of PEs and allow us to reinterpret sinusoidal PEs in a principled way.\nMoreover, we propose a new probing test (called `identical word probing') and  mathematical  indicators to quantitatively detect the general  attention patterns with respect to the above properties. An empirical evaluation of seven PEs (and their combinations) for classification (GLUE) and span prediction (SQuAD) shows that: (1) both  classification and span prediction benefit from  translation invariance and local monotonicity, while symmetry slightly decreases performance;\n(2) The fully-learnable absolute PE performs better in classification, while relative PEs perform better in span prediction.  We contribute the first formal and quantitative analysis of desiderata for PEs, and  a principled discussion about their correlation to the performance of typical downstream tasks.", "one-sentence_summary": "This paper amis to understand and evaluate position embeddings, especially in pretrain language models", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|on_position_embeddings_in_bert", "pdf": "/pdf/be0283e323f1b118c975dbc46f7f75c59b467fe0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021on,\ntitle={On Position Embeddings in {\\{}BERT{\\}}},\nauthor={Benyou Wang and Lifeng Shang and Christina Lioma and Xin Jiang and Hao Yang and Qun Liu and Jakob Grue Simonsen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=onxoVA9FxMw}\n}"}, "tags": [], "invitation": {"reply": {"forum": "onxoVA9FxMw", "replyto": "onxoVA9FxMw", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040383157, "tmdate": 1610473976365, "id": "ICLR.cc/2021/Conference/Paper557/-/Decision"}}}, {"id": "lQsimEZqETL", "original": null, "number": 4, "cdate": 1603954456446, "ddate": null, "tcdate": 1603954456446, "tmdate": 1606947339649, "tddate": null, "forum": "onxoVA9FxMw", "replyto": "onxoVA9FxMw", "invitation": "ICLR.cc/2021/Conference/Paper557/-/Official_Review", "content": {"title": "Interesting Take on Comparing Position Embeddings", "review": "This paper proposes a formal framework to compare position embeddings (PEs) and presents an empirical study comparing variants of absolute position embeddings (APEs) and relative position embeddings (RPEs) on three properties: 1) monotonicity, 2) translation invariance, and 3) symmetry and evaluates their performance on classification (GLUE) and span prediction tasks (SQUAD). The authors also report results on learnable sinusoidal APEs and learnable sinusoidal RPEs, PE variants which had not been previously proposed. \n\nThe first three properties seem well-motivated (monotonicity and translation invariance), but it is not obvious that symmetry should be a property of an ideal PE, or at least the paper is not convincing on this front. In a sentence (ABCD), doesn\u2019t the word A typically have a different relationship to B than B does to A?\n\nThe identical word probing test was a clever way to disentangle the impact of the word from that of the PEs. \n\nWhile it does seem valuable to more rigorously compare PEs as they are critical components of SOTA language models, the experimental results were not particularly convincing (e.g. although it\u2019s a very appealing story, it didn\u2019t seem so clear from the tables that APEs did better at classification and that RPEs did better at span prediction.)\n\nThe writing quality was borderline and there were a number of small errors:\n- fully-learnable APEs nearly meet all properties even under no constrains\u201d -> \u201cconstraints\u201d\n- Under the equation at the beginning of Section 3.1, \u201cword-word correspondence\u201d is repeated four times, which I am sure was not the intention. \n- nit: \u201csince relative distance with the same offset will be embedded as a same embedding.\u201d -> \u201cthe same embedding\u201d\n- \u201ccompared to far-way\u201d -> \u201cfaraway\u201d\n- \u201cattends more on forwarding tokens than backward tokens\u201d -> \u201cforward tokens\u201d?\n- nit: \u201cIn Transformer, where attention calculation does not\u2026\u201d -> \u201cwhere the attention calculation\u201d\n- \u201callows PEs o better perceive word order\u201d -> \u201cto\u201d", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper557/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper557/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Position Embeddings in BERT", "authorids": ["~Benyou_Wang2", "~Lifeng_Shang1", "~Christina_Lioma1", "~Xin_Jiang1", "~Hao_Yang7", "~Qun_Liu1", "~Jakob_Grue_Simonsen1"], "authors": ["Benyou Wang", "Lifeng Shang", "Christina Lioma", "Xin Jiang", "Hao Yang", "Qun Liu", "Jakob Grue Simonsen"], "keywords": ["Position Embedding", "BERT", "pretrained language model."], "abstract": "Various Position Embeddings (PEs) have been proposed in Transformer based architectures~(e.g. BERT) to model word order. These are empirically-driven and perform well, but no formal framework exists to systematically study them. To address this, we present three properties of PEs that capture word distance in vector space:  translation invariance, monotonicity, and  symmetry. These properties formally capture the behaviour of PEs and allow us to reinterpret sinusoidal PEs in a principled way.\nMoreover, we propose a new probing test (called `identical word probing') and  mathematical  indicators to quantitatively detect the general  attention patterns with respect to the above properties. An empirical evaluation of seven PEs (and their combinations) for classification (GLUE) and span prediction (SQuAD) shows that: (1) both  classification and span prediction benefit from  translation invariance and local monotonicity, while symmetry slightly decreases performance;\n(2) The fully-learnable absolute PE performs better in classification, while relative PEs perform better in span prediction.  We contribute the first formal and quantitative analysis of desiderata for PEs, and  a principled discussion about their correlation to the performance of typical downstream tasks.", "one-sentence_summary": "This paper amis to understand and evaluate position embeddings, especially in pretrain language models", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|on_position_embeddings_in_bert", "pdf": "/pdf/be0283e323f1b118c975dbc46f7f75c59b467fe0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021on,\ntitle={On Position Embeddings in {\\{}BERT{\\}}},\nauthor={Benyou Wang and Lifeng Shang and Christina Lioma and Xin Jiang and Hao Yang and Qun Liu and Jakob Grue Simonsen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=onxoVA9FxMw}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "onxoVA9FxMw", "replyto": "onxoVA9FxMw", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper557/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538140462, "tmdate": 1606915767655, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper557/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper557/-/Official_Review"}}}, {"id": "7OEdSRfr94P", "original": null, "number": 1, "cdate": 1603731372101, "ddate": null, "tcdate": 1603731372101, "tmdate": 1606553052685, "tddate": null, "forum": "onxoVA9FxMw", "replyto": "onxoVA9FxMw", "invitation": "ICLR.cc/2021/Conference/Paper557/-/Official_Review", "content": {"title": "An empirical study work that lacks convincing quantitative analysis", "review": "This paper studies the position embeddings of transform-based models, and proposed a unified position embedding evaluation method in three aspects, i.e., Monotonicity, Translation invariance and Symmetry, which can well summarise the properties of the existing position embedding methods.\n\n### Strengths of the paper\n\n1. It is good that the authors summarise three property for position embedding models, and discuss four related position embedding models under the three properties.\n2. The three properties proposed by this paper are suitable for most position embedding models.\n3. The experimental details are complete and easy to reproduce. Extensive experiment details are provided in the appendix.\n\n### Weaknesses of the paper\n\n1. The presentation and organization of this paper should be improved. Typos and language issues can be easily found, see the minor comments below. The contributions are not well highlighted in both the abstract and introduction section. \n\n2. The authors take many efforts to conduct experiments for the position embedding of BERTs, and provide an empirical study of the four existing position embedding models (fully learnable APEs (Gehring et al., 2017), (2) fixed sinusoidal APEs (Vaswani et al., 2017), (3) fully learnable RPEs (Shaw et al., 2018), and (4) fixed sinusoidal RPEs (Wei et al., 2019).). The author tested these models and their combinations over three benchmarking datasets, however, as an empirical study paper, the analysis is too weak.\n\n(1) For the qualitative result shown in Table 2 and Table 3, neither insight nor connection to three properties is provided for the result. In fact, the result shown in Section 4 is intuitive and not something new, since these are the basic motivations of the RPEs and learnable PEs. Moreover, the effectiveness of using both RPE and APE has already been validated in \"Self-Attention with Structural Position Representations, arXiv:1909.00383. 2019\".\n\n(2)  I would like to see, in the experiment part, some new experimental results and conclusions in terms of the four summarised properties, which are key contributions the authors claimed. However, what I see is just some position vector embedding similarities (i.e. Fig. 2 and Fig.3) in terms of the four position embedding models, where the results are also somehow expected and intuitive. The conclusions of the experimental results are too subjective. For example, from the analysis of Fig (2), the conclusion that : \u201cLastly, note that fully-learnable RPEs also do not significantly distinguish far-distant RPEs (from -64 to -20 and from 20 to 64), suggesting that truncating RPEs into a distance of 64, like (Shaw et al., 2018), is reasonable.\u201d  is a bit farfetched. In this figure the white part is as narrow as (-5,+5),  a further quantitative evidence in other forms for this conclusion will be more preferable. The same issue also exists in the conclusion of Figure (3): \u201cThis may be an advantage of RPEs over APEs to perceive forward and backward words, especially in span prediction tasks where capturing this matters.\u201d\n\n3. This paper lacks the discussion with the latest position embedding models such as,\n\n   [1] \"Learning to Encode Position for Transformer with Continuous Dynamical Model\". ICML. 2020\n\n   [2] \"Encoding Word Order in Complex Embeddings\",  ICLR. 2019, where the 'Translation' property is also discussed.\n\nOther minor comments:\n\n1. In the introduction section, \"distances in $\\mathbb{N}$ and $\\mathbb{R}^{D}$ \", $\\mathbb{N}$ and $\\mathbb{R}^{D}$ should be explained when they appear in the first place.\n\n2. The references should be formatted in a unified manner.\n\n3. Table 2, the bold indicators for the best performances are put on the wrong numbers, e.g. in QNLI, the bold should be 89.5,  in WNLI task it should be 51.3, and in STS-B it should be 87.5.\n\n   \nOverall, I think this paper indeed shows some interesting empirical results of position embedding models for BERT. But, the analysis is monotonous and too subjective, lacking the necessary mathematical quantitative indicators, which prevents it from being a general way to verify the conclusions in this paper.\n\n\n------\n### Comments after the discussion\n\nThank you for your detailed response. I think most of my concerns were addressed so I updated the score to 6.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper557/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper557/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Position Embeddings in BERT", "authorids": ["~Benyou_Wang2", "~Lifeng_Shang1", "~Christina_Lioma1", "~Xin_Jiang1", "~Hao_Yang7", "~Qun_Liu1", "~Jakob_Grue_Simonsen1"], "authors": ["Benyou Wang", "Lifeng Shang", "Christina Lioma", "Xin Jiang", "Hao Yang", "Qun Liu", "Jakob Grue Simonsen"], "keywords": ["Position Embedding", "BERT", "pretrained language model."], "abstract": "Various Position Embeddings (PEs) have been proposed in Transformer based architectures~(e.g. BERT) to model word order. These are empirically-driven and perform well, but no formal framework exists to systematically study them. To address this, we present three properties of PEs that capture word distance in vector space:  translation invariance, monotonicity, and  symmetry. These properties formally capture the behaviour of PEs and allow us to reinterpret sinusoidal PEs in a principled way.\nMoreover, we propose a new probing test (called `identical word probing') and  mathematical  indicators to quantitatively detect the general  attention patterns with respect to the above properties. An empirical evaluation of seven PEs (and their combinations) for classification (GLUE) and span prediction (SQuAD) shows that: (1) both  classification and span prediction benefit from  translation invariance and local monotonicity, while symmetry slightly decreases performance;\n(2) The fully-learnable absolute PE performs better in classification, while relative PEs perform better in span prediction.  We contribute the first formal and quantitative analysis of desiderata for PEs, and  a principled discussion about their correlation to the performance of typical downstream tasks.", "one-sentence_summary": "This paper amis to understand and evaluate position embeddings, especially in pretrain language models", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|on_position_embeddings_in_bert", "pdf": "/pdf/be0283e323f1b118c975dbc46f7f75c59b467fe0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021on,\ntitle={On Position Embeddings in {\\{}BERT{\\}}},\nauthor={Benyou Wang and Lifeng Shang and Christina Lioma and Xin Jiang and Hao Yang and Qun Liu and Jakob Grue Simonsen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=onxoVA9FxMw}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "onxoVA9FxMw", "replyto": "onxoVA9FxMw", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper557/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538140462, "tmdate": 1606915767655, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper557/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper557/-/Official_Review"}}}, {"id": "8ki42ZkgicZ", "original": null, "number": 2, "cdate": 1603929891000, "ddate": null, "tcdate": 1603929891000, "tmdate": 1606508415151, "tddate": null, "forum": "onxoVA9FxMw", "replyto": "onxoVA9FxMw", "invitation": "ICLR.cc/2021/Conference/Paper557/-/Official_Review", "content": {"title": "interesting study of PEs", "review": "Updates after author responses and revisions:\n\nI was positive about this paper previously and am glad to see that the authors have done a great deal to try to respond to our concerns and strengthen the paper. I am more positive about the paper now and have increased my score to an 8. I think this paper is going to be useful for the community and I know I will reference it later and direct others to it who are interested in learning more about position embeddings in transformers (whether or not it actually gets published). \n\n--------------\n\nThis paper studies position embeddings (PEs) in transformers, suggesting a few reasonable formal properties of PEs and determining whether these properties are captured by various choices for defining PEs. The paper\u00a0considers both absolute and relative PEs, and both fully-learnable and sinusoidal. A new variation is to learn frequencies in the sinusoidal PEs. Experiments are conducted by training BERT with various choices for PEs and evaluating on GLUE tasks and SQuAD. Fully-learnable absolute PEs, the default in BERT, works quite well overall, but adding learnable sinusoidal relative PEs as well can improve performance on average. There are also visualizations of PE dot products for the various methods, showing that learning PEs in the context of BERT training can yield PEs that satisfy the formal properties (for the most part) laid out by the authors. \n\nI really enjoyed reading this paper. PEs in BERT have been the subject of a lot of informal discussion over the past couple of years, but I don't think I've read a\u00a0paper that studies the topic with such breadth and depth. With some doable improvements and clarifications, I think the paper can become an excellent resource for others interested in representing position and distance in transformer-like models.\u00a0\n\nI like the idea of learned sinusoidal embeddings and think that idea can be potentially useful for other researchers. The paper notes that with the fixed sinusoidal embeddings, distances beyond about 50 are not distinguished, but with learning they can be. The experiments show that learning frequencies in sinusoidal PEs works better than using fixed sinusoidal PEs. I'm curious what the learned frequencies look like and how similar they are to the original frequencies chosen by the transformer authors.\u00a0\n\nSome other thoughts I had while reading:\u00a0\n\nAt least for language tasks with a given window, could we just re-use the learned frequencies and use fixed sinusoidal PEs in the future with those same learned frequencies? I guess a similar choice can be made with fully\u00a0learned PEs. Do we really need to learn PEs from scratch every time? What aspects of the data or task would influence this? Maybe given their experience, the authors could hand-design useful general-purpose PEs? \n\nBased on the discussion in Sec. 5.3, maybe a good recommendation for BERT-like models would be to use a single learnable PE only for [CLS] and something else (e.g., learned sinusoidal APEs or RPEs) for other positions? This seems easy enough to do in practice and may bring the best of both worlds. \n\n\nBelow are some questions and points of confusion I had:\n\nIn the fully-learnable APE experiments described in Sec 4.2, for the first 10 epochs, were the unseen PEs randomly initialized and finetuned in the downstream SQuAD experiments?\u00a0\n\nI'm confused as to why some of the visualizations in Fig 3 show white bands along the diagonal (d, f, and g). I would have expected all to have dark bands along the diagonal (as in b, c, d, and h).\u00a0\n\nI was super confused by the identical word probing parts of the paper. Sec. 3.1 includes a sentence beginning\u00a0with \"This shows that the selection of words\". The sentence is awkward and confusing to me. I don't know what the \"This\" is referring to. It seems like there should have been a result reported there for the \"This\" to refer to. The following sentence, starting with \"Namely\", is also confusing to me. The description of identical word probing then points to Sec 5.2, and the Sec 5.2 section title is \"IDENTICAL WORD PROBING\", but it seems to be focused on describing Fig 3, which has as its caption \"Dot products between relative position vectors\". Where are the identical word probing results actually reported?\u00a0\n\nIn the \"Pre-training\" paragraph of Section 4, it is mentioned that a pretrained BERT checkpoint is used, but also that other BERT models with other position embeddings were trained. How was the pretrained model actually used? Was it used to initialize the other models or were all models trained from scratch?\n\nMinor things:\u00a0\n\nIn the first equation in Sec 3.2, there are a couple of instances of K/2 which I think should\u00a0be D/2 instead. Also, the vectors start with \\omega_1 but the summation at the end of the equation starts with \\omega_0.\u00a0\n\nI'm confused by the bolding in Table 2. The best number in each column is not always in boldface (QNLI, WNLI, etc.). Also, sometimes when there is a tie for the best result, multiple numbers are in bold, while other times only one result is in bold.\u00a0\n\n\nTypos:\n\np. 1: \"constrains\" --> \"constraints\"\n\nfootnote 4: \"seeing\" --> \"See\"\n\nSec. 4.1: \"outperform notably\" --> \"notably\u00a0outperform\"\n\nSec. 5.3: \"PEs o\" --> \"PEs to\"\n\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper557/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper557/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Position Embeddings in BERT", "authorids": ["~Benyou_Wang2", "~Lifeng_Shang1", "~Christina_Lioma1", "~Xin_Jiang1", "~Hao_Yang7", "~Qun_Liu1", "~Jakob_Grue_Simonsen1"], "authors": ["Benyou Wang", "Lifeng Shang", "Christina Lioma", "Xin Jiang", "Hao Yang", "Qun Liu", "Jakob Grue Simonsen"], "keywords": ["Position Embedding", "BERT", "pretrained language model."], "abstract": "Various Position Embeddings (PEs) have been proposed in Transformer based architectures~(e.g. BERT) to model word order. These are empirically-driven and perform well, but no formal framework exists to systematically study them. To address this, we present three properties of PEs that capture word distance in vector space:  translation invariance, monotonicity, and  symmetry. These properties formally capture the behaviour of PEs and allow us to reinterpret sinusoidal PEs in a principled way.\nMoreover, we propose a new probing test (called `identical word probing') and  mathematical  indicators to quantitatively detect the general  attention patterns with respect to the above properties. An empirical evaluation of seven PEs (and their combinations) for classification (GLUE) and span prediction (SQuAD) shows that: (1) both  classification and span prediction benefit from  translation invariance and local monotonicity, while symmetry slightly decreases performance;\n(2) The fully-learnable absolute PE performs better in classification, while relative PEs perform better in span prediction.  We contribute the first formal and quantitative analysis of desiderata for PEs, and  a principled discussion about their correlation to the performance of typical downstream tasks.", "one-sentence_summary": "This paper amis to understand and evaluate position embeddings, especially in pretrain language models", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|on_position_embeddings_in_bert", "pdf": "/pdf/be0283e323f1b118c975dbc46f7f75c59b467fe0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021on,\ntitle={On Position Embeddings in {\\{}BERT{\\}}},\nauthor={Benyou Wang and Lifeng Shang and Christina Lioma and Xin Jiang and Hao Yang and Qun Liu and Jakob Grue Simonsen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=onxoVA9FxMw}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "onxoVA9FxMw", "replyto": "onxoVA9FxMw", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper557/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538140462, "tmdate": 1606915767655, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper557/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper557/-/Official_Review"}}}, {"id": "sUDu4-E3naI", "original": null, "number": 11, "cdate": 1606183005690, "ddate": null, "tcdate": 1606183005690, "tmdate": 1606183005690, "tddate": null, "forum": "onxoVA9FxMw", "replyto": "onxoVA9FxMw", "invitation": "ICLR.cc/2021/Conference/Paper557/-/Official_Comment", "content": {"title": "The end of disucssion phase approaching", "comment": "Dear Reviewers and Area Chair,\n\n\nCould you please go over our responses and revision to have more possible interactions before this Tuesday (24th Nov.)?\nBased on the first-round reviews, we have provided the suggested experiments, quantitative analysis, detailed explanations, and some interesting discussion. \n\nThanks for your time reviewing the paper, and your comments have substantially improved the quality of this paper.\n\n\nIn the revision,\n\n(1) We have added quantitative measurement for the three properties in Sec. 4.1.2 (Table 2) and Appendix B;\n\n(2) We have conducted an analysis of the correlations between properties and performance in individual tasks, in Sec. 4.3 (Table 4).\n\n(3) We have investigated the difference between learned frequencies and predefined ones (Vaswani et al. 2017) in Appendix A.3.\n\n(4) We have shown how models are insensitive to long-distance offsets in Appendix C.\n\n(5) We have investigated how PEs evolve with an increasing number of training steps in Appendix F, in particular, PEs gradually increasingly satisfy some of the three main properties we examine in the paper.\n\n(6)The white band effect indicates words do not attend themselves in learnable RPEs, as previously stated by Clark et al. 2019.\n\n(7)Comapring the position related attention patterns with GPT and a machine translation model in App. I\n\nAlso, we carefully proofread the whole paper: fixed language iusses and removed all subjective statements to make the discussion factual. \n\nBest,\n\nThe authors"}, "signatures": ["ICLR.cc/2021/Conference/Paper557/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper557/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Position Embeddings in BERT", "authorids": ["~Benyou_Wang2", "~Lifeng_Shang1", "~Christina_Lioma1", "~Xin_Jiang1", "~Hao_Yang7", "~Qun_Liu1", "~Jakob_Grue_Simonsen1"], "authors": ["Benyou Wang", "Lifeng Shang", "Christina Lioma", "Xin Jiang", "Hao Yang", "Qun Liu", "Jakob Grue Simonsen"], "keywords": ["Position Embedding", "BERT", "pretrained language model."], "abstract": "Various Position Embeddings (PEs) have been proposed in Transformer based architectures~(e.g. BERT) to model word order. These are empirically-driven and perform well, but no formal framework exists to systematically study them. To address this, we present three properties of PEs that capture word distance in vector space:  translation invariance, monotonicity, and  symmetry. These properties formally capture the behaviour of PEs and allow us to reinterpret sinusoidal PEs in a principled way.\nMoreover, we propose a new probing test (called `identical word probing') and  mathematical  indicators to quantitatively detect the general  attention patterns with respect to the above properties. An empirical evaluation of seven PEs (and their combinations) for classification (GLUE) and span prediction (SQuAD) shows that: (1) both  classification and span prediction benefit from  translation invariance and local monotonicity, while symmetry slightly decreases performance;\n(2) The fully-learnable absolute PE performs better in classification, while relative PEs perform better in span prediction.  We contribute the first formal and quantitative analysis of desiderata for PEs, and  a principled discussion about their correlation to the performance of typical downstream tasks.", "one-sentence_summary": "This paper amis to understand and evaluate position embeddings, especially in pretrain language models", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|on_position_embeddings_in_bert", "pdf": "/pdf/be0283e323f1b118c975dbc46f7f75c59b467fe0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021on,\ntitle={On Position Embeddings in {\\{}BERT{\\}}},\nauthor={Benyou Wang and Lifeng Shang and Christina Lioma and Xin Jiang and Hao Yang and Qun Liu and Jakob Grue Simonsen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=onxoVA9FxMw}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "onxoVA9FxMw", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper557/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper557/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper557/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper557/Authors|ICLR.cc/2021/Conference/Paper557/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper557/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869681, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper557/-/Official_Comment"}}}, {"id": "5lATlOh-Q3z", "original": null, "number": 10, "cdate": 1606130463768, "ddate": null, "tcdate": 1606130463768, "tmdate": 1606182491809, "tddate": null, "forum": "onxoVA9FxMw", "replyto": "7OEdSRfr94P", "invitation": "ICLR.cc/2021/Conference/Paper557/-/Official_Comment", "content": {"title": "Reply to Reviewer3's comment", "comment": "**Comment1:  For the qualitative result shown in Table 2 and Table 3, neither insight nor connection to three properties is provided for the result. In fact, the result shown in Section 4 is intuitive and not something new, since these are the basic motivations of the RPEs and learnable PEs. Moreover, the effectiveness of using both RPE and APE has already been validated in \"Self-Attention with Structural Position Representations, arXiv:1909.00383. 2019\".**\n\nWe have added quantitative indicators for the three properties in Sec. 4.2 (Table 2), and have added a correlation study to quantitatively measure how these properties correlate with the performance of downstream tasks, in Sec. 6 (Table 5):  both classification and span prediction benefit from the translation invariance and local monotonicity properties of PEs, while symmetry slightly decreases their performance because of inability to perceive directions. See. Tab. 5 for more details.\n\nAlso, we have added the suggested reference in App. H.\n\n**Comment2:  I would like to see, in the experiment part, some new experimental results and conclusions in terms of the four summarised properties, which are key contributions the authors claimed. However, what I see is just some position vector embedding similarities (i.e. Fig. 2 and Fig.3) in terms of the four position embedding models, where the results are also somehow expected and intuitive. The conclusions of the experimental results are too subjective. For example, from the analysis of Fig (2), the conclusion that : \u201cLastly, note that fully-learnable RPEs also do not significantly distinguish far-distant RPEs (from -64 to -20 and from 20 to 64), suggesting that truncating RPEs into a distance of 64, like (Shaw et al., 2018), is reasonable.\u201d is a bit farfetched. In this figure the white part is as narrow as (-5,+5), a further quantitative evidence in other forms for this conclusion will be more preferable. The same issue also exists in the conclusion of Figure (3): \u201cThis may be an advantage of RPEs over APEs to perceive forward and backward words, especially in span prediction tasks where capturing this matters.\u201d**\n\n\n1. Other than position vector embedding similarities, we have added some results concerning a new probing test,  have added quantitative indicators to measure the properties, and have added quantitative correlations between the properties and downstream tasks.\n\n2. We apologize that there were some subjective conclusions in the initial versions.  As suggested by you, we have (1) designed some quantitative indicators to measuring these properties; and (2) we have computed quantitative correlations between the properties and downstream tasks. All subjective arguments have been carefully checked and removed in the latest version.\n\n\n\n3. We have revised the description of distinguishable far-distant RPEs by carefully stating the white parts and dark parts. The white parts (-5,+5) indicates the relative position vectors for small offsets (e.g., $\\{p_{-5},\\cdots, P_0, \\cdots, P_{-5} \\}$ are notably different to other relative position vectors. \nIn Tab 6 of App. D, we checked the dark parts by adopting a normalized metric, i.e., cosine similarity between any two relative position vectors, the results show that RPE with big offsets (e.g., $\\{P_{-64},\\cdots,P_{-20}\\}$ and $\\{ P_{20},\\cdots, P_{60}\\}$) are very similar (with cosine similarities approaching or exceeding 0.95).\n\n4. We have added two quantitative indicators regarding directions in the new version, and carefully revised the augment about `an advantage of RPEs over APE'.\n\n\n\n**Comment3: This paper lacks the discussion with the latest position embedding models such as Liu et al. ICML 2020 and Wang et al. ICLR 2020,**\n\n\nWe have now added more discussion of the related work (Liu et al. ICML 2020 and Wang et al. ICLR 2020) in the App. H due to page limitations. We also discuss the recently-released paper by Wang and Yun-Nung  below.\n\nWang, Yu-An, and Yun-Nung Chen. \"What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding.\" arXiv preprint arXiv:2010.04903 (2020). Accepted by EMNLP, publicly available after the ICLR deadline.\n\n**Comment4:  Overall, I think this paper indeed shows some interesting empirical results of position embedding models for BERT. But, the analysis is monotonous and too subjective, lacking the necessary mathematical quantitative indicators, which prevents it from being a general way to verify the conclusions in this paper.**\n\nIn the latest version, we have removed all subjective statements and made the discussion factual using newly-proposed quantitative indicators and statistical correlations for the defined properties.\nWe have added more interesting findings as stated above thanks to the extra page available page (we have also moved one of the original subsections to the appendix to save more space for analysis)."}, "signatures": ["ICLR.cc/2021/Conference/Paper557/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper557/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Position Embeddings in BERT", "authorids": ["~Benyou_Wang2", "~Lifeng_Shang1", "~Christina_Lioma1", "~Xin_Jiang1", "~Hao_Yang7", "~Qun_Liu1", "~Jakob_Grue_Simonsen1"], "authors": ["Benyou Wang", "Lifeng Shang", "Christina Lioma", "Xin Jiang", "Hao Yang", "Qun Liu", "Jakob Grue Simonsen"], "keywords": ["Position Embedding", "BERT", "pretrained language model."], "abstract": "Various Position Embeddings (PEs) have been proposed in Transformer based architectures~(e.g. BERT) to model word order. These are empirically-driven and perform well, but no formal framework exists to systematically study them. To address this, we present three properties of PEs that capture word distance in vector space:  translation invariance, monotonicity, and  symmetry. These properties formally capture the behaviour of PEs and allow us to reinterpret sinusoidal PEs in a principled way.\nMoreover, we propose a new probing test (called `identical word probing') and  mathematical  indicators to quantitatively detect the general  attention patterns with respect to the above properties. An empirical evaluation of seven PEs (and their combinations) for classification (GLUE) and span prediction (SQuAD) shows that: (1) both  classification and span prediction benefit from  translation invariance and local monotonicity, while symmetry slightly decreases performance;\n(2) The fully-learnable absolute PE performs better in classification, while relative PEs perform better in span prediction.  We contribute the first formal and quantitative analysis of desiderata for PEs, and  a principled discussion about their correlation to the performance of typical downstream tasks.", "one-sentence_summary": "This paper amis to understand and evaluate position embeddings, especially in pretrain language models", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|on_position_embeddings_in_bert", "pdf": "/pdf/be0283e323f1b118c975dbc46f7f75c59b467fe0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021on,\ntitle={On Position Embeddings in {\\{}BERT{\\}}},\nauthor={Benyou Wang and Lifeng Shang and Christina Lioma and Xin Jiang and Hao Yang and Qun Liu and Jakob Grue Simonsen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=onxoVA9FxMw}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "onxoVA9FxMw", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper557/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper557/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper557/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper557/Authors|ICLR.cc/2021/Conference/Paper557/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper557/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869681, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper557/-/Official_Comment"}}}, {"id": "H5mcuDykRX", "original": null, "number": 9, "cdate": 1606130257151, "ddate": null, "tcdate": 1606130257151, "tmdate": 1606182214666, "tddate": null, "forum": "onxoVA9FxMw", "replyto": "7OEdSRfr94P", "invitation": "ICLR.cc/2021/Conference/Paper557/-/Official_Comment", "content": {"title": "In revision, we added quantitative  measurement for the three properties and  conduct statistical correlations between properties and performance in individual tasks", "comment": "\nWe thank you for the helpful comments and suggestions. We provide our responses as follows.\n\n**Weaknesses1:  The presentation and organization of this paper should be improved. Typos and language issues can be easily found, see the minor comments below. The contributions are not well highlighted in both the abstract and introduction section**\n\nWe apologize for the typos and language issues; we have now fixed them.\nIn the latest version, we have rewritten the introduction and abstract; all contributions are now listed in the introduction. In addition, the paper has been scoured for typos and language issues.\n\n**Weaknesses2: The authors take many efforts to conduct experiments for the position embedding of BERTs, and provide an empirical study of the four existing position embedding models (fully learnable APEs (Gehring et al., 2017), (2) fixed sinusoidal APEs (Vaswani et al., 2017), (3) fully learnable RPEs (Shaw et al., 2018), and (4) fixed sinusoidal RPEs (Wei et al., 2019)). The author tested these models and their combinations over three benchmarking datasets, however, as an empirical study paper, the analysis is too weak.**\n\nIn the latest version, we have rewritten most of the paper devoted to the analysis of the experimental results, and we have added the following analyses:\n\n(1)  We have added quantitative  measurement for the three properties in Sec. 4.1.2 (Table 2) and Appendix B;\n\n(2) We have conducted an analysis of the correlations between properties and performance in individual tasks, in Sec. 4.3 (Table 4).\n\n(3)  We have investigated the difference between learned frequencies and predefined ones (Vaswani et al. 2017) in  Appendix A.3. \n\n(4) We have shown how models are insensitive to long-distance offsets in Appendix C.\n\n(5) We have investigated how PEs evolve with an increasing number of training steps in Appendix F, in particular, PEs gradually increasingly satisfy some of the three main properties we examine in the paper.\n\n(6)The white band effect indicates words do not attend themselves in learnable RPEs, as previously stated by Clark et al. 2019.\n\n(7)Comapring the position related attention patterns with GPT and a machine translation model in App. I\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper557/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper557/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Position Embeddings in BERT", "authorids": ["~Benyou_Wang2", "~Lifeng_Shang1", "~Christina_Lioma1", "~Xin_Jiang1", "~Hao_Yang7", "~Qun_Liu1", "~Jakob_Grue_Simonsen1"], "authors": ["Benyou Wang", "Lifeng Shang", "Christina Lioma", "Xin Jiang", "Hao Yang", "Qun Liu", "Jakob Grue Simonsen"], "keywords": ["Position Embedding", "BERT", "pretrained language model."], "abstract": "Various Position Embeddings (PEs) have been proposed in Transformer based architectures~(e.g. BERT) to model word order. These are empirically-driven and perform well, but no formal framework exists to systematically study them. To address this, we present three properties of PEs that capture word distance in vector space:  translation invariance, monotonicity, and  symmetry. These properties formally capture the behaviour of PEs and allow us to reinterpret sinusoidal PEs in a principled way.\nMoreover, we propose a new probing test (called `identical word probing') and  mathematical  indicators to quantitatively detect the general  attention patterns with respect to the above properties. An empirical evaluation of seven PEs (and their combinations) for classification (GLUE) and span prediction (SQuAD) shows that: (1) both  classification and span prediction benefit from  translation invariance and local monotonicity, while symmetry slightly decreases performance;\n(2) The fully-learnable absolute PE performs better in classification, while relative PEs perform better in span prediction.  We contribute the first formal and quantitative analysis of desiderata for PEs, and  a principled discussion about their correlation to the performance of typical downstream tasks.", "one-sentence_summary": "This paper amis to understand and evaluate position embeddings, especially in pretrain language models", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|on_position_embeddings_in_bert", "pdf": "/pdf/be0283e323f1b118c975dbc46f7f75c59b467fe0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021on,\ntitle={On Position Embeddings in {\\{}BERT{\\}}},\nauthor={Benyou Wang and Lifeng Shang and Christina Lioma and Xin Jiang and Hao Yang and Qun Liu and Jakob Grue Simonsen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=onxoVA9FxMw}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "onxoVA9FxMw", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper557/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper557/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper557/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper557/Authors|ICLR.cc/2021/Conference/Paper557/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper557/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869681, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper557/-/Official_Comment"}}}, {"id": "tmXSZ-3kG9", "original": null, "number": 7, "cdate": 1606129846997, "ddate": null, "tcdate": 1606129846997, "tmdate": 1606131632576, "tddate": null, "forum": "onxoVA9FxMw", "replyto": "8ki42ZkgicZ", "invitation": "ICLR.cc/2021/Conference/Paper557/-/Official_Comment", "content": {"title": "Reply to Reviewer2's thoughts", "comment": "\nWe thank you for the helpful comments and suggestions.\nWe provide our responses as follows.\n\n**I'm curious what the learned frequencies look like and how similar they are to the original frequencies chosen by the transformer authors**\n\n\nIn appendix A3, Figure 4(a)  shows the learned frequencies, and Figure 4(b) shows how dot product between two n-distance position vectors learned from pre-trained models and fine-tuned tasks. The difference between the original one is not notable,  probably due to the fact that we initialize it using the frequencies of original transformers Vaswani et al.\\ ( $\\omega_i = ({1}/{10000})^{2i/D}$).\n\nIt is worth noting that Parascandolo et al.\\ observed that  there are infinitely many and shallow local minima during training with  sine/cosine functions; therefore, having a good  initialization (like  $\\omega_i = ({1}/{10000})^{2i/D}$) may be important. Anyway, we will release the result which trains from scratch (random initialization for frequencies and training BERT from scratch) in the next version and keep you updated, since it takes a few weeks. \n\n[1] Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. \"Attention is all you need.\" In Advances in neural information processing systems, pp. 5998-6008. 2017.\n\n[2] Parascandolo, Giambattista, Heikki Huttunen, and Tuomas Virtanen. \"Taming the waves: sine as activation function in deep neural networks.\" (2016).\n\n\n**Thought1 : Do we really need to learn PEs from scratch every time?**\n\n\nGenerally, we need to learn PEs from scratch for individual tasks since it could fit the tasks better. Wang & Chen reuses these PEs as initialization in various tasks and see  difference in performance. \n\nWang, Yu-An, and Yun-Nung Chen. \"What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding.\" arXiv preprint arXiv:2010.04903 (2020). Accepted by EMNLP, publicly available after the ICLR deadline.\n\n\n**Thought2: What aspects of the data or task would influence this? **\nAs concluded in the revised paper,   both classification and span prediction benefit from the translation invariance and local monotonicity properties of PEs, while symmetry slightly decreases their performance because of the inability to perceive directions. See. Tab. 5 for more details.\n\n**Thought3: Maybe given their experience, the authors could hand-design useful general-purpose PEs?**\n\n\nFor APEs, ideally,  we suggest that it should: (1) untie the  [CLS] with other positions like Ke et al. 2020; (2) For any position embeddings $P \\in  \\mathbb{R}^{L \\times D}$, the outer product between itself should result in a Toeplitz matrix (a matrix in which each descending diagonal from left to right is constant):\n\n$P P^T = \\begin{bmatrix} r_0 & r_1 \t& \\cdots \t& r_{L-1}   \t\\\\\\\\  r'_{-1} \t& r_0 \t& \\cdots \t& r_{L-2}  \\\\\\\\ \\cdots \t& \\cdots \t& \\cdots \t& \\cdots    \\\\\\\\     r'_{-L+1}  & r'_{-L+2} \t& \\cdots \t& r_0\t\\\\\\\\      \\end{bmatrix}$ ( The matrix has some edit error for last row, see following matrix by replacing the last  row with the one in this line)\n\n$P P^T = \\begin{bmatrix} r_0 & r_1 \t& \\cdots  \t& r_{L-1} \\\\\\\\ r'_{-1} \t& r_0 \t& \\cdots\t   &  r_{L-2}   \\\\\\\\  \\cdots \t& \\cdots \t& \\cdots \t& \\cdots    \\\\\\\\   \\cdots & \\cdots   &   \\cdots      & r_0  \\\\\\\\    \\end{bmatrix}$\n\nsuch that\n$r_0 >  r_{-1} > \\cdots > r_{-L+1} $ and\n$r_0 >  r_{-1}' > \\cdots > r_{-L+1}' $\n\nComparing to APEs, RPEs are more preferable since they satisfy translation invariance during parameterizations. The other two properties are somehow too complicated to analyze.\n\n\n**Thought4: Based on the discussion in Sec. 5.3, maybe a good recommendation for BERT-like models would be to use a single learnable PE only for [CLS] and something else (e.g., learned sinusoidal APEs or RPEs) for other positions? This seems easy enough to do in practice and may bring the best of both worlds.**\n\nWe totally agree. A very recent paper (Ke et al. 2020) combines CLS with absolute position embeddings; it would be nice to try this with RPEs. We have also discussed it in Appendix H of our paper (and we believe that our paper provides additional motivation for (Ke et al. 2020)).\n\nKe, Guolin, Di He, and Tie-Yan Liu. \"Rethinking the Positional Encoding in Language Pre-training.\" arXiv preprint arXiv:2006.15595 (2020).\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper557/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper557/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Position Embeddings in BERT", "authorids": ["~Benyou_Wang2", "~Lifeng_Shang1", "~Christina_Lioma1", "~Xin_Jiang1", "~Hao_Yang7", "~Qun_Liu1", "~Jakob_Grue_Simonsen1"], "authors": ["Benyou Wang", "Lifeng Shang", "Christina Lioma", "Xin Jiang", "Hao Yang", "Qun Liu", "Jakob Grue Simonsen"], "keywords": ["Position Embedding", "BERT", "pretrained language model."], "abstract": "Various Position Embeddings (PEs) have been proposed in Transformer based architectures~(e.g. BERT) to model word order. These are empirically-driven and perform well, but no formal framework exists to systematically study them. To address this, we present three properties of PEs that capture word distance in vector space:  translation invariance, monotonicity, and  symmetry. These properties formally capture the behaviour of PEs and allow us to reinterpret sinusoidal PEs in a principled way.\nMoreover, we propose a new probing test (called `identical word probing') and  mathematical  indicators to quantitatively detect the general  attention patterns with respect to the above properties. An empirical evaluation of seven PEs (and their combinations) for classification (GLUE) and span prediction (SQuAD) shows that: (1) both  classification and span prediction benefit from  translation invariance and local monotonicity, while symmetry slightly decreases performance;\n(2) The fully-learnable absolute PE performs better in classification, while relative PEs perform better in span prediction.  We contribute the first formal and quantitative analysis of desiderata for PEs, and  a principled discussion about their correlation to the performance of typical downstream tasks.", "one-sentence_summary": "This paper amis to understand and evaluate position embeddings, especially in pretrain language models", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|on_position_embeddings_in_bert", "pdf": "/pdf/be0283e323f1b118c975dbc46f7f75c59b467fe0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021on,\ntitle={On Position Embeddings in {\\{}BERT{\\}}},\nauthor={Benyou Wang and Lifeng Shang and Christina Lioma and Xin Jiang and Hao Yang and Qun Liu and Jakob Grue Simonsen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=onxoVA9FxMw}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "onxoVA9FxMw", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper557/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper557/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper557/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper557/Authors|ICLR.cc/2021/Conference/Paper557/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper557/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869681, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper557/-/Official_Comment"}}}, {"id": "YJUKK0t_KYA", "original": null, "number": 8, "cdate": 1606130031050, "ddate": null, "tcdate": 1606130031050, "tmdate": 1606130094831, "tddate": null, "forum": "onxoVA9FxMw", "replyto": "8ki42ZkgicZ", "invitation": "ICLR.cc/2021/Conference/Paper557/-/Official_Comment", "content": {"title": "Reply to Reviewer2's questions and  confusion", "comment": "\n**Q1:  In the fully-learnable APE experiments described in Sec 4.2, for the first 10 epochs, were the unseen PEs randomly initialized and fine-tuned in the downstream SQuAD experiments?**\n\nYes, the unseen PEs are randomly initialized and fine-tuned in the downstream SQuAD experiments. We have clarified this in the latest version.\n\n**Q2: I'm confused as to why some of the visualizations in Fig 3 show white bands along the diagonal (d, f, and g). I would have expected all to have dark bands along the diagonal (as in b, c, d, and h).**\n\nIn the latest version, we have investigated this \"white band\" phenomenon using identical word probing tests with (an average of) larger-scaled and more diverse words (see * newly-added Appendix J*). The cause of the effect is that some words generally do not attend themselves, as also reported in (Clark et al., 2019).\n\nClark, Kevin, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. \"What does bert look at? an analysis of BERT's attention.\" arXiv preprint arXiv:1906.04341 (2019).\n\nOne interesting point is that the white band effect only involves learnable RPEs: the more trainable parameters in the RPE, the more notable the effect becomes. Hence, the effect is stronger in fully-learnable RPEs than in learnable sinusoidal RPEs, which in turn exhibit the effect more strongly than fixed sinusoidal RPEs. We leave the further investigation of this for future work.\n\n**Q3: I was super confused by the identical word probing parts of the paper. Sec. 3.1 includes a sentence beginning with \"This shows that the selection of words\". The sentence is awkward and confusing to me.  ...**\n\nWe apologize for it,  we have improved the writing in the latest version, and have carefully proofread the original text.\n\nThe caption to describe Fig.3  was wrong; Fig.3  was supposed to concern identical word probing of A. And we rewrote \"this shows ...\".\n\n**Q4: In the \"Pre-training\" paragraph of Section 4, it is mentioned that a pre-trained BERT checkpoint is used, but also that other BERT models with other position embeddings were trained. How was the pre-trained model actually used? Was it used to initialize the other models or were all models trained from scratch?**\n\nIn all BERT models with different PE variants, we reuse most of the trained parameters (except for PE components) of a BERT base checkpoint as initialization, while PEs were re-designed. In the BERT-base setting, they are not trained from scratch, which takes a longer time to train. In the BERT-medium setting, we instead tried to train from scratch as this could potentially be more efficient (training on the BERT medium setting is in general much faster).\n\nWe have clarified the above in the revised version.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper557/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper557/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Position Embeddings in BERT", "authorids": ["~Benyou_Wang2", "~Lifeng_Shang1", "~Christina_Lioma1", "~Xin_Jiang1", "~Hao_Yang7", "~Qun_Liu1", "~Jakob_Grue_Simonsen1"], "authors": ["Benyou Wang", "Lifeng Shang", "Christina Lioma", "Xin Jiang", "Hao Yang", "Qun Liu", "Jakob Grue Simonsen"], "keywords": ["Position Embedding", "BERT", "pretrained language model."], "abstract": "Various Position Embeddings (PEs) have been proposed in Transformer based architectures~(e.g. BERT) to model word order. These are empirically-driven and perform well, but no formal framework exists to systematically study them. To address this, we present three properties of PEs that capture word distance in vector space:  translation invariance, monotonicity, and  symmetry. These properties formally capture the behaviour of PEs and allow us to reinterpret sinusoidal PEs in a principled way.\nMoreover, we propose a new probing test (called `identical word probing') and  mathematical  indicators to quantitatively detect the general  attention patterns with respect to the above properties. An empirical evaluation of seven PEs (and their combinations) for classification (GLUE) and span prediction (SQuAD) shows that: (1) both  classification and span prediction benefit from  translation invariance and local monotonicity, while symmetry slightly decreases performance;\n(2) The fully-learnable absolute PE performs better in classification, while relative PEs perform better in span prediction.  We contribute the first formal and quantitative analysis of desiderata for PEs, and  a principled discussion about their correlation to the performance of typical downstream tasks.", "one-sentence_summary": "This paper amis to understand and evaluate position embeddings, especially in pretrain language models", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|on_position_embeddings_in_bert", "pdf": "/pdf/be0283e323f1b118c975dbc46f7f75c59b467fe0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021on,\ntitle={On Position Embeddings in {\\{}BERT{\\}}},\nauthor={Benyou Wang and Lifeng Shang and Christina Lioma and Xin Jiang and Hao Yang and Qun Liu and Jakob Grue Simonsen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=onxoVA9FxMw}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "onxoVA9FxMw", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper557/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper557/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper557/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper557/Authors|ICLR.cc/2021/Conference/Paper557/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper557/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869681, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper557/-/Official_Comment"}}}, {"id": "Rsmy2PdjHH", "original": null, "number": 6, "cdate": 1606128402250, "ddate": null, "tcdate": 1606128402250, "tmdate": 1606128402250, "tddate": null, "forum": "onxoVA9FxMw", "replyto": "12pkZxA4ho5", "invitation": "ICLR.cc/2021/Conference/Paper557/-/Official_Comment", "content": {"title": "In the revision, we quantitatively concluded how these properties benefit/harm downstream tasks", "comment": "We thank you for the helpful comments and suggestions.\nWe provide our responses as follows.\n\n\n\n **Weaknesses1: the analyses are specific to BERT-like masked language modeling, it isn\u2019t clear if PEs behave similarly in encoder-decoder like models for summarization/translation or decoder only language models**\n\n\nIn the latest version, we conduct the probing tests for well-trained GPT2 and English-to-French machine translation models. Please see Appendix I.  \n\nWe found that there are some differences:\n1. Monotonicity: compared to BERT and the machine translation, GPT2  satisfies local monotonicity  better than other models, showing capturing distance between neighboring tokens matters in language model.\n\n2.Translation invariance:   GPT2 satisfies Translation invariance poorer than other models, since tokens in it also additionally attend to a few beginning tokens no matter how far the attended tokens are. \n\n3 Direction Awareness: GPT2 shows the biggest symmetrical discrepancy, since GPT2, which aims to predict the next word, adopts an attention mask of succeeding tokens to avoid information leakage. Plus, MT Encoder slightly attend more to the succeeding tokens while BERT attend more on the preceding tokens\n\n\n\n**Weaknesses2: It provides certain properties one can hope PEs to exhibit but doesn\u2019t talk too much about circumstances under which these would be good inductive biases to have.**\n\nIn the latest version, we have adopted multiple quantitative indicators to measure how well BERT models with individual PEs satisfy the three properties. In addition, we compute correlations between said indicators and performance: the result of this investigation is that both the text classification tasks (in GLUE) and span prediction tasks (in SQuAD V1.0 and V2.0) can benefit from local monotonicity and translation invariance, but that symmetry results in performance deterioration, possibly due to the inability of symmetric functions to properly reflect the transition from  query vector to key vector when calculating attention. We believe that this conclusion may also hold for other tasks, but have not performed an experimental evaluation of this. The next reply also helps answer the question regarding circumstances.\n\n\n\n**Q1: Under what circumstances do properties like monotonicity in learnable PEs arise? Would PEs for an autoregressive variant also display similar properties? (would be interesting to see what happens with say a transformer LM trained from scratch on wikitext-103)**\n\n(1) Monotonicity (especially in  small offsets) should generally arise if tasks need to capture word order (most tasks should be). Removing monotonicity suggests that the models become similar to bag-of-words models that do not consider word order.\n\n(2) Translation invariance probably arises due to the input data processing in BERT. Absolute positions of words in pre-trained language models (such as BERT) are arbitrarily replaceable. See the discussion on translation invariance in Sec 6.\n\n(3)  Symmetry should in general be avoided because it degrades performance. It is likely that it cannot properly express directionality when calculating attentions between query vectors and key vectors.\n\n**Q2: It isn\u2019t quite clear to me at first glance why the preservation of the order of distances is necessary for position embeddings when used with models that build nonlinear functions of these embeddings.**\n\nIt is quite difficult to make this evident theoretically, unless we assume more about the particular non-linear functions. We empirically find that the learned PEs preserve the order of distance. We have added a brief comment to that effect in the paper.\n\n\n**Q3: Would absolute position embeddings that are randomly initialized or all initialized orthogonal to one another (something like 1-hot vectors of dimension L) and fixed work? This would be interesting to see (especially the latter) because it does not satisfy monotonicity or translation invariance and violates the desiderata in eq (1)**\n\nFor random initialization, see Fig. 8 in App. G, showing the dot products between any two position vectors in a BERT medium training from scratch. We could see a  gradual evolution of dot products between position vectors. At the onset, it indeed does not satisfy monotonicity or translation invariance, but with an increasing number of training steps, we observe clearly local monotonicity and translation invariance.\n\n**Q4: Is Figure 7 based on BERT fine-tuned for NER on the CONLL/Ontonotes dataset?**\nNo, these figures are for BERT pre-trained models without fine-tuning.  We have clarified this in the latest version.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper557/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper557/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Position Embeddings in BERT", "authorids": ["~Benyou_Wang2", "~Lifeng_Shang1", "~Christina_Lioma1", "~Xin_Jiang1", "~Hao_Yang7", "~Qun_Liu1", "~Jakob_Grue_Simonsen1"], "authors": ["Benyou Wang", "Lifeng Shang", "Christina Lioma", "Xin Jiang", "Hao Yang", "Qun Liu", "Jakob Grue Simonsen"], "keywords": ["Position Embedding", "BERT", "pretrained language model."], "abstract": "Various Position Embeddings (PEs) have been proposed in Transformer based architectures~(e.g. BERT) to model word order. These are empirically-driven and perform well, but no formal framework exists to systematically study them. To address this, we present three properties of PEs that capture word distance in vector space:  translation invariance, monotonicity, and  symmetry. These properties formally capture the behaviour of PEs and allow us to reinterpret sinusoidal PEs in a principled way.\nMoreover, we propose a new probing test (called `identical word probing') and  mathematical  indicators to quantitatively detect the general  attention patterns with respect to the above properties. An empirical evaluation of seven PEs (and their combinations) for classification (GLUE) and span prediction (SQuAD) shows that: (1) both  classification and span prediction benefit from  translation invariance and local monotonicity, while symmetry slightly decreases performance;\n(2) The fully-learnable absolute PE performs better in classification, while relative PEs perform better in span prediction.  We contribute the first formal and quantitative analysis of desiderata for PEs, and  a principled discussion about their correlation to the performance of typical downstream tasks.", "one-sentence_summary": "This paper amis to understand and evaluate position embeddings, especially in pretrain language models", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|on_position_embeddings_in_bert", "pdf": "/pdf/be0283e323f1b118c975dbc46f7f75c59b467fe0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021on,\ntitle={On Position Embeddings in {\\{}BERT{\\}}},\nauthor={Benyou Wang and Lifeng Shang and Christina Lioma and Xin Jiang and Hao Yang and Qun Liu and Jakob Grue Simonsen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=onxoVA9FxMw}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "onxoVA9FxMw", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper557/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper557/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper557/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper557/Authors|ICLR.cc/2021/Conference/Paper557/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper557/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869681, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper557/-/Official_Comment"}}}, {"id": "P4csZLwlm_d", "original": null, "number": 5, "cdate": 1606128102671, "ddate": null, "tcdate": 1606128102671, "tmdate": 1606128102671, "tddate": null, "forum": "onxoVA9FxMw", "replyto": "lQsimEZqETL", "invitation": "ICLR.cc/2021/Conference/Paper557/-/Official_Comment", "content": {"title": "Reply to AnonReviewer4", "comment": "We thank you for the helpful comments and suggestions. We provide our responses as follows.\n\n**Q1: assume  symmetry by default** \n\nIn the latest version, we now clearly state that the three properties are not gold standards for any position embedding, but that the properties are worthy and interesting to examine in light of the previous literature on position embeddings.  Finally, the empirical results did evidence that directions matter, that is, `word A does have a different relationship to B than B does to A', as you suggested. \n\nThere are two reasons to test   symmetry for PEs:\n\n (**1**)  Denote by $\\phi(\\cdot,\\cdot)$ some function that calculates closeness/proximity between embedded positions; in the literature, $\\phi(\\cdot,\\cdot)$ is typically a mapping $f: \\mathbb{R}^D,  \\mathbb{R}^D \\rightarrow \\mathbb{R}^+  $. This is also consistent with attention calculation, for example $\\textrm{softmax} \\left((w_i+p_i) W^{Q,1} ((w_j+p_j) W^{K,1} )^T\\right)$ (note that this involves a position-position   correspondence $p_i W^{Q,1} (W^{K,1})^T  p_j ^T$ ) also returns a positive real output.\nAs laid out in the paper, attention values of both forward and backward attending need to be mapped to positive reals, but unless more is known about $\\phi(\\cdot,\\cdot)$, the function itself does not necessarily yield any information about directionality.\n (**2**) a typical and well-defined way to choose a proximity function $\\phi(\\cdot,\\cdot)$ is to use an inner product (for example, the dot product $p_i  p_j ^T$ ), and (real-valued) inner products by definition satisfy symmetry.\n\n \n\n**Q2: The experimental results were not particularly convincing. Conclusion about APEs did better at classification and that RPEs did better at span prediction.**\n\n\nThe conclusion we made was that the **fully-learnable APE** (instead of general **APEs**) performed better at classification. BERT with the **fully-learnable APE** is the baseline for all PE variants because (1) it is implemented in the original BERT, and (2) it is simple and efficient. In the latest version, we have rewritten the related content to make the above more clear.\n\nWe made the conclusion above because: \n\nIn classification tasks (GLUE), No PE variants significantly outperform the * fully-learnable APE*.  \n(1) In all BERT models using a single type of PEs (without considering combinations between APEs and RPEs), all PE variants (including the fixed sin. APE, learnable sin. APE, fully-learnable RPE, fixed sin. RPE, and learnable sin. RPE)  perform worse than the fully-learnable APE, as shown in first seven rows of Table 3.\n(2) Although an APE-RPE mixed variant (*learnable sin. APE + fully-learnable RPE*) performs better than fully-learnable  APE (82.5 vs 82.2), but this  improvement is not statistically significant. \n\nIn span prediction, without considering APE-RPE mixed variants, all RPE variants significantly outperform APE variants.\n\n\n\n **Q3: The writing quality was borderline and there were a number of small errors**\n\nThe paper has now been carefully proofread.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper557/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper557/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Position Embeddings in BERT", "authorids": ["~Benyou_Wang2", "~Lifeng_Shang1", "~Christina_Lioma1", "~Xin_Jiang1", "~Hao_Yang7", "~Qun_Liu1", "~Jakob_Grue_Simonsen1"], "authors": ["Benyou Wang", "Lifeng Shang", "Christina Lioma", "Xin Jiang", "Hao Yang", "Qun Liu", "Jakob Grue Simonsen"], "keywords": ["Position Embedding", "BERT", "pretrained language model."], "abstract": "Various Position Embeddings (PEs) have been proposed in Transformer based architectures~(e.g. BERT) to model word order. These are empirically-driven and perform well, but no formal framework exists to systematically study them. To address this, we present three properties of PEs that capture word distance in vector space:  translation invariance, monotonicity, and  symmetry. These properties formally capture the behaviour of PEs and allow us to reinterpret sinusoidal PEs in a principled way.\nMoreover, we propose a new probing test (called `identical word probing') and  mathematical  indicators to quantitatively detect the general  attention patterns with respect to the above properties. An empirical evaluation of seven PEs (and their combinations) for classification (GLUE) and span prediction (SQuAD) shows that: (1) both  classification and span prediction benefit from  translation invariance and local monotonicity, while symmetry slightly decreases performance;\n(2) The fully-learnable absolute PE performs better in classification, while relative PEs perform better in span prediction.  We contribute the first formal and quantitative analysis of desiderata for PEs, and  a principled discussion about their correlation to the performance of typical downstream tasks.", "one-sentence_summary": "This paper amis to understand and evaluate position embeddings, especially in pretrain language models", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|on_position_embeddings_in_bert", "pdf": "/pdf/be0283e323f1b118c975dbc46f7f75c59b467fe0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021on,\ntitle={On Position Embeddings in {\\{}BERT{\\}}},\nauthor={Benyou Wang and Lifeng Shang and Christina Lioma and Xin Jiang and Hao Yang and Qun Liu and Jakob Grue Simonsen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=onxoVA9FxMw}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "onxoVA9FxMw", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper557/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper557/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper557/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper557/Authors|ICLR.cc/2021/Conference/Paper557/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper557/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869681, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper557/-/Official_Comment"}}}, {"id": "12pkZxA4ho5", "original": null, "number": 3, "cdate": 1603950864377, "ddate": null, "tcdate": 1603950864377, "tmdate": 1605024660757, "tddate": null, "forum": "onxoVA9FxMw", "replyto": "onxoVA9FxMw", "invitation": "ICLR.cc/2021/Conference/Paper557/-/Official_Review", "content": {"title": "Official Review - AnonReviewer1", "review": "The paper presents a systematic analysis of approaches used to encode position information in transformers and in particular BERT-based models. The paper investigates absolute and relative position embedding strategies that use either fixed/learnable sinusoidal or fully learnable position embeddings. These embeddings are characterized based on different properties that are either inherent from their formulation or observed empirically such as monotonicity, translation invariance, and symmetry. Interestingly these properties appear to emerge naturally when having learnable parameters in APEs and RPEs.\n\nDifferent PE strategies are empirically validated by pre-training BERT with different PEs (including a combination of absolute and relative position representations) and fine-tuning on GLUE and SQuAD (1.1 and 2.0). Visualizations of the dot products between position vectors for different PE strategies are presented as well that demonstrate the monotonicity (or local monotonicity), symmetry, and translation invariance.\n\nOverall, the paper is well written, motivated, and systematically studies an important design decision in transformers. The overall methodology is sound and should prove useful to the community when studying position embeddings in transformers.\n\nStrengths\n\nThe paper is well written, well-motivated, and is systematic in its claims, experiments, and methodology.\nIt takes a good step forward in characterizing desirable properties of position embeddings and studying if they emerge directly from their parameterization or via training.\nIt studies a variety of position embedding strategies and their conjunction and experiments with fairly realistic models and benchmarks.\n\nWeaknesses\n\nWhile the analyses are well carried out, they are still specific to BERT-like masked language modeling training and it isn\u2019t clear if PEs behave similarly in encoder-decoder like models for summarization/translation or decoder only language models.\nIt does not control for the choice of (pre)training objective, for example, if trained from scratch, purely as a retrieval model or on supervised text classification, would different learned PE patterns emerge?\n\nIt provides certain properties one can hope PEs to exhibit but doesn\u2019t talk too much about circumstances under which these would be good inductive biases to have.\n\nQuestions & Comments:\n\nUnder what circumstances do properties like monotonicity in learnable PEs arise? Would PEs for an autoregressive variant also display similar properties? (would be interesting to see what happens with say a transformer LM trained from scratch on wikitext-103)\n\nIt isn\u2019t quite clear to me at first glance why the preservation of the order of distances is necessary for position embeddings when used with models that build nonlinear functions of these embeddings.\n\nWould absolute position embeddings that are randomly initialized or all initialized orthogonal to one another (something like 1-hot vectors of dimension L) *and fixed*  work? This would be interesting to see (especially the latter) because it does not satisfy monotonicity or translation invariance and violates the desiderata in eq (1)\n\nIs Figure 7 based on BERT fine-tuned for NER on the CONLL/Ontonotes dataset?\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper557/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper557/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Position Embeddings in BERT", "authorids": ["~Benyou_Wang2", "~Lifeng_Shang1", "~Christina_Lioma1", "~Xin_Jiang1", "~Hao_Yang7", "~Qun_Liu1", "~Jakob_Grue_Simonsen1"], "authors": ["Benyou Wang", "Lifeng Shang", "Christina Lioma", "Xin Jiang", "Hao Yang", "Qun Liu", "Jakob Grue Simonsen"], "keywords": ["Position Embedding", "BERT", "pretrained language model."], "abstract": "Various Position Embeddings (PEs) have been proposed in Transformer based architectures~(e.g. BERT) to model word order. These are empirically-driven and perform well, but no formal framework exists to systematically study them. To address this, we present three properties of PEs that capture word distance in vector space:  translation invariance, monotonicity, and  symmetry. These properties formally capture the behaviour of PEs and allow us to reinterpret sinusoidal PEs in a principled way.\nMoreover, we propose a new probing test (called `identical word probing') and  mathematical  indicators to quantitatively detect the general  attention patterns with respect to the above properties. An empirical evaluation of seven PEs (and their combinations) for classification (GLUE) and span prediction (SQuAD) shows that: (1) both  classification and span prediction benefit from  translation invariance and local monotonicity, while symmetry slightly decreases performance;\n(2) The fully-learnable absolute PE performs better in classification, while relative PEs perform better in span prediction.  We contribute the first formal and quantitative analysis of desiderata for PEs, and  a principled discussion about their correlation to the performance of typical downstream tasks.", "one-sentence_summary": "This paper amis to understand and evaluate position embeddings, especially in pretrain language models", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|on_position_embeddings_in_bert", "pdf": "/pdf/be0283e323f1b118c975dbc46f7f75c59b467fe0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021on,\ntitle={On Position Embeddings in {\\{}BERT{\\}}},\nauthor={Benyou Wang and Lifeng Shang and Christina Lioma and Xin Jiang and Hao Yang and Qun Liu and Jakob Grue Simonsen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=onxoVA9FxMw}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "onxoVA9FxMw", "replyto": "onxoVA9FxMw", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper557/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538140462, "tmdate": 1606915767655, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper557/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper557/-/Official_Review"}}}], "count": 13}