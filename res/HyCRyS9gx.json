{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396448783, "tcdate": 1486396448783, "number": 1, "id": "HkYXnfLul", "invitation": "ICLR.cc/2017/conference/-/paper231/acceptance", "forum": "HyCRyS9gx", "replyto": "HyCRyS9gx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "This work extends variational autoencoders to adapt to a new dataset containing a small number of examples. While this work is promising, two of the reviewers had serious concerns about clarity. A new version of the paper has been submitted, however I still find it too hard to follow and would find it hard to accurately describe what was done having read the main body of the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Adaptation in Generative Models with Generative Matching Networks", "abstract": "Despite recent advances, the remaining bottlenecks in deep generative models are necessity of extensive training and difficulties with generalization from small number of training examples.\nBoth problems may be addressed by conditional generative models that are trained to adapt the generative distribution to additional input data.\nSo far this idea was explored only under certain limitations such as restricting the input data to be a single object or multiple objects representing the same concept.  \nIn this work we develop a new class of deep generative model called generative matching networks which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks and the ideas from meta-learning.\nBy conditioning on the additional input dataset, generative matching networks may instantly learn new concepts that were not available during the training but conform to a similar generative process, without explicit limitations on the number of additional input objects or the number of concepts they represent. \nOur experiments on the Omniglot dataset demonstrate that generative matching networks can significantly improve predictive performance on the fly as more additional data is available to the model and also adapt the latent space which is beneficial in the context of feature extraction.", "pdf": "/pdf/612d0be08c6f03132298219d89615a4017aa3974.pdf", "TL;DR": "A nonparametric conditional generative model with fast small-shot adaptation", "paperhash": "bartunov|fast_adaptation_in_generative_models_with_generative_matching_networks", "conflicts": ["google.com", "company.yandex.ru", "hse.ru", "cs.hse.ru", "msu.ru", "skoltech.ru", "ccas.ru"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Sergey Bartunov", "Dmitry P. Vetrov"], "authorids": ["sbos.net@gmail.com", "vetrovd@yandex.ru"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396449351, "id": "ICLR.cc/2017/conference/-/paper231/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HyCRyS9gx", "replyto": "HyCRyS9gx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396449351}}}, {"tddate": null, "tmdate": 1484492349185, "tcdate": 1484492349185, "number": 7, "id": "BJrSAWtUe", "invitation": "ICLR.cc/2017/conference/-/paper231/public/comment", "forum": "HyCRyS9gx", "replyto": "rJjne8XNg", "signatures": ["~Sergey_Bartunov1"], "readers": ["everyone"], "writers": ["~Sergey_Bartunov1"], "content": {"title": "Author reply", "comment": "Thank you for your review and the patience with which you assessed our paper.\n\nThe paper in it\u2019s pre-review form definitely had problems with clarity in certain parts.\nWe have rewritten them based on the useful feedback provided by reviewers. \n\nBesides that, we would like to address other issues mentioned in your review.\n\n> Most importantly, I can\u2019t tell which aspects are meant to be novel, since there are only a few sentences devoted to matching networks, even though this work builds closely upon them. (I brought this up in my Reviewer Question, and the paper has not been revised to make this clearer.)\n\nThe originally proposed matching networks were designed for supervised learning problems. In order to e.g. classify an image the model interpolated between conditioning objects in the one-hot label space. In contrast, we operate in an unsupervised learning scenario where label information is not available. Hence, generative matching networks interpolate in a prototype space Psi which is itself defined by the model. \nMoreover, in contrast to discriminative (original) matching networks, our model employs the matching procedure in more than just one part of the network. We use it in the prior p(z | X), the conditional likelihood p(x | z, X) and the approximate posterior q(z | x, X). \n\n> I\u2019m also confused about the meta-learning setup. One natural formulation for meta-learning of generative models would be that the inputs consist of small datasets X, and the task is to predict the distribution from which X was sampled. But this would imply a uniform weighting of data points, which is different from the proposed method. \n\nYour intuition is correct, we implicitly aim to model the distribution of a sampled small dataset.\nHowever, the weights you are mentioning are different from the similarity weights used in equations (5) and (6). The similarity weights depend on a query (be it value of the latent variable z or an observation x), hence they are non-uniform. \nHowever, when sampling from a data-depending prior p(z | X), in average all objects from X should be used with the same frequency in order to maximize the objective.\n\n> Based on 3.1, it seems like one additionally has some sort of query q, but it\u2019s not clear what this represents. \n\nSince the matching procedure is used in all parts of our model, namely prior, likelihood and variational approximation where different information is used to match conditioning data, we decided to first describe the generic matching procedure that operates with an abstract query and then specify the particular form of a query for each part. \nThus, the example of what may be a query is provided in the second paragraph of section 3.1 where we introduce the matching procedure and in the next section we describe the details.\n\nYou may also refer to the post-review revision of the paper where we, hopefully, made this more clear. \n\n> In terms of experimental validation, there aren\u2019t any comparisons against prior work. This seems necessary, since several other methods have already been proposed which are similar in spirit.\n\nTo the best of our knowledge, the existing class of conditional generative models that explicitly learn fast adaptation mechanisms instead of relying on computationally hard (and thus arguably fast) inference currently consists of two models: sequential generative models (Rezende et al, 2016) and the neural statistician (Amos & Storkey, 2016). \nAs we mention in section 4.2, sequential generative models unfortunately are reported to overfit in the one-shot generation scenario when using the canonical train/test split of Omniglot.\nWe also explain that does not seem to be feasible to evaluate the full neural statistican model as it is described in the original paper due to intractable predictive density (see appendix D for the details).\nPlease note, the authors also didn\u2019t report loglikelihood evaluation in their paper and we have contacted with them to discuss the evaluation and at the moment we could not find the reliable way of estimating the predictive density. Hence, we implemented the similar model that does not have a global \u201ccontext\u201d variable causing the intractability, but otherwise uses the way of summarizing the conditioning data and adaptation."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Adaptation in Generative Models with Generative Matching Networks", "abstract": "Despite recent advances, the remaining bottlenecks in deep generative models are necessity of extensive training and difficulties with generalization from small number of training examples.\nBoth problems may be addressed by conditional generative models that are trained to adapt the generative distribution to additional input data.\nSo far this idea was explored only under certain limitations such as restricting the input data to be a single object or multiple objects representing the same concept.  \nIn this work we develop a new class of deep generative model called generative matching networks which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks and the ideas from meta-learning.\nBy conditioning on the additional input dataset, generative matching networks may instantly learn new concepts that were not available during the training but conform to a similar generative process, without explicit limitations on the number of additional input objects or the number of concepts they represent. \nOur experiments on the Omniglot dataset demonstrate that generative matching networks can significantly improve predictive performance on the fly as more additional data is available to the model and also adapt the latent space which is beneficial in the context of feature extraction.", "pdf": "/pdf/612d0be08c6f03132298219d89615a4017aa3974.pdf", "TL;DR": "A nonparametric conditional generative model with fast small-shot adaptation", "paperhash": "bartunov|fast_adaptation_in_generative_models_with_generative_matching_networks", "conflicts": ["google.com", "company.yandex.ru", "hse.ru", "cs.hse.ru", "msu.ru", "skoltech.ru", "ccas.ru"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Sergey Bartunov", "Dmitry P. Vetrov"], "authorids": ["sbos.net@gmail.com", "vetrovd@yandex.ru"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287673645, "id": "ICLR.cc/2017/conference/-/paper231/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyCRyS9gx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper231/reviewers", "ICLR.cc/2017/conference/paper231/areachairs"], "cdate": 1485287673645}}}, {"tddate": null, "tmdate": 1484351394044, "tcdate": 1484351394044, "number": 6, "id": "Sy9owyDLg", "invitation": "ICLR.cc/2017/conference/-/paper231/public/comment", "forum": "HyCRyS9gx", "replyto": "HyCRyS9gx", "signatures": ["~Sergey_Bartunov1"], "readers": ["everyone"], "writers": ["~Sergey_Bartunov1"], "content": {"title": "Update", "comment": "We would like to thank the reviewers for their efforts!\n\nWe received many useful comments that helped us to improve the text.\nUsing the provided feedback, we have edited the paper and uploaded a new version which, hopefully, addresses the issues raised in reviews.\n\nWe have also sent our response to all three reviews and would be glad to continue the discussion."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Adaptation in Generative Models with Generative Matching Networks", "abstract": "Despite recent advances, the remaining bottlenecks in deep generative models are necessity of extensive training and difficulties with generalization from small number of training examples.\nBoth problems may be addressed by conditional generative models that are trained to adapt the generative distribution to additional input data.\nSo far this idea was explored only under certain limitations such as restricting the input data to be a single object or multiple objects representing the same concept.  \nIn this work we develop a new class of deep generative model called generative matching networks which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks and the ideas from meta-learning.\nBy conditioning on the additional input dataset, generative matching networks may instantly learn new concepts that were not available during the training but conform to a similar generative process, without explicit limitations on the number of additional input objects or the number of concepts they represent. \nOur experiments on the Omniglot dataset demonstrate that generative matching networks can significantly improve predictive performance on the fly as more additional data is available to the model and also adapt the latent space which is beneficial in the context of feature extraction.", "pdf": "/pdf/612d0be08c6f03132298219d89615a4017aa3974.pdf", "TL;DR": "A nonparametric conditional generative model with fast small-shot adaptation", "paperhash": "bartunov|fast_adaptation_in_generative_models_with_generative_matching_networks", "conflicts": ["google.com", "company.yandex.ru", "hse.ru", "cs.hse.ru", "msu.ru", "skoltech.ru", "ccas.ru"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Sergey Bartunov", "Dmitry P. Vetrov"], "authorids": ["sbos.net@gmail.com", "vetrovd@yandex.ru"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287673645, "id": "ICLR.cc/2017/conference/-/paper231/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyCRyS9gx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper231/reviewers", "ICLR.cc/2017/conference/paper231/areachairs"], "cdate": 1485287673645}}}, {"tddate": null, "tmdate": 1484350705560, "tcdate": 1484350705560, "number": 4, "id": "rJcgSyP8g", "invitation": "ICLR.cc/2017/conference/-/paper231/public/comment", "forum": "HyCRyS9gx", "replyto": "SyIs6QfVx", "signatures": ["~Sergey_Bartunov1"], "readers": ["everyone"], "writers": ["~Sergey_Bartunov1"], "content": {"title": "Author reply", "comment": "First, we would like to thank you for a thoughtful review of our work.\nWe definitely agree that bringing more clarity would improve the paper, so we revised the text based on the provided feedback.\n\nWe would like to address the issues raised in your review one by one.\n\n>There are a lot of details that I feel are scattered throughout, and I did not get a sense after reading this paper that I would be able to implement the method and replicate the results. My suggestion is to consolidate the major implementation details into a single section, and be explicit about the functional form of the different embedding functions and their variants.\n>There is a lack of definition of the different functions. Some basic insight into the functional forms of f, g, \\phi, sim and R would be nice. Otherwise it is very unclear to me what\u2019s going on.\n\nBy no means we wanted to hide any implementation details.\nHence, we released the code to reproduce our results on Github, but of course a paper should also be self-contained.\nSection 3 contains an outline of the model while implementation details and concrete functional forms of functions f, g and psi as well as missing information is provided in Appendix A which we now refer to in the beginning and the end of the section.\n\n>I was a bit disappointed to see that weak supervision in the form of labels had to be used. How does the method perform in a completely unsupervised setting? This could be an interesting baseline.\n\nWe agree that completely unsupervised approach for training would be much more appealing. \nThe most straightforward way of doing that is mentioned in the beginning of section 3.3 which is maximization of marginal likelihood of the whole dataset.\nIt is infeasible on modern widely-available hardware as it would require an enormous amount of memory and computations even for a relatively small dataset such as Omniglot.\nHowever, it is likely that it would lead to even better results with our model as it would have to perform more active discrimination to match relevant objects and and the same time more relevant objects would be available.\n\nHence, models such as the neural statistician (Amos & Storkey, 2016) and sequential generative models (Rezende et al, 2016) which are closest to our model have to put certain limitations on the datasets they are modelling during the training. \nFor example, forming a dataset with examples of the same class which is used in (Amos & Storkey, 2016) is maybe even stronger form of supervision which we certainly relax in generative matching networks thus making a step towards less supervision.\n\n>Section 3.2: \u201conly state of the recurrent controller was used for matching\u201d, my reading of this section (after several passes) is that the pseudo-input is used in the place of a regular input. Is this correct? Otherwise, this sentence/section needs more clarification. I noticed upon further reading in section 4.2 that there are two versions of the model: one in which a pseudo input is used, and one in which a pseudo input is not used (the conditional version). What is the difference in functional form between these? That is, how do the formulas for the embeddings f and g change between these settings?\n\nWe made this place clear in the paper. \nThere is no functional difference betweens these two versions of the model.\nThe one that uses a pseudo-input simply sums over T+1 objects in eq. (6) where the last additional object is pseudo-input. It is not necessary to actually have an actual input object, so instead we just model activations of functions f, g and \\psi for that pseudo-input as trainable parameters (see Figure 1).\n\n>\u201csince the result was fully contrastive we did not apply any further binarization\u201d what does it mean for a result to be fully contrastive?\n\n\nThis means that the result was already binary, i.e. pixel values were either 0 or 255.\n\n> For clarity, the figures and table refer to the number of shots, but this is never defined. I assume this is T here. This should be made consistent.\n\n\nWe changed the notation, it is now referred to as number of conditional objects. \nSo it is not exactly T, but rather T-1.\n\n> Figure 2: why is the value of T only 9 in this case? What does it mean for it to be 0? It is stated earlier that T should go up to 20 (I assume #shot corresponds to T). \n\nWe performed this experiment with T=10 to qualitative assess dependence on the number of attention steps without spending much time to train large number of models. \n\n> It also looks like the results continue to improve with an increased number of steps, I would like to see the results for 5 and maybe 6 steps as well. Presumably there will come a point where you get diminishing returns.\n\nYes, you are right. We actually tried larger number of steps but they lead to insignificant improvements, so we decided that more parameter configurations would rather mess up the plot.\nOne can see that even between \u201csteps=4, prior steps=4\u201d and \u201csteps=4, prior steps=1\u201d there is very little difference on the figure.\n\n> Table 1: is the VAE a fair baseline?\n\nYes, we think that VAE is a fair baseline in the sense that as all other models in our comparison it defines a marginal distribution over observed objects.\nOf course it has less available information than other models as it cannot be conditioned on an additional dataset, but this is exactly why it makes sense to compare against it.\nFor example, there could be a situation when generative matching networks perform worse than just VAE when no or just a few conditioning objects are available which certainly would be undesirable. \n\n> MNIST is much more common than Omniglot for evaluating generative models. Would it be possible to perform similar experiments on this dataset? That way it can be compared with many more models.\n\n\nOf course, MNIST is a much more standard dataset, but comparing to Omniglot it is unsuitable for evaluating few-shot learning systems.\nIn particular, it contains just 10 classes which are shared between train and test parts, so it is unclear what experimental protocol would demonstrate an ability to adapt to unseen classes in such a setting.\nThe lack of diversity implies that t is not beneficial for the model to learn how to learn on the fly because it is easier to \u201cmemorize\u201d class information directly into weights.\n\nPlease note that we still evaluate our model on MNIST in Appendix B.\n\n> Further, why are the negative log-likelihood values monotonically decreasing in the number of shots? That is, is there ever a case where increasing the number of shots can hurt things? \n\nAssuming that our model can efficiently re-use conditioning examples belonging to the same class as the new object thus discriminating between classes, conditional likelihood of a new object x_t may decrease relative to conditional likelihoods of a previous object x_{t-1} only if x_t belongs to a class that was not represented so far in the dataset.\n\nSo it is definitely possible to construct a dataset that conditional log-likelihoods would be non-monotone, e.g. the following sequence of objects belonging to classes A and B: A B A A B. \n\nOur data-generating process consists of uniform selection of C classes, followed by uniform sampling of objects corresponding to the selected classes. \nDue to uniformity of the sampling processes, the probability that x_t belongs to a new class decreases with t and thus in expectation conditional likelihoods p(x_t | x_{<t}) must increase.\n\n> What happens at T=30? 40?\n\nUnfortunately, our computational resources don\u2019t allow us to train models on larger datasets than T=20. \nHowever, the model should benefit from larger number of relevant conditional objects. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Adaptation in Generative Models with Generative Matching Networks", "abstract": "Despite recent advances, the remaining bottlenecks in deep generative models are necessity of extensive training and difficulties with generalization from small number of training examples.\nBoth problems may be addressed by conditional generative models that are trained to adapt the generative distribution to additional input data.\nSo far this idea was explored only under certain limitations such as restricting the input data to be a single object or multiple objects representing the same concept.  \nIn this work we develop a new class of deep generative model called generative matching networks which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks and the ideas from meta-learning.\nBy conditioning on the additional input dataset, generative matching networks may instantly learn new concepts that were not available during the training but conform to a similar generative process, without explicit limitations on the number of additional input objects or the number of concepts they represent. \nOur experiments on the Omniglot dataset demonstrate that generative matching networks can significantly improve predictive performance on the fly as more additional data is available to the model and also adapt the latent space which is beneficial in the context of feature extraction.", "pdf": "/pdf/612d0be08c6f03132298219d89615a4017aa3974.pdf", "TL;DR": "A nonparametric conditional generative model with fast small-shot adaptation", "paperhash": "bartunov|fast_adaptation_in_generative_models_with_generative_matching_networks", "conflicts": ["google.com", "company.yandex.ru", "hse.ru", "cs.hse.ru", "msu.ru", "skoltech.ru", "ccas.ru"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Sergey Bartunov", "Dmitry P. Vetrov"], "authorids": ["sbos.net@gmail.com", "vetrovd@yandex.ru"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287673645, "id": "ICLR.cc/2017/conference/-/paper231/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyCRyS9gx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper231/reviewers", "ICLR.cc/2017/conference/paper231/areachairs"], "cdate": 1485287673645}}}, {"tddate": null, "tmdate": 1484350650552, "tcdate": 1484350650552, "number": 3, "id": "B1za4kDUx", "invitation": "ICLR.cc/2017/conference/-/paper231/public/comment", "forum": "HyCRyS9gx", "replyto": "r1esDxMNl", "signatures": ["~Sergey_Bartunov1"], "readers": ["everyone"], "writers": ["~Sergey_Bartunov1"], "content": {"title": "Author reply", "comment": "Thank you for your review.\n\nWe agree that explicit comparison to the neural statistician model (also submitted to ICLR) would make the comparison more direct.\nWe also would like to mention that before submission we contacted authors of the paper and discussed with Harrison Edwards the issues of estimating conditional log-likelihoods. At that moment, noone of us came up with a reliable solution so we had to resort to closest \u201ctractable analog\u201d as a baseline that could allow us to demonstrate the benefits of our model."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Adaptation in Generative Models with Generative Matching Networks", "abstract": "Despite recent advances, the remaining bottlenecks in deep generative models are necessity of extensive training and difficulties with generalization from small number of training examples.\nBoth problems may be addressed by conditional generative models that are trained to adapt the generative distribution to additional input data.\nSo far this idea was explored only under certain limitations such as restricting the input data to be a single object or multiple objects representing the same concept.  \nIn this work we develop a new class of deep generative model called generative matching networks which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks and the ideas from meta-learning.\nBy conditioning on the additional input dataset, generative matching networks may instantly learn new concepts that were not available during the training but conform to a similar generative process, without explicit limitations on the number of additional input objects or the number of concepts they represent. \nOur experiments on the Omniglot dataset demonstrate that generative matching networks can significantly improve predictive performance on the fly as more additional data is available to the model and also adapt the latent space which is beneficial in the context of feature extraction.", "pdf": "/pdf/612d0be08c6f03132298219d89615a4017aa3974.pdf", "TL;DR": "A nonparametric conditional generative model with fast small-shot adaptation", "paperhash": "bartunov|fast_adaptation_in_generative_models_with_generative_matching_networks", "conflicts": ["google.com", "company.yandex.ru", "hse.ru", "cs.hse.ru", "msu.ru", "skoltech.ru", "ccas.ru"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Sergey Bartunov", "Dmitry P. Vetrov"], "authorids": ["sbos.net@gmail.com", "vetrovd@yandex.ru"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287673645, "id": "ICLR.cc/2017/conference/-/paper231/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyCRyS9gx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper231/reviewers", "ICLR.cc/2017/conference/paper231/areachairs"], "cdate": 1485287673645}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484350621708, "tcdate": 1478279125647, "number": 231, "id": "HyCRyS9gx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HyCRyS9gx", "signatures": ["~Sergey_Bartunov1"], "readers": ["everyone"], "content": {"title": "Fast Adaptation in Generative Models with Generative Matching Networks", "abstract": "Despite recent advances, the remaining bottlenecks in deep generative models are necessity of extensive training and difficulties with generalization from small number of training examples.\nBoth problems may be addressed by conditional generative models that are trained to adapt the generative distribution to additional input data.\nSo far this idea was explored only under certain limitations such as restricting the input data to be a single object or multiple objects representing the same concept.  \nIn this work we develop a new class of deep generative model called generative matching networks which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks and the ideas from meta-learning.\nBy conditioning on the additional input dataset, generative matching networks may instantly learn new concepts that were not available during the training but conform to a similar generative process, without explicit limitations on the number of additional input objects or the number of concepts they represent. \nOur experiments on the Omniglot dataset demonstrate that generative matching networks can significantly improve predictive performance on the fly as more additional data is available to the model and also adapt the latent space which is beneficial in the context of feature extraction.", "pdf": "/pdf/612d0be08c6f03132298219d89615a4017aa3974.pdf", "TL;DR": "A nonparametric conditional generative model with fast small-shot adaptation", "paperhash": "bartunov|fast_adaptation_in_generative_models_with_generative_matching_networks", "conflicts": ["google.com", "company.yandex.ru", "hse.ru", "cs.hse.ru", "msu.ru", "skoltech.ru", "ccas.ru"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Sergey Bartunov", "Dmitry P. Vetrov"], "authorids": ["sbos.net@gmail.com", "vetrovd@yandex.ru"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 12, "writable": false, "overwriting": ["r1IvyjVYl"], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1482018994947, "tcdate": 1482018994947, "number": 3, "id": "rJjne8XNg", "invitation": "ICLR.cc/2017/conference/-/paper231/official/review", "forum": "HyCRyS9gx", "replyto": "HyCRyS9gx", "signatures": ["ICLR.cc/2017/conference/paper231/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper231/AnonReviewer2"], "content": {"title": "reasonable approach, but the exposition is hard to follow, and it's not clear what is novel", "rating": "4: Ok but not good enough - rejection", "review": "This paper presents a meta-learning algorithm which learns to learn generative models from a small set of examples. It\u2019s similar in structure to the matching networks of Vinyals et al. (2016), and is trained in a meta-learning framework where the inputs correspond to datasets. Results are shown on Omniglot in terms of log-likelihoods and in terms of generated samples. \n\nThe proposed idea seems reasonable, but I\u2019m struggling to understand various aspects of the paper. The exposition is hard to follow, partly because existing methods are described using terminology fairly different from that of the original authors. Most importantly, I can\u2019t tell which aspects are meant to be novel, since there are only a few sentences devoted to matching networks, even though this work builds closely upon them. (I brought this up in my Reviewer Question, and the paper has not been revised to make this clearer.)\n\nI\u2019m also confused about the meta-learning setup. One natural formulation for meta-learning of generative models would be that the inputs consist of small datasets X, and the task is to predict the distribution from which X was sampled. But this would imply a uniform weighting of data points, which is different from the proposed method. Based on 3.1, it seems like one additionally has some sort of query q, but it\u2019s not clear what this represents. \n\nIn terms of experimental validation, there aren\u2019t any comparisons against prior work. This seems necessary, since several other methods have already been proposed which are similar in spirit. \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Adaptation in Generative Models with Generative Matching Networks", "abstract": "Despite recent advances, the remaining bottlenecks in deep generative models are necessity of extensive training and difficulties with generalization from small number of training examples.\nBoth problems may be addressed by conditional generative models that are trained to adapt the generative distribution to additional input data.\nSo far this idea was explored only under certain limitations such as restricting the input data to be a single object or multiple objects representing the same concept.  \nIn this work we develop a new class of deep generative model called generative matching networks which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks and the ideas from meta-learning.\nBy conditioning on the additional input dataset, generative matching networks may instantly learn new concepts that were not available during the training but conform to a similar generative process, without explicit limitations on the number of additional input objects or the number of concepts they represent. \nOur experiments on the Omniglot dataset demonstrate that generative matching networks can significantly improve predictive performance on the fly as more additional data is available to the model and also adapt the latent space which is beneficial in the context of feature extraction.", "pdf": "/pdf/612d0be08c6f03132298219d89615a4017aa3974.pdf", "TL;DR": "A nonparametric conditional generative model with fast small-shot adaptation", "paperhash": "bartunov|fast_adaptation_in_generative_models_with_generative_matching_networks", "conflicts": ["google.com", "company.yandex.ru", "hse.ru", "cs.hse.ru", "msu.ru", "skoltech.ru", "ccas.ru"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Sergey Bartunov", "Dmitry P. Vetrov"], "authorids": ["sbos.net@gmail.com", "vetrovd@yandex.ru"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512655062, "id": "ICLR.cc/2017/conference/-/paper231/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper231/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper231/AnonReviewer3", "ICLR.cc/2017/conference/paper231/AnonReviewer1", "ICLR.cc/2017/conference/paper231/AnonReviewer2"], "reply": {"forum": "HyCRyS9gx", "replyto": "HyCRyS9gx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper231/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper231/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512655062}}}, {"tddate": null, "tmdate": 1481944478041, "tcdate": 1481944478041, "number": 2, "id": "SyIs6QfVx", "invitation": "ICLR.cc/2017/conference/-/paper231/official/review", "forum": "HyCRyS9gx", "replyto": "HyCRyS9gx", "signatures": ["ICLR.cc/2017/conference/paper231/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper231/AnonReviewer1"], "content": {"title": "Interesting idea, but clarity issues make the paper difficult to follow", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposes an interesting idea for rapidly adapting generative models in the low data regime. The idea is to use similar techniques that are used in one-shot learning, specifically ideas from matching networks. To that end, the authors propose the generative matching networks model, which is effectively a variational auto-encoder that can be conditioned on an input dataset. Given a query point, the model matches the query point to points in the conditioning set using an attention model in an embedding space (this is similar to matching networks). The results on the Omniglot dataset show that this method is successfully able to rapidly adapt to new input distributions given few examples.\n\nI think that the method is very interesting, however the major issue for me with this paper is a lack of clarity. I outline more details below, but overall I found the paper somewhat difficult to follow. There are a lot of details that I feel are scattered throughout, and I did not get a sense after reading this paper that I would be able to implement the method and replicate the results. My suggestion is to consolidate the major implementation details into a single section, and be explicit about the functional form of the different embedding functions and their variants.\n\nI was a bit disappointed to see that weak supervision in the form of labels had to be used. How does the method perform in a completely unsupervised setting? This could be an interesting baseline.\n\nThere is a lack of definition of the different functions. Some basic insight into the functional forms of f, g, \\phi, sim and R would be nice. Otherwise it is very unclear to me what\u2019s going on.\n\nSection 3.2: \u201conly state of the recurrent controller was used for matching\u201d, my reading of this section (after several passes) is that the pseudo-input is used in the place of a regular input. Is this correct? Otherwise, this sentence/section needs more clarification. I noticed upon further reading in section 4.2 that there are two versions of the model: one in which a pseudo input is used, and one in which a pseudo input is not used (the conditional version). What is the difference in functional form between these? That is, how do the formulas for the embeddings f and g change between these settings?\n\n\u201csince the result was fully contrastive we did not apply any further binarization\u201d what does it mean for a result to be fully contrastive?\n\nFor clarity, the figures and table refer to the number of shots, but this is never defined. I assume this is T here. This should be made consistent.\n\nFigure 2: why is the value of T only 9 in this case? What does it mean for it to be 0? It is stated earlier that T should go up to 20 (I assume #shot corresponds to T). It also looks like the results continue to improve with an increased number of steps, I would like to see the results for 5 and maybe 6 steps as well. Presumably there will come a point where you get diminishing returns.\n\nTable 1: is the VAE a fair baseline? You mention that Ctest affects Pd() in the evaluation. The fact that the VAE does not have an associated Ctest implies that the two models are being evaluated with a different metric. Can the authors clarify this? It\u2019s important that the comparison is apples-to-apples.\n\nMNIST is much more common than Omniglot for evaluating generative models. Would it be possible to perform similar experiments on this dataset? That way it can be compared with many more models.\n\nFurther, why are the negative log-likelihood values monotonically decreasing in the number of shots? That is, is there ever a case where increasing the number of shots can hurt things? What happens at T=30? 40?\n\nAs a minor grammatical issue, the paper is missing determiners in several sentences. At one point, the model is referred to as \u201cshe\u201d instead of \u201cit\u201d. \u201cOn figure 3\u201d should be changed to \u201cin figure 3\u201d in the experiments section.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Adaptation in Generative Models with Generative Matching Networks", "abstract": "Despite recent advances, the remaining bottlenecks in deep generative models are necessity of extensive training and difficulties with generalization from small number of training examples.\nBoth problems may be addressed by conditional generative models that are trained to adapt the generative distribution to additional input data.\nSo far this idea was explored only under certain limitations such as restricting the input data to be a single object or multiple objects representing the same concept.  \nIn this work we develop a new class of deep generative model called generative matching networks which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks and the ideas from meta-learning.\nBy conditioning on the additional input dataset, generative matching networks may instantly learn new concepts that were not available during the training but conform to a similar generative process, without explicit limitations on the number of additional input objects or the number of concepts they represent. \nOur experiments on the Omniglot dataset demonstrate that generative matching networks can significantly improve predictive performance on the fly as more additional data is available to the model and also adapt the latent space which is beneficial in the context of feature extraction.", "pdf": "/pdf/612d0be08c6f03132298219d89615a4017aa3974.pdf", "TL;DR": "A nonparametric conditional generative model with fast small-shot adaptation", "paperhash": "bartunov|fast_adaptation_in_generative_models_with_generative_matching_networks", "conflicts": ["google.com", "company.yandex.ru", "hse.ru", "cs.hse.ru", "msu.ru", "skoltech.ru", "ccas.ru"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Sergey Bartunov", "Dmitry P. Vetrov"], "authorids": ["sbos.net@gmail.com", "vetrovd@yandex.ru"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512655062, "id": "ICLR.cc/2017/conference/-/paper231/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper231/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper231/AnonReviewer3", "ICLR.cc/2017/conference/paper231/AnonReviewer1", "ICLR.cc/2017/conference/paper231/AnonReviewer2"], "reply": {"forum": "HyCRyS9gx", "replyto": "HyCRyS9gx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper231/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper231/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512655062}}}, {"tddate": null, "tmdate": 1481930647712, "tcdate": 1481930647712, "number": 1, "id": "r1esDxMNl", "invitation": "ICLR.cc/2017/conference/-/paper231/official/review", "forum": "HyCRyS9gx", "replyto": "HyCRyS9gx", "signatures": ["ICLR.cc/2017/conference/paper231/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper231/AnonReviewer3"], "content": {"title": "Interesting paper, good experiments", "rating": "7: Good paper, accept", "review": "The paper explores a VAE architecture and training procedure that allows to generate new samples of a concept based on several exemplars that are shown to the model. The proposed architecture processes the set of exemplars with a recurrent neural network and aggregation procedure similar to the one used in Matching Networks. The resulting \"summary\" is used to condition a generative model (a VAE) that produces new samples of the same kind as the exemplars shown. The proposed aggregation and conditioning procedure are better suited to sets of exemplars that come from several classes than simple averaging.\nPerhaps surprisingly the model generalizes from generation conditioned on samples from 2 classes to generation conditioned on samples from 4 classes.\nThe experiments are conducted on the OMNIGLOT dataset and are quite convincing. An explicit comparison to previous works is lacking, but this is explained in the appendices, and a comparison to architectures similar to previous work is presented.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Adaptation in Generative Models with Generative Matching Networks", "abstract": "Despite recent advances, the remaining bottlenecks in deep generative models are necessity of extensive training and difficulties with generalization from small number of training examples.\nBoth problems may be addressed by conditional generative models that are trained to adapt the generative distribution to additional input data.\nSo far this idea was explored only under certain limitations such as restricting the input data to be a single object or multiple objects representing the same concept.  \nIn this work we develop a new class of deep generative model called generative matching networks which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks and the ideas from meta-learning.\nBy conditioning on the additional input dataset, generative matching networks may instantly learn new concepts that were not available during the training but conform to a similar generative process, without explicit limitations on the number of additional input objects or the number of concepts they represent. \nOur experiments on the Omniglot dataset demonstrate that generative matching networks can significantly improve predictive performance on the fly as more additional data is available to the model and also adapt the latent space which is beneficial in the context of feature extraction.", "pdf": "/pdf/612d0be08c6f03132298219d89615a4017aa3974.pdf", "TL;DR": "A nonparametric conditional generative model with fast small-shot adaptation", "paperhash": "bartunov|fast_adaptation_in_generative_models_with_generative_matching_networks", "conflicts": ["google.com", "company.yandex.ru", "hse.ru", "cs.hse.ru", "msu.ru", "skoltech.ru", "ccas.ru"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Sergey Bartunov", "Dmitry P. Vetrov"], "authorids": ["sbos.net@gmail.com", "vetrovd@yandex.ru"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512655062, "id": "ICLR.cc/2017/conference/-/paper231/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper231/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper231/AnonReviewer3", "ICLR.cc/2017/conference/paper231/AnonReviewer1", "ICLR.cc/2017/conference/paper231/AnonReviewer2"], "reply": {"forum": "HyCRyS9gx", "replyto": "HyCRyS9gx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper231/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper231/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512655062}}}, {"tddate": null, "tmdate": 1480803021328, "tcdate": 1480803021321, "number": 2, "id": "ByrRfTe7g", "invitation": "ICLR.cc/2017/conference/-/paper231/public/comment", "forum": "HyCRyS9gx", "replyto": "Hynxguymg", "signatures": ["~Sergey_Bartunov1"], "readers": ["everyone"], "writers": ["~Sergey_Bartunov1"], "content": {"title": "Answer", "comment": "Yes, that's correct. In the models which are denoted as \"average\" the matching procedure is replaced by averaging prototype vectors and hence the attention distribution is uniform.\n\nWe have just uploaded a new revision of the paper where we tried to make this more explicit."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Adaptation in Generative Models with Generative Matching Networks", "abstract": "Despite recent advances, the remaining bottlenecks in deep generative models are necessity of extensive training and difficulties with generalization from small number of training examples.\nBoth problems may be addressed by conditional generative models that are trained to adapt the generative distribution to additional input data.\nSo far this idea was explored only under certain limitations such as restricting the input data to be a single object or multiple objects representing the same concept.  \nIn this work we develop a new class of deep generative model called generative matching networks which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks and the ideas from meta-learning.\nBy conditioning on the additional input dataset, generative matching networks may instantly learn new concepts that were not available during the training but conform to a similar generative process, without explicit limitations on the number of additional input objects or the number of concepts they represent. \nOur experiments on the Omniglot dataset demonstrate that generative matching networks can significantly improve predictive performance on the fly as more additional data is available to the model and also adapt the latent space which is beneficial in the context of feature extraction.", "pdf": "/pdf/612d0be08c6f03132298219d89615a4017aa3974.pdf", "TL;DR": "A nonparametric conditional generative model with fast small-shot adaptation", "paperhash": "bartunov|fast_adaptation_in_generative_models_with_generative_matching_networks", "conflicts": ["google.com", "company.yandex.ru", "hse.ru", "cs.hse.ru", "msu.ru", "skoltech.ru", "ccas.ru"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Sergey Bartunov", "Dmitry P. Vetrov"], "authorids": ["sbos.net@gmail.com", "vetrovd@yandex.ru"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287673645, "id": "ICLR.cc/2017/conference/-/paper231/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyCRyS9gx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper231/reviewers", "ICLR.cc/2017/conference/paper231/areachairs"], "cdate": 1485287673645}}}, {"tddate": null, "tmdate": 1480716276513, "tcdate": 1480716276508, "number": 2, "id": "Hynxguymg", "invitation": "ICLR.cc/2017/conference/-/paper231/pre-review/question", "forum": "HyCRyS9gx", "replyto": "HyCRyS9gx", "signatures": ["ICLR.cc/2017/conference/paper231/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper231/AnonReviewer1"], "content": {"title": "Averaging", "question": "Could you clarify what the averaging method is doing in Figure 3 and Table 1? Is this simply averaging the latent representations of the dataset as opposed to using a soft attention mechanism + LSTM?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Adaptation in Generative Models with Generative Matching Networks", "abstract": "Despite recent advances, the remaining bottlenecks in deep generative models are necessity of extensive training and difficulties with generalization from small number of training examples.\nBoth problems may be addressed by conditional generative models that are trained to adapt the generative distribution to additional input data.\nSo far this idea was explored only under certain limitations such as restricting the input data to be a single object or multiple objects representing the same concept.  \nIn this work we develop a new class of deep generative model called generative matching networks which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks and the ideas from meta-learning.\nBy conditioning on the additional input dataset, generative matching networks may instantly learn new concepts that were not available during the training but conform to a similar generative process, without explicit limitations on the number of additional input objects or the number of concepts they represent. \nOur experiments on the Omniglot dataset demonstrate that generative matching networks can significantly improve predictive performance on the fly as more additional data is available to the model and also adapt the latent space which is beneficial in the context of feature extraction.", "pdf": "/pdf/612d0be08c6f03132298219d89615a4017aa3974.pdf", "TL;DR": "A nonparametric conditional generative model with fast small-shot adaptation", "paperhash": "bartunov|fast_adaptation_in_generative_models_with_generative_matching_networks", "conflicts": ["google.com", "company.yandex.ru", "hse.ru", "cs.hse.ru", "msu.ru", "skoltech.ru", "ccas.ru"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Sergey Bartunov", "Dmitry P. Vetrov"], "authorids": ["sbos.net@gmail.com", "vetrovd@yandex.ru"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959391934, "id": "ICLR.cc/2017/conference/-/paper231/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper231/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper231/AnonReviewer2", "ICLR.cc/2017/conference/paper231/AnonReviewer1"], "reply": {"forum": "HyCRyS9gx", "replyto": "HyCRyS9gx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper231/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper231/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959391934}}}, {"tddate": null, "tmdate": 1480541718447, "tcdate": 1480541718442, "number": 1, "id": "rk0f8a3Mg", "invitation": "ICLR.cc/2017/conference/-/paper231/public/comment", "forum": "HyCRyS9gx", "replyto": "rJtpvUozg", "signatures": ["~Sergey_Bartunov1"], "readers": ["everyone"], "writers": ["~Sergey_Bartunov1"], "content": {"title": "Answer", "comment": "Thank you for the question.\n\nThat is totally correct, as we mention in the paper our approach is inspired by the matching networks architecture which was originally proposed for supervised learning problems. \nThe matching procedure described in section 3.1 is mostly similar to the one used in discriminative matching networks.\nHowever, there is a key difference in that since no label information is available to the network we interpolate between conditioning data not in a label space, but rather in a prototype space which is defined by the model itself and is also learned together with a matching (or feature, if you like) space.\n\nThis is a good point, our paper would benefit from highlighting these similarities and differences which we will address in future revisions."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Adaptation in Generative Models with Generative Matching Networks", "abstract": "Despite recent advances, the remaining bottlenecks in deep generative models are necessity of extensive training and difficulties with generalization from small number of training examples.\nBoth problems may be addressed by conditional generative models that are trained to adapt the generative distribution to additional input data.\nSo far this idea was explored only under certain limitations such as restricting the input data to be a single object or multiple objects representing the same concept.  \nIn this work we develop a new class of deep generative model called generative matching networks which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks and the ideas from meta-learning.\nBy conditioning on the additional input dataset, generative matching networks may instantly learn new concepts that were not available during the training but conform to a similar generative process, without explicit limitations on the number of additional input objects or the number of concepts they represent. \nOur experiments on the Omniglot dataset demonstrate that generative matching networks can significantly improve predictive performance on the fly as more additional data is available to the model and also adapt the latent space which is beneficial in the context of feature extraction.", "pdf": "/pdf/612d0be08c6f03132298219d89615a4017aa3974.pdf", "TL;DR": "A nonparametric conditional generative model with fast small-shot adaptation", "paperhash": "bartunov|fast_adaptation_in_generative_models_with_generative_matching_networks", "conflicts": ["google.com", "company.yandex.ru", "hse.ru", "cs.hse.ru", "msu.ru", "skoltech.ru", "ccas.ru"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Sergey Bartunov", "Dmitry P. Vetrov"], "authorids": ["sbos.net@gmail.com", "vetrovd@yandex.ru"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287673645, "id": "ICLR.cc/2017/conference/-/paper231/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyCRyS9gx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper231/reviewers", "ICLR.cc/2017/conference/paper231/areachairs"], "cdate": 1485287673645}}}, {"tddate": null, "tmdate": 1480447936631, "tcdate": 1480447936627, "number": 1, "id": "rJtpvUozg", "invitation": "ICLR.cc/2017/conference/-/paper231/pre-review/question", "forum": "HyCRyS9gx", "replyto": "HyCRyS9gx", "signatures": ["ICLR.cc/2017/conference/paper231/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper231/AnonReviewer2"], "content": {"title": "relationship with matching networks?", "question": "Can you expand on the relationship between your approach and the matching networks of Vinyals et al. (2016)?  You seem to be building on that work, but it's only discussed briefly in the introduction.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Adaptation in Generative Models with Generative Matching Networks", "abstract": "Despite recent advances, the remaining bottlenecks in deep generative models are necessity of extensive training and difficulties with generalization from small number of training examples.\nBoth problems may be addressed by conditional generative models that are trained to adapt the generative distribution to additional input data.\nSo far this idea was explored only under certain limitations such as restricting the input data to be a single object or multiple objects representing the same concept.  \nIn this work we develop a new class of deep generative model called generative matching networks which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks and the ideas from meta-learning.\nBy conditioning on the additional input dataset, generative matching networks may instantly learn new concepts that were not available during the training but conform to a similar generative process, without explicit limitations on the number of additional input objects or the number of concepts they represent. \nOur experiments on the Omniglot dataset demonstrate that generative matching networks can significantly improve predictive performance on the fly as more additional data is available to the model and also adapt the latent space which is beneficial in the context of feature extraction.", "pdf": "/pdf/612d0be08c6f03132298219d89615a4017aa3974.pdf", "TL;DR": "A nonparametric conditional generative model with fast small-shot adaptation", "paperhash": "bartunov|fast_adaptation_in_generative_models_with_generative_matching_networks", "conflicts": ["google.com", "company.yandex.ru", "hse.ru", "cs.hse.ru", "msu.ru", "skoltech.ru", "ccas.ru"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Sergey Bartunov", "Dmitry P. Vetrov"], "authorids": ["sbos.net@gmail.com", "vetrovd@yandex.ru"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959391934, "id": "ICLR.cc/2017/conference/-/paper231/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper231/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper231/AnonReviewer2", "ICLR.cc/2017/conference/paper231/AnonReviewer1"], "reply": {"forum": "HyCRyS9gx", "replyto": "HyCRyS9gx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper231/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper231/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959391934}}}], "count": 13}