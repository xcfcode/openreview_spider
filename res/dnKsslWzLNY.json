{"notes": [{"id": "dnKsslWzLNY", "original": "JCySo76hRvO", "number": 1517, "cdate": 1601308168520, "ddate": null, "tcdate": 1601308168520, "tmdate": 1614985654646, "tddate": null, "forum": "dnKsslWzLNY", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "On the Universal Approximability and Complexity Bounds of Deep Learning in Hybrid Quantum-Classical Computing", "authorids": ["~Weiwen_Jiang1", "~Yukun_Ding1", "~Yiyu_Shi1"], "authors": ["Weiwen Jiang", "Yukun Ding", "Yiyu Shi"], "keywords": ["deep learning", "hybrid quantum-classical computing", "universal approximability"], "abstract": "With the continuously increasing number of quantum bits in quantum computers, there are growing interests in exploring applications that can harvest the power of them. Recently, several attempts were made to implement neural networks, known to be computationally intensive, in hybrid quantum-classical scheme computing. While encouraging results are shown, two fundamental questions need to be answered: (1) whether neural networks in hybrid quantum-classical computing can leverage quantum power and meanwhile approximate any function within a given error bound, i.e., universal approximability; (2) how do these neural networks compare with ones on a classical computer in terms of representation power? This work sheds light on these two questions from a theoretical perspective.", "one-sentence_summary": "This paper proves the universal approximability of neural networks on a quantum computer for a wide class of functions as well as the associated bounds. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|on_the_universal_approximability_and_complexity_bounds_of_deep_learning_in_hybrid_quantumclassical_computing", "pdf": "/pdf/884968f81a10499778d74b41376cb12924ab1239.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=J22qDO79rj", "_bibtex": "@misc{\njiang2021on,\ntitle={On the Universal Approximability and Complexity Bounds of Deep Learning in Hybrid Quantum-Classical Computing},\nauthor={Weiwen Jiang and Yukun Ding and Yiyu Shi},\nyear={2021},\nurl={https://openreview.net/forum?id=dnKsslWzLNY}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "ocXe8fv0Ham", "original": null, "number": 1, "cdate": 1610040507766, "ddate": null, "tcdate": 1610040507766, "tmdate": 1610474115206, "tddate": null, "forum": "dnKsslWzLNY", "replyto": "dnKsslWzLNY", "invitation": "ICLR.cc/2021/Conference/Paper1517/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper provides approximation results for functions that can be represented by hybrid quantum-classical circuits. It is felt that venues such as QIP would be a more suitable venue, and perhaps some experiments/simulations could be added."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Universal Approximability and Complexity Bounds of Deep Learning in Hybrid Quantum-Classical Computing", "authorids": ["~Weiwen_Jiang1", "~Yukun_Ding1", "~Yiyu_Shi1"], "authors": ["Weiwen Jiang", "Yukun Ding", "Yiyu Shi"], "keywords": ["deep learning", "hybrid quantum-classical computing", "universal approximability"], "abstract": "With the continuously increasing number of quantum bits in quantum computers, there are growing interests in exploring applications that can harvest the power of them. Recently, several attempts were made to implement neural networks, known to be computationally intensive, in hybrid quantum-classical scheme computing. While encouraging results are shown, two fundamental questions need to be answered: (1) whether neural networks in hybrid quantum-classical computing can leverage quantum power and meanwhile approximate any function within a given error bound, i.e., universal approximability; (2) how do these neural networks compare with ones on a classical computer in terms of representation power? This work sheds light on these two questions from a theoretical perspective.", "one-sentence_summary": "This paper proves the universal approximability of neural networks on a quantum computer for a wide class of functions as well as the associated bounds. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|on_the_universal_approximability_and_complexity_bounds_of_deep_learning_in_hybrid_quantumclassical_computing", "pdf": "/pdf/884968f81a10499778d74b41376cb12924ab1239.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=J22qDO79rj", "_bibtex": "@misc{\njiang2021on,\ntitle={On the Universal Approximability and Complexity Bounds of Deep Learning in Hybrid Quantum-Classical Computing},\nauthor={Weiwen Jiang and Yukun Ding and Yiyu Shi},\nyear={2021},\nurl={https://openreview.net/forum?id=dnKsslWzLNY}\n}"}, "tags": [], "invitation": {"reply": {"forum": "dnKsslWzLNY", "replyto": "dnKsslWzLNY", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040507753, "tmdate": 1610474115190, "id": "ICLR.cc/2021/Conference/Paper1517/-/Decision"}}}, {"id": "DyMreCdsikF", "original": null, "number": 3, "cdate": 1604251536026, "ddate": null, "tcdate": 1604251536026, "tmdate": 1606803505038, "tddate": null, "forum": "dnKsslWzLNY", "replyto": "dnKsslWzLNY", "invitation": "ICLR.cc/2021/Conference/Paper1517/-/Official_Review", "content": {"title": "hard to understand context and significance", "review": "This paper provides new approximation theorems for a family of functions representable by hybrid quantum-classical circuits. \n\n\nSpecifically, the paper looks at neural nets that can be evaluated quickly by a three-stage circuit, where the second stage is quantum and the first and third phases are classical. The paper shows that neural networks in such a class allow for more efficient approximation of certain functions than (a) was previously known and (b) is possible using a related class of networks that can be implemented using an entirely classical circuit. \n\n\n\nOverall, I found the paper\u2019s significance hard to evaluate. Here are a few questions I had a hard Time finding answers to in the text of the paper: \n\n- Why is this problem important to begin with? Is the bottleneck with neural nets their evaluation, or their training? Is the hope that by using quantum computers to evaluate complex NN\u2019s we can allow for learning more complex functions? How would one train such networks?  I felt like the big picture was missing. I know my way around quantum computing and algorithms but still found the paper hard to read and understand. I think a typical ICLR audience-member would be entirely lost.\n\n- The comparison with known bounds for classical circuits in Section 5 was quite hard to understand. Taking it at face value, the gains in complexity from moving to the quantum model seem limited: gains of $poly\\log(1/\\epsilon)$ in depth and circuit size? Is it clear that no better classical circuits are known for approximating the functions of interest? \n\nSome specific comments and suggestions: \n\n- Equation 1: What does it mean that y is bounded when sigma is a polynomial? Does sigma have to be such that it\u2019s value is bounded on the range of possible inputs (something like $-(d+1)$ to $(d_1)$)?\n- Expand the notation in the definition of equation (2). I interpreted it to mean that derivatives of up to order n are defined almost everywhere, and bounded. But the comment after that (when n=1 and f is not differentiable) confused me. \n- Lemma 3.3: how complex is the network as a function of delta? I had trouble mapping the form of $f_2$ into the form of the network. I guess it has something like depth 3 and a number of neurons equal to the number of possible $\\mathbf{ k, v}$ pairs?\n- I felt like what was missing at the end of Section 3.2 was a self-contained recap statement. \n- Moving the main Theorem statements (4.5 and 4.6) earlier to the introduction (along with the comparison to bounds for classical circuits) would help. (That is, explaining the result top-down, rather than bottom-up.) The effort to keep everything in mind until I got to final statements exceeded my stack depth. I like to think that I would have had no trouble understanding a more self-contained exposition of the main results.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper1517/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1517/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Universal Approximability and Complexity Bounds of Deep Learning in Hybrid Quantum-Classical Computing", "authorids": ["~Weiwen_Jiang1", "~Yukun_Ding1", "~Yiyu_Shi1"], "authors": ["Weiwen Jiang", "Yukun Ding", "Yiyu Shi"], "keywords": ["deep learning", "hybrid quantum-classical computing", "universal approximability"], "abstract": "With the continuously increasing number of quantum bits in quantum computers, there are growing interests in exploring applications that can harvest the power of them. Recently, several attempts were made to implement neural networks, known to be computationally intensive, in hybrid quantum-classical scheme computing. While encouraging results are shown, two fundamental questions need to be answered: (1) whether neural networks in hybrid quantum-classical computing can leverage quantum power and meanwhile approximate any function within a given error bound, i.e., universal approximability; (2) how do these neural networks compare with ones on a classical computer in terms of representation power? This work sheds light on these two questions from a theoretical perspective.", "one-sentence_summary": "This paper proves the universal approximability of neural networks on a quantum computer for a wide class of functions as well as the associated bounds. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|on_the_universal_approximability_and_complexity_bounds_of_deep_learning_in_hybrid_quantumclassical_computing", "pdf": "/pdf/884968f81a10499778d74b41376cb12924ab1239.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=J22qDO79rj", "_bibtex": "@misc{\njiang2021on,\ntitle={On the Universal Approximability and Complexity Bounds of Deep Learning in Hybrid Quantum-Classical Computing},\nauthor={Weiwen Jiang and Yukun Ding and Yiyu Shi},\nyear={2021},\nurl={https://openreview.net/forum?id=dnKsslWzLNY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "dnKsslWzLNY", "replyto": "dnKsslWzLNY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1517/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538116806, "tmdate": 1606915802319, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1517/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1517/-/Official_Review"}}}, {"id": "waxGSduJ0_P", "original": null, "number": 5, "cdate": 1605326610773, "ddate": null, "tcdate": 1605326610773, "tmdate": 1605328315712, "tddate": null, "forum": "dnKsslWzLNY", "replyto": "dnKsslWzLNY", "invitation": "ICLR.cc/2021/Conference/Paper1517/-/Official_Comment", "content": {"title": "Summary of Changes of Revision-1", "comment": "We would like to express our sincere thanks to the reviewers for their valuable time and constructive comments to help us to improve the quality of this paper. We are glad to report that this revision has complied with all the review comments. We denote this version of revision as \"Revision-1\".\n\nIn the responses, we have listed the detailed actions for each comment. The review comments are shown in **Bold font**, our detailed responses are shown in \"Normal font\", and the position of modifications in Revision-1 using *Italic font*. In Revision-1, we have highlighted the modified contents using \"red color\".\n\n#### Summary of Changes:\n* (Section 2) We have added one set of experimental results to demonstrate that the current state of implementing neural networks on quantum accelerators. In addition, we have added more discussions on the motivation of this work and emphasized the importance of the study on the proof of the universal approximability of neural networks design for hybrid quantum-classic computing; in particular the TensorFlow Quantum.\n* (Section 3) We have added a new section by moving the main results to this section. We have also clarified that the proposed prologue-acceleration-epilogue computing scheme is a special case of hybrid quantum-classical computing used in TensorFlow Quantum, which implies that if we prove the neural networks designed for the target computing scheme having the universal approximability, the conclusion can be held for the wide hybrid quantum-classical computing platform, like TensorFlow Quantum. \n* (Section 4) We have rewritten the function space to make it clearer and added a recap statement after Sec 4.2. In addition, we have provided examples to explain the operations in the designed quantum circuits.\n* (Section 5) We have added the practical insights derived from the main results obtained in this work and discussed how the findings in this work can be used for future research in the quantum machine learning field.\n* (Reference) We have added 8 new related references.\n  * [1] Marcello Benedetti, Delfina Garcia-Pintos, Oscar Perdomo, Vicente Leyton-Ortega, Yunseong Nam, and Alejandro Perdomo-Ortiz. A generative modeling approach for benchmarking and training shallow quantum circuits. npj Quantum Information, 5(1):1\u20139, 2019.\n  *[2] Marco Cerezo, Akira Sone, Tyler Volkoff, Lukasz Cincio, and Patrick J Coles. Cost-function-dependent barren plateaus in shallow quantum neural networks. arXiv preprint arXiv:2001.00550,2020.\n  *[3] Iris Cong, Soonwon Choi, and Mikhail D Lukin.  Quantum convolutional neural networks. Nature Physics, 15(12):1273\u20131278, 2019.\n  *[4] Alberto Delgado. Function Approximation with Quantum Circuit. 2018.\n  *[5] William Huggins, Piyush Patil, Bradley Mitchell, K Birgitta Whaley, and E Miles Stoudenmire. Towards quantum machine learning with tensor networks. Quantum Science and technology, 4(2):024001, 2019.\n  *[6] Yann LeCun, L \u0301eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.\n  *[7] Alejandro Perdomo-Ortiz, Marcello Benedetti, John Realpe-G \u0301omez, and Rupak Biswas. Opportunities and challenges for quantum-assisted machine learning in near-term quantum computers. Quantum Science and Technology, 3(3):030502, 2018.\n  *[8] Rongxin Xia and Sabre Kais.  Hybrid Quantum-Classical Neural Network for Calculating Ground State Energies of Molecules. Entropy, 22(8):828, 2020.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1517/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1517/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Universal Approximability and Complexity Bounds of Deep Learning in Hybrid Quantum-Classical Computing", "authorids": ["~Weiwen_Jiang1", "~Yukun_Ding1", "~Yiyu_Shi1"], "authors": ["Weiwen Jiang", "Yukun Ding", "Yiyu Shi"], "keywords": ["deep learning", "hybrid quantum-classical computing", "universal approximability"], "abstract": "With the continuously increasing number of quantum bits in quantum computers, there are growing interests in exploring applications that can harvest the power of them. Recently, several attempts were made to implement neural networks, known to be computationally intensive, in hybrid quantum-classical scheme computing. While encouraging results are shown, two fundamental questions need to be answered: (1) whether neural networks in hybrid quantum-classical computing can leverage quantum power and meanwhile approximate any function within a given error bound, i.e., universal approximability; (2) how do these neural networks compare with ones on a classical computer in terms of representation power? This work sheds light on these two questions from a theoretical perspective.", "one-sentence_summary": "This paper proves the universal approximability of neural networks on a quantum computer for a wide class of functions as well as the associated bounds. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|on_the_universal_approximability_and_complexity_bounds_of_deep_learning_in_hybrid_quantumclassical_computing", "pdf": "/pdf/884968f81a10499778d74b41376cb12924ab1239.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=J22qDO79rj", "_bibtex": "@misc{\njiang2021on,\ntitle={On the Universal Approximability and Complexity Bounds of Deep Learning in Hybrid Quantum-Classical Computing},\nauthor={Weiwen Jiang and Yukun Ding and Yiyu Shi},\nyear={2021},\nurl={https://openreview.net/forum?id=dnKsslWzLNY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "dnKsslWzLNY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1517/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1517/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1517/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1517/Authors|ICLR.cc/2021/Conference/Paper1517/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1517/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858787, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1517/-/Official_Comment"}}}, {"id": "EpfI9XIaeZi", "original": null, "number": 9, "cdate": 1605328251013, "ddate": null, "tcdate": 1605328251013, "tmdate": 1605328251013, "tddate": null, "forum": "dnKsslWzLNY", "replyto": "eQVq6oD4lBl", "invitation": "ICLR.cc/2021/Conference/Paper1517/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Thanks for your encouragement for the value of the studied problem. We appreciate your valuable time to give us helpful comments and check the correctness of the proofs. In Revision-1, we have carefully considered your comments and taken actions to address them. The detailed actions are reported as follows.\n\n**C1. How can the results apply to the current existing topologies of QNN/variation circuits?**\n\nYes, as you said it addresses models for TensorFlow quantum and your understanding of the workflow for the proposed computing scheme is correct. In *Sec 3 of Revision-1*, we have clarified that the employed prologue-acceleration-epilogue computing scheme is a special case of TensorFlow quantum. In consequence, since we have proved that the neural networks designed for the target computing scheme have universal approximability, then we can derive that the quantum neural network designed for TensorFlow quantum also has the universal approximability. \n\n**C2. Literature review on quantum expressiveness of variational circuits.**\n\nThanks for the suggestion, and we have added the literature review of quantum expressiveness in *Sec 2.2 of Revision-1*.\n\n**C3. Experiments on QNN built on the target computing scheme.**\n\nFirst of all, we would like to emphasize that similar to most existing works on neural network complexity bounds, the proof-by-construction method we used in this paper is not necessarily the best way to build QNNs for real tasks. It simply shows that a quantum neural network does exist to approximate a wide class of functions with arbitrarily small error (the reason why what we have derived is only an upper bound in Big-O notation). Depending on the actual problem, there can be much better ways to construct/train the network.   \n\nFollowing your suggestion, in *Sec 2.2 of Revision-1*, we have added the experimental results on a classification task using the pairwise classifiers in MNIST dataset, as shown in Fig. 1. Using the classical ReLU neural network as a baseline, results have demonstrated that the QNN run on the proposed prologue-acceleration-epilogue computing scheme can achieve competitive classification accuracy with baseline, where the average accuracy gap is less than 0.5%. \n\n**C4. Adding examples to show the proposed architecture to be used in practice.**\n\nIn *Appendix B of Revision-1*, we have added an example to demonstrate how to apply the existing QNN to the proposed architecture. Specifically, let denote C1 for the prologue, Q for the acceleration, and C2 for the epilogue. To use this platform, the input data can be prepared at C1 phase. Then, the existing QNN like that proposed in (Francesco et al, 2019; Jiang et al, 2020) can be implemented on Q portion for the neural computation. Finally, a ReLU layer can be an add-on to the output of Q for the post-processing. Following this procedure, we obtain almost the same accuracy with ReLU network for the pairwise classifiers in MNIST dataset, as shown in Fig. 1. In the whole procedure, it only needs two interfaces for the classical-quantum data transfer. Together with the results, it demonstrates that the proposed architecture can be used in practice.\n\n**C5. The statement of computing $y^2$ which is finally stored in zero state is not clear.**\n\nIn *Sec 4.4 of Revision-1*, we have added discussions on how we compute the nth-order non-linear function (n>1). Specifically, we apply two sets of qubits (called sub-system), and they will first conduct the neural computation independently. The results are stored in the zero state for each sub-system. Then, by combining these two sub-systems (say S1 and S2) into one system (say S), the amplitude of the zero state for S will be the multiplication of amplitudes of zero states of S1 and S2. This is because the combination represents the tensor product. Therefore, for the computation of $y^2$, its final results will be stored at zero state of system S.\n\n**C6. The complexity of encoding unitary data to qubits is only true if it's just one single column.**\n\nIn *Sec 4.3 of Revision-1*, we have clarified that the proposed architecture is to encode the data in vector $A_1$, and only the vector will be encoded to the qubits, which is the same case as described in the comment.\n\n**C7. Adding the literature on the power of quantum shallow circuits.**\n\nThanks for the suggestion. In Revision-1, we have added new references, and correspondingly in *Sec 5 of Revision-1*, we have discussed the insights obtained by the depth complexity and demonstrated that the power of the shallow quantum neural network in the recent works.\n\n**C8. Comments and suggestions on splitting Figure 1 into two figures, notation definition (i.e., p of Yarotsky), notation consistency (i.e., multiplications and number of qubits), and typos.**\n\nThanks for pointing out all these details. In Revision-1, per your suggestions, we have fixed these typos, added the missing definitions, and resolved the inconsistent issue."}, "signatures": ["ICLR.cc/2021/Conference/Paper1517/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1517/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Universal Approximability and Complexity Bounds of Deep Learning in Hybrid Quantum-Classical Computing", "authorids": ["~Weiwen_Jiang1", "~Yukun_Ding1", "~Yiyu_Shi1"], "authors": ["Weiwen Jiang", "Yukun Ding", "Yiyu Shi"], "keywords": ["deep learning", "hybrid quantum-classical computing", "universal approximability"], "abstract": "With the continuously increasing number of quantum bits in quantum computers, there are growing interests in exploring applications that can harvest the power of them. Recently, several attempts were made to implement neural networks, known to be computationally intensive, in hybrid quantum-classical scheme computing. While encouraging results are shown, two fundamental questions need to be answered: (1) whether neural networks in hybrid quantum-classical computing can leverage quantum power and meanwhile approximate any function within a given error bound, i.e., universal approximability; (2) how do these neural networks compare with ones on a classical computer in terms of representation power? This work sheds light on these two questions from a theoretical perspective.", "one-sentence_summary": "This paper proves the universal approximability of neural networks on a quantum computer for a wide class of functions as well as the associated bounds. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|on_the_universal_approximability_and_complexity_bounds_of_deep_learning_in_hybrid_quantumclassical_computing", "pdf": "/pdf/884968f81a10499778d74b41376cb12924ab1239.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=J22qDO79rj", "_bibtex": "@misc{\njiang2021on,\ntitle={On the Universal Approximability and Complexity Bounds of Deep Learning in Hybrid Quantum-Classical Computing},\nauthor={Weiwen Jiang and Yukun Ding and Yiyu Shi},\nyear={2021},\nurl={https://openreview.net/forum?id=dnKsslWzLNY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "dnKsslWzLNY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1517/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1517/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1517/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1517/Authors|ICLR.cc/2021/Conference/Paper1517/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1517/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858787, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1517/-/Official_Comment"}}}, {"id": "b4iogG244Yq", "original": null, "number": 8, "cdate": 1605327730830, "ddate": null, "tcdate": 1605327730830, "tmdate": 1605327730830, "tddate": null, "forum": "dnKsslWzLNY", "replyto": "IPcJ3lSTNE", "invitation": "ICLR.cc/2021/Conference/Paper1517/-/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "Thanks for your encouragement and your valuable time to give us helpful comments. In Revision-1, we have carefully considered your comments. The detailed response is listed as follows.\n\n**C1. Pros: The paper shows how some techniques used to obtain classical approximation theorems can be extended to quantum gates. The slight improvement from the addition of a quantum element raises the possibility of showing a stronger separation in the future.**\n\nThanks for the comment. Yes, the proof of universal approximability is the main contribution of this work. We use the bound-by-construction approach for the derivation and we further analyze the complexity bounds for the whole system.\n\n**C2. Cons: The results in their current form do not really show any benefit (from the point of view of approximation) for quantum neural networks, since the bounds are only logarithmically better than classical bounds.**\n\nThe primary goal of this paper is to prove the universal approximability of neural networks on quantum computers, a missing piece in the literature. This question arises from the fact that the quantum circuit has limitations in computation operations to achieve high parallelism, such that classical neural networks cannot be directly mapped to quantum circuits. For example, quantum circuits do not allow common activation functions, like ReLU and Sigmod. This question is important since the failure to attain universal approximability will fundamentally hinder quantum neural networks from achieving state-of-the-art performance on various machine learning tasks, say image classification. And we prove it by using a bound-by-construction approach.\n\nBased on the constructed neural network, we further give the complexity bound analysis. Our conclusion is that neural networks on quantum computers can achieve a lower depth upper bound and smaller circuit size compared with those on classical computers. But it is in big O notation, so the actual reduction in depth and circuit size is not bounded by polylog(1/\u03f5) \u2013 it can be much higher as has been empirically shown in the literature for various real implementations. \n\n**C3. The technique used seems to be to use quantum units to mimic subfunctions in a manner quite similar to classical units, and does not provide much insight into why we should expect a 'quantum improvement' here.**\n\nThanks for the comment. In *Sec 2.2 of Revision-1*, we have clarified that there emerging works focusing on implementing classical neural networks to quantum computing to make full use of high-parallelism of quantum computing and regard quantum computing as an accelerator. They have demonstrated competitive accuracy against the classical neural networks with reduced time complexity (Francesco et al, 2019; Jiang et al, 2020). \nIn Sec 5 of Revision-1, we have discussed the insights obtained from the derived results and future works. Specifically, results in this work provide theoretical and practical insights into the design of neural networks for quantum computing to fully harvest the quantum power in the hybrid quantum-classical computing scheme.\n* Neural networks designed for hybrid quantum-classical computing, including TensorFlow Quantum, have the ability to approximate a wide class of functions with arbitrarily small error.\n* For the near-term hybrid quantum-classical neural network designs, the proposed prologue-acceleration-epilogue architecture is a promising computing scheme to achieve high accuracy with only two interfaces between quantum and classical portions for data conversion.\n* Based on the depth complexity of $O(1)$ for a hybrid quantum-classical neural network, it inspires that the design of neural network for quantum computing may consider a \u201cshallow\u201d quantum circuit, instead of the \u201cdeep\u201d version on classical computers. The power of shallow circuits has been demonstrated in recent works.\n\nPutting all together, this work demonstrated the combination of machine learning and quantum computing is a promising research direction, and the results can guide future research works in the design of neural networks for quantum computing. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1517/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1517/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Universal Approximability and Complexity Bounds of Deep Learning in Hybrid Quantum-Classical Computing", "authorids": ["~Weiwen_Jiang1", "~Yukun_Ding1", "~Yiyu_Shi1"], "authors": ["Weiwen Jiang", "Yukun Ding", "Yiyu Shi"], "keywords": ["deep learning", "hybrid quantum-classical computing", "universal approximability"], "abstract": "With the continuously increasing number of quantum bits in quantum computers, there are growing interests in exploring applications that can harvest the power of them. Recently, several attempts were made to implement neural networks, known to be computationally intensive, in hybrid quantum-classical scheme computing. While encouraging results are shown, two fundamental questions need to be answered: (1) whether neural networks in hybrid quantum-classical computing can leverage quantum power and meanwhile approximate any function within a given error bound, i.e., universal approximability; (2) how do these neural networks compare with ones on a classical computer in terms of representation power? This work sheds light on these two questions from a theoretical perspective.", "one-sentence_summary": "This paper proves the universal approximability of neural networks on a quantum computer for a wide class of functions as well as the associated bounds. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|on_the_universal_approximability_and_complexity_bounds_of_deep_learning_in_hybrid_quantumclassical_computing", "pdf": "/pdf/884968f81a10499778d74b41376cb12924ab1239.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=J22qDO79rj", "_bibtex": "@misc{\njiang2021on,\ntitle={On the Universal Approximability and Complexity Bounds of Deep Learning in Hybrid Quantum-Classical Computing},\nauthor={Weiwen Jiang and Yukun Ding and Yiyu Shi},\nyear={2021},\nurl={https://openreview.net/forum?id=dnKsslWzLNY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "dnKsslWzLNY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1517/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1517/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1517/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1517/Authors|ICLR.cc/2021/Conference/Paper1517/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1517/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858787, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1517/-/Official_Comment"}}}, {"id": "uwiM__0hTEi", "original": null, "number": 7, "cdate": 1605327479528, "ddate": null, "tcdate": 1605327479528, "tmdate": 1605327516094, "tddate": null, "forum": "dnKsslWzLNY", "replyto": "N6aWe8XnCnJ", "invitation": "ICLR.cc/2021/Conference/Paper1517/-/Official_Comment", "content": {"title": "Response to AnonReviewer2 (Part 2)", "comment": "**C7. Question on Lemma 3.3.**\n\nFirst, in *Sec A2 of Revision-1*, we have clarified that the complexity of the neural network build to obtain f2 is related to the possible k,v pairs. More specifically, the number of terms is $(S+1)^d(\\frac{d^n-1}{d-1})$. For the implementation of each term in BPNN, it involves the multiplication of a constant and an input variable with the $n^{th}$ order non-linear function, which can be implemented using the basic neural operations in Figure 2 with a constant number of weights (i.e., 2 for constant generation and 6 for multiplication). Then, all the terms will be sum up using a linear activation function, and the number of weights is the same as the number of terms. As a result, the complexity of the corresponding neural network is $O(S^dd^{n-1})$.\n\nIn *Sec 4.2 of Revision-1*, we have added an example using Figure 2 to demonstrate how to map the form of f2 in Lemma 4.3 into the form of a network.\n\n**C8. Self-contained recap statement of Sec 3.2.**\n\nThanks for the suggestion. In *Sec 4.2 of Revision-1*, we have added the recap statement at the end of this section.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1517/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1517/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Universal Approximability and Complexity Bounds of Deep Learning in Hybrid Quantum-Classical Computing", "authorids": ["~Weiwen_Jiang1", "~Yukun_Ding1", "~Yiyu_Shi1"], "authors": ["Weiwen Jiang", "Yukun Ding", "Yiyu Shi"], "keywords": ["deep learning", "hybrid quantum-classical computing", "universal approximability"], "abstract": "With the continuously increasing number of quantum bits in quantum computers, there are growing interests in exploring applications that can harvest the power of them. Recently, several attempts were made to implement neural networks, known to be computationally intensive, in hybrid quantum-classical scheme computing. While encouraging results are shown, two fundamental questions need to be answered: (1) whether neural networks in hybrid quantum-classical computing can leverage quantum power and meanwhile approximate any function within a given error bound, i.e., universal approximability; (2) how do these neural networks compare with ones on a classical computer in terms of representation power? This work sheds light on these two questions from a theoretical perspective.", "one-sentence_summary": "This paper proves the universal approximability of neural networks on a quantum computer for a wide class of functions as well as the associated bounds. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|on_the_universal_approximability_and_complexity_bounds_of_deep_learning_in_hybrid_quantumclassical_computing", "pdf": "/pdf/884968f81a10499778d74b41376cb12924ab1239.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=J22qDO79rj", "_bibtex": "@misc{\njiang2021on,\ntitle={On the Universal Approximability and Complexity Bounds of Deep Learning in Hybrid Quantum-Classical Computing},\nauthor={Weiwen Jiang and Yukun Ding and Yiyu Shi},\nyear={2021},\nurl={https://openreview.net/forum?id=dnKsslWzLNY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "dnKsslWzLNY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1517/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1517/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1517/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1517/Authors|ICLR.cc/2021/Conference/Paper1517/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1517/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858787, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1517/-/Official_Comment"}}}, {"id": "N6aWe8XnCnJ", "original": null, "number": 6, "cdate": 1605327359166, "ddate": null, "tcdate": 1605327359166, "tmdate": 1605327397409, "tddate": null, "forum": "dnKsslWzLNY", "replyto": "DyMreCdsikF", "invitation": "ICLR.cc/2021/Conference/Paper1517/-/Official_Comment", "content": {"title": "Response to AnonReviewer2 (Part 1)", "comment": "We appreciate your valuable time to give us helpful comments. In Revision-1, we have carefully considered your comments and taken actions to clarify or address them. The detailed actions are reported as follows.\n\n**C1: Why the problem is important?**\n\nThe importance of the problem was initially discussed in Section 2 of the paper. In *Sec 2.2 of Revision-1*, we have added additional discussion on the motivation of this work. Specifically, although there are increasing interests in quantum machine learning (like google Tensorflow Quantum and many academic research efforts) and the recent quantum neural networks have demonstrated competitive accuracy against the classical neural networks with reduced time complexity, a fundamental question remains unknown: there are existing theories that show neural networks on classical computers have universal approximability, i.e., they can approximate a wide class of functions with arbitrarily small error, but do neural networks on quantum computers have the same universal approximability? This question has risen from the fact that the quantum circuit has limitations in computation operations to achieve high parallelism, such that classical neural networks cannot be directly mapped to quantum circuits. For example, quantum circuits do not allow common activation functions, like ReLU and Sigmod. This question is important since the failure to attain universal approximability will fundamentally hinder quantum neural networks from achieving state-of-the-art performance on various machine learning tasks, say image classification. \n\n**C2: Readability problem caused by the organization.**\n\nIn Revision-1, we have followed your suggestion to move the main results to *Section 3* to organize the whole paper in a top-down structure. Thanks for the suggestion. In addition, we have added examples and discussions to make the paper easy to follow.\n\n**C3: What is the hope of using quantum computers for NN? What is the efficiency of quantum neural networks for evaluation and training?**\n\nIn general, quantum computing has the potential to accelerate the neural networks for both evaluation and training with proper design, as has been shown in various literature already. But it is not the goal of this work. We have clarified in *Sec 2.2 and Sec 3 of Revision-1*. In this paper, we are trying to explore a more fundamental theoretical problem: due to the limitation in quantum computing scheme, can neural networks still have the universal approximability when they are running on quantum computers?  Following similar paths how the literature demonstrates the universal approximability of neural networks on classical computers, in this paper, we show that neural networks as function approximators can also achieve universal approximability on quantum computers. This provides theoretical support to continue the exploration of the implementation of neural networks on quantum computers.  \n\n**C4: Comparison of known bounds.**\n\nFirst, the primary goal of this paper is to prove the universal approximability of neural networks on quantum computers, a missing piece in the literature. Based on the constructed network, we further give the complexity bound analysis. Our conclusion is that neural networks on quantum computers can achieve a lower depth upper bound and smaller circuit size compared with those on classical computers. But it is in big O notation, so the actual reduction in depth and circuit size is not bounded by polylog(1/\u03f5) \u2013 it can be much higher as has been empirically shown in the literature for various real implementations.  Second, these bounds used in the comparison are recently developed for neural networks on classical computers. We are not aware of better bounds in the literature. \n\n**C5: Question on Equation 1.**\n\nWe have clarified in *Sec 4.1 of Revision-1* that the network requires that the square of the sum of all inputs to be normalized to 1, which is the requirement by using quantum states for computation. As a result, y will be in [-1, 1] after an nth-order polynomial function of sigma, where $n\\ge 1$.\n\n**C6: Question on Equation 2.**\n\nIn *Sec 4.2 of Revision-1*, we have clarified that the notation $D^{n}f(x)$ means the weak derivative (not strong derivative) of up to order n. Therefore, the function does not need to be differentiable. We have removed the confusing statement of $n=1$.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1517/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1517/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Universal Approximability and Complexity Bounds of Deep Learning in Hybrid Quantum-Classical Computing", "authorids": ["~Weiwen_Jiang1", "~Yukun_Ding1", "~Yiyu_Shi1"], "authors": ["Weiwen Jiang", "Yukun Ding", "Yiyu Shi"], "keywords": ["deep learning", "hybrid quantum-classical computing", "universal approximability"], "abstract": "With the continuously increasing number of quantum bits in quantum computers, there are growing interests in exploring applications that can harvest the power of them. Recently, several attempts were made to implement neural networks, known to be computationally intensive, in hybrid quantum-classical scheme computing. While encouraging results are shown, two fundamental questions need to be answered: (1) whether neural networks in hybrid quantum-classical computing can leverage quantum power and meanwhile approximate any function within a given error bound, i.e., universal approximability; (2) how do these neural networks compare with ones on a classical computer in terms of representation power? This work sheds light on these two questions from a theoretical perspective.", "one-sentence_summary": "This paper proves the universal approximability of neural networks on a quantum computer for a wide class of functions as well as the associated bounds. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|on_the_universal_approximability_and_complexity_bounds_of_deep_learning_in_hybrid_quantumclassical_computing", "pdf": "/pdf/884968f81a10499778d74b41376cb12924ab1239.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=J22qDO79rj", "_bibtex": "@misc{\njiang2021on,\ntitle={On the Universal Approximability and Complexity Bounds of Deep Learning in Hybrid Quantum-Classical Computing},\nauthor={Weiwen Jiang and Yukun Ding and Yiyu Shi},\nyear={2021},\nurl={https://openreview.net/forum?id=dnKsslWzLNY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "dnKsslWzLNY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1517/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1517/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1517/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1517/Authors|ICLR.cc/2021/Conference/Paper1517/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1517/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858787, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1517/-/Official_Comment"}}}, {"id": "eQVq6oD4lBl", "original": null, "number": 1, "cdate": 1603876187193, "ddate": null, "tcdate": 1603876187193, "tmdate": 1605024423861, "tddate": null, "forum": "dnKsslWzLNY", "replyto": "dnKsslWzLNY", "invitation": "ICLR.cc/2021/Conference/Paper1517/-/Official_Review", "content": {"title": "On the Universal Approximability and Complexity Bounds of Deep Learning in Hybrid Quantum-Classical Computing", "review": "The problem studied in this work is of interest in the quantum machine learning community, as the power of small and noisy quantum computers for machine learning problems is far from being understood. Therefore, it is important to study the expressivity of quantum neural networks as function approximators. This work uses the model introduced by Tensorflow Quantum, where different neurons can be implemented on either quantum or classical computers.\n\nHowever, it is unclear how this result applies to current topologies of QNN/variational circuits used in current literature. From my knowledge of quantum variational circuits, the architecture proposed is different. To my understanding, this work addresses specifically the model proposed for TensorFlow quantum, and this should probably be made more explicit, as it's somehow different from current literature, where quantum neural network architectures are the sole computational node, and no classical computation is performed classically (besides the optimization of the parameters of the variational circuit). The model described in this work is called the \"prologue, acceleration, epilogue\".\n\nIf I understood the work properly, the role of quantum computers is to evaluate on a quantum computer the \"binary\" part of a Binary Polynomial Neural Network (this is what the authors call the acceleration phase), after in a prologue part the data is loaded as initial quantum state with a log-depth circuit.Then, in the epilogue phase, a nonlinearity, like a a ReLU function is applied classically,\n \nIt is really interesting the comparison with the approximation function of neural networks in classical computers. Perhaps more (recent) literature review on quantum expressiveness of other variational circuits can be added.\n \nI checked some of the proofs in the manuscript, and they are correct. The paper is nicely written, but perhaps might benefit some more clarity, especially in the proofs in the appendix.\n\nOverall, the submission would have benefited from experiments, showing that a QNN built with the architecture proposed in this work can achieve high accuracy in classification/regression tasks. This can be either done on small quantum computers, or even simulated in GPUs or large classical computers. Also, it would have been beneficial to write more clearly section 3.3, perhaps with an example, on how the classical optimizer is meant to choose the parameters of this circuit in a machine learning problem, i.e. how this architecture is meant to be used in practice.\n\nOther remarks are the following:\n- Please use a consistent notation for multiplication. If my understanding is correct, In proposition 3.2 and the subsequent lines, you use the notation $x \\times y$, $xy$, and $x \\dot y$ to denote the same operation.\n\n-  In section 4.1 it's not clear to me what this sentence means:\n At the end of these operations, both zero states $\\ket{0}$ in $Q_1$ and $Q_2$ are y, and the $\\ket{0..}$ state in the combination of these two systems, $Q_{1,2}$ will be $y^2$. How can the zero register be $y^2$?\n \n-  What is the $p$ in the Discussion section, when discussing the result of Yarotsky?\n\n- I think in some parts of the paper the authors use $n$ for the number of qubits, and then $\\log n$.\n\n- Figure 1 could be split into two figures (left and right), and the notation in the figure could be better explained as the figure is referenced many times in the paper.\n\n- Is written in the section where the  prologue phase is described \"As pointed by Bravo-Prietoet al. (2020), unitary matrix A can be decomposed to the quantum circuit with gate complexity ofO(logn), where logn is the number of qbits.\" This is only true if the matrix $A$ is of the kind specified before, i.e. it's just one single column.\n\n- The fact that the depth of BPNN in hybrid quantum-classical computing can be of $O(1)$ is a strong result that perhaps should be compared more with the literature on the power of quantum shallow circuits or constant depth circuits.  \n\nAlso, I think the work should conform to the widespread and standard notation of using qubits and not qbits.\n\n Some typos:\n - Proof Proposition 4.2. \"the Of\" should be \"the of\".\n - After lemma 3.3 \"which is introduced in the following texts\" -> which is introduced next (or in the following section).\n - \"To take the advantages of high-parallel in quantum computing, we made an observation on the network structure of BPNN as described in the following Property.\" Might be improved. It might be changed into \"high-parallelism\" and rephrased the whole sentence.\n -Section 4.2\n \"d input variables and f has weak derivative\" should be derivativeS.\n - . This brings flexibility in implementing functions (e.g., ReLU), while at the same calls for interface for massive data transfer between quantum and classical computers. Perhaps you wanted to say \"at the same time calls for fast interfaces\"", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1517/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1517/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Universal Approximability and Complexity Bounds of Deep Learning in Hybrid Quantum-Classical Computing", "authorids": ["~Weiwen_Jiang1", "~Yukun_Ding1", "~Yiyu_Shi1"], "authors": ["Weiwen Jiang", "Yukun Ding", "Yiyu Shi"], "keywords": ["deep learning", "hybrid quantum-classical computing", "universal approximability"], "abstract": "With the continuously increasing number of quantum bits in quantum computers, there are growing interests in exploring applications that can harvest the power of them. Recently, several attempts were made to implement neural networks, known to be computationally intensive, in hybrid quantum-classical scheme computing. While encouraging results are shown, two fundamental questions need to be answered: (1) whether neural networks in hybrid quantum-classical computing can leverage quantum power and meanwhile approximate any function within a given error bound, i.e., universal approximability; (2) how do these neural networks compare with ones on a classical computer in terms of representation power? This work sheds light on these two questions from a theoretical perspective.", "one-sentence_summary": "This paper proves the universal approximability of neural networks on a quantum computer for a wide class of functions as well as the associated bounds. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|on_the_universal_approximability_and_complexity_bounds_of_deep_learning_in_hybrid_quantumclassical_computing", "pdf": "/pdf/884968f81a10499778d74b41376cb12924ab1239.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=J22qDO79rj", "_bibtex": "@misc{\njiang2021on,\ntitle={On the Universal Approximability and Complexity Bounds of Deep Learning in Hybrid Quantum-Classical Computing},\nauthor={Weiwen Jiang and Yukun Ding and Yiyu Shi},\nyear={2021},\nurl={https://openreview.net/forum?id=dnKsslWzLNY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "dnKsslWzLNY", "replyto": "dnKsslWzLNY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1517/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538116806, "tmdate": 1606915802319, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1517/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1517/-/Official_Review"}}}, {"id": "IPcJ3lSTNE", "original": null, "number": 2, "cdate": 1603950227276, "ddate": null, "tcdate": 1603950227276, "tmdate": 1605024423795, "tddate": null, "forum": "dnKsslWzLNY", "replyto": "dnKsslWzLNY", "invitation": "ICLR.cc/2021/Conference/Paper1517/-/Official_Review", "content": {"title": "Universal approximation theorem for parameterized quantum circuits", "review": "The paper considers the expressivity and approximation properties of machine learning models where a parameterized quantum circuit is used to 'accelerate' a classical neural network. The results consider a model with a date encoder, a quantum circuit, and then a classical feedforward neural net for post-processing. To make the results non-trivial only models with asymptotically similar classical and quantum complexity are considered.\n\nUsing a technique best on Taylor polynomial approximations, the paper finds that a large class of smooth functions can be approximated using O(log(1/\\epsilon)^(n/d)) quantum gates, qubits,  and classical width. This is slightly (logarithmic factors of  1/\\epsilon) smaller than the best known upper bounds with ReLU or unconstrained quadratic networks.\n\nPros:\n\nThe paper shows how some techniques used to obtain classical approximation theorems can be extended to quantum gates. The slight improvement from the addition of a quantum element raises the possibility of showing a stronger separation in the future.\n\nCons:\nThe results in their current form do not really show any benefit (from the point of view of approximation) for quantum neural networks, since the bounds are only logarithmicallly better than classical bounds. \n\nThe technique used seems to be to use quantum units to mimic subfunctions in a manner quite similar to classical units, and does not provide much insight into why we should expect a 'quantum improvement' here.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1517/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1517/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Universal Approximability and Complexity Bounds of Deep Learning in Hybrid Quantum-Classical Computing", "authorids": ["~Weiwen_Jiang1", "~Yukun_Ding1", "~Yiyu_Shi1"], "authors": ["Weiwen Jiang", "Yukun Ding", "Yiyu Shi"], "keywords": ["deep learning", "hybrid quantum-classical computing", "universal approximability"], "abstract": "With the continuously increasing number of quantum bits in quantum computers, there are growing interests in exploring applications that can harvest the power of them. Recently, several attempts were made to implement neural networks, known to be computationally intensive, in hybrid quantum-classical scheme computing. While encouraging results are shown, two fundamental questions need to be answered: (1) whether neural networks in hybrid quantum-classical computing can leverage quantum power and meanwhile approximate any function within a given error bound, i.e., universal approximability; (2) how do these neural networks compare with ones on a classical computer in terms of representation power? This work sheds light on these two questions from a theoretical perspective.", "one-sentence_summary": "This paper proves the universal approximability of neural networks on a quantum computer for a wide class of functions as well as the associated bounds. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jiang|on_the_universal_approximability_and_complexity_bounds_of_deep_learning_in_hybrid_quantumclassical_computing", "pdf": "/pdf/884968f81a10499778d74b41376cb12924ab1239.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=J22qDO79rj", "_bibtex": "@misc{\njiang2021on,\ntitle={On the Universal Approximability and Complexity Bounds of Deep Learning in Hybrid Quantum-Classical Computing},\nauthor={Weiwen Jiang and Yukun Ding and Yiyu Shi},\nyear={2021},\nurl={https://openreview.net/forum?id=dnKsslWzLNY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "dnKsslWzLNY", "replyto": "dnKsslWzLNY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1517/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538116806, "tmdate": 1606915802319, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1517/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1517/-/Official_Review"}}}], "count": 10}