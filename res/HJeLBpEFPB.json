{"notes": [{"id": "HJeLBpEFPB", "original": "HJeCtQIwPH", "number": 523, "cdate": 1569439037525, "ddate": null, "tcdate": 1569439037525, "tmdate": 1577168281812, "tddate": null, "forum": "HJeLBpEFPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Unsupervised Universal Self-Attention Network for Graph Classification", "authors": ["Dai Quoc Nguyen", "Tu Dinh Nguyen", "Dinh Phung"], "authorids": ["dai.nguyen@monash.edu", "tu.dinh.nguyen@monash.edu", "dinh.phung@monash.edu"], "keywords": ["Graph embedding", "graph classification", "universal self-attention network", "graph neural network"], "abstract": "Existing graph embedding models often have weaknesses in exploiting graph structure similarities, potential dependencies among nodes and global network properties. To this end, we present U2GAN, a novel unsupervised model leveraging on the strength of the recently introduced universal self-attention network (Dehghani et al., 2019), to learn low-dimensional embeddings of graphs which can be used for graph classification. In particular, given an input graph, U2GAN first applies a self-attention computation, which is then followed by a recurrent transition to iteratively memorize its attention on vector representations of each node and its neighbors across each iteration. Thus, U2GAN can address the weaknesses in the existing models in order to produce plausible node embeddings whose sum is the final embedding of the whole graph. Experimental results show that our unsupervised U2GAN produces new state-of-the-art performances on a range of well-known benchmark datasets for the graph classification task. It even outperforms supervised methods in most of benchmark cases.", "pdf": "/pdf/890c4ec4c1c8106153bfa0fca28406f98ef8184d.pdf", "paperhash": "nguyen|unsupervised_universal_selfattention_network_for_graph_classification", "original_pdf": "/attachment/949c4e9cde5186acf1c78f9cab891fd75caee3df.pdf", "_bibtex": "@misc{\nnguyen2020unsupervised,\ntitle={Unsupervised Universal Self-Attention Network for Graph Classification},\nauthor={Dai Quoc Nguyen and Tu Dinh Nguyen and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeLBpEFPB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 21, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "5oh8LNTeE", "original": null, "number": 1, "cdate": 1576798698839, "ddate": null, "tcdate": 1576798698839, "tmdate": 1576800936981, "tddate": null, "forum": "HJeLBpEFPB", "replyto": "HJeLBpEFPB", "invitation": "ICLR.cc/2020/Conference/Paper523/-/Decision", "content": {"decision": "Reject", "comment": "All three reviewers are consistently negative on this paper. Thus a reject is recommended.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Universal Self-Attention Network for Graph Classification", "authors": ["Dai Quoc Nguyen", "Tu Dinh Nguyen", "Dinh Phung"], "authorids": ["dai.nguyen@monash.edu", "tu.dinh.nguyen@monash.edu", "dinh.phung@monash.edu"], "keywords": ["Graph embedding", "graph classification", "universal self-attention network", "graph neural network"], "abstract": "Existing graph embedding models often have weaknesses in exploiting graph structure similarities, potential dependencies among nodes and global network properties. To this end, we present U2GAN, a novel unsupervised model leveraging on the strength of the recently introduced universal self-attention network (Dehghani et al., 2019), to learn low-dimensional embeddings of graphs which can be used for graph classification. In particular, given an input graph, U2GAN first applies a self-attention computation, which is then followed by a recurrent transition to iteratively memorize its attention on vector representations of each node and its neighbors across each iteration. Thus, U2GAN can address the weaknesses in the existing models in order to produce plausible node embeddings whose sum is the final embedding of the whole graph. Experimental results show that our unsupervised U2GAN produces new state-of-the-art performances on a range of well-known benchmark datasets for the graph classification task. It even outperforms supervised methods in most of benchmark cases.", "pdf": "/pdf/890c4ec4c1c8106153bfa0fca28406f98ef8184d.pdf", "paperhash": "nguyen|unsupervised_universal_selfattention_network_for_graph_classification", "original_pdf": "/attachment/949c4e9cde5186acf1c78f9cab891fd75caee3df.pdf", "_bibtex": "@misc{\nnguyen2020unsupervised,\ntitle={Unsupervised Universal Self-Attention Network for Graph Classification},\nauthor={Dai Quoc Nguyen and Tu Dinh Nguyen and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeLBpEFPB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJeLBpEFPB", "replyto": "HJeLBpEFPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795712223, "tmdate": 1576800261567, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper523/-/Decision"}}}, {"id": "r1xOur17iS", "original": null, "number": 17, "cdate": 1573217648386, "ddate": null, "tcdate": 1573217648386, "tmdate": 1573857459507, "tddate": null, "forum": "HJeLBpEFPB", "replyto": "BkltXbVRKB", "invitation": "ICLR.cc/2020/Conference/Paper523/-/Official_Comment", "content": {"title": "Responses to your comments", "comment": "Thank you for your comments.\n\nFor some clarifications:\n\n1.\tGAT borrows the standard attention from [1] in using a single-layer feedforward neural network parametrized by a weight vector and then applying the non-linear function followed by the softmax function to compute importance weights of neighbors for a given node. Therefore, GAT is much different with the self-attention mechanism. You can see more details about GAT in Appendix A of our revision.\n\n[1] Neural machine translation by jointly learning to align and translate. ICLR 2015.\n\n2.\tRegarding the unsupervised fashion, we aim to maximize the similarity between h^T_v and the node embedding o_v of a given node v, and also to minimize the similarity between h^T_v and the embeddings of \"negative nodes\". In addition, h^T_v may depend on sampling the neighbors of node v. Hence, after training, we choose to use o_v as the final representation of node v.\n\nRegarding the supervised fashion where we do not need to learn the node embeddings \"separately\", we can use h^T_v as the final representation of node v (i.e., o_v = h^T_v) in order to produce the vector representation o_G.\n\n3.\tWe shared our code and running command scripts to make sure: (i) you can verify that our implementation is correct, and (ii) you can reproduce our experimental results. \n\nAs mentioned in our revision, our \"supervised\" model produces new state-of-the-art accuracies on DD, IMDBBINARY, IMDBMULTI, PROTEINS and PTC; and obtains competitive accuracies on remaining datasets.\n\n4.\tTo the best of our knowledge, our work is the first to show that a unsupervised model can noticeably outperform most of up-to-date supervised approaches by a large margin. This is important in both industry and academic applications in reality where expanding unsupervised GNN models is more suitable due to the limited availability of class labels.\n\nWe are looking forward to your new updated comments on our revised submission.\n\nThanks."}, "signatures": ["ICLR.cc/2020/Conference/Paper523/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Universal Self-Attention Network for Graph Classification", "authors": ["Dai Quoc Nguyen", "Tu Dinh Nguyen", "Dinh Phung"], "authorids": ["dai.nguyen@monash.edu", "tu.dinh.nguyen@monash.edu", "dinh.phung@monash.edu"], "keywords": ["Graph embedding", "graph classification", "universal self-attention network", "graph neural network"], "abstract": "Existing graph embedding models often have weaknesses in exploiting graph structure similarities, potential dependencies among nodes and global network properties. To this end, we present U2GAN, a novel unsupervised model leveraging on the strength of the recently introduced universal self-attention network (Dehghani et al., 2019), to learn low-dimensional embeddings of graphs which can be used for graph classification. In particular, given an input graph, U2GAN first applies a self-attention computation, which is then followed by a recurrent transition to iteratively memorize its attention on vector representations of each node and its neighbors across each iteration. Thus, U2GAN can address the weaknesses in the existing models in order to produce plausible node embeddings whose sum is the final embedding of the whole graph. Experimental results show that our unsupervised U2GAN produces new state-of-the-art performances on a range of well-known benchmark datasets for the graph classification task. It even outperforms supervised methods in most of benchmark cases.", "pdf": "/pdf/890c4ec4c1c8106153bfa0fca28406f98ef8184d.pdf", "paperhash": "nguyen|unsupervised_universal_selfattention_network_for_graph_classification", "original_pdf": "/attachment/949c4e9cde5186acf1c78f9cab891fd75caee3df.pdf", "_bibtex": "@misc{\nnguyen2020unsupervised,\ntitle={Unsupervised Universal Self-Attention Network for Graph Classification},\nauthor={Dai Quoc Nguyen and Tu Dinh Nguyen and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeLBpEFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeLBpEFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper523/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper523/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper523/Authors|ICLR.cc/2020/Conference/Paper523/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170173, "tmdate": 1576860534730, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper523/-/Official_Comment"}}}, {"id": "rkgHD6AfoB", "original": null, "number": 15, "cdate": 1573215580806, "ddate": null, "tcdate": 1573215580806, "tmdate": 1573792946152, "tddate": null, "forum": "HJeLBpEFPB", "replyto": "Hyxp_0n1jH", "invitation": "ICLR.cc/2020/Conference/Paper523/-/Official_Comment", "content": {"title": "Update our revised submission, experimental results in the supervised fashion", "comment": "Dear the Reviewers, the Area Chairs and the Program Chairs,\n\nWe updated our revised submission, in which we included our experimental results in the supervised fashion. As mentioned in our revision, our \"supervised\" model produces new state-of-the-art accuracies on DD, IMDBBINARY, IMDBMULTI, PROTEINS and PTC; and obtains competitive accuracies on remaining datasets.\nWe are training and tuning our model hyper-parameters due to a limitation of time and computing resources, hence the current \"supervised\" results is not final.\n\nOur proposed model now works in both the supervised and unsupervised fashions, thus we changed our paper title to \"A UNIVERSAL SELF-ATTENTION GRAPH NEURAL NETWORK\".\n\nTo the best of our knowledge, our work is the first to show that a unsupervised model can noticeably outperform up-to-date supervised approaches by a large margin.  Therefore, we suggest that future GNN works should pay more attention to the unsupervised fashion as well as not comparing supervised models with unsupervised models together. \nThis is important in both industry and academic applications in reality where expanding unsupervised GNN models is more suitable due to the limited availability of class labels.\n\nWe are looking forward to new comments from the the Reviewers as well as the Area Chairs on our revised submission.\n\nThank you very much."}, "signatures": ["ICLR.cc/2020/Conference/Paper523/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Universal Self-Attention Network for Graph Classification", "authors": ["Dai Quoc Nguyen", "Tu Dinh Nguyen", "Dinh Phung"], "authorids": ["dai.nguyen@monash.edu", "tu.dinh.nguyen@monash.edu", "dinh.phung@monash.edu"], "keywords": ["Graph embedding", "graph classification", "universal self-attention network", "graph neural network"], "abstract": "Existing graph embedding models often have weaknesses in exploiting graph structure similarities, potential dependencies among nodes and global network properties. To this end, we present U2GAN, a novel unsupervised model leveraging on the strength of the recently introduced universal self-attention network (Dehghani et al., 2019), to learn low-dimensional embeddings of graphs which can be used for graph classification. In particular, given an input graph, U2GAN first applies a self-attention computation, which is then followed by a recurrent transition to iteratively memorize its attention on vector representations of each node and its neighbors across each iteration. Thus, U2GAN can address the weaknesses in the existing models in order to produce plausible node embeddings whose sum is the final embedding of the whole graph. Experimental results show that our unsupervised U2GAN produces new state-of-the-art performances on a range of well-known benchmark datasets for the graph classification task. It even outperforms supervised methods in most of benchmark cases.", "pdf": "/pdf/890c4ec4c1c8106153bfa0fca28406f98ef8184d.pdf", "paperhash": "nguyen|unsupervised_universal_selfattention_network_for_graph_classification", "original_pdf": "/attachment/949c4e9cde5186acf1c78f9cab891fd75caee3df.pdf", "_bibtex": "@misc{\nnguyen2020unsupervised,\ntitle={Unsupervised Universal Self-Attention Network for Graph Classification},\nauthor={Dai Quoc Nguyen and Tu Dinh Nguyen and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeLBpEFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeLBpEFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper523/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper523/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper523/Authors|ICLR.cc/2020/Conference/Paper523/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170173, "tmdate": 1576860534730, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper523/-/Official_Comment"}}}, {"id": "rkx44w1XoB", "original": null, "number": 18, "cdate": 1573218091896, "ddate": null, "tcdate": 1573218091896, "tmdate": 1573780376806, "tddate": null, "forum": "HJeLBpEFPB", "replyto": "HyxQAQLatr", "invitation": "ICLR.cc/2020/Conference/Paper523/-/Official_Comment", "content": {"title": "Responses to your comments", "comment": "Thank you for your comments.\n\nFor some clarifications:\n\n1.\tWe find that the the propagating phase in the GNN approaches (e.g. GraphSAGE, GCN, GIN) is not advanced enough to be able to determine latent potential relationships among nodes. You can see the the propagating phase of GCN, GraphSAGE and GAT in Appendix A. \n\n2.\tGAT borrows the standard attention from [1] in using a single-layer feedforward neural network parametrized by a weight vector and then applying the non-linear function followed by the softmax function to compute importance weights of neighbors for a given node. Therefore, GAT is much different with the self-attention mechanism. You can see more details about GAT in Appendix A of our revision.\n\n[1] Neural machine translation by jointly learning to align and translate. ICLR 2015.\n\n3.\t Xu et. al. [2] showed that the sum pooling performs better than the mean and max poolings. Finally, our current results with the sum pooling are very promising.\n[2] How Powerful are Graph Neural Networks? Xu et. al., ICLR 2019.\n\n4.        To the best of our knowledge, our work is the first to show that a unsupervised model can noticeably outperform most of up-to-date supervised approaches by a large margin. This is important in both industry and academic applications in reality where expanding unsupervised GNN models is more suitable due to the limited availability of class labels.\n\nWe plan to investigate the effectiveness of our model on other important tasks such as node classification and link prediction in the future work.\n\n5.       As mentioned in our revision, our \"supervised\" model produces new state-of-the-art accuracies on DD, IMDBBINARY, IMDBMULTI, PROTEINS and PTC; and obtains competitive accuracies on remaining datasets.\n\n6.\tWe included the results of GraphSAGE in our revision.  Existing works do not include DeepWalk or Node2Vec as baselines for the downstream task of graph classification. \n\nWe do not have experimental results for GAT. But Shchur et. al. [3] showed that GCN outperforms GAT in different split settings.\n\n[3] Pitfalls of Graph Neural Network Evaluation. Shchur et. al. 2018.\n\nWe did not include the embedding visualization of other baselines because those methods are very different from our unsupervised model.\n\nWe are looking forward to your new updated comments on our revised submission.\n\nThanks."}, "signatures": ["ICLR.cc/2020/Conference/Paper523/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Universal Self-Attention Network for Graph Classification", "authors": ["Dai Quoc Nguyen", "Tu Dinh Nguyen", "Dinh Phung"], "authorids": ["dai.nguyen@monash.edu", "tu.dinh.nguyen@monash.edu", "dinh.phung@monash.edu"], "keywords": ["Graph embedding", "graph classification", "universal self-attention network", "graph neural network"], "abstract": "Existing graph embedding models often have weaknesses in exploiting graph structure similarities, potential dependencies among nodes and global network properties. To this end, we present U2GAN, a novel unsupervised model leveraging on the strength of the recently introduced universal self-attention network (Dehghani et al., 2019), to learn low-dimensional embeddings of graphs which can be used for graph classification. In particular, given an input graph, U2GAN first applies a self-attention computation, which is then followed by a recurrent transition to iteratively memorize its attention on vector representations of each node and its neighbors across each iteration. Thus, U2GAN can address the weaknesses in the existing models in order to produce plausible node embeddings whose sum is the final embedding of the whole graph. Experimental results show that our unsupervised U2GAN produces new state-of-the-art performances on a range of well-known benchmark datasets for the graph classification task. It even outperforms supervised methods in most of benchmark cases.", "pdf": "/pdf/890c4ec4c1c8106153bfa0fca28406f98ef8184d.pdf", "paperhash": "nguyen|unsupervised_universal_selfattention_network_for_graph_classification", "original_pdf": "/attachment/949c4e9cde5186acf1c78f9cab891fd75caee3df.pdf", "_bibtex": "@misc{\nnguyen2020unsupervised,\ntitle={Unsupervised Universal Self-Attention Network for Graph Classification},\nauthor={Dai Quoc Nguyen and Tu Dinh Nguyen and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeLBpEFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeLBpEFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper523/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper523/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper523/Authors|ICLR.cc/2020/Conference/Paper523/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170173, "tmdate": 1576860534730, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper523/-/Official_Comment"}}}, {"id": "S1gYNy1QiS", "original": null, "number": 16, "cdate": 1573216048628, "ddate": null, "tcdate": 1573216048628, "tmdate": 1573776318174, "tddate": null, "forum": "HJeLBpEFPB", "replyto": "Hkl1LHjCYS", "invitation": "ICLR.cc/2020/Conference/Paper523/-/Official_Comment", "content": {"title": "Responses to your comments", "comment": "Thank you for your comments.\n\nFor some clarifications:\n\n1.\tAs described after Equation 6 in the manuscript, at the final step (the T-th step), each node v can have multiple vectors as you mentioned; and we consider to only use the vector (at the center) at the final step to infer the embedding of the node v.\n\n2.\tWe\u2019ve discussed with Borsi that the graph classification task and \u201cNot using test graphs\u201d do not means we are testing inductive setting like in the node classification task. We suggested Boris to point a paper which describes an inductive setting for the graph classification task as reference, but got no response for this.\n\n3.\tTo have better comprehensive experiments, we have implemented our model in the supervised fashion, and posted the results on October 28. In the revised manuscript, we show that our supervised model produces new state-of-the-art accuracies on DD, IMDBBINARY, IMDBMULTI, PROTEINS and PTC; and obtains competitive accuracies on remaining datasets.\n\nWe have also shared our code to make sure you can verify and reproduce our results.\n\n4.\tIn the layer-based GNN architecture like GCN/GAT, we have to construct (k+1) layers (i.e., multiple layers stacked on top of each other) to reach to k-hops neighbors of a given node.\n\nIn our proposed model, T is not equal to the number of GCN/GAT layers. Not only T=1, but also for any value of T, we update the vector representation of each node v by recursively propagating the representations of its neighbors. This is reason why we do not need to implement a layer-based GNN architecture like GCN/GAT. \n\nWe are looking forward to your new updated comments on our revised submission.\n\nThanks."}, "signatures": ["ICLR.cc/2020/Conference/Paper523/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Universal Self-Attention Network for Graph Classification", "authors": ["Dai Quoc Nguyen", "Tu Dinh Nguyen", "Dinh Phung"], "authorids": ["dai.nguyen@monash.edu", "tu.dinh.nguyen@monash.edu", "dinh.phung@monash.edu"], "keywords": ["Graph embedding", "graph classification", "universal self-attention network", "graph neural network"], "abstract": "Existing graph embedding models often have weaknesses in exploiting graph structure similarities, potential dependencies among nodes and global network properties. To this end, we present U2GAN, a novel unsupervised model leveraging on the strength of the recently introduced universal self-attention network (Dehghani et al., 2019), to learn low-dimensional embeddings of graphs which can be used for graph classification. In particular, given an input graph, U2GAN first applies a self-attention computation, which is then followed by a recurrent transition to iteratively memorize its attention on vector representations of each node and its neighbors across each iteration. Thus, U2GAN can address the weaknesses in the existing models in order to produce plausible node embeddings whose sum is the final embedding of the whole graph. Experimental results show that our unsupervised U2GAN produces new state-of-the-art performances on a range of well-known benchmark datasets for the graph classification task. It even outperforms supervised methods in most of benchmark cases.", "pdf": "/pdf/890c4ec4c1c8106153bfa0fca28406f98ef8184d.pdf", "paperhash": "nguyen|unsupervised_universal_selfattention_network_for_graph_classification", "original_pdf": "/attachment/949c4e9cde5186acf1c78f9cab891fd75caee3df.pdf", "_bibtex": "@misc{\nnguyen2020unsupervised,\ntitle={Unsupervised Universal Self-Attention Network for Graph Classification},\nauthor={Dai Quoc Nguyen and Tu Dinh Nguyen and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeLBpEFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeLBpEFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper523/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper523/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper523/Authors|ICLR.cc/2020/Conference/Paper523/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170173, "tmdate": 1576860534730, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper523/-/Official_Comment"}}}, {"id": "Hyxp_0n1jH", "original": null, "number": 14, "cdate": 1573011060910, "ddate": null, "tcdate": 1573011060910, "tmdate": 1573144193056, "tddate": null, "forum": "HJeLBpEFPB", "replyto": "HJeLBpEFPB", "invitation": "ICLR.cc/2020/Conference/Paper523/-/Official_Comment", "content": {"title": "Reviewers might have had influenced by Boris Knyazev's comment, completely ignored our contribution", "comment": "Dear the Area Chairs and the Program Chairs,\n\nWe share our code and running command scripts to make sure that our implementation is correct and reviewers can reproduce our experimental results.  We have followed several existing works in comparing supervised models with unsupervised models due to the use of the same 10-fold cross-validation scheme.\n\nHowever, we feel that reviewers (especially AnonReviewer2 on \"too good to be true\") might have had influenced by Boris Knyazev's comment on the correctness of evaluation, and hence they might have had completely ignored our contribution where, to our best of knowledge, our work is the first to show that a unsupervised model can noticeably outperform most of up-to-date supervised approaches by a large margin. \nAs a consequence, we thought we might not be allowed to compare the supervised models with our unsupervised model together, hence we posted a new comment on October 28 about \"Update our U2GAN in a \"supervised\" fashion\".\n\nWe note that our most important contribution now is to suggest that future GNN works should pay more attention to the unsupervised fashion. This is important in both industry and academic applications in reality where expanding unsupervised GNN models is more suitable due to a limitation of available class labels. \n\nWe are updating our paper including the experimental results of our supervised fashion for our rebuttal. Recently, our \"supervised\" fashion produces new state-of-the-art accuracies on DD, IMDBBINARY, IMDBMULTI, PROTEINS and PTC ; and obtains a competitive accuracies on MUTAG.\n\nWe hope that the Area Chairs and the Program Chairs can guide reviewers to have constructive discussions with us in the rebuttal process. In addition, it is very grateful if the Area Chairs can find the 4-th reviewer who can verify our implementation and is firstly not biased by Boris Knyazev's comment.\n\nThank you very much."}, "signatures": ["ICLR.cc/2020/Conference/Paper523/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Universal Self-Attention Network for Graph Classification", "authors": ["Dai Quoc Nguyen", "Tu Dinh Nguyen", "Dinh Phung"], "authorids": ["dai.nguyen@monash.edu", "tu.dinh.nguyen@monash.edu", "dinh.phung@monash.edu"], "keywords": ["Graph embedding", "graph classification", "universal self-attention network", "graph neural network"], "abstract": "Existing graph embedding models often have weaknesses in exploiting graph structure similarities, potential dependencies among nodes and global network properties. To this end, we present U2GAN, a novel unsupervised model leveraging on the strength of the recently introduced universal self-attention network (Dehghani et al., 2019), to learn low-dimensional embeddings of graphs which can be used for graph classification. In particular, given an input graph, U2GAN first applies a self-attention computation, which is then followed by a recurrent transition to iteratively memorize its attention on vector representations of each node and its neighbors across each iteration. Thus, U2GAN can address the weaknesses in the existing models in order to produce plausible node embeddings whose sum is the final embedding of the whole graph. Experimental results show that our unsupervised U2GAN produces new state-of-the-art performances on a range of well-known benchmark datasets for the graph classification task. It even outperforms supervised methods in most of benchmark cases.", "pdf": "/pdf/890c4ec4c1c8106153bfa0fca28406f98ef8184d.pdf", "paperhash": "nguyen|unsupervised_universal_selfattention_network_for_graph_classification", "original_pdf": "/attachment/949c4e9cde5186acf1c78f9cab891fd75caee3df.pdf", "_bibtex": "@misc{\nnguyen2020unsupervised,\ntitle={Unsupervised Universal Self-Attention Network for Graph Classification},\nauthor={Dai Quoc Nguyen and Tu Dinh Nguyen and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeLBpEFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeLBpEFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper523/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper523/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper523/Authors|ICLR.cc/2020/Conference/Paper523/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170173, "tmdate": 1576860534730, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper523/-/Official_Comment"}}}, {"id": "HyxQAQLatr", "original": null, "number": 1, "cdate": 1571804106723, "ddate": null, "tcdate": 1571804106723, "tmdate": 1572972584810, "tddate": null, "forum": "HJeLBpEFPB", "replyto": "HJeLBpEFPB", "invitation": "ICLR.cc/2020/Conference/Paper523/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "In this paper, the authors developed a graph embedding method called U2GAN based on self-attention mechanism. Similar to many existing graph neural network, U2GAN samples and aggregate neighboring features for each node in a graph. The aggregation function is similar to GAT, i.e., a query based attention layer. The difference is an incorporation of a transition function followed the attention layer. By minimizing Eq. 7, node embeddings can be inferred, which are summed up to obtain a graph embedding, for the downstream graph classification task.\n\nThere are several points that are unclear in the paper.\n1.  The major argument of the advantage of using self-attention for neighborhood aggregation is to facilitate memorizing the dependencies between nodes and explore the graph structure similarities locally and globally. This argument, however, was not clearly discussed in the paper. First, it is not clear on why existing GNN methods, such as GCN, GraphSage, and GAT, cannot do so. Second, it is not clear on how does the proposed U2GAN achieve it. The current paper only provides some high-level descriptions. A more specific or theoretical discussion is desired.\n2. Since the attention based aggregation is similar to GAT, a discussion on the difference is important.\n3. Several model designs are not well justified. In Eq. 2, 3, the reason to employ Layernorm is missing. In Eq. 7, the intuition on how does the loss function help learn effective embeddings remains to be clarified. Also, it may be better to evaluate different pooling method to obtain graph embedding to justify the choice of sum in Eq. 1.\n4. Since the proposed method aims to learn node embeddings in an unsupervised manner, it is better to see some descriptions on why graph classification was selected as the task in evaluation, instead of node classification.\n5. In the experiments, some methods such as deepwalk, node2vec, graphsage and GAT are missing in comparison. In particular, due to the similarity between the proposed method GAT, it is interesting to evaluate GAT by replacing its supervised loss by Eq. 7 as a compared method. Moreover, in fig. 4, other visualizations of other methods can be compared to demonstrate the difference between the proposed method and others.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper523/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper523/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Universal Self-Attention Network for Graph Classification", "authors": ["Dai Quoc Nguyen", "Tu Dinh Nguyen", "Dinh Phung"], "authorids": ["dai.nguyen@monash.edu", "tu.dinh.nguyen@monash.edu", "dinh.phung@monash.edu"], "keywords": ["Graph embedding", "graph classification", "universal self-attention network", "graph neural network"], "abstract": "Existing graph embedding models often have weaknesses in exploiting graph structure similarities, potential dependencies among nodes and global network properties. To this end, we present U2GAN, a novel unsupervised model leveraging on the strength of the recently introduced universal self-attention network (Dehghani et al., 2019), to learn low-dimensional embeddings of graphs which can be used for graph classification. In particular, given an input graph, U2GAN first applies a self-attention computation, which is then followed by a recurrent transition to iteratively memorize its attention on vector representations of each node and its neighbors across each iteration. Thus, U2GAN can address the weaknesses in the existing models in order to produce plausible node embeddings whose sum is the final embedding of the whole graph. Experimental results show that our unsupervised U2GAN produces new state-of-the-art performances on a range of well-known benchmark datasets for the graph classification task. It even outperforms supervised methods in most of benchmark cases.", "pdf": "/pdf/890c4ec4c1c8106153bfa0fca28406f98ef8184d.pdf", "paperhash": "nguyen|unsupervised_universal_selfattention_network_for_graph_classification", "original_pdf": "/attachment/949c4e9cde5186acf1c78f9cab891fd75caee3df.pdf", "_bibtex": "@misc{\nnguyen2020unsupervised,\ntitle={Unsupervised Universal Self-Attention Network for Graph Classification},\nauthor={Dai Quoc Nguyen and Tu Dinh Nguyen and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeLBpEFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJeLBpEFPB", "replyto": "HJeLBpEFPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper523/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper523/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575087888044, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper523/Reviewers"], "noninvitees": [], "tcdate": 1570237750909, "tmdate": 1575087888058, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper523/-/Official_Review"}}}, {"id": "BkltXbVRKB", "original": null, "number": 2, "cdate": 1571860768553, "ddate": null, "tcdate": 1571860768553, "tmdate": 1572972584760, "tddate": null, "forum": "HJeLBpEFPB", "replyto": "HJeLBpEFPB", "invitation": "ICLR.cc/2020/Conference/Paper523/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\n\n\nThe paper presents an new unsupervised model for graph classification. It borrows the idea from universal self-attention network and applies it to graph learning. It achieves surprisingly good results on benchmark datasets. Despite the good results, I do not think the technical quality is good enough to make it accepted. My concerns contain the following aspects:\n\n1.\tIf we compare the proposed model with the graph attention networks (GAT), it just adds the recurrent transition and the layer normalizer, which are also from the universal self-attention. This makes the paper not novel enough.  Furthermore, adding these components are not so related to unsupervised learning, it does not add any value to the unsupervised learning strategy.\n2.\tThe description of the unsupervised learning objective is not clear. From Algorithm 1, it seems $o_v$ is equal to $h_v^T$, I cannot understand the meaning of Eq. (7) at all.\n3.\tThe results are too good to be true. Although we cannot judge it based on this belief, the authors have to convince the readers and explain how the huge performance gain is obtained (on some datasets U2GAN is even 27% higher than all of other methods).  I understand the experimental setting is transductive, but even that cannot explain everything. To justify the experiments, the authors need to do a lot of ablation study, such as comparing with supervised learning version of this model, while in the paper there is no ablation model to explain it.\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper523/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper523/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Universal Self-Attention Network for Graph Classification", "authors": ["Dai Quoc Nguyen", "Tu Dinh Nguyen", "Dinh Phung"], "authorids": ["dai.nguyen@monash.edu", "tu.dinh.nguyen@monash.edu", "dinh.phung@monash.edu"], "keywords": ["Graph embedding", "graph classification", "universal self-attention network", "graph neural network"], "abstract": "Existing graph embedding models often have weaknesses in exploiting graph structure similarities, potential dependencies among nodes and global network properties. To this end, we present U2GAN, a novel unsupervised model leveraging on the strength of the recently introduced universal self-attention network (Dehghani et al., 2019), to learn low-dimensional embeddings of graphs which can be used for graph classification. In particular, given an input graph, U2GAN first applies a self-attention computation, which is then followed by a recurrent transition to iteratively memorize its attention on vector representations of each node and its neighbors across each iteration. Thus, U2GAN can address the weaknesses in the existing models in order to produce plausible node embeddings whose sum is the final embedding of the whole graph. Experimental results show that our unsupervised U2GAN produces new state-of-the-art performances on a range of well-known benchmark datasets for the graph classification task. It even outperforms supervised methods in most of benchmark cases.", "pdf": "/pdf/890c4ec4c1c8106153bfa0fca28406f98ef8184d.pdf", "paperhash": "nguyen|unsupervised_universal_selfattention_network_for_graph_classification", "original_pdf": "/attachment/949c4e9cde5186acf1c78f9cab891fd75caee3df.pdf", "_bibtex": "@misc{\nnguyen2020unsupervised,\ntitle={Unsupervised Universal Self-Attention Network for Graph Classification},\nauthor={Dai Quoc Nguyen and Tu Dinh Nguyen and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeLBpEFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJeLBpEFPB", "replyto": "HJeLBpEFPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper523/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper523/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575087888044, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper523/Reviewers"], "noninvitees": [], "tcdate": 1570237750909, "tmdate": 1575087888058, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper523/-/Official_Review"}}}, {"id": "Hkl1LHjCYS", "original": null, "number": 3, "cdate": 1571890503059, "ddate": null, "tcdate": 1571890503059, "tmdate": 1572972584718, "tddate": null, "forum": "HJeLBpEFPB", "replyto": "HJeLBpEFPB", "invitation": "ICLR.cc/2020/Conference/Paper523/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The submission proposes a graph neural network based on propagation with the attention mechanism. Then the output function uses the summation of node vectors to read out information about the graph. \n\n While the design is good, all components are all known techniques: the sampling procedure is like GraphSAGE; the propagation rule is similar to GAT, and the output function is wide uses in graph neural networks. \n\nCritics: \n\nThe writing is not clear. At the top of page 4: quote: \"... and produce an output sequence {h_vi^t}i=1^N+1\". Do you keep only the vector h_v1 and throw away other vectors? Because you will also put v_2 at the center and compute its vector in a different self-attention computation. If this is the case, why not just say the output is h_v1? If this is not the case, then each node will have multiple vectors: one is computed when the node is at the center, and others are computed when the node is sampled as a neighbor. \n\nBelow Boris Knyazev has several comments, which are not well addressed by authors. There is a discussion about transductive learning and inductive learning. However, it seems the authors still don't know how to run inductive learning on the graph classification task (quote \"... still do not have a standard inductive setting for the graph classification task where we only use a part of each graph during training.\"). Boris does not suggest to use part of each graph; instead, he suggests not using test graphs. I believe this is the standard practice in inductive learning (e.g. kernel methods). \n\nAnother comment from Boris about the case when T=1, and the response is \"T=1 does not correspond to a single layer network\". I don't understand the response either. When T=1, a node only gets information from its neighbors. It is similar to a one-layer GCN or GAT, in which a node also only gets information from its immediate neighbors. \n\nI also don't understand why the author insists that the proposed model has a layer-based architecture. In my view, it is a graph neural network by the standard of propagation rule and output function. \n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper523/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper523/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Universal Self-Attention Network for Graph Classification", "authors": ["Dai Quoc Nguyen", "Tu Dinh Nguyen", "Dinh Phung"], "authorids": ["dai.nguyen@monash.edu", "tu.dinh.nguyen@monash.edu", "dinh.phung@monash.edu"], "keywords": ["Graph embedding", "graph classification", "universal self-attention network", "graph neural network"], "abstract": "Existing graph embedding models often have weaknesses in exploiting graph structure similarities, potential dependencies among nodes and global network properties. To this end, we present U2GAN, a novel unsupervised model leveraging on the strength of the recently introduced universal self-attention network (Dehghani et al., 2019), to learn low-dimensional embeddings of graphs which can be used for graph classification. In particular, given an input graph, U2GAN first applies a self-attention computation, which is then followed by a recurrent transition to iteratively memorize its attention on vector representations of each node and its neighbors across each iteration. Thus, U2GAN can address the weaknesses in the existing models in order to produce plausible node embeddings whose sum is the final embedding of the whole graph. Experimental results show that our unsupervised U2GAN produces new state-of-the-art performances on a range of well-known benchmark datasets for the graph classification task. It even outperforms supervised methods in most of benchmark cases.", "pdf": "/pdf/890c4ec4c1c8106153bfa0fca28406f98ef8184d.pdf", "paperhash": "nguyen|unsupervised_universal_selfattention_network_for_graph_classification", "original_pdf": "/attachment/949c4e9cde5186acf1c78f9cab891fd75caee3df.pdf", "_bibtex": "@misc{\nnguyen2020unsupervised,\ntitle={Unsupervised Universal Self-Attention Network for Graph Classification},\nauthor={Dai Quoc Nguyen and Tu Dinh Nguyen and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeLBpEFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJeLBpEFPB", "replyto": "HJeLBpEFPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper523/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper523/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575087888044, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper523/Reviewers"], "noninvitees": [], "tcdate": 1570237750909, "tmdate": 1575087888058, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper523/-/Official_Review"}}}, {"id": "BJlxmiYO5B", "original": null, "number": 12, "cdate": 1572539160027, "ddate": null, "tcdate": 1572539160027, "tmdate": 1572790624870, "tddate": null, "forum": "HJeLBpEFPB", "replyto": "SkxGatT7cS", "invitation": "ICLR.cc/2020/Conference/Paper523/-/Official_Comment", "content": {"title": "Update some initial results of our \"supervised\" U2GAN", "comment": "We can see that our \"supervised\" U2GAN produces new state-of-the-art accuracies on DD, IMDBBINARY, IMDBMULTI, PROTEINS and PTC ; and obtains a competitive accuracies on MUTAG.\n\nDataset\t\t\t| \tResult (% accuracy)\n--------------------------------------------------------\nDD   \t\t\t|\t81.24 +- 1.84\nIMDBBINARY   \t|\t79.40 +- 4.35\nIMDBMULTI   \t|\t56.20 +- 3.35\nMUTAG   \t\t| \t89.97 +- 3.65\nPROTEINS   \t\t|\t78.53 +- 4.07\nPTC   \t\t   \t|\t79.36 +- 4.06\n-------------------------------------------------------\nWe will update the final results when we finish training and tuning the model's hyper-parameters for all 11 datasets.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper523/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Universal Self-Attention Network for Graph Classification", "authors": ["Dai Quoc Nguyen", "Tu Dinh Nguyen", "Dinh Phung"], "authorids": ["dai.nguyen@monash.edu", "tu.dinh.nguyen@monash.edu", "dinh.phung@monash.edu"], "keywords": ["Graph embedding", "graph classification", "universal self-attention network", "graph neural network"], "abstract": "Existing graph embedding models often have weaknesses in exploiting graph structure similarities, potential dependencies among nodes and global network properties. To this end, we present U2GAN, a novel unsupervised model leveraging on the strength of the recently introduced universal self-attention network (Dehghani et al., 2019), to learn low-dimensional embeddings of graphs which can be used for graph classification. In particular, given an input graph, U2GAN first applies a self-attention computation, which is then followed by a recurrent transition to iteratively memorize its attention on vector representations of each node and its neighbors across each iteration. Thus, U2GAN can address the weaknesses in the existing models in order to produce plausible node embeddings whose sum is the final embedding of the whole graph. Experimental results show that our unsupervised U2GAN produces new state-of-the-art performances on a range of well-known benchmark datasets for the graph classification task. It even outperforms supervised methods in most of benchmark cases.", "pdf": "/pdf/890c4ec4c1c8106153bfa0fca28406f98ef8184d.pdf", "paperhash": "nguyen|unsupervised_universal_selfattention_network_for_graph_classification", "original_pdf": "/attachment/949c4e9cde5186acf1c78f9cab891fd75caee3df.pdf", "_bibtex": "@misc{\nnguyen2020unsupervised,\ntitle={Unsupervised Universal Self-Attention Network for Graph Classification},\nauthor={Dai Quoc Nguyen and Tu Dinh Nguyen and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeLBpEFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeLBpEFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper523/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper523/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper523/Authors|ICLR.cc/2020/Conference/Paper523/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170173, "tmdate": 1576860534730, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper523/-/Official_Comment"}}}, {"id": "SkxGatT7cS", "original": null, "number": 11, "cdate": 1572227513681, "ddate": null, "tcdate": 1572227513681, "tmdate": 1572227513681, "tddate": null, "forum": "HJeLBpEFPB", "replyto": "HJeLBpEFPB", "invitation": "ICLR.cc/2020/Conference/Paper523/-/Official_Comment", "content": {"title": "Update our U2GAN in a \"supervised\" fashion", "comment": "To ensure fairness, we've followed several existing works in comparing supervised models with unsupervised models due to the use of the same 10-fold cross-validation scheme. \nWe observe that our unsupervised U2GAN significantly outperforms the supervised methods in most of benchmark cases.\nHowever, in light of Boris Knyazev's comment that we should not directly compare together, we have decided to further implement our U2GAN in a \"supervised\" fashion where we additionally construct a single fully connected layer to predict the graph labels. \nThis attempts to further provide additional evidence for the use of our model in general setting (either supervised or unsupervised).\nWe will update the results of our \"supervised\" U2GAN when we finish training models for all 11 experimental datasets.\n\nThanks."}, "signatures": ["ICLR.cc/2020/Conference/Paper523/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Universal Self-Attention Network for Graph Classification", "authors": ["Dai Quoc Nguyen", "Tu Dinh Nguyen", "Dinh Phung"], "authorids": ["dai.nguyen@monash.edu", "tu.dinh.nguyen@monash.edu", "dinh.phung@monash.edu"], "keywords": ["Graph embedding", "graph classification", "universal self-attention network", "graph neural network"], "abstract": "Existing graph embedding models often have weaknesses in exploiting graph structure similarities, potential dependencies among nodes and global network properties. To this end, we present U2GAN, a novel unsupervised model leveraging on the strength of the recently introduced universal self-attention network (Dehghani et al., 2019), to learn low-dimensional embeddings of graphs which can be used for graph classification. In particular, given an input graph, U2GAN first applies a self-attention computation, which is then followed by a recurrent transition to iteratively memorize its attention on vector representations of each node and its neighbors across each iteration. Thus, U2GAN can address the weaknesses in the existing models in order to produce plausible node embeddings whose sum is the final embedding of the whole graph. Experimental results show that our unsupervised U2GAN produces new state-of-the-art performances on a range of well-known benchmark datasets for the graph classification task. It even outperforms supervised methods in most of benchmark cases.", "pdf": "/pdf/890c4ec4c1c8106153bfa0fca28406f98ef8184d.pdf", "paperhash": "nguyen|unsupervised_universal_selfattention_network_for_graph_classification", "original_pdf": "/attachment/949c4e9cde5186acf1c78f9cab891fd75caee3df.pdf", "_bibtex": "@misc{\nnguyen2020unsupervised,\ntitle={Unsupervised Universal Self-Attention Network for Graph Classification},\nauthor={Dai Quoc Nguyen and Tu Dinh Nguyen and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeLBpEFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeLBpEFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper523/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper523/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper523/Authors|ICLR.cc/2020/Conference/Paper523/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170173, "tmdate": 1576860534730, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper523/-/Official_Comment"}}}, {"id": "H1eBhX99KS", "original": null, "number": 9, "cdate": 1571623852983, "ddate": null, "tcdate": 1571623852983, "tmdate": 1571631139117, "tddate": null, "forum": "HJeLBpEFPB", "replyto": "r1eTcsGcFS", "invitation": "ICLR.cc/2020/Conference/Paper523/-/Official_Comment", "content": {"comment": "Hi Pham, thank you very much for your comments.\n\nWe note that node embeddings o are in R^d; weight matrices K, Q and V are in R^{d x d}; and weight matrices W1 and W2 are in R^{k x d} and R^{d x k} respectively. We fix k to 1024; and d is the dimension of node feature vectors as shown in Table 1 (i.e., d is small). Thus, the training time is reasonable. For example, we show the average of training time for each epoch in the below table.\n\nDataset\t\t|\tAverage time for each epoch (minutes)\n------------------------------\nCOLLAB\t\t| \t2.66\nDD\t\t\t|\t1.40\nNCI109 \t\t|\t1.29\nNCI1 \t\t|\t1.27\nPROTEINS   \t|\t0.40\nIMDB-M \t|\t0.20\nIMDB-B \t\t|\t0.18\nPTC \t\t|\t0.07\nMUTAG \t\t|\t0.02\n------------------------------\n\nThanks", "title": "Training time is reasonable"}, "signatures": ["ICLR.cc/2020/Conference/Paper523/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Universal Self-Attention Network for Graph Classification", "authors": ["Dai Quoc Nguyen", "Tu Dinh Nguyen", "Dinh Phung"], "authorids": ["dai.nguyen@monash.edu", "tu.dinh.nguyen@monash.edu", "dinh.phung@monash.edu"], "keywords": ["Graph embedding", "graph classification", "universal self-attention network", "graph neural network"], "abstract": "Existing graph embedding models often have weaknesses in exploiting graph structure similarities, potential dependencies among nodes and global network properties. To this end, we present U2GAN, a novel unsupervised model leveraging on the strength of the recently introduced universal self-attention network (Dehghani et al., 2019), to learn low-dimensional embeddings of graphs which can be used for graph classification. In particular, given an input graph, U2GAN first applies a self-attention computation, which is then followed by a recurrent transition to iteratively memorize its attention on vector representations of each node and its neighbors across each iteration. Thus, U2GAN can address the weaknesses in the existing models in order to produce plausible node embeddings whose sum is the final embedding of the whole graph. Experimental results show that our unsupervised U2GAN produces new state-of-the-art performances on a range of well-known benchmark datasets for the graph classification task. It even outperforms supervised methods in most of benchmark cases.", "pdf": "/pdf/890c4ec4c1c8106153bfa0fca28406f98ef8184d.pdf", "paperhash": "nguyen|unsupervised_universal_selfattention_network_for_graph_classification", "original_pdf": "/attachment/949c4e9cde5186acf1c78f9cab891fd75caee3df.pdf", "_bibtex": "@misc{\nnguyen2020unsupervised,\ntitle={Unsupervised Universal Self-Attention Network for Graph Classification},\nauthor={Dai Quoc Nguyen and Tu Dinh Nguyen and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeLBpEFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeLBpEFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper523/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper523/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper523/Authors|ICLR.cc/2020/Conference/Paper523/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170173, "tmdate": 1576860534730, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper523/-/Official_Comment"}}}, {"id": "r1eTcsGcFS", "original": null, "number": 4, "cdate": 1571593109443, "ddate": null, "tcdate": 1571593109443, "tmdate": 1571593109443, "tddate": null, "forum": "HJeLBpEFPB", "replyto": "HJeLBpEFPB", "invitation": "ICLR.cc/2020/Conference/Paper523/-/Public_Comment", "content": {"comment": "Hi, it's a nice work and easy to follow. However, I wonder the training time because of a very large number of parameters in Transformer-based architectures.", "title": "Nice results. How about the training time?"}, "signatures": ["~Nam_T._Pham1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Nam_T._Pham1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Universal Self-Attention Network for Graph Classification", "authors": ["Dai Quoc Nguyen", "Tu Dinh Nguyen", "Dinh Phung"], "authorids": ["dai.nguyen@monash.edu", "tu.dinh.nguyen@monash.edu", "dinh.phung@monash.edu"], "keywords": ["Graph embedding", "graph classification", "universal self-attention network", "graph neural network"], "abstract": "Existing graph embedding models often have weaknesses in exploiting graph structure similarities, potential dependencies among nodes and global network properties. To this end, we present U2GAN, a novel unsupervised model leveraging on the strength of the recently introduced universal self-attention network (Dehghani et al., 2019), to learn low-dimensional embeddings of graphs which can be used for graph classification. In particular, given an input graph, U2GAN first applies a self-attention computation, which is then followed by a recurrent transition to iteratively memorize its attention on vector representations of each node and its neighbors across each iteration. Thus, U2GAN can address the weaknesses in the existing models in order to produce plausible node embeddings whose sum is the final embedding of the whole graph. Experimental results show that our unsupervised U2GAN produces new state-of-the-art performances on a range of well-known benchmark datasets for the graph classification task. It even outperforms supervised methods in most of benchmark cases.", "pdf": "/pdf/890c4ec4c1c8106153bfa0fca28406f98ef8184d.pdf", "paperhash": "nguyen|unsupervised_universal_selfattention_network_for_graph_classification", "original_pdf": "/attachment/949c4e9cde5186acf1c78f9cab891fd75caee3df.pdf", "_bibtex": "@misc{\nnguyen2020unsupervised,\ntitle={Unsupervised Universal Self-Attention Network for Graph Classification},\nauthor={Dai Quoc Nguyen and Tu Dinh Nguyen and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeLBpEFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeLBpEFPB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504207913, "tmdate": 1576860568391, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper523/-/Public_Comment"}}}, {"id": "SkeBb529_r", "original": null, "number": 5, "cdate": 1570585085219, "ddate": null, "tcdate": 1570585085219, "tmdate": 1570648462746, "tddate": null, "forum": "HJeLBpEFPB", "replyto": "rkx_gVw5uS", "invitation": "ICLR.cc/2020/Conference/Paper523/-/Official_Comment", "content": {"comment": "Thank you very much for your comments.\n\n2. We will discuss this in our paper if it is needful.\n\n3. You can see the \"Constructing U2GAN\" paragraph in page 3, we uniformly sample a set of N neighbors for each v \u2208 V, and then use node v and its N neighbors as an input sequence for the U2GAN learning process. We illustrate this in Figure 2.\nWe emphasize again that we do not construct a layer-based architecture like general GNNs.\n\n4. It is to note that K, Q and V are in R^{d x d}, in which d is the dimension of node feature vectors as shown in Table 1. For example, on MUTAG, K, Q and V are 7x7 matrices; and then W1, W2 are 1024x7 and 7x1024 matrices respectively.\n\nThanks.", "title": "K, Q and V are in R^{d x d}. d is the dimension of node feature vectors. not 1024."}, "signatures": ["ICLR.cc/2020/Conference/Paper523/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Universal Self-Attention Network for Graph Classification", "authors": ["Dai Quoc Nguyen", "Tu Dinh Nguyen", "Dinh Phung"], "authorids": ["dai.nguyen@monash.edu", "tu.dinh.nguyen@monash.edu", "dinh.phung@monash.edu"], "keywords": ["Graph embedding", "graph classification", "universal self-attention network", "graph neural network"], "abstract": "Existing graph embedding models often have weaknesses in exploiting graph structure similarities, potential dependencies among nodes and global network properties. To this end, we present U2GAN, a novel unsupervised model leveraging on the strength of the recently introduced universal self-attention network (Dehghani et al., 2019), to learn low-dimensional embeddings of graphs which can be used for graph classification. In particular, given an input graph, U2GAN first applies a self-attention computation, which is then followed by a recurrent transition to iteratively memorize its attention on vector representations of each node and its neighbors across each iteration. Thus, U2GAN can address the weaknesses in the existing models in order to produce plausible node embeddings whose sum is the final embedding of the whole graph. Experimental results show that our unsupervised U2GAN produces new state-of-the-art performances on a range of well-known benchmark datasets for the graph classification task. It even outperforms supervised methods in most of benchmark cases.", "pdf": "/pdf/890c4ec4c1c8106153bfa0fca28406f98ef8184d.pdf", "paperhash": "nguyen|unsupervised_universal_selfattention_network_for_graph_classification", "original_pdf": "/attachment/949c4e9cde5186acf1c78f9cab891fd75caee3df.pdf", "_bibtex": "@misc{\nnguyen2020unsupervised,\ntitle={Unsupervised Universal Self-Attention Network for Graph Classification},\nauthor={Dai Quoc Nguyen and Tu Dinh Nguyen and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeLBpEFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeLBpEFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper523/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper523/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper523/Authors|ICLR.cc/2020/Conference/Paper523/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170173, "tmdate": 1576860534730, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper523/-/Official_Comment"}}}, {"id": "rygwXZ6c_H", "original": null, "number": 7, "cdate": 1570586910754, "ddate": null, "tcdate": 1570586910754, "tmdate": 1570646911922, "tddate": null, "forum": "HJeLBpEFPB", "replyto": "HJgAQyv9_r", "invitation": "ICLR.cc/2020/Conference/Paper523/-/Official_Comment", "content": {"comment": "Again, thank you for your interests. \n\nAlthough the node classification task is out of domain in this topic, we are willing to discuss something.\n\nWe would like to suggest you to read papers [1] and [2] to see a standard inductive setting on Cora/Citeseer/Pubmed rather than the \"Deep Graph Infomax\". Follow this standard inductive setting where \"only a part of the input graph is used to train the node embedding model, and the trained model can be then used to infer embeddings for newly unseen nodes in the remaining part of the input graph\", you can see in [2] that GCN gives a low performance on Cora/Citeseer/Pubmed. Last year, we found the similar low performance with GAT on Cora/Citeseer/Pubmed in this standard inductive setting. We will verify the paper you mentioned in this setting when we have time.\n\n[1] is the first paper working on the standard inductive setting on Cora/Citeseer/Pubmed, but this setting is less mentioned. And nowadays, many papers only rely on another inductive setting from the GraphSAGE paper on the PPI dataset.\n\n[1] Revisiting Semi-supervised Learning with Graph Embeddings. ICML 2016.\n[2] Learning Graph Representations with Embedding Propagation. NIPS 2017.\n\nNote that the experimental setting for the graph classification task is similar to the transductive setting for the node classification task. You can see that the overview in our result table is similar to Table 2 in both the GCN and GAT papers where reporting the unsupervised and supervised models together. \n\nTo our best of knowledge, we still do not have a standard inductive setting for the graph classification task where we only use a part of each graph during training.\n\nThanks.\n", "title": "Regarding the inductive setting on the node classification task"}, "signatures": ["ICLR.cc/2020/Conference/Paper523/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Universal Self-Attention Network for Graph Classification", "authors": ["Dai Quoc Nguyen", "Tu Dinh Nguyen", "Dinh Phung"], "authorids": ["dai.nguyen@monash.edu", "tu.dinh.nguyen@monash.edu", "dinh.phung@monash.edu"], "keywords": ["Graph embedding", "graph classification", "universal self-attention network", "graph neural network"], "abstract": "Existing graph embedding models often have weaknesses in exploiting graph structure similarities, potential dependencies among nodes and global network properties. To this end, we present U2GAN, a novel unsupervised model leveraging on the strength of the recently introduced universal self-attention network (Dehghani et al., 2019), to learn low-dimensional embeddings of graphs which can be used for graph classification. In particular, given an input graph, U2GAN first applies a self-attention computation, which is then followed by a recurrent transition to iteratively memorize its attention on vector representations of each node and its neighbors across each iteration. Thus, U2GAN can address the weaknesses in the existing models in order to produce plausible node embeddings whose sum is the final embedding of the whole graph. Experimental results show that our unsupervised U2GAN produces new state-of-the-art performances on a range of well-known benchmark datasets for the graph classification task. It even outperforms supervised methods in most of benchmark cases.", "pdf": "/pdf/890c4ec4c1c8106153bfa0fca28406f98ef8184d.pdf", "paperhash": "nguyen|unsupervised_universal_selfattention_network_for_graph_classification", "original_pdf": "/attachment/949c4e9cde5186acf1c78f9cab891fd75caee3df.pdf", "_bibtex": "@misc{\nnguyen2020unsupervised,\ntitle={Unsupervised Universal Self-Attention Network for Graph Classification},\nauthor={Dai Quoc Nguyen and Tu Dinh Nguyen and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeLBpEFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeLBpEFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper523/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper523/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper523/Authors|ICLR.cc/2020/Conference/Paper523/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170173, "tmdate": 1576860534730, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper523/-/Official_Comment"}}}, {"id": "SJgjubaqOS", "original": null, "number": 8, "cdate": 1570586995004, "ddate": null, "tcdate": 1570586995004, "tmdate": 1570600475443, "tddate": null, "forum": "HJeLBpEFPB", "replyto": "HJgAQyv9_r", "invitation": "ICLR.cc/2020/Conference/Paper523/-/Official_Comment", "content": {"comment": "Thank you for your comments.\n\nYour point means that we should compare our unsupervised model only with other unsupervised models such as DGK and AWE. We agree with this point as most of the unsupervised models do the same thing.\n\nHowever, most of the supervised models consider that it is fair to compare the results from the unsupervised and supervised models together, because of using the same 10-fold cross-validation scheme (Please look into the result tables in the original papers of the supervised models we refer in our paper). Therefore, this is reason why we follow to include the supervised models and nicely separate them as shown in our result table.\n\nThanks.", "title": "Only focus on the graph classification task"}, "signatures": ["ICLR.cc/2020/Conference/Paper523/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Universal Self-Attention Network for Graph Classification", "authors": ["Dai Quoc Nguyen", "Tu Dinh Nguyen", "Dinh Phung"], "authorids": ["dai.nguyen@monash.edu", "tu.dinh.nguyen@monash.edu", "dinh.phung@monash.edu"], "keywords": ["Graph embedding", "graph classification", "universal self-attention network", "graph neural network"], "abstract": "Existing graph embedding models often have weaknesses in exploiting graph structure similarities, potential dependencies among nodes and global network properties. To this end, we present U2GAN, a novel unsupervised model leveraging on the strength of the recently introduced universal self-attention network (Dehghani et al., 2019), to learn low-dimensional embeddings of graphs which can be used for graph classification. In particular, given an input graph, U2GAN first applies a self-attention computation, which is then followed by a recurrent transition to iteratively memorize its attention on vector representations of each node and its neighbors across each iteration. Thus, U2GAN can address the weaknesses in the existing models in order to produce plausible node embeddings whose sum is the final embedding of the whole graph. Experimental results show that our unsupervised U2GAN produces new state-of-the-art performances on a range of well-known benchmark datasets for the graph classification task. It even outperforms supervised methods in most of benchmark cases.", "pdf": "/pdf/890c4ec4c1c8106153bfa0fca28406f98ef8184d.pdf", "paperhash": "nguyen|unsupervised_universal_selfattention_network_for_graph_classification", "original_pdf": "/attachment/949c4e9cde5186acf1c78f9cab891fd75caee3df.pdf", "_bibtex": "@misc{\nnguyen2020unsupervised,\ntitle={Unsupervised Universal Self-Attention Network for Graph Classification},\nauthor={Dai Quoc Nguyen and Tu Dinh Nguyen and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeLBpEFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeLBpEFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper523/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper523/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper523/Authors|ICLR.cc/2020/Conference/Paper523/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170173, "tmdate": 1576860534730, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper523/-/Official_Comment"}}}, {"id": "B1eMNak8dr", "original": null, "number": 3, "cdate": 1570270506146, "ddate": null, "tcdate": 1570270506146, "tmdate": 1570588467816, "tddate": null, "forum": "HJeLBpEFPB", "replyto": "HkxZJjgHOH", "invitation": "ICLR.cc/2020/Conference/Paper523/-/Official_Comment", "content": {"comment": "Thank you for your other comments.\n\n#2. The attention mechanism used in GAT extends from the standard attention technique in [1] and it does not use the query, key, and value projection matrices to compute the attention weights.\n\n#3. It is to note that T=1 does not correspond to a single layer network as we do not construct a layer-based architecture like GCN/GAT.\n\nIn our model, the representations of each node and its sampled neighbors are iteratively refined in each iteration. Therefore, each node can aggregate information from k-hops neighbors in subsequent iterations. Thus, our model work well even T=1. You can see our intuition in the last paragraph of page 3.\n\n#4. The weight matrices W1 and W2 in Eq. 4 are shared over all timesteps t. \n\nAs I mentioned in the above comment, the evaluation protocol is fair enough as many previous works report the published results taken from the original papers.\n\n#2&5. Thank you for pointing out the Graph Transformer Network and the Deep Graph Infomax which we do not know yet.  The architecture of the Graph Transformer Network is to derive graph-to-graph mapping which is quite different from our model. We will read the Deep Graph Infomax paper and see whether we can discuss the differences.  \n\n#7. The bionformatics datasets are sparse, but not for REDDIT (even when the average number of neighbors per node in REDDIT is around 2.3 as you see in Table 1) because there are central nodes which link to most of nodes in each graph, thus each graph in REDDIT is not sparse.\n\n\n[1] Neural Machine Translation by Jointly Learning to Align and Translate. ICLR 2015.\n\nWe hope you are fine with our answers.\nThanks.", "title": "Address other concerns"}, "signatures": ["ICLR.cc/2020/Conference/Paper523/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Universal Self-Attention Network for Graph Classification", "authors": ["Dai Quoc Nguyen", "Tu Dinh Nguyen", "Dinh Phung"], "authorids": ["dai.nguyen@monash.edu", "tu.dinh.nguyen@monash.edu", "dinh.phung@monash.edu"], "keywords": ["Graph embedding", "graph classification", "universal self-attention network", "graph neural network"], "abstract": "Existing graph embedding models often have weaknesses in exploiting graph structure similarities, potential dependencies among nodes and global network properties. To this end, we present U2GAN, a novel unsupervised model leveraging on the strength of the recently introduced universal self-attention network (Dehghani et al., 2019), to learn low-dimensional embeddings of graphs which can be used for graph classification. In particular, given an input graph, U2GAN first applies a self-attention computation, which is then followed by a recurrent transition to iteratively memorize its attention on vector representations of each node and its neighbors across each iteration. Thus, U2GAN can address the weaknesses in the existing models in order to produce plausible node embeddings whose sum is the final embedding of the whole graph. Experimental results show that our unsupervised U2GAN produces new state-of-the-art performances on a range of well-known benchmark datasets for the graph classification task. It even outperforms supervised methods in most of benchmark cases.", "pdf": "/pdf/890c4ec4c1c8106153bfa0fca28406f98ef8184d.pdf", "paperhash": "nguyen|unsupervised_universal_selfattention_network_for_graph_classification", "original_pdf": "/attachment/949c4e9cde5186acf1c78f9cab891fd75caee3df.pdf", "_bibtex": "@misc{\nnguyen2020unsupervised,\ntitle={Unsupervised Universal Self-Attention Network for Graph Classification},\nauthor={Dai Quoc Nguyen and Tu Dinh Nguyen and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeLBpEFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeLBpEFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper523/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper523/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper523/Authors|ICLR.cc/2020/Conference/Paper523/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170173, "tmdate": 1576860534730, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper523/-/Official_Comment"}}}, {"id": "rkx_gVw5uS", "original": null, "number": 3, "cdate": 1570563056362, "ddate": null, "tcdate": 1570563056362, "tmdate": 1570563056362, "tddate": null, "forum": "HJeLBpEFPB", "replyto": "B1eMNak8dr", "invitation": "ICLR.cc/2020/Conference/Paper523/-/Public_Comment", "content": {"comment": "2. I think this should be discussed in the paper with more details. \n3. This still sounds very much like a single layer graph network . From the formulas I don't see a sequence anywhere except for \"Coordinate embedding\", which you only apply to MUTAG and PROTEINS. \n4. It still would be useful to know the number of parameters. It looks like V, Q and K matrices alone will have 3x1024x1024, i.e. 3 million, parameters, which seems to be quite a lot for datasets such as MUTAG with 188 graphs.\n\nThanks.\n\n", "title": "Thanks "}, "signatures": ["~Boris_Knyazev1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Boris_Knyazev1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Universal Self-Attention Network for Graph Classification", "authors": ["Dai Quoc Nguyen", "Tu Dinh Nguyen", "Dinh Phung"], "authorids": ["dai.nguyen@monash.edu", "tu.dinh.nguyen@monash.edu", "dinh.phung@monash.edu"], "keywords": ["Graph embedding", "graph classification", "universal self-attention network", "graph neural network"], "abstract": "Existing graph embedding models often have weaknesses in exploiting graph structure similarities, potential dependencies among nodes and global network properties. To this end, we present U2GAN, a novel unsupervised model leveraging on the strength of the recently introduced universal self-attention network (Dehghani et al., 2019), to learn low-dimensional embeddings of graphs which can be used for graph classification. In particular, given an input graph, U2GAN first applies a self-attention computation, which is then followed by a recurrent transition to iteratively memorize its attention on vector representations of each node and its neighbors across each iteration. Thus, U2GAN can address the weaknesses in the existing models in order to produce plausible node embeddings whose sum is the final embedding of the whole graph. Experimental results show that our unsupervised U2GAN produces new state-of-the-art performances on a range of well-known benchmark datasets for the graph classification task. It even outperforms supervised methods in most of benchmark cases.", "pdf": "/pdf/890c4ec4c1c8106153bfa0fca28406f98ef8184d.pdf", "paperhash": "nguyen|unsupervised_universal_selfattention_network_for_graph_classification", "original_pdf": "/attachment/949c4e9cde5186acf1c78f9cab891fd75caee3df.pdf", "_bibtex": "@misc{\nnguyen2020unsupervised,\ntitle={Unsupervised Universal Self-Attention Network for Graph Classification},\nauthor={Dai Quoc Nguyen and Tu Dinh Nguyen and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeLBpEFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeLBpEFPB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504207913, "tmdate": 1576860568391, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper523/-/Public_Comment"}}}, {"id": "HJgAQyv9_r", "original": null, "number": 2, "cdate": 1570561830226, "ddate": null, "tcdate": 1570561830226, "tmdate": 1570561830226, "tddate": null, "forum": "HJeLBpEFPB", "replyto": "Bke5Dp18_B", "invitation": "ICLR.cc/2020/Conference/Paper523/-/Public_Comment", "content": {"comment": "Leveraging information about test graphs, such as graph structure, node features, etc. (but of course not labels!) during training is often called transductive learning. This is what people often do in node classification tasks Cora/Citeseer/Pubmed. In graph classification and in some other node classification tasks, people usually solve the inductive learning task, where we assume that we don't know anything about test graphs. \nTransductive and inductive learning methods should be compared separately, for example as in the \"Deep Graph Infomax\" paper in Table 2.\nSo, I think it would be nice to report the results with and without using test graph features. Otherwise, it's simply unclear where improvements are coming from.", "title": "Thanks for a response"}, "signatures": ["~Boris_Knyazev1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Boris_Knyazev1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Universal Self-Attention Network for Graph Classification", "authors": ["Dai Quoc Nguyen", "Tu Dinh Nguyen", "Dinh Phung"], "authorids": ["dai.nguyen@monash.edu", "tu.dinh.nguyen@monash.edu", "dinh.phung@monash.edu"], "keywords": ["Graph embedding", "graph classification", "universal self-attention network", "graph neural network"], "abstract": "Existing graph embedding models often have weaknesses in exploiting graph structure similarities, potential dependencies among nodes and global network properties. To this end, we present U2GAN, a novel unsupervised model leveraging on the strength of the recently introduced universal self-attention network (Dehghani et al., 2019), to learn low-dimensional embeddings of graphs which can be used for graph classification. In particular, given an input graph, U2GAN first applies a self-attention computation, which is then followed by a recurrent transition to iteratively memorize its attention on vector representations of each node and its neighbors across each iteration. Thus, U2GAN can address the weaknesses in the existing models in order to produce plausible node embeddings whose sum is the final embedding of the whole graph. Experimental results show that our unsupervised U2GAN produces new state-of-the-art performances on a range of well-known benchmark datasets for the graph classification task. It even outperforms supervised methods in most of benchmark cases.", "pdf": "/pdf/890c4ec4c1c8106153bfa0fca28406f98ef8184d.pdf", "paperhash": "nguyen|unsupervised_universal_selfattention_network_for_graph_classification", "original_pdf": "/attachment/949c4e9cde5186acf1c78f9cab891fd75caee3df.pdf", "_bibtex": "@misc{\nnguyen2020unsupervised,\ntitle={Unsupervised Universal Self-Attention Network for Graph Classification},\nauthor={Dai Quoc Nguyen and Tu Dinh Nguyen and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeLBpEFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeLBpEFPB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504207913, "tmdate": 1576860568391, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper523/-/Public_Comment"}}}, {"id": "Bke5Dp18_B", "original": null, "number": 4, "cdate": 1570270561877, "ddate": null, "tcdate": 1570270561877, "tmdate": 1570536517755, "tddate": null, "forum": "HJeLBpEFPB", "replyto": "HkxZJjgHOH", "invitation": "ICLR.cc/2020/Conference/Paper523/-/Official_Comment", "content": {"comment": "Thank you for your comments.\n\nWe would say that our evaluation protocol is correct. \n\nFor the graph classification task, it is to note that the unsupervised models do not use the graph labels during training, while the supervised models leverage on the graph labels to improve the accuracy performance. \n\nTherefore, many people (including you) usually see that the unsupervised models are outperformed by the supervised models, and then ignore a fact that the unsupervised models (including our model) can use the entire dataset ONLY w.r.t feature vectors and nodes, to learn graph embeddings. For more detail, you can check this fact in DGK [1], graph2vec [2], AWE [3] or in the code of another ICLR 2020 submission [4]. \n\nWe emphasize that the evaluation is fair enough; and other baselines and our model use the same 10-fold cross-validation scheme. In addition, we use the same data splits as used in [5,6] for our implementation. Our code is available only to Reviewers and ACs via a private comment.\n\n[1] Deep Graph Kernels. KDD 2015.\n[2] graph2vec: Learning Distributed Representations of Graphs. 2017.\n[3] Anonymous Walk Embeddings. ICML 2018.\n[4] HOW CAN WE GENERALISE LEARNING DISTRIBUTED REPRESENTATIONS OF GRAPHS? ICLR 2020 submission.\n\n[5] How Powerful are Graph Neural Networks? ICLR 2019.\n[6] An End-to-End Deep Learning Architecture for Graph Classification. AAAI 2018.\n\nTo this end, we respectfully disagree with your suggestion that the unsupervised models (e.g., [1,2,3,4]) should only use the training set to learn graph embeddings. Your suggestion is unfair, not only for the unsupervised models in the graph classification talk, but also in other classification tasks such as node classification (e.g., DeepWalk, Node2Vec and LINE); and besides, you do not know a fact that some supervised models such as GCN/GAT also use feature vectors of nodes in the test set during training on Cora/Citeseer/Pubmed.\n\nThanks.", "title": "The evaluation is fair enough and correct"}, "signatures": ["ICLR.cc/2020/Conference/Paper523/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Universal Self-Attention Network for Graph Classification", "authors": ["Dai Quoc Nguyen", "Tu Dinh Nguyen", "Dinh Phung"], "authorids": ["dai.nguyen@monash.edu", "tu.dinh.nguyen@monash.edu", "dinh.phung@monash.edu"], "keywords": ["Graph embedding", "graph classification", "universal self-attention network", "graph neural network"], "abstract": "Existing graph embedding models often have weaknesses in exploiting graph structure similarities, potential dependencies among nodes and global network properties. To this end, we present U2GAN, a novel unsupervised model leveraging on the strength of the recently introduced universal self-attention network (Dehghani et al., 2019), to learn low-dimensional embeddings of graphs which can be used for graph classification. In particular, given an input graph, U2GAN first applies a self-attention computation, which is then followed by a recurrent transition to iteratively memorize its attention on vector representations of each node and its neighbors across each iteration. Thus, U2GAN can address the weaknesses in the existing models in order to produce plausible node embeddings whose sum is the final embedding of the whole graph. Experimental results show that our unsupervised U2GAN produces new state-of-the-art performances on a range of well-known benchmark datasets for the graph classification task. It even outperforms supervised methods in most of benchmark cases.", "pdf": "/pdf/890c4ec4c1c8106153bfa0fca28406f98ef8184d.pdf", "paperhash": "nguyen|unsupervised_universal_selfattention_network_for_graph_classification", "original_pdf": "/attachment/949c4e9cde5186acf1c78f9cab891fd75caee3df.pdf", "_bibtex": "@misc{\nnguyen2020unsupervised,\ntitle={Unsupervised Universal Self-Attention Network for Graph Classification},\nauthor={Dai Quoc Nguyen and Tu Dinh Nguyen and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeLBpEFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeLBpEFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper523/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper523/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper523/Authors|ICLR.cc/2020/Conference/Paper523/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170173, "tmdate": 1576860534730, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper523/-/Official_Comment"}}}, {"id": "HkxZJjgHOH", "original": null, "number": 1, "cdate": 1570208473351, "ddate": null, "tcdate": 1570208473351, "tmdate": 1570208473351, "tddate": null, "forum": "HJeLBpEFPB", "replyto": "HJeLBpEFPB", "invitation": "ICLR.cc/2020/Conference/Paper523/-/Public_Comment", "content": {"comment": "This is an interesting work that tries to unify the Transformers and Graph Networks and proposed a model called U2GAN. The quantitative results are impressive.\n\nBut it would nice if the authors could address the following concerns.\n\n1. I'm not sure that the evaluation is correct. Please correct me if I'm wrong. \nIn Section 4.3, the authors say that \"For each dataset, after obtaining the graph embeddings, we perform the same evaluation process from ... , which is using 10-fold cross-validation scheme to calculate the classification performance for a fair comparison.\"\nIt sounds like the authors first use the entire dataset (train+val splits) to learn node embeddings. Then you sum node embeddings for each graph to obtain graph embeddings. Then you split the dataset into the train and val splits and train an SVM. If that's the case, it's unfair compared to baselines which do not have access to test graphs during training.\nThe authors should first split data and then use only the training set to learn node embeddings.\nPlease clarify. It would be also nice to provide a link to the code to check correctness.\n\n2. The self-attention mechanism looks very similar to attention over weights in Graph Attention Networks (GAT) [1]. In particular, Eq.(5,6) in this submission are basically the same as Eq.(3,4) in [1]. \nIt's also very similar to Graph Transformer Network [2]. The difference and contribution compared to both papers should be discussed. Empirical comparison would be a bonus. One difference is unsupervised training in this submission VS supervised training in [1,2]. What are other differences?\n\n3. Another concern is the results in Figure 3, where it's shown that even for T=1 the results are at least 92.5% for COLLAB and 91% for DD. But, as far as I understand, T=1 corresponds to a single layer graph network, in which each node has only access to its first-hop neighbors according to Section 3. How is it possible for a single layer network to perform so well?\nAblation studies should be performed to show the contribution of each component of the proposed model compared to some widely used baseline, such as GCN. How important is LayerNorm, residual connections? \n\n4. Also, are weights W1 and W2 in Eq.(4) shared over all propagation steps t? What's the number of trainable parameters in your model? Is it comparable to baselines? \nFor baselines, the authors used results reported in previous works, but this can be problematic, because the network architecture and hyperparameters can be very different making the comparison unfair. A better way is to run baseline experiments with the same hyperparameters and an equivalent architecture (or number of trainable parameters).\n\n5. It would be interesting to compare to \"Deep Graph Infomax\" [3], which is another way to train unsupervised. At least, the benefits of this submission compared to [3] should be discussed.\n\n6. Figure 4 does not tell me much without a comparison to the baseline embeddings.\n\n7. In Figures 3,5,6, results for N=4,8,16 for the bionformatics datasets and REDDIT should be very similar, because the graphs are very sparse. Why the results are different in some cases? I believe there is a lot of noise in those plots, so for a better comparison the plots should be computed for results averaged after multiple runs.\n\n8. U2GAN sounds confusing, because of \"GAN\" which makes me think it is related to Generative Adversarial Networks, which is not.\n\nThanks.\n\n[1] Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, Yoshua Bengio, Graph Attention Networks, ICLR, 2018\n[2] Yuan Li, Xiaodan Liang, Zhiting Hu, Yinbo Chen, Eric P. Xing, Graph Transformer Network, submitted to ICLR, 2019\n[3] Petar Veli\u010dkovi\u0107, William Fedus, William L. Hamilton, Pietro Li\u00f2, Yoshua Bengio, R Devon Hjelm, Deep Graph Infomax, ICLR 2019\n", "title": "Impressive results, but I'm skeptical about the correctness of evaluation"}, "signatures": ["~Boris_Knyazev1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Boris_Knyazev1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Universal Self-Attention Network for Graph Classification", "authors": ["Dai Quoc Nguyen", "Tu Dinh Nguyen", "Dinh Phung"], "authorids": ["dai.nguyen@monash.edu", "tu.dinh.nguyen@monash.edu", "dinh.phung@monash.edu"], "keywords": ["Graph embedding", "graph classification", "universal self-attention network", "graph neural network"], "abstract": "Existing graph embedding models often have weaknesses in exploiting graph structure similarities, potential dependencies among nodes and global network properties. To this end, we present U2GAN, a novel unsupervised model leveraging on the strength of the recently introduced universal self-attention network (Dehghani et al., 2019), to learn low-dimensional embeddings of graphs which can be used for graph classification. In particular, given an input graph, U2GAN first applies a self-attention computation, which is then followed by a recurrent transition to iteratively memorize its attention on vector representations of each node and its neighbors across each iteration. Thus, U2GAN can address the weaknesses in the existing models in order to produce plausible node embeddings whose sum is the final embedding of the whole graph. Experimental results show that our unsupervised U2GAN produces new state-of-the-art performances on a range of well-known benchmark datasets for the graph classification task. It even outperforms supervised methods in most of benchmark cases.", "pdf": "/pdf/890c4ec4c1c8106153bfa0fca28406f98ef8184d.pdf", "paperhash": "nguyen|unsupervised_universal_selfattention_network_for_graph_classification", "original_pdf": "/attachment/949c4e9cde5186acf1c78f9cab891fd75caee3df.pdf", "_bibtex": "@misc{\nnguyen2020unsupervised,\ntitle={Unsupervised Universal Self-Attention Network for Graph Classification},\nauthor={Dai Quoc Nguyen and Tu Dinh Nguyen and Dinh Phung},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeLBpEFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJeLBpEFPB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504207913, "tmdate": 1576860568391, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper523/Authors", "ICLR.cc/2020/Conference/Paper523/Reviewers", "ICLR.cc/2020/Conference/Paper523/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper523/-/Public_Comment"}}}], "count": 22}